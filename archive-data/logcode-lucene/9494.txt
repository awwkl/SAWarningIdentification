GitDiffStart: 7f243824d5eaf50c24d9c71d49e22bb7ecb47069 | Fri Feb 8 14:35:50 2013 +0000
diff --git a/lucene/CHANGES.txt b/lucene/CHANGES.txt
index 79301b0..40155a3 100644
--- a/lucene/CHANGES.txt
+++ b/lucene/CHANGES.txt
@@ -63,6 +63,9 @@ Changes in backwards compatibility policy
   FacetsAggregator. You can still use StandardFacetsAccumulator, which works
   with the old API (for now). (Shai Erera) 
 
+* LUCENE-4761: Facet packages reorganized. Should be easy to fix your import
+  statements, if you use an IDE such as Eclipse. (Shai Erera)
+
 Optimizations
 
 * LUCENE-4687: BloomFilterPostingsFormat now lazily initializes delegate
diff --git a/lucene/demo/src/java/org/apache/lucene/demo/facet/ExampleResult.java b/lucene/demo/src/java/org/apache/lucene/demo/facet/ExampleResult.java
index e0d3abd..144f6dc 100644
--- a/lucene/demo/src/java/org/apache/lucene/demo/facet/ExampleResult.java
+++ b/lucene/demo/src/java/org/apache/lucene/demo/facet/ExampleResult.java
@@ -2,7 +2,7 @@ package org.apache.lucene.demo.facet;
 
 import java.util.List;
 
-import org.apache.lucene.facet.search.results.FacetResult;
+import org.apache.lucene.facet.search.FacetResult;
 
 /*
  * Licensed to the Apache Software Foundation (ASF) under one or more
diff --git a/lucene/demo/src/java/org/apache/lucene/demo/facet/adaptive/AdaptiveMain.java b/lucene/demo/src/java/org/apache/lucene/demo/facet/adaptive/AdaptiveMain.java
index 994d03f..5b35a9b 100644
--- a/lucene/demo/src/java/org/apache/lucene/demo/facet/adaptive/AdaptiveMain.java
+++ b/lucene/demo/src/java/org/apache/lucene/demo/facet/adaptive/AdaptiveMain.java
@@ -10,7 +10,7 @@ import org.apache.lucene.demo.facet.ExampleUtils;
 import org.apache.lucene.demo.facet.simple.SimpleIndexer;
 import org.apache.lucene.demo.facet.simple.SimpleSearcher;
 import org.apache.lucene.facet.search.AdaptiveFacetsAccumulator;
-import org.apache.lucene.facet.search.results.FacetResult;
+import org.apache.lucene.facet.search.FacetResult;
 
 /*
  * Licensed to the Apache Software Foundation (ASF) under one or more
diff --git a/lucene/demo/src/java/org/apache/lucene/demo/facet/adaptive/AdaptiveSearcher.java b/lucene/demo/src/java/org/apache/lucene/demo/facet/adaptive/AdaptiveSearcher.java
index 481e4a3..2f17360 100644
--- a/lucene/demo/src/java/org/apache/lucene/demo/facet/adaptive/AdaptiveSearcher.java
+++ b/lucene/demo/src/java/org/apache/lucene/demo/facet/adaptive/AdaptiveSearcher.java
@@ -4,11 +4,11 @@ import java.util.List;
 
 import org.apache.lucene.demo.facet.ExampleUtils;
 import org.apache.lucene.demo.facet.simple.SimpleUtils;
+import org.apache.lucene.facet.params.FacetSearchParams;
 import org.apache.lucene.facet.search.AdaptiveFacetsAccumulator;
+import org.apache.lucene.facet.search.CountFacetRequest;
+import org.apache.lucene.facet.search.FacetResult;
 import org.apache.lucene.facet.search.FacetsCollector;
-import org.apache.lucene.facet.search.params.CountFacetRequest;
-import org.apache.lucene.facet.search.params.FacetSearchParams;
-import org.apache.lucene.facet.search.results.FacetResult;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
diff --git a/lucene/demo/src/java/org/apache/lucene/demo/facet/association/CategoryAssociationsMain.java b/lucene/demo/src/java/org/apache/lucene/demo/facet/association/CategoryAssociationsMain.java
index 2a80951..4961465 100644
--- a/lucene/demo/src/java/org/apache/lucene/demo/facet/association/CategoryAssociationsMain.java
+++ b/lucene/demo/src/java/org/apache/lucene/demo/facet/association/CategoryAssociationsMain.java
@@ -7,7 +7,7 @@ import org.apache.lucene.store.RAMDirectory;
 
 import org.apache.lucene.demo.facet.ExampleResult;
 import org.apache.lucene.demo.facet.ExampleUtils;
-import org.apache.lucene.facet.search.results.FacetResult;
+import org.apache.lucene.facet.search.FacetResult;
 
 /*
  * Licensed to the Apache Software Foundation (ASF) under one or more
diff --git a/lucene/demo/src/java/org/apache/lucene/demo/facet/association/CategoryAssociationsSearcher.java b/lucene/demo/src/java/org/apache/lucene/demo/facet/association/CategoryAssociationsSearcher.java
index 5e6d7ec..0e33db3 100644
--- a/lucene/demo/src/java/org/apache/lucene/demo/facet/association/CategoryAssociationsSearcher.java
+++ b/lucene/demo/src/java/org/apache/lucene/demo/facet/association/CategoryAssociationsSearcher.java
@@ -7,9 +7,9 @@ import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.store.Directory;
 
 import org.apache.lucene.demo.facet.simple.SimpleSearcher;
-import org.apache.lucene.facet.search.params.associations.AssociationFloatSumFacetRequest;
-import org.apache.lucene.facet.search.params.associations.AssociationIntSumFacetRequest;
-import org.apache.lucene.facet.search.results.FacetResult;
+import org.apache.lucene.facet.associations.AssociationFloatSumFacetRequest;
+import org.apache.lucene.facet.associations.AssociationIntSumFacetRequest;
+import org.apache.lucene.facet.search.FacetResult;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
diff --git a/lucene/demo/src/java/org/apache/lucene/demo/facet/multiCL/MultiCLIndexer.java b/lucene/demo/src/java/org/apache/lucene/demo/facet/multiCL/MultiCLIndexer.java
index d98d28e..56003c2 100644
--- a/lucene/demo/src/java/org/apache/lucene/demo/facet/multiCL/MultiCLIndexer.java
+++ b/lucene/demo/src/java/org/apache/lucene/demo/facet/multiCL/MultiCLIndexer.java
@@ -12,9 +12,9 @@ import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.TextField;
 import org.apache.lucene.facet.index.FacetFields;
-import org.apache.lucene.facet.index.params.CategoryListParams;
-import org.apache.lucene.facet.index.params.FacetIndexingParams;
-import org.apache.lucene.facet.index.params.PerDimensionIndexingParams;
+import org.apache.lucene.facet.params.CategoryListParams;
+import org.apache.lucene.facet.params.FacetIndexingParams;
+import org.apache.lucene.facet.params.PerDimensionIndexingParams;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
 import org.apache.lucene.index.IndexWriter;
diff --git a/lucene/demo/src/java/org/apache/lucene/demo/facet/multiCL/MultiCLMain.java b/lucene/demo/src/java/org/apache/lucene/demo/facet/multiCL/MultiCLMain.java
index c82ca1f..2628a9b 100644
--- a/lucene/demo/src/java/org/apache/lucene/demo/facet/multiCL/MultiCLMain.java
+++ b/lucene/demo/src/java/org/apache/lucene/demo/facet/multiCL/MultiCLMain.java
@@ -7,7 +7,7 @@ import org.apache.lucene.store.RAMDirectory;
 
 import org.apache.lucene.demo.facet.ExampleResult;
 import org.apache.lucene.demo.facet.ExampleUtils;
-import org.apache.lucene.facet.search.results.FacetResult;
+import org.apache.lucene.facet.search.FacetResult;
 
 /*
  * Licensed to the Apache Software Foundation (ASF) under one or more
diff --git a/lucene/demo/src/java/org/apache/lucene/demo/facet/multiCL/MultiCLSearcher.java b/lucene/demo/src/java/org/apache/lucene/demo/facet/multiCL/MultiCLSearcher.java
index 6b5b07f..ebbafc1 100644
--- a/lucene/demo/src/java/org/apache/lucene/demo/facet/multiCL/MultiCLSearcher.java
+++ b/lucene/demo/src/java/org/apache/lucene/demo/facet/multiCL/MultiCLSearcher.java
@@ -15,12 +15,12 @@ import org.apache.lucene.store.Directory;
 import org.apache.lucene.search.MultiCollector;
 import org.apache.lucene.demo.facet.ExampleUtils;
 import org.apache.lucene.demo.facet.simple.SimpleUtils;
-import org.apache.lucene.facet.index.params.FacetIndexingParams;
+import org.apache.lucene.facet.params.FacetIndexingParams;
+import org.apache.lucene.facet.params.FacetSearchParams;
+import org.apache.lucene.facet.search.CountFacetRequest;
+import org.apache.lucene.facet.search.FacetRequest;
+import org.apache.lucene.facet.search.FacetResult;
 import org.apache.lucene.facet.search.FacetsCollector;
-import org.apache.lucene.facet.search.params.CountFacetRequest;
-import org.apache.lucene.facet.search.params.FacetRequest;
-import org.apache.lucene.facet.search.params.FacetSearchParams;
-import org.apache.lucene.facet.search.results.FacetResult;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
diff --git a/lucene/demo/src/java/org/apache/lucene/demo/facet/simple/SimpleMain.java b/lucene/demo/src/java/org/apache/lucene/demo/facet/simple/SimpleMain.java
index 22448f8..1fe0eb5 100644
--- a/lucene/demo/src/java/org/apache/lucene/demo/facet/simple/SimpleMain.java
+++ b/lucene/demo/src/java/org/apache/lucene/demo/facet/simple/SimpleMain.java
@@ -9,7 +9,7 @@ import org.apache.lucene.store.RAMDirectory;
 
 import org.apache.lucene.demo.facet.ExampleResult;
 import org.apache.lucene.demo.facet.ExampleUtils;
-import org.apache.lucene.facet.search.results.FacetResult;
+import org.apache.lucene.facet.search.FacetResult;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
 
diff --git a/lucene/demo/src/java/org/apache/lucene/demo/facet/simple/SimpleSearcher.java b/lucene/demo/src/java/org/apache/lucene/demo/facet/simple/SimpleSearcher.java
index 10b14ca..a3ca726 100644
--- a/lucene/demo/src/java/org/apache/lucene/demo/facet/simple/SimpleSearcher.java
+++ b/lucene/demo/src/java/org/apache/lucene/demo/facet/simple/SimpleSearcher.java
@@ -4,14 +4,14 @@ import java.util.Iterator;
 import java.util.List;
 
 import org.apache.lucene.demo.facet.ExampleUtils;
-import org.apache.lucene.facet.index.params.FacetIndexingParams;
+import org.apache.lucene.facet.params.FacetIndexingParams;
+import org.apache.lucene.facet.params.FacetSearchParams;
+import org.apache.lucene.facet.search.CountFacetRequest;
 import org.apache.lucene.facet.search.DrillDown;
+import org.apache.lucene.facet.search.FacetRequest;
+import org.apache.lucene.facet.search.FacetResult;
+import org.apache.lucene.facet.search.FacetResultNode;
 import org.apache.lucene.facet.search.FacetsCollector;
-import org.apache.lucene.facet.search.params.CountFacetRequest;
-import org.apache.lucene.facet.search.params.FacetRequest;
-import org.apache.lucene.facet.search.params.FacetSearchParams;
-import org.apache.lucene.facet.search.results.FacetResult;
-import org.apache.lucene.facet.search.results.FacetResultNode;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.index.IndexReader;
diff --git a/lucene/demo/src/test/org/apache/lucene/demo/facet/TestAssociationExample.java b/lucene/demo/src/test/org/apache/lucene/demo/facet/TestAssociationExample.java
index a601078..cdb9a3c 100644
--- a/lucene/demo/src/test/org/apache/lucene/demo/facet/TestAssociationExample.java
+++ b/lucene/demo/src/test/org/apache/lucene/demo/facet/TestAssociationExample.java
@@ -5,7 +5,7 @@ import org.junit.Test;
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.demo.facet.ExampleResult;
 import org.apache.lucene.demo.facet.association.CategoryAssociationsMain;
-import org.apache.lucene.facet.search.results.FacetResultNode;
+import org.apache.lucene.facet.search.FacetResultNode;
 
 /*
  * Licensed to the Apache Software Foundation (ASF) under one or more
diff --git a/lucene/demo/src/test/org/apache/lucene/demo/facet/TestMultiCLExample.java b/lucene/demo/src/test/org/apache/lucene/demo/facet/TestMultiCLExample.java
index a422261..eaec26c 100644
--- a/lucene/demo/src/test/org/apache/lucene/demo/facet/TestMultiCLExample.java
+++ b/lucene/demo/src/test/org/apache/lucene/demo/facet/TestMultiCLExample.java
@@ -8,8 +8,8 @@ import org.junit.Test;
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.demo.facet.ExampleResult;
 import org.apache.lucene.demo.facet.multiCL.MultiCLMain;
-import org.apache.lucene.facet.search.results.FacetResult;
-import org.apache.lucene.facet.search.results.FacetResultNode;
+import org.apache.lucene.facet.search.FacetResult;
+import org.apache.lucene.facet.search.FacetResultNode;
 
 /*
  * Licensed to the Apache Software Foundation (ASF) under one or more
diff --git a/lucene/demo/src/test/org/apache/lucene/demo/facet/TestSimpleExample.java b/lucene/demo/src/test/org/apache/lucene/demo/facet/TestSimpleExample.java
index 7fb9e70..7a4b833 100644
--- a/lucene/demo/src/test/org/apache/lucene/demo/facet/TestSimpleExample.java
+++ b/lucene/demo/src/test/org/apache/lucene/demo/facet/TestSimpleExample.java
@@ -7,8 +7,8 @@ import org.junit.Test;
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.demo.facet.ExampleResult;
 import org.apache.lucene.demo.facet.simple.SimpleMain;
-import org.apache.lucene.facet.search.results.FacetResult;
-import org.apache.lucene.facet.search.results.FacetResultNode;
+import org.apache.lucene.facet.search.FacetResult;
+import org.apache.lucene.facet.search.FacetResultNode;
 
 /*
  * Licensed to the Apache Software Foundation (ASF) under one or more
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/FacetException.java b/lucene/facet/src/java/org/apache/lucene/facet/FacetException.java
deleted file mode 100644
index d028192..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/FacetException.java
+++ /dev/null
@@ -1,46 +0,0 @@
-package org.apache.lucene.facet;
-
-import java.io.IOException;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * A parent class for exceptions thrown by the Facets code.
- * 
- * @lucene.experimental
- */
-public class FacetException extends IOException {
-
-  public FacetException() {
-    super();
-  }
-
-  public FacetException(String message) {
-    super(message);
-  }
-
-  public FacetException(String message, Throwable cause) {
-    super(message);
-    initCause(cause);
-  }
-
-  public FacetException(Throwable cause) {
-    initCause(cause);
-  }
-
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/associations/AssociationFloatSumAggregator.java b/lucene/facet/src/java/org/apache/lucene/facet/associations/AssociationFloatSumAggregator.java
new file mode 100644
index 0000000..eee7c73
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/associations/AssociationFloatSumAggregator.java
@@ -0,0 +1,82 @@
+package org.apache.lucene.facet.associations;
+
+import java.io.IOException;
+
+import org.apache.lucene.facet.collections.IntToFloatMap;
+import org.apache.lucene.facet.params.CategoryListParams;
+import org.apache.lucene.facet.search.Aggregator;
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.util.IntsRef;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * An {@link Aggregator} which computes the weight of a category as the sum of
+ * the float values associated with it in the result documents.
+ * 
+ * @lucene.experimental
+ */
+public class AssociationFloatSumAggregator implements Aggregator {
+
+  protected final String field;
+  protected final float[] sumArray;
+  protected final FloatAssociationsIterator associations;
+
+  public AssociationFloatSumAggregator(float[] sumArray) throws IOException {
+    this(CategoryListParams.DEFAULT_FIELD, sumArray);
+  }
+  
+  public AssociationFloatSumAggregator(String field, float[] sumArray) throws IOException {
+    this.field = field;
+    associations = new FloatAssociationsIterator(field, new CategoryFloatAssociation());
+    this.sumArray = sumArray;
+  }
+
+  @Override
+  public void aggregate(int docID, float score, IntsRef ordinals) throws IOException {
+    IntToFloatMap values = associations.getAssociations(docID);
+    if (values != null) {
+      for (int i = 0; i < ordinals.length; i++) {
+        int ord = ordinals.ints[i];
+        if (values.containsKey(ord)) {
+          sumArray[ord] += values.get(ord);
+        }
+      }
+    }
+  }
+  
+  @Override
+  public boolean equals(Object obj) {
+    if (obj == null || obj.getClass() != this.getClass()) {
+      return false;
+    }
+    AssociationFloatSumAggregator that = (AssociationFloatSumAggregator) obj;
+    return that.field.equals(field) && that.sumArray == sumArray;
+  }
+
+  @Override
+  public int hashCode() {
+    return field.hashCode();
+  }
+
+  @Override
+  public boolean setNextReader(AtomicReaderContext context) throws IOException {
+    return associations.setNextReader(context);
+  }
+  
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/associations/AssociationFloatSumFacetRequest.java b/lucene/facet/src/java/org/apache/lucene/facet/associations/AssociationFloatSumFacetRequest.java
new file mode 100644
index 0000000..898598d
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/associations/AssociationFloatSumFacetRequest.java
@@ -0,0 +1,64 @@
+package org.apache.lucene.facet.associations;
+
+import java.io.IOException;
+
+import org.apache.lucene.facet.search.Aggregator;
+import org.apache.lucene.facet.search.FacetArrays;
+import org.apache.lucene.facet.search.FacetRequest;
+import org.apache.lucene.facet.taxonomy.CategoryPath;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * A {@link FacetRequest} for weighting facets according to their float
+ * association by summing the association values. Note that this class caches
+ * the associations data in-memory by default. You can override
+ * {@link #createAggregator(boolean, FacetArrays, TaxonomyReader)} to return an
+ * {@link AssociationFloatSumAggregator} which does otherwise.
+ * 
+ * @lucene.experimental
+ */
+public class AssociationFloatSumFacetRequest extends FacetRequest {
+
+  /**
+   * Create a float association facet request for a given node in the
+   * taxonomy.
+   */
+  public AssociationFloatSumFacetRequest(CategoryPath path, int num) {
+    super(path, num);
+  }
+
+  @Override
+  public Aggregator createAggregator(boolean useComplements, FacetArrays arrays, TaxonomyReader taxonomy) 
+      throws IOException {
+    assert !useComplements : "complements are not supported by this FacetRequest";
+    return new AssociationFloatSumAggregator(arrays.getFloatArray());
+  }
+
+  @Override
+  public double getValueOf(FacetArrays arrays, int ordinal) {
+    return arrays.getFloatArray()[ordinal];
+  }
+
+  @Override
+  public FacetArraysSource getFacetArraysSource() {
+    return FacetArraysSource.FLOAT;
+  }
+  
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/associations/AssociationIntSumAggregator.java b/lucene/facet/src/java/org/apache/lucene/facet/associations/AssociationIntSumAggregator.java
new file mode 100644
index 0000000..101f02f
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/associations/AssociationIntSumAggregator.java
@@ -0,0 +1,82 @@
+package org.apache.lucene.facet.associations;
+
+import java.io.IOException;
+
+import org.apache.lucene.facet.collections.IntToIntMap;
+import org.apache.lucene.facet.params.CategoryListParams;
+import org.apache.lucene.facet.search.Aggregator;
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.util.IntsRef;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * An {@link Aggregator} which computes the weight of a category as the sum of
+ * the integer values associated with it in the result documents.
+ * 
+ * @lucene.experimental
+ */
+public class AssociationIntSumAggregator implements Aggregator {
+
+  protected final String field;
+  protected final int[] sumArray;
+  protected final IntAssociationsIterator associations;
+
+  public AssociationIntSumAggregator(int[] sumArray) throws IOException {
+    this(CategoryListParams.DEFAULT_FIELD, sumArray);
+  }
+  
+  public AssociationIntSumAggregator(String field, int[] sumArray) throws IOException {
+    this.field = field;
+    associations = new IntAssociationsIterator(field, new CategoryIntAssociation());
+    this.sumArray = sumArray;
+  }
+
+  @Override
+  public void aggregate(int docID, float score, IntsRef ordinals) throws IOException {
+    IntToIntMap values = associations.getAssociations(docID);
+    if (values != null) {
+      for (int i = 0; i < ordinals.length; i++) {
+        int ord = ordinals.ints[i];
+        if (values.containsKey(ord)) {
+          sumArray[ord] += values.get(ord);
+        }
+      }
+    }
+  }
+  
+  @Override
+  public boolean equals(Object obj) {
+    if (obj == null || obj.getClass() != this.getClass()) {
+      return false;
+    }
+    AssociationIntSumAggregator that = (AssociationIntSumAggregator) obj;
+    return that.field.equals(field) && that.sumArray == sumArray;
+  }
+
+  @Override
+  public int hashCode() {
+    return field.hashCode();
+  }
+
+  @Override
+  public boolean setNextReader(AtomicReaderContext context) throws IOException {
+    return associations.setNextReader(context);
+  }
+
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/associations/AssociationIntSumFacetRequest.java b/lucene/facet/src/java/org/apache/lucene/facet/associations/AssociationIntSumFacetRequest.java
new file mode 100644
index 0000000..34bcf39
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/associations/AssociationIntSumFacetRequest.java
@@ -0,0 +1,64 @@
+package org.apache.lucene.facet.associations;
+
+import java.io.IOException;
+
+import org.apache.lucene.facet.search.Aggregator;
+import org.apache.lucene.facet.search.FacetArrays;
+import org.apache.lucene.facet.search.FacetRequest;
+import org.apache.lucene.facet.taxonomy.CategoryPath;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * A {@link FacetRequest} for weighting facets according to their integer
+ * association by summing the association values. Note that this class caches
+ * the associations data in-memory by default. You can override
+ * {@link #createAggregator(boolean, FacetArrays, TaxonomyReader)} to return an
+ * {@link AssociationFloatSumAggregator} which does otherwise.
+ * 
+ * @lucene.experimental
+ */
+public class AssociationIntSumFacetRequest extends FacetRequest {
+
+  /**
+   * Create an integer association facet request for a given node in the
+   * taxonomy.
+   */
+  public AssociationIntSumFacetRequest(CategoryPath path, int num) {
+    super(path, num);
+  }
+
+  @Override
+  public Aggregator createAggregator(boolean useComplements, FacetArrays arrays, TaxonomyReader taxonomy) 
+      throws IOException {
+    assert !useComplements : "complements are not supported by this FacetRequest";
+    return new AssociationIntSumAggregator(arrays.getIntArray());
+  }
+
+  @Override
+  public FacetArraysSource getFacetArraysSource() {
+    return FacetArraysSource.INT;
+  }
+
+  @Override
+  public double getValueOf(FacetArrays arrays, int ordinal) {
+    return arrays.getIntArray()[ordinal];
+  }
+
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/associations/AssociationIntSumFacetsAggregator.java b/lucene/facet/src/java/org/apache/lucene/facet/associations/AssociationIntSumFacetsAggregator.java
new file mode 100644
index 0000000..96c50d1
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/associations/AssociationIntSumFacetsAggregator.java
@@ -0,0 +1,47 @@
+package org.apache.lucene.facet.associations;
+
+import java.io.IOException;
+
+import org.apache.lucene.facet.params.CategoryListParams;
+import org.apache.lucene.facet.search.FacetArrays;
+import org.apache.lucene.facet.search.FacetRequest;
+import org.apache.lucene.facet.search.FacetsAggregator;
+import org.apache.lucene.facet.search.FacetsCollector.MatchingDocs;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * A {@link FacetsAggregator} which computes the weight of a category as the sum
+ * of the integer values associated with it in the result documents.
+ */
+public class AssociationIntSumFacetsAggregator implements FacetsAggregator {
+  
+  @Override
+  public void aggregate(MatchingDocs matchingDocs, CategoryListParams clp,
+      FacetArrays facetArrays) throws IOException {}
+  
+  @Override
+  public void rollupValues(FacetRequest fr, int ordinal, int[] children,
+      int[] siblings, FacetArrays facetArrays) {}
+  
+  @Override
+  public boolean requiresDocScores() {
+    return false;
+  }
+  
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/associations/AssociationsDrillDownStream.java b/lucene/facet/src/java/org/apache/lucene/facet/associations/AssociationsDrillDownStream.java
index fdc7ff6..c607fd7 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/associations/AssociationsDrillDownStream.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/associations/AssociationsDrillDownStream.java
@@ -2,7 +2,7 @@ package org.apache.lucene.facet.associations;
 
 import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
 import org.apache.lucene.facet.index.DrillDownStream;
-import org.apache.lucene.facet.index.params.FacetIndexingParams;
+import org.apache.lucene.facet.params.FacetIndexingParams;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.store.ByteArrayDataOutput;
 import org.apache.lucene.util.BytesRef;
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/associations/AssociationsFacetFields.java b/lucene/facet/src/java/org/apache/lucene/facet/associations/AssociationsFacetFields.java
index 0e61117..2cdfef2 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/associations/AssociationsFacetFields.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/associations/AssociationsFacetFields.java
@@ -10,8 +10,8 @@ import org.apache.lucene.document.TextField;
 import org.apache.lucene.facet.index.CountingListBuilder;
 import org.apache.lucene.facet.index.DrillDownStream;
 import org.apache.lucene.facet.index.FacetFields;
-import org.apache.lucene.facet.index.params.CategoryListParams;
-import org.apache.lucene.facet.index.params.FacetIndexingParams;
+import org.apache.lucene.facet.params.CategoryListParams;
+import org.apache.lucene.facet.params.FacetIndexingParams;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
 import org.apache.lucene.index.FieldInfo.IndexOptions;
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/associations/FloatAssociationsIterator.java b/lucene/facet/src/java/org/apache/lucene/facet/associations/FloatAssociationsIterator.java
index daa791e..e897e9f 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/associations/FloatAssociationsIterator.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/associations/FloatAssociationsIterator.java
@@ -2,7 +2,7 @@ package org.apache.lucene.facet.associations;
 
 import java.io.IOException;
 
-import org.apache.lucene.util.collections.IntToFloatMap;
+import org.apache.lucene.facet.collections.IntToFloatMap;
 
 /*
  * Licensed to the Apache Software Foundation (ASF) under one or more
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/associations/IntAssociationsIterator.java b/lucene/facet/src/java/org/apache/lucene/facet/associations/IntAssociationsIterator.java
index 0a7b74b..b60c2b6 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/associations/IntAssociationsIterator.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/associations/IntAssociationsIterator.java
@@ -2,7 +2,7 @@ package org.apache.lucene.facet.associations;
 
 import java.io.IOException;
 
-import org.apache.lucene.util.collections.IntToIntMap;
+import org.apache.lucene.facet.collections.IntToIntMap;
 
 /*
  * Licensed to the Apache Software Foundation (ASF) under one or more
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/associations/package.html b/lucene/facet/src/java/org/apache/lucene/facet/associations/package.html
index 79f2de7..e20828c 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/associations/package.html
+++ b/lucene/facet/src/java/org/apache/lucene/facet/associations/package.html
@@ -19,11 +19,7 @@
 <title>Category Association</title>
 </head>
 <body>
-<h1>Category Association</h1>
-
-Allows association of arbitrary values with a category. The value can be used e.g. to compute
-the category's weight during faceted search. Two association implementations exist for
-{@link org.apache.lucene.facet.associations.CategoryIntAssociation int} and
-{@link org.apache.lucene.facet.associations.CategoryFloatAssociation float} values.
+Allows associating arbitrary values with a category. The value can be used e.g. to compute
+the category's weight during faceted search.
 </body>
 </html>
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/associations/search/AssociationIntSumFacetsAggregator.java b/lucene/facet/src/java/org/apache/lucene/facet/associations/search/AssociationIntSumFacetsAggregator.java
deleted file mode 100644
index 0f5fb15..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/associations/search/AssociationIntSumFacetsAggregator.java
+++ /dev/null
@@ -1,46 +0,0 @@
-package org.apache.lucene.facet.associations.search;
-
-import java.io.IOException;
-
-import org.apache.lucene.facet.index.params.CategoryListParams;
-import org.apache.lucene.facet.search.FacetArrays;
-import org.apache.lucene.facet.search.FacetsAggregator;
-import org.apache.lucene.facet.search.FacetsCollector.MatchingDocs;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * A {@link FacetsAggregator} which computes the weight of a category as the sum
- * of the integer values associated with it in the result documents.
- */
-public class AssociationIntSumFacetsAggregator implements FacetsAggregator {
-  
-  @Override
-  public void aggregate(MatchingDocs matchingDocs, CategoryListParams clp,
-      FacetArrays facetArrays) throws IOException {}
-  
-  @Override
-  public void rollupValues(int ordinal, int[] children, int[] siblings,
-      FacetArrays facetArrays) {}
-  
-  @Override
-  public boolean requiresDocScores() {
-    return false;
-  }
-  
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/associations/search/package.html b/lucene/facet/src/java/org/apache/lucene/facet/associations/search/package.html
deleted file mode 100644
index 2925f07..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/associations/search/package.html
+++ /dev/null
@@ -1,26 +0,0 @@
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-<title>Category Association</title>
-</head>
-<body>
-<h1>Category Association</h1>
-
-Associations search code
-</body>
-</html>
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/collections/ArrayHashMap.java b/lucene/facet/src/java/org/apache/lucene/facet/collections/ArrayHashMap.java
new file mode 100644
index 0000000..93fe9e1
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/collections/ArrayHashMap.java
@@ -0,0 +1,554 @@
+package org.apache.lucene.facet.collections;
+
+import java.util.Arrays;
+import java.util.Iterator;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * An Array-based hashtable which maps, similar to Java's HashMap, only
+ * performance tests showed it performs better.
+ * <p>
+ * The hashtable is constructed with a given capacity, or 16 as a default. In
+ * case there's not enough room for new pairs, the hashtable grows. Capacity is
+ * adjusted to a power of 2, and there are 2 * capacity entries for the hash.
+ * The pre allocated arrays (for keys, values) are at length of capacity + 1,
+ * where index 0 is used as 'Ground' or 'NULL'.
+ * <p>
+ * The arrays are allocated ahead of hash operations, and form an 'empty space'
+ * list, to which the &lt;key,value&gt; pair is allocated.
+ * 
+ * @lucene.experimental
+ */
+public class ArrayHashMap<K,V> implements Iterable<V> {
+
+  /** Implements an IntIterator which iterates over all the allocated indexes. */
+  private final class IndexIterator implements IntIterator {
+    /**
+     * The last used baseHashIndex. Needed for "jumping" from one hash entry
+     * to another.
+     */
+    private int baseHashIndex = 0;
+
+    /** The next not-yet-visited index. */
+    private int index = 0;
+
+    /** Index of the last visited pair. Used in {@link #remove()}. */
+    private int lastIndex = 0;
+
+    /**
+     * Create the Iterator, make <code>index</code> point to the "first"
+     * index which is not empty. If such does not exist (eg. the map is
+     * empty) it would be zero.
+     */
+    public IndexIterator() {
+      for (baseHashIndex = 0; baseHashIndex < baseHash.length; ++baseHashIndex) {
+        index = baseHash[baseHashIndex];
+        if (index != 0) {
+          break;
+        }
+      }
+    }
+
+    @Override
+    public boolean hasNext() {
+      return index != 0;
+    }
+
+    @Override
+    public int next() {
+      // Save the last index visited
+      lastIndex = index;
+
+      // next the index
+      index = next[index];
+
+      // if the next index points to the 'Ground' it means we're done with
+      // the current hash entry and we need to jump to the next one. This
+      // is done until all the hash entries had been visited.
+      while (index == 0 && ++baseHashIndex < baseHash.length) {
+        index = baseHash[baseHashIndex];
+      }
+
+      return lastIndex;
+    }
+
+    @Override
+    @SuppressWarnings("unchecked")
+    public void remove() {
+      ArrayHashMap.this.remove((K) keys[lastIndex]);
+    }
+
+  }
+
+  /** Implements an Iterator, used for iteration over the map's keys. */
+  private final class KeyIterator implements Iterator<K> {
+    private IntIterator iterator = new IndexIterator();
+
+    KeyIterator() { }
+
+    @Override
+    public boolean hasNext() {
+      return iterator.hasNext();
+    }
+
+    @Override
+    @SuppressWarnings("unchecked")
+    public K next() {
+      return (K) keys[iterator.next()];
+    }
+
+    @Override
+    public void remove() {
+      iterator.remove();
+    }
+  }
+
+  /** Implements an Iterator, used for iteration over the map's values. */
+  private final class ValueIterator implements Iterator<V> {
+    private IntIterator iterator = new IndexIterator();
+
+    ValueIterator() { }
+
+    @Override
+    public boolean hasNext() {
+      return iterator.hasNext();
+    }
+
+    @Override
+    @SuppressWarnings("unchecked")
+    public V next() {
+      return (V) values[iterator.next()];
+    }
+
+    @Override
+    public void remove() {
+      iterator.remove();
+    }
+  }
+
+  /** Default capacity - in case no capacity was specified in the constructor */
+  private static final int DEFAULT_CAPACITY = 16;
+
+  /**
+   * Holds the base hash entries. if the capacity is 2^N, than the base hash
+   * holds 2^(N+1).
+   */
+  int[] baseHash;
+
+  /**
+   * The current capacity of the map. Always 2^N and never less than 16. We
+   * never use the zero index. It is needed to improve performance and is also
+   * used as "ground".
+   */
+  private int capacity;
+
+  /**
+   * All objects are being allocated at map creation. Those objects are "free"
+   * or empty. Whenever a new pair comes along, a pair is being "allocated" or
+   * taken from the free-linked list. as this is just a free list.
+   */
+  private int firstEmpty;
+
+  /** hashFactor is always (2^(N+1)) - 1. Used for faster hashing. */
+  private int hashFactor;
+
+  /** Holds the unique keys. */
+  Object[] keys;
+
+  /**
+   * In case of collisions, we implement a double linked list of the colliding
+   * hash's with the following next[] and prev[]. Those are also used to store
+   * the "empty" list.
+   */
+  int[] next;
+
+  private int prev;
+
+  /** Number of currently stored objects in the map. */
+  private int size;
+
+  /** Holds the values. */
+  Object[] values;
+
+  /** Constructs a map with default capacity. */
+  public ArrayHashMap() {
+    this(DEFAULT_CAPACITY);
+  }
+
+  /**
+   * Constructs a map with given capacity. Capacity is adjusted to a native
+   * power of 2, with minimum of 16.
+   * 
+   * @param capacity minimum capacity for the map.
+   */
+  public ArrayHashMap(int capacity) {
+    this.capacity = 16;
+    while (this.capacity < capacity) {
+      // Multiply by 2 as long as we're still under the requested capacity
+      this.capacity <<= 1;
+    }
+
+    // As mentioned, we use the first index (0) as 'Ground', so we need the
+    // length of the arrays to be one more than the capacity
+    int arrayLength = this.capacity + 1;
+
+    values = new Object[arrayLength];
+    keys = new Object[arrayLength];
+    next = new int[arrayLength];
+
+    // Hash entries are twice as big as the capacity.
+    int baseHashSize = this.capacity << 1;
+
+    baseHash = new int[baseHashSize];
+
+    // The has factor is 2^M - 1 which is used as an "AND" hashing operator.
+    // {@link #calcBaseHash()}
+    hashFactor = baseHashSize - 1;
+
+    size = 0;
+
+    clear();
+  }
+
+  /**
+   * Adds a pair to the map. Takes the first empty position from the
+   * empty-linked-list's head - {@link #firstEmpty}. New pairs are always
+   * inserted to baseHash, and are followed by the old colliding pair.
+   */
+  private void prvt_put(K key, V value) {
+    // Hash entry to which the new pair would be inserted
+    int hashIndex = calcBaseHashIndex(key);
+
+    // 'Allocating' a pair from the "Empty" list.
+    int objectIndex = firstEmpty;
+
+    // Setting data
+    firstEmpty = next[firstEmpty];
+    values[objectIndex] = value;
+    keys[objectIndex] = key;
+
+    // Inserting the new pair as the first node in the specific hash entry
+    next[objectIndex] = baseHash[hashIndex];
+    baseHash[hashIndex] = objectIndex;
+
+    // Announcing a new pair was added!
+    ++size;
+  }
+
+  /** Calculating the baseHash index using the internal internal <code>hashFactor</code>. */
+  protected int calcBaseHashIndex(K key) {
+    return key.hashCode() & hashFactor;
+  }
+
+  /** Empties the map. Generates the "Empty" space list for later allocation. */
+  public void clear() {
+    // Clears the hash entries
+    Arrays.fill(baseHash, 0);
+
+    // Set size to zero
+    size = 0;
+
+    // Mark all array entries as empty. This is done with
+    // <code>firstEmpty</code> pointing to the first valid index (1 as 0 is
+    // used as 'Ground').
+    firstEmpty = 1;
+
+    // And setting all the <code>next[i]</code> to point at
+    // <code>i+1</code>.
+    for (int i = 1; i < capacity;) {
+      next[i] = ++i;
+    }
+
+    // Surly, the last one should point to the 'Ground'.
+    next[capacity] = 0;
+  }
+
+  /** Returns true iff the key exists in the map. */
+  public boolean containsKey(K key) {
+    return find(key) != 0;
+  }
+
+  /** Returns true iff the object exists in the map. */
+  public boolean containsValue(Object o) {
+    for (Iterator<V> iterator = iterator(); iterator.hasNext();) {
+      V object = iterator.next();
+      if (object.equals(o)) {
+        return true;
+      }
+    }
+    return false;
+  }
+
+  /** Returns the index of the given key, or zero if the key wasn't found. */
+  protected int find(K key) {
+    // Calculate the hash entry.
+    int baseHashIndex = calcBaseHashIndex(key);
+
+    // Start from the hash entry.
+    int localIndex = baseHash[baseHashIndex];
+
+    // while the index does not point to the 'Ground'
+    while (localIndex != 0) {
+      // returns the index found in case of of a matching key.
+      if (keys[localIndex].equals(key)) {
+        return localIndex;
+      }
+
+      // next the local index
+      localIndex = next[localIndex];
+    }
+
+    // If we got this far, it could only mean we did not find the key we
+    // were asked for. return 'Ground' index.
+    return 0;
+  }
+
+  /**
+   * Finds the actual index of a given key with it's baseHashIndex. Some methods
+   * use the baseHashIndex. If those call {@link #find} there's no need to
+   * re-calculate that hash.
+   * 
+   * @return the index of the given key, or 0 if the key wasn't found.
+   */
+  private int findForRemove(K key, int baseHashIndex) {
+    // Start from the hash entry.
+    prev = 0;
+    int index = baseHash[baseHashIndex];
+
+    // while the index does not point to the 'Ground'
+    while (index != 0) {
+      // returns the index found in case of of a matching key.
+      if (keys[index].equals(key)) {
+        return index;
+      }
+
+      // next the local index
+      prev = index;
+      index = next[index];
+    }
+
+    // If we got thus far, it could only mean we did not find the key we
+    // were asked for. return 'Ground' index.
+    return prev = 0;
+  }
+
+  /** Returns the object mapped with the given key, or null if the key wasn't found. */
+  @SuppressWarnings("unchecked")
+  public V get(K key) {
+    return (V) values[find(key)];
+  }
+
+  /**
+   * Allocates a new map of double the capacity, and fast-insert the old
+   * key-value pairs.
+   */
+  @SuppressWarnings("unchecked")
+  protected void grow() {
+    ArrayHashMap<K,V> newmap = new ArrayHashMap<K,V>(capacity * 2);
+
+    // Iterates fast over the collection. Any valid pair is put into the new
+    // map without checking for duplicates or if there's enough space for
+    // it.
+    for (IndexIterator iterator = new IndexIterator(); iterator.hasNext();) {
+      int index = iterator.next();
+      newmap.prvt_put((K) keys[index], (V) values[index]);
+    }
+
+    // Copy that's data into this.
+    capacity = newmap.capacity;
+    size = newmap.size;
+    firstEmpty = newmap.firstEmpty;
+    values = newmap.values;
+    keys = newmap.keys;
+    next = newmap.next;
+    baseHash = newmap.baseHash;
+    hashFactor = newmap.hashFactor;
+  }
+
+  /** Returns true iff the map is empty. */
+  public boolean isEmpty() {
+    return size == 0;
+  }
+
+  /** Returns an iterator on the mapped objects. */
+  @Override
+  public Iterator<V> iterator() {
+    return new ValueIterator();
+  }
+
+  /** Returns an iterator on the map keys. */
+  public Iterator<K> keyIterator() {
+    return new KeyIterator();
+  }
+
+  /** Prints the baseHash array, used for debugging purposes. */
+  @SuppressWarnings("unused")
+  private String getBaseHashAsString() {
+    return Arrays.toString(this.baseHash);
+  }
+
+  /**
+   * Inserts the &lt;key,value&gt; pair into the map. If the key already exists,
+   * this method updates the mapped value to the given one, returning the old
+   * mapped value.
+   * 
+   * @return the old mapped value, or null if the key didn't exist.
+   */
+  @SuppressWarnings("unchecked")
+  public V put(K key, V e) {
+    // Does key exists?
+    int index = find(key);
+
+    // Yes!
+    if (index != 0) {
+      // Set new data and exit.
+      V old = (V) values[index];
+      values[index] = e;
+      return old;
+    }
+
+    // Is there enough room for a new pair?
+    if (size == capacity) {
+      // No? Than grow up!
+      grow();
+    }
+
+    // Now that everything is set, the pair can be just put inside with no
+    // worries.
+    prvt_put(key, e);
+
+    return null;
+  }
+
+  /**
+   * Removes a &lt;key,value&gt; pair from the map and returns the mapped value,
+   * or null if the none existed.
+   * 
+   * @param key used to find the value to remove
+   * @return the removed value or null if none existed.
+   */
+  @SuppressWarnings("unchecked")
+  public V remove(K key) {
+    int baseHashIndex = calcBaseHashIndex(key);
+    int index = findForRemove(key, baseHashIndex);
+    if (index != 0) {
+      // If it is the first in the collision list, we should promote its
+      // next colliding element.
+      if (prev == 0) {
+        baseHash[baseHashIndex] = next[index];
+      }
+
+      next[prev] = next[index];
+      next[index] = firstEmpty;
+      firstEmpty = index;
+      --size;
+      return (V) values[index];
+    }
+
+    return null;
+  }
+
+  /** Returns number of pairs currently in the map. */
+  public int size() {
+    return this.size;
+  }
+
+  /**
+   * Translates the mapped pairs' values into an array of Objects
+   * 
+   * @return an object array of all the values currently in the map.
+   */
+  public Object[] toArray() {
+    int j = -1;
+    Object[] array = new Object[size];
+
+    // Iterates over the values, adding them to the array.
+    for (Iterator<V> iterator = iterator(); iterator.hasNext();) {
+      array[++j] = iterator.next();
+    }
+    return array;
+  }
+
+  /**
+   * Translates the mapped pairs' values into an array of V
+   * 
+   * @param a the array into which the elements of the list are to be stored, if
+   *        it is big enough; otherwise, use as much space as it can.
+   * @return an array containing the elements of the list
+   */
+  public V[] toArray(V[] a) {
+    int j = 0;
+    // Iterates over the values, adding them to the array.
+    for (Iterator<V> iterator = iterator(); j < a.length
+    && iterator.hasNext(); ++j) {
+      a[j] = iterator.next();
+    }
+    if (j < a.length) {
+      a[j] = null;
+    }
+
+    return a;
+  }
+
+  @Override
+  public String toString() {
+    StringBuffer sb = new StringBuffer();
+    sb.append('{');
+    Iterator<K> keyIterator = keyIterator();
+    while (keyIterator.hasNext()) {
+      K key = keyIterator.next();
+      sb.append(key);
+      sb.append('=');
+      sb.append(get(key));
+      if (keyIterator.hasNext()) {
+        sb.append(',');
+        sb.append(' ');
+      }
+    }
+    sb.append('}');
+    return sb.toString();
+  }
+
+  @Override
+  public int hashCode() {
+    return getClass().hashCode() ^ size();
+  }
+
+  @SuppressWarnings("unchecked")
+  @Override
+  public boolean equals(Object o) {
+    ArrayHashMap<K, V> that = (ArrayHashMap<K,V>)o;
+    if (that.size() != this.size()) {
+      return false;
+    }
+
+    Iterator<K> it = keyIterator();
+    while (it.hasNext()) {
+      K key = it.next();
+      V v1 = this.get(key);
+      V v2 = that.get(key);
+      if ((v1 == null && v2 != null) ||
+          (v1 != null && v2 == null) ||
+          (!v1.equals(v2))) {
+        return false;
+      }
+    }
+    return true;
+  }
+}
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/collections/DoubleIterator.java b/lucene/facet/src/java/org/apache/lucene/facet/collections/DoubleIterator.java
new file mode 100644
index 0000000..00a0e73
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/collections/DoubleIterator.java
@@ -0,0 +1,31 @@
+package org.apache.lucene.facet.collections;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Iterator interface for primitive double iteration. *
+ * 
+ * @lucene.experimental
+ */
+public interface DoubleIterator {
+
+  boolean hasNext();
+  double next();
+  void remove();
+
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/collections/FloatIterator.java b/lucene/facet/src/java/org/apache/lucene/facet/collections/FloatIterator.java
new file mode 100644
index 0000000..677b6fa
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/collections/FloatIterator.java
@@ -0,0 +1,31 @@
+package org.apache.lucene.facet.collections;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Iterator interface for primitive int iteration. *
+ * 
+ * @lucene.experimental
+ */
+public interface FloatIterator {
+
+  boolean hasNext();
+  float next();
+  void remove();
+
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/collections/FloatToObjectMap.java b/lucene/facet/src/java/org/apache/lucene/facet/collections/FloatToObjectMap.java
new file mode 100644
index 0000000..da5c4f8
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/collections/FloatToObjectMap.java
@@ -0,0 +1,634 @@
+package org.apache.lucene.facet.collections;
+
+import java.util.Arrays;
+import java.util.Iterator;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+
+ * An Array-based hashtable which maps primitive float to Objects of generic type
+ * T.<br>
+ * The hashtable is constracted with a given capacity, or 16 as a default. In
+ * case there's not enough room for new pairs, the hashtable grows. <br>
+ * Capacity is adjusted to a power of 2, and there are 2 * capacity entries for
+ * the hash.
+ * 
+ * The pre allocated arrays (for keys, values) are at length of capacity + 1,
+ * when index 0 is used as 'Ground' or 'NULL'.<br>
+ * 
+ * The arrays are allocated ahead of hash operations, and form an 'empty space'
+ * list, to which the key,value pair is allocated.
+ * 
+ * @lucene.experimental
+ */
+public class FloatToObjectMap<T> implements Iterable<T> {
+
+  /**
+   * Implements an IntIterator which iterates over all the allocated indexes.
+   */
+  private final class IndexIterator implements IntIterator {
+    /**
+     * The last used baseHashIndex. Needed for "jumping" from one hash entry
+     * to another.
+     */
+    private int baseHashIndex = 0;
+
+    /**
+     * The next not-yet-visited index.
+     */
+    private int index = 0;
+
+    /**
+     * Index of the last visited pair. Used in {@link #remove()}.
+     */
+    private int lastIndex = 0;
+
+    /**
+     * Create the Iterator, make <code>index</code> point to the "first"
+     * index which is not empty. If such does not exist (eg. the map is
+     * empty) it would be zero.
+     */
+    public IndexIterator() {
+      for (baseHashIndex = 0; baseHashIndex < baseHash.length; ++baseHashIndex) {
+        index = baseHash[baseHashIndex];
+        if (index != 0) {
+          break;
+        }
+      }
+    }
+
+    @Override
+    public boolean hasNext() {
+      return (index != 0);
+    }
+
+    @Override
+    public int next() {
+      // Save the last index visited
+      lastIndex = index;
+
+      // next the index
+      index = next[index];
+
+      // if the next index points to the 'Ground' it means we're done with
+      // the current hash entry and we need to jump to the next one. This
+      // is done until all the hash entries had been visited.
+      while (index == 0 && ++baseHashIndex < baseHash.length) {
+        index = baseHash[baseHashIndex];
+      }
+
+      return lastIndex;
+    }
+
+    @Override
+    public void remove() {
+      FloatToObjectMap.this.remove(keys[lastIndex]);
+    }
+
+  }
+
+  /**
+   * Implements an IntIterator, used for iteration over the map's keys.
+   */
+  private final class KeyIterator implements FloatIterator {
+    private IntIterator iterator = new IndexIterator();
+
+    KeyIterator() { }
+
+    @Override
+    public boolean hasNext() {
+      return iterator.hasNext();
+    }
+
+    @Override
+    public float next() {
+      return keys[iterator.next()];
+    }
+
+    @Override
+    public void remove() {
+      iterator.remove();
+    }
+  }
+
+  /**
+   * Implements an Iterator of a generic type T used for iteration over the
+   * map's values.
+   */
+  private final class ValueIterator implements Iterator<T> {
+    private IntIterator iterator = new IndexIterator();
+
+    ValueIterator() { }
+
+    @Override
+    public boolean hasNext() {
+      return iterator.hasNext();
+    }
+
+    @Override
+    @SuppressWarnings("unchecked")
+    public T next() {
+      return (T) values[iterator.next()];
+    }
+
+    @Override
+    public void remove() {
+      iterator.remove();
+    }
+  }
+
+  /**
+   * Default capacity - in case no capacity was specified in the constructor
+   */
+  private static int defaultCapacity = 16;
+
+  /**
+   * Holds the base hash entries. if the capacity is 2^N, than the base hash
+   * holds 2^(N+1). It can hold
+   */
+  int[] baseHash;
+
+  /**
+   * The current capacity of the map. Always 2^N and never less than 16. We
+   * never use the zero index. It is needed to improve performance and is also
+   * used as "ground".
+   */
+  private int capacity;
+  /**
+   * All objects are being allocated at map creation. Those objects are "free"
+   * or empty. Whenever a new pair comes along, a pair is being "allocated" or
+   * taken from the free-linked list. as this is just a free list.
+   */
+  private int firstEmpty;
+
+  /**
+   * hashFactor is always (2^(N+1)) - 1. Used for faster hashing.
+   */
+  private int hashFactor;
+
+  /**
+   * This array holds the unique keys
+   */
+  float[] keys;
+
+  /**
+   * In case of collisions, we implement a double linked list of the colliding
+   * hash's with the following next[] and prev[]. Those are also used to store
+   * the "empty" list.
+   */
+  int[] next;
+
+  private int prev;
+
+  /**
+   * Number of currently objects in the map.
+   */
+  private int size;
+
+  /**
+   * This array holds the values
+   */
+  Object[] values;
+
+  /**
+   * Constructs a map with default capacity.
+   */
+  public FloatToObjectMap() {
+    this(defaultCapacity);
+  }
+
+  /**
+   * Constructs a map with given capacity. Capacity is adjusted to a native
+   * power of 2, with minimum of 16.
+   * 
+   * @param capacity
+   *            minimum capacity for the map.
+   */
+  public FloatToObjectMap(int capacity) {
+    this.capacity = 16;
+    // Minimum capacity is 16..
+    while (this.capacity < capacity) {
+      // Multiply by 2 as long as we're still under the requested capacity
+      this.capacity <<= 1;
+    }
+
+    // As mentioned, we use the first index (0) as 'Ground', so we need the
+    // length of the arrays to be one more than the capacity
+    int arrayLength = this.capacity + 1;
+
+    this.values = new Object[arrayLength];
+    this.keys = new float[arrayLength];
+    this.next = new int[arrayLength];
+
+    // Hash entries are twice as big as the capacity.
+    int baseHashSize = this.capacity << 1;
+
+    this.baseHash = new int[baseHashSize];
+
+    // The has factor is 2^M - 1 which is used as an "AND" hashing operator.
+    // {@link #calcBaseHash()}
+    this.hashFactor = baseHashSize - 1;
+
+    this.size = 0;
+
+    clear();
+  }
+
+  /**
+   * Adds a pair to the map. Takes the first empty position from the
+   * empty-linked-list's head - {@link #firstEmpty}.
+   * 
+   * New pairs are always inserted to baseHash, and are followed by the old
+   * colliding pair.
+   * 
+   * @param key
+   *            integer which maps the given Object
+   * @param e
+   *            element which is being mapped using the given key
+   */
+  private void prvt_put(float key, T e) {
+    // Hash entry to which the new pair would be inserted
+    int hashIndex = calcBaseHashIndex(key);
+
+    // 'Allocating' a pair from the "Empty" list.
+    int objectIndex = firstEmpty;
+
+    // Setting data
+    firstEmpty = next[firstEmpty];
+    values[objectIndex] = e;
+    keys[objectIndex] = key;
+
+    // Inserting the new pair as the first node in the specific hash entry
+    next[objectIndex] = baseHash[hashIndex];
+    baseHash[hashIndex] = objectIndex;
+
+    // Announcing a new pair was added!
+    ++size;
+  }
+
+  /**
+   * Calculating the baseHash index using the internal <code>hashFactor</code>.
+   */
+  protected int calcBaseHashIndex(float key) {
+    return Float.floatToIntBits(key) & hashFactor;
+  }
+
+  /**
+   * Empties the map. Generates the "Empty" space list for later allocation.
+   */
+  public void clear() {
+    // Clears the hash entries
+    Arrays.fill(this.baseHash, 0);
+
+    // Set size to zero
+    size = 0;
+
+    // Mark all array entries as empty. This is done with
+    // <code>firstEmpty</code> pointing to the first valid index (1 as 0 is
+    // used as 'Ground').
+    firstEmpty = 1;
+
+    // And setting all the <code>next[i]</code> to point at
+    // <code>i+1</code>.
+    for (int i = 1; i < this.capacity;) {
+      next[i] = ++i;
+    }
+
+    // Surly, the last one should point to the 'Ground'.
+    next[this.capacity] = 0;
+  }
+
+  /**
+   * Checks if a given key exists in the map.
+   * 
+   * @param key
+   *            that is checked against the map data.
+   * @return true if the key exists in the map. false otherwise.
+   */
+  public boolean containsKey(float key) {
+    return find(key) != 0;
+  }
+
+  /**
+   * Checks if the given object exists in the map.<br>
+   * This method iterates over the collection, trying to find an equal object.
+   * 
+   * @param o
+   *            object that is checked against the map data.
+   * @return true if the object exists in the map (in .equals() meaning).
+   *         false otherwise.
+   */
+  public boolean containsValue(Object o) {
+    for (Iterator<T> iterator = iterator(); iterator.hasNext();) {
+      T object = iterator.next();
+      if (object.equals(o)) {
+        return true;
+      }
+    }
+    return false;
+  }
+
+  /**
+   * Find the actual index of a given key.
+   * 
+   * @return index of the key. zero if the key wasn't found.
+   */
+  protected int find(float key) {
+    // Calculate the hash entry.
+    int baseHashIndex = calcBaseHashIndex(key);
+
+    // Start from the hash entry.
+    int localIndex = baseHash[baseHashIndex];
+
+    // while the index does not point to the 'Ground'
+    while (localIndex != 0) {
+      // returns the index found in case of of a matching key.
+      if (keys[localIndex] == key) {
+        return localIndex;
+      }
+
+      // next the local index
+      localIndex = next[localIndex];
+    }
+
+    // If we got this far, it could only mean we did not find the key we
+    // were asked for. return 'Ground' index.
+    return 0;
+  }
+
+  /**
+   * Find the actual index of a given key with it's baseHashIndex.<br>
+   * Some methods use the baseHashIndex. If those call {@link #find} there's
+   * no need to re-calculate that hash.
+   * 
+   * @return the index of the given key, or 0 as 'Ground' if the key wasn't
+   *         found.
+   */
+  private int findForRemove(float key, int baseHashIndex) {
+    // Start from the hash entry.
+    this.prev = 0;
+    int index = baseHash[baseHashIndex];
+
+    // while the index does not point to the 'Ground'
+    while (index != 0) {
+      // returns the index found in case of of a matching key.
+      if (keys[index] == key) {
+        return index;
+      }
+
+      // next the local index
+      prev = index;
+      index = next[index];
+    }
+
+    // If we got this far, it could only mean we did not find the key we
+    // were asked for. return 'Ground' index.
+    this.prev = 0;
+    return 0;
+  }
+
+  /**
+   * Returns the object mapped with the given key.
+   * 
+   * @param key
+   *            int who's mapped object we're interested in.
+   * @return an object mapped by the given key. null if the key wasn't found.
+   */
+  @SuppressWarnings("unchecked")
+  public T get(float key) {
+    return (T) values[find(key)];
+  }
+
+  /**
+   * Grows the map. Allocates a new map of double the capacity, and
+   * fast-insert the old key-value pairs.
+   */
+  @SuppressWarnings("unchecked")
+  protected void grow() {
+    FloatToObjectMap<T> that = new FloatToObjectMap<T>(
+        this.capacity * 2);
+
+    // Iterates fast over the collection. Any valid pair is put into the new
+    // map without checking for duplicates or if there's enough space for
+    // it.
+    for (IndexIterator iterator = new IndexIterator(); iterator.hasNext();) {
+      int index = iterator.next();
+      that.prvt_put(this.keys[index], (T) this.values[index]);
+    }
+
+    // Copy that's data into this.
+    this.capacity = that.capacity;
+    this.size = that.size;
+    this.firstEmpty = that.firstEmpty;
+    this.values = that.values;
+    this.keys = that.keys;
+    this.next = that.next;
+    this.baseHash = that.baseHash;
+    this.hashFactor = that.hashFactor;
+  }
+
+  /**
+   * 
+   * @return true if the map is empty. false otherwise.
+   */
+  public boolean isEmpty() {
+    return size == 0;
+  }
+
+  /**
+   * Returns a new iterator for the mapped objects.
+   */
+  @Override
+  public Iterator<T> iterator() {
+    return new ValueIterator();
+  }
+
+  /** Returns an iterator on the map keys. */
+  public FloatIterator keyIterator() {
+    return new KeyIterator();
+  }
+
+  /**
+   * Prints the baseHash array, used for DEBUG purposes.
+   */
+  @SuppressWarnings("unused")
+  private String getBaseHashAsString() {
+    return Arrays.toString(this.baseHash);
+  }
+
+  /**
+   * Inserts the &lt;key,value&gt; pair into the map. If the key already exists,
+   * this method updates the mapped value to the given one, returning the old
+   * mapped value.
+   * 
+   * @return the old mapped value, or null if the key didn't exist.
+   */
+  @SuppressWarnings("unchecked")
+  public T put(float key, T e) {
+    // Does key exists?
+    int index = find(key);
+
+    // Yes!
+    if (index != 0) {
+      // Set new data and exit.
+      T old = (T) values[index];
+      values[index] = e;
+      return old;
+    }
+
+    // Is there enough room for a new pair?
+    if (size == capacity) {
+      // No? Than grow up!
+      grow();
+    }
+
+    // Now that everything is set, the pair can be just put inside with no
+    // worries.
+    prvt_put(key, e);
+
+    return null;
+  }
+
+  /**
+   * Removes a &lt;key,value&gt; pair from the map and returns the mapped value,
+   * or null if the none existed.
+   * 
+   * @param key used to find the value to remove
+   * @return the removed value or null if none existed.
+   */
+  @SuppressWarnings("unchecked")
+  public T remove(float key) {
+    int baseHashIndex = calcBaseHashIndex(key);
+    int index = findForRemove(key, baseHashIndex);
+    if (index != 0) {
+      // If it is the first in the collision list, we should promote its
+      // next colliding element.
+      if (prev == 0) {
+        baseHash[baseHashIndex] = next[index];
+      }
+
+      next[prev] = next[index];
+      next[index] = firstEmpty;
+      firstEmpty = index;
+      --size;
+      return (T) values[index];
+    }
+
+    return null;
+  }
+
+  /**
+   * @return number of pairs currently in the map
+   */
+  public int size() {
+    return this.size;
+  }
+
+  /**
+   * Translates the mapped pairs' values into an array of Objects
+   * 
+   * @return an object array of all the values currently in the map.
+   */
+  public Object[] toArray() {
+    int j = -1;
+    Object[] array = new Object[size];
+
+    // Iterates over the values, adding them to the array.
+    for (Iterator<T> iterator = iterator(); iterator.hasNext();) {
+      array[++j] = iterator.next();
+    }
+    return array;
+  }
+
+  /**
+   * Translates the mapped pairs' values into an array of T
+   * 
+   * @param a
+   *            the array into which the elements of the list are to be
+   *            stored, if it is big enough; otherwise, use whatever space we
+   *            have, setting the one after the true data as null.
+   * 
+   * @return an array containing the elements of the list
+   * 
+   */
+  public T[] toArray(T[] a) {
+    int j = 0;
+    // Iterates over the values, adding them to the array.
+    for (Iterator<T> iterator = iterator(); j < a.length
+    && iterator.hasNext(); ++j) {
+      a[j] = iterator.next();
+    }
+
+    if (j < a.length) {
+      a[j] = null;
+    }
+
+    return a;
+  }
+
+  @Override
+  public String toString() {
+    StringBuffer sb = new StringBuffer();
+    sb.append('{');
+    FloatIterator keyIterator = keyIterator();
+    while (keyIterator.hasNext()) {
+      float key = keyIterator.next();
+      sb.append(key);
+      sb.append('=');
+      sb.append(get(key));
+      if (keyIterator.hasNext()) {
+        sb.append(',');
+        sb.append(' ');
+      }
+    }
+    sb.append('}');
+    return sb.toString();
+  }
+
+  @Override
+  public int hashCode() {
+    return getClass().hashCode() ^ size();
+  }
+
+  @SuppressWarnings("unchecked")
+  @Override
+  public boolean equals(Object o) {
+    FloatToObjectMap<T> that = (FloatToObjectMap<T>)o;
+    if (that.size() != this.size()) {
+      return false;
+    }
+
+    FloatIterator it = keyIterator();
+    while (it.hasNext()) {
+      float key = it.next();
+      if (!that.containsKey(key)) {
+        return false;
+      }
+
+      T v1 = this.get(key);
+      T v2 = that.get(key);
+      if ((v1 == null && v2 != null) ||
+          (v1 != null && v2 == null) ||
+          (!v1.equals(v2))) {
+        return false;
+      }
+    }
+    return true;
+  }
+}
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/collections/IntArray.java b/lucene/facet/src/java/org/apache/lucene/facet/collections/IntArray.java
new file mode 100644
index 0000000..9cb6e1c
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/collections/IntArray.java
@@ -0,0 +1,252 @@
+package org.apache.lucene.facet.collections;
+
+import java.util.Arrays;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * A Class wrapper for a grow-able int[] which can be sorted and intersect with
+ * other IntArrays.
+ * 
+ * @lucene.experimental
+ */
+public class IntArray {
+
+  /**
+   * The int[] which holds the data
+   */
+  private int[] data;
+
+  /**
+   * Holds the number of items in the array.
+   */
+  private int size;
+
+  /**
+   * A flag which indicates whether a sort should occur of the array is
+   * already sorted.
+   */
+  private boolean shouldSort;
+
+  /**
+   * Construct a default IntArray, size 0 and surly a sort should not occur.
+   */
+  public IntArray() {
+    init(true);
+  }
+
+  private void init(boolean realloc) {
+    size = 0;
+    if (realloc) {
+      data = new int[0];
+    }
+    shouldSort = false;
+  }
+
+  /**
+   * Intersects the data with a given {@link IntHashSet}.
+   * 
+   * @param set
+   *            A given ArrayHashSetInt which holds the data to be intersected
+   *            against
+   */
+  public void intersect(IntHashSet set) {
+    int newSize = 0;
+    for (int i = 0; i < size; ++i) {
+      if (set.contains(data[i])) {
+        data[newSize] = data[i];
+        ++newSize;
+      }
+    }
+    this.size = newSize;
+  }
+
+  /**
+   * Intersects the data with a given IntArray
+   * 
+   * @param other
+   *            A given IntArray which holds the data to be intersected agains
+   */
+  public void intersect(IntArray other) {
+    sort();
+    other.sort();
+
+    int myIndex = 0;
+    int otherIndex = 0;
+    int newSize = 0;
+    if (this.size > other.size) {
+      while (otherIndex < other.size && myIndex < size) {
+        while (otherIndex < other.size
+            && other.data[otherIndex] < data[myIndex]) {
+          ++otherIndex;
+        }
+        if (otherIndex == other.size) {
+          break;
+        }
+        while (myIndex < size && other.data[otherIndex] > data[myIndex]) {
+          ++myIndex;
+        }
+        if (other.data[otherIndex] == data[myIndex]) {
+          data[newSize++] = data[myIndex];
+          ++otherIndex;
+          ++myIndex;
+        }
+      }
+    } else {
+      while (otherIndex < other.size && myIndex < size) {
+        while (myIndex < size && other.data[otherIndex] > data[myIndex]) {
+          ++myIndex;
+        }
+        if (myIndex == size) {
+          break;
+        }
+        while (otherIndex < other.size
+            && other.data[otherIndex] < data[myIndex]) {
+          ++otherIndex;
+        }
+        if (other.data[otherIndex] == data[myIndex]) {
+          data[newSize++] = data[myIndex];
+          ++otherIndex;
+          ++myIndex;
+        }
+      }
+    }
+    this.size = newSize;
+  }
+
+  /**
+   * Return the size of the Array. Not the allocated size, but the number of
+   * values actually set.
+   * 
+   * @return the (filled) size of the array
+   */
+  public int size() {
+    return size;
+  }
+
+  /**
+   * Adds a value to the array.
+   * 
+   * @param value
+   *            value to be added
+   */
+  public void addToArray(int value) {
+    if (size == data.length) {
+      int[] newArray = new int[2 * size + 1];
+      System.arraycopy(data, 0, newArray, 0, size);
+      data = newArray;
+    }
+    data[size] = value;
+    ++size;
+    shouldSort = true;
+  }
+
+  /**
+   * Equals method. Checking the sizes, than the values from the last index to
+   * the first (Statistically for random should be the same but for our
+   * specific use would find differences faster).
+   */
+  @Override
+  public boolean equals(Object o) {
+    if (!(o instanceof IntArray)) {
+      return false;
+    }
+
+    IntArray array = (IntArray) o;
+    if (array.size != size) {
+      return false;
+    }
+
+    sort();
+    array.sort();
+
+    boolean equal = true;
+
+    for (int i = size; i > 0 && equal;) {
+      --i;
+      equal = (array.data[i] == this.data[i]);
+    }
+
+    return equal;
+  }
+
+  /**
+   * Sorts the data. If it is needed.
+   */
+  public void sort() {
+    if (shouldSort) {
+      shouldSort = false;
+      Arrays.sort(data, 0, size);
+    }
+  }
+
+  /**
+   * Calculates a hash-code for HashTables
+   */
+  @Override
+  public int hashCode() {
+    int hash = 0;
+    for (int i = 0; i < size; ++i) {
+      hash = data[i] ^ (hash * 31);
+    }
+    return hash;
+  }
+
+  /**
+   * Get an element from a specific index.
+   * 
+   * @param i
+   *            index of which element should be retrieved.
+   */
+  public int get(int i) {
+    if (i >= size) {
+      throw new ArrayIndexOutOfBoundsException(i);
+    }
+    return this.data[i];
+  }
+
+  public void set(int idx, int value) {
+    if (idx >= size) {
+      throw new ArrayIndexOutOfBoundsException(idx);
+    }
+    this.data[idx] = value;
+  }
+
+  /**
+   * toString or not toString. That is the question!
+   */
+  @Override
+  public String toString() {
+    String s = "(" + size + ") ";
+    for (int i = 0; i < size; ++i) {
+      s += "" + data[i] + ", ";
+    }
+    return s;
+  }
+
+  /**
+   * Clear the IntArray (set all elements to zero).
+   * @param resize - if resize is true, then clear actually allocates
+   * a new array of size 0, essentially 'clearing' the array and freeing
+   * memory.
+   */
+  public void clear(boolean resize) {
+    init(resize);
+  }
+
+}
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/collections/IntHashSet.java b/lucene/facet/src/java/org/apache/lucene/facet/collections/IntHashSet.java
new file mode 100644
index 0000000..3d5a817
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/collections/IntHashSet.java
@@ -0,0 +1,548 @@
+package org.apache.lucene.facet.collections;
+
+import java.util.Arrays;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * A Set or primitive int. Implemented as a HashMap of int->int. *
+ * 
+ * @lucene.experimental
+ */
+public class IntHashSet {
+  
+  // TODO (Facet): This is wasteful as the "values" are actually the "keys" and
+  // we could spare this amount of space (capacity * sizeof(int)). Perhaps even
+  // though it is not OOP, we should re-implement the hash for just that cause.
+
+  /**
+   * Implements an IntIterator which iterates over all the allocated indexes.
+   */
+  private final class IndexIterator implements IntIterator {
+    /**
+     * The last used baseHashIndex. Needed for "jumping" from one hash entry
+     * to another.
+     */
+    private int baseHashIndex = 0;
+
+    /**
+     * The next not-yet-visited index.
+     */
+    private int index = 0;
+
+    /**
+     * Index of the last visited pair. Used in {@link #remove()}.
+     */
+    private int lastIndex = 0;
+
+    /**
+     * Create the Iterator, make <code>index</code> point to the "first"
+     * index which is not empty. If such does not exist (eg. the map is
+     * empty) it would be zero.
+     */
+    public IndexIterator() {
+      for (baseHashIndex = 0; baseHashIndex < baseHash.length; ++baseHashIndex) {
+        index = baseHash[baseHashIndex];
+        if (index != 0) {
+          break;
+        }
+      }
+    }
+
+    @Override
+    public boolean hasNext() {
+      return (index != 0);
+    }
+
+    @Override
+    public int next() {
+      // Save the last index visited
+      lastIndex = index;
+
+      // next the index
+      index = next[index];
+
+      // if the next index points to the 'Ground' it means we're done with
+      // the current hash entry and we need to jump to the next one. This
+      // is done until all the hash entries had been visited.
+      while (index == 0 && ++baseHashIndex < baseHash.length) {
+        index = baseHash[baseHashIndex];
+      }
+
+      return lastIndex;
+    }
+
+    @Override
+    public void remove() {
+      IntHashSet.this.remove(keys[lastIndex]);
+    }
+
+  }
+
+  /**
+   * Implements an IntIterator, used for iteration over the map's keys.
+   */
+  private final class KeyIterator implements IntIterator {
+    private IntIterator iterator = new IndexIterator();
+
+    KeyIterator() { }
+    
+    @Override
+    public boolean hasNext() {
+      return iterator.hasNext();
+    }
+
+    @Override
+    public int next() {
+      return keys[iterator.next()];
+    }
+
+    @Override
+    public void remove() {
+      iterator.remove();
+    }
+  }
+
+  /**
+   * Default capacity - in case no capacity was specified in the constructor
+   */
+  private static int defaultCapacity = 16;
+
+  /**
+   * Holds the base hash entries. if the capacity is 2^N, than the base hash
+   * holds 2^(N+1). It can hold
+   */
+  int[] baseHash;
+
+  /**
+   * The current capacity of the map. Always 2^N and never less than 16. We
+   * never use the zero index. It is needed to improve performance and is also
+   * used as "ground".
+   */
+  private int capacity;
+  
+  /**
+   * All objects are being allocated at map creation. Those objects are "free"
+   * or empty. Whenever a new pair comes along, a pair is being "allocated" or
+   * taken from the free-linked list. as this is just a free list.
+   */
+  private int firstEmpty;
+
+  /**
+   * hashFactor is always (2^(N+1)) - 1. Used for faster hashing.
+   */
+  private int hashFactor;
+
+  /**
+   * This array holds the unique keys
+   */
+  int[] keys;
+
+  /**
+   * In case of collisions, we implement a double linked list of the colliding
+   * hash's with the following next[] and prev[]. Those are also used to store
+   * the "empty" list.
+   */
+  int[] next;
+
+  private int prev;
+
+  /**
+   * Number of currently objects in the map.
+   */
+  private int size;
+
+  /**
+   * Constructs a map with default capacity.
+   */
+  public IntHashSet() {
+    this(defaultCapacity);
+  }
+
+  /**
+   * Constructs a map with given capacity. Capacity is adjusted to a native
+   * power of 2, with minimum of 16.
+   * 
+   * @param capacity
+   *            minimum capacity for the map.
+   */
+  public IntHashSet(int capacity) {
+    this.capacity = 16;
+    // Minimum capacity is 16..
+    while (this.capacity < capacity) {
+      // Multiply by 2 as long as we're still under the requested capacity
+      this.capacity <<= 1;
+    }
+
+    // As mentioned, we use the first index (0) as 'Ground', so we need the
+    // length of the arrays to be one more than the capacity
+    int arrayLength = this.capacity + 1;
+
+    this.keys = new int[arrayLength];
+    this.next = new int[arrayLength];
+
+    // Hash entries are twice as big as the capacity.
+    int baseHashSize = this.capacity << 1;
+
+    this.baseHash = new int[baseHashSize];
+
+    // The has factor is 2^M - 1 which is used as an "AND" hashing operator.
+    // {@link #calcBaseHash()}
+    this.hashFactor = baseHashSize - 1;
+
+    this.size = 0;
+
+    clear();
+  }
+
+  /**
+   * Adds a pair to the map. Takes the first empty position from the
+   * empty-linked-list's head - {@link #firstEmpty}.
+   * 
+   * New pairs are always inserted to baseHash, and are followed by the old
+   * colliding pair.
+   * 
+   * @param key
+   *            integer which maps the given value
+   */
+  private void prvt_add(int key) {
+    // Hash entry to which the new pair would be inserted
+    int hashIndex = calcBaseHashIndex(key);
+
+    // 'Allocating' a pair from the "Empty" list.
+    int objectIndex = firstEmpty;
+
+    // Setting data
+    firstEmpty = next[firstEmpty];
+    keys[objectIndex] = key;
+
+    // Inserting the new pair as the first node in the specific hash entry
+    next[objectIndex] = baseHash[hashIndex];
+    baseHash[hashIndex] = objectIndex;
+
+    // Announcing a new pair was added!
+    ++size;
+  }
+
+  /**
+   * Calculating the baseHash index using the internal <code>hashFactor</code>
+   * .
+   */
+  protected int calcBaseHashIndex(int key) {
+    return key & hashFactor;
+  }
+
+  /**
+   * Empties the map. Generates the "Empty" space list for later allocation.
+   */
+  public void clear() {
+    // Clears the hash entries
+    Arrays.fill(this.baseHash, 0);
+
+    // Set size to zero
+    size = 0;
+
+    // Mark all array entries as empty. This is done with
+    // <code>firstEmpty</code> pointing to the first valid index (1 as 0 is
+    // used as 'Ground').
+    firstEmpty = 1;
+
+    // And setting all the <code>next[i]</code> to point at
+    // <code>i+1</code>.
+    for (int i = 1; i < this.capacity;) {
+      next[i] = ++i;
+    }
+
+    // Surly, the last one should point to the 'Ground'.
+    next[this.capacity] = 0;
+  }
+
+  /**
+   * Checks if a given key exists in the map.
+   * 
+   * @param value
+   *            that is checked against the map data.
+   * @return true if the key exists in the map. false otherwise.
+   */
+  public boolean contains(int value) {
+    return find(value) != 0;
+  }
+
+  /**
+   * Find the actual index of a given key.
+   * 
+   * @return index of the key. zero if the key wasn't found.
+   */
+  protected int find(int key) {
+    // Calculate the hash entry.
+    int baseHashIndex = calcBaseHashIndex(key);
+
+    // Start from the hash entry.
+    int localIndex = baseHash[baseHashIndex];
+
+    // while the index does not point to the 'Ground'
+    while (localIndex != 0) {
+      // returns the index found in case of of a matching key.
+      if (keys[localIndex] == key) {
+        return localIndex;
+      }
+
+      // next the local index
+      localIndex = next[localIndex];
+    }
+
+    // If we got this far, it could only mean we did not find the key we
+    // were asked for. return 'Ground' index.
+    return 0;
+  }
+
+  /**
+   * Find the actual index of a given key with it's baseHashIndex.<br>
+   * Some methods use the baseHashIndex. If those call {@link #find} there's
+   * no need to re-calculate that hash.
+   * 
+   * @return the index of the given key, or 0 as 'Ground' if the key wasn't
+   *         found.
+   */
+  private int findForRemove(int key, int baseHashIndex) {
+    // Start from the hash entry.
+    this.prev = 0;
+    int index = baseHash[baseHashIndex];
+
+    // while the index does not point to the 'Ground'
+    while (index != 0) {
+      // returns the index found in case of of a matching key.
+      if (keys[index] == key) {
+        return index;
+      }
+
+      // next the local index
+      prev = index;
+      index = next[index];
+    }
+
+    // If we got this far, it could only mean we did not find the key we
+    // were asked for. return 'Ground' index.
+    this.prev = 0;
+    return 0;
+  }
+
+  /**
+   * Grows the map. Allocates a new map of double the capacity, and
+   * fast-insert the old key-value pairs.
+   */
+  protected void grow() {
+    IntHashSet that = new IntHashSet(this.capacity * 2);
+
+    // Iterates fast over the collection. Any valid pair is put into the new
+    // map without checking for duplicates or if there's enough space for
+    // it.
+    for (IndexIterator iterator = new IndexIterator(); iterator.hasNext();) {
+      int index = iterator.next();
+      that.prvt_add(this.keys[index]);
+    }
+    // for (int i = capacity; i > 0; --i) {
+    //
+    // that._add(this.keys[i]);
+    //
+    // }
+
+    // Copy that's data into this.
+    this.capacity = that.capacity;
+    this.size = that.size;
+    this.firstEmpty = that.firstEmpty;
+    this.keys = that.keys;
+    this.next = that.next;
+    this.baseHash = that.baseHash;
+    this.hashFactor = that.hashFactor;
+  }
+
+  /**
+   * 
+   * @return true if the map is empty. false otherwise.
+   */
+  public boolean isEmpty() {
+    return size == 0;
+  }
+
+  /**
+   * Returns a new iterator for the mapped objects.
+   */
+  public IntIterator iterator() {
+    return new KeyIterator();
+  }
+
+  /**
+   * Prints the baseHash array, used for debug purposes.
+   */
+  @SuppressWarnings("unused")
+  private String getBaseHashAsString() {
+    return Arrays.toString(this.baseHash);
+  }
+
+  /**
+   * Add a mapping int key -> int value.
+   * <p>
+   * If the key was already inside just
+   * updating the value it refers to as the given object.
+   * <p>
+   * Otherwise if the map is full, first {@link #grow()} the map.
+   * 
+   * @param value
+   *            integer which maps the given value
+   * @return true always.
+   */
+  public boolean add(int value) {
+    // Does key exists?
+    int index = find(value);
+
+    // Yes!
+    if (index != 0) {
+      return true;
+    }
+
+    // Is there enough room for a new pair?
+    if (size == capacity) {
+      // No? Than grow up!
+      grow();
+    }
+
+    // Now that everything is set, the pair can be just put inside with no
+    // worries.
+    prvt_add(value);
+
+    return true;
+  }
+
+  /**
+   * Remove a pair from the map, specified by it's key.
+   * 
+   * @param value
+   *            specify the value to be removed
+   * 
+   * @return true if the map was changed (the key was found and removed).
+   *         false otherwise.
+   */
+  public boolean remove(int value) {
+    int baseHashIndex = calcBaseHashIndex(value);
+    int index = findForRemove(value, baseHashIndex);
+    if (index != 0) {
+      // If it is the first in the collision list, we should promote its
+      // next colliding element.
+      if (prev == 0) {
+        baseHash[baseHashIndex] = next[index];
+      }
+
+      next[prev] = next[index];
+      next[index] = firstEmpty;
+      firstEmpty = index;
+      --size;
+      return true;
+    }
+
+    return false;
+  }
+
+  /**
+   * @return number of pairs currently in the map
+   */
+  public int size() {
+    return this.size;
+  }
+
+  /**
+   * Translates the mapped pairs' values into an array of Objects
+   * 
+   * @return an object array of all the values currently in the map.
+   */
+  public int[] toArray() {
+    int j = -1;
+    int[] array = new int[size];
+
+    // Iterates over the values, adding them to the array.
+    for (IntIterator iterator = iterator(); iterator.hasNext();) {
+      array[++j] = iterator.next();
+    }
+    return array;
+  }
+
+  /**
+   * Translates the mapped pairs' values into an array of ints
+   * 
+   * @param a
+   *            the array into which the elements of the map are to be stored,
+   *            if it is big enough; otherwise, a new array of the same
+   *            runtime type is allocated for this purpose.
+   * 
+   * @return an array containing the values stored in the map
+   * 
+   */
+  public int[] toArray(int[] a) {
+    int j = 0;
+    if (a.length < size) {
+      a = new int[size];
+    }
+    // Iterates over the values, adding them to the array.
+    for (IntIterator iterator = iterator(); j < a.length
+        && iterator.hasNext(); ++j) {
+      a[j] = iterator.next();
+    }
+    return a;
+  }
+
+  /**
+   * I have no idea why would anyone call it - but for debug purposes.<br>
+   * Prints the entire map, including the index, key, object, next and prev.
+   */
+  @Override
+  public String toString() {
+    StringBuffer sb = new StringBuffer();
+    sb.append('{');
+    IntIterator iterator = iterator();
+    while (iterator.hasNext()) {
+      sb.append(iterator.next());
+      if (iterator.hasNext()) {
+        sb.append(',');
+        sb.append(' ');
+      }
+    }
+    sb.append('}');
+    return sb.toString();
+  }
+
+  public String toHashString() {
+    String string = "\n";
+    StringBuffer sb = new StringBuffer();
+
+    for (int i = 0; i < this.baseHash.length; i++) {
+      StringBuffer sb2 = new StringBuffer();
+      boolean shouldAppend = false;
+      sb2.append(i + ".\t");
+      for (int index = baseHash[i]; index != 0; index = next[index]) {
+        sb2.append(" -> " + keys[index] + "@" + index);
+        shouldAppend = true;
+      }
+      if (shouldAppend) {
+        sb.append(sb2);
+        sb.append(string);
+      }
+    }
+
+    return sb.toString();
+  }
+}
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/collections/IntIterator.java b/lucene/facet/src/java/org/apache/lucene/facet/collections/IntIterator.java
new file mode 100644
index 0000000..a69b2cb
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/collections/IntIterator.java
@@ -0,0 +1,31 @@
+package org.apache.lucene.facet.collections;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Iterator interface for primitive int iteration. *
+ * 
+ * @lucene.experimental
+ */
+public interface IntIterator {
+
+  boolean hasNext();
+  int next();
+  void remove();
+
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/collections/IntToDoubleMap.java b/lucene/facet/src/java/org/apache/lucene/facet/collections/IntToDoubleMap.java
new file mode 100644
index 0000000..b787ab6
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/collections/IntToDoubleMap.java
@@ -0,0 +1,631 @@
+package org.apache.lucene.facet.collections;
+
+import java.util.Arrays;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * An Array-based hashtable which maps primitive int to a primitive double.<br>
+ * The hashtable is constracted with a given capacity, or 16 as a default. In
+ * case there's not enough room for new pairs, the hashtable grows. <br>
+ * Capacity is adjusted to a power of 2, and there are 2 * capacity entries for
+ * the hash.
+ * 
+ * The pre allocated arrays (for keys, values) are at length of capacity + 1,
+ * when index 0 is used as 'Ground' or 'NULL'.<br>
+ * 
+ * The arrays are allocated ahead of hash operations, and form an 'empty space'
+ * list, to which the key,value pair is allocated.
+ * 
+ * @lucene.experimental
+ */
+public class IntToDoubleMap {
+
+  public static final double GROUND = Double.NaN;
+
+  /**
+   * Implements an IntIterator which iterates over all the allocated indexes.
+   */
+  private final class IndexIterator implements IntIterator {
+    /**
+     * The last used baseHashIndex. Needed for "jumping" from one hash entry
+     * to another.
+     */
+    private int baseHashIndex = 0;
+
+    /**
+     * The next not-yet-visited index.
+     */
+    private int index = 0;
+
+    /**
+     * Index of the last visited pair. Used in {@link #remove()}.
+     */
+    private int lastIndex = 0;
+
+    /**
+     * Create the Iterator, make <code>index</code> point to the "first"
+     * index which is not empty. If such does not exist (eg. the map is
+     * empty) it would be zero.
+     */
+    public IndexIterator() {
+      for (baseHashIndex = 0; baseHashIndex < baseHash.length; ++baseHashIndex) {
+        index = baseHash[baseHashIndex];
+        if (index != 0) {
+          break;
+        }
+      }
+    }
+
+    @Override
+    public boolean hasNext() {
+      return (index != 0);
+    }
+
+    @Override
+    public int next() {
+      // Save the last index visited
+      lastIndex = index;
+
+      // next the index
+      index = next[index];
+
+      // if the next index points to the 'Ground' it means we're done with
+      // the current hash entry and we need to jump to the next one. This
+      // is done until all the hash entries had been visited.
+      while (index == 0 && ++baseHashIndex < baseHash.length) {
+        index = baseHash[baseHashIndex];
+      }
+
+      return lastIndex;
+    }
+
+    @Override
+    public void remove() {
+      IntToDoubleMap.this.remove(keys[lastIndex]);
+    }
+
+  }
+
+  /**
+   * Implements an IntIterator, used for iteration over the map's keys.
+   */
+  private final class KeyIterator implements IntIterator {
+    private IntIterator iterator = new IndexIterator();
+
+    KeyIterator() { }
+    
+    @Override
+    public boolean hasNext() {
+      return iterator.hasNext();
+    }
+
+    @Override
+    public int next() {
+      return keys[iterator.next()];
+    }
+
+    @Override
+    public void remove() {
+      iterator.remove();
+    }
+  }
+
+  /**
+   * Implements an Iterator of a generic type T used for iteration over the
+   * map's values.
+   */
+  private final class ValueIterator implements DoubleIterator {
+    private IntIterator iterator = new IndexIterator();
+
+    ValueIterator() { }
+    
+    @Override
+    public boolean hasNext() {
+      return iterator.hasNext();
+    }
+
+    @Override
+    public double next() {
+      return values[iterator.next()];
+    }
+
+    @Override
+    public void remove() {
+      iterator.remove();
+    }
+  }
+
+  /**
+   * Default capacity - in case no capacity was specified in the constructor
+   */
+  private static int defaultCapacity = 16;
+
+  /**
+   * Holds the base hash entries. if the capacity is 2^N, than the base hash
+   * holds 2^(N+1). It can hold
+   */
+  int[] baseHash;
+
+  /**
+   * The current capacity of the map. Always 2^N and never less than 16. We
+   * never use the zero index. It is needed to improve performance and is also
+   * used as "ground".
+   */
+  private int capacity;
+  /**
+   * All objects are being allocated at map creation. Those objects are "free"
+   * or empty. Whenever a new pair comes along, a pair is being "allocated" or
+   * taken from the free-linked list. as this is just a free list.
+   */
+  private int firstEmpty;
+
+  /**
+   * hashFactor is always (2^(N+1)) - 1. Used for faster hashing.
+   */
+  private int hashFactor;
+
+  /**
+   * This array holds the unique keys
+   */
+  int[] keys;
+
+  /**
+   * In case of collisions, we implement a double linked list of the colliding
+   * hash's with the following next[] and prev[]. Those are also used to store
+   * the "empty" list.
+   */
+  int[] next;
+
+  private int prev;
+
+  /**
+   * Number of currently objects in the map.
+   */
+  private int size;
+
+  /**
+   * This array holds the values
+   */
+  double[] values;
+
+  /**
+   * Constructs a map with default capacity.
+   */
+  public IntToDoubleMap() {
+    this(defaultCapacity);
+  }
+
+  /**
+   * Constructs a map with given capacity. Capacity is adjusted to a native
+   * power of 2, with minimum of 16.
+   * 
+   * @param capacity
+   *            minimum capacity for the map.
+   */
+  public IntToDoubleMap(int capacity) {
+    this.capacity = 16;
+    // Minimum capacity is 16..
+    while (this.capacity < capacity) {
+      // Multiply by 2 as long as we're still under the requested capacity
+      this.capacity <<= 1;
+    }
+
+    // As mentioned, we use the first index (0) as 'Ground', so we need the
+    // length of the arrays to be one more than the capacity
+    int arrayLength = this.capacity + 1;
+
+    this.values = new double[arrayLength];
+    this.keys = new int[arrayLength];
+    this.next = new int[arrayLength];
+
+    // Hash entries are twice as big as the capacity.
+    int baseHashSize = this.capacity << 1;
+
+    this.baseHash = new int[baseHashSize];
+
+    this.values[0] = GROUND;
+
+    // The has factor is 2^M - 1 which is used as an "AND" hashing operator.
+    // {@link #calcBaseHash()}
+    this.hashFactor = baseHashSize - 1;
+
+    this.size = 0;
+
+    clear();
+  }
+
+  /**
+   * Adds a pair to the map. Takes the first empty position from the
+   * empty-linked-list's head - {@link #firstEmpty}.
+   * 
+   * New pairs are always inserted to baseHash, and are followed by the old
+   * colliding pair.
+   * 
+   * @param key
+   *            integer which maps the given Object
+   * @param v
+   *            double value which is being mapped using the given key
+   */
+  private void prvt_put(int key, double v) {
+    // Hash entry to which the new pair would be inserted
+    int hashIndex = calcBaseHashIndex(key);
+
+    // 'Allocating' a pair from the "Empty" list.
+    int objectIndex = firstEmpty;
+
+    // Setting data
+    firstEmpty = next[firstEmpty];
+    values[objectIndex] = v;
+    keys[objectIndex] = key;
+
+    // Inserting the new pair as the first node in the specific hash entry
+    next[objectIndex] = baseHash[hashIndex];
+    baseHash[hashIndex] = objectIndex;
+
+    // Announcing a new pair was added!
+    ++size;
+  }
+
+  /**
+   * Calculating the baseHash index using the internal <code>hashFactor</code>
+   * .
+   */
+  protected int calcBaseHashIndex(int key) {
+    return key & hashFactor;
+  }
+
+  /**
+   * Empties the map. Generates the "Empty" space list for later allocation.
+   */
+  public void clear() {
+    // Clears the hash entries
+    Arrays.fill(this.baseHash, 0);
+
+    // Set size to zero
+    size = 0;
+
+    // Mark all array entries as empty. This is done with
+    // <code>firstEmpty</code> pointing to the first valid index (1 as 0 is
+    // used as 'Ground').
+    firstEmpty = 1;
+
+    // And setting all the <code>next[i]</code> to point at
+    // <code>i+1</code>.
+    for (int i = 1; i < this.capacity;) {
+      next[i] = ++i;
+    }
+
+    // Surly, the last one should point to the 'Ground'.
+    next[this.capacity] = 0;
+  }
+
+  /**
+   * Checks if a given key exists in the map.
+   * 
+   * @param key
+   *            that is checked against the map data.
+   * @return true if the key exists in the map. false otherwise.
+   */
+  public boolean containsKey(int key) {
+    return find(key) != 0;
+  }
+
+  /**
+   * Checks if the given value exists in the map.<br>
+   * This method iterates over the collection, trying to find an equal object.
+   * 
+   * @param value
+   *            double value that is checked against the map data.
+   * @return true if the value exists in the map, false otherwise.
+   */
+  public boolean containsValue(double value) {
+    for (DoubleIterator iterator = iterator(); iterator.hasNext();) {
+      double d = iterator.next();
+      if (d == value) {
+        return true;
+      }
+    }
+    return false;
+  }
+
+  /**
+   * Find the actual index of a given key.
+   * 
+   * @return index of the key. zero if the key wasn't found.
+   */
+  protected int find(int key) {
+    // Calculate the hash entry.
+    int baseHashIndex = calcBaseHashIndex(key);
+
+    // Start from the hash entry.
+    int localIndex = baseHash[baseHashIndex];
+
+    // while the index does not point to the 'Ground'
+    while (localIndex != 0) {
+      // returns the index found in case of of a matching key.
+      if (keys[localIndex] == key) {
+        return localIndex;
+      }
+        
+      // next the local index
+      localIndex = next[localIndex];
+    }
+
+    // If we got this far, it could only mean we did not find the key we
+    // were asked for. return 'Ground' index.
+    return 0;
+  }
+
+  /**
+   * Find the actual index of a given key with it's baseHashIndex.<br>
+   * Some methods use the baseHashIndex. If those call {@link #find} there's
+   * no need to re-calculate that hash.
+   * 
+   * @return the index of the given key, or 0 as 'Ground' if the key wasn't
+   *         found.
+   */
+  private int findForRemove(int key, int baseHashIndex) {
+    // Start from the hash entry.
+    this.prev = 0;
+    int index = baseHash[baseHashIndex];
+
+    // while the index does not point to the 'Ground'
+    while (index != 0) {
+      // returns the index found in case of of a matching key.
+      if (keys[index] == key) {
+        return index;
+      }
+
+      // next the local index
+      prev = index;
+      index = next[index];
+    }
+
+    // If we got this far, it could only mean we did not find the key we
+    // were asked for. return 'Ground' index.
+    this.prev = 0;
+    return 0;
+  }
+
+  /**
+   * Returns the value mapped with the given key.
+   * 
+   * @param key
+   *            int who's mapped object we're interested in.
+   * @return a double value mapped by the given key. Double.NaN if the key wasn't found.
+   */
+  public double get(int key) {
+    return values[find(key)];
+  }
+
+  /**
+   * Grows the map. Allocates a new map of double the capacity, and
+   * fast-insert the old key-value pairs.
+   */
+  protected void grow() {
+    IntToDoubleMap that = new IntToDoubleMap(
+        this.capacity * 2);
+
+    // Iterates fast over the collection. Any valid pair is put into the new
+    // map without checking for duplicates or if there's enough space for
+    // it.
+    for (IndexIterator iterator = new IndexIterator(); iterator.hasNext();) {
+      int index = iterator.next();
+      that.prvt_put(this.keys[index], this.values[index]);
+    }
+
+    // Copy that's data into this.
+    this.capacity = that.capacity;
+    this.size = that.size;
+    this.firstEmpty = that.firstEmpty;
+    this.values = that.values;
+    this.keys = that.keys;
+    this.next = that.next;
+    this.baseHash = that.baseHash;
+    this.hashFactor = that.hashFactor;
+  }
+
+  /**
+   * 
+   * @return true if the map is empty. false otherwise.
+   */
+  public boolean isEmpty() {
+    return size == 0;
+  }
+
+  /**
+   * Returns a new iterator for the mapped double values.
+   */
+  public DoubleIterator iterator() {
+    return new ValueIterator();
+  }
+
+  /** Returns an iterator on the map keys. */
+  public IntIterator keyIterator() {
+    return new KeyIterator();
+  }
+
+  /**
+   * Prints the baseHash array, used for debug purposes.
+   */
+  @SuppressWarnings("unused")
+  private String getBaseHashAsString() {
+    return Arrays.toString(this.baseHash);
+  }
+
+  /**
+   * Inserts the &lt;key,value&gt; pair into the map. If the key already exists,
+   * this method updates the mapped value to the given one, returning the old
+   * mapped value.
+   * 
+   * @return the old mapped value, or {@link Double#NaN} if the key didn't exist.
+   */
+  public double put(int key, double v) {
+    // Does key exists?
+    int index = find(key);
+
+    // Yes!
+    if (index != 0) {
+      // Set new data and exit.
+      double old = values[index];
+      values[index] = v;
+      return old;
+    }
+
+    // Is there enough room for a new pair?
+    if (size == capacity) {
+      // No? Than grow up!
+      grow();
+    }
+
+    // Now that everything is set, the pair can be just put inside with no
+    // worries.
+    prvt_put(key, v);
+
+    return Double.NaN;
+  }
+
+  /**
+   * Removes a &lt;key,value&gt; pair from the map and returns the mapped value,
+   * or {@link Double#NaN} if the none existed.
+   * 
+   * @param key used to find the value to remove
+   * @return the removed value or {@link Double#NaN} if none existed.
+   */
+  public double remove(int key) {
+    int baseHashIndex = calcBaseHashIndex(key);
+    int index = findForRemove(key, baseHashIndex);
+    if (index != 0) {
+      // If it is the first in the collision list, we should promote its
+      // next colliding element.
+      if (prev == 0) {
+        baseHash[baseHashIndex] = next[index];
+      }
+
+      next[prev] = next[index];
+      next[index] = firstEmpty;
+      firstEmpty = index;
+      --size;
+      return values[index];
+    }
+
+    return Double.NaN;
+  }
+
+  /**
+   * @return number of pairs currently in the map
+   */
+  public int size() {
+    return this.size;
+  }
+
+  /**
+   * Translates the mapped pairs' values into an array of Objects
+   * 
+   * @return a double array of all the values currently in the map.
+   */
+  public double[] toArray() {
+    int j = -1;
+    double[] array = new double[size];
+
+    // Iterates over the values, adding them to the array.
+    for (DoubleIterator iterator = iterator(); iterator.hasNext();) {
+      array[++j] = iterator.next();
+    }
+    return array;
+  }
+
+  /**
+   * Translates the mapped pairs' values into an array of T
+   * 
+   * @param a
+   *            the array into which the elements of the list are to be
+   *            stored. If it is big enough use whatever space we need,
+   *            setting the one after the true data as {@link Double#NaN}.
+   * 
+   * @return an array containing the elements of the list, using the given
+   *         parameter if big enough, otherwise allocate an appropriate array
+   *         and return it.
+   * 
+   */
+  public double[] toArray(double[] a) {
+    int j = 0;
+    if (a.length < this.size()) {
+      a = new double[this.size()];
+    }
+
+    // Iterates over the values, adding them to the array.
+    for (DoubleIterator iterator = iterator(); iterator.hasNext(); ++j) {
+      a[j] = iterator.next();
+    }
+
+    if (j < a.length) {
+      a[j] = Double.NaN;
+    }
+
+    return a;
+  }
+
+  @Override
+  public String toString() {
+    StringBuffer sb = new StringBuffer();
+    sb.append('{');
+    IntIterator keyIterator = keyIterator();
+    while (keyIterator.hasNext()) {
+      int key = keyIterator.next();
+      sb.append(key);
+      sb.append('=');
+      sb.append(get(key));
+      if (keyIterator.hasNext()) {
+        sb.append(',');
+        sb.append(' ');
+      }
+    }
+    sb.append('}');
+    return sb.toString();
+  }
+  
+  @Override
+  public int hashCode() {
+    return getClass().hashCode() ^ size();
+  }
+  
+  @Override
+  public boolean equals(Object o) {
+    IntToDoubleMap that = (IntToDoubleMap)o;
+    if (that.size() != this.size()) {
+      return false;
+    }
+    
+    IntIterator it = keyIterator();
+    while (it.hasNext()) {
+      int key = it.next();
+      if (!that.containsKey(key)) {
+        return false;
+      }
+
+      double v1 = this.get(key);
+      double v2 = that.get(key);
+      if (Double.compare(v1, v2) != 0) {
+        return false;
+      }
+    }
+    return true;
+  }
+}
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/collections/IntToFloatMap.java b/lucene/facet/src/java/org/apache/lucene/facet/collections/IntToFloatMap.java
new file mode 100644
index 0000000..3e9d7ae
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/collections/IntToFloatMap.java
@@ -0,0 +1,631 @@
+package org.apache.lucene.facet.collections;
+
+import java.util.Arrays;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * An Array-based hashtable which maps primitive int to a primitive float.<br>
+ * The hashtable is constracted with a given capacity, or 16 as a default. In
+ * case there's not enough room for new pairs, the hashtable grows. <br>
+ * Capacity is adjusted to a power of 2, and there are 2 * capacity entries for
+ * the hash.
+ * 
+ * The pre allocated arrays (for keys, values) are at length of capacity + 1,
+ * when index 0 is used as 'Ground' or 'NULL'.<br>
+ * 
+ * The arrays are allocated ahead of hash operations, and form an 'empty space'
+ * list, to which the key,value pair is allocated.
+ * 
+ * @lucene.experimental
+ */
+public class IntToFloatMap {
+
+  public static final float GROUND = Float.NaN;
+
+  /**
+   * Implements an IntIterator which iterates over all the allocated indexes.
+   */
+  private final class IndexIterator implements IntIterator {
+    /**
+     * The last used baseHashIndex. Needed for "jumping" from one hash entry
+     * to another.
+     */
+    private int baseHashIndex = 0;
+
+    /**
+     * The next not-yet-visited index.
+     */
+    private int index = 0;
+
+    /**
+     * Index of the last visited pair. Used in {@link #remove()}.
+     */
+    private int lastIndex = 0;
+
+    /**
+     * Create the Iterator, make <code>index</code> point to the "first"
+     * index which is not empty. If such does not exist (eg. the map is
+     * empty) it would be zero.
+     */
+    public IndexIterator() {
+      for (baseHashIndex = 0; baseHashIndex < baseHash.length; ++baseHashIndex) {
+        index = baseHash[baseHashIndex];
+        if (index != 0) {
+          break;
+        }
+      }
+    }
+
+    @Override
+    public boolean hasNext() {
+      return (index != 0);
+    }
+
+    @Override
+    public int next() {
+      // Save the last index visited
+      lastIndex = index;
+
+      // next the index
+      index = next[index];
+
+      // if the next index points to the 'Ground' it means we're done with
+      // the current hash entry and we need to jump to the next one. This
+      // is done until all the hash entries had been visited.
+      while (index == 0 && ++baseHashIndex < baseHash.length) {
+        index = baseHash[baseHashIndex];
+      }
+
+      return lastIndex;
+    }
+
+    @Override
+    public void remove() {
+      IntToFloatMap.this.remove(keys[lastIndex]);
+    }
+
+  }
+
+  /**
+   * Implements an IntIterator, used for iteration over the map's keys.
+   */
+  private final class KeyIterator implements IntIterator {
+    private IntIterator iterator = new IndexIterator();
+
+    KeyIterator() { }
+    
+    @Override
+    public boolean hasNext() {
+      return iterator.hasNext();
+    }
+
+    @Override
+    public int next() {
+      return keys[iterator.next()];
+    }
+
+    @Override
+    public void remove() {
+      iterator.remove();
+    }
+  }
+
+  /**
+   * Implements an Iterator of a generic type T used for iteration over the
+   * map's values.
+   */
+  private final class ValueIterator implements FloatIterator {
+    private IntIterator iterator = new IndexIterator();
+
+    ValueIterator() { }
+    
+    @Override
+    public boolean hasNext() {
+      return iterator.hasNext();
+    }
+
+    @Override
+    public float next() {
+      return values[iterator.next()];
+    }
+
+    @Override
+    public void remove() {
+      iterator.remove();
+    }
+  }
+
+  /**
+   * Default capacity - in case no capacity was specified in the constructor
+   */
+  private static int defaultCapacity = 16;
+
+  /**
+   * Holds the base hash entries. if the capacity is 2^N, than the base hash
+   * holds 2^(N+1). It can hold
+   */
+  int[] baseHash;
+
+  /**
+   * The current capacity of the map. Always 2^N and never less than 16. We
+   * never use the zero index. It is needed to improve performance and is also
+   * used as "ground".
+   */
+  private int capacity;
+  /**
+   * All objects are being allocated at map creation. Those objects are "free"
+   * or empty. Whenever a new pair comes along, a pair is being "allocated" or
+   * taken from the free-linked list. as this is just a free list.
+   */
+  private int firstEmpty;
+
+  /**
+   * hashFactor is always (2^(N+1)) - 1. Used for faster hashing.
+   */
+  private int hashFactor;
+
+  /**
+   * This array holds the unique keys
+   */
+  int[] keys;
+
+  /**
+   * In case of collisions, we implement a float linked list of the colliding
+   * hash's with the following next[] and prev[]. Those are also used to store
+   * the "empty" list.
+   */
+  int[] next;
+
+  private int prev;
+
+  /**
+   * Number of currently objects in the map.
+   */
+  private int size;
+
+  /**
+   * This array holds the values
+   */
+  float[] values;
+
+  /**
+   * Constructs a map with default capacity.
+   */
+  public IntToFloatMap() {
+    this(defaultCapacity);
+  }
+
+  /**
+   * Constructs a map with given capacity. Capacity is adjusted to a native
+   * power of 2, with minimum of 16.
+   * 
+   * @param capacity
+   *            minimum capacity for the map.
+   */
+  public IntToFloatMap(int capacity) {
+    this.capacity = 16;
+    // Minimum capacity is 16..
+    while (this.capacity < capacity) {
+      // Multiply by 2 as long as we're still under the requested capacity
+      this.capacity <<= 1;
+    }
+
+    // As mentioned, we use the first index (0) as 'Ground', so we need the
+    // length of the arrays to be one more than the capacity
+    int arrayLength = this.capacity + 1;
+
+    this.values = new float[arrayLength];
+    this.keys = new int[arrayLength];
+    this.next = new int[arrayLength];
+
+    // Hash entries are twice as big as the capacity.
+    int baseHashSize = this.capacity << 1;
+
+    this.baseHash = new int[baseHashSize];
+
+    this.values[0] = GROUND;
+
+    // The has factor is 2^M - 1 which is used as an "AND" hashing operator.
+    // {@link #calcBaseHash()}
+    this.hashFactor = baseHashSize - 1;
+
+    this.size = 0;
+
+    clear();
+  }
+
+  /**
+   * Adds a pair to the map. Takes the first empty position from the
+   * empty-linked-list's head - {@link #firstEmpty}.
+   * 
+   * New pairs are always inserted to baseHash, and are followed by the old
+   * colliding pair.
+   * 
+   * @param key
+   *            integer which maps the given Object
+   * @param v
+   *            float value which is being mapped using the given key
+   */
+  private void prvt_put(int key, float v) {
+    // Hash entry to which the new pair would be inserted
+    int hashIndex = calcBaseHashIndex(key);
+
+    // 'Allocating' a pair from the "Empty" list.
+    int objectIndex = firstEmpty;
+
+    // Setting data
+    firstEmpty = next[firstEmpty];
+    values[objectIndex] = v;
+    keys[objectIndex] = key;
+
+    // Inserting the new pair as the first node in the specific hash entry
+    next[objectIndex] = baseHash[hashIndex];
+    baseHash[hashIndex] = objectIndex;
+
+    // Announcing a new pair was added!
+    ++size;
+  }
+
+  /**
+   * Calculating the baseHash index using the internal <code>hashFactor</code>
+   * .
+   */
+  protected int calcBaseHashIndex(int key) {
+    return key & hashFactor;
+  }
+
+  /**
+   * Empties the map. Generates the "Empty" space list for later allocation.
+   */
+  public void clear() {
+    // Clears the hash entries
+    Arrays.fill(this.baseHash, 0);
+
+    // Set size to zero
+    size = 0;
+
+    // Mark all array entries as empty. This is done with
+    // <code>firstEmpty</code> pointing to the first valid index (1 as 0 is
+    // used as 'Ground').
+    firstEmpty = 1;
+
+    // And setting all the <code>next[i]</code> to point at
+    // <code>i+1</code>.
+    for (int i = 1; i < this.capacity;) {
+      next[i] = ++i;
+    }
+
+    // Surly, the last one should point to the 'Ground'.
+    next[this.capacity] = 0;
+  }
+
+  /**
+   * Checks if a given key exists in the map.
+   * 
+   * @param key
+   *            that is checked against the map data.
+   * @return true if the key exists in the map. false otherwise.
+   */
+  public boolean containsKey(int key) {
+    return find(key) != 0;
+  }
+
+  /**
+   * Checks if the given value exists in the map.<br>
+   * This method iterates over the collection, trying to find an equal object.
+   * 
+   * @param value
+   *            float value that is checked against the map data.
+   * @return true if the value exists in the map, false otherwise.
+   */
+  public boolean containsValue(float value) {
+    for (FloatIterator iterator = iterator(); iterator.hasNext();) {
+      float d = iterator.next();
+      if (d == value) {
+        return true;
+      }
+    }
+    return false;
+  }
+
+  /**
+   * Find the actual index of a given key.
+   * 
+   * @return index of the key. zero if the key wasn't found.
+   */
+  protected int find(int key) {
+    // Calculate the hash entry.
+    int baseHashIndex = calcBaseHashIndex(key);
+
+    // Start from the hash entry.
+    int localIndex = baseHash[baseHashIndex];
+
+    // while the index does not point to the 'Ground'
+    while (localIndex != 0) {
+      // returns the index found in case of of a matching key.
+      if (keys[localIndex] == key) {
+        return localIndex;
+      }
+        
+      // next the local index
+      localIndex = next[localIndex];
+    }
+
+    // If we got this far, it could only mean we did not find the key we
+    // were asked for. return 'Ground' index.
+    return 0;
+  }
+
+  /**
+   * Find the actual index of a given key with it's baseHashIndex.<br>
+   * Some methods use the baseHashIndex. If those call {@link #find} there's
+   * no need to re-calculate that hash.
+   * 
+   * @return the index of the given key, or 0 as 'Ground' if the key wasn't
+   *         found.
+   */
+  private int findForRemove(int key, int baseHashIndex) {
+    // Start from the hash entry.
+    this.prev = 0;
+    int index = baseHash[baseHashIndex];
+
+    // while the index does not point to the 'Ground'
+    while (index != 0) {
+      // returns the index found in case of of a matching key.
+      if (keys[index] == key) {
+        return index;
+      }
+
+      // next the local index
+      prev = index;
+      index = next[index];
+    }
+
+    // If we got this far, it could only mean we did not find the key we
+    // were asked for. return 'Ground' index.
+    this.prev = 0;
+    return 0;
+  }
+
+  /**
+   * Returns the value mapped with the given key.
+   * 
+   * @param key
+   *            int who's mapped object we're interested in.
+   * @return a float value mapped by the given key. float.NaN if the key wasn't found.
+   */
+  public float get(int key) {
+    return values[find(key)];
+  }
+
+  /**
+   * Grows the map. Allocates a new map of float the capacity, and
+   * fast-insert the old key-value pairs.
+   */
+  protected void grow() {
+    IntToFloatMap that = new IntToFloatMap(
+        this.capacity * 2);
+
+    // Iterates fast over the collection. Any valid pair is put into the new
+    // map without checking for duplicates or if there's enough space for
+    // it.
+    for (IndexIterator iterator = new IndexIterator(); iterator.hasNext();) {
+      int index = iterator.next();
+      that.prvt_put(this.keys[index], this.values[index]);
+    }
+
+    // Copy that's data into this.
+    this.capacity = that.capacity;
+    this.size = that.size;
+    this.firstEmpty = that.firstEmpty;
+    this.values = that.values;
+    this.keys = that.keys;
+    this.next = that.next;
+    this.baseHash = that.baseHash;
+    this.hashFactor = that.hashFactor;
+  }
+
+  /**
+   * 
+   * @return true if the map is empty. false otherwise.
+   */
+  public boolean isEmpty() {
+    return size == 0;
+  }
+
+  /**
+   * Returns a new iterator for the mapped float values.
+   */
+  public FloatIterator iterator() {
+    return new ValueIterator();
+  }
+
+  /** Returns an iterator on the map keys. */
+  public IntIterator keyIterator() {
+    return new KeyIterator();
+  }
+
+  /**
+   * Prints the baseHash array, used for debug purposes.
+   */
+  @SuppressWarnings("unused")
+  private String getBaseHashAsString() {
+    return Arrays.toString(this.baseHash);
+  }
+
+  /**
+   * Inserts the &lt;key,value&gt; pair into the map. If the key already exists,
+   * this method updates the mapped value to the given one, returning the old
+   * mapped value.
+   * 
+   * @return the old mapped value, or {@link Float#NaN} if the key didn't exist.
+   */
+  public float put(int key, float v) {
+    // Does key exists?
+    int index = find(key);
+
+    // Yes!
+    if (index != 0) {
+      // Set new data and exit.
+      float old = values[index];
+      values[index] = v;
+      return old;
+    }
+
+    // Is there enough room for a new pair?
+    if (size == capacity) {
+      // No? Than grow up!
+      grow();
+    }
+
+    // Now that everything is set, the pair can be just put inside with no
+    // worries.
+    prvt_put(key, v);
+
+    return Float.NaN;
+  }
+
+  /**
+   * Removes a &lt;key,value&gt; pair from the map and returns the mapped value,
+   * or {@link Float#NaN} if the none existed.
+   * 
+   * @param key used to find the value to remove
+   * @return the removed value or {@link Float#NaN} if none existed.
+   */
+  public float remove(int key) {
+    int baseHashIndex = calcBaseHashIndex(key);
+    int index = findForRemove(key, baseHashIndex);
+    if (index != 0) {
+      // If it is the first in the collision list, we should promote its
+      // next colliding element.
+      if (prev == 0) {
+        baseHash[baseHashIndex] = next[index];
+      }
+
+      next[prev] = next[index];
+      next[index] = firstEmpty;
+      firstEmpty = index;
+      --size;
+      return values[index];
+    }
+
+    return Float.NaN;
+  }
+
+  /**
+   * @return number of pairs currently in the map
+   */
+  public int size() {
+    return this.size;
+  }
+
+  /**
+   * Translates the mapped pairs' values into an array of Objects
+   * 
+   * @return a float array of all the values currently in the map.
+   */
+  public float[] toArray() {
+    int j = -1;
+    float[] array = new float[size];
+
+    // Iterates over the values, adding them to the array.
+    for (FloatIterator iterator = iterator(); iterator.hasNext();) {
+      array[++j] = iterator.next();
+    }
+    return array;
+  }
+
+  /**
+   * Translates the mapped pairs' values into an array of T
+   * 
+   * @param a
+   *            the array into which the elements of the list are to be
+   *            stored. If it is big enough use whatever space we need,
+   *            setting the one after the true data as {@link Float#NaN}.
+   * 
+   * @return an array containing the elements of the list, using the given
+   *         parameter if big enough, otherwise allocate an appropriate array
+   *         and return it.
+   * 
+   */
+  public float[] toArray(float[] a) {
+    int j = 0;
+    if (a.length < this.size()) {
+      a = new float[this.size()];
+    }
+
+    // Iterates over the values, adding them to the array.
+    for (FloatIterator iterator = iterator(); iterator.hasNext(); ++j) {
+      a[j] = iterator.next();
+    }
+
+    if (j < a.length) {
+      a[j] = Float.NaN;
+    }
+
+    return a;
+  }
+
+  @Override
+  public String toString() {
+    StringBuffer sb = new StringBuffer();
+    sb.append('{');
+    IntIterator keyIterator = keyIterator();
+    while (keyIterator.hasNext()) {
+      int key = keyIterator.next();
+      sb.append(key);
+      sb.append('=');
+      sb.append(get(key));
+      if (keyIterator.hasNext()) {
+        sb.append(',');
+        sb.append(' ');
+      }
+    }
+    sb.append('}');
+    return sb.toString();
+  }
+  
+  @Override
+  public int hashCode() {
+    return getClass().hashCode() ^ size();
+  }
+  
+  @Override
+  public boolean equals(Object o) {
+    IntToFloatMap that = (IntToFloatMap)o;
+    if (that.size() != this.size()) {
+      return false;
+    }
+    
+    IntIterator it = keyIterator();
+    while (it.hasNext()) {
+      int key = it.next();
+      if (!that.containsKey(key)) {
+        return false;
+      }
+
+      float v1 = this.get(key);
+      float v2 = that.get(key);
+      if (Float.compare(v1, v2) != 0) {
+        return false;
+      }
+    }
+    return true;
+  }
+}
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/collections/IntToIntMap.java b/lucene/facet/src/java/org/apache/lucene/facet/collections/IntToIntMap.java
new file mode 100644
index 0000000..f2f27fc
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/collections/IntToIntMap.java
@@ -0,0 +1,622 @@
+package org.apache.lucene.facet.collections;
+
+import java.util.Arrays;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * An Array-based hashtable which maps primitive int to primitive int.<br>
+ * The hashtable is constracted with a given capacity, or 16 as a default. In
+ * case there's not enough room for new pairs, the hashtable grows. <br>
+ * Capacity is adjusted to a power of 2, and there are 2 * capacity entries for
+ * the hash.
+ * 
+ * The pre allocated arrays (for keys, values) are at length of capacity + 1,
+ * when index 0 is used as 'Ground' or 'NULL'.<br>
+ * 
+ * The arrays are allocated ahead of hash operations, and form an 'empty space'
+ * list, to which the key,value pair is allocated.
+ * 
+ * @lucene.experimental
+ */
+public class IntToIntMap {
+
+  public static final int GROUD = -1;
+  /**
+   * Implements an IntIterator which iterates over all the allocated indexes.
+   */
+  private final class IndexIterator implements IntIterator {
+    /**
+     * The last used baseHashIndex. Needed for "jumping" from one hash entry
+     * to another.
+     */
+    private int baseHashIndex = 0;
+
+    /**
+     * The next not-yet-visited index.
+     */
+    private int index = 0;
+
+    /**
+     * Index of the last visited pair. Used in {@link #remove()}.
+     */
+    private int lastIndex = 0;
+
+    /**
+     * Create the Iterator, make <code>index</code> point to the "first"
+     * index which is not empty. If such does not exist (eg. the map is
+     * empty) it would be zero.
+     */
+    public IndexIterator() {
+      for (baseHashIndex = 0; baseHashIndex < baseHash.length; ++baseHashIndex) {
+        index = baseHash[baseHashIndex];
+        if (index != 0) {
+          break;
+        }
+      }
+    }
+
+    @Override
+    public boolean hasNext() {
+      return (index != 0);
+    }
+
+    @Override
+    public int next() {
+      // Save the last index visited
+      lastIndex = index;
+
+      // next the index
+      index = next[index];
+
+      // if the next index points to the 'Ground' it means we're done with
+      // the current hash entry and we need to jump to the next one. This
+      // is done until all the hash entries had been visited.
+      while (index == 0 && ++baseHashIndex < baseHash.length) {
+        index = baseHash[baseHashIndex];
+      }
+
+      return lastIndex;
+    }
+
+    @Override
+    public void remove() {
+      IntToIntMap.this.remove(keys[lastIndex]);
+    }
+
+  }
+
+  /**
+   * Implements an IntIterator, used for iteration over the map's keys.
+   */
+  private final class KeyIterator implements IntIterator {
+    private IntIterator iterator = new IndexIterator();
+
+    KeyIterator() { }
+    
+    @Override
+    public boolean hasNext() {
+      return iterator.hasNext();
+    }
+
+    @Override
+    public int next() {
+      return keys[iterator.next()];
+    }
+
+    @Override
+    public void remove() {
+      iterator.remove();
+    }
+  }
+
+  /**
+   * Implements an IntIterator used for iteration over the map's values.
+   */
+  private final class ValueIterator implements IntIterator {
+    private IntIterator iterator = new IndexIterator();
+
+    ValueIterator() { }
+    
+    @Override
+    public boolean hasNext() {
+      return iterator.hasNext();
+    }
+
+    @Override
+    public int next() {
+      return values[iterator.next()];
+    }
+
+    @Override
+    public void remove() {
+      iterator.remove();
+    }
+  }
+
+  /**
+   * Default capacity - in case no capacity was specified in the constructor
+   */
+  private static int defaultCapacity = 16;
+
+  /**
+   * Holds the base hash entries. if the capacity is 2^N, than the base hash
+   * holds 2^(N+1). It can hold
+   */
+  int[] baseHash;
+
+  /**
+   * The current capacity of the map. Always 2^N and never less than 16. We
+   * never use the zero index. It is needed to improve performance and is also
+   * used as "ground".
+   */
+  private int capacity;
+  /**
+   * All objects are being allocated at map creation. Those objects are "free"
+   * or empty. Whenever a new pair comes along, a pair is being "allocated" or
+   * taken from the free-linked list. as this is just a free list.
+   */
+  private int firstEmpty;
+
+  /**
+   * hashFactor is always (2^(N+1)) - 1. Used for faster hashing.
+   */
+  private int hashFactor;
+
+  /**
+   * This array holds the unique keys
+   */
+  int[] keys;
+
+  /**
+   * In case of collisions, we implement a double linked list of the colliding
+   * hash's with the following next[] and prev[]. Those are also used to store
+   * the "empty" list.
+   */
+  int[] next;
+
+  private int prev;
+
+  /**
+   * Number of currently objects in the map.
+   */
+  private int size;
+
+  /**
+   * This array holds the values
+   */
+  int[] values;
+
+  /**
+   * Constructs a map with default capacity.
+   */
+  public IntToIntMap() {
+    this(defaultCapacity);
+  }
+
+  /**
+   * Constructs a map with given capacity. Capacity is adjusted to a native
+   * power of 2, with minimum of 16.
+   * 
+   * @param capacity
+   *            minimum capacity for the map.
+   */
+  public IntToIntMap(int capacity) {
+    this.capacity = 16;
+    // Minimum capacity is 16..
+    while (this.capacity < capacity) {
+      // Multiply by 2 as long as we're still under the requested capacity
+      this.capacity <<= 1;
+    }
+
+    // As mentioned, we use the first index (0) as 'Ground', so we need the
+    // length of the arrays to be one more than the capacity
+    int arrayLength = this.capacity + 1;
+
+    this.values = new int[arrayLength];
+    this.keys = new int[arrayLength];
+    this.next = new int[arrayLength];
+
+    this.values[0] = GROUD;
+
+    // Hash entries are twice as big as the capacity.
+    int baseHashSize = this.capacity << 1;
+
+    this.baseHash = new int[baseHashSize];
+
+    // The has factor is 2^M - 1 which is used as an "AND" hashing operator.
+    // {@link #calcBaseHash()}
+    this.hashFactor = baseHashSize - 1;
+
+    this.size = 0;
+
+    clear();
+  }
+
+  /**
+   * Adds a pair to the map. Takes the first empty position from the
+   * empty-linked-list's head - {@link #firstEmpty}.
+   * 
+   * New pairs are always inserted to baseHash, and are followed by the old
+   * colliding pair.
+   * 
+   * @param key
+   *            integer which maps the given value
+   * @param e
+   *            value which is being mapped using the given key
+   */
+  private void prvt_put(int key, int e) {
+    // Hash entry to which the new pair would be inserted
+    int hashIndex = calcBaseHashIndex(key);
+
+    // 'Allocating' a pair from the "Empty" list.
+    int objectIndex = firstEmpty;
+
+    // Setting data
+    firstEmpty = next[firstEmpty];
+    values[objectIndex] = e;
+    keys[objectIndex] = key;
+
+    // Inserting the new pair as the first node in the specific hash entry
+    next[objectIndex] = baseHash[hashIndex];
+    baseHash[hashIndex] = objectIndex;
+
+    // Announcing a new pair was added!
+    ++size;
+  }
+
+  /**
+   * Calculating the baseHash index using the internal <code>hashFactor</code>.
+   */
+  protected int calcBaseHashIndex(int key) {
+    return key & hashFactor;
+  }
+
+  /**
+   * Empties the map. Generates the "Empty" space list for later allocation.
+   */
+  public void clear() {
+    // Clears the hash entries
+    Arrays.fill(this.baseHash, 0);
+
+    // Set size to zero
+    size = 0;
+
+    // Mark all array entries as empty. This is done with
+    // <code>firstEmpty</code> pointing to the first valid index (1 as 0 is
+    // used as 'Ground').
+    firstEmpty = 1;
+
+    // And setting all the <code>next[i]</code> to point at
+    // <code>i+1</code>.
+    for (int i = 1; i < this.capacity;) {
+      next[i] = ++i;
+    }
+
+    // Surly, the last one should point to the 'Ground'.
+    next[this.capacity] = 0;
+  }
+
+  /**
+   * Checks if a given key exists in the map.
+   * 
+   * @param key
+   *            that is checked against the map data.
+   * @return true if the key exists in the map. false otherwise.
+   */
+  public boolean containsKey(int key) {
+    return find(key) != 0;
+  }
+
+  /**
+   * Checks if the given object exists in the map.<br>
+   * This method iterates over the collection, trying to find an equal object.
+   * 
+   * @param v
+   *            value that is checked against the map data.
+   * @return true if the value exists in the map (in .equals() meaning).
+   *         false otherwise.
+   */
+  public boolean containsValue(int v) {
+    for (IntIterator iterator = iterator(); iterator.hasNext();) {
+      if (v == iterator.next()) {
+        return true;
+      }
+    }
+    return false;
+  }
+
+  /**
+   * Find the actual index of a given key.
+   * 
+   * @return index of the key. zero if the key wasn't found.
+   */
+  protected int find(int key) {
+    // Calculate the hash entry.
+    int baseHashIndex = calcBaseHashIndex(key);
+
+    // Start from the hash entry.
+    int localIndex = baseHash[baseHashIndex];
+
+    // while the index does not point to the 'Ground'
+    while (localIndex != 0) {
+      // returns the index found in case of of a matching key.
+      if (keys[localIndex] == key) {
+        return localIndex;
+      }
+      
+      // next the local index
+      localIndex = next[localIndex];
+    }
+
+    // If we got this far, it could only mean we did not find the key we
+    // were asked for. return 'Ground' index.
+    return 0;
+  }
+
+  /**
+   * Find the actual index of a given key with it's baseHashIndex.<br>
+   * Some methods use the baseHashIndex. If those call {@link #find} there's
+   * no need to re-calculate that hash.
+   * 
+   * @return the index of the given key, or 0 as 'Ground' if the key wasn't
+   *         found.
+   */
+  private int findForRemove(int key, int baseHashIndex) {
+    // Start from the hash entry.
+    this.prev = 0;
+    int index = baseHash[baseHashIndex];
+
+    // while the index does not point to the 'Ground'
+    while (index != 0) {
+      // returns the index found in case of of a matching key.
+      if (keys[index] == key) {
+        return index;
+      }
+
+      // next the local index
+      prev = index;
+      index = next[index];
+    }
+
+    // If we got this far, it could only mean we did not find the key we
+    // were asked for. return 'Ground' index.
+    this.prev = 0;
+    return 0;
+  }
+
+  /**
+   * Returns the object mapped with the given key.
+   * 
+   * @param key
+   *            int who's mapped object we're interested in.
+   * @return an object mapped by the given key. null if the key wasn't found.
+   */
+  public int get(int key) {
+    return values[find(key)];
+  }
+
+  /**
+   * Grows the map. Allocates a new map of double the capacity, and
+   * fast-insert the old key-value pairs.
+   */
+  protected void grow() {
+    IntToIntMap that = new IntToIntMap(
+        this.capacity * 2);
+
+    // Iterates fast over the collection. Any valid pair is put into the new
+    // map without checking for duplicates or if there's enough space for
+    // it.
+    for (IndexIterator iterator = new IndexIterator(); iterator.hasNext();) {
+      int index = iterator.next();
+      that.prvt_put(this.keys[index], this.values[index]);
+    }
+
+    // Copy that's data into this.
+    this.capacity = that.capacity;
+    this.size = that.size;
+    this.firstEmpty = that.firstEmpty;
+    this.values = that.values;
+    this.keys = that.keys;
+    this.next = that.next;
+    this.baseHash = that.baseHash;
+    this.hashFactor = that.hashFactor;
+  }
+
+  /**
+   * 
+   * @return true if the map is empty. false otherwise.
+   */
+  public boolean isEmpty() {
+    return size == 0;
+  }
+
+  /**
+   * Returns a new iterator for the mapped objects.
+   */
+  public IntIterator iterator() {
+    return new ValueIterator();
+  }
+
+  /** Returns an iterator on the map keys. */
+  public IntIterator keyIterator() {
+    return new KeyIterator();
+  }
+
+  /**
+   * Prints the baseHash array, used for debug purposes.
+   */
+  @SuppressWarnings("unused")
+  private String getBaseHashAsString() {
+    return Arrays.toString(this.baseHash);
+  }
+
+  /**
+   * Inserts the &lt;key,value&gt; pair into the map. If the key already exists,
+   * this method updates the mapped value to the given one, returning the old
+   * mapped value.
+   * 
+   * @return the old mapped value, or 0 if the key didn't exist.
+   */
+  public int put(int key, int e) {
+    // Does key exists?
+    int index = find(key);
+
+    // Yes!
+    if (index != 0) {
+      // Set new data and exit.
+      int old = values[index];
+      values[index] = e;
+      return old;
+    }
+
+    // Is there enough room for a new pair?
+    if (size == capacity) {
+      // No? Than grow up!
+      grow();
+    }
+
+    // Now that everything is set, the pair can be just put inside with no
+    // worries.
+    prvt_put(key, e);
+
+    return 0;
+  }
+
+  /**
+   * Removes a &lt;key,value&gt; pair from the map and returns the mapped value,
+   * or 0 if the none existed.
+   * 
+   * @param key used to find the value to remove
+   * @return the removed value or 0 if none existed.
+   */
+  public int remove(int key) {
+    int baseHashIndex = calcBaseHashIndex(key);
+    int index = findForRemove(key, baseHashIndex);
+    if (index != 0) {
+      // If it is the first in the collision list, we should promote its
+      // next colliding element.
+      if (prev == 0) {
+        baseHash[baseHashIndex] = next[index];
+      }
+
+      next[prev] = next[index];
+      next[index] = firstEmpty;
+      firstEmpty = index;
+      --size;
+      return values[index];
+    }
+
+    return 0;
+  }
+
+  /**
+   * @return number of pairs currently in the map
+   */
+  public int size() {
+    return this.size;
+  }
+
+  /**
+   * Translates the mapped pairs' values into an array of Objects
+   * 
+   * @return an object array of all the values currently in the map.
+   */
+  public int[] toArray() {
+    int j = -1;
+    int[] array = new int[size];
+
+    // Iterates over the values, adding them to the array.
+    for (IntIterator iterator = iterator(); iterator.hasNext();) {
+      array[++j] = iterator.next();
+    }
+    return array;
+  }
+
+  /**
+   * Translates the mapped pairs' values into an array of ints
+   * 
+   * @param a
+   *            the array into which the elements of the map are to be
+   *            stored, if it is big enough; otherwise, a new array of the
+   *            same runtime type is allocated for this purpose.
+   * 
+   * @return an array containing the values stored in the map
+   * 
+   */
+  public int[] toArray(int[] a) {
+    int j = 0;
+    if (a.length < size) {
+      a = new int[size];
+    }
+    // Iterates over the values, adding them to the array.
+    for (IntIterator iterator = iterator(); j < a.length
+      && iterator.hasNext(); ++j) {
+      a[j] = iterator.next();
+    }
+    return a;
+  }
+
+  @Override
+  public String toString() {
+    StringBuffer sb = new StringBuffer();
+    sb.append('{');
+    IntIterator keyIterator = keyIterator();
+    while (keyIterator.hasNext()) {
+      int key = keyIterator.next();
+      sb.append(key);
+      sb.append('=');
+      sb.append(get(key));
+      if (keyIterator.hasNext()) {
+        sb.append(',');
+        sb.append(' ');
+      }
+    }
+    sb.append('}');
+    return sb.toString();
+  }
+  
+  @Override
+  public int hashCode() {
+    return getClass().hashCode() ^ size();
+  }
+  
+  @Override
+  public boolean equals(Object o) {
+    IntToIntMap that = (IntToIntMap)o;
+    if (that.size() != this.size()) {
+      return false;
+    }
+    
+    IntIterator it = keyIterator();
+    while (it.hasNext()) {
+      int key = it.next();
+      
+      if (!that.containsKey(key)) {
+        return false;
+      }
+      
+      int v1 = this.get(key);
+      int v2 = that.get(key);
+      if (v1 != v2) {
+        return false;
+      }
+    }
+    return true;
+  }
+}
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/collections/IntToObjectMap.java b/lucene/facet/src/java/org/apache/lucene/facet/collections/IntToObjectMap.java
new file mode 100644
index 0000000..9db16ec
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/collections/IntToObjectMap.java
@@ -0,0 +1,634 @@
+package org.apache.lucene.facet.collections;
+
+import java.util.Arrays;
+import java.util.Iterator;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * An Array-based hashtable which maps primitive int to Objects of generic type
+ * T.<br>
+ * The hashtable is constracted with a given capacity, or 16 as a default. In
+ * case there's not enough room for new pairs, the hashtable grows. <br>
+ * Capacity is adjusted to a power of 2, and there are 2 * capacity entries for
+ * the hash.
+ * 
+ * The pre allocated arrays (for keys, values) are at length of capacity + 1,
+ * when index 0 is used as 'Ground' or 'NULL'.<br>
+ * 
+ * The arrays are allocated ahead of hash operations, and form an 'empty space'
+ * list, to which the key,value pair is allocated.
+ * 
+ * @lucene.experimental
+ */
+public class IntToObjectMap<T> implements Iterable<T> {
+
+  /**
+   * Implements an IntIterator which iterates over all the allocated indexes.
+   */
+  private final class IndexIterator implements IntIterator {
+    /**
+     * The last used baseHashIndex. Needed for "jumping" from one hash entry
+     * to another.
+     */
+    private int baseHashIndex = 0;
+
+    /**
+     * The next not-yet-visited index.
+     */
+    private int index = 0;
+
+    /**
+     * Index of the last visited pair. Used in {@link #remove()}.
+     */
+    private int lastIndex = 0;
+
+    /**
+     * Create the Iterator, make <code>index</code> point to the "first"
+     * index which is not empty. If such does not exist (eg. the map is
+     * empty) it would be zero.
+     */
+    public IndexIterator() {
+      for (baseHashIndex = 0; baseHashIndex < baseHash.length; ++baseHashIndex) {
+        index = baseHash[baseHashIndex];
+        if (index != 0) {
+          break;
+        }
+      }
+    }
+
+    @Override
+    public boolean hasNext() {
+      return (index != 0);
+    }
+
+    @Override
+    public int next() {
+      // Save the last index visited
+      lastIndex = index;
+
+      // next the index
+      index = next[index];
+
+      // if the next index points to the 'Ground' it means we're done with
+      // the current hash entry and we need to jump to the next one. This
+      // is done until all the hash entries had been visited.
+      while (index == 0 && ++baseHashIndex < baseHash.length) {
+        index = baseHash[baseHashIndex];
+      }
+
+      return lastIndex;
+    }
+
+    @Override
+    public void remove() {
+      IntToObjectMap.this.remove(keys[lastIndex]);
+    }
+
+  }
+
+  /**
+   * Implements an IntIterator, used for iteration over the map's keys.
+   */
+  private final class KeyIterator implements IntIterator {
+    private IntIterator iterator = new IndexIterator();
+
+    KeyIterator() { }
+    
+    @Override
+    public boolean hasNext() {
+      return iterator.hasNext();
+    }
+
+    @Override
+    public int next() {
+      return keys[iterator.next()];
+    }
+
+    @Override
+    public void remove() {
+      iterator.remove();
+    }
+  }
+
+  /**
+   * Implements an Iterator of a generic type T used for iteration over the
+   * map's values.
+   */
+  private final class ValueIterator implements Iterator<T> {
+    private IntIterator iterator = new IndexIterator();
+
+    ValueIterator() { }
+    
+    @Override
+    public boolean hasNext() {
+      return iterator.hasNext();
+    }
+
+    @Override
+    @SuppressWarnings("unchecked")
+    public T next() {
+      return (T) values[iterator.next()];
+    }
+
+    @Override
+    public void remove() {
+      iterator.remove();
+    }
+  }
+
+  /**
+   * Default capacity - in case no capacity was specified in the constructor
+   */
+  private static int defaultCapacity = 16;
+
+  /**
+   * Holds the base hash entries. if the capacity is 2^N, than the base hash
+   * holds 2^(N+1). It can hold
+   */
+  int[] baseHash;
+
+  /**
+   * The current capacity of the map. Always 2^N and never less than 16. We
+   * never use the zero index. It is needed to improve performance and is also
+   * used as "ground".
+   */
+  private int capacity;
+  /**
+   * All objects are being allocated at map creation. Those objects are "free"
+   * or empty. Whenever a new pair comes along, a pair is being "allocated" or
+   * taken from the free-linked list. as this is just a free list.
+   */
+  private int firstEmpty;
+
+  /**
+   * hashFactor is always (2^(N+1)) - 1. Used for faster hashing.
+   */
+  private int hashFactor;
+
+  /**
+   * This array holds the unique keys
+   */
+  int[] keys;
+
+  /**
+   * In case of collisions, we implement a double linked list of the colliding
+   * hash's with the following next[] and prev[]. Those are also used to store
+   * the "empty" list.
+   */
+  int[] next;
+
+  private int prev;
+
+  /**
+   * Number of currently objects in the map.
+   */
+  private int size;
+
+  /**
+   * This array holds the values
+   */
+  Object[] values;
+
+  /**
+   * Constructs a map with default capacity.
+   */
+  public IntToObjectMap() {
+    this(defaultCapacity);
+  }
+
+  /**
+   * Constructs a map with given capacity. Capacity is adjusted to a native
+   * power of 2, with minimum of 16.
+   * 
+   * @param capacity
+   *            minimum capacity for the map.
+   */
+  public IntToObjectMap(int capacity) {
+    this.capacity = 16;
+    // Minimum capacity is 16..
+    while (this.capacity < capacity) {
+      // Multiply by 2 as long as we're still under the requested capacity
+      this.capacity <<= 1;
+    }
+
+    // As mentioned, we use the first index (0) as 'Ground', so we need the
+    // length of the arrays to be one more than the capacity
+    int arrayLength = this.capacity + 1;
+
+    this.values = new Object[arrayLength];
+    this.keys = new int[arrayLength];
+    this.next = new int[arrayLength];
+
+    // Hash entries are twice as big as the capacity.
+    int baseHashSize = this.capacity << 1;
+
+    this.baseHash = new int[baseHashSize];
+
+    // The has factor is 2^M - 1 which is used as an "AND" hashing operator.
+    // {@link #calcBaseHash()}
+    this.hashFactor = baseHashSize - 1;
+
+    this.size = 0;
+
+    clear();
+  }
+
+  /**
+   * Adds a pair to the map. Takes the first empty position from the
+   * empty-linked-list's head - {@link #firstEmpty}.
+   * 
+   * New pairs are always inserted to baseHash, and are followed by the old
+   * colliding pair.
+   * 
+   * @param key
+   *            integer which maps the given Object
+   * @param e
+   *            element which is being mapped using the given key
+   */
+  private void prvt_put(int key, T e) {
+    // Hash entry to which the new pair would be inserted
+    int hashIndex = calcBaseHashIndex(key);
+
+    // 'Allocating' a pair from the "Empty" list.
+    int objectIndex = firstEmpty;
+
+    // Setting data
+    firstEmpty = next[firstEmpty];
+    values[objectIndex] = e;
+    keys[objectIndex] = key;
+
+    // Inserting the new pair as the first node in the specific hash entry
+    next[objectIndex] = baseHash[hashIndex];
+    baseHash[hashIndex] = objectIndex;
+
+    // Announcing a new pair was added!
+    ++size;
+  }
+
+  /**
+   * Calculating the baseHash index using the internal <code>hashFactor</code>.
+   * 
+   */
+  protected int calcBaseHashIndex(int key) {
+    return key & hashFactor;
+  }
+
+  /**
+   * Empties the map. Generates the "Empty" space list for later allocation.
+   */
+  public void clear() {
+    // Clears the hash entries
+    Arrays.fill(this.baseHash, 0);
+
+    // Set size to zero
+    size = 0;
+
+    // Mark all array entries as empty. This is done with
+    // <code>firstEmpty</code> pointing to the first valid index (1 as 0 is
+    // used as 'Ground').
+    firstEmpty = 1;
+
+    // And setting all the <code>next[i]</code> to point at
+    // <code>i+1</code>.
+    for (int i = 1; i < this.capacity;) {
+      next[i] = ++i;
+    }
+
+    // Surly, the last one should point to the 'Ground'.
+    next[this.capacity] = 0;
+  }
+
+  /**
+   * Checks if a given key exists in the map.
+   * 
+   * @param key
+   *            that is checked against the map data.
+   * @return true if the key exists in the map. false otherwise.
+   */
+  public boolean containsKey(int key) {
+    return find(key) != 0;
+  }
+
+  /**
+   * Checks if the given object exists in the map.<br>
+   * This method iterates over the collection, trying to find an equal object.
+   * 
+   * @param o
+   *            object that is checked against the map data.
+   * @return true if the object exists in the map (in .equals() meaning).
+   *         false otherwise.
+   */
+  public boolean containsValue(Object o) {
+    for (Iterator<T> iterator = iterator(); iterator.hasNext();) {
+      T object = iterator.next();
+      if (object.equals(o)) {
+        return true;
+      }
+    }
+    return false;
+  }
+
+  /**
+   * Find the actual index of a given key.
+   * 
+   * @return index of the key. zero if the key wasn't found.
+   */
+  protected int find(int key) {
+    // Calculate the hash entry.
+    int baseHashIndex = calcBaseHashIndex(key);
+
+    // Start from the hash entry.
+    int localIndex = baseHash[baseHashIndex];
+
+    // while the index does not point to the 'Ground'
+    while (localIndex != 0) {
+      // returns the index found in case of of a matching key.
+      if (keys[localIndex] == key) {
+        return localIndex;
+      }
+
+      // next the local index
+      localIndex = next[localIndex];
+    }
+
+    // If we got this far, it could only mean we did not find the key we
+    // were asked for. return 'Ground' index.
+    return 0;
+  }
+
+  /**
+   * Find the actual index of a given key with it's baseHashIndex.<br>
+   * Some methods use the baseHashIndex. If those call {@link #find} there's
+   * no need to re-calculate that hash.
+   * 
+   * @return the index of the given key, or 0 as 'Ground' if the key wasn't
+   *         found.
+   */
+  private int findForRemove(int key, int baseHashIndex) {
+    // Start from the hash entry.
+    this.prev = 0;
+    int index = baseHash[baseHashIndex];
+
+    // while the index does not point to the 'Ground'
+    while (index != 0) {
+      // returns the index found in case of of a matching key.
+      if (keys[index] == key) {
+        return index;
+      }
+
+      // next the local index
+      prev = index;
+      index = next[index];
+    }
+
+    // If we got this far, it could only mean we did not find the key we
+    // were asked for. return 'Ground' index.
+    this.prev = 0;
+    return 0;
+  }
+
+  /**
+   * Returns the object mapped with the given key.
+   * 
+   * @param key
+   *            int who's mapped object we're interested in.
+   * @return an object mapped by the given key. null if the key wasn't found.
+   */
+  @SuppressWarnings("unchecked")
+  public T get(int key) {
+    return (T) values[find(key)];
+  }
+
+  /**
+   * Grows the map. Allocates a new map of double the capacity, and
+   * fast-insert the old key-value pairs.
+   */
+  @SuppressWarnings("unchecked")
+  protected void grow() {
+    IntToObjectMap<T> that = new IntToObjectMap<T>(
+        this.capacity * 2);
+
+    // Iterates fast over the collection. Any valid pair is put into the new
+    // map without checking for duplicates or if there's enough space for
+    // it.
+    for (IndexIterator iterator = new IndexIterator(); iterator.hasNext();) {
+      int index = iterator.next();
+      that.prvt_put(this.keys[index], (T) this.values[index]);
+    }
+
+    // Copy that's data into this.
+    this.capacity = that.capacity;
+    this.size = that.size;
+    this.firstEmpty = that.firstEmpty;
+    this.values = that.values;
+    this.keys = that.keys;
+    this.next = that.next;
+    this.baseHash = that.baseHash;
+    this.hashFactor = that.hashFactor;
+  }
+
+  /**
+   * 
+   * @return true if the map is empty. false otherwise.
+   */
+  public boolean isEmpty() {
+    return size == 0;
+  }
+
+  /**
+   * Returns a new iterator for the mapped objects.
+   */
+  @Override
+  public Iterator<T> iterator() {
+    return new ValueIterator();
+  }
+
+  /** Returns an iterator on the map keys. */
+  public IntIterator keyIterator() {
+    return new KeyIterator();
+  }
+
+  /**
+   * Prints the baseHash array, used for debug purposes.
+   */
+  @SuppressWarnings("unused")
+  private String getBaseHashAsString() {
+    return Arrays.toString(baseHash);
+  }
+
+  /**
+   * Inserts the &lt;key,value&gt; pair into the map. If the key already exists,
+   * this method updates the mapped value to the given one, returning the old
+   * mapped value.
+   * 
+   * @return the old mapped value, or null if the key didn't exist.
+   */
+  @SuppressWarnings("unchecked")
+  public T put(int key, T e) {
+    // Does key exists?
+    int index = find(key);
+
+    // Yes!
+    if (index != 0) {
+      // Set new data and exit.
+      T old = (T) values[index];
+      values[index] = e;
+      return old;
+    }
+
+    // Is there enough room for a new pair?
+    if (size == capacity) {
+      // No? Than grow up!
+      grow();
+    }
+
+    // Now that everything is set, the pair can be just put inside with no
+    // worries.
+    prvt_put(key, e);
+
+    return null;
+  }
+
+  /**
+   * Removes a &lt;key,value&gt; pair from the map and returns the mapped value,
+   * or null if the none existed.
+   * 
+   * @param key used to find the value to remove
+   * @return the removed value or null if none existed.
+   */
+  @SuppressWarnings("unchecked")
+  public T remove(int key) {
+    int baseHashIndex = calcBaseHashIndex(key);
+    int index = findForRemove(key, baseHashIndex);
+    if (index != 0) {
+      // If it is the first in the collision list, we should promote its
+      // next colliding element.
+      if (prev == 0) {
+        baseHash[baseHashIndex] = next[index];
+      }
+
+      next[prev] = next[index];
+      next[index] = firstEmpty;
+      firstEmpty = index;
+      --size;
+      return (T) values[index];
+    }
+
+    return null;
+  }
+
+  /**
+   * @return number of pairs currently in the map
+   */
+  public int size() {
+    return this.size;
+  }
+
+  /**
+   * Translates the mapped pairs' values into an array of Objects
+   * 
+   * @return an object array of all the values currently in the map.
+   */
+  public Object[] toArray() {
+    int j = -1;
+    Object[] array = new Object[size];
+
+    // Iterates over the values, adding them to the array.
+    for (Iterator<T> iterator = iterator(); iterator.hasNext();) {
+      array[++j] = iterator.next();
+    }
+    return array;
+  }
+
+  /**
+   * Translates the mapped pairs' values into an array of T
+   * 
+   * @param a
+   *            the array into which the elements of the list are to be
+   *            stored, if it is big enough; otherwise, use whatever space we
+   *            have, setting the one after the true data as null.
+   * 
+   * @return an array containing the elements of the list
+   * 
+   */
+  public T[] toArray(T[] a) {
+    int j = 0;
+    // Iterates over the values, adding them to the array.
+    for (Iterator<T> iterator = iterator(); j < a.length
+    && iterator.hasNext(); ++j) {
+      a[j] = iterator.next();
+    }
+
+    if (j < a.length) {
+      a[j] = null;
+    }
+
+    return a;
+  }
+
+  @Override
+  public String toString() {
+    StringBuffer sb = new StringBuffer();
+    sb.append('{');
+    IntIterator keyIterator = keyIterator();
+    while (keyIterator.hasNext()) {
+      int key = keyIterator.next();
+      sb.append(key);
+      sb.append('=');
+      sb.append(get(key));
+      if (keyIterator.hasNext()) {
+        sb.append(',');
+        sb.append(' ');
+      }
+    }
+    sb.append('}');
+    return sb.toString();
+  }
+  
+  @Override
+  public int hashCode() {
+    return getClass().hashCode() ^ size();
+  }
+  
+  @SuppressWarnings("unchecked")
+  @Override
+  public boolean equals(Object o) {
+    IntToObjectMap<T> that = (IntToObjectMap<T>)o;
+    if (that.size() != this.size()) {
+      return false;
+    }
+    
+    IntIterator it = keyIterator();
+    while (it.hasNext()) {
+      int key = it.next();
+      if (!that.containsKey(key)) {
+        return false;
+      }
+
+      T v1 = this.get(key);
+      T v2 = that.get(key);
+      if ((v1 == null && v2 != null) ||
+          (v1 != null && v2 == null) ||
+          (!v1.equals(v2))) {
+        return false;
+      }
+    }
+    return true;
+  }
+}
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/collections/LRUHashMap.java b/lucene/facet/src/java/org/apache/lucene/facet/collections/LRUHashMap.java
new file mode 100644
index 0000000..5565319
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/collections/LRUHashMap.java
@@ -0,0 +1,111 @@
+package org.apache.lucene.facet.collections;
+
+import java.util.LinkedHashMap;
+import java.util.Map;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * LRUHashMap is an extension of Java's HashMap, which has a bounded size();
+ * When it reaches that size, each time a new element is added, the least
+ * recently used (LRU) entry is removed.
+ * <p>
+ * Java makes it very easy to implement LRUHashMap - all its functionality is
+ * already available from {@link java.util.LinkedHashMap}, and we just need to
+ * configure that properly.
+ * <p>
+ * Note that like HashMap, LRUHashMap is unsynchronized, and the user MUST
+ * synchronize the access to it if used from several threads. Moreover, while
+ * with HashMap this is only a concern if one of the threads is modifies the
+ * map, with LURHashMap every read is a modification (because the LRU order
+ * needs to be remembered) so proper synchronization is always necessary.
+ * <p>
+ * With the usual synchronization mechanisms available to the user, this
+ * unfortunately means that LRUHashMap will probably perform sub-optimally under
+ * heavy contention: while one thread uses the hash table (reads or writes), any
+ * other thread will be blocked from using it - or even just starting to use it
+ * (e.g., calculating the hash function). A more efficient approach would be not
+ * to use LinkedHashMap at all, but rather to use a non-locking (as much as
+ * possible) thread-safe solution, something along the lines of
+ * java.util.concurrent.ConcurrentHashMap (though that particular class does not
+ * support the additional LRU semantics, which will need to be added separately
+ * using a concurrent linked list or additional storage of timestamps (in an
+ * array or inside the entry objects), or whatever).
+ * 
+ * @lucene.experimental
+ */
+public class LRUHashMap<K,V> extends LinkedHashMap<K,V> {
+
+  private int maxSize;
+
+  /**
+   * Create a new hash map with a bounded size and with least recently
+   * used entries removed.
+   * @param maxSize
+   *     the maximum size (in number of entries) to which the map can grow
+   *     before the least recently used entries start being removed.<BR>
+   *      Setting maxSize to a very large value, like
+   *      {@link Integer#MAX_VALUE} is allowed, but is less efficient than
+   *      using {@link java.util.HashMap} because our class needs
+   *      to keep track of the use order (via an additional doubly-linked
+   *      list) which is not used when the map's size is always below the
+   *      maximum size. 
+   */
+  public LRUHashMap(int maxSize) {
+    super(16, 0.75f, true);
+    this.maxSize = maxSize;
+  }
+
+  /**
+   * Return the max size
+   */
+  public int getMaxSize() {
+    return maxSize;
+  }
+
+  /**
+   * setMaxSize() allows changing the map's maximal number of elements
+   * which was defined at construction time.
+   * <P>
+   * Note that if the map is already larger than maxSize, the current 
+   * implementation does not shrink it (by removing the oldest elements);
+   * Rather, the map remains in its current size as new elements are
+   * added, and will only start shrinking (until settling again on the
+   * give maxSize) if existing elements are explicitly deleted.  
+   */
+  public void setMaxSize(int maxSize) {
+    this.maxSize = maxSize;
+  }
+
+  // We override LinkedHashMap's removeEldestEntry() method. This method
+  // is called every time a new entry is added, and if we return true
+  // here, the eldest element will be deleted automatically. In our case,
+  // we return true if the size of the map grew beyond our limit - ignoring
+  // what is that eldest element that we'll be deleting.
+  @Override
+  protected boolean removeEldestEntry(Map.Entry<K, V> eldest) {
+    return size() > maxSize;
+  }
+
+  @SuppressWarnings("unchecked")
+  @Override
+  public LRUHashMap<K,V> clone() {
+    return (LRUHashMap<K,V>) super.clone();
+  }
+  
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/collections/ObjectToFloatMap.java b/lucene/facet/src/java/org/apache/lucene/facet/collections/ObjectToFloatMap.java
new file mode 100644
index 0000000..7c7a95c
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/collections/ObjectToFloatMap.java
@@ -0,0 +1,623 @@
+package org.apache.lucene.facet.collections;
+
+import java.util.Arrays;
+import java.util.Iterator;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * An Array-based hashtable which maps Objects of generic type
+ * T to primitive float values.<br>
+ * The hashtable is constructed with a given capacity, or 16 as a default. In
+ * case there's not enough room for new pairs, the hashtable grows. <br>
+ * Capacity is adjusted to a power of 2, and there are 2 * capacity entries for
+ * the hash.
+ * 
+ * The pre allocated arrays (for keys, values) are at length of capacity + 1,
+ * when index 0 is used as 'Ground' or 'NULL'.<br>
+ * 
+ * The arrays are allocated ahead of hash operations, and form an 'empty space'
+ * list, to which the key,value pair is allocated.
+ * 
+ * @lucene.experimental
+ */
+public class ObjectToFloatMap<K> {
+
+  /**
+   * Implements an IntIterator which iterates over all the allocated indexes.
+   */
+  private final class IndexIterator implements IntIterator {
+    /**
+     * The last used baseHashIndex. Needed for "jumping" from one hash entry
+     * to another.
+     */
+    private int baseHashIndex = 0;
+
+    /**
+     * The next not-yet-visited index.
+     */
+    private int index = 0;
+
+    /**
+     * Index of the last visited pair. Used in {@link #remove()}.
+     */
+    private int lastIndex = 0;
+
+    /**
+     * Create the Iterator, make <code>index</code> point to the "first"
+     * index which is not empty. If such does not exist (eg. the map is
+     * empty) it would be zero.
+     */
+    public IndexIterator() {
+      for (baseHashIndex = 0; baseHashIndex < baseHash.length; ++baseHashIndex) {
+        index = baseHash[baseHashIndex];
+        if (index != 0) {
+          break;
+        }
+      }
+    }
+
+    @Override
+    public boolean hasNext() {
+      return (index != 0);
+    }
+
+    @Override
+    public int next() {
+      // Save the last index visited
+      lastIndex = index;
+
+      // next the index
+      index = next[index];
+
+      // if the next index points to the 'Ground' it means we're done with
+      // the current hash entry and we need to jump to the next one. This
+      // is done until all the hash entries had been visited.
+      while (index == 0 && ++baseHashIndex < baseHash.length) {
+        index = baseHash[baseHashIndex];
+      }
+
+      return lastIndex;
+    }
+
+    @Override
+    @SuppressWarnings("unchecked")
+    public void remove() {
+      ObjectToFloatMap.this.remove((K) keys[lastIndex]);
+    }
+
+  }
+
+  /**
+   * Implements an IntIterator, used for iteration over the map's keys.
+   */
+  private final class KeyIterator implements Iterator<K> {
+    private IntIterator iterator = new IndexIterator();
+
+    KeyIterator() { }
+    
+    @Override
+    public boolean hasNext() {
+      return iterator.hasNext();
+    }
+
+    @Override
+    @SuppressWarnings("unchecked")
+    public K next() {
+      return (K) keys[iterator.next()];
+    }
+
+    @Override
+    public void remove() {
+      iterator.remove();
+    }
+  }
+
+  /**
+   * Implements an Iterator of a generic type T used for iteration over the
+   * map's values.
+   */
+  private final class ValueIterator implements FloatIterator {
+    private IntIterator iterator = new IndexIterator();
+
+    ValueIterator() { }
+    
+    @Override
+    public boolean hasNext() {
+      return iterator.hasNext();
+    }
+
+    @Override
+    public float next() {
+      return values[iterator.next()];
+    }
+
+    @Override
+    public void remove() {
+      iterator.remove();
+    }
+  }
+
+  /**
+   * Default capacity - in case no capacity was specified in the constructor
+   */
+  private static int defaultCapacity = 16;
+
+  /**
+   * Holds the base hash entries. if the capacity is 2^N, than the base hash
+   * holds 2^(N+1). It can hold
+   */
+  int[] baseHash;
+
+  /**
+   * The current capacity of the map. Always 2^N and never less than 16. We
+   * never use the zero index. It is needed to improve performance and is also
+   * used as "ground".
+   */
+  private int capacity;
+  /**
+   * All objects are being allocated at map creation. Those objects are "free"
+   * or empty. Whenever a new pair comes along, a pair is being "allocated" or
+   * taken from the free-linked list. as this is just a free list.
+   */
+  private int firstEmpty;
+
+  /**
+   * hashFactor is always (2^(N+1)) - 1. Used for faster hashing.
+   */
+  private int hashFactor;
+
+  /**
+   * This array holds the unique keys
+   */
+  Object[] keys;
+
+  /**
+   * In case of collisions, we implement a double linked list of the colliding
+   * hash's with the following next[] and prev[]. Those are also used to store
+   * the "empty" list.
+   */
+  int[] next;
+
+  private int prev;
+
+  /**
+   * Number of currently objects in the map.
+   */
+  private int size;
+
+  /**
+   * This array holds the values
+   */
+  float[] values;
+
+  /**
+   * Constructs a map with default capacity.
+   */
+  public ObjectToFloatMap() {
+    this(defaultCapacity);
+  }
+
+  /**
+   * Constructs a map with given capacity. Capacity is adjusted to a native
+   * power of 2, with minimum of 16.
+   * 
+   * @param capacity
+   *            minimum capacity for the map.
+   */
+  public ObjectToFloatMap(int capacity) {
+    this.capacity = 16;
+    // Minimum capacity is 16..
+    while (this.capacity < capacity) {
+      // Multiply by 2 as long as we're still under the requested capacity
+      this.capacity <<= 1;
+    }
+
+    // As mentioned, we use the first index (0) as 'Ground', so we need the
+    // length of the arrays to be one more than the capacity
+    int arrayLength = this.capacity + 1;
+
+    this.values = new float[arrayLength];
+    this.keys = new Object[arrayLength];
+    this.next = new int[arrayLength];
+
+    // Hash entries are twice as big as the capacity.
+    int baseHashSize = this.capacity << 1;
+
+    this.baseHash = new int[baseHashSize];
+
+    // The has factor is 2^M - 1 which is used as an "AND" hashing operator.
+    // {@link #calcBaseHash()}
+    this.hashFactor = baseHashSize - 1;
+
+    this.size = 0;
+
+    clear();
+  }
+
+  /**
+   * Adds a pair to the map. Takes the first empty position from the
+   * empty-linked-list's head - {@link #firstEmpty}.
+   * 
+   * New pairs are always inserted to baseHash, and are followed by the old
+   * colliding pair.
+   * 
+   * @param key
+   *            integer which maps the given Object
+   * @param e
+   *            element which is being mapped using the given key
+   */
+  private void prvt_put(K key, float e) {
+    // Hash entry to which the new pair would be inserted
+    int hashIndex = calcBaseHashIndex(key);
+
+    // 'Allocating' a pair from the "Empty" list.
+    int objectIndex = firstEmpty;
+
+    // Setting data
+    firstEmpty = next[firstEmpty];
+    values[objectIndex] = e;
+    keys[objectIndex] = key;
+
+    // Inserting the new pair as the first node in the specific hash entry
+    next[objectIndex] = baseHash[hashIndex];
+    baseHash[hashIndex] = objectIndex;
+
+    // Announcing a new pair was added!
+    ++size;
+  }
+
+  /**
+   * Calculating the baseHash index using the internal <code>hashFactor</code>.
+   */
+  protected int calcBaseHashIndex(K key) {
+    return key.hashCode() & hashFactor;
+  }
+
+  /**
+   * Empties the map. Generates the "Empty" space list for later allocation.
+   */
+  public void clear() {
+    // Clears the hash entries
+    Arrays.fill(this.baseHash, 0);
+
+    // Set size to zero
+    size = 0;
+
+    values[0] = Float.NaN;
+
+    // Mark all array entries as empty. This is done with
+    // <code>firstEmpty</code> pointing to the first valid index (1 as 0 is
+    // used as 'Ground').
+    firstEmpty = 1;
+
+    // And setting all the <code>next[i]</code> to point at
+    // <code>i+1</code>.
+    for (int i = 1; i < this.capacity;) {
+      next[i] = ++i;
+    }
+
+    // Surly, the last one should point to the 'Ground'.
+    next[this.capacity] = 0;
+  }
+
+  /**
+   * Checks if a given key exists in the map.
+   * 
+   * @param key
+   *            that is checked against the map data.
+   * @return true if the key exists in the map. false otherwise.
+   */
+  public boolean containsKey(K key) {
+    return find(key) != 0;
+  }
+
+  /**
+   * Checks if the given object exists in the map.<br>
+   * This method iterates over the collection, trying to find an equal object.
+   * 
+   * @param o
+   *            object that is checked against the map data.
+   * @return true if the object exists in the map (in .equals() meaning).
+   *         false otherwise.
+   */
+  public boolean containsValue(float o) {
+    for (FloatIterator iterator = iterator(); iterator.hasNext();) {
+      if (o == iterator.next()) {
+        return true;
+      }
+    }
+    return false;
+  }
+
+  /**
+   * Find the actual index of a given key.
+   * 
+   * @return index of the key. zero if the key wasn't found.
+   */
+  protected int find(K key) {
+    // Calculate the hash entry.
+    int baseHashIndex = calcBaseHashIndex(key);
+
+    // Start from the hash entry.
+    int localIndex = baseHash[baseHashIndex];
+
+    // while the index does not point to the 'Ground'
+    while (localIndex != 0) {
+      // returns the index found in case of of a matching key.
+      if (keys[localIndex].equals(key)) {
+        return localIndex;
+      }
+
+      // next the local index
+      localIndex = next[localIndex];
+    }
+
+    // If we got this far, it could only mean we did not find the key we
+    // were asked for. return 'Ground' index.
+    return 0;
+  }
+
+  /**
+   * Find the actual index of a given key with it's baseHashIndex.<br>
+   * Some methods use the baseHashIndex. If those call {@link #find} there's
+   * no need to re-calculate that hash.
+   * 
+   * @return the index of the given key, or 0 as 'Ground' if the key wasn't
+   *         found.
+   */
+  private int findForRemove(K key, int baseHashIndex) {
+    // Start from the hash entry.
+    this.prev = 0;
+    int index = baseHash[baseHashIndex];
+
+    // while the index does not point to the 'Ground'
+    while (index != 0) {
+      // returns the index found in case of of a matching key.
+      if (keys[index].equals(key)) {
+        return index;
+      }
+
+      // next the local index
+      prev = index;
+      index = next[index];
+    }
+
+    // If we got this far, it could only mean we did not find the key we
+    // were asked for. return 'Ground' index.
+    this.prev = 0;
+    return 0;
+  }
+
+  /**
+   * Returns the float mapped with the given key.
+   * 
+   * @param key
+   *            object who's mapped float we're interested in.
+   * @return a float mapped by the given key. Float.NaN if the key wasn't found.
+   */
+  public float get(K key) {
+    return values[find(key)];
+  }
+
+  /**
+   * Grows the map. Allocates a new map of double the capacity, and
+   * fast-insert the old key-value pairs.
+   */
+  @SuppressWarnings("unchecked")
+  protected void grow() {
+    ObjectToFloatMap<K> that = new ObjectToFloatMap<K>(
+        this.capacity * 2);
+
+    // Iterates fast over the collection. Any valid pair is put into the new
+    // map without checking for duplicates or if there's enough space for
+    // it.
+    for (IndexIterator iterator = new IndexIterator(); iterator.hasNext();) {
+      int index = iterator.next();
+      that.prvt_put((K) this.keys[index], this.values[index]);
+    }
+
+    // Copy that's data into this.
+    this.capacity = that.capacity;
+    this.size = that.size;
+    this.firstEmpty = that.firstEmpty;
+    this.values = that.values;
+    this.keys = that.keys;
+    this.next = that.next;
+    this.baseHash = that.baseHash;
+    this.hashFactor = that.hashFactor;
+  }
+
+  /**
+   * 
+   * @return true if the map is empty. false otherwise.
+   */
+  public boolean isEmpty() {
+    return size == 0;
+  }
+
+  /**
+   * Returns a new iterator for the mapped floats.
+   */
+  public FloatIterator iterator() {
+    return new ValueIterator();
+  }
+
+  /** Returns an iterator on the map keys. */
+  public Iterator<K> keyIterator() {
+    return new KeyIterator();
+  }
+
+  /**
+   * Prints the baseHash array, used for debug purposes.
+   */
+  @SuppressWarnings("unused")
+  private String getBaseHashAsString() {
+    return Arrays.toString(baseHash);
+  }
+
+  /**
+   * Inserts the &lt;key,value&gt; pair into the map. If the key already exists,
+   * this method updates the mapped value to the given one, returning the old
+   * mapped value.
+   * 
+   * @return the old mapped value, or {@link Float#NaN} if the key didn't exist.
+   */
+  public float put(K key, float e) {
+    // Does key exists?
+    int index = find(key);
+
+    // Yes!
+    if (index != 0) {
+      // Set new data and exit.
+      float old = values[index];
+      values[index] = e;
+      return old;
+    }
+
+    // Is there enough room for a new pair?
+    if (size == capacity) {
+      // No? Than grow up!
+      grow();
+    }
+    
+    // Now that everything is set, the pair can be just put inside with no
+    // worries.
+    prvt_put(key, e);
+
+    return Float.NaN;
+  }
+
+  /**
+   * Removes a &lt;key,value&gt; pair from the map and returns the mapped value,
+   * or {@link Float#NaN} if the none existed.
+   * 
+   * @param key used to find the value to remove
+   * @return the removed value or {@link Float#NaN} if none existed.
+   */
+  public float remove(K key) {
+    int baseHashIndex = calcBaseHashIndex(key);
+    int index = findForRemove(key, baseHashIndex);
+    if (index != 0) {
+      // If it is the first in the collision list, we should promote its
+      // next colliding element.
+      if (prev == 0) {
+        baseHash[baseHashIndex] = next[index];
+      }
+
+      next[prev] = next[index];
+      next[index] = firstEmpty;
+      firstEmpty = index;
+      --size;
+      return values[index];
+    }
+
+    return Float.NaN;
+  }
+
+  /**
+   * @return number of pairs currently in the map
+   */
+  public int size() {
+    return this.size;
+  }
+
+  /**
+   * Translates the mapped pairs' values into an array of Objects
+   * 
+   * @return an object array of all the values currently in the map.
+   */
+  public float[] toArray() {
+    int j = -1;
+    float[] array = new float[size];
+
+    // Iterates over the values, adding them to the array.
+    for (FloatIterator iterator = iterator(); iterator.hasNext();) {
+      array[++j] = iterator.next();
+    }
+    return array;
+  }
+
+  /**
+   * Translates the mapped pairs' values into an array of T
+   * 
+   * @param a
+   *            the array into which the elements of the list are to be
+   *            stored, if it is big enough; otherwise, use as much space as it can.
+   * 
+   * @return an array containing the elements of the list
+   * 
+   */
+  public float[] toArray(float[] a) {
+    int j = 0;
+    // Iterates over the values, adding them to the array.
+    for (FloatIterator iterator = iterator(); j < a.length
+    && iterator.hasNext(); ++j) {
+      a[j] = iterator.next();
+    }
+    if (j < a.length) {
+      a[j] = Float.NaN;
+    }
+
+    return a;
+  }
+
+  @Override
+  public String toString() {
+    StringBuffer sb = new StringBuffer();
+    sb.append('{');
+    Iterator<K> keyIterator = keyIterator();
+    while (keyIterator.hasNext()) {
+      K key = keyIterator.next();
+      sb.append(key);
+      sb.append('=');
+      sb.append(get(key));
+      if (keyIterator.hasNext()) {
+        sb.append(',');
+        sb.append(' ');
+      }
+    }
+    sb.append('}');
+    return sb.toString();
+  }
+  
+  @Override
+  public int hashCode() {
+    return getClass().hashCode() ^ size();
+  }
+  
+  @SuppressWarnings("unchecked")
+  @Override
+  public boolean equals(Object o) {
+    ObjectToFloatMap<K> that = (ObjectToFloatMap<K>)o;
+    if (that.size() != this.size()) {
+      return false;
+    }
+    
+    Iterator<K> it = keyIterator();
+    while (it.hasNext()) {
+      K key = it.next();
+      float v1 = this.get(key);
+      float v2 = that.get(key);
+      if (Float.compare(v1, v2) != 0) {
+        return false;
+      }
+    }
+    return true;
+  }
+}
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/collections/ObjectToIntMap.java b/lucene/facet/src/java/org/apache/lucene/facet/collections/ObjectToIntMap.java
new file mode 100644
index 0000000..a7ce7a6
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/collections/ObjectToIntMap.java
@@ -0,0 +1,622 @@
+package org.apache.lucene.facet.collections;
+
+import java.util.Arrays;
+import java.util.Iterator;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * An Array-based hashtable which maps Objects of generic type
+ * T to primitive int values.<br>
+ * The hashtable is constructed with a given capacity, or 16 as a default. In
+ * case there's not enough room for new pairs, the hashtable grows. <br>
+ * Capacity is adjusted to a power of 2, and there are 2 * capacity entries for
+ * the hash.
+ * 
+ * The pre allocated arrays (for keys, values) are at length of capacity + 1,
+ * when index 0 is used as 'Ground' or 'NULL'.<br>
+ * 
+ * The arrays are allocated ahead of hash operations, and form an 'empty space'
+ * list, to which the key,value pair is allocated.
+ * 
+ * @lucene.experimental
+ */
+public class ObjectToIntMap<K> {
+
+  /**
+   * Implements an IntIterator which iterates over all the allocated indexes.
+   */
+  private final class IndexIterator implements IntIterator {
+    /**
+     * The last used baseHashIndex. Needed for "jumping" from one hash entry
+     * to another.
+     */
+    private int baseHashIndex = 0;
+
+    /**
+     * The next not-yet-visited index.
+     */
+    private int index = 0;
+
+    /**
+     * Index of the last visited pair. Used in {@link #remove()}.
+     */
+    private int lastIndex = 0;
+
+    /**
+     * Create the Iterator, make <code>index</code> point to the "first"
+     * index which is not empty. If such does not exist (eg. the map is
+     * empty) it would be zero.
+     */
+    public IndexIterator() {
+      for (baseHashIndex = 0; baseHashIndex < baseHash.length; ++baseHashIndex) {
+        index = baseHash[baseHashIndex];
+        if (index != 0) {
+          break;
+        }
+      }
+    }
+
+    @Override
+    public boolean hasNext() {
+      return (index != 0);
+    }
+
+    @Override
+    public int next() {
+      // Save the last index visited
+      lastIndex = index;
+
+      // next the index
+      index = next[index];
+
+      // if the next index points to the 'Ground' it means we're done with
+      // the current hash entry and we need to jump to the next one. This
+      // is done until all the hash entries had been visited.
+      while (index == 0 && ++baseHashIndex < baseHash.length) {
+        index = baseHash[baseHashIndex];
+      }
+
+      return lastIndex;
+    }
+
+    @Override
+    @SuppressWarnings("unchecked")
+    public void remove() {
+      ObjectToIntMap.this.remove((K) keys[lastIndex]);
+    }
+
+  }
+
+  /**
+   * Implements an IntIterator, used for iteration over the map's keys.
+   */
+  private final class KeyIterator implements Iterator<K> {
+    private IntIterator iterator = new IndexIterator();
+
+    KeyIterator() { }
+    
+    @Override
+    public boolean hasNext() {
+      return iterator.hasNext();
+    }
+
+    @Override
+    @SuppressWarnings("unchecked")
+    public K next() {
+      return (K) keys[iterator.next()];
+    }
+
+    @Override
+    public void remove() {
+      iterator.remove();
+    }
+  }
+
+  /**
+   * Implements an Iterator of a generic type T used for iteration over the
+   * map's values.
+   */
+  private final class ValueIterator implements IntIterator {
+    private IntIterator iterator = new IndexIterator();
+
+    ValueIterator() {}
+    
+    @Override
+    public boolean hasNext() {
+      return iterator.hasNext();
+    }
+
+    @Override
+    public int next() {
+      return values[iterator.next()];
+    }
+
+    @Override
+    public void remove() {
+      iterator.remove();
+    }
+  }
+
+  /**
+   * Default capacity - in case no capacity was specified in the constructor
+   */
+  private static int defaultCapacity = 16;
+
+  /**
+   * Holds the base hash entries. if the capacity is 2^N, than the base hash
+   * holds 2^(N+1). It can hold
+   */
+  int[] baseHash;
+
+  /**
+   * The current capacity of the map. Always 2^N and never less than 16. We
+   * never use the zero index. It is needed to improve performance and is also
+   * used as "ground".
+   */
+  private int capacity;
+  /**
+   * All objects are being allocated at map creation. Those objects are "free"
+   * or empty. Whenever a new pair comes along, a pair is being "allocated" or
+   * taken from the free-linked list. as this is just a free list.
+   */
+  private int firstEmpty;
+
+  /**
+   * hashFactor is always (2^(N+1)) - 1. Used for faster hashing.
+   */
+  private int hashFactor;
+
+  /**
+   * This array holds the unique keys
+   */
+  Object[] keys;
+
+  /**
+   * In case of collisions, we implement a double linked list of the colliding
+   * hash's with the following next[] and prev[]. Those are also used to store
+   * the "empty" list.
+   */
+  int[] next;
+
+  private int prev;
+
+  /**
+   * Number of currently objects in the map.
+   */
+  private int size;
+
+  /**
+   * This array holds the values
+   */
+  int[] values;
+
+  /**
+   * Constructs a map with default capacity.
+   */
+  public ObjectToIntMap() {
+    this(defaultCapacity);
+  }
+
+  /**
+   * Constructs a map with given capacity. Capacity is adjusted to a native
+   * power of 2, with minimum of 16.
+   * 
+   * @param capacity
+   *            minimum capacity for the map.
+   */
+  public ObjectToIntMap(int capacity) {
+    this.capacity = 16;
+    // Minimum capacity is 16..
+    while (this.capacity < capacity) {
+      // Multiply by 2 as long as we're still under the requested capacity
+      this.capacity <<= 1;
+    }
+
+    // As mentioned, we use the first index (0) as 'Ground', so we need the
+    // length of the arrays to be one more than the capacity
+    int arrayLength = this.capacity + 1;
+
+    this.values = new int[arrayLength];
+    this.keys = new Object[arrayLength];
+    this.next = new int[arrayLength];
+
+    // Hash entries are twice as big as the capacity.
+    int baseHashSize = this.capacity << 1;
+
+    this.baseHash = new int[baseHashSize];
+
+    // The has factor is 2^M - 1 which is used as an "AND" hashing operator.
+    // {@link #calcBaseHash()}
+    this.hashFactor = baseHashSize - 1;
+
+    this.size = 0;
+
+    clear();
+  }
+
+  /**
+   * Adds a pair to the map. Takes the first empty position from the
+   * empty-linked-list's head - {@link #firstEmpty}.
+   * 
+   * New pairs are always inserted to baseHash, and are followed by the old
+   * colliding pair.
+   * 
+   * @param key
+   *            integer which maps the given Object
+   * @param e
+   *            element which is being mapped using the given key
+   */
+  private void prvt_put(K key, int e) {
+    // Hash entry to which the new pair would be inserted
+    int hashIndex = calcBaseHashIndex(key);
+
+    // 'Allocating' a pair from the "Empty" list.
+    int objectIndex = firstEmpty;
+
+    // Setting data
+    firstEmpty = next[firstEmpty];
+    values[objectIndex] = e;
+    keys[objectIndex] = key;
+
+    // Inserting the new pair as the first node in the specific hash entry
+    next[objectIndex] = baseHash[hashIndex];
+    baseHash[hashIndex] = objectIndex;
+
+    // Announcing a new pair was added!
+    ++size;
+  }
+
+  /**
+   * Calculating the baseHash index using the internal <code>hashFactor</code>.
+   */
+  protected int calcBaseHashIndex(K key) {
+    return key.hashCode() & hashFactor;
+  }
+
+  /**
+   * Empties the map. Generates the "Empty" space list for later allocation.
+   */
+  public void clear() {
+    // Clears the hash entries
+    Arrays.fill(this.baseHash, 0);
+
+    // Set size to zero
+    size = 0;
+
+    values[0] = Integer.MAX_VALUE;
+
+    // Mark all array entries as empty. This is done with
+    // <code>firstEmpty</code> pointing to the first valid index (1 as 0 is
+    // used as 'Ground').
+    firstEmpty = 1;
+
+    // And setting all the <code>next[i]</code> to point at
+    // <code>i+1</code>.
+    for (int i = 1; i < this.capacity;) {
+      next[i] = ++i;
+    }
+
+    // Surly, the last one should point to the 'Ground'.
+    next[this.capacity] = 0;
+  }
+
+  /**
+   * Checks if a given key exists in the map.
+   * 
+   * @param key
+   *            that is checked against the map data.
+   * @return true if the key exists in the map. false otherwise.
+   */
+  public boolean containsKey(K key) {
+    return find(key) != 0;
+  }
+
+  /**
+   * Checks if the given object exists in the map.<br>
+   * This method iterates over the collection, trying to find an equal object.
+   * 
+   * @param o
+   *            object that is checked against the map data.
+   * @return true if the object exists in the map (in .equals() meaning).
+   *         false otherwise.
+   */
+  public boolean containsValue(int o) {
+    for (IntIterator iterator = iterator(); iterator.hasNext();) {
+      if (o == iterator.next()) {
+        return true;
+      }
+    }
+    return false;
+  }
+
+  /**
+   * Find the actual index of a given key.
+   * 
+   * @return index of the key. zero if the key wasn't found.
+   */
+  protected int find(K key) {
+    // Calculate the hash entry.
+    int baseHashIndex = calcBaseHashIndex(key);
+
+    // Start from the hash entry.
+    int localIndex = baseHash[baseHashIndex];
+
+    // while the index does not point to the 'Ground'
+    while (localIndex != 0) {
+      // returns the index found in case of of a matching key.
+      if (keys[localIndex].equals(key)) {
+        return localIndex;
+      }
+
+      // next the local index
+      localIndex = next[localIndex];
+    }
+
+    // If we got this far, it could only mean we did not find the key we
+    // were asked for. return 'Ground' index.
+    return 0;
+  }
+
+  /**
+   * Find the actual index of a given key with it's baseHashIndex.<br>
+   * Some methods use the baseHashIndex. If those call {@link #find} there's
+   * no need to re-calculate that hash.
+   * 
+   * @return the index of the given key, or 0 as 'Ground' if the key wasn't
+   *         found.
+   */
+  private int findForRemove(K key, int baseHashIndex) {
+    // Start from the hash entry.
+    this.prev = 0;
+    int index = baseHash[baseHashIndex];
+
+    // while the index does not point to the 'Ground'
+    while (index != 0) {
+      // returns the index found in case of of a matching key.
+      if (keys[index].equals(key)) {
+        return index;
+      }
+
+      // next the local index
+      prev = index;
+      index = next[index];
+    }
+
+    // If we got this far, it could only mean we did not find the key we
+    // were asked for. return 'Ground' index.
+    this.prev = 0;
+    return 0;
+  }
+
+  /**
+   * Returns the int mapped with the given key.
+   * 
+   * @param key
+   *            int who's mapped object we're interested in.
+   * @return an object mapped by the given key. null if the key wasn't found.
+   */
+  public int get(K key) {
+    return values[find(key)];
+  }
+
+  /**
+   * Grows the map. Allocates a new map of double the capacity, and
+   * fast-insert the old key-value pairs.
+   */
+  @SuppressWarnings("unchecked")
+  protected void grow() {
+    ObjectToIntMap<K> that = new ObjectToIntMap<K>(
+        this.capacity * 2);
+
+    // Iterates fast over the collection. Any valid pair is put into the new
+    // map without checking for duplicates or if there's enough space for
+    // it.
+    for (IndexIterator iterator = new IndexIterator(); iterator.hasNext();) {
+      int index = iterator.next();
+      that.prvt_put((K) this.keys[index], this.values[index]);
+    }
+
+    // Copy that's data into this.
+    this.capacity = that.capacity;
+    this.size = that.size;
+    this.firstEmpty = that.firstEmpty;
+    this.values = that.values;
+    this.keys = that.keys;
+    this.next = that.next;
+    this.baseHash = that.baseHash;
+    this.hashFactor = that.hashFactor;
+  }
+
+  /**
+   * 
+   * @return true if the map is empty. false otherwise.
+   */
+  public boolean isEmpty() {
+    return size == 0;
+  }
+
+  /**
+   * Returns a new iterator for the mapped objects.
+   */
+  public IntIterator iterator() {
+    return new ValueIterator();
+  }
+
+  public Iterator<K> keyIterator() {
+    return new KeyIterator();
+  }
+
+  /**
+   * Prints the baseHash array, used for debug purposes.
+   */
+  @SuppressWarnings("unused")
+  private String getBaseHashAsString() {
+    return Arrays.toString(baseHash);
+  }
+
+  /**
+   * Inserts the &lt;key,value&gt; pair into the map. If the key already exists,
+   * this method updates the mapped value to the given one, returning the old
+   * mapped value.
+   * 
+   * @return the old mapped value, or 0 if the key didn't exist.
+   */
+  public int put(K key, int e) {
+    // Does key exists?
+    int index = find(key);
+
+    // Yes!
+    if (index != 0) {
+      // Set new data and exit.
+      int old = values[index];
+      values[index] = e;
+      return old;
+    }
+
+    // Is there enough room for a new pair?
+    if (size == capacity) {
+      // No? Than grow up!
+      grow();
+    }
+
+    // Now that everything is set, the pair can be just put inside with no
+    // worries.
+    prvt_put(key, e);
+
+    return 0;
+  }
+
+  /**
+   * Removes a &lt;key,value&gt; pair from the map and returns the mapped value,
+   * or 0 if the none existed.
+   * 
+   * @param key used to find the value to remove
+   * @return the removed value or 0 if none existed.
+   */
+  public int remove(K key) {
+    int baseHashIndex = calcBaseHashIndex(key);
+    int index = findForRemove(key, baseHashIndex);
+    if (index != 0) {
+      // If it is the first in the collision list, we should promote its
+      // next colliding element.
+      if (prev == 0) {
+        baseHash[baseHashIndex] = next[index];
+      }
+
+      next[prev] = next[index];
+      next[index] = firstEmpty;
+      firstEmpty = index;
+      --size;
+      return values[index];
+    }
+
+    return 0;
+  }
+
+  /**
+   * @return number of pairs currently in the map
+   */
+  public int size() {
+    return this.size;
+  }
+
+  /**
+   * Translates the mapped pairs' values into an array of Objects
+   * 
+   * @return an object array of all the values currently in the map.
+   */
+  public int[] toArray() {
+    int j = -1;
+    int[] array = new int[size];
+
+    // Iterates over the values, adding them to the array.
+    for (IntIterator iterator = iterator(); iterator.hasNext();) {
+      array[++j] = iterator.next();
+    }
+    return array;
+  }
+
+  /**
+   * Translates the mapped pairs' values into an array of T
+   * 
+   * @param a
+   *            the array into which the elements of the list are to be
+   *            stored, if it is big enough; otherwise, use as much space as it can.
+   * 
+   * @return an array containing the elements of the list
+   * 
+   */
+  public int[] toArray(int[] a) {
+    int j = 0;
+    // Iterates over the values, adding them to the array.
+    for (IntIterator iterator = iterator(); j < a.length
+    && iterator.hasNext(); ++j) {
+      a[j] = iterator.next();
+    }
+    if (j < a.length) {
+      a[j] = Integer.MAX_VALUE;
+    }
+
+    return a;
+  }
+
+  @Override
+  public String toString() {
+    StringBuffer sb = new StringBuffer();
+    sb.append('{');
+    Iterator<K> keyIterator = keyIterator();
+    while (keyIterator.hasNext()) {
+      K key = keyIterator.next();
+      sb.append(key);
+      sb.append('=');
+      sb.append(get(key));
+      if (keyIterator.hasNext()) {
+        sb.append(',');
+        sb.append(' ');
+      }
+    }
+    sb.append('}');
+    return sb.toString();
+  }
+  
+  @Override
+  public int hashCode() {
+    return getClass().hashCode() ^ size();
+  }
+  
+  @SuppressWarnings("unchecked")
+  @Override
+  public boolean equals(Object o) {
+    ObjectToIntMap<K> that = (ObjectToIntMap<K>)o;
+    if (that.size() != this.size()) {
+      return false;
+    }
+    
+    Iterator<K> it = keyIterator();
+    while (it.hasNext()) {
+      K key = it.next();
+      int v1 = this.get(key);
+      int v2 = that.get(key);
+      if (Float.compare(v1, v2) != 0) {
+        return false;
+      }
+    }
+    return true;
+  }
+}
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/collections/package.html b/lucene/facet/src/java/org/apache/lucene/facet/collections/package.html
new file mode 100644
index 0000000..792be18
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/collections/package.html
@@ -0,0 +1,24 @@
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+<html>
+<head>
+<title>Facets Collections</title>
+</head>
+<body>
+Various optimized Collections implementations.
+</body>
+</html>
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/complements/ComplementCountingAggregator.java b/lucene/facet/src/java/org/apache/lucene/facet/complements/ComplementCountingAggregator.java
new file mode 100644
index 0000000..e12dd6f
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/complements/ComplementCountingAggregator.java
@@ -0,0 +1,45 @@
+package org.apache.lucene.facet.complements;
+
+import java.io.IOException;
+
+import org.apache.lucene.facet.search.CountingAggregator;
+import org.apache.lucene.util.IntsRef;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * A {@link CountingAggregator} used during complement counting.
+ * 
+ * @lucene.experimental
+ */
+public class ComplementCountingAggregator extends CountingAggregator {
+
+  public ComplementCountingAggregator(int[] counterArray) {
+    super(counterArray);
+  }
+
+  @Override
+  public void aggregate(int docID, float score, IntsRef ordinals) throws IOException {
+    for (int i = 0; i < ordinals.length; i++) {
+      int ord = ordinals.ints[i];
+      assert counterArray[ord] != 0 : "complement aggregation: count is about to become negative for ordinal " + ord;
+      --counterArray[ord];
+    }
+  }
+
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/complements/TotalFacetCounts.java b/lucene/facet/src/java/org/apache/lucene/facet/complements/TotalFacetCounts.java
new file mode 100644
index 0000000..b177864
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/complements/TotalFacetCounts.java
@@ -0,0 +1,180 @@
+package org.apache.lucene.facet.complements;
+
+import java.io.BufferedInputStream;
+import java.io.BufferedOutputStream;
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.File;
+import java.io.FileInputStream;
+import java.io.FileOutputStream;
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.concurrent.atomic.AtomicInteger;
+
+import org.apache.lucene.facet.params.CategoryListParams;
+import org.apache.lucene.facet.params.FacetIndexingParams;
+import org.apache.lucene.facet.params.FacetSearchParams;
+import org.apache.lucene.facet.search.Aggregator;
+import org.apache.lucene.facet.search.CategoryListIterator;
+import org.apache.lucene.facet.search.CountFacetRequest;
+import org.apache.lucene.facet.search.CountingAggregator;
+import org.apache.lucene.facet.search.FacetArrays;
+import org.apache.lucene.facet.search.FacetRequest;
+import org.apache.lucene.facet.search.StandardFacetsAccumulator;
+import org.apache.lucene.facet.taxonomy.CategoryPath;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.facet.util.PartitionsUtils;
+import org.apache.lucene.facet.util.ScoredDocIdsUtils;
+import org.apache.lucene.index.IndexReader;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Maintain Total Facet Counts per partition, for given parameters:
+ * <ul> 
+ *  <li>Index reader of an index</li>
+ *  <li>Taxonomy index reader</li>
+ *  <li>Facet indexing params (and particularly the category list params)</li>
+ *  <li></li>
+ * </ul>
+ * The total facet counts are maintained as an array of arrays of integers, 
+ * where a separate array is kept for each partition.
+ * 
+ * @lucene.experimental
+ */
+public class TotalFacetCounts {
+  
+  /** total facet counts per partition: totalCounts[partition][ordinal%partitionLength] */
+  private int[][] totalCounts = null;
+  
+  private final TaxonomyReader taxonomy;
+  private final FacetIndexingParams facetIndexingParams;
+
+  private final static AtomicInteger atomicGen4Test = new AtomicInteger(1);
+  /** Creation type for test purposes */
+  enum CreationType { Computed, Loaded } // for testing
+  final int gen4test;
+  final CreationType createType4test;
+  
+  /** 
+   * Construct by key - from index Directory or by recomputing.
+   */
+  private TotalFacetCounts (TaxonomyReader taxonomy, FacetIndexingParams facetIndexingParams,
+      int[][] counts, CreationType createType4Test) {
+    this.taxonomy = taxonomy;
+    this.facetIndexingParams = facetIndexingParams;
+    this.totalCounts = counts;
+    this.createType4test = createType4Test;
+    this.gen4test = atomicGen4Test.incrementAndGet();
+  }
+
+  /**
+   * Fill a partition's array with the TotalCountsArray values.
+   * @param partitionArray array to fill
+   * @param partition number of required partition 
+   */
+  public void fillTotalCountsForPartition(int[] partitionArray, int partition) {
+    int partitionSize = partitionArray.length;
+    int[] countArray = totalCounts[partition];
+    if (countArray == null) {
+      countArray = new int[partitionSize];
+      totalCounts[partition] = countArray;
+    }
+    int length = Math.min(partitionSize, countArray.length);
+    System.arraycopy(countArray, 0, partitionArray, 0, length);
+  }
+  
+  /**
+   * Return the total count of an input category
+   * @param ordinal ordinal of category whose total count is required 
+   */
+  public int getTotalCount(int ordinal) {
+    int partition = PartitionsUtils.partitionNumber(facetIndexingParams,ordinal);
+    int offset = ordinal % PartitionsUtils.partitionSize(facetIndexingParams, taxonomy);
+    return totalCounts[partition][offset];
+  }
+  
+  static TotalFacetCounts loadFromFile(File inputFile, TaxonomyReader taxonomy, 
+      FacetIndexingParams facetIndexingParams) throws IOException {
+    DataInputStream dis = new DataInputStream(new BufferedInputStream(new FileInputStream(inputFile)));
+    try {
+      int[][] counts = new int[dis.readInt()][];
+      for (int i=0; i<counts.length; i++) {
+        int size = dis.readInt();
+        if (size<0) {
+          counts[i] = null;
+        } else {
+          counts[i] = new int[size];
+          for (int j=0; j<size; j++) {
+            counts[i][j] = dis.readInt();
+          }
+        }
+      }
+      return new TotalFacetCounts(taxonomy, facetIndexingParams, counts, CreationType.Loaded);
+    } finally {
+      dis.close();
+    }
+  }
+
+  static void storeToFile(File outputFile, TotalFacetCounts tfc) throws IOException {
+    DataOutputStream dos = new DataOutputStream(new BufferedOutputStream(new FileOutputStream(outputFile)));
+    try {
+      dos.writeInt(tfc.totalCounts.length);
+      for (int[] counts : tfc.totalCounts) {
+        if (counts == null) {
+          dos.writeInt(-1);
+        } else {
+          dos.writeInt(counts.length);
+          for (int i : counts) {
+            dos.writeInt(i);
+          }
+        }
+      }
+    } finally {
+      dos.close();
+    }
+  }
+  
+  // needed because FacetSearchParams do not allow empty FacetRequests
+  private static final FacetRequest DUMMY_REQ = new CountFacetRequest(CategoryPath.EMPTY, 1);
+
+  static TotalFacetCounts compute(final IndexReader indexReader, final TaxonomyReader taxonomy, 
+      final FacetIndexingParams facetIndexingParams) throws IOException {
+    int partitionSize = PartitionsUtils.partitionSize(facetIndexingParams, taxonomy);
+    final int[][] counts = new int[(int) Math.ceil(taxonomy.getSize()  /(float) partitionSize)][partitionSize];
+    FacetSearchParams newSearchParams = new FacetSearchParams(facetIndexingParams, DUMMY_REQ); 
+      //createAllListsSearchParams(facetIndexingParams,  this.totalCounts);
+    StandardFacetsAccumulator sfa = new StandardFacetsAccumulator(newSearchParams, indexReader, taxonomy) {
+      @Override
+      protected HashMap<CategoryListIterator, Aggregator> getCategoryListMap(
+          FacetArrays facetArrays, int partition) throws IOException {
+        
+        Aggregator aggregator = new CountingAggregator(counts[partition]);
+        HashMap<CategoryListIterator, Aggregator> map = new HashMap<CategoryListIterator, Aggregator>();
+        for (CategoryListParams clp: facetIndexingParams.getAllCategoryListParams()) {
+          map.put(clp.createCategoryListIterator(partition), aggregator);
+        }
+        return map;
+      }
+    };
+    sfa.setComplementThreshold(StandardFacetsAccumulator.DISABLE_COMPLEMENT);
+    sfa.accumulate(ScoredDocIdsUtils.createAllDocsScoredDocIDs(indexReader));
+    return new TotalFacetCounts(taxonomy, facetIndexingParams, counts, CreationType.Computed);
+  }
+  
+}
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/complements/TotalFacetCountsCache.java b/lucene/facet/src/java/org/apache/lucene/facet/complements/TotalFacetCountsCache.java
new file mode 100644
index 0000000..6000e8c
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/complements/TotalFacetCountsCache.java
@@ -0,0 +1,299 @@
+package org.apache.lucene.facet.complements;
+
+import java.io.File;
+import java.io.IOException;
+import java.util.Iterator;
+import java.util.LinkedHashMap;
+import java.util.concurrent.ConcurrentHashMap;
+import java.util.concurrent.ConcurrentLinkedQueue;
+
+import org.apache.lucene.facet.params.CategoryListParams;
+import org.apache.lucene.facet.params.FacetIndexingParams;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.index.IndexReader;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Manage an LRU cache for {@link TotalFacetCounts} per index, taxonomy, and
+ * facet indexing params.
+ * 
+ * @lucene.experimental
+ */
+public final class TotalFacetCountsCache {
+  
+  /**
+   * Default size of in memory cache for computed total facet counts.
+   * Set to 2 for the case when an application reopened a reader and 
+   * the original one is still in use (Otherwise there will be 
+   * switching again and again between the two.) 
+   */
+  public static final int DEFAULT_CACHE_SIZE = 2; 
+
+  private static final TotalFacetCountsCache singleton = new TotalFacetCountsCache();
+  
+  /**
+   * Get the single instance of this cache
+   */
+  public static TotalFacetCountsCache getSingleton() {
+    return singleton;
+  }
+  
+  /**
+   * In-memory cache of TFCs.
+   * <ul>  
+   * <li>It's size is kept within limits through {@link #trimCache()}.
+   * <li>An LRU eviction policy is applied, by maintaining active keys in {@link #lruKeys}. 
+   * <li>After each addition to the cache, trimCache is called, to remove entries least recently used.
+   * </ul>  
+   * @see #markRecentlyUsed(TFCKey)
+   */
+  private ConcurrentHashMap<TFCKey,TotalFacetCounts> cache = new ConcurrentHashMap<TFCKey,TotalFacetCounts>();
+  
+  /**
+   * A queue of active keys for applying LRU policy on eviction from the {@link #cache}.
+   * @see #markRecentlyUsed(TFCKey)
+   */
+  private ConcurrentLinkedQueue<TFCKey> lruKeys = new ConcurrentLinkedQueue<TFCKey>();
+  
+  private int maxCacheSize = DEFAULT_CACHE_SIZE; 
+  
+  /** private constructor for singleton pattern */ 
+  private TotalFacetCountsCache() {
+  }
+  
+  /**
+   * Get the total facet counts for a reader/taxonomy pair and facet indexing
+   * parameters. If not in cache, computed here and added to the cache for later
+   * use.
+   * 
+   * @param indexReader
+   *          the documents index
+   * @param taxonomy
+   *          the taxonomy index
+   * @param facetIndexingParams
+   *          facet indexing parameters
+   * @return the total facet counts.
+   */
+  public TotalFacetCounts getTotalCounts(IndexReader indexReader, TaxonomyReader taxonomy,
+      FacetIndexingParams facetIndexingParams) throws IOException {
+    // create the key
+    TFCKey key = new TFCKey(indexReader, taxonomy, facetIndexingParams);
+    // it is important that this call is not synchronized, so that available TFC 
+    // would not wait for one that needs to be computed.  
+    TotalFacetCounts tfc = cache.get(key);
+    if (tfc != null) {
+      markRecentlyUsed(key); 
+      return tfc;
+    }
+    return computeAndCache(key);
+  }
+
+  /**
+   * Mark key as it as recently used.
+   * <p>
+   * <b>Implementation notes: Synchronization considerations and the interaction between lruKeys and cache:</b>
+   * <ol>
+   *  <li>A concurrent {@link LinkedHashMap} would have made this class much simpler.
+   *      But unfortunately, Java does not provide one.
+   *      Instead, we combine two concurrent objects:
+   *  <ul>
+   *   <li>{@link ConcurrentHashMap} for the cached TFCs.
+   *   <li>{@link ConcurrentLinkedQueue} for active keys
+   *  </ul>
+   *  <li>Both {@link #lruKeys} and {@link #cache} are concurrently safe.
+   *  <li>Checks for a cached item through getTotalCounts() are not synchronized.
+   *      Therefore, the case that a needed TFC is in the cache is very fast:
+   *      it does not wait for the computation of other TFCs.
+   *  <li>computeAndCache() is synchronized, and, has a (double) check of the required
+   *       TFC, to avoid computing the same TFC twice. 
+   *  <li>A race condition in this method (markRecentlyUsed) might result in two copies 
+   *      of the same 'key' in lruKeys, but this is handled by the loop in trimCache(), 
+   *      where an attempt to remove the same key twice is a no-op.
+   * </ol>
+   */
+  private void markRecentlyUsed(TFCKey key) {
+    lruKeys.remove(key);  
+    lruKeys.add(key);
+  }
+
+  private synchronized void trimCache() {
+    // loop until cache is of desired  size.
+    while (cache.size()>maxCacheSize ) { 
+      TFCKey key = lruKeys.poll();
+      if (key==null) { //defensive
+        // it is defensive since lruKeys presumably covers the cache keys 
+        key = cache.keys().nextElement(); 
+      }
+      // remove this element. Note that an attempt to remove with the same key again is a no-op,
+      // which gracefully handles the possible race in markRecentlyUsed(). 
+      cache.remove(key);
+    }
+  }
+  
+  /**
+   * compute TFC and cache it, after verifying it was not just added - for this
+   * matter this method is synchronized, which is not too bad, because there is
+   * lots of work done in the computations.
+   */
+  private synchronized TotalFacetCounts computeAndCache(TFCKey key) throws IOException {
+    TotalFacetCounts tfc = cache.get(key); 
+    if (tfc == null) {
+      tfc = TotalFacetCounts.compute(key.indexReader, key.taxonomy, key.facetIndexingParams);
+      lruKeys.add(key);
+      cache.put(key,tfc);
+      trimCache();
+    }
+    return tfc;
+  }
+
+  /**
+   * Load {@link TotalFacetCounts} matching input parameters from the provided
+   * outputFile and add them into the cache for the provided indexReader,
+   * taxonomy, and facetIndexingParams. If a {@link TotalFacetCounts} for these
+   * parameters already exists in the cache, it will be replaced by the loaded
+   * one.
+   * 
+   * @param inputFile
+   *          file from which to read the data
+   * @param indexReader
+   *          the documents index
+   * @param taxonomy
+   *          the taxonomy index
+   * @param facetIndexingParams
+   *          the facet indexing parameters
+   * @throws IOException
+   *           on error
+   */
+  public synchronized void load(File inputFile, IndexReader indexReader, TaxonomyReader taxonomy,
+      FacetIndexingParams facetIndexingParams) throws IOException {
+    if (!inputFile.isFile() || !inputFile.exists() || !inputFile.canRead()) {
+      throw new IllegalArgumentException("Exepecting an existing readable file: "+inputFile);
+    }
+    TFCKey key = new TFCKey(indexReader, taxonomy, facetIndexingParams);
+    TotalFacetCounts tfc = TotalFacetCounts.loadFromFile(inputFile, taxonomy, facetIndexingParams);
+    cache.put(key,tfc);
+    trimCache();
+    markRecentlyUsed(key);
+  }
+  
+  /**
+   * Store the {@link TotalFacetCounts} matching input parameters into the
+   * provided outputFile, making them available for a later call to
+   * {@link #load(File, IndexReader, TaxonomyReader, FacetIndexingParams)}. If
+   * these {@link TotalFacetCounts} are available in the cache, they are used.
+   * But if they are not in the cache, this call will first compute them (which
+   * will also add them to the cache).
+   * 
+   * @param outputFile
+   *          file to store in.
+   * @param indexReader
+   *          the documents index
+   * @param taxonomy
+   *          the taxonomy index
+   * @param facetIndexingParams
+   *          the facet indexing parameters
+   * @throws IOException
+   *           on error
+   * @see #load(File, IndexReader, TaxonomyReader, FacetIndexingParams)
+   */
+  public void store(File outputFile, IndexReader indexReader, TaxonomyReader taxonomy,
+      FacetIndexingParams facetIndexingParams) throws IOException {
+    File parentFile = outputFile.getParentFile();
+    if (
+        ( outputFile.exists() && (!outputFile.isFile()      || !outputFile.canWrite())) ||
+        (!outputFile.exists() && (!parentFile.isDirectory() || !parentFile.canWrite()))
+      ) {
+      throw new IllegalArgumentException("Exepecting a writable file: "+outputFile);
+    }
+    TotalFacetCounts tfc = getTotalCounts(indexReader, taxonomy, facetIndexingParams);
+    TotalFacetCounts.storeToFile(outputFile, tfc);  
+  }
+  
+  private static class TFCKey {
+    final IndexReader indexReader;
+    final TaxonomyReader taxonomy;
+    private final Iterable<CategoryListParams> clps;
+    private final int hashCode;
+    private final int nDels; // needed when a reader used for faceted search was just used for deletion. 
+    final FacetIndexingParams facetIndexingParams;
+    
+    public TFCKey(IndexReader indexReader, TaxonomyReader taxonomy,
+        FacetIndexingParams facetIndexingParams) {
+      this.indexReader = indexReader;
+      this.taxonomy = taxonomy;
+      this.facetIndexingParams = facetIndexingParams;
+      this.clps = facetIndexingParams.getAllCategoryListParams();
+      this.nDels = indexReader.numDeletedDocs();
+      hashCode = indexReader.hashCode() ^ taxonomy.hashCode();
+    }
+    
+    @Override
+    public int hashCode() {
+      return hashCode;
+    }
+    
+    @Override
+    public boolean equals(Object other) {
+      TFCKey o = (TFCKey) other; 
+      if (indexReader != o.indexReader || taxonomy != o.taxonomy || nDels != o.nDels) {
+        return false;
+      }
+      Iterator<CategoryListParams> it1 = clps.iterator();
+      Iterator<CategoryListParams> it2 = o.clps.iterator();
+      while (it1.hasNext() && it2.hasNext()) {
+        if (!it1.next().equals(it2.next())) {
+          return false;
+        }
+      }
+      return it1.hasNext() == it2.hasNext();
+    }
+  }
+
+  /**
+   * Clear the cache.
+   */
+  public synchronized void clear() {
+    cache.clear();
+    lruKeys.clear();
+  }
+  
+  /**
+   * @return the maximal cache size
+   */
+  public int getCacheSize() {
+    return maxCacheSize;
+  }
+
+  /**
+   * Set the number of TotalFacetCounts arrays that will remain in memory cache.
+   * <p>
+   * If new size is smaller than current size, the cache is appropriately trimmed.
+   * <p>
+   * Minimal size is 1, so passing zero or negative size would result in size of 1.
+   * @param size new size to set
+   */
+  public void setCacheSize(int size) {
+    if (size < 1) size = 1;
+    int origSize = maxCacheSize;
+    maxCacheSize = size;
+    if (maxCacheSize < origSize) { // need to trim only if the cache was reduced
+      trimCache();
+    }
+  }
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/complements/package.html b/lucene/facet/src/java/org/apache/lucene/facet/complements/package.html
new file mode 100644
index 0000000..beca697
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/complements/package.html
@@ -0,0 +1,27 @@
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+<html>
+<head>
+<title>Facets Complements counting</title>
+</head>
+<body>
+Allows to cache the total counts of categories, so that during search which 
+returns a large number of results (>60% of segment size), the complement set 
+of matching documents is counted. Useful for queries that visit a large 
+number of documents, e.g. overview queries.
+</body>
+</html>
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/encoding/ChunksIntEncoder.java b/lucene/facet/src/java/org/apache/lucene/facet/encoding/ChunksIntEncoder.java
new file mode 100644
index 0000000..464f4e9
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/encoding/ChunksIntEncoder.java
@@ -0,0 +1,115 @@
+package org.apache.lucene.facet.encoding;
+
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IntsRef;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * An {@link IntEncoder} which encodes values in chunks. Implementations of this
+ * class assume the data which needs encoding consists of small, consecutive
+ * values, and therefore the encoder is able to compress them better. You can
+ * read more on the two implementations {@link FourFlagsIntEncoder} and
+ * {@link EightFlagsIntEncoder}.
+ * <p>
+ * Extensions of this class need to implement {@link #encode(IntsRef, BytesRef)}
+ * in order to build the proper indicator (flags). When enough values were
+ * accumulated (typically the batch size), extensions can call
+ * {@link #encodeChunk(BytesRef)} to flush the indicator and the rest of the
+ * values.
+ * <p>
+ * <b>NOTE:</b> flags encoders do not accept values &le; 0 (zero) in their
+ * {@link #encode(IntsRef, BytesRef)}. For performance reasons they do not check
+ * that condition, however if such value is passed the result stream may be
+ * corrupt or an exception will be thrown. Also, these encoders perform the best
+ * when there are many consecutive small values (depends on the encoder
+ * implementation). If that is not the case, the encoder will occupy 1 more byte
+ * for every <i>batch</i> number of integers, over whatever
+ * {@link VInt8IntEncoder} would have occupied. Therefore make sure to check
+ * whether your data fits into the conditions of the specific encoder.
+ * <p>
+ * For the reasons mentioned above, these encoders are usually chained with
+ * {@link UniqueValuesIntEncoder} and {@link DGapIntEncoder}.
+ * 
+ * @lucene.experimental
+ */
+public abstract class ChunksIntEncoder extends IntEncoder {
+  
+  /** Holds the values which must be encoded, outside the indicator. */
+  protected final IntsRef encodeQueue;
+  
+  /** Represents bits flag byte. */
+  protected int indicator = 0;
+  
+  /** Counts the current ordinal of the encoded value. */
+  protected byte ordinal = 0;
+  
+  protected ChunksIntEncoder(int chunkSize) {
+    encodeQueue = new IntsRef(chunkSize);
+  }
+  
+  /**
+   * Encodes the values of the current chunk. First it writes the indicator, and
+   * then it encodes the values outside the indicator.
+   */
+  protected void encodeChunk(BytesRef buf) {
+    // ensure there's enough room in the buffer
+    int maxBytesRequired = buf.length + 1 + encodeQueue.length * 4; /* indicator + at most 4 bytes per positive VInt */
+    if (buf.bytes.length < maxBytesRequired) {
+      buf.grow(maxBytesRequired);
+    }
+    
+    buf.bytes[buf.length++] = ((byte) indicator);
+    for (int i = 0; i < encodeQueue.length; i++) {
+      // it is better if the encoding is inlined like so, and not e.g.
+      // in a utility method
+      int value = encodeQueue.ints[i];
+      if ((value & ~0x7F) == 0) {
+        buf.bytes[buf.length] = (byte) value;
+        buf.length++;
+      } else if ((value & ~0x3FFF) == 0) {
+        buf.bytes[buf.length] = (byte) (0x80 | ((value & 0x3F80) >> 7));
+        buf.bytes[buf.length + 1] = (byte) (value & 0x7F);
+        buf.length += 2;
+      } else if ((value & ~0x1FFFFF) == 0) {
+        buf.bytes[buf.length] = (byte) (0x80 | ((value & 0x1FC000) >> 14));
+        buf.bytes[buf.length + 1] = (byte) (0x80 | ((value & 0x3F80) >> 7));
+        buf.bytes[buf.length + 2] = (byte) (value & 0x7F);
+        buf.length += 3;
+      } else if ((value & ~0xFFFFFFF) == 0) {
+        buf.bytes[buf.length] = (byte) (0x80 | ((value & 0xFE00000) >> 21));
+        buf.bytes[buf.length + 1] = (byte) (0x80 | ((value & 0x1FC000) >> 14));
+        buf.bytes[buf.length + 2] = (byte) (0x80 | ((value & 0x3F80) >> 7));
+        buf.bytes[buf.length + 3] = (byte) (value & 0x7F);
+        buf.length += 4;
+      } else {
+        buf.bytes[buf.length] = (byte) (0x80 | ((value & 0xF0000000) >> 28));
+        buf.bytes[buf.length + 1] = (byte) (0x80 | ((value & 0xFE00000) >> 21));
+        buf.bytes[buf.length + 2] = (byte) (0x80 | ((value & 0x1FC000) >> 14));
+        buf.bytes[buf.length + 3] = (byte) (0x80 | ((value & 0x3F80) >> 7));
+        buf.bytes[buf.length + 4] = (byte) (value & 0x7F);
+        buf.length += 5;
+      }
+    }
+    
+    ordinal = 0;
+    indicator = 0;
+    encodeQueue.length = 0;
+  }
+  
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/encoding/DGapIntDecoder.java b/lucene/facet/src/java/org/apache/lucene/facet/encoding/DGapIntDecoder.java
new file mode 100644
index 0000000..5537663
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/encoding/DGapIntDecoder.java
@@ -0,0 +1,52 @@
+package org.apache.lucene.facet.encoding;
+
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IntsRef;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * An {@link IntDecoder} which wraps another decoder and reverts the d-gap that
+ * was encoded by {@link DGapIntEncoder}.
+ * 
+ * @lucene.experimental
+ */
+public final class DGapIntDecoder extends IntDecoder {
+
+  private final IntDecoder decoder;
+
+  public DGapIntDecoder(IntDecoder decoder) {
+    this.decoder = decoder;
+  }
+
+  @Override
+  public void decode(BytesRef buf, IntsRef values) {
+    decoder.decode(buf, values);
+    int prev = 0;
+    for (int i = 0; i < values.length; i++) {
+      values.ints[i] += prev;
+      prev = values.ints[i];
+    }
+  }
+
+  @Override
+  public String toString() {
+    return "DGap(" + decoder.toString() + ")";
+  }
+
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/encoding/DGapIntEncoder.java b/lucene/facet/src/java/org/apache/lucene/facet/encoding/DGapIntEncoder.java
new file mode 100644
index 0000000..0aaf5d3
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/encoding/DGapIntEncoder.java
@@ -0,0 +1,67 @@
+package org.apache.lucene.facet.encoding;
+
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IntsRef;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * An {@link IntEncoderFilter} which encodes the gap between the given values,
+ * rather than the values themselves. This encoder usually yields better
+ * encoding performance space-wise (i.e., the final encoded values consume less
+ * space) if the values are 'close' to each other.
+ * <p>
+ * <b>NOTE:</b> this encoder assumes the values are given to
+ * {@link #encode(IntsRef, BytesRef)} in an ascending sorted manner, which ensures only
+ * positive values are encoded and thus yields better performance. If you are
+ * not sure whether the values are sorted or not, it is possible to chain this
+ * encoder with {@link SortingIntEncoder} to ensure the values will be
+ * sorted before encoding.
+ * 
+ * @lucene.experimental
+ */
+public final class DGapIntEncoder extends IntEncoderFilter {
+
+  /** Initializes with the given encoder. */
+  public DGapIntEncoder(IntEncoder encoder) {
+    super(encoder);
+  }
+
+  @Override
+  public void encode(IntsRef values, BytesRef buf) {
+    int prev = 0;
+    int upto = values.offset + values.length;
+    for (int i = values.offset; i < upto; i++) {
+      int tmp = values.ints[i];
+      values.ints[i] -= prev;
+      prev = tmp;
+    }
+    encoder.encode(values, buf);
+  }
+
+  @Override
+  public IntDecoder createMatchingDecoder() {
+    return new DGapIntDecoder(encoder.createMatchingDecoder());
+  }
+  
+  @Override
+  public String toString() {
+    return "DGap(" + encoder.toString() + ")";
+  }
+  
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/encoding/DGapVInt8IntDecoder.java b/lucene/facet/src/java/org/apache/lucene/facet/encoding/DGapVInt8IntDecoder.java
new file mode 100644
index 0000000..c72c118
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/encoding/DGapVInt8IntDecoder.java
@@ -0,0 +1,67 @@
+package org.apache.lucene.facet.encoding;
+
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IntsRef;
+import org.apache.lucene.util.RamUsageEstimator;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Decodes values encoded by {@link DGapVInt8IntDecoder}.
+ * 
+ * @lucene.experimental
+ */
+public final class DGapVInt8IntDecoder extends IntDecoder {
+
+  @Override
+  public void decode(BytesRef buf, IntsRef values) {
+    values.offset = values.length = 0;
+
+    // grow the buffer up front, even if by a large number of values (buf.length)
+    // that saves the need to check inside the loop for every decoded value if
+    // the buffer needs to grow.
+    if (values.ints.length < buf.length) {
+      values.ints = new int[ArrayUtil.oversize(buf.length, RamUsageEstimator.NUM_BYTES_INT)];
+    }
+
+    // it is better if the decoding is inlined like so, and not e.g.
+    // in a utility method
+    int upto = buf.offset + buf.length;
+    int value = 0;
+    int offset = buf.offset;
+    int prev = 0;
+    while (offset < upto) {
+      byte b = buf.bytes[offset++];
+      if (b >= 0) {
+        values.ints[values.length] = ((value << 7) | b) + prev;
+        value = 0;
+        prev = values.ints[values.length];
+        values.length++;
+      } else {
+        value = (value << 7) | (b & 0x7F);
+      }
+    }
+  }
+
+  @Override
+  public String toString() {
+    return "DGapVInt8";
+  }
+
+} 
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/encoding/DGapVInt8IntEncoder.java b/lucene/facet/src/java/org/apache/lucene/facet/encoding/DGapVInt8IntEncoder.java
new file mode 100644
index 0000000..bc06383
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/encoding/DGapVInt8IntEncoder.java
@@ -0,0 +1,89 @@
+package org.apache.lucene.facet.encoding;
+
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IntsRef;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * An {@link IntEncoder} which implements variable length encoding for the gap
+ * between values. It's a specialized form of the combination of
+ * {@link DGapIntEncoder} and {@link VInt8IntEncoder}.
+ * 
+ * @see VInt8IntEncoder
+ * @see DGapIntEncoder
+ * 
+ * @lucene.experimental
+ */
+public final class DGapVInt8IntEncoder extends IntEncoder {
+
+  @Override
+  public void encode(IntsRef values, BytesRef buf) {
+    buf.offset = buf.length = 0;
+    int maxBytesNeeded = 5 * values.length; // at most 5 bytes per VInt
+    if (buf.bytes.length < maxBytesNeeded) {
+      buf.grow(maxBytesNeeded);
+    }
+    
+    int upto = values.offset + values.length;
+    int prev = 0;
+    for (int i = values.offset; i < upto; i++) {
+      // it is better if the encoding is inlined like so, and not e.g.
+      // in a utility method
+      int value = values.ints[i] - prev;
+      if ((value & ~0x7F) == 0) {
+        buf.bytes[buf.length] = (byte) value;
+        buf.length++;
+      } else if ((value & ~0x3FFF) == 0) {
+        buf.bytes[buf.length] = (byte) (0x80 | ((value & 0x3F80) >> 7));
+        buf.bytes[buf.length + 1] = (byte) (value & 0x7F);
+        buf.length += 2;
+      } else if ((value & ~0x1FFFFF) == 0) {
+        buf.bytes[buf.length] = (byte) (0x80 | ((value & 0x1FC000) >> 14));
+        buf.bytes[buf.length + 1] = (byte) (0x80 | ((value & 0x3F80) >> 7));
+        buf.bytes[buf.length + 2] = (byte) (value & 0x7F);
+        buf.length += 3;
+      } else if ((value & ~0xFFFFFFF) == 0) {
+        buf.bytes[buf.length] = (byte) (0x80 | ((value & 0xFE00000) >> 21));
+        buf.bytes[buf.length + 1] = (byte) (0x80 | ((value & 0x1FC000) >> 14));
+        buf.bytes[buf.length + 2] = (byte) (0x80 | ((value & 0x3F80) >> 7));
+        buf.bytes[buf.length + 3] = (byte) (value & 0x7F);
+        buf.length += 4;
+      } else {
+        buf.bytes[buf.length] = (byte) (0x80 | ((value & 0xF0000000) >> 28));
+        buf.bytes[buf.length + 1] = (byte) (0x80 | ((value & 0xFE00000) >> 21));
+        buf.bytes[buf.length + 2] = (byte) (0x80 | ((value & 0x1FC000) >> 14));
+        buf.bytes[buf.length + 3] = (byte) (0x80 | ((value & 0x3F80) >> 7));
+        buf.bytes[buf.length + 4] = (byte) (value & 0x7F);
+        buf.length += 5;
+      }
+      prev = values.ints[i];
+    }
+  }
+
+  @Override
+  public IntDecoder createMatchingDecoder() {
+    return new DGapVInt8IntDecoder();
+  }
+
+  @Override
+  public String toString() {
+    return "DGapVInt8";
+  }
+
+} 
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/encoding/EightFlagsIntDecoder.java b/lucene/facet/src/java/org/apache/lucene/facet/encoding/EightFlagsIntDecoder.java
new file mode 100644
index 0000000..3bf12ef
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/encoding/EightFlagsIntDecoder.java
@@ -0,0 +1,92 @@
+package org.apache.lucene.facet.encoding;
+
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IntsRef;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Decodes values encoded with {@link EightFlagsIntEncoder}.
+ * 
+ * @lucene.experimental
+ */
+public class EightFlagsIntDecoder extends IntDecoder {
+
+  /*
+   * Holds all combinations of <i>indicator</i> for fast decoding (saves time
+   * on real-time bit manipulation)
+   */
+  private static final byte[][] DECODE_TABLE = new byte[256][8];
+
+  /** Generating all combinations of <i>indicator</i> into separate flags. */
+  static {
+    for (int i = 256; i != 0;) {
+      --i;
+      for (int j = 8; j != 0;) {
+        --j;
+        DECODE_TABLE[i][j] = (byte) ((i >>> j) & 0x1);
+      }
+    }
+  }
+
+  @Override
+  public void decode(BytesRef buf, IntsRef values) {
+    values.offset = values.length = 0;
+    int upto = buf.offset + buf.length;
+    int offset = buf.offset;
+    while (offset < upto) {
+      // read indicator
+      int indicator = buf.bytes[offset++] & 0xFF;
+      int ordinal = 0;
+
+      int capacityNeeded = values.length + 8;
+      if (values.ints.length < capacityNeeded) {
+        values.grow(capacityNeeded);
+      }
+
+      // process indicator, until we read 8 values, or end-of-buffer
+      while (ordinal != 8) {
+        if (DECODE_TABLE[indicator][ordinal++] == 0) {
+          if (offset == upto) { // end of buffer
+            return;
+          }
+          // it is better if the decoding is inlined like so, and not e.g.
+          // in a utility method
+          int value = 0;
+          while (true) {
+            byte b = buf.bytes[offset++];
+            if (b >= 0) {
+              values.ints[values.length++] = ((value << 7) | b) + 2;
+              break;
+            } else {
+              value = (value << 7) | (b & 0x7F);
+            }
+          }
+        } else {
+          values.ints[values.length++] = 1;
+        }
+      }
+    }
+  }
+
+  @Override
+  public String toString() {
+    return "EightFlags(VInt8)";
+  }
+
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/encoding/EightFlagsIntEncoder.java b/lucene/facet/src/java/org/apache/lucene/facet/encoding/EightFlagsIntEncoder.java
new file mode 100644
index 0000000..5b1fec8
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/encoding/EightFlagsIntEncoder.java
@@ -0,0 +1,96 @@
+package org.apache.lucene.facet.encoding;
+
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IntsRef;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * A {@link ChunksIntEncoder} which encodes data in chunks of 8. Every group
+ * starts with a single byte (called indicator) which represents 8 - 1 bit
+ * flags, where the value:
+ * <ul>
+ * <li>1 means the encoded value is '1'
+ * <li>0 means the value is encoded using {@link VInt8IntEncoder}, and the
+ * encoded bytes follow the indicator.<br>
+ * Since value 0 is illegal, and 1 is encoded in the indicator, the actual value
+ * that is encoded is <code>value-2</code>, which saves some more bits.
+ * </ul>
+ * Encoding example:
+ * <ul>
+ * <li>Original values: 6, 16, 5, 9, 7, 1
+ * <li>After sorting: 1, 5, 6, 7, 9, 16
+ * <li>D-Gap computing: 1, 4, 1, 1, 2, 5 (so far - done by
+ * {@link DGapIntEncoder})
+ * <li>Encoding: 1,0,1,1,0,0,0,0 as the indicator, by 2 (4-2), 0 (2-2), 3 (5-2).
+ * <li>Binary encode: <u>0 | 0 | 0 | 0 | 1 | 1 | 0 | 1</u> 00000010 00000000
+ * 00000011 (indicator is <u>underlined</u>).<br>
+ * <b>NOTE:</b> the order of the values in the indicator is lsb &rArr; msb,
+ * which allows for more efficient decoding.
+ * </ul>
+ * 
+ * @lucene.experimental
+ */
+public class EightFlagsIntEncoder extends ChunksIntEncoder {
+
+  /*
+   * Holds all combinations of <i>indicator</i> flags for fast encoding (saves
+   * time on bit manipulation at encode time)
+   */
+  private static final byte[] ENCODE_TABLE = new byte[] { 0x1, 0x2, 0x4, 0x8, 0x10, 0x20, 0x40, (byte) 0x80 };
+
+  public EightFlagsIntEncoder() {
+    super(8);
+  }
+
+  @Override
+  public void encode(IntsRef values, BytesRef buf) {
+    buf.offset = buf.length = 0;
+    int upto = values.offset + values.length;
+    for (int i = values.offset; i < upto; i++) {
+      int value = values.ints[i];
+      if (value == 1) {
+        indicator |= ENCODE_TABLE[ordinal];
+      } else {
+        encodeQueue.ints[encodeQueue.length++] = value - 2;
+      }
+      ++ordinal;
+      
+      // encode the chunk and the indicator
+      if (ordinal == 8) {
+        encodeChunk(buf);
+      }
+    }
+    
+    // encode remaining values
+    if (ordinal != 0) {
+      encodeChunk(buf);
+    }
+  }
+
+  @Override
+  public IntDecoder createMatchingDecoder() {
+    return new EightFlagsIntDecoder();
+  }
+
+  @Override
+  public String toString() {
+    return "EightFlags(VInt)";
+  }
+
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/encoding/FourFlagsIntDecoder.java b/lucene/facet/src/java/org/apache/lucene/facet/encoding/FourFlagsIntDecoder.java
new file mode 100644
index 0000000..b2cb5d1
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/encoding/FourFlagsIntDecoder.java
@@ -0,0 +1,92 @@
+package org.apache.lucene.facet.encoding;
+
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IntsRef;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Decodes values encoded with {@link FourFlagsIntEncoder}.
+ * 
+ * @lucene.experimental
+ */
+public class FourFlagsIntDecoder extends IntDecoder {
+
+  /**
+   * Holds all combinations of <i>indicator</i> for fast decoding (saves time
+   * on real-time bit manipulation)
+   */
+  private final static byte[][] DECODE_TABLE = new byte[256][4];
+
+  /** Generating all combinations of <i>indicator</i> into separate flags. */
+  static {
+    for (int i = 256; i != 0;) {
+      --i;
+      for (int j = 4; j != 0;) {
+        --j;
+        DECODE_TABLE[i][j] = (byte) ((i >>> (j << 1)) & 0x3);
+      }
+    }
+  }
+
+  @Override
+  public void decode(BytesRef buf, IntsRef values) {
+    values.offset = values.length = 0;
+    int upto = buf.offset + buf.length;
+    int offset = buf.offset;
+    while (offset < upto) {
+      // read indicator
+      int indicator = buf.bytes[offset++] & 0xFF;
+      int ordinal = 0;
+      
+      int capacityNeeded = values.length + 4;
+      if (values.ints.length < capacityNeeded) {
+        values.grow(capacityNeeded);
+      }
+      
+      while (ordinal != 4) {
+        byte decodeVal = DECODE_TABLE[indicator][ordinal++];
+        if (decodeVal == 0) {
+          if (offset == upto) { // end of buffer
+            return;
+          }
+          // it is better if the decoding is inlined like so, and not e.g.
+          // in a utility method
+          int value = 0;
+          while (true) {
+            byte b = buf.bytes[offset++];
+            if (b >= 0) {
+              values.ints[values.length++] = ((value << 7) | b) + 4;
+              break;
+            } else {
+              value = (value << 7) | (b & 0x7F);
+            }
+          }
+        } else {
+          values.ints[values.length++] = decodeVal;
+        }
+      }
+    }
+  }
+
+  @Override
+  public String toString() {
+    return "FourFlags(VInt)";
+  }
+
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/encoding/FourFlagsIntEncoder.java b/lucene/facet/src/java/org/apache/lucene/facet/encoding/FourFlagsIntEncoder.java
new file mode 100644
index 0000000..edf448e
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/encoding/FourFlagsIntEncoder.java
@@ -0,0 +1,102 @@
+package org.apache.lucene.facet.encoding;
+
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IntsRef;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * A {@link ChunksIntEncoder} which encodes values in chunks of 4. Every group
+ * starts with a single byte (called indicator) which represents 4 - 2 bit
+ * flags, where the values:
+ * <ul>
+ * <li>1, 2 or 3 mean the encoded value is '1', '2' or '3' respectively.
+ * <li>0 means the value is encoded using {@link VInt8IntEncoder}, and the
+ * encoded bytes follow the indicator.<br>
+ * Since value 0 is illegal, and 1-3 are encoded in the indicator, the actual
+ * value that is encoded is <code>value-4</code>, which saves some more bits.
+ * </ul>
+ * Encoding example:
+ * <ul>
+ * <li>Original values: 6, 16, 5, 9, 7, 1, 11
+ * <li>After sorting: 1, 5, 6, 7, 9, 11, 16
+ * <li>D-Gap computing: 1, 4, 1, 1, 2, 5 (so far - done by
+ * {@link DGapIntEncoder})
+ * <li>Encoding: 1,0,1,1 as the first indicator, followed by 0 (4-4), than
+ * 2,0,0,0 as the second indicator, followed by 1 (5-4) encoded with.
+ * <li>Binary encode: <u>01 | 01 | 00 | 01</u> 00000000 <u>00 | 00 | 00 | 10</u>
+ * 00000001 (indicators are <u>underlined</u>).<br>
+ * <b>NOTE:</b> the order of the values in the indicator is lsb &rArr; msb,
+ * which allows for more efficient decoding.
+ * </ul>
+ * 
+ * @lucene.experimental
+ */
+public class FourFlagsIntEncoder extends ChunksIntEncoder {
+
+  /*
+   * Holds all combinations of <i>indicator</i> flags for fast encoding (saves
+   * time on bit manipulation @ encode time)
+   */
+  private static final byte[][] ENCODE_TABLE = new byte[][] {
+    new byte[] { 0x00, 0x00, 0x00, 0x00 },
+    new byte[] { 0x01, 0x04, 0x10, 0x40 },
+    new byte[] { 0x02, 0x08, 0x20, (byte) 0x80 },
+    new byte[] { 0x03, 0x0C, 0x30, (byte) 0xC0 },
+  };
+
+  public FourFlagsIntEncoder() {
+    super(4);
+  }
+
+  @Override
+  public void encode(IntsRef values, BytesRef buf) {
+    buf.offset = buf.length = 0;
+    int upto = values.offset + values.length;
+    for (int i = values.offset; i < upto; i++) {
+      int value = values.ints[i];
+      if (value <= 3) {
+        indicator |= ENCODE_TABLE[value][ordinal];
+      } else {
+        encodeQueue.ints[encodeQueue.length++] = value - 4;
+      }
+      ++ordinal;
+      
+      // encode the chunk and the indicator
+      if (ordinal == 4) {
+        encodeChunk(buf);
+      }
+    }
+    
+    // encode remaining values
+    if (ordinal != 0) {
+      encodeChunk(buf);
+    }
+  }
+
+  @Override
+  public IntDecoder createMatchingDecoder() {
+    return new FourFlagsIntDecoder();
+  }
+
+  @Override
+  public String toString() {
+    return "FourFlags(VInt)";
+  }
+
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/encoding/IntDecoder.java b/lucene/facet/src/java/org/apache/lucene/facet/encoding/IntDecoder.java
new file mode 100644
index 0000000..954c84d
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/encoding/IntDecoder.java
@@ -0,0 +1,36 @@
+package org.apache.lucene.facet.encoding;
+
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IntsRef;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Decodes integers from a set {@link BytesRef}.
+ * 
+ * @lucene.experimental
+ */
+public abstract class IntDecoder {
+  
+  /**
+   * Decodes the values from the buffer into the given {@link IntsRef}. Note
+   * that {@code values.offset} and {@code values.length} are set to 0.
+   */
+  public abstract void decode(BytesRef buf, IntsRef values);
+
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/encoding/IntEncoder.java b/lucene/facet/src/java/org/apache/lucene/facet/encoding/IntEncoder.java
new file mode 100644
index 0000000..5d3e74d
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/encoding/IntEncoder.java
@@ -0,0 +1,46 @@
+package org.apache.lucene.facet.encoding;
+
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IntsRef;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Encodes integers to a set {@link BytesRef}. For convenience, each encoder
+ * implements {@link #createMatchingDecoder()} for easy access to the matching
+ * decoder.
+ * 
+ * @lucene.experimental
+ */
+public abstract class IntEncoder {
+
+  public IntEncoder() {}
+
+  /**
+   * Encodes the values to the given buffer. Note that the buffer's offset and
+   * length are set to 0.
+   */
+  public abstract void encode(IntsRef values, BytesRef buf);
+
+  /**
+   * Returns an {@link IntDecoder} which can decode the values that were encoded
+   * with this encoder.
+   */
+  public abstract IntDecoder createMatchingDecoder();
+  
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/encoding/IntEncoderFilter.java b/lucene/facet/src/java/org/apache/lucene/facet/encoding/IntEncoderFilter.java
new file mode 100644
index 0000000..3c1f24a
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/encoding/IntEncoderFilter.java
@@ -0,0 +1,34 @@
+package org.apache.lucene.facet.encoding;
+
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * An abstract implementation of {@link IntEncoder} which wraps another encoder.
+ * 
+ * @lucene.experimental
+ */
+public abstract class IntEncoderFilter extends IntEncoder {
+
+  protected final IntEncoder encoder;
+
+  protected IntEncoderFilter(IntEncoder encoder) {
+    this.encoder = encoder;
+  }
+
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/encoding/NOnesIntDecoder.java b/lucene/facet/src/java/org/apache/lucene/facet/encoding/NOnesIntDecoder.java
new file mode 100644
index 0000000..5819b62
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/encoding/NOnesIntDecoder.java
@@ -0,0 +1,86 @@
+package org.apache.lucene.facet.encoding;
+
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IntsRef;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Decodes values encoded encoded with {@link NOnesIntEncoder}.
+ * 
+ * @lucene.experimental
+ */
+public class NOnesIntDecoder extends FourFlagsIntDecoder {
+
+  // Number of consecutive '1's to generate upon decoding a '2'
+  private final int n;
+  private final IntsRef internalBuffer;
+  
+  /**
+   * Constructs a decoder with a given N (Number of consecutive '1's which are
+   * translated into a single target value '2'.
+   */
+  public NOnesIntDecoder(int n) {
+    this.n = n;
+    // initial size (room for 100 integers)
+    internalBuffer = new IntsRef(100);
+  }
+
+  @Override
+  public void decode(BytesRef buf, IntsRef values) {
+    values.offset = values.length = 0;
+    internalBuffer.length = 0;
+    super.decode(buf, internalBuffer);
+    if (values.ints.length < internalBuffer.length) {
+      // need space for internalBuffer.length to internalBuffer.length*N,
+      // grow mildly at first
+      values.grow(internalBuffer.length * n/2);
+    }
+    
+    for (int i = 0; i < internalBuffer.length; i++) {
+      int decode = internalBuffer.ints[i];
+      if (decode == 1) {
+        if (values.length == values.ints.length) {
+          values.grow(values.length + 10); // grow by few items, however not too many
+        }
+        // 1 is 1
+        values.ints[values.length++] = 1;
+      } else if (decode == 2) {
+        if (values.length + n >= values.ints.length) {
+          values.grow(values.length + n); // grow by few items, however not too many
+        }
+        // '2' means N 1's
+        for (int j = 0; j < n; j++) {
+          values.ints[values.length++] = 1;
+        }
+      } else {
+        if (values.length == values.ints.length) {
+          values.grow(values.length + 10); // grow by few items, however not too many
+        }
+        // any other value is val-1
+        values.ints[values.length++] = decode - 1;
+      }
+    }
+  }
+
+  @Override
+  public String toString() {
+    return "NOnes(" + n + ") (" + super.toString() + ")";
+  }
+
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/encoding/NOnesIntEncoder.java b/lucene/facet/src/java/org/apache/lucene/facet/encoding/NOnesIntEncoder.java
new file mode 100644
index 0000000..6c117de
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/encoding/NOnesIntEncoder.java
@@ -0,0 +1,114 @@
+package org.apache.lucene.facet.encoding;
+
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IntsRef;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * A variation of {@link FourFlagsIntEncoder} which translates the data as
+ * follows:
+ * <ul>
+ * <li>Values &ge; 2 are trnalsated to <code>value+1</code> (2 &rArr; 3, 3
+ * &rArr; 4 and so forth).
+ * <li>Any <code>N</code> occurrences of 1 are encoded as a single 2.
+ * <li>Otherwise, each 1 is encoded as 1.
+ * </ul>
+ * <p>
+ * Encoding examples:
+ * <ul>
+ * <li>N = 4: the data 1,1,1,1,1 is translated to: 2, 1
+ * <li>N = 3: the data 1,2,3,4,1,1,1,1,5 is translated to 1,3,4,5,2,1,6
+ * </ul>
+ * <b>NOTE:</b> this encoder does not support values &le; 0 and
+ * {@link Integer#MAX_VALUE}. 0 is not supported because it's not supported by
+ * {@link FourFlagsIntEncoder} and {@link Integer#MAX_VALUE} because this
+ * encoder translates N to N+1, which will cause an overflow and
+ * {@link Integer#MAX_VALUE} will become a negative number, which is not
+ * supported as well.<br>
+ * This does not mean you cannot encode {@link Integer#MAX_VALUE}. If it is not
+ * the first value to encode, and you wrap this encoder with
+ * {@link DGapIntEncoder}, then the value that will be sent to this encoder will
+ * be <code>MAX_VAL - prev</code>.
+ * 
+ * @lucene.experimental
+ */
+public class NOnesIntEncoder extends FourFlagsIntEncoder {
+
+  private final IntsRef internalBuffer;
+  
+  /** Number of consecutive '1's to be translated into single target value '2'. */
+  private final int n;
+
+  /**
+   * Constructs an encoder with a given value of N (N: Number of consecutive
+   * '1's to be translated into single target value '2').
+   */
+  public NOnesIntEncoder(int n) {
+    this.n = n;
+    internalBuffer = new IntsRef(n);
+  }
+
+  @Override
+  public void encode(IntsRef values, BytesRef buf) {
+    internalBuffer.length = 0;
+    // make sure the internal buffer is large enough
+    if (values.length > internalBuffer.ints.length) {
+      internalBuffer.grow(values.length);
+    }
+    
+    int onesCounter = 0;
+    int upto = values.offset + values.length;
+    for (int i = values.offset; i < upto; i++) {
+      int value = values.ints[i];
+      if (value == 1) {
+        // every N 1's should be encoded as '2'
+        if (++onesCounter == n) {
+          internalBuffer.ints[internalBuffer.length++] = 2;
+          onesCounter = 0;
+        }
+      } else {
+        // there might have been 1's that we need to encode
+        while (onesCounter > 0) {
+          --onesCounter;
+          internalBuffer.ints[internalBuffer.length++] = 1;
+        }
+        
+        // encode value as value+1
+        internalBuffer.ints[internalBuffer.length++] = value + 1;
+      }
+    }
+    // there might have been 1's that we need to encode
+    while (onesCounter > 0) {
+      --onesCounter;
+      internalBuffer.ints[internalBuffer.length++] = 1;
+    }
+    super.encode(internalBuffer, buf);
+  }
+
+  @Override
+  public IntDecoder createMatchingDecoder() {
+    return new NOnesIntDecoder(n);
+  }
+
+  @Override
+  public String toString() {
+    return "NOnes(" + n + ") (" + super.toString() + ")";
+  }
+
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/encoding/SimpleIntDecoder.java b/lucene/facet/src/java/org/apache/lucene/facet/encoding/SimpleIntDecoder.java
new file mode 100644
index 0000000..82c489a
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/encoding/SimpleIntDecoder.java
@@ -0,0 +1,56 @@
+package org.apache.lucene.facet.encoding;
+
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IntsRef;
+import org.apache.lucene.util.RamUsageEstimator;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Decodes values encoded with {@link SimpleIntEncoder}.
+ * 
+ * @lucene.experimental
+ */
+public final class SimpleIntDecoder extends IntDecoder {
+
+  @Override
+  public void decode(BytesRef buf, IntsRef values) {
+    values.offset = values.length = 0;
+    int numValues = buf.length / 4; // every value is 4 bytes
+    if (values.ints.length < numValues) { // offset and length are 0
+      values.ints = new int[ArrayUtil.oversize(numValues, RamUsageEstimator.NUM_BYTES_INT)];
+    }
+    
+    int offset = buf.offset;
+    int upto = buf.offset + buf.length;
+    while (offset < upto) {
+      values.ints[values.length++] = 
+          ((buf.bytes[offset++] & 0xFF) << 24) | 
+          ((buf.bytes[offset++] & 0xFF) << 16) | 
+          ((buf.bytes[offset++] & 0xFF) <<  8) | 
+          (buf.bytes[offset++] & 0xFF);
+    }
+  }
+
+  @Override
+  public String toString() {
+    return "Simple";
+  }
+
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/encoding/SimpleIntEncoder.java b/lucene/facet/src/java/org/apache/lucene/facet/encoding/SimpleIntEncoder.java
new file mode 100644
index 0000000..f2eb6b9
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/encoding/SimpleIntEncoder.java
@@ -0,0 +1,59 @@
+package org.apache.lucene.facet.encoding;
+
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IntsRef;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * A simple {@link IntEncoder}, writing an integer as 4 raw bytes. *
+ * 
+ * @lucene.experimental
+ */
+public final class SimpleIntEncoder extends IntEncoder {
+
+  @Override
+  public void encode(IntsRef values, BytesRef buf) {
+    buf.offset = buf.length = 0;
+    // ensure there's enough room in the buffer
+    int bytesNeeded = values.length * 4;
+    if (buf.bytes.length < bytesNeeded) {
+      buf.grow(bytesNeeded);
+    }
+    
+    int upto = values.offset + values.length;
+    for (int i = values.offset; i < upto; i++) {
+      int value = values.ints[i];
+      buf.bytes[buf.length++] = (byte) (value >>> 24);
+      buf.bytes[buf.length++] = (byte) ((value >> 16) & 0xFF);
+      buf.bytes[buf.length++] = (byte) ((value >> 8) & 0xFF);
+      buf.bytes[buf.length++] = (byte) (value & 0xFF);
+    }
+  }
+
+  @Override
+  public IntDecoder createMatchingDecoder() {
+    return new SimpleIntDecoder();
+  }
+
+  @Override
+  public String toString() {
+    return "Simple";
+  }
+
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/encoding/SortingIntEncoder.java b/lucene/facet/src/java/org/apache/lucene/facet/encoding/SortingIntEncoder.java
new file mode 100644
index 0000000..c162c61
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/encoding/SortingIntEncoder.java
@@ -0,0 +1,54 @@
+package org.apache.lucene.facet.encoding;
+
+import java.util.Arrays;
+
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IntsRef;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * An {@link IntEncoderFilter} which sorts the values to encode in ascending
+ * order before encoding them.
+ * 
+ * @lucene.experimental
+ */
+public final class SortingIntEncoder extends IntEncoderFilter {
+
+  /** Initializes with the given encoder. */
+  public SortingIntEncoder(IntEncoder encoder) {
+    super(encoder);
+  }
+
+  @Override
+  public void encode(IntsRef values, BytesRef buf) {
+    Arrays.sort(values.ints, values.offset, values.offset + values.length);
+    encoder.encode(values, buf);
+  }
+
+  @Override
+  public IntDecoder createMatchingDecoder() {
+    return encoder.createMatchingDecoder();
+  }
+  
+  @Override
+  public String toString() {
+    return "Sorting(" + encoder.toString() + ")";
+  }
+  
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/encoding/UniqueValuesIntEncoder.java b/lucene/facet/src/java/org/apache/lucene/facet/encoding/UniqueValuesIntEncoder.java
new file mode 100644
index 0000000..fe5388d
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/encoding/UniqueValuesIntEncoder.java
@@ -0,0 +1,63 @@
+package org.apache.lucene.facet.encoding;
+
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IntsRef;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * An {@link IntEncoderFilter} which ensures only unique values are encoded. The
+ * implementation assumes the values given to {@link #encode(IntsRef, BytesRef)} are sorted.
+ * If this is not the case, you can chain this encoder with
+ * {@link SortingIntEncoder}.
+ * 
+ * @lucene.experimental
+ */
+public final class UniqueValuesIntEncoder extends IntEncoderFilter {
+
+  /** Constructs a new instance with the given encoder. */
+  public UniqueValuesIntEncoder(IntEncoder encoder) {
+    super(encoder);
+  }
+
+  @Override
+  public void encode(IntsRef values, BytesRef buf) {
+    int prev = values.ints[values.offset];
+    int idx = values.offset + 1;
+    int upto = values.offset + values.length;
+    for (int i = idx; i < upto; i++) {
+      if (values.ints[i] != prev) {
+        values.ints[idx++] = values.ints[i];
+        prev = values.ints[i];
+      }
+    }
+    values.length = idx - values.offset;
+    encoder.encode(values, buf);
+  }
+
+  @Override
+  public IntDecoder createMatchingDecoder() {
+    return encoder.createMatchingDecoder();
+  }
+  
+  @Override
+  public String toString() {
+    return "Unique(" + encoder.toString() + ")";
+  }
+  
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/encoding/VInt8IntDecoder.java b/lucene/facet/src/java/org/apache/lucene/facet/encoding/VInt8IntDecoder.java
new file mode 100644
index 0000000..785eafe
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/encoding/VInt8IntDecoder.java
@@ -0,0 +1,64 @@
+package org.apache.lucene.facet.encoding;
+
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IntsRef;
+import org.apache.lucene.util.RamUsageEstimator;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Decodes values encoded by {@link VInt8IntEncoder}.
+ * 
+ * @lucene.experimental
+ */
+public final class VInt8IntDecoder extends IntDecoder {
+
+  @Override
+  public void decode(BytesRef buf, IntsRef values) {
+    values.offset = values.length = 0;
+
+    // grow the buffer up front, even if by a large number of values (buf.length)
+    // that saves the need to check inside the loop for every decoded value if
+    // the buffer needs to grow.
+    if (values.ints.length < buf.length) {
+      values.ints = new int[ArrayUtil.oversize(buf.length, RamUsageEstimator.NUM_BYTES_INT)];
+    }
+
+    // it is better if the decoding is inlined like so, and not e.g.
+    // in a utility method
+    int upto = buf.offset + buf.length;
+    int value = 0;
+    int offset = buf.offset;
+    while (offset < upto) {
+      byte b = buf.bytes[offset++];
+      if (b >= 0) {
+        values.ints[values.length++] = (value << 7) | b;
+        value = 0;
+      } else {
+        value = (value << 7) | (b & 0x7F);
+      }
+    }
+  }
+
+  @Override
+  public String toString() {
+    return "VInt8";
+  }
+
+} 
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/encoding/VInt8IntEncoder.java b/lucene/facet/src/java/org/apache/lucene/facet/encoding/VInt8IntEncoder.java
new file mode 100644
index 0000000..5fdda79
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/encoding/VInt8IntEncoder.java
@@ -0,0 +1,104 @@
+package org.apache.lucene.facet.encoding;
+
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IntsRef;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * An {@link IntEncoder} which implements variable length encoding. A number is
+ * encoded as follows:
+ * <ul>
+ * <li>If it is less than 127 and non-negative, i.e. uses only 7 bits, it is
+ * encoded as a single byte: 0bbbbbbb.
+ * <li>If it occupies more than 7 bits, it is represented as a series of bytes,
+ * each byte carrying 7 bits. All but the last byte have the MSB set, the last
+ * one has it unset.
+ * </ul>
+ * Example:
+ * <ol>
+ * <li>n = 117 = 01110101: This has less than 8 significant bits, therefore is
+ * encoded as 01110101 = 0x75.
+ * <li>n = 100000 = (binary) 11000011010100000. This has 17 significant bits,
+ * thus needs three Vint8 bytes. Pad it to a multiple of 7 bits, then split it
+ * into chunks of 7 and add an MSB, 0 for the last byte, 1 for the others:
+ * 1|0000110 1|0001101 0|0100000 = 0x86 0x8D 0x20.
+ * </ol>
+ * <b>NOTE:</b> although this encoder is not limited to values &ge; 0, it is not
+ * recommended for use with negative values, as their encoding will result in 5
+ * bytes written to the output stream, rather than 4. For such values, either
+ * use {@link SimpleIntEncoder} or write your own version of variable length
+ * encoding, which can better handle negative values.
+ * 
+ * @lucene.experimental
+ */
+public final class VInt8IntEncoder extends IntEncoder {
+
+  @Override
+  public void encode(IntsRef values, BytesRef buf) {
+    buf.offset = buf.length = 0;
+    int maxBytesNeeded = 5 * values.length; // at most 5 bytes per VInt
+    if (buf.bytes.length < maxBytesNeeded) {
+      buf.grow(maxBytesNeeded);
+    }
+    
+    int upto = values.offset + values.length;
+    for (int i = values.offset; i < upto; i++) {
+      // it is better if the encoding is inlined like so, and not e.g.
+      // in a utility method
+      int value = values.ints[i];
+      if ((value & ~0x7F) == 0) {
+        buf.bytes[buf.length] = (byte) value;
+        buf.length++;
+      } else if ((value & ~0x3FFF) == 0) {
+        buf.bytes[buf.length] = (byte) (0x80 | ((value & 0x3F80) >> 7));
+        buf.bytes[buf.length + 1] = (byte) (value & 0x7F);
+        buf.length += 2;
+      } else if ((value & ~0x1FFFFF) == 0) {
+        buf.bytes[buf.length] = (byte) (0x80 | ((value & 0x1FC000) >> 14));
+        buf.bytes[buf.length + 1] = (byte) (0x80 | ((value & 0x3F80) >> 7));
+        buf.bytes[buf.length + 2] = (byte) (value & 0x7F);
+        buf.length += 3;
+      } else if ((value & ~0xFFFFFFF) == 0) {
+        buf.bytes[buf.length] = (byte) (0x80 | ((value & 0xFE00000) >> 21));
+        buf.bytes[buf.length + 1] = (byte) (0x80 | ((value & 0x1FC000) >> 14));
+        buf.bytes[buf.length + 2] = (byte) (0x80 | ((value & 0x3F80) >> 7));
+        buf.bytes[buf.length + 3] = (byte) (value & 0x7F);
+        buf.length += 4;
+      } else {
+        buf.bytes[buf.length] = (byte) (0x80 | ((value & 0xF0000000) >> 28));
+        buf.bytes[buf.length + 1] = (byte) (0x80 | ((value & 0xFE00000) >> 21));
+        buf.bytes[buf.length + 2] = (byte) (0x80 | ((value & 0x1FC000) >> 14));
+        buf.bytes[buf.length + 3] = (byte) (0x80 | ((value & 0x3F80) >> 7));
+        buf.bytes[buf.length + 4] = (byte) (value & 0x7F);
+        buf.length += 5;
+      }
+    }
+  }
+
+  @Override
+  public IntDecoder createMatchingDecoder() {
+    return new VInt8IntDecoder();
+  }
+
+  @Override
+  public String toString() {
+    return "VInt8";
+  }
+
+} 
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/encoding/package.html b/lucene/facet/src/java/org/apache/lucene/facet/encoding/package.html
new file mode 100644
index 0000000..b1b95bc
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/encoding/package.html
@@ -0,0 +1,24 @@
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+<html>
+<head>
+<title>Facets Encoding</title>
+</head>
+<body>
+Offers various encoders and decoders for category ordinals.
+</body>
+</html>
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/index/CountingListBuilder.java b/lucene/facet/src/java/org/apache/lucene/facet/index/CountingListBuilder.java
index e41ca4b..c7a3cf2 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/index/CountingListBuilder.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/index/CountingListBuilder.java
@@ -7,15 +7,15 @@ import java.util.Iterator;
 import java.util.Map;
 import java.util.Map.Entry;
 
-import org.apache.lucene.facet.index.params.CategoryListParams;
-import org.apache.lucene.facet.index.params.CategoryListParams.OrdinalPolicy;
-import org.apache.lucene.facet.index.params.FacetIndexingParams;
+import org.apache.lucene.facet.encoding.IntEncoder;
+import org.apache.lucene.facet.params.CategoryListParams;
+import org.apache.lucene.facet.params.FacetIndexingParams;
+import org.apache.lucene.facet.params.CategoryListParams.OrdinalPolicy;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
 import org.apache.lucene.facet.util.PartitionsUtils;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.IntsRef;
-import org.apache.lucene.util.encoding.IntEncoder;
 
 /*
  * Licensed to the Apache Software Foundation (ASF) under one or more
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/index/DrillDownStream.java b/lucene/facet/src/java/org/apache/lucene/facet/index/DrillDownStream.java
index 2535af2..0851f69 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/index/DrillDownStream.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/index/DrillDownStream.java
@@ -5,7 +5,7 @@ import java.util.Iterator;
 
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-import org.apache.lucene.facet.index.params.FacetIndexingParams;
+import org.apache.lucene.facet.params.FacetIndexingParams;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 
 /*
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/index/FacetFields.java b/lucene/facet/src/java/org/apache/lucene/facet/index/FacetFields.java
index a17265e..6251116 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/index/FacetFields.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/index/FacetFields.java
@@ -13,8 +13,8 @@ import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.FieldType;
 import org.apache.lucene.document.TextField;
-import org.apache.lucene.facet.index.params.CategoryListParams;
-import org.apache.lucene.facet.index.params.FacetIndexingParams;
+import org.apache.lucene.facet.params.CategoryListParams;
+import org.apache.lucene.facet.params.FacetIndexingParams;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
 import org.apache.lucene.index.FieldInfo.IndexOptions;
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/index/FacetsPayloadMigrationReader.java b/lucene/facet/src/java/org/apache/lucene/facet/index/FacetsPayloadMigrationReader.java
deleted file mode 100644
index d818113..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/index/FacetsPayloadMigrationReader.java
+++ /dev/null
@@ -1,252 +0,0 @@
-package org.apache.lucene.facet.index;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.Map;
-
-import org.apache.lucene.facet.index.params.CategoryListParams;
-import org.apache.lucene.facet.index.params.FacetIndexingParams;
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.BinaryDocValues;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.DocsAndPositionsEnum;
-import org.apache.lucene.index.FieldInfo.DocValuesType;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.Fields;
-import org.apache.lucene.index.FilterAtomicReader;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.index.Terms;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.search.DocIdSetIterator;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.BytesRef;
-
-/**
- * A {@link FilterAtomicReader} for migrating a facets index which encodes
- * category ordinals in a payload to {@link BinaryDocValues}. To migrate the index,
- * you should build a mapping from a field (String) to term ({@link Term}),
- * which denotes under which BinaryDocValues field to put the data encoded in the
- * matching term's payload. You can follow the code example below to migrate an
- * existing index:
- * 
- * <pre class="prettyprint">
- * // Add the index and migrate payload to DocValues on the go
- * DirectoryReader reader = DirectoryReader.open(oldDir);
- * IndexWriterConfig conf = new IndexWriterConfig(VER, ANALYZER);
- * IndexWriter writer = new IndexWriter(newDir, conf);
- * List&lt;AtomicReaderContext&gt; leaves = reader.leaves();
- * AtomicReader wrappedLeaves[] = new AtomicReader[leaves.size()];
- * for (int i = 0; i &lt; leaves.size(); i++) {
- *   wrappedLeaves[i] = new FacetPayloadMigrationReader(leaves.get(i).reader(),
- *       fieldTerms);
- * }
- * writer.addIndexes(new MultiReader(wrappedLeaves));
- * writer.commit();
- * </pre>
- * 
- * <p>
- * <b>NOTE:</b> to build the field-to-term map you can use
- * {@link #buildFieldTermsMap(Directory, FacetIndexingParams)}, as long as the
- * index to migrate contains the ordinals payload under
- * {@link #PAYLOAD_TERM_TEXT}.
- * 
- * @lucene.experimental
- */
-public class FacetsPayloadMigrationReader extends FilterAtomicReader {  
-
-  private class PayloadMigratingBinaryDocValues extends BinaryDocValues {
-
-    private Fields fields;
-    private Term term;
-    private DocsAndPositionsEnum dpe;
-    private int curDocID = -1;
-    private int lastRequestedDocID;
-
-    private DocsAndPositionsEnum getDPE() {
-      try {
-        DocsAndPositionsEnum dpe = null;
-        if (fields != null) {
-          Terms terms = fields.terms(term.field());
-          if (terms != null) {
-            TermsEnum te = terms.iterator(null); // no use for reusing
-            if (te.seekExact(term.bytes(), true)) {
-              // we're not expected to be called for deleted documents
-              dpe = te.docsAndPositions(null, null, DocsAndPositionsEnum.FLAG_PAYLOADS);
-            }
-          }
-        }
-        return dpe;
-      } catch (IOException ioe) {
-        throw new RuntimeException(ioe);
-      }
-    }
-    
-    protected PayloadMigratingBinaryDocValues(Fields fields, Term term) {
-      this.fields = fields;
-      this.term = term;
-      this.dpe = getDPE();
-      if (dpe == null) {
-        curDocID = DocIdSetIterator.NO_MORE_DOCS;
-      } else {
-        try {
-          curDocID = dpe.nextDoc();
-        } catch (IOException e) {
-          throw new RuntimeException(e);
-        }
-      }
-    }
-    
-    @Override
-    public void get(int docID, BytesRef result) {
-      try {
-        // If caller is moving backwards (eg, during merge,
-        // the consuming DV format is free to iterate over
-        // our values as many times as it wants), we must
-        // re-init the dpe:
-        if (docID <= lastRequestedDocID) {
-          dpe = getDPE();
-          if (dpe == null) {
-            curDocID = DocIdSetIterator.NO_MORE_DOCS;
-          } else{
-            curDocID = dpe.nextDoc();
-          }
-        }
-        lastRequestedDocID = docID;
-        if (curDocID > docID) {
-          // document does not exist
-          result.length = 0;
-          return;
-        }
-      
-        if (curDocID < docID) {
-          curDocID = dpe.advance(docID);
-          if (curDocID != docID) { // requested document does not have a payload
-            result.length = 0;
-            return;
-          }
-        }
-        
-        dpe.nextPosition();
-        result.copyBytes(dpe.getPayload());
-      } catch (IOException e) {
-        throw new RuntimeException(e);
-      }
-    }
-  }
-  
-  /** The {@link Term} text of the ordinals payload. */
-  public static final String PAYLOAD_TERM_TEXT = "$fulltree$";
-
-  /**
-   * A utility method for building the field-to-Term map, given the
-   * {@link FacetIndexingParams} and the directory of the index to migrate. The
-   * map that will be built will correspond to partitions as well as multiple
-   * {@link CategoryListParams}.
-   * <p>
-   * <b>NOTE:</b> since {@link CategoryListParams} no longer define a
-   * {@link Term}, this method assumes that the term used by the different
-   * {@link CategoryListParams} is {@link #PAYLOAD_TERM_TEXT}. If this is not
-   * the case, then you should build the map yourself, using the terms in your
-   * index.
-   */
-  public static Map<String,Term> buildFieldTermsMap(Directory dir, FacetIndexingParams fip) throws IOException {
-    // only add field-Term mapping that will actually have DocValues in the end.
-    // therefore traverse the index terms and add what exists. this pertains to
-    // multiple CLPs, as well as partitions
-    DirectoryReader reader = DirectoryReader.open(dir);
-    final Map<String,Term> fieldTerms = new HashMap<String,Term>();
-    for (AtomicReaderContext context : reader.leaves()) {
-      for (CategoryListParams clp : fip.getAllCategoryListParams()) {
-        Terms terms = context.reader().terms(clp.field);
-        if (terms != null) {
-          TermsEnum te = terms.iterator(null);
-          BytesRef termBytes = null;
-          while ((termBytes = te.next()) != null) {
-            String term = termBytes.utf8ToString();
-            if (term.startsWith(PAYLOAD_TERM_TEXT )) {
-              if (term.equals(PAYLOAD_TERM_TEXT)) {
-                fieldTerms.put(clp.field, new Term(clp.field, term));
-              } else {
-                fieldTerms.put(clp.field + term.substring(PAYLOAD_TERM_TEXT.length()), new Term(clp.field, term));
-              }
-            }
-          }
-        }        
-      }
-    }
-    reader.close();
-    return fieldTerms;
-  }
-  
-  private final Map<String,Term> fieldTerms;
-  
-  /**
-   * Wraps an {@link AtomicReader} and migrates the payload to {@link BinaryDocValues}
-   * fields by using the given mapping.
-   */
-  public FacetsPayloadMigrationReader(AtomicReader in, Map<String,Term> fieldTerms) {
-    super(in);
-    this.fieldTerms = fieldTerms;
-  }
-  
-  @Override
-  public BinaryDocValues getBinaryDocValues(String field) throws IOException {
-    Term term = fieldTerms.get(field);
-    if (term == null) {
-      return super.getBinaryDocValues(field);
-    } else {
-      // we shouldn't return null, even if the term does not exist or has no
-      // payloads, since we already marked the field as having DocValues.
-      return new PayloadMigratingBinaryDocValues(fields(), term);
-    }
-  }
-
-  @Override
-  public FieldInfos getFieldInfos() {
-    FieldInfos innerInfos = super.getFieldInfos();
-    ArrayList<FieldInfo> infos = new ArrayList<FieldInfo>(innerInfos.size());
-    // if there are partitions, then the source index contains one field for all their terms
-    // while with DocValues, we simulate that by multiple fields.
-    HashSet<String> leftoverFields = new HashSet<String>(fieldTerms.keySet());
-    int number = -1;
-    for (FieldInfo info : innerInfos) {
-      if (fieldTerms.containsKey(info.name)) {
-        // mark this field as having a DocValues
-        infos.add(new FieldInfo(info.name, true, info.number,
-            info.hasVectors(), info.omitsNorms(), info.hasPayloads(),
-            info.getIndexOptions(), DocValuesType.BINARY,
-            info.getNormType(), info.attributes()));
-        leftoverFields.remove(info.name);
-      } else {
-        infos.add(info);
-      }
-      number = Math.max(number, info.number);
-    }
-    for (String field : leftoverFields) {
-      infos.add(new FieldInfo(field, false, ++number, false, false, false,
-          null, DocValuesType.BINARY, null, null));
-    }
-    return new FieldInfos(infos.toArray(new FieldInfo[infos.size()]));
-  }
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/index/OrdinalMappingAtomicReader.java b/lucene/facet/src/java/org/apache/lucene/facet/index/OrdinalMappingAtomicReader.java
deleted file mode 100644
index e9ef0df..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/index/OrdinalMappingAtomicReader.java
+++ /dev/null
@@ -1,145 +0,0 @@
-package org.apache.lucene.facet.index;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-
-import org.apache.lucene.facet.index.params.CategoryListParams;
-import org.apache.lucene.facet.index.params.FacetIndexingParams;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter.OrdinalMap;
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.BinaryDocValues;
-import org.apache.lucene.index.FilterAtomicReader;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IntsRef;
-import org.apache.lucene.util.encoding.IntDecoder;
-import org.apache.lucene.util.encoding.IntEncoder;
-
-/**
- * A {@link FilterAtomicReader} for updating facets ordinal references,
- * based on an ordinal map. You should use this code in conjunction with merging
- * taxonomies - after you merge taxonomies, you receive an {@link OrdinalMap}
- * which maps the 'old' ordinals to the 'new' ones. You can use that map to
- * re-map the doc values which contain the facets information (ordinals) either
- * before or while merging the indexes.
- * <p>
- * For re-mapping the ordinals during index merge, do the following:
- * 
- * <pre class="prettyprint">
- * // merge the old taxonomy with the new one.
- * OrdinalMap map = DirectoryTaxonomyWriter.addTaxonomies();
- * int[] ordmap = map.getMap();
- * 
- * // Add the index and re-map ordinals on the go
- * DirectoryReader reader = DirectoryReader.open(oldDir);
- * IndexWriterConfig conf = new IndexWriterConfig(VER, ANALYZER);
- * IndexWriter writer = new IndexWriter(newDir, conf);
- * List&lt;AtomicReaderContext&gt; leaves = reader.leaves();
- *   AtomicReader wrappedLeaves[] = new AtomicReader[leaves.size()];
- *   for (int i = 0; i < leaves.size(); i++) {
- *     wrappedLeaves[i] = new OrdinalMappingAtomicReader(leaves.get(i).reader(), ordmap);
- *   }
- * writer.addIndexes(new MultiReader(wrappedLeaves));
- * writer.commit();
- * </pre>
- * 
- * @lucene.experimental
- */
-public class OrdinalMappingAtomicReader extends FilterAtomicReader {
-  
-  private final int[] ordinalMap;
-  
-  private final Map<String,CategoryListParams> dvFieldMap = new HashMap<String,CategoryListParams>();
-  
-  /**
-   * Wraps an AtomicReader, mapping ordinals according to the ordinalMap.
-   * Calls {@link #OrdinalMappingAtomicReader(AtomicReader, int[], FacetIndexingParams)
-   * OrdinalMappingAtomicReader(in, ordinalMap, new DefaultFacetIndexingParams())}
-   */
-  public OrdinalMappingAtomicReader(AtomicReader in, int[] ordinalMap) {
-    this(in, ordinalMap, FacetIndexingParams.ALL_PARENTS);
-  }
-  
-  /**
-   * Wraps an AtomicReader, mapping ordinals according to the ordinalMap,
-   * using the provided indexingParams.
-   */
-  public OrdinalMappingAtomicReader(AtomicReader in, int[] ordinalMap, FacetIndexingParams indexingParams) {
-    super(in);
-    this.ordinalMap = ordinalMap;
-    for (CategoryListParams params: indexingParams.getAllCategoryListParams()) {
-      dvFieldMap.put(params.field, params);
-    }
-  }
-
-  @Override
-  public BinaryDocValues getBinaryDocValues(String field) throws IOException {
-    BinaryDocValues inner = super.getBinaryDocValues(field);
-    if (inner == null) {
-      return inner;
-    }
-    
-    CategoryListParams clp = dvFieldMap.get(field);
-    if (clp == null) {
-      return inner;
-    } else {
-      return new OrdinalMappingBinaryDocValues(clp, inner);
-    }
-  }
-  
-  private class OrdinalMappingBinaryDocValues extends BinaryDocValues {
-
-    private final IntEncoder encoder;
-    private final IntDecoder decoder;
-    private final IntsRef ordinals = new IntsRef(32);
-    private final BinaryDocValues delegate;
-    private final BytesRef scratch = new BytesRef();
-    
-    protected OrdinalMappingBinaryDocValues(CategoryListParams clp, BinaryDocValues delegate) {
-      this.delegate = delegate;
-      encoder = clp.createEncoder();
-      decoder = encoder.createMatchingDecoder();
-    }
-    
-    @SuppressWarnings("synthetic-access")
-    @Override
-    public void get(int docID, BytesRef result) {
-      // NOTE: this isn't quite koscher, because in general
-      // multiple threads can call BinaryDV.get which would
-      // then conflict on the single scratch instance, but
-      // because this impl is only used for merging, we know
-      // only 1 thread calls us:
-      delegate.get(docID, scratch);
-      if (scratch.length > 0) {
-        // We must use scratch (and not re-use result) here,
-        // else encoder may overwrite the DV provider's
-        // private byte[]:
-        decoder.decode(scratch, ordinals);
-        
-        // map the ordinals
-        for (int i = 0; i < ordinals.length; i++) {
-          ordinals.ints[i] = ordinalMap[ordinals.ints[i]];
-        }
-        
-        encoder.encode(ordinals, result);
-      }
-    }
-  }
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/index/package.html b/lucene/facet/src/java/org/apache/lucene/facet/index/package.html
index b936d23..1b94556 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/index/package.html
+++ b/lucene/facet/src/java/org/apache/lucene/facet/index/package.html
@@ -16,16 +16,10 @@
 -->
 <html>
 <head>
-<title>Indexing of document categories</title>
+<title>Facets indexing code</title>
 </head>
 <body>
-<h1>Indexing of document categories</h1>
-
-Attachment of 
-{@link org.apache.lucene.facet.taxonomy.CategoryPath CategoryPath}'s 
-or {@link org.apache.lucene.facet.associations.CategoryAssociation CategoryAssociation}'s 
-to a given document using a 
-{@link org.apache.lucene.facet.taxonomy.TaxonomyWriter Taxonomy}.
+Facets indexing code.
 
 </body>
 </html>
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/index/params/CategoryListParams.java b/lucene/facet/src/java/org/apache/lucene/facet/index/params/CategoryListParams.java
deleted file mode 100644
index dcf8ed1..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/index/params/CategoryListParams.java
+++ /dev/null
@@ -1,187 +0,0 @@
-package org.apache.lucene.facet.index.params;
-
-import java.io.IOException;
-
-import org.apache.lucene.facet.search.CategoryListIterator;
-import org.apache.lucene.facet.search.DocValuesCategoryListIterator;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.util.PartitionsUtils;
-import org.apache.lucene.util.encoding.DGapVInt8IntEncoder;
-import org.apache.lucene.util.encoding.IntDecoder;
-import org.apache.lucene.util.encoding.IntEncoder;
-import org.apache.lucene.util.encoding.SortingIntEncoder;
-import org.apache.lucene.util.encoding.UniqueValuesIntEncoder;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Contains parameters for a category list *
- * 
- * @lucene.experimental
- */
-public class CategoryListParams {
-
-  /**
-   * Defines which category ordinals are encoded for every document. This also
-   * affects how category ordinals are aggregated, check the different policies
-   * for more details.
-   */
-  public static enum OrdinalPolicy {
-    /**
-     * Encodes only the ordinals of leaf nodes. That is, for the category A/B/C,
-     * the ordinals of A and A/B will not be encoded. This policy is efficient
-     * for hierarchical dimensions, as it reduces the number of ordinals that
-     * are visited per document. During faceted search, this policy behaves
-     * exactly like {@link #ALL_PARENTS}, and the counts of all path components
-     * will be computed as well.
-     * 
-     * <p>
-     * <b>NOTE:</b> this {@link OrdinalPolicy} requires a special collector or
-     * accumulator, which will fix the parents' counts.
-     * 
-     * <p>
-     * <b>NOTE:</b> since only leaf nodes are encoded for the document, you
-     * should use this policy when the same document doesn't share two
-     * categories that have a mutual parent, or otherwise the counts will be
-     * wrong (the mutual parent will be over-counted). For example, if a
-     * document has the categories A/B/C and A/B/D, then with this policy the
-     * counts of "A" and "B" will be 2, which is wrong. If you intend to index
-     * hierarchical dimensions, with more than one category per document, you
-     * should use either {@link #ALL_PARENTS} or {@link #ALL_BUT_DIMENSION}.
-     */
-    NO_PARENTS,
-    
-    /**
-     * Encodes the ordinals of all path components. That is, the category A/B/C
-     * will encode the ordinals of A and A/B as well. If you don't require the
-     * dimension's count during search, consider using
-     * {@link #ALL_BUT_DIMENSION}.
-     */
-    ALL_PARENTS,
-    
-    /**
-     * Encodes the ordinals of all path components except the dimension. The
-     * dimension of a category is defined to be the first components in
-     * {@link CategoryPath#components}. For the category A/B/C, the ordinal of
-     * A/B will be encoded as well, however not the ordinal of A.
-     * 
-     * <p>
-     * <b>NOTE:</b> when facets are aggregated, this policy behaves exactly like
-     * {@link #ALL_PARENTS}, except that the dimension is never counted. I.e. if
-     * you ask to count the facet "A", then while in {@link #ALL_PARENTS} you
-     * will get counts for "A" <u>and its children</u>, with this policy you
-     * will get counts for <u>only its children</u>. This policy is the default
-     * one, and makes sense for using with flat dimensions, whenever your
-     * application does not require the dimension's count. Otherwise, use
-     * {@link #ALL_PARENTS}.
-     */
-    ALL_BUT_DIMENSION
-  }
-  
-  /** The default field used to store the facets information. */
-  public static final String DEFAULT_FIELD = "$facets";
-
-  /**
-   * The default {@link OrdinalPolicy} that's used when encoding a document's
-   * category ordinals.
-   */
-  public static final OrdinalPolicy DEFAULT_ORDINAL_POLICY = OrdinalPolicy.ALL_BUT_DIMENSION;
-  
-  public final String field;
-
-  private final int hashCode;
-
-  /** Constructs a default category list parameters object, using {@link #DEFAULT_FIELD}. */
-  public CategoryListParams() {
-    this(DEFAULT_FIELD);
-  }
-
-  /** Constructs a category list parameters object, using the given field. */
-  public CategoryListParams(String field) {
-    this.field = field;
-    // Pre-compute the hashCode because these objects are immutable.  Saves
-    // some time on the comparisons later.
-    this.hashCode = field.hashCode();
-  }
-  
-  /**
-   * Allows to override how categories are encoded and decoded. A matching
-   * {@link IntDecoder} is provided by the {@link IntEncoder}.
-   * <p>
-   * Default implementation creates a new Sorting(<b>Unique</b>(DGap)) encoder.
-   * Uniqueness in this regard means when the same category appears twice in a
-   * document, only one appearance would be encoded. This has effect on facet
-   * counting results.
-   * <p>
-   * Some possible considerations when overriding may be:
-   * <ul>
-   * <li>an application "knows" that all categories are unique. So no need to
-   * pass through the unique filter.</li>
-   * <li>Another application might wish to count multiple occurrences of the
-   * same category, or, use a faster encoding which will consume more space.</li>
-   * </ul>
-   * In any event when changing this value make sure you know what you are
-   * doing, and test the results - e.g. counts, if the application is about
-   * counting facets.
-   */
-  public IntEncoder createEncoder() {
-    return new SortingIntEncoder(new UniqueValuesIntEncoder(new DGapVInt8IntEncoder()));
-  }
-
-  @Override
-  public boolean equals(Object o) {
-    if (o == this) {
-      return true;
-    }
-    if (!(o instanceof CategoryListParams)) {
-      return false;
-    }
-    CategoryListParams other = (CategoryListParams) o;
-    if (hashCode != other.hashCode) {
-      return false;
-    }
-    return field.equals(other.field);
-  }
-
-  @Override
-  public int hashCode() {
-    return hashCode;
-  }
-
-  /** Create the {@link CategoryListIterator} for the specified partition. */
-  public CategoryListIterator createCategoryListIterator(int partition) throws IOException {
-    String categoryListTermStr = PartitionsUtils.partitionName(partition);
-    String docValuesField = field + categoryListTermStr;
-    return new DocValuesCategoryListIterator(docValuesField, createEncoder().createMatchingDecoder());
-  }
-  
-  /**
-   * Returns the {@link OrdinalPolicy} to use for the given dimension. This
-   * {@link CategoryListParams} always returns {@link #DEFAULT_ORDINAL_POLICY}
-   * for all dimensions.
-   */
-  public OrdinalPolicy getOrdinalPolicy(String dimension) {
-    return DEFAULT_ORDINAL_POLICY;
-  }
-  
-  @Override
-  public String toString() {
-    return "field=" + field + " encoder=" + createEncoder() + " ordinalPolicy=" + getOrdinalPolicy(null);
-  }
-  
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/index/params/FacetIndexingParams.java b/lucene/facet/src/java/org/apache/lucene/facet/index/params/FacetIndexingParams.java
deleted file mode 100644
index c5abe40..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/index/params/FacetIndexingParams.java
+++ /dev/null
@@ -1,180 +0,0 @@
-package org.apache.lucene.facet.index.params;
-
-import java.util.Collections;
-import java.util.List;
-
-import org.apache.lucene.facet.index.params.CategoryListParams.OrdinalPolicy;
-import org.apache.lucene.facet.search.FacetArrays;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Defines parameters that are needed for facets indexing. Note that this class
- * does not have any setters. That's because overriding the default parameters
- * is considered expert. If you wish to override them, simply extend this class
- * and override the relevant getter.
- * 
- * <p>
- * <b>NOTE:</b> This class is also used during faceted search in order to e.g.
- * know which field holds the drill-down terms or the fulltree posting.
- * Therefore this class should be initialized once and you should refrain from
- * changing it. Also note that if you make any changes to it (e.g. suddenly
- * deciding that drill-down terms should be read from a different field) and use
- * it on an existing index, things may not work as expected.
- * 
- * @lucene.experimental
- */
-public class FacetIndexingParams {
-  
-  // the default CLP, can be a singleton
-  protected static final CategoryListParams DEFAULT_CATEGORY_LIST_PARAMS = new CategoryListParams();
-
-  /**
-   * A {@link FacetIndexingParams} which fixes a single
-   * {@link CategoryListParams} with {@link OrdinalPolicy#ALL_PARENTS}.
-   */
-  public static final FacetIndexingParams ALL_PARENTS = new FacetIndexingParams();
-  
-  /**
-   * The default delimiter with which {@link CategoryPath#components} are
-   * concatenated when written to the index, e.g. as drill-down terms. If you
-   * choose to override it by overiding {@link #getFacetDelimChar()}, you should
-   * make sure that you return a character that's not found in any path
-   * component.
-   */
-  public static final char DEFAULT_FACET_DELIM_CHAR = '\uF749';
-  
-  private final int partitionSize = Integer.MAX_VALUE;
-
-  protected final CategoryListParams clParams;
-
-  /**
-   * Initializes new default params. You should use this constructor only if you
-   * intend to override any of the getters, otherwise you can use
-   * {@link #ALL_PARENTS} to save unnecessary object allocations.
-   */
-  public FacetIndexingParams() {
-    this(DEFAULT_CATEGORY_LIST_PARAMS);
-  }
-
-  /** Initializes new params with the given {@link CategoryListParams}. */
-  public FacetIndexingParams(CategoryListParams categoryListParams) {
-    clParams = categoryListParams;
-  }
-
-  /**
-   * Returns the {@link CategoryListParams} for this {@link CategoryPath}. The
-   * default implementation returns the same {@link CategoryListParams} for all
-   * categories (even if {@code category} is {@code null}).
-   * 
-   * @see PerDimensionIndexingParams
-   */
-  public CategoryListParams getCategoryListParams(CategoryPath category) {
-    return clParams;
-  }
-
-  /**
-   * Copies the text required to execute a drill-down query on the given
-   * category to the given {@code char[]}, and returns the number of characters
-   * that were written.
-   * <p>
-   * <b>NOTE:</b> You should make sure that the {@code char[]} is large enough,
-   * by e.g. calling {@link CategoryPath#fullPathLength()}.
-   */
-  public int drillDownTermText(CategoryPath path, char[] buffer) {
-    return path.copyFullPath(buffer, 0, getFacetDelimChar());
-  }
-  
-  /**
-   * Returns the size of a partition. <i>Partitions</i> allow you to divide
-   * (hence, partition) the categories space into small sets to e.g. improve RAM
-   * consumption during faceted search. For instance, {@code partitionSize=100K}
-   * would mean that if your taxonomy index contains 420K categories, they will
-   * be divided into 5 groups and at search time a {@link FacetArrays} will be
-   * allocated at the size of the partition.
-   * 
-   * <p>
-   * This is real advanced setting and should be changed with care. By default,
-   * all categories are put in one partition. You should modify this setting if
-   * you have really large taxonomies (e.g. 1M+ nodes).
-   */
-  public int getPartitionSize() {
-    return partitionSize;
-  }
-  
-  /**
-   * Returns a list of all {@link CategoryListParams categoryListParams} that
-   * are used for facets indexing.
-   */
-  public List<CategoryListParams> getAllCategoryListParams() {
-    return Collections.singletonList(clParams);
-  }
-
-  @Override
-  public int hashCode() {
-    final int prime = 31;
-    int result = 1;
-    result = prime * result + ((clParams == null) ? 0 : clParams.hashCode());
-    result = prime * result + partitionSize;
-    
-    for (CategoryListParams clp : getAllCategoryListParams()) {
-      result ^= clp.hashCode();
-    }
-    
-    return result;
-  }
-
-  @Override
-  public boolean equals(Object obj) {
-    if (this == obj) {
-      return true;
-    }
-    if (obj == null) {
-      return false;
-    }
-    if (!(obj instanceof FacetIndexingParams)) {
-      return false;
-    }
-    FacetIndexingParams other = (FacetIndexingParams) obj;
-    if (clParams == null) {
-      if (other.clParams != null) {
-        return false;
-      }
-    } else if (!clParams.equals(other.clParams)) {
-      return false;
-    }
-    if (partitionSize != other.partitionSize) {
-      return false;
-    }
-    
-    Iterable<CategoryListParams> cLs = getAllCategoryListParams();
-    Iterable<CategoryListParams> otherCLs = other.getAllCategoryListParams();
-    
-    return cLs.equals(otherCLs);
-  }
-
-  /**
-   * Returns the delimiter character used internally for concatenating category
-   * path components, e.g. for drill-down terms.
-   */
-  public char getFacetDelimChar() {
-    return DEFAULT_FACET_DELIM_CHAR;
-  }
-
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/index/params/PerDimensionIndexingParams.java b/lucene/facet/src/java/org/apache/lucene/facet/index/params/PerDimensionIndexingParams.java
deleted file mode 100644
index c4b971a..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/index/params/PerDimensionIndexingParams.java
+++ /dev/null
@@ -1,96 +0,0 @@
-package org.apache.lucene.facet.index.params;
-
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.Map.Entry;
-
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * A {@link FacetIndexingParams} that utilizes different category lists, defined
- * by the dimension specified by a {@link CategoryPath category} (see
- * {@link #PerDimensionIndexingParams(Map, CategoryListParams)}.
- * <p>
- * A 'dimension' is defined as the first or "zero-th" component in a
- * {@link CategoryPath}. For example, if a category is defined as
- * "Author/American/Mark Twain", then the dimension would be "Author".
- * 
- * @lucene.experimental
- */
-public class PerDimensionIndexingParams extends FacetIndexingParams {
-
-  private final Map<String, CategoryListParams> clParamsMap;
-
-  /**
-   * Initializes a new instance with the given dimension-to-params mapping. The
-   * dimension is considered as what's returned by
-   * {@link CategoryPath#components cp.components[0]}.
-   * 
-   * <p>
-   * <b>NOTE:</b> for any dimension whose {@link CategoryListParams} is not
-   * defined in the mapping, a default {@link CategoryListParams} will be used.
-   * 
-   * @see #PerDimensionIndexingParams(Map, CategoryListParams)
-   */
-  public PerDimensionIndexingParams(Map<CategoryPath, CategoryListParams> paramsMap) {
-    this(paramsMap, DEFAULT_CATEGORY_LIST_PARAMS);
-  }
-
-  /**
-   * Same as {@link #PerDimensionIndexingParams(Map)}, only the given
-   * {@link CategoryListParams} will be used for any dimension that is not
-   * specified in the given mapping.
-   */
-  public PerDimensionIndexingParams(Map<CategoryPath, CategoryListParams> paramsMap, 
-      CategoryListParams categoryListParams) {
-    super(categoryListParams);
-    clParamsMap = new HashMap<String,CategoryListParams>();
-    for (Entry<CategoryPath, CategoryListParams> e : paramsMap.entrySet()) {
-      clParamsMap.put(e.getKey().components[0], e.getValue());
-    }
-  }
-
-  @Override
-  public List<CategoryListParams> getAllCategoryListParams() {
-    ArrayList<CategoryListParams> vals = new ArrayList<CategoryListParams>(clParamsMap.values());
-    vals.add(clParams); // add the default too
-    return vals;
-  }
-
-  /**
-   * Returns the {@link CategoryListParams} for the corresponding dimension
-   * which is returned by {@code category.getComponent(0)}. If {@code category}
-   * is {@code null}, or was not specified in the map given to the constructor,
-   * returns the default {@link CategoryListParams}.
-   */
-  @Override
-  public CategoryListParams getCategoryListParams(CategoryPath category) {
-    if (category != null) {
-      CategoryListParams clParams = clParamsMap.get(category.components[0]);
-      if (clParams != null) {
-        return clParams;
-      }
-    }
-    return clParams;
-  }
-
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/index/params/PerDimensionOrdinalPolicy.java b/lucene/facet/src/java/org/apache/lucene/facet/index/params/PerDimensionOrdinalPolicy.java
deleted file mode 100644
index c91b1a6..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/index/params/PerDimensionOrdinalPolicy.java
+++ /dev/null
@@ -1,55 +0,0 @@
-package org.apache.lucene.facet.index.params;
-
-import java.util.Map;
-
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * A {@link CategoryListParams} which allow controlling the
- * {@link CategoryListParams.OrdinalPolicy} used for each dimension. The
- * dimension is specified as the first component in
- * {@link CategoryPath#components}.
- */
-public class PerDimensionOrdinalPolicy extends CategoryListParams {
-
-  private final Map<String,OrdinalPolicy> policies;
-  private final OrdinalPolicy defaultOP;
-  
-  public PerDimensionOrdinalPolicy(Map<String,OrdinalPolicy> policies) {
-    this(policies, DEFAULT_ORDINAL_POLICY);
-  }
-  
-  public PerDimensionOrdinalPolicy(Map<String,OrdinalPolicy> policies, OrdinalPolicy defaultOP) {
-    this.defaultOP = defaultOP;
-    this.policies = policies;
-  }
-
-  @Override
-  public OrdinalPolicy getOrdinalPolicy(String dimension) {
-    OrdinalPolicy op = policies.get(dimension);
-    return op == null ? defaultOP : op;
-  }
-  
-  @Override
-  public String toString() {
-    return super.toString() + " policies=" + policies;
-  }
-  
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/index/params/package.html b/lucene/facet/src/java/org/apache/lucene/facet/index/params/package.html
deleted file mode 100644
index 1e86752..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/index/params/package.html
+++ /dev/null
@@ -1,28 +0,0 @@
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-<title>Indexing-time specifications for handling facets</title>
-</head>
-<body>
-<h1>Indexing-time specifications for handling facets</h1>
-
-Parameters on how facets are to be written to the index,  
-such as which fields and terms are used to refer to the facets posting list.
-
-</body>
-</html>
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/package.html b/lucene/facet/src/java/org/apache/lucene/facet/package.html
deleted file mode 100644
index a08531a..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/package.html
+++ /dev/null
@@ -1,24 +0,0 @@
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-  <head>
-    <title>Faceted Indexing and Search</title>
-  </head>
-  <body>
-    Provides faceted indexing and search capabilities. The <a href="doc-files/userguide.html">userguide</a> is recommended for a start.  
-  </body>
-</html>
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/params/CategoryListParams.java b/lucene/facet/src/java/org/apache/lucene/facet/params/CategoryListParams.java
new file mode 100644
index 0000000..a457bcd
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/params/CategoryListParams.java
@@ -0,0 +1,187 @@
+package org.apache.lucene.facet.params;
+
+import java.io.IOException;
+
+import org.apache.lucene.facet.encoding.DGapVInt8IntEncoder;
+import org.apache.lucene.facet.encoding.IntDecoder;
+import org.apache.lucene.facet.encoding.IntEncoder;
+import org.apache.lucene.facet.encoding.SortingIntEncoder;
+import org.apache.lucene.facet.encoding.UniqueValuesIntEncoder;
+import org.apache.lucene.facet.search.CategoryListIterator;
+import org.apache.lucene.facet.search.DocValuesCategoryListIterator;
+import org.apache.lucene.facet.taxonomy.CategoryPath;
+import org.apache.lucene.facet.util.PartitionsUtils;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Contains parameters for a category list *
+ * 
+ * @lucene.experimental
+ */
+public class CategoryListParams {
+
+  /**
+   * Defines which category ordinals are encoded for every document. This also
+   * affects how category ordinals are aggregated, check the different policies
+   * for more details.
+   */
+  public static enum OrdinalPolicy {
+    /**
+     * Encodes only the ordinals of leaf nodes. That is, for the category A/B/C,
+     * the ordinals of A and A/B will not be encoded. This policy is efficient
+     * for hierarchical dimensions, as it reduces the number of ordinals that
+     * are visited per document. During faceted search, this policy behaves
+     * exactly like {@link #ALL_PARENTS}, and the counts of all path components
+     * will be computed as well.
+     * 
+     * <p>
+     * <b>NOTE:</b> this {@link OrdinalPolicy} requires a special collector or
+     * accumulator, which will fix the parents' counts.
+     * 
+     * <p>
+     * <b>NOTE:</b> since only leaf nodes are encoded for the document, you
+     * should use this policy when the same document doesn't share two
+     * categories that have a mutual parent, or otherwise the counts will be
+     * wrong (the mutual parent will be over-counted). For example, if a
+     * document has the categories A/B/C and A/B/D, then with this policy the
+     * counts of "A" and "B" will be 2, which is wrong. If you intend to index
+     * hierarchical dimensions, with more than one category per document, you
+     * should use either {@link #ALL_PARENTS} or {@link #ALL_BUT_DIMENSION}.
+     */
+    NO_PARENTS,
+    
+    /**
+     * Encodes the ordinals of all path components. That is, the category A/B/C
+     * will encode the ordinals of A and A/B as well. If you don't require the
+     * dimension's count during search, consider using
+     * {@link #ALL_BUT_DIMENSION}.
+     */
+    ALL_PARENTS,
+    
+    /**
+     * Encodes the ordinals of all path components except the dimension. The
+     * dimension of a category is defined to be the first components in
+     * {@link CategoryPath#components}. For the category A/B/C, the ordinal of
+     * A/B will be encoded as well, however not the ordinal of A.
+     * 
+     * <p>
+     * <b>NOTE:</b> when facets are aggregated, this policy behaves exactly like
+     * {@link #ALL_PARENTS}, except that the dimension is never counted. I.e. if
+     * you ask to count the facet "A", then while in {@link #ALL_PARENTS} you
+     * will get counts for "A" <u>and its children</u>, with this policy you
+     * will get counts for <u>only its children</u>. This policy is the default
+     * one, and makes sense for using with flat dimensions, whenever your
+     * application does not require the dimension's count. Otherwise, use
+     * {@link #ALL_PARENTS}.
+     */
+    ALL_BUT_DIMENSION
+  }
+  
+  /** The default field used to store the facets information. */
+  public static final String DEFAULT_FIELD = "$facets";
+
+  /**
+   * The default {@link OrdinalPolicy} that's used when encoding a document's
+   * category ordinals.
+   */
+  public static final OrdinalPolicy DEFAULT_ORDINAL_POLICY = OrdinalPolicy.ALL_BUT_DIMENSION;
+  
+  public final String field;
+
+  private final int hashCode;
+
+  /** Constructs a default category list parameters object, using {@link #DEFAULT_FIELD}. */
+  public CategoryListParams() {
+    this(DEFAULT_FIELD);
+  }
+
+  /** Constructs a category list parameters object, using the given field. */
+  public CategoryListParams(String field) {
+    this.field = field;
+    // Pre-compute the hashCode because these objects are immutable.  Saves
+    // some time on the comparisons later.
+    this.hashCode = field.hashCode();
+  }
+  
+  /**
+   * Allows to override how categories are encoded and decoded. A matching
+   * {@link IntDecoder} is provided by the {@link IntEncoder}.
+   * <p>
+   * Default implementation creates a new Sorting(<b>Unique</b>(DGap)) encoder.
+   * Uniqueness in this regard means when the same category appears twice in a
+   * document, only one appearance would be encoded. This has effect on facet
+   * counting results.
+   * <p>
+   * Some possible considerations when overriding may be:
+   * <ul>
+   * <li>an application "knows" that all categories are unique. So no need to
+   * pass through the unique filter.</li>
+   * <li>Another application might wish to count multiple occurrences of the
+   * same category, or, use a faster encoding which will consume more space.</li>
+   * </ul>
+   * In any event when changing this value make sure you know what you are
+   * doing, and test the results - e.g. counts, if the application is about
+   * counting facets.
+   */
+  public IntEncoder createEncoder() {
+    return new SortingIntEncoder(new UniqueValuesIntEncoder(new DGapVInt8IntEncoder()));
+  }
+
+  @Override
+  public boolean equals(Object o) {
+    if (o == this) {
+      return true;
+    }
+    if (!(o instanceof CategoryListParams)) {
+      return false;
+    }
+    CategoryListParams other = (CategoryListParams) o;
+    if (hashCode != other.hashCode) {
+      return false;
+    }
+    return field.equals(other.field);
+  }
+
+  @Override
+  public int hashCode() {
+    return hashCode;
+  }
+
+  /** Create the {@link CategoryListIterator} for the specified partition. */
+  public CategoryListIterator createCategoryListIterator(int partition) throws IOException {
+    String categoryListTermStr = PartitionsUtils.partitionName(partition);
+    String docValuesField = field + categoryListTermStr;
+    return new DocValuesCategoryListIterator(docValuesField, createEncoder().createMatchingDecoder());
+  }
+  
+  /**
+   * Returns the {@link OrdinalPolicy} to use for the given dimension. This
+   * {@link CategoryListParams} always returns {@link #DEFAULT_ORDINAL_POLICY}
+   * for all dimensions.
+   */
+  public OrdinalPolicy getOrdinalPolicy(String dimension) {
+    return DEFAULT_ORDINAL_POLICY;
+  }
+  
+  @Override
+  public String toString() {
+    return "field=" + field + " encoder=" + createEncoder() + " ordinalPolicy=" + getOrdinalPolicy(null);
+  }
+  
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/params/FacetIndexingParams.java b/lucene/facet/src/java/org/apache/lucene/facet/params/FacetIndexingParams.java
new file mode 100644
index 0000000..edd8ef9
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/params/FacetIndexingParams.java
@@ -0,0 +1,180 @@
+package org.apache.lucene.facet.params;
+
+import java.util.Collections;
+import java.util.List;
+
+import org.apache.lucene.facet.params.CategoryListParams.OrdinalPolicy;
+import org.apache.lucene.facet.search.FacetArrays;
+import org.apache.lucene.facet.taxonomy.CategoryPath;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Defines parameters that are needed for facets indexing. Note that this class
+ * does not have any setters. That's because overriding the default parameters
+ * is considered expert. If you wish to override them, simply extend this class
+ * and override the relevant getter.
+ * 
+ * <p>
+ * <b>NOTE:</b> This class is also used during faceted search in order to e.g.
+ * know which field holds the drill-down terms or the fulltree posting.
+ * Therefore this class should be initialized once and you should refrain from
+ * changing it. Also note that if you make any changes to it (e.g. suddenly
+ * deciding that drill-down terms should be read from a different field) and use
+ * it on an existing index, things may not work as expected.
+ * 
+ * @lucene.experimental
+ */
+public class FacetIndexingParams {
+  
+  // the default CLP, can be a singleton
+  protected static final CategoryListParams DEFAULT_CATEGORY_LIST_PARAMS = new CategoryListParams();
+
+  /**
+   * A {@link FacetIndexingParams} which fixes a single
+   * {@link CategoryListParams} with {@link OrdinalPolicy#ALL_PARENTS}.
+   */
+  public static final FacetIndexingParams ALL_PARENTS = new FacetIndexingParams();
+  
+  /**
+   * The default delimiter with which {@link CategoryPath#components} are
+   * concatenated when written to the index, e.g. as drill-down terms. If you
+   * choose to override it by overiding {@link #getFacetDelimChar()}, you should
+   * make sure that you return a character that's not found in any path
+   * component.
+   */
+  public static final char DEFAULT_FACET_DELIM_CHAR = '\uF749';
+  
+  private final int partitionSize = Integer.MAX_VALUE;
+
+  protected final CategoryListParams clParams;
+
+  /**
+   * Initializes new default params. You should use this constructor only if you
+   * intend to override any of the getters, otherwise you can use
+   * {@link #ALL_PARENTS} to save unnecessary object allocations.
+   */
+  public FacetIndexingParams() {
+    this(DEFAULT_CATEGORY_LIST_PARAMS);
+  }
+
+  /** Initializes new params with the given {@link CategoryListParams}. */
+  public FacetIndexingParams(CategoryListParams categoryListParams) {
+    clParams = categoryListParams;
+  }
+
+  /**
+   * Returns the {@link CategoryListParams} for this {@link CategoryPath}. The
+   * default implementation returns the same {@link CategoryListParams} for all
+   * categories (even if {@code category} is {@code null}).
+   * 
+   * @see PerDimensionIndexingParams
+   */
+  public CategoryListParams getCategoryListParams(CategoryPath category) {
+    return clParams;
+  }
+
+  /**
+   * Copies the text required to execute a drill-down query on the given
+   * category to the given {@code char[]}, and returns the number of characters
+   * that were written.
+   * <p>
+   * <b>NOTE:</b> You should make sure that the {@code char[]} is large enough,
+   * by e.g. calling {@link CategoryPath#fullPathLength()}.
+   */
+  public int drillDownTermText(CategoryPath path, char[] buffer) {
+    return path.copyFullPath(buffer, 0, getFacetDelimChar());
+  }
+  
+  /**
+   * Returns the size of a partition. <i>Partitions</i> allow you to divide
+   * (hence, partition) the categories space into small sets to e.g. improve RAM
+   * consumption during faceted search. For instance, {@code partitionSize=100K}
+   * would mean that if your taxonomy index contains 420K categories, they will
+   * be divided into 5 groups and at search time a {@link FacetArrays} will be
+   * allocated at the size of the partition.
+   * 
+   * <p>
+   * This is real advanced setting and should be changed with care. By default,
+   * all categories are put in one partition. You should modify this setting if
+   * you have really large taxonomies (e.g. 1M+ nodes).
+   */
+  public int getPartitionSize() {
+    return partitionSize;
+  }
+  
+  /**
+   * Returns a list of all {@link CategoryListParams categoryListParams} that
+   * are used for facets indexing.
+   */
+  public List<CategoryListParams> getAllCategoryListParams() {
+    return Collections.singletonList(clParams);
+  }
+
+  @Override
+  public int hashCode() {
+    final int prime = 31;
+    int result = 1;
+    result = prime * result + ((clParams == null) ? 0 : clParams.hashCode());
+    result = prime * result + partitionSize;
+    
+    for (CategoryListParams clp : getAllCategoryListParams()) {
+      result ^= clp.hashCode();
+    }
+    
+    return result;
+  }
+
+  @Override
+  public boolean equals(Object obj) {
+    if (this == obj) {
+      return true;
+    }
+    if (obj == null) {
+      return false;
+    }
+    if (!(obj instanceof FacetIndexingParams)) {
+      return false;
+    }
+    FacetIndexingParams other = (FacetIndexingParams) obj;
+    if (clParams == null) {
+      if (other.clParams != null) {
+        return false;
+      }
+    } else if (!clParams.equals(other.clParams)) {
+      return false;
+    }
+    if (partitionSize != other.partitionSize) {
+      return false;
+    }
+    
+    Iterable<CategoryListParams> cLs = getAllCategoryListParams();
+    Iterable<CategoryListParams> otherCLs = other.getAllCategoryListParams();
+    
+    return cLs.equals(otherCLs);
+  }
+
+  /**
+   * Returns the delimiter character used internally for concatenating category
+   * path components, e.g. for drill-down terms.
+   */
+  public char getFacetDelimChar() {
+    return DEFAULT_FACET_DELIM_CHAR;
+  }
+
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/params/FacetSearchParams.java b/lucene/facet/src/java/org/apache/lucene/facet/params/FacetSearchParams.java
new file mode 100644
index 0000000..1bf3dfe
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/params/FacetSearchParams.java
@@ -0,0 +1,97 @@
+package org.apache.lucene.facet.params;
+
+import java.util.Arrays;
+import java.util.List;
+
+import org.apache.lucene.facet.search.FacetRequest;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Defines parameters that are needed for faceted search. The list of
+ * {@link FacetRequest facet requests} denotes the facets for which aggregated
+ * should be done.
+ * <p>
+ * One can pass {@link FacetIndexingParams} in order to tell the search code how
+ * to read the facets information. Note that you must use the same
+ * {@link FacetIndexingParams} that were used for indexing.
+ * 
+ * @lucene.experimental
+ */
+public class FacetSearchParams {
+
+  public final FacetIndexingParams indexingParams;
+  public final List<FacetRequest> facetRequests;
+  
+  /**
+   * Initializes with the given {@link FacetRequest requests} and default
+   * {@link FacetIndexingParams#ALL_PARENTS}. If you used a different
+   * {@link FacetIndexingParams}, you should use
+   * {@link #FacetSearchParams(FacetIndexingParams, List)}.
+   */
+  public FacetSearchParams(FacetRequest... facetRequests) {
+    this(FacetIndexingParams.ALL_PARENTS, Arrays.asList(facetRequests));
+  }
+  
+  /**
+   * Initializes with the given {@link FacetRequest requests} and default
+   * {@link FacetIndexingParams#ALL_PARENTS}. If you used a different
+   * {@link FacetIndexingParams}, you should use
+   * {@link #FacetSearchParams(FacetIndexingParams, List)}.
+   */
+  public FacetSearchParams(List<FacetRequest> facetRequests) {
+    this(FacetIndexingParams.ALL_PARENTS, facetRequests);
+  }
+  
+  /**
+   * Initializes with the given {@link FacetRequest requests} and
+   * {@link FacetIndexingParams}.
+   */
+  public FacetSearchParams(FacetIndexingParams indexingParams, FacetRequest... facetRequests) {
+    this(indexingParams, Arrays.asList(facetRequests));
+  }
+
+  /**
+   * Initializes with the given {@link FacetRequest requests} and
+   * {@link FacetIndexingParams}.
+   */
+  public FacetSearchParams(FacetIndexingParams indexingParams, List<FacetRequest> facetRequests) {
+    if (facetRequests == null || facetRequests.size() == 0) {
+      throw new IllegalArgumentException("at least one FacetRequest must be defined");
+    }
+    this.facetRequests = facetRequests;
+    this.indexingParams = indexingParams;
+  }
+  
+  @Override
+  public String toString() {
+    final String INDENT = "  ";
+    final char NEWLINE = '\n';
+
+    StringBuilder sb = new StringBuilder("IndexingParams: ");
+    sb.append(NEWLINE).append(INDENT).append(indexingParams);
+    
+    sb.append(NEWLINE).append("FacetRequests:");
+    for (FacetRequest facetRequest : facetRequests) {
+      sb.append(NEWLINE).append(INDENT).append(facetRequest);
+    }
+    
+    return sb.toString();
+  }
+  
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/params/PerDimensionIndexingParams.java b/lucene/facet/src/java/org/apache/lucene/facet/params/PerDimensionIndexingParams.java
new file mode 100644
index 0000000..18251e7
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/params/PerDimensionIndexingParams.java
@@ -0,0 +1,96 @@
+package org.apache.lucene.facet.params;
+
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Map.Entry;
+
+import org.apache.lucene.facet.taxonomy.CategoryPath;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * A {@link FacetIndexingParams} that utilizes different category lists, defined
+ * by the dimension specified by a {@link CategoryPath category} (see
+ * {@link #PerDimensionIndexingParams(Map, CategoryListParams)}.
+ * <p>
+ * A 'dimension' is defined as the first or "zero-th" component in a
+ * {@link CategoryPath}. For example, if a category is defined as
+ * "Author/American/Mark Twain", then the dimension would be "Author".
+ * 
+ * @lucene.experimental
+ */
+public class PerDimensionIndexingParams extends FacetIndexingParams {
+
+  private final Map<String, CategoryListParams> clParamsMap;
+
+  /**
+   * Initializes a new instance with the given dimension-to-params mapping. The
+   * dimension is considered as what's returned by
+   * {@link CategoryPath#components cp.components[0]}.
+   * 
+   * <p>
+   * <b>NOTE:</b> for any dimension whose {@link CategoryListParams} is not
+   * defined in the mapping, a default {@link CategoryListParams} will be used.
+   * 
+   * @see #PerDimensionIndexingParams(Map, CategoryListParams)
+   */
+  public PerDimensionIndexingParams(Map<CategoryPath, CategoryListParams> paramsMap) {
+    this(paramsMap, DEFAULT_CATEGORY_LIST_PARAMS);
+  }
+
+  /**
+   * Same as {@link #PerDimensionIndexingParams(Map)}, only the given
+   * {@link CategoryListParams} will be used for any dimension that is not
+   * specified in the given mapping.
+   */
+  public PerDimensionIndexingParams(Map<CategoryPath, CategoryListParams> paramsMap, 
+      CategoryListParams categoryListParams) {
+    super(categoryListParams);
+    clParamsMap = new HashMap<String,CategoryListParams>();
+    for (Entry<CategoryPath, CategoryListParams> e : paramsMap.entrySet()) {
+      clParamsMap.put(e.getKey().components[0], e.getValue());
+    }
+  }
+
+  @Override
+  public List<CategoryListParams> getAllCategoryListParams() {
+    ArrayList<CategoryListParams> vals = new ArrayList<CategoryListParams>(clParamsMap.values());
+    vals.add(clParams); // add the default too
+    return vals;
+  }
+
+  /**
+   * Returns the {@link CategoryListParams} for the corresponding dimension
+   * which is returned by {@code category.getComponent(0)}. If {@code category}
+   * is {@code null}, or was not specified in the map given to the constructor,
+   * returns the default {@link CategoryListParams}.
+   */
+  @Override
+  public CategoryListParams getCategoryListParams(CategoryPath category) {
+    if (category != null) {
+      CategoryListParams clParams = clParamsMap.get(category.components[0]);
+      if (clParams != null) {
+        return clParams;
+      }
+    }
+    return clParams;
+  }
+
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/params/PerDimensionOrdinalPolicy.java b/lucene/facet/src/java/org/apache/lucene/facet/params/PerDimensionOrdinalPolicy.java
new file mode 100644
index 0000000..9446508
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/params/PerDimensionOrdinalPolicy.java
@@ -0,0 +1,55 @@
+package org.apache.lucene.facet.params;
+
+import java.util.Map;
+
+import org.apache.lucene.facet.taxonomy.CategoryPath;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * A {@link CategoryListParams} which allow controlling the
+ * {@link CategoryListParams.OrdinalPolicy} used for each dimension. The
+ * dimension is specified as the first component in
+ * {@link CategoryPath#components}.
+ */
+public class PerDimensionOrdinalPolicy extends CategoryListParams {
+
+  private final Map<String,OrdinalPolicy> policies;
+  private final OrdinalPolicy defaultOP;
+  
+  public PerDimensionOrdinalPolicy(Map<String,OrdinalPolicy> policies) {
+    this(policies, DEFAULT_ORDINAL_POLICY);
+  }
+  
+  public PerDimensionOrdinalPolicy(Map<String,OrdinalPolicy> policies, OrdinalPolicy defaultOP) {
+    this.defaultOP = defaultOP;
+    this.policies = policies;
+  }
+
+  @Override
+  public OrdinalPolicy getOrdinalPolicy(String dimension) {
+    OrdinalPolicy op = policies.get(dimension);
+    return op == null ? defaultOP : op;
+  }
+  
+  @Override
+  public String toString() {
+    return super.toString() + " policies=" + policies;
+  }
+  
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/params/package.html b/lucene/facet/src/java/org/apache/lucene/facet/params/package.html
new file mode 100644
index 0000000..dd2d8a4
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/params/package.html
@@ -0,0 +1,25 @@
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+<html>
+<head>
+<title>Facets indexing and search parameters</title>
+</head>
+<body>
+Facets indexing and search parameters. Define how facets are indexed 
+as well as which categories need to be aggregated.
+</body>
+</html>
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/partitions/IntermediateFacetResult.java b/lucene/facet/src/java/org/apache/lucene/facet/partitions/IntermediateFacetResult.java
new file mode 100644
index 0000000..5434c29
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/partitions/IntermediateFacetResult.java
@@ -0,0 +1,42 @@
+package org.apache.lucene.facet.partitions;
+
+import org.apache.lucene.facet.search.FacetRequest;
+import org.apache.lucene.facet.search.FacetResult;
+import org.apache.lucene.facet.search.FacetResultsHandler;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Intermediate {@link FacetResult} of faceted search.
+ * <p>
+ * This is an empty interface on purpose.
+ * <p>
+ * It allows {@link FacetResultsHandler} to return intermediate result objects 
+ * that only it knows how to interpret, and so the handler has maximal freedom
+ * in defining what an intermediate result is, depending on its specific logic.  
+ * 
+ * @lucene.experimental
+ */
+public interface IntermediateFacetResult {
+
+  /**
+   * Facet request for which this temporary result was created.
+   */
+  FacetRequest getFacetRequest();
+
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/partitions/PartitionsFacetResultsHandler.java b/lucene/facet/src/java/org/apache/lucene/facet/partitions/PartitionsFacetResultsHandler.java
new file mode 100644
index 0000000..c5ec23f
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/partitions/PartitionsFacetResultsHandler.java
@@ -0,0 +1,137 @@
+package org.apache.lucene.facet.partitions;
+
+import java.io.IOException;
+
+import org.apache.lucene.facet.search.FacetArrays;
+import org.apache.lucene.facet.search.FacetRequest;
+import org.apache.lucene.facet.search.FacetResult;
+import org.apache.lucene.facet.search.FacetResultNode;
+import org.apache.lucene.facet.search.FacetResultsHandler;
+import org.apache.lucene.facet.search.ScoredDocIDs;
+import org.apache.lucene.facet.search.StandardFacetsAccumulator;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * A {@link FacetResultsHandler} designed to work with facet partitions.
+ * 
+ * @lucene.experimental
+ */
+public abstract class PartitionsFacetResultsHandler extends FacetResultsHandler {
+  
+  public PartitionsFacetResultsHandler(TaxonomyReader taxonomyReader, FacetRequest facetRequest, 
+      FacetArrays facetArrays) {
+    super(taxonomyReader, facetRequest, facetArrays);
+  }
+
+
+  /**
+   * Fetch results of a single partition, given facet arrays for that partition,
+   * and based on the matching documents and faceted search parameters.
+   * @param offset
+   *          offset in input arrays where partition starts
+   * 
+   * @return temporary facet result, potentially, to be passed back to
+   *         <b>this</b> result handler for merging, or <b>null</b> in case that
+   *         constructor parameter, <code>facetRequest</code>, requests an
+   *         illegal FacetResult, like, e.g., a root node category path that
+   *         does not exist in constructor parameter <code>taxonomyReader</code>
+   *         .
+   * @throws IOException
+   *           on error
+   */
+  public abstract IntermediateFacetResult fetchPartitionResult(int offset) throws IOException;
+
+  /**
+   * Merge results of several facet partitions. Logic of the merge is undefined
+   * and open for interpretations. For example, a merge implementation could
+   * keep top K results. Passed {@link IntermediateFacetResult} must be ones
+   * that were created by this handler otherwise a {@link ClassCastException} is
+   * thrown. In addition, all passed {@link IntermediateFacetResult} must have
+   * the same {@link FacetRequest} otherwise an {@link IllegalArgumentException}
+   * is thrown.
+   * 
+   * @param tmpResults one or more temporary results created by <b>this</b>
+   *        handler.
+   * @return temporary facet result that represents to union, as specified by
+   *         <b>this</b> handler, of the input temporary facet results.
+   * @throws IOException on error.
+   * @throws ClassCastException if the temporary result passed was not created
+   *         by this handler
+   * @throws IllegalArgumentException if passed <code>facetResults</code> do not
+   *         have the same {@link FacetRequest}
+   * @see IntermediateFacetResult#getFacetRequest()
+   */
+  public abstract IntermediateFacetResult mergeResults(IntermediateFacetResult... tmpResults) throws IOException;
+
+  /**
+   * Create a facet result from the temporary result.
+   * @param tmpResult temporary result to be rendered as a {@link FacetResult}
+   * @throws IOException on error.
+   */
+  public abstract FacetResult renderFacetResult(IntermediateFacetResult tmpResult) throws IOException ;
+
+  /**
+   * Perform any rearrangement as required on a facet result that has changed after
+   * it was rendered.
+   * <P>
+   * Possible use case: a sampling facets accumulator invoked another 
+   * other facets accumulator on a sample set of documents, obtained
+   * rendered facet results, fixed their counts, and now it is needed 
+   * to sort the results differently according to the fixed counts. 
+   * @param facetResult result to be rearranged.
+   * @see FacetResultNode#value
+   */
+  public abstract FacetResult rearrangeFacetResult(FacetResult facetResult);
+
+  /**
+   * Label results according to settings in {@link FacetRequest}, such as
+   * {@link FacetRequest#getNumLabel()}. Usually invoked by
+   * {@link StandardFacetsAccumulator#accumulate(ScoredDocIDs)}
+   * 
+   * @param facetResult
+   *          facet result to be labeled.
+   * @throws IOException
+   *           on error
+   */
+  public abstract void labelResult(FacetResult facetResult) throws IOException;
+
+  /**
+   * Check if an array contains the partition which contains ordinal
+   * 
+   * @param ordinal
+   *          checked facet
+   * @param facetArrays
+   *          facet arrays for the certain partition
+   * @param offset
+   *          offset in input arrays where partition starts
+   */
+  protected boolean isSelfPartition (int ordinal, FacetArrays facetArrays, int offset) {
+    int partitionSize = facetArrays.arrayLength;
+    return ordinal / partitionSize == offset / partitionSize;
+  }
+  
+  @Override
+  public final FacetResult compute() throws IOException {
+    FacetResult res = renderFacetResult(fetchPartitionResult(0));
+    labelResult(res);
+    return res;
+  }
+
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/partitions/package.html b/lucene/facet/src/java/org/apache/lucene/facet/partitions/package.html
new file mode 100644
index 0000000..a4d8d62
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/partitions/package.html
@@ -0,0 +1,27 @@
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+<html>
+<head>
+<title>Category Partitions</title>
+</head>
+<body>
+<h1>Category Partitions</h1>
+
+Allows partitioning the category ordinals space, so that less RAM is consumed during search.
+Only meaningful for very large taxonomies (tens of millions of categories).
+</body>
+</html>
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/partitions/search/IntermediateFacetResult.java b/lucene/facet/src/java/org/apache/lucene/facet/partitions/search/IntermediateFacetResult.java
deleted file mode 100644
index 9cda7a5..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/partitions/search/IntermediateFacetResult.java
+++ /dev/null
@@ -1,42 +0,0 @@
-package org.apache.lucene.facet.partitions.search;
-
-import org.apache.lucene.facet.search.FacetResultsHandler;
-import org.apache.lucene.facet.search.params.FacetRequest;
-import org.apache.lucene.facet.search.results.FacetResult;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Intermediate {@link FacetResult} of faceted search.
- * <p>
- * This is an empty interface on purpose.
- * <p>
- * It allows {@link FacetResultsHandler} to return intermediate result objects 
- * that only it knows how to interpret, and so the handler has maximal freedom
- * in defining what an intermediate result is, depending on its specific logic.  
- * 
- * @lucene.experimental
- */
-public interface IntermediateFacetResult {
-
-  /**
-   * Facet request for which this temporary result was created.
-   */
-  FacetRequest getFacetRequest();
-
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/partitions/search/PartitionsFacetResultsHandler.java b/lucene/facet/src/java/org/apache/lucene/facet/partitions/search/PartitionsFacetResultsHandler.java
deleted file mode 100644
index 09a9ea6..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/partitions/search/PartitionsFacetResultsHandler.java
+++ /dev/null
@@ -1,137 +0,0 @@
-package org.apache.lucene.facet.partitions.search;
-
-import java.io.IOException;
-
-import org.apache.lucene.facet.search.FacetArrays;
-import org.apache.lucene.facet.search.FacetResultsHandler;
-import org.apache.lucene.facet.search.ScoredDocIDs;
-import org.apache.lucene.facet.search.StandardFacetsAccumulator;
-import org.apache.lucene.facet.search.params.FacetRequest;
-import org.apache.lucene.facet.search.results.FacetResult;
-import org.apache.lucene.facet.search.results.FacetResultNode;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * A {@link FacetResultsHandler} designed to work with facet partitions.
- * 
- * @lucene.experimental
- */
-public abstract class PartitionsFacetResultsHandler extends FacetResultsHandler {
-  
-  public PartitionsFacetResultsHandler(TaxonomyReader taxonomyReader, FacetRequest facetRequest, 
-      FacetArrays facetArrays) {
-    super(taxonomyReader, facetRequest, facetArrays);
-  }
-
-
-  /**
-   * Fetch results of a single partition, given facet arrays for that partition,
-   * and based on the matching documents and faceted search parameters.
-   * @param offset
-   *          offset in input arrays where partition starts
-   * 
-   * @return temporary facet result, potentially, to be passed back to
-   *         <b>this</b> result handler for merging, or <b>null</b> in case that
-   *         constructor parameter, <code>facetRequest</code>, requests an
-   *         illegal FacetResult, like, e.g., a root node category path that
-   *         does not exist in constructor parameter <code>taxonomyReader</code>
-   *         .
-   * @throws IOException
-   *           on error
-   */
-  public abstract IntermediateFacetResult fetchPartitionResult(int offset) throws IOException;
-
-  /**
-   * Merge results of several facet partitions. Logic of the merge is undefined
-   * and open for interpretations. For example, a merge implementation could
-   * keep top K results. Passed {@link IntermediateFacetResult} must be ones
-   * that were created by this handler otherwise a {@link ClassCastException} is
-   * thrown. In addition, all passed {@link IntermediateFacetResult} must have
-   * the same {@link FacetRequest} otherwise an {@link IllegalArgumentException}
-   * is thrown.
-   * 
-   * @param tmpResults one or more temporary results created by <b>this</b>
-   *        handler.
-   * @return temporary facet result that represents to union, as specified by
-   *         <b>this</b> handler, of the input temporary facet results.
-   * @throws IOException on error.
-   * @throws ClassCastException if the temporary result passed was not created
-   *         by this handler
-   * @throws IllegalArgumentException if passed <code>facetResults</code> do not
-   *         have the same {@link FacetRequest}
-   * @see IntermediateFacetResult#getFacetRequest()
-   */
-  public abstract IntermediateFacetResult mergeResults(IntermediateFacetResult... tmpResults) throws IOException;
-
-  /**
-   * Create a facet result from the temporary result.
-   * @param tmpResult temporary result to be rendered as a {@link FacetResult}
-   * @throws IOException on error.
-   */
-  public abstract FacetResult renderFacetResult(IntermediateFacetResult tmpResult) throws IOException ;
-
-  /**
-   * Perform any rearrangement as required on a facet result that has changed after
-   * it was rendered.
-   * <P>
-   * Possible use case: a sampling facets accumulator invoked another 
-   * other facets accumulator on a sample set of documents, obtained
-   * rendered facet results, fixed their counts, and now it is needed 
-   * to sort the results differently according to the fixed counts. 
-   * @param facetResult result to be rearranged.
-   * @see FacetResultNode#value
-   */
-  public abstract FacetResult rearrangeFacetResult(FacetResult facetResult);
-
-  /**
-   * Label results according to settings in {@link FacetRequest}, such as
-   * {@link FacetRequest#getNumLabel()}. Usually invoked by
-   * {@link StandardFacetsAccumulator#accumulate(ScoredDocIDs)}
-   * 
-   * @param facetResult
-   *          facet result to be labeled.
-   * @throws IOException
-   *           on error
-   */
-  public abstract void labelResult(FacetResult facetResult) throws IOException;
-
-  /**
-   * Check if an array contains the partition which contains ordinal
-   * 
-   * @param ordinal
-   *          checked facet
-   * @param facetArrays
-   *          facet arrays for the certain partition
-   * @param offset
-   *          offset in input arrays where partition starts
-   */
-  protected boolean isSelfPartition (int ordinal, FacetArrays facetArrays, int offset) {
-    int partitionSize = facetArrays.arrayLength;
-    return ordinal / partitionSize == offset / partitionSize;
-  }
-  
-  @Override
-  public final FacetResult compute() throws IOException {
-    FacetResult res = renderFacetResult(fetchPartitionResult(0));
-    labelResult(res);
-    return res;
-  }
-
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/partitions/search/package.html b/lucene/facet/src/java/org/apache/lucene/facet/partitions/search/package.html
deleted file mode 100644
index a4d8d62..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/partitions/search/package.html
+++ /dev/null
@@ -1,27 +0,0 @@
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-<title>Category Partitions</title>
-</head>
-<body>
-<h1>Category Partitions</h1>
-
-Allows partitioning the category ordinals space, so that less RAM is consumed during search.
-Only meaningful for very large taxonomies (tens of millions of categories).
-</body>
-</html>
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/sampling/RandomSampler.java b/lucene/facet/src/java/org/apache/lucene/facet/sampling/RandomSampler.java
new file mode 100644
index 0000000..7dae2db
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/sampling/RandomSampler.java
@@ -0,0 +1,71 @@
+package org.apache.lucene.facet.sampling;
+
+import java.io.IOException;
+import java.util.Random;
+
+import org.apache.lucene.facet.search.ScoredDocIDs;
+import org.apache.lucene.facet.search.ScoredDocIDsIterator;
+import org.apache.lucene.facet.util.ScoredDocIdsUtils;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Simple random sampler
+ */
+public class RandomSampler extends Sampler {
+  
+  private final Random random;
+
+  public RandomSampler() {
+    super();
+    this.random = new Random();
+  }
+
+  public RandomSampler(SamplingParams params, Random random) throws IllegalArgumentException {
+    super(params);
+    this.random = random;
+  }
+
+  @Override
+  protected SampleResult createSample(ScoredDocIDs docids, int actualSize, int sampleSetSize) throws IOException {
+    final int[] sample = new int[sampleSetSize];
+    final int maxStep = (actualSize * 2 ) / sampleSetSize; //floor
+    int remaining = actualSize;
+    ScoredDocIDsIterator it = docids.iterator();
+    int i = 0;
+    // select sample docs with random skipStep, make sure to leave sufficient #docs for selection after last skip
+    while (i<sample.length && remaining>(sampleSetSize-maxStep-i)) {
+      int skipStep = 1 + random.nextInt(maxStep);
+      // Skip over 'skipStep' documents
+      for (int j=0; j<skipStep; j++) {
+        it.next();
+        -- remaining;
+      }
+      sample[i++] = it.getDocID();
+    }
+    // Add leftover documents to the sample set
+    while (i<sample.length) {
+      it.next();
+      sample[i++] = it.getDocID();
+    }
+    ScoredDocIDs sampleRes = ScoredDocIdsUtils.createScoredDocIDsSubset(docids, sample);
+    SampleResult res = new SampleResult(sampleRes, sampleSetSize/(double)actualSize);
+    return res;
+  }
+  
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/sampling/RepeatableSampler.java b/lucene/facet/src/java/org/apache/lucene/facet/sampling/RepeatableSampler.java
new file mode 100644
index 0000000..f1ae6b7
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/sampling/RepeatableSampler.java
@@ -0,0 +1,406 @@
+package org.apache.lucene.facet.sampling;
+
+import java.io.IOException;
+import java.util.Arrays;
+import java.util.logging.Level;
+import java.util.logging.Logger;
+
+import org.apache.lucene.util.PriorityQueue;
+
+import org.apache.lucene.facet.search.ScoredDocIDs;
+import org.apache.lucene.facet.search.ScoredDocIDsIterator;
+import org.apache.lucene.facet.util.ScoredDocIdsUtils;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Take random samples of large collections.
+ * @lucene.experimental
+ */
+public class RepeatableSampler extends Sampler {
+
+  private static final Logger logger = Logger.getLogger(RepeatableSampler.class.getName());
+
+  public RepeatableSampler(SamplingParams params) {
+    super(params);
+  }
+  
+  @Override
+  protected SampleResult createSample(ScoredDocIDs docids, int actualSize,
+      int sampleSetSize) throws IOException {
+    int[] sampleSet = null;
+    try {
+      sampleSet = repeatableSample(docids, actualSize,
+          sampleSetSize);
+    } catch (IOException e) {
+      if (logger.isLoggable(Level.WARNING)) {
+        logger.log(Level.WARNING, "sampling failed: "+e.getMessage()+" - falling back to no sampling!", e);
+      }
+      return new SampleResult(docids, 1d);
+    }
+
+    ScoredDocIDs sampled = ScoredDocIdsUtils.createScoredDocIDsSubset(docids,
+        sampleSet);
+    if (logger.isLoggable(Level.FINEST)) {
+      logger.finest("******************** " + sampled.size());
+    }
+    return new SampleResult(sampled, sampled.size()/(double)docids.size());
+  }
+  
+  /**
+   * Returns <code>sampleSize</code> values from the first <code>collectionSize</code>
+   * locations of <code>collection</code>, chosen using
+   * the <code>TRAVERSAL</code> algorithm. The sample values are not sorted.
+   * @param collection The values from which a sample is wanted.
+   * @param collectionSize The number of values (from the first) from which to draw the sample.
+   * @param sampleSize The number of values to return.
+   * @return An array of values chosen from the collection.
+   * @see Algorithm#TRAVERSAL
+   */
+  private static int[] repeatableSample(ScoredDocIDs collection,
+      int collectionSize, int sampleSize)
+  throws IOException {
+    return repeatableSample(collection, collectionSize,
+        sampleSize, Algorithm.HASHING, Sorted.NO);
+  }
+
+  /**
+   * Returns <code>sampleSize</code> values from the first <code>collectionSize</code>
+   * locations of <code>collection</code>, chosen using <code>algorithm</code>.
+   * @param collection The values from which a sample is wanted.
+   * @param collectionSize The number of values (from the first) from which to draw the sample.
+   * @param sampleSize The number of values to return.
+   * @param algorithm Which algorithm to use.
+   * @param sorted Sorted.YES to sort the sample values in ascending order before returning;
+   * Sorted.NO to return them in essentially random order.
+   * @return An array of values chosen from the collection.
+   */
+  private static int[] repeatableSample(ScoredDocIDs collection,
+      int collectionSize, int sampleSize,
+      Algorithm algorithm, Sorted sorted)
+  throws IOException {
+    if (collection == null) {
+      throw new IOException("docIdSet is null");
+    }
+    if (sampleSize < 1) {
+      throw new IOException("sampleSize < 1 (" + sampleSize + ")");
+    }
+    if (collectionSize < sampleSize) {
+      throw new IOException("collectionSize (" + collectionSize + ") less than sampleSize (" + sampleSize + ")");
+    }
+    int[] sample = new int[sampleSize];
+    long[] times = new long[4];
+    if (algorithm == Algorithm.TRAVERSAL) {
+      sample1(collection, collectionSize, sample, times);
+    } else if (algorithm == Algorithm.HASHING) {
+      sample2(collection, collectionSize, sample, times);
+    } else {
+      throw new IllegalArgumentException("Invalid algorithm selection");
+    }
+    if (sorted == Sorted.YES) {
+      Arrays.sort(sample);
+    }
+    if (returnTimings) {
+      times[3] = System.currentTimeMillis();
+      if (logger.isLoggable(Level.FINEST)) {
+        logger.finest("Times: " + (times[1] - times[0]) + "ms, "
+            + (times[2] - times[1]) + "ms, " + (times[3] - times[2])+"ms");
+      }
+    }
+    return sample;
+  }
+
+  /**
+   * Returns <code>sample</code>.length values chosen from the first <code>collectionSize</code>
+   * locations of <code>collection</code>, using the TRAVERSAL algorithm. The sample is
+   * pseudorandom: no subset of the original collection
+   * is in principle more likely to occur than any other, but for a given collection
+   * and sample size, the same sample will always be returned. This algorithm walks the
+   * original collection in a methodical way that is guaranteed not to visit any location
+   * more than once, which makes sampling without replacement faster because removals don't
+   * have to be tracked, and the number of operations is proportional to the sample size,
+   * not the collection size.
+   * Times for performance measurement
+   * are returned in <code>times</code>, which must be an array of at least three longs, containing
+   * nanosecond event times. The first
+   * is set when the algorithm starts; the second, when the step size has been calculated;
+   * and the third when the sample has been taken.
+   * @param collection The set to be sampled.
+   * @param collectionSize The number of values to use (starting from first).
+   * @param sample The array in which to return the sample.
+   * @param times The times of three events, for measuring performance.
+   */
+  private static void sample1(ScoredDocIDs collection, int collectionSize, int[] sample, long[] times) 
+  throws IOException {
+    ScoredDocIDsIterator it = collection.iterator();
+    if (returnTimings) {
+      times[0] = System.currentTimeMillis();
+    }
+    int sampleSize = sample.length;
+    int prime = findGoodStepSize(collectionSize, sampleSize);
+    int mod = prime % collectionSize;
+    if (returnTimings) {
+      times[1] = System.currentTimeMillis();
+    }
+    int sampleCount = 0;
+    int index = 0;
+    for (; sampleCount < sampleSize;) {
+      if (index + mod < collectionSize) {
+        for (int i = 0; i < mod; i++, index++) {
+          it.next();
+        }
+      } else {
+        index = index + mod - collectionSize;
+        it = collection.iterator();
+        for (int i = 0; i < index; i++) {
+          it.next();
+        }
+      }
+      sample[sampleCount++] = it.getDocID();
+    }
+    if (returnTimings) {
+      times[2] = System.currentTimeMillis();
+    }
+  }
+
+  /**
+   * Returns a value which will allow the caller to walk
+   * a collection of <code>collectionSize</code> values, without repeating or missing
+   * any, and spanning the collection from beginning to end at least once with
+   * <code>sampleSize</code> visited locations. Choosing a value
+   * that is relatively prime to the collection size ensures that stepping by that size (modulo
+   * the collection size) will hit all locations without repeating, eliminating the need to
+   * track previously visited locations for a "without replacement" sample. Starting with the
+   * square root of the collection size ensures that either the first or second prime tried will
+   * work (they can't both divide the collection size). It also has the property that N steps of
+   * size N will span a collection of N**2 elements once. If the sample is bigger than N, it will
+   * wrap multiple times (without repeating). If the sample is smaller, a step size is chosen
+   * that will result in at least one spanning of the collection.
+   * 
+   * @param collectionSize The number of values in the collection to be sampled.
+   * @param sampleSize The number of values wanted in the sample.
+   * @return A good increment value for walking the collection.
+   */
+  private static int findGoodStepSize(int collectionSize, int sampleSize) {
+    int i = (int) Math.sqrt(collectionSize);
+    if (sampleSize < i) {
+      i = collectionSize / sampleSize;
+    }
+    do {
+      i = findNextPrimeAfter(i);
+    } while (collectionSize % i == 0);
+    return i;
+  }
+
+  /**
+   * Returns the first prime number that is larger than <code>n</code>.
+   * @param n A number less than the prime to be returned.
+   * @return The smallest prime larger than <code>n</code>.
+   */
+  private static int findNextPrimeAfter(int n) {
+    n += (n % 2 == 0) ? 1 : 2; // next odd
+    foundFactor: for (;; n += 2) { //TODO labels??!!
+      int sri = (int) (Math.sqrt(n));
+      for (int primeIndex = 0; primeIndex < N_PRIMES; primeIndex++) {
+        int p = primes[primeIndex];
+        if (p > sri) {
+          return n;
+        }
+        if (n % p == 0) {
+          continue foundFactor;
+        }
+      }
+      for (int p = primes[N_PRIMES - 1] + 2;; p += 2) {
+        if (p > sri) {
+          return n;
+        }
+        if (n % p == 0) {
+          continue foundFactor;
+        }
+      }
+    }
+  }
+
+  /**
+   * The first N_PRIMES primes, after 2.
+   */
+  private static final int N_PRIMES = 4000;
+  private static int[] primes = new int[N_PRIMES];
+  static {
+    primes[0] = 3;
+    for (int count = 1; count < N_PRIMES; count++) {
+      primes[count] = findNextPrimeAfter(primes[count - 1]);
+    }
+  }
+
+  /**
+   * Returns <code>sample</code>.length values chosen from the first <code>collectionSize</code>
+   * locations of <code>collection</code>, using the HASHING algorithm. Performance measurements
+   * are returned in <code>times</code>, which must be an array of at least three longs. The first
+   * will be set when the algorithm starts; the second, when a hash key has been calculated and
+   * inserted into the priority queue for every element in the collection; and the third when the
+   * original elements associated with the keys remaining in the PQ have been stored in the sample
+   * array for return.
+   * <P>
+   * This algorithm slows as the sample size becomes a significant fraction of the collection
+   * size, because the PQ is as large as the sample set, and will not do early rejection of values
+   * below the minimum until it fills up, and a larger PQ contains more small values to be purged,
+   * resulting in less early rejection and more logN insertions.
+   * 
+   * @param collection The set to be sampled.
+   * @param collectionSize The number of values to use (starting from first).
+   * @param sample The array in which to return the sample.
+   * @param times The times of three events, for measuring performance.
+   */
+  private static void sample2(ScoredDocIDs collection, int collectionSize, int[] sample, long[] times) 
+  throws IOException {
+    if (returnTimings) {
+      times[0] = System.currentTimeMillis();
+    }
+    int sampleSize = sample.length;
+    IntPriorityQueue pq = new IntPriorityQueue(sampleSize);
+    /*
+     * Convert every value in the collection to a hashed "weight" value, and insert
+     * into a bounded PQ (retains only sampleSize highest weights).
+     */
+    ScoredDocIDsIterator it = collection.iterator();
+    MI mi = null;
+    while (it.next()) {
+      if (mi == null) {
+        mi = new MI();
+      }
+      mi.value = (int) (it.getDocID() * PHI_32) & 0x7FFFFFFF;
+      mi = pq.insertWithOverflow(mi);
+    }
+    if (returnTimings) {
+      times[1] = System.currentTimeMillis();
+    }
+    /*
+     * Extract heap, convert weights back to original values, and return as integers.
+     */
+    Object[] heap = pq.getHeap();
+    for (int si = 0; si < sampleSize; si++) {
+      sample[si] = (int)(((MI) heap[si+1]).value * PHI_32I) & 0x7FFFFFFF;
+    }
+    if (returnTimings) {
+      times[2] = System.currentTimeMillis();
+    }
+  }
+  
+  /**
+   * A mutable integer that lets queue objects be reused once they start overflowing.
+   */
+  private static class MI {
+    MI() { }
+    public int value;
+  }
+
+  /**
+   * A bounded priority queue for Integers, to retain a specified number of
+   * the highest-weighted values for return as a random sample.
+   */
+  private static class IntPriorityQueue extends PriorityQueue<MI> {
+
+    /**
+     * Creates a bounded PQ of size <code>size</code>.
+     * @param size The number of elements to retain.
+     */
+    public IntPriorityQueue(int size) {
+      super(size);
+    }
+
+    /**
+     * Returns the underlying data structure for faster access. Extracting elements
+     * one at a time would require N logN time, and since we want the elements sorted
+     * in ascending order by value (not weight), the array is useful as-is.
+     * @return The underlying heap array.
+     */
+    public Object[] getHeap() {
+      return getHeapArray();
+    }
+
+    /**
+     * Returns true if <code>o1<code>'s weight is less than that of <code>o2</code>, for
+     * ordering in the PQ.
+     * @return True if <code>o1</code> weighs less than <code>o2</code>.
+     */
+    @Override
+    public boolean lessThan(MI o1, MI o2) {
+      return o1.value < o2.value;
+    }
+
+  }
+
+  /**
+   * For specifying which sampling algorithm to use.
+   */
+  private enum Algorithm {
+
+    /**
+     * Specifies a methodical traversal algorithm, which is guaranteed to span the collection
+     * at least once, and never to return duplicates. Faster than the hashing algorithm and
+     * uses much less space, but the randomness of the sample may be affected by systematic
+     * variations in the collection. Requires only an array for the sample, and visits only
+     * the number of elements in the sample set, not the full set.
+     */
+    // TODO (Facet): This one produces a bimodal distribution (very flat around
+    // each peak!) for collection size 10M and sample sizes 10k and 10544.
+    // Figure out why.
+    TRAVERSAL,
+
+    /**
+     * Specifies a Fibonacci-style hash algorithm (see Knuth, S&S), which generates a less
+     * systematically distributed subset of the sampled collection than the traversal method,
+     * but requires a bounded priority queue the size of the sample, and creates an object
+     * containing a sampled value and its hash, for every element in the full set. 
+     */
+    HASHING
+  }
+
+  /**
+   * For specifying whether to sort the sample.
+   */
+  private enum Sorted {
+
+    /**
+     * Sort resulting sample before returning.
+     */
+    YES,
+
+    /**
+     *Do not sort the resulting sample. 
+     */
+    NO
+  }
+
+  /**
+   * Magic number 1: prime closest to phi, in 32 bits.
+   */
+  private static final long PHI_32 = 2654435769L;
+
+  /**
+   * Magic number 2: multiplicative inverse of PHI_32, modulo 2**32.
+   */
+  private static final long PHI_32I = 340573321L;
+
+  /**
+   * Switch to cause methods to return timings.
+   */
+  private static boolean returnTimings = false;
+
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/sampling/SampleFixer.java b/lucene/facet/src/java/org/apache/lucene/facet/sampling/SampleFixer.java
new file mode 100644
index 0000000..a3305f8
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/sampling/SampleFixer.java
@@ -0,0 +1,44 @@
+package org.apache.lucene.facet.sampling;
+
+import java.io.IOException;
+
+import org.apache.lucene.facet.search.FacetResult;
+import org.apache.lucene.facet.search.ScoredDocIDs;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Fixer of sample facet accumulation results
+ * 
+ * @lucene.experimental
+ */
+public interface SampleFixer {
+  
+  /**
+   * Alter the input result, fixing it to account for the sampling. This
+   * implementation can compute accurate or estimated counts for the sampled facets. 
+   * For example, a faster correction could just multiply by a compensating factor.
+   * 
+   * @param origDocIds
+   *          full set of matching documents.
+   * @param fres
+   *          sample result to be fixed.
+   * @throws IOException If there is a low-level I/O error.
+   */
+  public void fixResult(ScoredDocIDs origDocIds, FacetResult fres) throws IOException; 
+}
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/sampling/Sampler.java b/lucene/facet/src/java/org/apache/lucene/facet/sampling/Sampler.java
new file mode 100644
index 0000000..7a8f4ea
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/sampling/Sampler.java
@@ -0,0 +1,246 @@
+package org.apache.lucene.facet.sampling;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.lucene.facet.params.FacetSearchParams;
+import org.apache.lucene.facet.search.Aggregator;
+import org.apache.lucene.facet.search.FacetArrays;
+import org.apache.lucene.facet.search.FacetRequest;
+import org.apache.lucene.facet.search.FacetResult;
+import org.apache.lucene.facet.search.FacetResultNode;
+import org.apache.lucene.facet.search.ScoredDocIDs;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.index.IndexReader;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Sampling definition for facets accumulation
+ * <p>
+ * The Sampler uses TAKMI style counting to provide a 'best guess' top-K result
+ * set of the facets accumulated.
+ * <p>
+ * Note: Sampling accumulation (Accumulation over a sampled-set of the results),
+ * does not guarantee accurate values for
+ * {@link FacetResult#getNumValidDescendants()}.
+ * 
+ * @lucene.experimental
+ */
+public abstract class Sampler {
+
+  protected final SamplingParams samplingParams;
+  
+  /**
+   * Construct with {@link SamplingParams}
+   */
+  public Sampler() {
+    this(new SamplingParams()); 
+  }
+  
+  /**
+   * Construct with certain {@link SamplingParams}
+   * 
+   * @param params sampling params in effect
+   * @throws IllegalArgumentException if the provided SamplingParams are not valid 
+   */
+  public Sampler(SamplingParams params) throws IllegalArgumentException {
+    if (!params.validate()) {
+      throw new IllegalArgumentException("The provided SamplingParams are not valid!!");
+    }
+    this.samplingParams = params;
+  }
+
+  /**
+   * Check if this sampler would complement for the input docIds
+   */
+  public boolean shouldSample(ScoredDocIDs docIds) {
+    return docIds.size() > samplingParams.getSamplingThreshold();
+  }
+  
+  /**
+   * Compute a sample set out of the input set, based on the {@link SamplingParams#getSampleRatio()}
+   * in effect. Sub classes can override to alter how the sample set is
+   * computed.
+   * <p> 
+   * If the input set is of size smaller than {@link SamplingParams#getMinSampleSize()}, 
+   * the input set is returned (no sampling takes place).
+   * <p>
+   * Other than that, the returned set size will not be larger than {@link SamplingParams#getMaxSampleSize()} 
+   * nor smaller than {@link SamplingParams#getMinSampleSize()}.  
+   * @param docids
+   *          full set of matching documents out of which a sample is needed.
+   */
+  public SampleResult getSampleSet(ScoredDocIDs docids) throws IOException {
+    if (!shouldSample(docids)) {
+      return new SampleResult(docids, 1d);
+    }
+
+    int actualSize = docids.size();
+    int sampleSetSize = (int) (actualSize * samplingParams.getSampleRatio());
+    sampleSetSize = Math.max(sampleSetSize, samplingParams.getMinSampleSize());
+    sampleSetSize = Math.min(sampleSetSize, samplingParams.getMaxSampleSize());
+
+    return createSample(docids, actualSize, sampleSetSize);
+  }
+
+  /**
+   * Create and return a sample of the input set
+   * @param docids input set out of which a sample is to be created 
+   * @param actualSize original size of set, prior to sampling
+   * @param sampleSetSize required size of sample set
+   * @return sample of the input set in the required size
+   */
+  protected abstract SampleResult createSample(ScoredDocIDs docids, int actualSize, int sampleSetSize) 
+      throws IOException;
+
+  /**
+   * Get a fixer of sample facet accumulation results. Default implementation
+   * returns a <code>TakmiSampleFixer</code> which is adequate only for
+   * counting. For any other accumulator, provide a different fixer.
+   */
+  public SampleFixer getSampleFixer(IndexReader indexReader, TaxonomyReader taxonomyReader,
+      FacetSearchParams searchParams) {
+    return new TakmiSampleFixer(indexReader, taxonomyReader, searchParams);
+  }
+  
+  /**
+   * Result of sample computation
+   */
+  public final static class SampleResult {
+    public final ScoredDocIDs docids;
+    public final double actualSampleRatio;
+    protected SampleResult(ScoredDocIDs docids, double actualSampleRatio) {
+      this.docids = docids;
+      this.actualSampleRatio = actualSampleRatio;
+    }
+  }
+  
+  /**
+   * Return the sampling params in effect
+   */
+  public final SamplingParams getSamplingParams() {
+    return samplingParams;
+  }
+
+  /**
+   * Trim the input facet result.<br>
+   * Note: It is only valid to call this method with result obtained for a
+   * facet request created through {@link #overSampledSearchParams(FacetSearchParams)}.
+   * 
+   * @throws IllegalArgumentException
+   *             if called with results not obtained for requests created
+   *             through {@link #overSampledSearchParams(FacetSearchParams)}
+   */
+  public FacetResult trimResult(FacetResult facetResult) throws IllegalArgumentException {
+    double overSampleFactor = getSamplingParams().getOversampleFactor();
+    if (overSampleFactor <= 1) { // no factoring done?
+      return facetResult;
+    }
+    
+    OverSampledFacetRequest sampledFreq = null;
+    
+    try {
+      sampledFreq = (OverSampledFacetRequest) facetResult.getFacetRequest();
+    } catch (ClassCastException e) {
+      throw new IllegalArgumentException(
+          "It is only valid to call this method with result obtained for a " +
+          "facet request created through sampler.overSamlpingSearchParams()",
+          e);
+    }
+    
+    FacetRequest origFrq = sampledFreq.orig;
+
+    FacetResultNode trimmedRootNode = facetResult.getFacetResultNode();
+    trimSubResults(trimmedRootNode, origFrq.numResults);
+    
+    return new FacetResult(origFrq, trimmedRootNode, facetResult.getNumValidDescendants());
+  }
+  
+  /** Trim sub results to a given size. */
+  private void trimSubResults(FacetResultNode node, int size) {
+    if (node.subResults == FacetResultNode.EMPTY_SUB_RESULTS || node.subResults.size() == 0) {
+      return;
+    }
+
+    ArrayList<FacetResultNode> trimmed = new ArrayList<FacetResultNode>(size);
+    for (int i = 0; i < node.subResults.size() && i < size; i++) {
+      FacetResultNode trimmedNode = node.subResults.get(i);
+      trimSubResults(trimmedNode, size);
+      trimmed.add(trimmedNode);
+    }
+    
+    node.subResults = trimmed;
+  }
+
+  /**
+   * Over-sampled search params, wrapping each request with an over-sampled one.
+   */
+  public FacetSearchParams overSampledSearchParams(FacetSearchParams original) {
+    FacetSearchParams res = original;
+    // So now we can sample -> altering the searchParams to accommodate for the statistical error for the sampling
+    double overSampleFactor = getSamplingParams().getOversampleFactor();
+    if (overSampleFactor > 1) { // any factoring to do?
+      List<FacetRequest> facetRequests = new ArrayList<FacetRequest>();
+      for (FacetRequest frq : original.facetRequests) {
+        int overSampledNumResults = (int) Math.ceil(frq.numResults * overSampleFactor);
+        facetRequests.add(new OverSampledFacetRequest(frq, overSampledNumResults));
+      }
+      res = new FacetSearchParams(original.indexingParams, facetRequests);
+    }
+    return res;
+  }
+  
+  /**
+   * Wrapping a facet request for over sampling.
+   * Implementation detail: even if the original request is a count request, no 
+   * statistics will be computed for it as the wrapping is not a count request.
+   * This is ok, as the sampling accumulator is later computing the statistics
+   * over the original requests.
+   */
+  private static class OverSampledFacetRequest extends FacetRequest {
+    final FacetRequest orig;
+    public OverSampledFacetRequest(FacetRequest orig, int num) {
+      super(orig.categoryPath, num);
+      this.orig = orig;
+      setDepth(orig.getDepth());
+      setNumLabel(orig.getNumLabel());
+      setResultMode(orig.getResultMode());
+      setSortBy(orig.getSortBy());
+      setSortOrder(orig.getSortOrder());
+    }
+    
+    @Override
+    public Aggregator createAggregator(boolean useComplements, FacetArrays arrays, TaxonomyReader taxonomy) 
+        throws IOException {
+      return orig.createAggregator(useComplements, arrays, taxonomy);
+    }
+
+    @Override
+    public FacetArraysSource getFacetArraysSource() {
+      return orig.getFacetArraysSource();
+    }
+
+    @Override
+    public double getValueOf(FacetArrays arrays, int idx) {
+      return orig.getValueOf(arrays, idx);
+    }
+  }
+
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/sampling/SamplingAccumulator.java b/lucene/facet/src/java/org/apache/lucene/facet/sampling/SamplingAccumulator.java
new file mode 100644
index 0000000..54329e6
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/sampling/SamplingAccumulator.java
@@ -0,0 +1,122 @@
+package org.apache.lucene.facet.sampling;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.lucene.facet.params.FacetSearchParams;
+import org.apache.lucene.facet.partitions.PartitionsFacetResultsHandler;
+import org.apache.lucene.facet.sampling.Sampler.SampleResult;
+import org.apache.lucene.facet.search.FacetArrays;
+import org.apache.lucene.facet.search.FacetResult;
+import org.apache.lucene.facet.search.FacetsAccumulator;
+import org.apache.lucene.facet.search.ScoredDocIDs;
+import org.apache.lucene.facet.search.StandardFacetsAccumulator;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.index.IndexReader;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Facets accumulation with sampling.<br>
+ * <p>
+ * Note two major differences between this class and {@link SamplingWrapper}:
+ * <ol>
+ * <li>Latter can wrap any other {@link FacetsAccumulator} while this class
+ * directly extends {@link StandardFacetsAccumulator}.</li>
+ * <li>This class can effectively apply sampling on the complement set of
+ * matching document, thereby working efficiently with the complement
+ * optimization - see {@link StandardFacetsAccumulator#getComplementThreshold()}
+ * .</li>
+ * </ol>
+ * <p>
+ * Note: Sampling accumulation (Accumulation over a sampled-set of the results),
+ * does not guarantee accurate values for
+ * {@link FacetResult#getNumValidDescendants()}.
+ * 
+ * @see Sampler
+ * @lucene.experimental
+ */
+public class SamplingAccumulator extends StandardFacetsAccumulator {
+  
+  private double samplingRatio = -1d;
+  private final Sampler sampler;
+  
+  public SamplingAccumulator(Sampler sampler, FacetSearchParams searchParams,
+      IndexReader indexReader, TaxonomyReader taxonomyReader,
+      FacetArrays facetArrays) {
+    super(searchParams, indexReader, taxonomyReader, facetArrays);
+    this.sampler = sampler;
+  }
+
+  /**
+   * Constructor...
+   */
+  public SamplingAccumulator(
+      Sampler sampler,
+      FacetSearchParams searchParams,
+      IndexReader indexReader, TaxonomyReader taxonomyReader) {
+    super(searchParams, indexReader, taxonomyReader);
+    this.sampler = sampler;
+  }
+
+  @Override
+  public List<FacetResult> accumulate(ScoredDocIDs docids) throws IOException {
+    // Replacing the original searchParams with the over-sampled
+    FacetSearchParams original = searchParams;
+    searchParams = sampler.overSampledSearchParams(original);
+    
+    List<FacetResult> sampleRes = super.accumulate(docids);
+    
+    List<FacetResult> fixedRes = new ArrayList<FacetResult>();
+    for (FacetResult fres : sampleRes) {
+      // for sure fres is not null because this is guaranteed by the delegee.
+      PartitionsFacetResultsHandler frh = createFacetResultsHandler(fres.getFacetRequest());
+      // fix the result of current request
+      sampler.getSampleFixer(indexReader, taxonomyReader, searchParams).fixResult(docids, fres);
+      
+      fres = frh.rearrangeFacetResult(fres); // let delegee's handler do any arranging it needs to
+
+      // Using the sampler to trim the extra (over-sampled) results
+      fres = sampler.trimResult(fres);
+
+      // final labeling if allowed (because labeling is a costly operation)
+      frh.labelResult(fres);
+      fixedRes.add(fres); // add to final results
+    }
+    
+    searchParams = original; // Back to original params
+    
+    return fixedRes; 
+  }
+
+  @Override
+  protected ScoredDocIDs actualDocsToAccumulate(ScoredDocIDs docids) throws IOException {
+    SampleResult sampleRes = sampler.getSampleSet(docids);
+    samplingRatio = sampleRes.actualSampleRatio;
+    return sampleRes.docids;
+  }
+  
+  @Override
+  protected double getTotalCountsFactor() {
+    if (samplingRatio<0) {
+      throw new IllegalStateException("Total counts ratio unavailable because actualDocsToAccumulate() was not invoked");
+    }
+    return samplingRatio;
+  }
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/sampling/SamplingParams.java b/lucene/facet/src/java/org/apache/lucene/facet/sampling/SamplingParams.java
new file mode 100644
index 0000000..4366d57
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/sampling/SamplingParams.java
@@ -0,0 +1,169 @@
+package org.apache.lucene.facet.sampling;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Parameters for sampling, dictating whether sampling is to take place and how. 
+ * 
+ * @lucene.experimental
+ */
+public class SamplingParams {
+
+  /**
+   * Default factor by which more results are requested over the sample set.
+   * @see SamplingParams#getOversampleFactor()
+   */
+  public static final double DEFAULT_OVERSAMPLE_FACTOR = 2d;
+  
+  /**
+   * Default ratio between size of sample to original size of document set.
+   * @see Sampler#getSampleSet(org.apache.lucene.facet.search.ScoredDocIDs)
+   */
+  public static final double DEFAULT_SAMPLE_RATIO = 0.01;
+  
+  /**
+   * Default maximum size of sample.
+   * @see Sampler#getSampleSet(org.apache.lucene.facet.search.ScoredDocIDs)
+   */
+  public static final int DEFAULT_MAX_SAMPLE_SIZE = 10000;
+  
+  /**
+   * Default minimum size of sample.
+   * @see Sampler#getSampleSet(org.apache.lucene.facet.search.ScoredDocIDs)
+   */
+  public static final int DEFAULT_MIN_SAMPLE_SIZE = 100;
+  
+  /**
+   * Default sampling threshold, if number of results is less than this number - no sampling will take place
+   * @see SamplingParams#getSampleRatio()
+   */
+  public static final int DEFAULT_SAMPLING_THRESHOLD = 75000;
+
+  private int maxSampleSize = DEFAULT_MAX_SAMPLE_SIZE;
+  private int minSampleSize = DEFAULT_MIN_SAMPLE_SIZE;
+  private double sampleRatio = DEFAULT_SAMPLE_RATIO;
+  private int samplingThreshold = DEFAULT_SAMPLING_THRESHOLD;
+  private double oversampleFactor = DEFAULT_OVERSAMPLE_FACTOR;
+  
+  /**
+   * Return the maxSampleSize.
+   * In no case should the resulting sample size exceed this value.  
+   * @see Sampler#getSampleSet(org.apache.lucene.facet.search.ScoredDocIDs)
+   */
+  public final int getMaxSampleSize() {
+    return maxSampleSize;
+  }
+
+  /**
+   * Return the minSampleSize.
+   * In no case should the resulting sample size be smaller than this value.  
+   * @see Sampler#getSampleSet(org.apache.lucene.facet.search.ScoredDocIDs)
+   */
+  public final int getMinSampleSize() {
+    return minSampleSize;
+  }
+
+  /**
+   * @return the sampleRatio
+   * @see Sampler#getSampleSet(org.apache.lucene.facet.search.ScoredDocIDs)
+   */
+  public final double getSampleRatio() {
+    return sampleRatio;
+  }
+  
+  /**
+   * Return the samplingThreshold.
+   * Sampling would be performed only for document sets larger than this.  
+   */
+  public final int getSamplingThreshold() {
+    return samplingThreshold;
+  }
+
+  /**
+   * @param maxSampleSize
+   *          the maxSampleSize to set
+   * @see #getMaxSampleSize()
+   */
+  public void setMaxSampleSize(int maxSampleSize) {
+    this.maxSampleSize = maxSampleSize;
+  }
+
+  /**
+   * @param minSampleSize
+   *          the minSampleSize to set
+   * @see #getMinSampleSize()
+   */
+  public void setMinSampleSize(int minSampleSize) {
+    this.minSampleSize = minSampleSize;
+  }
+
+  /**
+   * @param sampleRatio
+   *          the sampleRatio to set
+   * @see #getSampleRatio()
+   */
+  public void setSampleRatio(double sampleRatio) {
+    this.sampleRatio = sampleRatio;
+  }
+
+  /**
+   * Set a sampling-threshold
+   * @see #getSamplingThreshold()
+   */
+  public void setSamplingThreshold(int samplingThreshold) {
+    this.samplingThreshold = samplingThreshold;
+  }
+
+  /**
+   * Check validity of sampling settings, making sure that
+   * <ul>
+   * <li> <code>minSampleSize <= maxSampleSize <= samplingThreshold </code></li>
+   * <li> <code>0 < samplingRatio <= 1 </code></li>
+   * </ul> 
+   * 
+   * @return true if valid, false otherwise
+   */
+  public boolean validate() {
+    return 
+      samplingThreshold >= maxSampleSize && 
+      maxSampleSize >= minSampleSize && 
+      sampleRatio > 0 &&
+      sampleRatio < 1;
+  }
+
+  /**
+   * Return the oversampleFactor. When sampling, we would collect that much more
+   * results, so that later, when selecting top out of these, chances are higher
+   * to get actual best results. Note that having this value larger than 1 only
+   * makes sense when using a SampleFixer which finds accurate results, such as
+   * <code>TakmiSampleFixer</code>. When this value is smaller than 1, it is
+   * ignored and no oversampling takes place.
+   */
+  public final double getOversampleFactor() {
+    return oversampleFactor;
+  }
+
+  /**
+   * @param oversampleFactor the oversampleFactor to set
+   * @see #getOversampleFactor()
+   */
+  public void setOversampleFactor(double oversampleFactor) {
+    this.oversampleFactor = oversampleFactor;
+  }
+
+}
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/sampling/SamplingWrapper.java b/lucene/facet/src/java/org/apache/lucene/facet/sampling/SamplingWrapper.java
new file mode 100644
index 0000000..829c671
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/sampling/SamplingWrapper.java
@@ -0,0 +1,92 @@
+package org.apache.lucene.facet.sampling;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.lucene.facet.params.FacetSearchParams;
+import org.apache.lucene.facet.partitions.PartitionsFacetResultsHandler;
+import org.apache.lucene.facet.sampling.Sampler.SampleResult;
+import org.apache.lucene.facet.search.FacetResult;
+import org.apache.lucene.facet.search.ScoredDocIDs;
+import org.apache.lucene.facet.search.StandardFacetsAccumulator;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Wrap any Facets Accumulator with sampling.
+ * <p>
+ * Note: Sampling accumulation (Accumulation over a sampled-set of the results),
+ * does not guarantee accurate values for
+ * {@link FacetResult#getNumValidDescendants()}.
+ * 
+ * @lucene.experimental
+ */
+public class SamplingWrapper extends StandardFacetsAccumulator {
+
+  private StandardFacetsAccumulator delegee;
+  private Sampler sampler;
+
+  public SamplingWrapper(StandardFacetsAccumulator delegee, Sampler sampler) {
+    super(delegee.searchParams, delegee.indexReader, delegee.taxonomyReader);
+    this.delegee = delegee;
+    this.sampler = sampler;
+  }
+
+  @Override
+  public List<FacetResult> accumulate(ScoredDocIDs docids) throws IOException {
+    // Replacing the original searchParams with the over-sampled (and without statistics-compute)
+    FacetSearchParams original = delegee.searchParams;
+    delegee.searchParams = sampler.overSampledSearchParams(original);
+    
+    SampleResult sampleSet = sampler.getSampleSet(docids);
+
+    List<FacetResult> sampleRes = delegee.accumulate(sampleSet.docids);
+
+    List<FacetResult> fixedRes = new ArrayList<FacetResult>();
+    for (FacetResult fres : sampleRes) {
+      // for sure fres is not null because this is guaranteed by the delegee.
+      PartitionsFacetResultsHandler frh = createFacetResultsHandler(fres.getFacetRequest());
+      // fix the result of current request
+      sampler.getSampleFixer(indexReader, taxonomyReader, searchParams).fixResult(docids, fres); 
+      fres = frh.rearrangeFacetResult(fres); // let delegee's handler do any
+      
+      // Using the sampler to trim the extra (over-sampled) results
+      fres = sampler.trimResult(fres);
+      
+      // final labeling if allowed (because labeling is a costly operation)
+      frh.labelResult(fres);
+      fixedRes.add(fres); // add to final results
+    }
+
+    delegee.searchParams = original; // Back to original params
+    
+    return fixedRes; 
+  }
+
+  @Override
+  public double getComplementThreshold() {
+    return delegee.getComplementThreshold();
+  }
+
+  @Override
+  public void setComplementThreshold(double complementThreshold) {
+    delegee.setComplementThreshold(complementThreshold);
+  }
+
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/sampling/TakmiSampleFixer.java b/lucene/facet/src/java/org/apache/lucene/facet/sampling/TakmiSampleFixer.java
new file mode 100644
index 0000000..0b3e3f1
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/sampling/TakmiSampleFixer.java
@@ -0,0 +1,182 @@
+package org.apache.lucene.facet.sampling;
+
+import java.io.IOException;
+
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.MultiFields;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.search.DocIdSetIterator;
+import org.apache.lucene.util.Bits;
+
+import org.apache.lucene.facet.params.FacetSearchParams;
+import org.apache.lucene.facet.search.DrillDown;
+import org.apache.lucene.facet.search.FacetResult;
+import org.apache.lucene.facet.search.FacetResultNode;
+import org.apache.lucene.facet.search.ScoredDocIDs;
+import org.apache.lucene.facet.search.ScoredDocIDsIterator;
+import org.apache.lucene.facet.taxonomy.CategoryPath;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Fix sampling results by counting the intersection between two lists: a
+ * TermDocs (list of documents in a certain category) and a DocIdSetIterator
+ * (list of documents matching the query).
+ * 
+ * 
+ * @lucene.experimental
+ */
+// TODO (Facet): implement also an estimated fixing by ratio (taking into
+// account "translation" of counts!)
+class TakmiSampleFixer implements SampleFixer {
+  
+  private TaxonomyReader taxonomyReader;
+  private IndexReader indexReader;
+  private FacetSearchParams searchParams;
+  
+  public TakmiSampleFixer(IndexReader indexReader,
+      TaxonomyReader taxonomyReader, FacetSearchParams searchParams) {
+    this.indexReader = indexReader;
+    this.taxonomyReader = taxonomyReader;
+    this.searchParams = searchParams;
+  }
+
+  @Override
+  public void fixResult(ScoredDocIDs origDocIds, FacetResult fres)
+      throws IOException {
+    FacetResultNode topRes = fres.getFacetResultNode();
+    fixResultNode(topRes, origDocIds);
+  }
+  
+  /**
+   * Fix result node count, and, recursively, fix all its children
+   * 
+   * @param facetResNode
+   *          result node to be fixed
+   * @param docIds
+   *          docids in effect
+   * @throws IOException If there is a low-level I/O error.
+   */
+  private void fixResultNode(FacetResultNode facetResNode, ScoredDocIDs docIds) throws IOException {
+    recount(facetResNode, docIds);
+    for (FacetResultNode frn : facetResNode.subResults) {
+      fixResultNode(frn, docIds);
+    }
+  }
+
+  /**
+   * Internal utility: recount for a facet result node
+   * 
+   * @param fresNode
+   *          result node to be recounted
+   * @param docIds
+   *          full set of matching documents.
+   * @throws IOException If there is a low-level I/O error.
+   */
+  private void recount(FacetResultNode fresNode, ScoredDocIDs docIds) throws IOException {
+    // TODO (Facet): change from void to return the new, smaller docSet, and use
+    // that for the children, as this will make their intersection ops faster.
+    // can do this only when the new set is "sufficiently" smaller.
+    
+    /* We need the category's path name in order to do its recounting.
+     * If it is missing, because the option to label only part of the
+     * facet results was exercise, we need to calculate them anyway, so
+     * in essence sampling with recounting spends some extra cycles for
+     * labeling results for which labels are not required. */
+    if (fresNode.label == null) {
+      fresNode.label = taxonomyReader.getPath(fresNode.ordinal);
+    }
+    CategoryPath catPath = fresNode.label;
+
+    Term drillDownTerm = DrillDown.term(searchParams, catPath);
+    // TODO (Facet): avoid Multi*?
+    Bits liveDocs = MultiFields.getLiveDocs(indexReader);
+    int updatedCount = countIntersection(MultiFields.getTermDocsEnum(indexReader, liveDocs,
+                                                                     drillDownTerm.field(), drillDownTerm.bytes(),
+                                                                     0), docIds.iterator());
+    fresNode.value = updatedCount;
+  }
+
+  /**
+   * Count the size of the intersection between two lists: a TermDocs (list of
+   * documents in a certain category) and a DocIdSetIterator (list of documents
+   * matching a query).
+   */
+  private static int countIntersection(DocsEnum p1, ScoredDocIDsIterator p2)
+      throws IOException {
+    // The documentation of of both TermDocs and DocIdSetIterator claim
+    // that we must do next() before doc(). So we do, and if one of the
+    // lists is empty, obviously return 0;
+    if (p1 == null || p1.nextDoc() == DocIdSetIterator.NO_MORE_DOCS) {
+      return 0;
+    }
+    if (!p2.next()) {
+      return 0;
+    }
+    
+    int d1 = p1.docID();
+    int d2 = p2.getDocID();
+
+    int count = 0;
+    for (;;) {
+      if (d1 == d2) {
+        ++count;
+        if (p1.nextDoc() == DocIdSetIterator.NO_MORE_DOCS) {
+          break; // end of list 1, nothing more in intersection
+        }
+        d1 = p1.docID();
+        if (!advance(p2, d1)) {
+          break; // end of list 2, nothing more in intersection
+        }
+        d2 = p2.getDocID();
+      } else if (d1 < d2) {
+        if (p1.advance(d2) == DocIdSetIterator.NO_MORE_DOCS) {
+          break; // end of list 1, nothing more in intersection
+        }
+        d1 = p1.docID();
+      } else /* d1>d2 */ {
+        if (!advance(p2, d1)) {
+          break; // end of list 2, nothing more in intersection
+        }
+        d2 = p2.getDocID();
+      }
+    }
+    return count;
+  }
+
+  /**
+   * utility: advance the iterator until finding (or exceeding) specific
+   * document
+   * 
+   * @param iterator
+   *          iterator being advanced
+   * @param targetDoc
+   *          target of advancing
+   * @return false if iterator exhausted, true otherwise.
+   */
+  private static boolean advance(ScoredDocIDsIterator iterator, int targetDoc) {
+    while (iterator.next()) {
+      if (iterator.getDocID() >= targetDoc) {
+        return true; // target reached
+      }
+    }
+    return false; // exhausted
+  }
+}
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/sampling/package.html b/lucene/facet/src/java/org/apache/lucene/facet/sampling/package.html
new file mode 100644
index 0000000..2d34182
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/sampling/package.html
@@ -0,0 +1,24 @@
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+<html>
+<head>
+<title>Facets sampling</title>
+</head>
+<body>
+Facets sampling.
+</body>
+</html>
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/AdaptiveFacetsAccumulator.java b/lucene/facet/src/java/org/apache/lucene/facet/search/AdaptiveFacetsAccumulator.java
index 2ffac82..8d34158 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/AdaptiveFacetsAccumulator.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/search/AdaptiveFacetsAccumulator.java
@@ -3,11 +3,10 @@ package org.apache.lucene.facet.search;
 import java.io.IOException;
 import java.util.List;
 
-import org.apache.lucene.facet.search.params.FacetSearchParams;
-import org.apache.lucene.facet.search.results.FacetResult;
-import org.apache.lucene.facet.search.sampling.RandomSampler;
-import org.apache.lucene.facet.search.sampling.Sampler;
-import org.apache.lucene.facet.search.sampling.SamplingAccumulator;
+import org.apache.lucene.facet.params.FacetSearchParams;
+import org.apache.lucene.facet.sampling.RandomSampler;
+import org.apache.lucene.facet.sampling.Sampler;
+import org.apache.lucene.facet.sampling.SamplingAccumulator;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.index.IndexReader;
 
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/Aggregator.java b/lucene/facet/src/java/org/apache/lucene/facet/search/Aggregator.java
new file mode 100644
index 0000000..e95af40
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/search/Aggregator.java
@@ -0,0 +1,48 @@
+package org.apache.lucene.facet.search;
+
+import java.io.IOException;
+
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.util.IntsRef;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Aggregates the categories of documents given to
+ * {@link #aggregate(int, float, IntsRef)}. Note that the document IDs are local
+ * to the reader given to {@link #setNextReader(AtomicReaderContext)}.
+ * 
+ * @lucene.experimental
+ */
+public interface Aggregator {
+
+  /**
+   * Sets the {@link AtomicReaderContext} for which
+   * {@link #aggregate(int, float, IntsRef)} calls will be made. If this method
+   * returns false, {@link #aggregate(int, float, IntsRef)} should not be called
+   * for this reader.
+   */
+  public boolean setNextReader(AtomicReaderContext context) throws IOException;
+  
+  /**
+   * Aggregate the ordinals of the given document ID (and its score). The given
+   * ordinals offset is always zero.
+   */
+  public void aggregate(int docID, float score, IntsRef ordinals) throws IOException;
+  
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/CountFacetRequest.java b/lucene/facet/src/java/org/apache/lucene/facet/search/CountFacetRequest.java
new file mode 100644
index 0000000..9519230
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/search/CountFacetRequest.java
@@ -0,0 +1,55 @@
+package org.apache.lucene.facet.search;
+
+import org.apache.lucene.facet.complements.ComplementCountingAggregator;
+import org.apache.lucene.facet.taxonomy.CategoryPath;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Facet request for counting facets.
+ * 
+ * @lucene.experimental
+ */
+public class CountFacetRequest extends FacetRequest {
+
+  public CountFacetRequest(CategoryPath path, int num) {
+    super(path, num);
+  }
+
+  @Override
+  public Aggregator createAggregator(boolean useComplements, FacetArrays arrays, TaxonomyReader taxonomy) {
+    // we rely on that, if needed, result is cleared by arrays!
+    int[] a = arrays.getIntArray();
+    if (useComplements) {
+      return new ComplementCountingAggregator(a);
+    }
+    return new CountingAggregator(a);
+  }
+
+  @Override
+  public double getValueOf(FacetArrays arrays, int ordinal) {
+    return arrays.getIntArray()[ordinal];
+  }
+
+  @Override
+  public FacetArraysSource getFacetArraysSource() {
+    return FacetArraysSource.INT;
+  }
+  
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/CountingAggregator.java b/lucene/facet/src/java/org/apache/lucene/facet/search/CountingAggregator.java
new file mode 100644
index 0000000..395581b
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/search/CountingAggregator.java
@@ -0,0 +1,66 @@
+package org.apache.lucene.facet.search;
+
+import java.io.IOException;
+
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.util.IntsRef;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * A CountingAggregator updates a counter array with the size of the whole
+ * taxonomy, counting the number of times each category appears in the given set
+ * of documents.
+ * 
+ * @lucene.experimental
+ */
+public class CountingAggregator implements Aggregator {
+
+  protected int[] counterArray;
+  
+  public CountingAggregator(int[] counterArray) {
+    this.counterArray = counterArray;
+  }
+  
+  @Override
+  public void aggregate(int docID, float score, IntsRef ordinals) throws IOException {
+    for (int i = 0; i < ordinals.length; i++) {
+      counterArray[ordinals.ints[i]]++;
+    }
+  }
+
+  @Override
+  public boolean equals(Object obj) {
+    if (obj == null || obj.getClass() != this.getClass()) {
+      return false;
+    }
+    CountingAggregator that = (CountingAggregator) obj;
+    return that.counterArray == this.counterArray;
+  }
+
+  @Override
+  public int hashCode() {
+    return counterArray == null ? 0 : counterArray.hashCode();
+  }
+  
+  @Override
+  public boolean setNextReader(AtomicReaderContext context) throws IOException {
+    return true;
+  }
+  
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/CountingFacetsAggregator.java b/lucene/facet/src/java/org/apache/lucene/facet/search/CountingFacetsAggregator.java
index 514a6af..6927444 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/CountingFacetsAggregator.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/search/CountingFacetsAggregator.java
@@ -2,7 +2,7 @@ package org.apache.lucene.facet.search;
 
 import java.io.IOException;
 
-import org.apache.lucene.facet.index.params.CategoryListParams;
+import org.apache.lucene.facet.params.CategoryListParams;
 import org.apache.lucene.facet.search.FacetsCollector.MatchingDocs;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.util.IntsRef;
@@ -70,7 +70,7 @@ public class CountingFacetsAggregator implements FacetsAggregator {
   }
 
   @Override
-  public void rollupValues(int ordinal, int[] children, int[] siblings, FacetArrays facetArrays) {
+  public void rollupValues(FacetRequest fr, int ordinal, int[] children, int[] siblings, FacetArrays facetArrays) {
     final int[] counts = facetArrays.getIntArray();
     counts[ordinal] += rollupCounts(children[ordinal], children, siblings, counts);
   }
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/DepthOneFacetResultsHandler.java b/lucene/facet/src/java/org/apache/lucene/facet/search/DepthOneFacetResultsHandler.java
index 1caa08f..439b375 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/DepthOneFacetResultsHandler.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/search/DepthOneFacetResultsHandler.java
@@ -6,11 +6,8 @@ import java.util.Arrays;
 import java.util.Collections;
 import java.util.Comparator;
 
-import org.apache.lucene.facet.search.params.FacetRequest;
-import org.apache.lucene.facet.search.params.FacetRequest.SortBy;
-import org.apache.lucene.facet.search.params.FacetRequest.SortOrder;
-import org.apache.lucene.facet.search.results.FacetResult;
-import org.apache.lucene.facet.search.results.FacetResultNode;
+import org.apache.lucene.facet.search.FacetRequest.SortBy;
+import org.apache.lucene.facet.search.FacetRequest.SortOrder;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.facet.taxonomy.directory.ParallelTaxonomyArrays;
 import org.apache.lucene.util.PriorityQueue;
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/DocValuesCategoryListIterator.java b/lucene/facet/src/java/org/apache/lucene/facet/search/DocValuesCategoryListIterator.java
index 74d6965..916aec2 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/DocValuesCategoryListIterator.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/search/DocValuesCategoryListIterator.java
@@ -2,11 +2,11 @@ package org.apache.lucene.facet.search;
 
 import java.io.IOException;
 
+import org.apache.lucene.facet.encoding.IntDecoder;
 import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.BinaryDocValues;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.IntsRef;
-import org.apache.lucene.util.encoding.IntDecoder;
 
 /*
  * Licensed to the Apache Software Foundation (ASF) under one or more
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/DrillDown.java b/lucene/facet/src/java/org/apache/lucene/facet/search/DrillDown.java
index 57d60f4..670df7e 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/DrillDown.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/search/DrillDown.java
@@ -7,9 +7,9 @@ import org.apache.lucene.search.Query;
 import org.apache.lucene.search.TermQuery;
 import org.apache.lucene.search.BooleanClause.Occur;
 
-import org.apache.lucene.facet.index.params.CategoryListParams;
-import org.apache.lucene.facet.index.params.FacetIndexingParams;
-import org.apache.lucene.facet.search.params.FacetSearchParams;
+import org.apache.lucene.facet.params.CategoryListParams;
+import org.apache.lucene.facet.params.FacetIndexingParams;
+import org.apache.lucene.facet.params.FacetSearchParams;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 
 /*
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/FacetRequest.java b/lucene/facet/src/java/org/apache/lucene/facet/search/FacetRequest.java
new file mode 100644
index 0000000..8673164
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/search/FacetRequest.java
@@ -0,0 +1,293 @@
+package org.apache.lucene.facet.search;
+
+import java.io.IOException;
+
+import org.apache.lucene.facet.taxonomy.CategoryPath;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Request to accumulate facet information for a specified facet and possibly 
+ * also some of its descendants, upto a specified depth.
+ * <p>
+ * The facet request additionally defines what information should 
+ * be computed within the facet results, if and how should results
+ * be ordered, etc.
+ * <P>
+ * An example facet request is to look at all sub-categories of "Author", and
+ * return the 10 with the highest counts (sorted by decreasing count). 
+ * 
+ * @lucene.experimental
+ */
+public abstract class FacetRequest {
+
+  /**
+   * Result structure manner of applying request's limits such as
+   * {@link FacetRequest#getNumLabel()} and {@link FacetRequest#numResults}.
+   * Only relevant when {@link FacetRequest#getDepth()} is &gt; 1.
+   */
+  public enum ResultMode { 
+    /** Limits are applied per node, and the result has a full tree structure. */
+    PER_NODE_IN_TREE, 
+
+    /** Limits are applied globally, on total number of results, and the result has a flat structure. */
+    GLOBAL_FLAT
+  }
+  
+  /**
+   * Specifies which array of {@link FacetArrays} should be used to resolve
+   * values. When set to {@link #INT} or {@link #FLOAT}, allows creating an
+   * optimized {@link FacetResultsHandler}, which does not call
+   * {@link FacetRequest#getValueOf(FacetArrays, int)} for every ordinals.
+   * <p>
+   * If set to {@link #BOTH}, the {@link FacetResultsHandler} will use
+   * {@link FacetRequest#getValueOf(FacetArrays, int)} to resolve ordinal
+   * values, although it is recommended that you consider writing a specialized
+   * {@link FacetResultsHandler}.
+   */
+  public enum FacetArraysSource { INT, FLOAT, BOTH }
+
+  /** Sort options for facet results. */
+  public enum SortBy { 
+    /** sort by category ordinal with the taxonomy */
+    ORDINAL, 
+
+    /** sort by computed category value */ 
+    VALUE 
+  }
+
+  /** Requested sort order for the results. */
+  public enum SortOrder { ASCENDING, DESCENDING }
+
+  /**
+   * Default depth for facets accumulation.
+   * @see #getDepth()
+   */
+  public static final int DEFAULT_DEPTH = 1;
+  
+  /**
+   * Default sort mode.
+   * @see #getSortBy()
+   */
+  public static final SortBy DEFAULT_SORT_BY = SortBy.VALUE;
+  
+  /**
+   * Default result mode
+   * @see #getResultMode()
+   */
+  public static final ResultMode DEFAULT_RESULT_MODE = ResultMode.PER_NODE_IN_TREE;
+  
+  public final CategoryPath categoryPath;
+  public final int numResults;
+  
+  private int numLabel;
+  private int depth;
+  private SortOrder sortOrder;
+  private SortBy sortBy;
+
+  /**
+   * Computed at construction, this hashCode is based on two final members
+   * {@link CategoryPath} and <code>numResults</code>
+   */
+  private final int hashCode;
+
+  private ResultMode resultMode = DEFAULT_RESULT_MODE;
+
+  /**
+   * Initialize the request with a given path, and a requested number of facets
+   * results. By default, all returned results would be labeled - to alter this
+   * default see {@link #setNumLabel(int)}.
+   * <p>
+   * <b>NOTE:</b> if <code>numResults</code> is given as
+   * <code>Integer.MAX_VALUE</code> than all the facet results would be
+   * returned, without any limit.
+   * <p>
+   * <b>NOTE:</b> it is assumed that the given {@link CategoryPath} is not
+   * modified after construction of this object. Otherwise, some things may not
+   * function properly, e.g. {@link #hashCode()}.
+   * 
+   * @throws IllegalArgumentException if numResults is &le; 0
+   */
+  public FacetRequest(CategoryPath path, int numResults) {
+    if (numResults <= 0) {
+      throw new IllegalArgumentException("num results must be a positive (>0) number: " + numResults);
+    }
+    if (path == null) {
+      throw new IllegalArgumentException("category path cannot be null!");
+    }
+    categoryPath = path;
+    this.numResults = numResults;
+    numLabel = numResults;
+    depth = DEFAULT_DEPTH;
+    sortBy = DEFAULT_SORT_BY;
+    sortOrder = SortOrder.DESCENDING;
+    
+    hashCode = categoryPath.hashCode() ^ this.numResults;
+  }
+
+  /**
+   * Create an aggregator for this facet request. Aggregator action depends on
+   * request definition. For a count request, it will usually increment the
+   * count for that facet.
+   * 
+   * @param useComplements
+   *          whether the complements optimization is being used for current
+   *          computation.
+   * @param arrays
+   *          provider for facet arrays in use for current computation.
+   * @param taxonomy
+   *          reader of taxonomy in effect.
+   * @throws IOException If there is a low-level I/O error.
+   */
+  public abstract Aggregator createAggregator(boolean useComplements, FacetArrays arrays, TaxonomyReader taxonomy) 
+      throws IOException;
+
+  @Override
+  public boolean equals(Object o) {
+    if (o instanceof FacetRequest) {
+      FacetRequest that = (FacetRequest)o;
+      return that.hashCode == this.hashCode &&
+          that.categoryPath.equals(this.categoryPath) &&
+          that.numResults == this.numResults &&
+          that.depth == this.depth &&
+          that.resultMode == this.resultMode &&
+          that.numLabel == this.numLabel;
+    }
+    return false;
+  }
+
+  /**
+   * How deeply to look under the given category. If the depth is 0,
+   * only the category itself is counted. If the depth is 1, its immediate
+   * children are also counted, and so on. If the depth is Integer.MAX_VALUE,
+   * all the category's descendants are counted.<br>
+   */
+  public final int getDepth() {
+    // TODO add AUTO_EXPAND option  
+    return depth;
+  }
+  
+  /**
+   * Returns the {@link FacetArraysSource} this {@link FacetRequest} uses in
+   * {@link #getValueOf(FacetArrays, int)}.
+   */
+  public abstract FacetArraysSource getFacetArraysSource();
+  
+  /**
+   * If getNumLabel() &lt; getNumResults(), only the first getNumLabel() results
+   * will have their category paths calculated, and the rest will only be
+   * available as ordinals (category numbers) and will have null paths.
+   * <P>
+   * If Integer.MAX_VALUE is specified, all results are labled.
+   * <P>
+   * The purpose of this parameter is to avoid having to run the whole faceted
+   * search again when the user asks for more values for the facet; The
+   * application can ask (getNumResults()) for more values than it needs to
+   * show, but keep getNumLabel() only the number it wants to immediately show.
+   * The slow-down caused by finding more values is negligible, because the
+   * slowest part - finding the categories' paths, is avoided.
+   * <p>
+   * Depending on the {@link #getResultMode() LimitsMode}, this limit is applied
+   * globally or per results node. In the global mode, if this limit is 3, only
+   * 3 top results would be labeled. In the per-node mode, if this limit is 3, 3
+   * top children of {@link #categoryPath the target category} would be labeled,
+   * as well as 3 top children of each of them, and so forth, until the depth
+   * defined by {@link #getDepth()}.
+   * 
+   * @see #getResultMode()
+   */
+  public final int getNumLabel() {
+    return numLabel;
+  }
+
+  /** Return the requested result mode. */
+  public final ResultMode getResultMode() {
+    return resultMode;
+  }
+
+  /** Specify how should results be sorted. */
+  public final SortBy getSortBy() {
+    return sortBy;
+  }
+
+  /** Return the requested order of results. */
+  public final SortOrder getSortOrder() {
+    return sortOrder;
+  }
+
+  /**
+   * Return the value of a category used for facets computations for this
+   * request. For a count request this would be the count for that facet, i.e.
+   * an integer number. but for other requests this can be the result of a more
+   * complex operation, and the result can be any double precision number.
+   * Having this method with a general name <b>value</b> which is double
+   * precision allows to have more compact API and code for handling counts and
+   * perhaps other requests (such as for associations) very similarly, and by
+   * the same code and API, avoiding code duplication.
+   * 
+   * @param arrays
+   *          provider for facet arrays in use for current computation.
+   * @param idx
+   *          an index into the count arrays now in effect in
+   *          <code>arrays</code>. E.g., for ordinal number <i>n</i>, with
+   *          partition, of size <i>partitionSize</i>, now covering <i>n</i>,
+   *          <code>getValueOf</code> would be invoked with <code>idx</code>
+   *          being <i>n</i> % <i>partitionSize</i>.
+   */
+  // TODO perhaps instead of getValueOf we can have a postProcess(FacetArrays)
+  // That, together with getFacetArraysSource should allow ResultHandlers to
+  // efficiently obtain the values from the arrays directly
+  public abstract double getValueOf(FacetArrays arrays, int idx);
+
+  @Override
+  public int hashCode() {
+    return hashCode; 
+  }
+
+  public void setDepth(int depth) {
+    this.depth = depth;
+  }
+
+  public void setNumLabel(int numLabel) {
+    this.numLabel = numLabel;
+  }
+  
+  /**
+   * @param resultMode the resultMode to set
+   * @see #getResultMode()
+   */
+  public void setResultMode(ResultMode resultMode) {
+    this.resultMode = resultMode;
+  }
+
+  public void setSortBy(SortBy sortBy) {
+    this.sortBy = sortBy;
+  }
+
+  public void setSortOrder(SortOrder sortOrder) {
+    this.sortOrder = sortOrder;
+  }
+  
+  @Override
+  public String toString() {
+    return categoryPath.toString()+" nRes="+numResults+" nLbl="+numLabel;
+  }
+
+}
+  
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/FacetResult.java b/lucene/facet/src/java/org/apache/lucene/facet/search/FacetResult.java
new file mode 100644
index 0000000..3d9732d
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/search/FacetResult.java
@@ -0,0 +1,101 @@
+package org.apache.lucene.facet.search;
+
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Result of faceted search.
+ * 
+ * @lucene.experimental
+ */
+public class FacetResult {
+  
+  private final FacetRequest facetRequest;
+  private final FacetResultNode rootNode;
+  private final int numValidDescendants;
+  
+  public FacetResult(FacetRequest facetRequest, FacetResultNode rootNode,  int numValidDescendants) {
+    this.facetRequest = facetRequest;
+    this.rootNode = rootNode;
+    this.numValidDescendants = numValidDescendants;
+  }
+  
+  /**
+   * Facet result node matching the root of the {@link #getFacetRequest() facet request}.
+   * @see #getFacetRequest()
+   * @see FacetRequest#categoryPath
+   */
+  public final FacetResultNode getFacetResultNode() {
+    return this.rootNode;
+  }
+  
+  /**
+   * Number of descendants of {@link #getFacetResultNode() root facet result
+   * node}, up till the requested depth. Typically -- have value != 0. This
+   * number does not include the root node.
+   * 
+   * @see #getFacetRequest()
+   * @see FacetRequest#getDepth()
+   */
+  public final int getNumValidDescendants() {
+    return this.numValidDescendants;
+  }
+  
+  /**
+   * Request for which this result was obtained.
+   */
+  public final FacetRequest getFacetRequest() {
+    return this.facetRequest;
+  }
+  
+  /**
+   * String representation of this facet result.
+   * Use with caution: might return a very long string.
+   * @param prefix prefix for each result line
+   * @see #toString()
+   */
+  public String toString(String prefix) {
+    StringBuilder sb = new StringBuilder();
+    String nl = "";
+    
+    // request
+    if (this.facetRequest != null) {
+      sb.append(nl).append(prefix).append("Request: ").append(
+          this.facetRequest.toString());
+      nl = "\n";
+    }
+    
+    // total facets
+    sb.append(nl).append(prefix).append("Num valid Descendants (up to specified depth): ").append(
+        this.numValidDescendants);
+    nl = "\n";
+    
+    // result node
+    if (this.rootNode != null) {
+      sb.append(nl).append(this.rootNode.toString(prefix + "\t"));
+    }
+    
+    return sb.toString();
+  }
+  
+  @Override
+  public String toString() {
+    return toString("");
+  }
+  
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/FacetResultNode.java b/lucene/facet/src/java/org/apache/lucene/facet/search/FacetResultNode.java
new file mode 100644
index 0000000..2a7733c
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/search/FacetResultNode.java
@@ -0,0 +1,99 @@
+package org.apache.lucene.facet.search;
+
+import java.util.Collections;
+import java.util.List;
+
+import org.apache.lucene.facet.search.FacetRequest.ResultMode;
+import org.apache.lucene.facet.taxonomy.CategoryPath;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Result of faceted search for a certain taxonomy node. This class serves as a
+ * bin of different attributes of the result node, such as its {@link #ordinal}
+ * as well as {@link #label}. You are not expected to modify those values.
+ * 
+ * @lucene.experimental
+ */
+public class FacetResultNode {
+
+  public static final List<FacetResultNode> EMPTY_SUB_RESULTS = Collections.emptyList();
+  
+  /** The category ordinal of this node. */
+  public int ordinal;
+
+  /**
+   * The {@link CategoryPath label} of this result. May be {@code null} if not
+   * computed, in which case use {@link TaxonomyReader#getPath(int)} to label
+   * it.
+   * <p>
+   * <b>NOTE:</b> by default, all nodes are labeled. Only when
+   * {@link FacetRequest#getNumLabel()} &lt;
+   * {@link FacetRequest#numResults} there will be unlabeled nodes.
+   */
+  public CategoryPath label;
+  
+  /**
+   * The value of this result. Its actual type depends on the
+   * {@link FacetRequest} used (e.g. in case of {@link CountFacetRequest} it is
+   * {@code int}).
+   */
+  public double value;
+
+  /**
+   * The sub-results of this result. If {@link FacetRequest#getResultMode()} is
+   * {@link ResultMode#PER_NODE_IN_TREE}, every sub result denotes an immediate
+   * child of this node. Otherwise, it is a descendant of any level.
+   * <p>
+   * <b>NOTE:</b> this member should not be {@code null}. To denote that a
+   * result does not have sub results, set it to {@link #EMPTY_SUB_RESULTS} (or
+   * don't modify it).
+   */
+  public List<FacetResultNode> subResults = EMPTY_SUB_RESULTS;
+
+  public FacetResultNode() {
+    // empty constructor
+  }
+  
+  public FacetResultNode(int ordinal, double value) {
+    this.ordinal = ordinal;
+    this.value = value;
+  }
+  
+  @Override
+  public String toString() {
+    return toString("");
+  }
+  
+  /** Returns a String representation of this facet result node. */
+  public String toString(String prefix) {
+    StringBuilder sb = new StringBuilder(prefix);
+    if (label == null) {
+      sb.append("not labeled (ordinal=").append(ordinal).append(")");
+    } else {
+      sb.append(label.toString());
+    }
+    sb.append(" (").append(Double.toString(value)).append(")");
+    for (FacetResultNode sub : subResults) {
+      sb.append("\n").append(prefix).append(sub.toString(prefix + "  "));
+    }
+    return sb.toString();
+  }
+
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/FacetResultsHandler.java b/lucene/facet/src/java/org/apache/lucene/facet/search/FacetResultsHandler.java
index 836de2c..3123c4d 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/FacetResultsHandler.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/search/FacetResultsHandler.java
@@ -2,8 +2,6 @@ package org.apache.lucene.facet.search;
 
 import java.io.IOException;
 
-import org.apache.lucene.facet.search.params.FacetRequest;
-import org.apache.lucene.facet.search.results.FacetResult;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 
 /*
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/FacetsAccumulator.java b/lucene/facet/src/java/org/apache/lucene/facet/search/FacetsAccumulator.java
index b86d8ad..b0220e1 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/FacetsAccumulator.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/search/FacetsAccumulator.java
@@ -7,20 +7,18 @@ import java.util.HashSet;
 import java.util.List;
 import java.util.Set;
 
-import org.apache.lucene.facet.index.params.CategoryListParams;
-import org.apache.lucene.facet.index.params.CategoryListParams.OrdinalPolicy;
+import org.apache.lucene.facet.encoding.DGapVInt8IntDecoder;
+import org.apache.lucene.facet.params.CategoryListParams;
+import org.apache.lucene.facet.params.FacetSearchParams;
+import org.apache.lucene.facet.params.CategoryListParams.OrdinalPolicy;
+import org.apache.lucene.facet.search.FacetRequest.FacetArraysSource;
+import org.apache.lucene.facet.search.FacetRequest.ResultMode;
+import org.apache.lucene.facet.search.FacetRequest.SortBy;
+import org.apache.lucene.facet.search.FacetRequest.SortOrder;
 import org.apache.lucene.facet.search.FacetsCollector.MatchingDocs;
-import org.apache.lucene.facet.search.params.FacetRequest;
-import org.apache.lucene.facet.search.params.FacetSearchParams;
-import org.apache.lucene.facet.search.params.FacetRequest.FacetArraysSource;
-import org.apache.lucene.facet.search.params.FacetRequest.ResultMode;
-import org.apache.lucene.facet.search.params.FacetRequest.SortBy;
-import org.apache.lucene.facet.search.params.FacetRequest.SortOrder;
-import org.apache.lucene.facet.search.results.FacetResult;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.facet.taxonomy.directory.ParallelTaxonomyArrays;
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.util.encoding.DGapVInt8IntDecoder;
 
 /*
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -47,10 +45,10 @@ import org.apache.lucene.util.encoding.DGapVInt8IntDecoder;
  */
 public class FacetsAccumulator {
 
-  protected final TaxonomyReader taxonomyReader;
-  protected final IndexReader indexReader;
-  protected final FacetArrays facetArrays;
-  protected FacetSearchParams searchParams;
+  public final TaxonomyReader taxonomyReader;
+  public final IndexReader indexReader;
+  public final FacetArrays facetArrays;
+  public FacetSearchParams searchParams;
 
   /**
    * Initializes the accumulator with the given search params, index reader and
@@ -162,7 +160,7 @@ public class FacetsAccumulator {
       OrdinalPolicy ordinalPolicy = clp .getOrdinalPolicy(fr.categoryPath.components[0]);
       if (ordinalPolicy == OrdinalPolicy.NO_PARENTS) {
         // rollup values
-        aggregator.rollupValues(rootOrd, children, siblings, facetArrays);
+        aggregator.rollupValues(fr, rootOrd, children, siblings, facetArrays);
       }
       
       FacetResultsHandler frh = createFacetResultsHandler(fr);
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/FacetsAggregator.java b/lucene/facet/src/java/org/apache/lucene/facet/search/FacetsAggregator.java
index 9e1b69b..befb195 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/FacetsAggregator.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/search/FacetsAggregator.java
@@ -2,8 +2,8 @@ package org.apache.lucene.facet.search;
 
 import java.io.IOException;
 
-import org.apache.lucene.facet.index.params.CategoryListParams;
-import org.apache.lucene.facet.index.params.CategoryListParams.OrdinalPolicy;
+import org.apache.lucene.facet.params.CategoryListParams;
+import org.apache.lucene.facet.params.CategoryListParams.OrdinalPolicy;
 import org.apache.lucene.facet.search.FacetsCollector.MatchingDocs;
 
 /*
@@ -41,7 +41,7 @@ public interface FacetsAggregator {
    * ordinal is the requested category, and you should use the children and
    * siblings arrays to traverse its sub-tree.
    */
-  public void rollupValues(int ordinal, int[] children, int[] siblings, FacetArrays facetArrays);
+  public void rollupValues(FacetRequest fr, int ordinal, int[] children, int[] siblings, FacetArrays facetArrays);
   
   /** Returns {@code true} if this aggregator requires document scores. */
   public boolean requiresDocScores();
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/FacetsCollector.java b/lucene/facet/src/java/org/apache/lucene/facet/search/FacetsCollector.java
index 5e961e8..27a786a 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/FacetsCollector.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/search/FacetsCollector.java
@@ -4,10 +4,7 @@ import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
 
-import org.apache.lucene.facet.search.params.CountFacetRequest;
-import org.apache.lucene.facet.search.params.FacetRequest;
-import org.apache.lucene.facet.search.params.FacetSearchParams;
-import org.apache.lucene.facet.search.results.FacetResult;
+import org.apache.lucene.facet.params.FacetSearchParams;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.index.AtomicReaderContext;
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/FastCountingFacetsAggregator.java b/lucene/facet/src/java/org/apache/lucene/facet/search/FastCountingFacetsAggregator.java
index bfc4e97..cd3b20d 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/FastCountingFacetsAggregator.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/search/FastCountingFacetsAggregator.java
@@ -2,15 +2,14 @@ package org.apache.lucene.facet.search;
 
 import java.io.IOException;
 
-import org.apache.lucene.facet.index.params.CategoryListParams;
+import org.apache.lucene.facet.encoding.DGapVInt8IntDecoder;
+import org.apache.lucene.facet.encoding.DGapVInt8IntEncoder;
+import org.apache.lucene.facet.params.CategoryListParams;
+import org.apache.lucene.facet.params.FacetSearchParams;
 import org.apache.lucene.facet.search.FacetsCollector.MatchingDocs;
-import org.apache.lucene.facet.search.params.FacetRequest;
-import org.apache.lucene.facet.search.params.FacetSearchParams;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.index.BinaryDocValues;
 import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.encoding.DGapVInt8IntDecoder;
-import org.apache.lucene.util.encoding.DGapVInt8IntEncoder;
 
 /*
  * Licensed to the Apache Software Foundation (ASF) under one or more
@@ -109,7 +108,7 @@ public final class FastCountingFacetsAggregator implements FacetsAggregator {
   }
 
   @Override
-  public final void rollupValues(int ordinal, int[] children, int[] siblings, FacetArrays facetArrays) {
+  public final void rollupValues(FacetRequest fr, int ordinal, int[] children, int[] siblings, FacetArrays facetArrays) {
     final int[] counts = facetArrays.getIntArray();
     counts[ordinal] += rollupCounts(children[ordinal], children, siblings, counts);
   }
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/FloatFacetResultsHandler.java b/lucene/facet/src/java/org/apache/lucene/facet/search/FloatFacetResultsHandler.java
index 2052396..f760edc 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/FloatFacetResultsHandler.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/search/FloatFacetResultsHandler.java
@@ -3,8 +3,6 @@ package org.apache.lucene.facet.search;
 import java.io.IOException;
 import java.util.ArrayList;
 
-import org.apache.lucene.facet.search.params.FacetRequest;
-import org.apache.lucene.facet.search.results.FacetResultNode;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.util.PriorityQueue;
 
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/IntFacetResultsHandler.java b/lucene/facet/src/java/org/apache/lucene/facet/search/IntFacetResultsHandler.java
index 34f5a22..fa98dc1 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/IntFacetResultsHandler.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/search/IntFacetResultsHandler.java
@@ -3,8 +3,6 @@ package org.apache.lucene.facet.search;
 import java.io.IOException;
 import java.util.ArrayList;
 
-import org.apache.lucene.facet.search.params.FacetRequest;
-import org.apache.lucene.facet.search.results.FacetResultNode;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.util.PriorityQueue;
 
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/PerCategoryListAggregator.java b/lucene/facet/src/java/org/apache/lucene/facet/search/PerCategoryListAggregator.java
new file mode 100644
index 0000000..790dba6
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/search/PerCategoryListAggregator.java
@@ -0,0 +1,62 @@
+package org.apache.lucene.facet.search;
+
+import java.io.IOException;
+import java.util.Map;
+
+import org.apache.lucene.facet.params.CategoryListParams;
+import org.apache.lucene.facet.params.FacetIndexingParams;
+import org.apache.lucene.facet.search.FacetsCollector.MatchingDocs;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * A {@link FacetsAggregator} which invokes the proper aggregator per
+ * {@link CategoryListParams}.
+ */
+public class PerCategoryListAggregator implements FacetsAggregator {
+  
+  private final Map<CategoryListParams,FacetsAggregator> aggregators;
+  private final FacetIndexingParams fip;
+  
+  public PerCategoryListAggregator(Map<CategoryListParams,FacetsAggregator> aggregators, FacetIndexingParams fip) {
+    this.aggregators = aggregators;
+    this.fip = fip;
+  }
+  
+  @Override
+  public void aggregate(MatchingDocs matchingDocs, CategoryListParams clp, FacetArrays facetArrays) throws IOException {
+    aggregators.get(clp).aggregate(matchingDocs, clp, facetArrays);
+  }
+  
+  @Override
+  public void rollupValues(FacetRequest fr, int ordinal, int[] children, int[] siblings, FacetArrays facetArrays) {
+    CategoryListParams clp = fip.getCategoryListParams(fr.categoryPath);
+    aggregators.get(clp).rollupValues(fr, ordinal, children, siblings, facetArrays);
+  }
+  
+  @Override
+  public boolean requiresDocScores() {
+    for (FacetsAggregator aggregator : aggregators.values()) {
+      if (aggregator.requiresDocScores()) {
+        return true;
+      }
+    }
+    return false;
+  }
+  
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/SamplingWrapper.java b/lucene/facet/src/java/org/apache/lucene/facet/search/SamplingWrapper.java
deleted file mode 100644
index 8f02d6e..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/SamplingWrapper.java
+++ /dev/null
@@ -1,91 +0,0 @@
-package org.apache.lucene.facet.search;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.lucene.facet.partitions.search.PartitionsFacetResultsHandler;
-import org.apache.lucene.facet.search.params.FacetSearchParams;
-import org.apache.lucene.facet.search.results.FacetResult;
-import org.apache.lucene.facet.search.sampling.Sampler;
-import org.apache.lucene.facet.search.sampling.Sampler.SampleResult;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Wrap any Facets Accumulator with sampling.
- * <p>
- * Note: Sampling accumulation (Accumulation over a sampled-set of the results),
- * does not guarantee accurate values for
- * {@link FacetResult#getNumValidDescendants()}.
- * 
- * @lucene.experimental
- */
-public class SamplingWrapper extends StandardFacetsAccumulator {
-
-  private StandardFacetsAccumulator delegee;
-  private Sampler sampler;
-
-  public SamplingWrapper(StandardFacetsAccumulator delegee, Sampler sampler) {
-    super(delegee.searchParams, delegee.indexReader, delegee.taxonomyReader);
-    this.delegee = delegee;
-    this.sampler = sampler;
-  }
-
-  @Override
-  public List<FacetResult> accumulate(ScoredDocIDs docids) throws IOException {
-    // Replacing the original searchParams with the over-sampled (and without statistics-compute)
-    FacetSearchParams original = delegee.searchParams;
-    delegee.searchParams = sampler.overSampledSearchParams(original);
-    
-    SampleResult sampleSet = sampler.getSampleSet(docids);
-
-    List<FacetResult> sampleRes = delegee.accumulate(sampleSet.docids);
-
-    List<FacetResult> fixedRes = new ArrayList<FacetResult>();
-    for (FacetResult fres : sampleRes) {
-      // for sure fres is not null because this is guaranteed by the delegee.
-      PartitionsFacetResultsHandler frh = createFacetResultsHandler(fres.getFacetRequest());
-      // fix the result of current request
-      sampler.getSampleFixer(indexReader, taxonomyReader, searchParams).fixResult(docids, fres); 
-      fres = frh.rearrangeFacetResult(fres); // let delegee's handler do any
-      
-      // Using the sampler to trim the extra (over-sampled) results
-      fres = sampler.trimResult(fres);
-      
-      // final labeling if allowed (because labeling is a costly operation)
-      frh.labelResult(fres);
-      fixedRes.add(fres); // add to final results
-    }
-
-    delegee.searchParams = original; // Back to original params
-    
-    return fixedRes; 
-  }
-
-  @Override
-  public double getComplementThreshold() {
-    return delegee.getComplementThreshold();
-  }
-
-  @Override
-  public void setComplementThreshold(double complementThreshold) {
-    delegee.setComplementThreshold(complementThreshold);
-  }
-
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/ScoringAggregator.java b/lucene/facet/src/java/org/apache/lucene/facet/search/ScoringAggregator.java
new file mode 100644
index 0000000..2ecf0b6
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/search/ScoringAggregator.java
@@ -0,0 +1,67 @@
+package org.apache.lucene.facet.search;
+
+import java.io.IOException;
+
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.util.IntsRef;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * An {@link Aggregator} which updates the weight of a category according to the
+ * scores of the documents it was found in.
+ * 
+ * @lucene.experimental
+ */
+public class ScoringAggregator implements Aggregator {
+
+  private final float[] scoreArray;
+  private final int hashCode;
+  
+  public ScoringAggregator(float[] counterArray) {
+    this.scoreArray = counterArray;
+    this.hashCode = scoreArray == null ? 0 : scoreArray.hashCode();
+  }
+
+  @Override
+  public void aggregate(int docID, float score, IntsRef ordinals) throws IOException {
+    for (int i = 0; i < ordinals.length; i++) {
+      scoreArray[ordinals.ints[i]] += score;
+    }
+  }
+  
+  @Override
+  public boolean equals(Object obj) {
+    if (obj == null || obj.getClass() != this.getClass()) {
+      return false;
+    }
+    ScoringAggregator that = (ScoringAggregator) obj;
+    return that.scoreArray == this.scoreArray;
+  }
+
+  @Override
+  public int hashCode() {
+    return hashCode;
+  }
+
+  @Override
+  public boolean setNextReader(AtomicReaderContext context) throws IOException {
+    return true;
+  }
+  
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/StandardFacetsAccumulator.java b/lucene/facet/src/java/org/apache/lucene/facet/search/StandardFacetsAccumulator.java
index 62898f9..6f9d9de 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/StandardFacetsAccumulator.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/search/StandardFacetsAccumulator.java
@@ -10,16 +10,14 @@ import java.util.Map.Entry;
 import java.util.logging.Level;
 import java.util.logging.Logger;
 
-import org.apache.lucene.facet.index.params.FacetIndexingParams;
-import org.apache.lucene.facet.partitions.search.PartitionsFacetResultsHandler;
+import org.apache.lucene.facet.complements.TotalFacetCounts;
+import org.apache.lucene.facet.complements.TotalFacetCountsCache;
+import org.apache.lucene.facet.params.FacetIndexingParams;
+import org.apache.lucene.facet.params.FacetSearchParams;
+import org.apache.lucene.facet.partitions.IntermediateFacetResult;
+import org.apache.lucene.facet.partitions.PartitionsFacetResultsHandler;
+import org.apache.lucene.facet.search.FacetRequest.ResultMode;
 import org.apache.lucene.facet.search.FacetsCollector.MatchingDocs;
-import org.apache.lucene.facet.search.aggregator.Aggregator;
-import org.apache.lucene.facet.search.params.CountFacetRequest;
-import org.apache.lucene.facet.search.params.FacetRequest;
-import org.apache.lucene.facet.search.params.FacetRequest.ResultMode;
-import org.apache.lucene.facet.search.params.FacetSearchParams;
-import org.apache.lucene.facet.search.results.FacetResult;
-import org.apache.lucene.facet.partitions.search.IntermediateFacetResult;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.facet.util.PartitionsUtils;
 import org.apache.lucene.facet.util.ScoredDocIdsUtils;
@@ -410,5 +408,10 @@ public class StandardFacetsAccumulator extends FacetsAccumulator {
   public void setComplementThreshold(double complementThreshold) {
     this.complementThreshold = complementThreshold;
   }
+
+  /** Returns true if complements are enabled. */
+  public boolean isUsingComplements() {
+    return isUsingComplements;
+  }
   
 }
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/SumScoreFacetRequest.java b/lucene/facet/src/java/org/apache/lucene/facet/search/SumScoreFacetRequest.java
new file mode 100644
index 0000000..7a08a12
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/search/SumScoreFacetRequest.java
@@ -0,0 +1,52 @@
+package org.apache.lucene.facet.search;
+
+import org.apache.lucene.facet.taxonomy.CategoryPath;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * A {@link FacetRequest} for weighting facets by summing the scores of matching
+ * documents.
+ * 
+ * @lucene.experimental
+ */
+public class SumScoreFacetRequest extends FacetRequest {
+
+  /** Create a score facet request for a given node in the taxonomy. */
+  public SumScoreFacetRequest(CategoryPath path, int num) {
+    super(path, num);
+  }
+
+  @Override
+  public Aggregator createAggregator(boolean useComplements, FacetArrays arrays, TaxonomyReader taxonomy) {
+    assert !useComplements : "complements are not supported by this FacetRequest";
+    return new ScoringAggregator(arrays.getFloatArray());
+  }
+
+  @Override
+  public double getValueOf(FacetArrays arrays, int ordinal) {
+    return arrays.getFloatArray()[ordinal];
+  }
+
+  @Override
+  public FacetArraysSource getFacetArraysSource() {
+    return FacetArraysSource.FLOAT;
+  }
+
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/SumScoreFacetsAggregator.java b/lucene/facet/src/java/org/apache/lucene/facet/search/SumScoreFacetsAggregator.java
index 589e0c4..516a3c6 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/SumScoreFacetsAggregator.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/search/SumScoreFacetsAggregator.java
@@ -2,7 +2,7 @@ package org.apache.lucene.facet.search;
 
 import java.io.IOException;
 
-import org.apache.lucene.facet.index.params.CategoryListParams;
+import org.apache.lucene.facet.params.CategoryListParams;
 import org.apache.lucene.facet.search.FacetsCollector.MatchingDocs;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.util.IntsRef;
@@ -65,7 +65,7 @@ public class SumScoreFacetsAggregator implements FacetsAggregator {
   }
 
   @Override
-  public void rollupValues(int ordinal, int[] children, int[] siblings, FacetArrays facetArrays) {
+  public void rollupValues(FacetRequest fr, int ordinal, int[] children, int[] siblings, FacetArrays facetArrays) {
     float[] scores = facetArrays.getFloatArray();
     scores[ordinal] += rollupScores(children[ordinal], children, siblings, scores);
   }
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/TopKFacetResultsHandler.java b/lucene/facet/src/java/org/apache/lucene/facet/search/TopKFacetResultsHandler.java
index f7a47a3..a74fc79 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/TopKFacetResultsHandler.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/search/TopKFacetResultsHandler.java
@@ -3,11 +3,8 @@ package org.apache.lucene.facet.search;
 import java.io.IOException;
 import java.util.ArrayList;
 
-import org.apache.lucene.facet.partitions.search.IntermediateFacetResult;
-import org.apache.lucene.facet.partitions.search.PartitionsFacetResultsHandler;
-import org.apache.lucene.facet.search.params.FacetRequest;
-import org.apache.lucene.facet.search.results.FacetResult;
-import org.apache.lucene.facet.search.results.FacetResultNode;
+import org.apache.lucene.facet.partitions.IntermediateFacetResult;
+import org.apache.lucene.facet.partitions.PartitionsFacetResultsHandler;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.facet.taxonomy.directory.ParallelTaxonomyArrays;
 import org.apache.lucene.facet.util.ResultSortUtils;
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/TopKInEachNodeHandler.java b/lucene/facet/src/java/org/apache/lucene/facet/search/TopKInEachNodeHandler.java
index c486a82..3b5f1c9 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/TopKInEachNodeHandler.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/search/TopKInEachNodeHandler.java
@@ -4,17 +4,14 @@ import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
 
-import org.apache.lucene.facet.partitions.search.IntermediateFacetResult;
-import org.apache.lucene.facet.partitions.search.PartitionsFacetResultsHandler;
-import org.apache.lucene.facet.search.params.FacetRequest;
-import org.apache.lucene.facet.search.params.FacetRequest.SortOrder;
-import org.apache.lucene.facet.search.results.FacetResult;
-import org.apache.lucene.facet.search.results.FacetResultNode;
+import org.apache.lucene.facet.collections.IntIterator;
+import org.apache.lucene.facet.collections.IntToObjectMap;
+import org.apache.lucene.facet.partitions.IntermediateFacetResult;
+import org.apache.lucene.facet.partitions.PartitionsFacetResultsHandler;
+import org.apache.lucene.facet.search.FacetRequest.SortOrder;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.facet.taxonomy.directory.ParallelTaxonomyArrays;
 import org.apache.lucene.util.PriorityQueue;
-import org.apache.lucene.util.collections.IntIterator;
-import org.apache.lucene.util.collections.IntToObjectMap;
 
 /*
  * Licensed to the Apache Software Foundation (ASF) under one or more
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/TotalFacetCounts.java b/lucene/facet/src/java/org/apache/lucene/facet/search/TotalFacetCounts.java
deleted file mode 100644
index 8919b73..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/TotalFacetCounts.java
+++ /dev/null
@@ -1,177 +0,0 @@
-package org.apache.lucene.facet.search;
-
-import java.io.BufferedInputStream;
-import java.io.BufferedOutputStream;
-import java.io.DataInputStream;
-import java.io.DataOutputStream;
-import java.io.File;
-import java.io.FileInputStream;
-import java.io.FileOutputStream;
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.concurrent.atomic.AtomicInteger;
-
-import org.apache.lucene.facet.index.params.CategoryListParams;
-import org.apache.lucene.facet.index.params.FacetIndexingParams;
-import org.apache.lucene.facet.search.aggregator.Aggregator;
-import org.apache.lucene.facet.search.aggregator.CountingAggregator;
-import org.apache.lucene.facet.search.params.CountFacetRequest;
-import org.apache.lucene.facet.search.params.FacetRequest;
-import org.apache.lucene.facet.search.params.FacetSearchParams;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.facet.util.PartitionsUtils;
-import org.apache.lucene.facet.util.ScoredDocIdsUtils;
-import org.apache.lucene.index.IndexReader;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Maintain Total Facet Counts per partition, for given parameters:
- * <ul> 
- *  <li>Index reader of an index</li>
- *  <li>Taxonomy index reader</li>
- *  <li>Facet indexing params (and particularly the category list params)</li>
- *  <li></li>
- * </ul>
- * The total facet counts are maintained as an array of arrays of integers, 
- * where a separate array is kept for each partition.
- * 
- * @lucene.experimental
- */
-public class TotalFacetCounts {
-  
-  /** total facet counts per partition: totalCounts[partition][ordinal%partitionLength] */
-  private int[][] totalCounts = null;
-  
-  private final TaxonomyReader taxonomy;
-  private final FacetIndexingParams facetIndexingParams;
-
-  private final static AtomicInteger atomicGen4Test = new AtomicInteger(1);
-  /** Creation type for test purposes */
-  enum CreationType { Computed, Loaded } // for testing
-  final int gen4test;
-  final CreationType createType4test;
-  
-  /** 
-   * Construct by key - from index Directory or by recomputing.
-   */
-  private TotalFacetCounts (TaxonomyReader taxonomy, FacetIndexingParams facetIndexingParams,
-      int[][] counts, CreationType createType4Test) {
-    this.taxonomy = taxonomy;
-    this.facetIndexingParams = facetIndexingParams;
-    this.totalCounts = counts;
-    this.createType4test = createType4Test;
-    this.gen4test = atomicGen4Test.incrementAndGet();
-  }
-
-  /**
-   * Fill a partition's array with the TotalCountsArray values.
-   * @param partitionArray array to fill
-   * @param partition number of required partition 
-   */
-  public void fillTotalCountsForPartition(int[] partitionArray, int partition) {
-    int partitionSize = partitionArray.length;
-    int[] countArray = totalCounts[partition];
-    if (countArray == null) {
-      countArray = new int[partitionSize];
-      totalCounts[partition] = countArray;
-    }
-    int length = Math.min(partitionSize, countArray.length);
-    System.arraycopy(countArray, 0, partitionArray, 0, length);
-  }
-  
-  /**
-   * Return the total count of an input category
-   * @param ordinal ordinal of category whose total count is required 
-   */
-  public int getTotalCount(int ordinal) {
-    int partition = PartitionsUtils.partitionNumber(facetIndexingParams,ordinal);
-    int offset = ordinal % PartitionsUtils.partitionSize(facetIndexingParams, taxonomy);
-    return totalCounts[partition][offset];
-  }
-  
-  static TotalFacetCounts loadFromFile(File inputFile, TaxonomyReader taxonomy, 
-      FacetIndexingParams facetIndexingParams) throws IOException {
-    DataInputStream dis = new DataInputStream(new BufferedInputStream(new FileInputStream(inputFile)));
-    try {
-      int[][] counts = new int[dis.readInt()][];
-      for (int i=0; i<counts.length; i++) {
-        int size = dis.readInt();
-        if (size<0) {
-          counts[i] = null;
-        } else {
-          counts[i] = new int[size];
-          for (int j=0; j<size; j++) {
-            counts[i][j] = dis.readInt();
-          }
-        }
-      }
-      return new TotalFacetCounts(taxonomy, facetIndexingParams, counts, CreationType.Loaded);
-    } finally {
-      dis.close();
-    }
-  }
-
-  static void storeToFile(File outputFile, TotalFacetCounts tfc) throws IOException {
-    DataOutputStream dos = new DataOutputStream(new BufferedOutputStream(new FileOutputStream(outputFile)));
-    try {
-      dos.writeInt(tfc.totalCounts.length);
-      for (int[] counts : tfc.totalCounts) {
-        if (counts == null) {
-          dos.writeInt(-1);
-        } else {
-          dos.writeInt(counts.length);
-          for (int i : counts) {
-            dos.writeInt(i);
-          }
-        }
-      }
-    } finally {
-      dos.close();
-    }
-  }
-  
-  // needed because FacetSearchParams do not allow empty FacetRequests
-  private static final FacetRequest DUMMY_REQ = new CountFacetRequest(CategoryPath.EMPTY, 1);
-
-  static TotalFacetCounts compute(final IndexReader indexReader, final TaxonomyReader taxonomy, 
-      final FacetIndexingParams facetIndexingParams) throws IOException {
-    int partitionSize = PartitionsUtils.partitionSize(facetIndexingParams, taxonomy);
-    final int[][] counts = new int[(int) Math.ceil(taxonomy.getSize()  /(float) partitionSize)][partitionSize];
-    FacetSearchParams newSearchParams = new FacetSearchParams(facetIndexingParams, DUMMY_REQ); 
-      //createAllListsSearchParams(facetIndexingParams,  this.totalCounts);
-    StandardFacetsAccumulator sfa = new StandardFacetsAccumulator(newSearchParams, indexReader, taxonomy) {
-      @Override
-      protected HashMap<CategoryListIterator, Aggregator> getCategoryListMap(
-          FacetArrays facetArrays, int partition) throws IOException {
-        
-        Aggregator aggregator = new CountingAggregator(counts[partition]);
-        HashMap<CategoryListIterator, Aggregator> map = new HashMap<CategoryListIterator, Aggregator>();
-        for (CategoryListParams clp: facetIndexingParams.getAllCategoryListParams()) {
-          map.put(clp.createCategoryListIterator(partition), aggregator);
-        }
-        return map;
-      }
-    };
-    sfa.setComplementThreshold(StandardFacetsAccumulator.DISABLE_COMPLEMENT);
-    sfa.accumulate(ScoredDocIdsUtils.createAllDocsScoredDocIDs(indexReader));
-    return new TotalFacetCounts(taxonomy, facetIndexingParams, counts, CreationType.Computed);
-  }
-  
-}
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/TotalFacetCountsCache.java b/lucene/facet/src/java/org/apache/lucene/facet/search/TotalFacetCountsCache.java
deleted file mode 100644
index 95ba960..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/TotalFacetCountsCache.java
+++ /dev/null
@@ -1,299 +0,0 @@
-package org.apache.lucene.facet.search;
-
-import java.io.File;
-import java.io.IOException;
-import java.util.Iterator;
-import java.util.LinkedHashMap;
-import java.util.concurrent.ConcurrentHashMap;
-import java.util.concurrent.ConcurrentLinkedQueue;
-
-import org.apache.lucene.facet.index.params.CategoryListParams;
-import org.apache.lucene.facet.index.params.FacetIndexingParams;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.index.IndexReader;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Manage an LRU cache for {@link TotalFacetCounts} per index, taxonomy, and
- * facet indexing params.
- * 
- * @lucene.experimental
- */
-public final class TotalFacetCountsCache {
-  
-  /**
-   * Default size of in memory cache for computed total facet counts.
-   * Set to 2 for the case when an application reopened a reader and 
-   * the original one is still in use (Otherwise there will be 
-   * switching again and again between the two.) 
-   */
-  public static final int DEFAULT_CACHE_SIZE = 2; 
-
-  private static final TotalFacetCountsCache singleton = new TotalFacetCountsCache();
-  
-  /**
-   * Get the single instance of this cache
-   */
-  public static TotalFacetCountsCache getSingleton() {
-    return singleton;
-  }
-  
-  /**
-   * In-memory cache of TFCs.
-   * <ul>  
-   * <li>It's size is kept within limits through {@link #trimCache()}.
-   * <li>An LRU eviction policy is applied, by maintaining active keys in {@link #lruKeys}. 
-   * <li>After each addition to the cache, trimCache is called, to remove entries least recently used.
-   * </ul>  
-   * @see #markRecentlyUsed(TFCKey)
-   */
-  private ConcurrentHashMap<TFCKey,TotalFacetCounts> cache = new ConcurrentHashMap<TFCKey,TotalFacetCounts>();
-  
-  /**
-   * A queue of active keys for applying LRU policy on eviction from the {@link #cache}.
-   * @see #markRecentlyUsed(TFCKey)
-   */
-  private ConcurrentLinkedQueue<TFCKey> lruKeys = new ConcurrentLinkedQueue<TFCKey>();
-  
-  private int maxCacheSize = DEFAULT_CACHE_SIZE; 
-  
-  /** private constructor for singleton pattern */ 
-  private TotalFacetCountsCache() {
-  }
-  
-  /**
-   * Get the total facet counts for a reader/taxonomy pair and facet indexing
-   * parameters. If not in cache, computed here and added to the cache for later
-   * use.
-   * 
-   * @param indexReader
-   *          the documents index
-   * @param taxonomy
-   *          the taxonomy index
-   * @param facetIndexingParams
-   *          facet indexing parameters
-   * @return the total facet counts.
-   */
-  public TotalFacetCounts getTotalCounts(IndexReader indexReader, TaxonomyReader taxonomy,
-      FacetIndexingParams facetIndexingParams) throws IOException {
-    // create the key
-    TFCKey key = new TFCKey(indexReader, taxonomy, facetIndexingParams);
-    // it is important that this call is not synchronized, so that available TFC 
-    // would not wait for one that needs to be computed.  
-    TotalFacetCounts tfc = cache.get(key);
-    if (tfc != null) {
-      markRecentlyUsed(key); 
-      return tfc;
-    }
-    return computeAndCache(key);
-  }
-
-  /**
-   * Mark key as it as recently used.
-   * <p>
-   * <b>Implementation notes: Synchronization considerations and the interaction between lruKeys and cache:</b>
-   * <ol>
-   *  <li>A concurrent {@link LinkedHashMap} would have made this class much simpler.
-   *      But unfortunately, Java does not provide one.
-   *      Instead, we combine two concurrent objects:
-   *  <ul>
-   *   <li>{@link ConcurrentHashMap} for the cached TFCs.
-   *   <li>{@link ConcurrentLinkedQueue} for active keys
-   *  </ul>
-   *  <li>Both {@link #lruKeys} and {@link #cache} are concurrently safe.
-   *  <li>Checks for a cached item through getTotalCounts() are not synchronized.
-   *      Therefore, the case that a needed TFC is in the cache is very fast:
-   *      it does not wait for the computation of other TFCs.
-   *  <li>computeAndCache() is synchronized, and, has a (double) check of the required
-   *       TFC, to avoid computing the same TFC twice. 
-   *  <li>A race condition in this method (markRecentlyUsed) might result in two copies 
-   *      of the same 'key' in lruKeys, but this is handled by the loop in trimCache(), 
-   *      where an attempt to remove the same key twice is a no-op.
-   * </ol>
-   */
-  private void markRecentlyUsed(TFCKey key) {
-    lruKeys.remove(key);  
-    lruKeys.add(key);
-  }
-
-  private synchronized void trimCache() {
-    // loop until cache is of desired  size.
-    while (cache.size()>maxCacheSize ) { 
-      TFCKey key = lruKeys.poll();
-      if (key==null) { //defensive
-        // it is defensive since lruKeys presumably covers the cache keys 
-        key = cache.keys().nextElement(); 
-      }
-      // remove this element. Note that an attempt to remove with the same key again is a no-op,
-      // which gracefully handles the possible race in markRecentlyUsed(). 
-      cache.remove(key);
-    }
-  }
-  
-  /**
-   * compute TFC and cache it, after verifying it was not just added - for this
-   * matter this method is synchronized, which is not too bad, because there is
-   * lots of work done in the computations.
-   */
-  private synchronized TotalFacetCounts computeAndCache(TFCKey key) throws IOException {
-    TotalFacetCounts tfc = cache.get(key); 
-    if (tfc == null) {
-      tfc = TotalFacetCounts.compute(key.indexReader, key.taxonomy, key.facetIndexingParams);
-      lruKeys.add(key);
-      cache.put(key,tfc);
-      trimCache();
-    }
-    return tfc;
-  }
-
-  /**
-   * Load {@link TotalFacetCounts} matching input parameters from the provided
-   * outputFile and add them into the cache for the provided indexReader,
-   * taxonomy, and facetIndexingParams. If a {@link TotalFacetCounts} for these
-   * parameters already exists in the cache, it will be replaced by the loaded
-   * one.
-   * 
-   * @param inputFile
-   *          file from which to read the data
-   * @param indexReader
-   *          the documents index
-   * @param taxonomy
-   *          the taxonomy index
-   * @param facetIndexingParams
-   *          the facet indexing parameters
-   * @throws IOException
-   *           on error
-   */
-  public synchronized void load(File inputFile, IndexReader indexReader, TaxonomyReader taxonomy,
-      FacetIndexingParams facetIndexingParams) throws IOException {
-    if (!inputFile.isFile() || !inputFile.exists() || !inputFile.canRead()) {
-      throw new IllegalArgumentException("Exepecting an existing readable file: "+inputFile);
-    }
-    TFCKey key = new TFCKey(indexReader, taxonomy, facetIndexingParams);
-    TotalFacetCounts tfc = TotalFacetCounts.loadFromFile(inputFile, taxonomy, facetIndexingParams);
-    cache.put(key,tfc);
-    trimCache();
-    markRecentlyUsed(key);
-  }
-  
-  /**
-   * Store the {@link TotalFacetCounts} matching input parameters into the
-   * provided outputFile, making them available for a later call to
-   * {@link #load(File, IndexReader, TaxonomyReader, FacetIndexingParams)}. If
-   * these {@link TotalFacetCounts} are available in the cache, they are used.
-   * But if they are not in the cache, this call will first compute them (which
-   * will also add them to the cache).
-   * 
-   * @param outputFile
-   *          file to store in.
-   * @param indexReader
-   *          the documents index
-   * @param taxonomy
-   *          the taxonomy index
-   * @param facetIndexingParams
-   *          the facet indexing parameters
-   * @throws IOException
-   *           on error
-   * @see #load(File, IndexReader, TaxonomyReader, FacetIndexingParams)
-   */
-  public void store(File outputFile, IndexReader indexReader, TaxonomyReader taxonomy,
-      FacetIndexingParams facetIndexingParams) throws IOException {
-    File parentFile = outputFile.getParentFile();
-    if (
-        ( outputFile.exists() && (!outputFile.isFile()      || !outputFile.canWrite())) ||
-        (!outputFile.exists() && (!parentFile.isDirectory() || !parentFile.canWrite()))
-      ) {
-      throw new IllegalArgumentException("Exepecting a writable file: "+outputFile);
-    }
-    TotalFacetCounts tfc = getTotalCounts(indexReader, taxonomy, facetIndexingParams);
-    TotalFacetCounts.storeToFile(outputFile, tfc);  
-  }
-  
-  private static class TFCKey {
-    final IndexReader indexReader;
-    final TaxonomyReader taxonomy;
-    private final Iterable<CategoryListParams> clps;
-    private final int hashCode;
-    private final int nDels; // needed when a reader used for faceted search was just used for deletion. 
-    final FacetIndexingParams facetIndexingParams;
-    
-    public TFCKey(IndexReader indexReader, TaxonomyReader taxonomy,
-        FacetIndexingParams facetIndexingParams) {
-      this.indexReader = indexReader;
-      this.taxonomy = taxonomy;
-      this.facetIndexingParams = facetIndexingParams;
-      this.clps = facetIndexingParams.getAllCategoryListParams();
-      this.nDels = indexReader.numDeletedDocs();
-      hashCode = indexReader.hashCode() ^ taxonomy.hashCode();
-    }
-    
-    @Override
-    public int hashCode() {
-      return hashCode;
-    }
-    
-    @Override
-    public boolean equals(Object other) {
-      TFCKey o = (TFCKey) other; 
-      if (indexReader != o.indexReader || taxonomy != o.taxonomy || nDels != o.nDels) {
-        return false;
-      }
-      Iterator<CategoryListParams> it1 = clps.iterator();
-      Iterator<CategoryListParams> it2 = o.clps.iterator();
-      while (it1.hasNext() && it2.hasNext()) {
-        if (!it1.next().equals(it2.next())) {
-          return false;
-        }
-      }
-      return it1.hasNext() == it2.hasNext();
-    }
-  }
-
-  /**
-   * Clear the cache.
-   */
-  public synchronized void clear() {
-    cache.clear();
-    lruKeys.clear();
-  }
-  
-  /**
-   * @return the maximal cache size
-   */
-  public int getCacheSize() {
-    return maxCacheSize;
-  }
-
-  /**
-   * Set the number of TotalFacetCounts arrays that will remain in memory cache.
-   * <p>
-   * If new size is smaller than current size, the cache is appropriately trimmed.
-   * <p>
-   * Minimal size is 1, so passing zero or negative size would result in size of 1.
-   * @param size new size to set
-   */
-  public void setCacheSize(int size) {
-    if (size < 1) size = 1;
-    int origSize = maxCacheSize;
-    maxCacheSize = size;
-    if (maxCacheSize < origSize) { // need to trim only if the cache was reduced
-      trimCache();
-    }
-  }
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/aggregator/Aggregator.java b/lucene/facet/src/java/org/apache/lucene/facet/search/aggregator/Aggregator.java
deleted file mode 100644
index 2eac915..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/aggregator/Aggregator.java
+++ /dev/null
@@ -1,48 +0,0 @@
-package org.apache.lucene.facet.search.aggregator;
-
-import java.io.IOException;
-
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.util.IntsRef;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Aggregates the categories of documents given to
- * {@link #aggregate(int, float, IntsRef)}. Note that the document IDs are local
- * to the reader given to {@link #setNextReader(AtomicReaderContext)}.
- * 
- * @lucene.experimental
- */
-public interface Aggregator {
-
-  /**
-   * Sets the {@link AtomicReaderContext} for which
-   * {@link #aggregate(int, float, IntsRef)} calls will be made. If this method
-   * returns false, {@link #aggregate(int, float, IntsRef)} should not be called
-   * for this reader.
-   */
-  public boolean setNextReader(AtomicReaderContext context) throws IOException;
-  
-  /**
-   * Aggregate the ordinals of the given document ID (and its score). The given
-   * ordinals offset is always zero.
-   */
-  public void aggregate(int docID, float score, IntsRef ordinals) throws IOException;
-  
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/aggregator/ComplementCountingAggregator.java b/lucene/facet/src/java/org/apache/lucene/facet/search/aggregator/ComplementCountingAggregator.java
deleted file mode 100644
index 50ca39f..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/aggregator/ComplementCountingAggregator.java
+++ /dev/null
@@ -1,44 +0,0 @@
-package org.apache.lucene.facet.search.aggregator;
-
-import java.io.IOException;
-
-import org.apache.lucene.util.IntsRef;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * A {@link CountingAggregator} used during complement counting.
- * 
- * @lucene.experimental
- */
-public class ComplementCountingAggregator extends CountingAggregator {
-
-  public ComplementCountingAggregator(int[] counterArray) {
-    super(counterArray);
-  }
-
-  @Override
-  public void aggregate(int docID, float score, IntsRef ordinals) throws IOException {
-    for (int i = 0; i < ordinals.length; i++) {
-      int ord = ordinals.ints[i];
-      assert counterArray[ord] != 0 : "complement aggregation: count is about to become negative for ordinal " + ord;
-      --counterArray[ord];
-    }
-  }
-
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/aggregator/CountingAggregator.java b/lucene/facet/src/java/org/apache/lucene/facet/search/aggregator/CountingAggregator.java
deleted file mode 100644
index 53c03ca..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/aggregator/CountingAggregator.java
+++ /dev/null
@@ -1,66 +0,0 @@
-package org.apache.lucene.facet.search.aggregator;
-
-import java.io.IOException;
-
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.util.IntsRef;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * A CountingAggregator updates a counter array with the size of the whole
- * taxonomy, counting the number of times each category appears in the given set
- * of documents.
- * 
- * @lucene.experimental
- */
-public class CountingAggregator implements Aggregator {
-
-  protected int[] counterArray;
-  
-  public CountingAggregator(int[] counterArray) {
-    this.counterArray = counterArray;
-  }
-  
-  @Override
-  public void aggregate(int docID, float score, IntsRef ordinals) throws IOException {
-    for (int i = 0; i < ordinals.length; i++) {
-      counterArray[ordinals.ints[i]]++;
-    }
-  }
-
-  @Override
-  public boolean equals(Object obj) {
-    if (obj == null || obj.getClass() != this.getClass()) {
-      return false;
-    }
-    CountingAggregator that = (CountingAggregator) obj;
-    return that.counterArray == this.counterArray;
-  }
-
-  @Override
-  public int hashCode() {
-    return counterArray == null ? 0 : counterArray.hashCode();
-  }
-  
-  @Override
-  public boolean setNextReader(AtomicReaderContext context) throws IOException {
-    return true;
-  }
-  
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/aggregator/ScoringAggregator.java b/lucene/facet/src/java/org/apache/lucene/facet/search/aggregator/ScoringAggregator.java
deleted file mode 100644
index 4b0a670..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/aggregator/ScoringAggregator.java
+++ /dev/null
@@ -1,67 +0,0 @@
-package org.apache.lucene.facet.search.aggregator;
-
-import java.io.IOException;
-
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.util.IntsRef;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * An {@link Aggregator} which updates the weight of a category according to the
- * scores of the documents it was found in.
- * 
- * @lucene.experimental
- */
-public class ScoringAggregator implements Aggregator {
-
-  private final float[] scoreArray;
-  private final int hashCode;
-  
-  public ScoringAggregator(float[] counterArray) {
-    this.scoreArray = counterArray;
-    this.hashCode = scoreArray == null ? 0 : scoreArray.hashCode();
-  }
-
-  @Override
-  public void aggregate(int docID, float score, IntsRef ordinals) throws IOException {
-    for (int i = 0; i < ordinals.length; i++) {
-      scoreArray[ordinals.ints[i]] += score;
-    }
-  }
-  
-  @Override
-  public boolean equals(Object obj) {
-    if (obj == null || obj.getClass() != this.getClass()) {
-      return false;
-    }
-    ScoringAggregator that = (ScoringAggregator) obj;
-    return that.scoreArray == this.scoreArray;
-  }
-
-  @Override
-  public int hashCode() {
-    return hashCode;
-  }
-
-  @Override
-  public boolean setNextReader(AtomicReaderContext context) throws IOException {
-    return true;
-  }
-  
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/aggregator/associations/AssociationFloatSumAggregator.java b/lucene/facet/src/java/org/apache/lucene/facet/search/aggregator/associations/AssociationFloatSumAggregator.java
deleted file mode 100644
index d35428f..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/aggregator/associations/AssociationFloatSumAggregator.java
+++ /dev/null
@@ -1,84 +0,0 @@
-package org.apache.lucene.facet.search.aggregator.associations;
-
-import java.io.IOException;
-
-import org.apache.lucene.facet.associations.CategoryFloatAssociation;
-import org.apache.lucene.facet.associations.FloatAssociationsIterator;
-import org.apache.lucene.facet.index.params.CategoryListParams;
-import org.apache.lucene.facet.search.aggregator.Aggregator;
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.util.IntsRef;
-import org.apache.lucene.util.collections.IntToFloatMap;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * An {@link Aggregator} which computes the weight of a category as the sum of
- * the float values associated with it in the result documents.
- * 
- * @lucene.experimental
- */
-public class AssociationFloatSumAggregator implements Aggregator {
-
-  protected final String field;
-  protected final float[] sumArray;
-  protected final FloatAssociationsIterator associations;
-
-  public AssociationFloatSumAggregator(float[] sumArray) throws IOException {
-    this(CategoryListParams.DEFAULT_FIELD, sumArray);
-  }
-  
-  public AssociationFloatSumAggregator(String field, float[] sumArray) throws IOException {
-    this.field = field;
-    associations = new FloatAssociationsIterator(field, new CategoryFloatAssociation());
-    this.sumArray = sumArray;
-  }
-
-  @Override
-  public void aggregate(int docID, float score, IntsRef ordinals) throws IOException {
-    IntToFloatMap values = associations.getAssociations(docID);
-    if (values != null) {
-      for (int i = 0; i < ordinals.length; i++) {
-        int ord = ordinals.ints[i];
-        if (values.containsKey(ord)) {
-          sumArray[ord] += values.get(ord);
-        }
-      }
-    }
-  }
-  
-  @Override
-  public boolean equals(Object obj) {
-    if (obj == null || obj.getClass() != this.getClass()) {
-      return false;
-    }
-    AssociationFloatSumAggregator that = (AssociationFloatSumAggregator) obj;
-    return that.field.equals(field) && that.sumArray == sumArray;
-  }
-
-  @Override
-  public int hashCode() {
-    return field.hashCode();
-  }
-
-  @Override
-  public boolean setNextReader(AtomicReaderContext context) throws IOException {
-    return associations.setNextReader(context);
-  }
-  
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/aggregator/associations/AssociationIntSumAggregator.java b/lucene/facet/src/java/org/apache/lucene/facet/search/aggregator/associations/AssociationIntSumAggregator.java
deleted file mode 100644
index ec20dc3..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/aggregator/associations/AssociationIntSumAggregator.java
+++ /dev/null
@@ -1,84 +0,0 @@
-package org.apache.lucene.facet.search.aggregator.associations;
-
-import java.io.IOException;
-
-import org.apache.lucene.facet.associations.CategoryIntAssociation;
-import org.apache.lucene.facet.associations.IntAssociationsIterator;
-import org.apache.lucene.facet.index.params.CategoryListParams;
-import org.apache.lucene.facet.search.aggregator.Aggregator;
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.util.IntsRef;
-import org.apache.lucene.util.collections.IntToIntMap;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * An {@link Aggregator} which computes the weight of a category as the sum of
- * the integer values associated with it in the result documents.
- * 
- * @lucene.experimental
- */
-public class AssociationIntSumAggregator implements Aggregator {
-
-  protected final String field;
-  protected final int[] sumArray;
-  protected final IntAssociationsIterator associations;
-
-  public AssociationIntSumAggregator(int[] sumArray) throws IOException {
-    this(CategoryListParams.DEFAULT_FIELD, sumArray);
-  }
-  
-  public AssociationIntSumAggregator(String field, int[] sumArray) throws IOException {
-    this.field = field;
-    associations = new IntAssociationsIterator(field, new CategoryIntAssociation());
-    this.sumArray = sumArray;
-  }
-
-  @Override
-  public void aggregate(int docID, float score, IntsRef ordinals) throws IOException {
-    IntToIntMap values = associations.getAssociations(docID);
-    if (values != null) {
-      for (int i = 0; i < ordinals.length; i++) {
-        int ord = ordinals.ints[i];
-        if (values.containsKey(ord)) {
-          sumArray[ord] += values.get(ord);
-        }
-      }
-    }
-  }
-  
-  @Override
-  public boolean equals(Object obj) {
-    if (obj == null || obj.getClass() != this.getClass()) {
-      return false;
-    }
-    AssociationIntSumAggregator that = (AssociationIntSumAggregator) obj;
-    return that.field.equals(field) && that.sumArray == sumArray;
-  }
-
-  @Override
-  public int hashCode() {
-    return field.hashCode();
-  }
-
-  @Override
-  public boolean setNextReader(AtomicReaderContext context) throws IOException {
-    return associations.setNextReader(context);
-  }
-
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/aggregator/associations/package.html b/lucene/facet/src/java/org/apache/lucene/facet/search/aggregator/associations/package.html
deleted file mode 100644
index 53e891c..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/aggregator/associations/package.html
+++ /dev/null
@@ -1,22 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html><head></head>
-<body>
-Association-based aggregators.
-</body>
-</html>
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/aggregator/package.html b/lucene/facet/src/java/org/apache/lucene/facet/search/aggregator/package.html
deleted file mode 100644
index 932c293..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/aggregator/package.html
+++ /dev/null
@@ -1,28 +0,0 @@
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-  <head>
-    <title>Aggregating Facets during Faceted Search</title>
-  </head>
-  <body>
-    <h1>Aggregating Facets during Faceted Search</h1>
-    
-    A facets aggregator is the parallel of Lucene's Collector. 
-    While Collector collected matching documents,
-    an aggregator aggregates facets of a matching document.
-  </body>
-</html>
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/package.html b/lucene/facet/src/java/org/apache/lucene/facet/search/package.html
index 5a99758..94b2b36 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/package.html
+++ b/lucene/facet/src/java/org/apache/lucene/facet/search/package.html
@@ -15,44 +15,10 @@
  limitations under the License.
 -->
 <html>
-  <head>
-    <title>Faceted Search API</title>
-  </head>
-  <body>
-    <h1>Faceted Search API</h1>
-    
-    API for faceted search has several interfaces - simple, top level ones, adequate for most users,
-    and advanced, more complicated ones, for the more advanced users. 
-    
-    <p>
-    
-    We now describe the simpler interfaces.
-    There are mainly 3 interfaces for faceted search:
-    <ol>
-      <li>{@link org.apache.lucene.facet.search.params.FacetRequest Facets Request}
-          defines requirements:
-          <ul> 
-              <li>which facets are required, e.g. depth</li>
-              <li>what is computed for each facet - e.g. count, score.</li>
-          </ul>
-      </li>
-      <li>{@link org.apache.lucene.facet.search.FacetsAccumulator Facets Extractor}
-          Controls how facets are extracted, with variations of:
-          <ul> 
-              <li>default (partitioned, like all extractors). </li>
-              <li>sampled - inspects only a fraction of the documents.</li>
-          </ul>
-      </li>
-      <li>{@link org.apache.lucene.facet.search.FacetResultsHandler Facet Results Handler }
-          Controls how results are further processed and merged (also between partitions):
-          <ul> 
-              <li>Top K.</li>
-              <li>Tree.</li>
-              <li>Tree with top K at each level</li>
-              <li>...</li>
-          </ul>
-      </li>
-    </ol>
-
-  </body>
+<head>
+<title>Facets search code</title>
+</head>
+<body>
+Facets search code.
+</body>
 </html>
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/params/CountFacetRequest.java b/lucene/facet/src/java/org/apache/lucene/facet/search/params/CountFacetRequest.java
deleted file mode 100644
index de033ef..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/params/CountFacetRequest.java
+++ /dev/null
@@ -1,58 +0,0 @@
-package org.apache.lucene.facet.search.params;
-
-import org.apache.lucene.facet.search.FacetArrays;
-import org.apache.lucene.facet.search.aggregator.Aggregator;
-import org.apache.lucene.facet.search.aggregator.ComplementCountingAggregator;
-import org.apache.lucene.facet.search.aggregator.CountingAggregator;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Facet request for counting facets.
- * 
- * @lucene.experimental
- */
-public class CountFacetRequest extends FacetRequest {
-
-  public CountFacetRequest(CategoryPath path, int num) {
-    super(path, num);
-  }
-
-  @Override
-  public Aggregator createAggregator(boolean useComplements, FacetArrays arrays, TaxonomyReader taxonomy) {
-    // we rely on that, if needed, result is cleared by arrays!
-    int[] a = arrays.getIntArray();
-    if (useComplements) {
-      return new ComplementCountingAggregator(a);
-    }
-    return new CountingAggregator(a);
-  }
-
-  @Override
-  public double getValueOf(FacetArrays arrays, int ordinal) {
-    return arrays.getIntArray()[ordinal];
-  }
-
-  @Override
-  public FacetArraysSource getFacetArraysSource() {
-    return FacetArraysSource.INT;
-  }
-  
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/params/FacetRequest.java b/lucene/facet/src/java/org/apache/lucene/facet/search/params/FacetRequest.java
deleted file mode 100644
index 948267f..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/params/FacetRequest.java
+++ /dev/null
@@ -1,296 +0,0 @@
-package org.apache.lucene.facet.search.params;
-
-import java.io.IOException;
-
-import org.apache.lucene.facet.search.FacetArrays;
-import org.apache.lucene.facet.search.FacetResultsHandler;
-import org.apache.lucene.facet.search.aggregator.Aggregator;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Request to accumulate facet information for a specified facet and possibly 
- * also some of its descendants, upto a specified depth.
- * <p>
- * The facet request additionally defines what information should 
- * be computed within the facet results, if and how should results
- * be ordered, etc.
- * <P>
- * An example facet request is to look at all sub-categories of "Author", and
- * return the 10 with the highest counts (sorted by decreasing count). 
- * 
- * @lucene.experimental
- */
-public abstract class FacetRequest {
-
-  /**
-   * Result structure manner of applying request's limits such as
-   * {@link FacetRequest#getNumLabel()} and {@link FacetRequest#numResults}.
-   * Only relevant when {@link FacetRequest#getDepth()} is &gt; 1.
-   */
-  public enum ResultMode { 
-    /** Limits are applied per node, and the result has a full tree structure. */
-    PER_NODE_IN_TREE, 
-
-    /** Limits are applied globally, on total number of results, and the result has a flat structure. */
-    GLOBAL_FLAT
-  }
-  
-  /**
-   * Specifies which array of {@link FacetArrays} should be used to resolve
-   * values. When set to {@link #INT} or {@link #FLOAT}, allows creating an
-   * optimized {@link FacetResultsHandler}, which does not call
-   * {@link FacetRequest#getValueOf(FacetArrays, int)} for every ordinals.
-   * <p>
-   * If set to {@link #BOTH}, the {@link FacetResultsHandler} will use
-   * {@link FacetRequest#getValueOf(FacetArrays, int)} to resolve ordinal
-   * values, although it is recommended that you consider writing a specialized
-   * {@link FacetResultsHandler}.
-   */
-  public enum FacetArraysSource { INT, FLOAT, BOTH }
-
-  /** Sort options for facet results. */
-  public enum SortBy { 
-    /** sort by category ordinal with the taxonomy */
-    ORDINAL, 
-
-    /** sort by computed category value */ 
-    VALUE 
-  }
-
-  /** Requested sort order for the results. */
-  public enum SortOrder { ASCENDING, DESCENDING }
-
-  /**
-   * Default depth for facets accumulation.
-   * @see #getDepth()
-   */
-  public static final int DEFAULT_DEPTH = 1;
-  
-  /**
-   * Default sort mode.
-   * @see #getSortBy()
-   */
-  public static final SortBy DEFAULT_SORT_BY = SortBy.VALUE;
-  
-  /**
-   * Default result mode
-   * @see #getResultMode()
-   */
-  public static final ResultMode DEFAULT_RESULT_MODE = ResultMode.PER_NODE_IN_TREE;
-  
-  public final CategoryPath categoryPath;
-  public final int numResults;
-  
-  private int numLabel;
-  private int depth;
-  private SortOrder sortOrder;
-  private SortBy sortBy;
-
-  /**
-   * Computed at construction, this hashCode is based on two final members
-   * {@link CategoryPath} and <code>numResults</code>
-   */
-  private final int hashCode;
-
-  private ResultMode resultMode = DEFAULT_RESULT_MODE;
-
-  /**
-   * Initialize the request with a given path, and a requested number of facets
-   * results. By default, all returned results would be labeled - to alter this
-   * default see {@link #setNumLabel(int)}.
-   * <p>
-   * <b>NOTE:</b> if <code>numResults</code> is given as
-   * <code>Integer.MAX_VALUE</code> than all the facet results would be
-   * returned, without any limit.
-   * <p>
-   * <b>NOTE:</b> it is assumed that the given {@link CategoryPath} is not
-   * modified after construction of this object. Otherwise, some things may not
-   * function properly, e.g. {@link #hashCode()}.
-   * 
-   * @throws IllegalArgumentException if numResults is &le; 0
-   */
-  public FacetRequest(CategoryPath path, int numResults) {
-    if (numResults <= 0) {
-      throw new IllegalArgumentException("num results must be a positive (>0) number: " + numResults);
-    }
-    if (path == null) {
-      throw new IllegalArgumentException("category path cannot be null!");
-    }
-    categoryPath = path;
-    this.numResults = numResults;
-    numLabel = numResults;
-    depth = DEFAULT_DEPTH;
-    sortBy = DEFAULT_SORT_BY;
-    sortOrder = SortOrder.DESCENDING;
-    
-    hashCode = categoryPath.hashCode() ^ this.numResults;
-  }
-
-  /**
-   * Create an aggregator for this facet request. Aggregator action depends on
-   * request definition. For a count request, it will usually increment the
-   * count for that facet.
-   * 
-   * @param useComplements
-   *          whether the complements optimization is being used for current
-   *          computation.
-   * @param arrays
-   *          provider for facet arrays in use for current computation.
-   * @param taxonomy
-   *          reader of taxonomy in effect.
-   * @throws IOException If there is a low-level I/O error.
-   */
-  public abstract Aggregator createAggregator(boolean useComplements, FacetArrays arrays, TaxonomyReader taxonomy) 
-      throws IOException;
-
-  @Override
-  public boolean equals(Object o) {
-    if (o instanceof FacetRequest) {
-      FacetRequest that = (FacetRequest)o;
-      return that.hashCode == this.hashCode &&
-          that.categoryPath.equals(this.categoryPath) &&
-          that.numResults == this.numResults &&
-          that.depth == this.depth &&
-          that.resultMode == this.resultMode &&
-          that.numLabel == this.numLabel;
-    }
-    return false;
-  }
-
-  /**
-   * How deeply to look under the given category. If the depth is 0,
-   * only the category itself is counted. If the depth is 1, its immediate
-   * children are also counted, and so on. If the depth is Integer.MAX_VALUE,
-   * all the category's descendants are counted.<br>
-   */
-  public final int getDepth() {
-    // TODO add AUTO_EXPAND option  
-    return depth;
-  }
-  
-  /**
-   * Returns the {@link FacetArraysSource} this {@link FacetRequest} uses in
-   * {@link #getValueOf(FacetArrays, int)}.
-   */
-  public abstract FacetArraysSource getFacetArraysSource();
-  
-  /**
-   * If getNumLabel() &lt; getNumResults(), only the first getNumLabel() results
-   * will have their category paths calculated, and the rest will only be
-   * available as ordinals (category numbers) and will have null paths.
-   * <P>
-   * If Integer.MAX_VALUE is specified, all results are labled.
-   * <P>
-   * The purpose of this parameter is to avoid having to run the whole faceted
-   * search again when the user asks for more values for the facet; The
-   * application can ask (getNumResults()) for more values than it needs to
-   * show, but keep getNumLabel() only the number it wants to immediately show.
-   * The slow-down caused by finding more values is negligible, because the
-   * slowest part - finding the categories' paths, is avoided.
-   * <p>
-   * Depending on the {@link #getResultMode() LimitsMode}, this limit is applied
-   * globally or per results node. In the global mode, if this limit is 3, only
-   * 3 top results would be labeled. In the per-node mode, if this limit is 3, 3
-   * top children of {@link #categoryPath the target category} would be labeled,
-   * as well as 3 top children of each of them, and so forth, until the depth
-   * defined by {@link #getDepth()}.
-   * 
-   * @see #getResultMode()
-   */
-  public final int getNumLabel() {
-    return numLabel;
-  }
-
-  /** Return the requested result mode. */
-  public final ResultMode getResultMode() {
-    return resultMode;
-  }
-
-  /** Specify how should results be sorted. */
-  public final SortBy getSortBy() {
-    return sortBy;
-  }
-
-  /** Return the requested order of results. */
-  public final SortOrder getSortOrder() {
-    return sortOrder;
-  }
-
-  /**
-   * Return the value of a category used for facets computations for this
-   * request. For a count request this would be the count for that facet, i.e.
-   * an integer number. but for other requests this can be the result of a more
-   * complex operation, and the result can be any double precision number.
-   * Having this method with a general name <b>value</b> which is double
-   * precision allows to have more compact API and code for handling counts and
-   * perhaps other requests (such as for associations) very similarly, and by
-   * the same code and API, avoiding code duplication.
-   * 
-   * @param arrays
-   *          provider for facet arrays in use for current computation.
-   * @param idx
-   *          an index into the count arrays now in effect in
-   *          <code>arrays</code>. E.g., for ordinal number <i>n</i>, with
-   *          partition, of size <i>partitionSize</i>, now covering <i>n</i>,
-   *          <code>getValueOf</code> would be invoked with <code>idx</code>
-   *          being <i>n</i> % <i>partitionSize</i>.
-   */
-  // TODO perhaps instead of getValueOf we can have a postProcess(FacetArrays)
-  // That, together with getFacetArraysSource should allow ResultHandlers to
-  // efficiently obtain the values from the arrays directly
-  public abstract double getValueOf(FacetArrays arrays, int idx);
-
-  @Override
-  public int hashCode() {
-    return hashCode; 
-  }
-
-  public void setDepth(int depth) {
-    this.depth = depth;
-  }
-
-  public void setNumLabel(int numLabel) {
-    this.numLabel = numLabel;
-  }
-  
-  /**
-   * @param resultMode the resultMode to set
-   * @see #getResultMode()
-   */
-  public void setResultMode(ResultMode resultMode) {
-    this.resultMode = resultMode;
-  }
-
-  public void setSortBy(SortBy sortBy) {
-    this.sortBy = sortBy;
-  }
-
-  public void setSortOrder(SortOrder sortOrder) {
-    this.sortOrder = sortOrder;
-  }
-  
-  @Override
-  public String toString() {
-    return categoryPath.toString()+" nRes="+numResults+" nLbl="+numLabel;
-  }
-
-}
-  
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/params/FacetSearchParams.java b/lucene/facet/src/java/org/apache/lucene/facet/search/params/FacetSearchParams.java
deleted file mode 100644
index 5a361c2..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/params/FacetSearchParams.java
+++ /dev/null
@@ -1,97 +0,0 @@
-package org.apache.lucene.facet.search.params;
-
-import java.util.Arrays;
-import java.util.List;
-
-import org.apache.lucene.facet.index.params.FacetIndexingParams;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Defines parameters that are needed for faceted search. The list of
- * {@link FacetRequest facet requests} denotes the facets for which aggregated
- * should be done.
- * <p>
- * One can pass {@link FacetIndexingParams} in order to tell the search code how
- * to read the facets information. Note that you must use the same
- * {@link FacetIndexingParams} that were used for indexing.
- * 
- * @lucene.experimental
- */
-public class FacetSearchParams {
-
-  public final FacetIndexingParams indexingParams;
-  public final List<FacetRequest> facetRequests;
-  
-  /**
-   * Initializes with the given {@link FacetRequest requests} and default
-   * {@link FacetIndexingParams#ALL_PARENTS}. If you used a different
-   * {@link FacetIndexingParams}, you should use
-   * {@link #FacetSearchParams(FacetIndexingParams, List)}.
-   */
-  public FacetSearchParams(FacetRequest... facetRequests) {
-    this(FacetIndexingParams.ALL_PARENTS, Arrays.asList(facetRequests));
-  }
-  
-  /**
-   * Initializes with the given {@link FacetRequest requests} and default
-   * {@link FacetIndexingParams#ALL_PARENTS}. If you used a different
-   * {@link FacetIndexingParams}, you should use
-   * {@link #FacetSearchParams(FacetIndexingParams, List)}.
-   */
-  public FacetSearchParams(List<FacetRequest> facetRequests) {
-    this(FacetIndexingParams.ALL_PARENTS, facetRequests);
-  }
-  
-  /**
-   * Initializes with the given {@link FacetRequest requests} and
-   * {@link FacetIndexingParams}.
-   */
-  public FacetSearchParams(FacetIndexingParams indexingParams, FacetRequest... facetRequests) {
-    this(indexingParams, Arrays.asList(facetRequests));
-  }
-
-  /**
-   * Initializes with the given {@link FacetRequest requests} and
-   * {@link FacetIndexingParams}.
-   */
-  public FacetSearchParams(FacetIndexingParams indexingParams, List<FacetRequest> facetRequests) {
-    if (facetRequests == null || facetRequests.size() == 0) {
-      throw new IllegalArgumentException("at least one FacetRequest must be defined");
-    }
-    this.facetRequests = facetRequests;
-    this.indexingParams = indexingParams;
-  }
-  
-  @Override
-  public String toString() {
-    final String INDENT = "  ";
-    final char NEWLINE = '\n';
-
-    StringBuilder sb = new StringBuilder("IndexingParams: ");
-    sb.append(NEWLINE).append(INDENT).append(indexingParams);
-    
-    sb.append(NEWLINE).append("FacetRequests:");
-    for (FacetRequest facetRequest : facetRequests) {
-      sb.append(NEWLINE).append(INDENT).append(facetRequest);
-    }
-    
-    return sb.toString();
-  }
-  
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/params/SumScoreFacetRequest.java b/lucene/facet/src/java/org/apache/lucene/facet/search/params/SumScoreFacetRequest.java
deleted file mode 100644
index cc5319f..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/params/SumScoreFacetRequest.java
+++ /dev/null
@@ -1,55 +0,0 @@
-package org.apache.lucene.facet.search.params;
-
-import org.apache.lucene.facet.search.FacetArrays;
-import org.apache.lucene.facet.search.aggregator.Aggregator;
-import org.apache.lucene.facet.search.aggregator.ScoringAggregator;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * A {@link FacetRequest} for weighting facets by summing the scores of matching
- * documents.
- * 
- * @lucene.experimental
- */
-public class SumScoreFacetRequest extends FacetRequest {
-
-  /** Create a score facet request for a given node in the taxonomy. */
-  public SumScoreFacetRequest(CategoryPath path, int num) {
-    super(path, num);
-  }
-
-  @Override
-  public Aggregator createAggregator(boolean useComplements, FacetArrays arrays, TaxonomyReader taxonomy) {
-    assert !useComplements : "complements are not supported by this FacetRequest";
-    return new ScoringAggregator(arrays.getFloatArray());
-  }
-
-  @Override
-  public double getValueOf(FacetArrays arrays, int ordinal) {
-    return arrays.getFloatArray()[ordinal];
-  }
-
-  @Override
-  public FacetArraysSource getFacetArraysSource() {
-    return FacetArraysSource.FLOAT;
-  }
-
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/params/associations/AssociationFloatSumFacetRequest.java b/lucene/facet/src/java/org/apache/lucene/facet/search/params/associations/AssociationFloatSumFacetRequest.java
deleted file mode 100644
index 0b7a281..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/params/associations/AssociationFloatSumFacetRequest.java
+++ /dev/null
@@ -1,65 +0,0 @@
-package org.apache.lucene.facet.search.params.associations;
-
-import java.io.IOException;
-
-import org.apache.lucene.facet.search.FacetArrays;
-import org.apache.lucene.facet.search.aggregator.Aggregator;
-import org.apache.lucene.facet.search.aggregator.associations.AssociationFloatSumAggregator;
-import org.apache.lucene.facet.search.params.FacetRequest;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * A {@link FacetRequest} for weighting facets according to their float
- * association by summing the association values. Note that this class caches
- * the associations data in-memory by default. You can override
- * {@link #createAggregator(boolean, FacetArrays, TaxonomyReader)} to return an
- * {@link AssociationFloatSumAggregator} which does otherwise.
- * 
- * @lucene.experimental
- */
-public class AssociationFloatSumFacetRequest extends FacetRequest {
-
-  /**
-   * Create a float association facet request for a given node in the
-   * taxonomy.
-   */
-  public AssociationFloatSumFacetRequest(CategoryPath path, int num) {
-    super(path, num);
-  }
-
-  @Override
-  public Aggregator createAggregator(boolean useComplements, FacetArrays arrays, TaxonomyReader taxonomy) 
-      throws IOException {
-    assert !useComplements : "complements are not supported by this FacetRequest";
-    return new AssociationFloatSumAggregator(arrays.getFloatArray());
-  }
-
-  @Override
-  public double getValueOf(FacetArrays arrays, int ordinal) {
-    return arrays.getFloatArray()[ordinal];
-  }
-
-  @Override
-  public FacetArraysSource getFacetArraysSource() {
-    return FacetArraysSource.FLOAT;
-  }
-  
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/params/associations/AssociationIntSumFacetRequest.java b/lucene/facet/src/java/org/apache/lucene/facet/search/params/associations/AssociationIntSumFacetRequest.java
deleted file mode 100644
index f42afe1..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/params/associations/AssociationIntSumFacetRequest.java
+++ /dev/null
@@ -1,66 +0,0 @@
-package org.apache.lucene.facet.search.params.associations;
-
-import java.io.IOException;
-
-import org.apache.lucene.facet.search.FacetArrays;
-import org.apache.lucene.facet.search.aggregator.Aggregator;
-import org.apache.lucene.facet.search.aggregator.associations.AssociationFloatSumAggregator;
-import org.apache.lucene.facet.search.aggregator.associations.AssociationIntSumAggregator;
-import org.apache.lucene.facet.search.params.FacetRequest;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * A {@link FacetRequest} for weighting facets according to their integer
- * association by summing the association values. Note that this class caches
- * the associations data in-memory by default. You can override
- * {@link #createAggregator(boolean, FacetArrays, TaxonomyReader)} to return an
- * {@link AssociationFloatSumAggregator} which does otherwise.
- * 
- * @lucene.experimental
- */
-public class AssociationIntSumFacetRequest extends FacetRequest {
-
-  /**
-   * Create an integer association facet request for a given node in the
-   * taxonomy.
-   */
-  public AssociationIntSumFacetRequest(CategoryPath path, int num) {
-    super(path, num);
-  }
-
-  @Override
-  public Aggregator createAggregator(boolean useComplements, FacetArrays arrays, TaxonomyReader taxonomy) 
-      throws IOException {
-    assert !useComplements : "complements are not supported by this FacetRequest";
-    return new AssociationIntSumAggregator(arrays.getIntArray());
-  }
-
-  @Override
-  public FacetArraysSource getFacetArraysSource() {
-    return FacetArraysSource.INT;
-  }
-
-  @Override
-  public double getValueOf(FacetArrays arrays, int ordinal) {
-    return arrays.getIntArray()[ordinal];
-  }
-
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/params/associations/package.html b/lucene/facet/src/java/org/apache/lucene/facet/search/params/associations/package.html
deleted file mode 100644
index 1e0d9a0..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/params/associations/package.html
+++ /dev/null
@@ -1,22 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html><head></head>
-<body>
-Association-based Parameters for Faceted Search.
-</body>
-</html>
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/params/package.html b/lucene/facet/src/java/org/apache/lucene/facet/search/params/package.html
deleted file mode 100644
index 353cc8d..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/params/package.html
+++ /dev/null
@@ -1,24 +0,0 @@
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-  <head>
-    <title>Parameters for Faceted Search</title>
-  </head>
-  <body>
-    <h1>Parameters for Faceted Search</h1>
-  </body>
-</html>
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/results/FacetResult.java b/lucene/facet/src/java/org/apache/lucene/facet/search/results/FacetResult.java
deleted file mode 100644
index 0a9e575..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/results/FacetResult.java
+++ /dev/null
@@ -1,102 +0,0 @@
-package org.apache.lucene.facet.search.results;
-
-import org.apache.lucene.facet.search.params.FacetRequest;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Result of faceted search.
- * 
- * @lucene.experimental
- */
-public class FacetResult {
-  
-  private final FacetRequest facetRequest;
-  private final FacetResultNode rootNode;
-  private final int numValidDescendants;
-  
-  public FacetResult(FacetRequest facetRequest, FacetResultNode rootNode,  int numValidDescendants) {
-    this.facetRequest = facetRequest;
-    this.rootNode = rootNode;
-    this.numValidDescendants = numValidDescendants;
-  }
-  
-  /**
-   * Facet result node matching the root of the {@link #getFacetRequest() facet request}.
-   * @see #getFacetRequest()
-   * @see FacetRequest#categoryPath
-   */
-  public final FacetResultNode getFacetResultNode() {
-    return this.rootNode;
-  }
-  
-  /**
-   * Number of descendants of {@link #getFacetResultNode() root facet result
-   * node}, up till the requested depth. Typically -- have value != 0. This
-   * number does not include the root node.
-   * 
-   * @see #getFacetRequest()
-   * @see FacetRequest#getDepth()
-   */
-  public final int getNumValidDescendants() {
-    return this.numValidDescendants;
-  }
-  
-  /**
-   * Request for which this result was obtained.
-   */
-  public final FacetRequest getFacetRequest() {
-    return this.facetRequest;
-  }
-  
-  /**
-   * String representation of this facet result.
-   * Use with caution: might return a very long string.
-   * @param prefix prefix for each result line
-   * @see #toString()
-   */
-  public String toString(String prefix) {
-    StringBuilder sb = new StringBuilder();
-    String nl = "";
-    
-    // request
-    if (this.facetRequest != null) {
-      sb.append(nl).append(prefix).append("Request: ").append(
-          this.facetRequest.toString());
-      nl = "\n";
-    }
-    
-    // total facets
-    sb.append(nl).append(prefix).append("Num valid Descendants (up to specified depth): ").append(
-        this.numValidDescendants);
-    nl = "\n";
-    
-    // result node
-    if (this.rootNode != null) {
-      sb.append(nl).append(this.rootNode.toString(prefix + "\t"));
-    }
-    
-    return sb.toString();
-  }
-  
-  @Override
-  public String toString() {
-    return toString("");
-  }
-  
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/results/FacetResultNode.java b/lucene/facet/src/java/org/apache/lucene/facet/search/results/FacetResultNode.java
deleted file mode 100644
index eb7e068..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/results/FacetResultNode.java
+++ /dev/null
@@ -1,101 +0,0 @@
-package org.apache.lucene.facet.search.results;
-
-import java.util.Collections;
-import java.util.List;
-
-import org.apache.lucene.facet.search.params.CountFacetRequest;
-import org.apache.lucene.facet.search.params.FacetRequest;
-import org.apache.lucene.facet.search.params.FacetRequest.ResultMode;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Result of faceted search for a certain taxonomy node. This class serves as a
- * bin of different attributes of the result node, such as its {@link #ordinal}
- * as well as {@link #label}. You are not expected to modify those values.
- * 
- * @lucene.experimental
- */
-public class FacetResultNode {
-
-  public static final List<FacetResultNode> EMPTY_SUB_RESULTS = Collections.emptyList();
-  
-  /** The category ordinal of this node. */
-  public int ordinal;
-
-  /**
-   * The {@link CategoryPath label} of this result. May be {@code null} if not
-   * computed, in which case use {@link TaxonomyReader#getPath(int)} to label
-   * it.
-   * <p>
-   * <b>NOTE:</b> by default, all nodes are labeled. Only when
-   * {@link FacetRequest#getNumLabel()} &lt;
-   * {@link FacetRequest#numResults} there will be unlabeled nodes.
-   */
-  public CategoryPath label;
-  
-  /**
-   * The value of this result. Its actual type depends on the
-   * {@link FacetRequest} used (e.g. in case of {@link CountFacetRequest} it is
-   * {@code int}).
-   */
-  public double value;
-
-  /**
-   * The sub-results of this result. If {@link FacetRequest#getResultMode()} is
-   * {@link ResultMode#PER_NODE_IN_TREE}, every sub result denotes an immediate
-   * child of this node. Otherwise, it is a descendant of any level.
-   * <p>
-   * <b>NOTE:</b> this member should not be {@code null}. To denote that a
-   * result does not have sub results, set it to {@link #EMPTY_SUB_RESULTS} (or
-   * don't modify it).
-   */
-  public List<FacetResultNode> subResults = EMPTY_SUB_RESULTS;
-
-  public FacetResultNode() {
-    // empty constructor
-  }
-  
-  public FacetResultNode(int ordinal, double value) {
-    this.ordinal = ordinal;
-    this.value = value;
-  }
-  
-  @Override
-  public String toString() {
-    return toString("");
-  }
-  
-  /** Returns a String representation of this facet result node. */
-  public String toString(String prefix) {
-    StringBuilder sb = new StringBuilder(prefix);
-    if (label == null) {
-      sb.append("not labeled (ordinal=").append(ordinal).append(")");
-    } else {
-      sb.append(label.toString());
-    }
-    sb.append(" (").append(Double.toString(value)).append(")");
-    for (FacetResultNode sub : subResults) {
-      sb.append("\n").append(prefix).append(sub.toString(prefix + "  "));
-    }
-    return sb.toString();
-  }
-
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/results/package.html b/lucene/facet/src/java/org/apache/lucene/facet/search/results/package.html
deleted file mode 100644
index 1a05c38..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/results/package.html
+++ /dev/null
@@ -1,27 +0,0 @@
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-  <head>
-    <title>Results of Faceted Search</title>
-  </head>
-  <body>
-    <h1>Results of Faceted Search</h1>
-    <p>
-    The results of facets accumulation are obtained as a list of {@link org.apache.lucene.facet.search.results.FacetResult} elements.
-    </p>
-  </body>
-</html>
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/sampling/RandomSampler.java b/lucene/facet/src/java/org/apache/lucene/facet/search/sampling/RandomSampler.java
deleted file mode 100644
index f90a8d9..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/sampling/RandomSampler.java
+++ /dev/null
@@ -1,71 +0,0 @@
-package org.apache.lucene.facet.search.sampling;
-
-import java.io.IOException;
-import java.util.Random;
-
-import org.apache.lucene.facet.search.ScoredDocIDs;
-import org.apache.lucene.facet.search.ScoredDocIDsIterator;
-import org.apache.lucene.facet.util.ScoredDocIdsUtils;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Simple random sampler
- */
-public class RandomSampler extends Sampler {
-  
-  private final Random random;
-
-  public RandomSampler() {
-    super();
-    this.random = new Random();
-  }
-
-  public RandomSampler(SamplingParams params, Random random) throws IllegalArgumentException {
-    super(params);
-    this.random = random;
-  }
-
-  @Override
-  protected SampleResult createSample(ScoredDocIDs docids, int actualSize, int sampleSetSize) throws IOException {
-    final int[] sample = new int[sampleSetSize];
-    final int maxStep = (actualSize * 2 ) / sampleSetSize; //floor
-    int remaining = actualSize;
-    ScoredDocIDsIterator it = docids.iterator();
-    int i = 0;
-    // select sample docs with random skipStep, make sure to leave sufficient #docs for selection after last skip
-    while (i<sample.length && remaining>(sampleSetSize-maxStep-i)) {
-      int skipStep = 1 + random.nextInt(maxStep);
-      // Skip over 'skipStep' documents
-      for (int j=0; j<skipStep; j++) {
-        it.next();
-        -- remaining;
-      }
-      sample[i++] = it.getDocID();
-    }
-    // Add leftover documents to the sample set
-    while (i<sample.length) {
-      it.next();
-      sample[i++] = it.getDocID();
-    }
-    ScoredDocIDs sampleRes = ScoredDocIdsUtils.createScoredDocIDsSubset(docids, sample);
-    SampleResult res = new SampleResult(sampleRes, sampleSetSize/(double)actualSize);
-    return res;
-  }
-  
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/sampling/RepeatableSampler.java b/lucene/facet/src/java/org/apache/lucene/facet/search/sampling/RepeatableSampler.java
deleted file mode 100644
index 892d2fe..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/sampling/RepeatableSampler.java
+++ /dev/null
@@ -1,406 +0,0 @@
-package org.apache.lucene.facet.search.sampling;
-
-import java.io.IOException;
-import java.util.Arrays;
-import java.util.logging.Level;
-import java.util.logging.Logger;
-
-import org.apache.lucene.util.PriorityQueue;
-
-import org.apache.lucene.facet.search.ScoredDocIDs;
-import org.apache.lucene.facet.search.ScoredDocIDsIterator;
-import org.apache.lucene.facet.util.ScoredDocIdsUtils;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Take random samples of large collections.
- * @lucene.experimental
- */
-public class RepeatableSampler extends Sampler {
-
-  private static final Logger logger = Logger.getLogger(RepeatableSampler.class.getName());
-
-  public RepeatableSampler(SamplingParams params) {
-    super(params);
-  }
-  
-  @Override
-  protected SampleResult createSample(ScoredDocIDs docids, int actualSize,
-      int sampleSetSize) throws IOException {
-    int[] sampleSet = null;
-    try {
-      sampleSet = repeatableSample(docids, actualSize,
-          sampleSetSize);
-    } catch (IOException e) {
-      if (logger.isLoggable(Level.WARNING)) {
-        logger.log(Level.WARNING, "sampling failed: "+e.getMessage()+" - falling back to no sampling!", e);
-      }
-      return new SampleResult(docids, 1d);
-    }
-
-    ScoredDocIDs sampled = ScoredDocIdsUtils.createScoredDocIDsSubset(docids,
-        sampleSet);
-    if (logger.isLoggable(Level.FINEST)) {
-      logger.finest("******************** " + sampled.size());
-    }
-    return new SampleResult(sampled, sampled.size()/(double)docids.size());
-  }
-  
-  /**
-   * Returns <code>sampleSize</code> values from the first <code>collectionSize</code>
-   * locations of <code>collection</code>, chosen using
-   * the <code>TRAVERSAL</code> algorithm. The sample values are not sorted.
-   * @param collection The values from which a sample is wanted.
-   * @param collectionSize The number of values (from the first) from which to draw the sample.
-   * @param sampleSize The number of values to return.
-   * @return An array of values chosen from the collection.
-   * @see Algorithm#TRAVERSAL
-   */
-  private static int[] repeatableSample(ScoredDocIDs collection,
-      int collectionSize, int sampleSize)
-  throws IOException {
-    return repeatableSample(collection, collectionSize,
-        sampleSize, Algorithm.HASHING, Sorted.NO);
-  }
-
-  /**
-   * Returns <code>sampleSize</code> values from the first <code>collectionSize</code>
-   * locations of <code>collection</code>, chosen using <code>algorithm</code>.
-   * @param collection The values from which a sample is wanted.
-   * @param collectionSize The number of values (from the first) from which to draw the sample.
-   * @param sampleSize The number of values to return.
-   * @param algorithm Which algorithm to use.
-   * @param sorted Sorted.YES to sort the sample values in ascending order before returning;
-   * Sorted.NO to return them in essentially random order.
-   * @return An array of values chosen from the collection.
-   */
-  private static int[] repeatableSample(ScoredDocIDs collection,
-      int collectionSize, int sampleSize,
-      Algorithm algorithm, Sorted sorted)
-  throws IOException {
-    if (collection == null) {
-      throw new IOException("docIdSet is null");
-    }
-    if (sampleSize < 1) {
-      throw new IOException("sampleSize < 1 (" + sampleSize + ")");
-    }
-    if (collectionSize < sampleSize) {
-      throw new IOException("collectionSize (" + collectionSize + ") less than sampleSize (" + sampleSize + ")");
-    }
-    int[] sample = new int[sampleSize];
-    long[] times = new long[4];
-    if (algorithm == Algorithm.TRAVERSAL) {
-      sample1(collection, collectionSize, sample, times);
-    } else if (algorithm == Algorithm.HASHING) {
-      sample2(collection, collectionSize, sample, times);
-    } else {
-      throw new IllegalArgumentException("Invalid algorithm selection");
-    }
-    if (sorted == Sorted.YES) {
-      Arrays.sort(sample);
-    }
-    if (returnTimings) {
-      times[3] = System.currentTimeMillis();
-      if (logger.isLoggable(Level.FINEST)) {
-        logger.finest("Times: " + (times[1] - times[0]) + "ms, "
-            + (times[2] - times[1]) + "ms, " + (times[3] - times[2])+"ms");
-      }
-    }
-    return sample;
-  }
-
-  /**
-   * Returns <code>sample</code>.length values chosen from the first <code>collectionSize</code>
-   * locations of <code>collection</code>, using the TRAVERSAL algorithm. The sample is
-   * pseudorandom: no subset of the original collection
-   * is in principle more likely to occur than any other, but for a given collection
-   * and sample size, the same sample will always be returned. This algorithm walks the
-   * original collection in a methodical way that is guaranteed not to visit any location
-   * more than once, which makes sampling without replacement faster because removals don't
-   * have to be tracked, and the number of operations is proportional to the sample size,
-   * not the collection size.
-   * Times for performance measurement
-   * are returned in <code>times</code>, which must be an array of at least three longs, containing
-   * nanosecond event times. The first
-   * is set when the algorithm starts; the second, when the step size has been calculated;
-   * and the third when the sample has been taken.
-   * @param collection The set to be sampled.
-   * @param collectionSize The number of values to use (starting from first).
-   * @param sample The array in which to return the sample.
-   * @param times The times of three events, for measuring performance.
-   */
-  private static void sample1(ScoredDocIDs collection, int collectionSize, int[] sample, long[] times) 
-  throws IOException {
-    ScoredDocIDsIterator it = collection.iterator();
-    if (returnTimings) {
-      times[0] = System.currentTimeMillis();
-    }
-    int sampleSize = sample.length;
-    int prime = findGoodStepSize(collectionSize, sampleSize);
-    int mod = prime % collectionSize;
-    if (returnTimings) {
-      times[1] = System.currentTimeMillis();
-    }
-    int sampleCount = 0;
-    int index = 0;
-    for (; sampleCount < sampleSize;) {
-      if (index + mod < collectionSize) {
-        for (int i = 0; i < mod; i++, index++) {
-          it.next();
-        }
-      } else {
-        index = index + mod - collectionSize;
-        it = collection.iterator();
-        for (int i = 0; i < index; i++) {
-          it.next();
-        }
-      }
-      sample[sampleCount++] = it.getDocID();
-    }
-    if (returnTimings) {
-      times[2] = System.currentTimeMillis();
-    }
-  }
-
-  /**
-   * Returns a value which will allow the caller to walk
-   * a collection of <code>collectionSize</code> values, without repeating or missing
-   * any, and spanning the collection from beginning to end at least once with
-   * <code>sampleSize</code> visited locations. Choosing a value
-   * that is relatively prime to the collection size ensures that stepping by that size (modulo
-   * the collection size) will hit all locations without repeating, eliminating the need to
-   * track previously visited locations for a "without replacement" sample. Starting with the
-   * square root of the collection size ensures that either the first or second prime tried will
-   * work (they can't both divide the collection size). It also has the property that N steps of
-   * size N will span a collection of N**2 elements once. If the sample is bigger than N, it will
-   * wrap multiple times (without repeating). If the sample is smaller, a step size is chosen
-   * that will result in at least one spanning of the collection.
-   * 
-   * @param collectionSize The number of values in the collection to be sampled.
-   * @param sampleSize The number of values wanted in the sample.
-   * @return A good increment value for walking the collection.
-   */
-  private static int findGoodStepSize(int collectionSize, int sampleSize) {
-    int i = (int) Math.sqrt(collectionSize);
-    if (sampleSize < i) {
-      i = collectionSize / sampleSize;
-    }
-    do {
-      i = findNextPrimeAfter(i);
-    } while (collectionSize % i == 0);
-    return i;
-  }
-
-  /**
-   * Returns the first prime number that is larger than <code>n</code>.
-   * @param n A number less than the prime to be returned.
-   * @return The smallest prime larger than <code>n</code>.
-   */
-  private static int findNextPrimeAfter(int n) {
-    n += (n % 2 == 0) ? 1 : 2; // next odd
-    foundFactor: for (;; n += 2) { //TODO labels??!!
-      int sri = (int) (Math.sqrt(n));
-      for (int primeIndex = 0; primeIndex < N_PRIMES; primeIndex++) {
-        int p = primes[primeIndex];
-        if (p > sri) {
-          return n;
-        }
-        if (n % p == 0) {
-          continue foundFactor;
-        }
-      }
-      for (int p = primes[N_PRIMES - 1] + 2;; p += 2) {
-        if (p > sri) {
-          return n;
-        }
-        if (n % p == 0) {
-          continue foundFactor;
-        }
-      }
-    }
-  }
-
-  /**
-   * The first N_PRIMES primes, after 2.
-   */
-  private static final int N_PRIMES = 4000;
-  private static int[] primes = new int[N_PRIMES];
-  static {
-    primes[0] = 3;
-    for (int count = 1; count < N_PRIMES; count++) {
-      primes[count] = findNextPrimeAfter(primes[count - 1]);
-    }
-  }
-
-  /**
-   * Returns <code>sample</code>.length values chosen from the first <code>collectionSize</code>
-   * locations of <code>collection</code>, using the HASHING algorithm. Performance measurements
-   * are returned in <code>times</code>, which must be an array of at least three longs. The first
-   * will be set when the algorithm starts; the second, when a hash key has been calculated and
-   * inserted into the priority queue for every element in the collection; and the third when the
-   * original elements associated with the keys remaining in the PQ have been stored in the sample
-   * array for return.
-   * <P>
-   * This algorithm slows as the sample size becomes a significant fraction of the collection
-   * size, because the PQ is as large as the sample set, and will not do early rejection of values
-   * below the minimum until it fills up, and a larger PQ contains more small values to be purged,
-   * resulting in less early rejection and more logN insertions.
-   * 
-   * @param collection The set to be sampled.
-   * @param collectionSize The number of values to use (starting from first).
-   * @param sample The array in which to return the sample.
-   * @param times The times of three events, for measuring performance.
-   */
-  private static void sample2(ScoredDocIDs collection, int collectionSize, int[] sample, long[] times) 
-  throws IOException {
-    if (returnTimings) {
-      times[0] = System.currentTimeMillis();
-    }
-    int sampleSize = sample.length;
-    IntPriorityQueue pq = new IntPriorityQueue(sampleSize);
-    /*
-     * Convert every value in the collection to a hashed "weight" value, and insert
-     * into a bounded PQ (retains only sampleSize highest weights).
-     */
-    ScoredDocIDsIterator it = collection.iterator();
-    MI mi = null;
-    while (it.next()) {
-      if (mi == null) {
-        mi = new MI();
-      }
-      mi.value = (int) (it.getDocID() * PHI_32) & 0x7FFFFFFF;
-      mi = pq.insertWithOverflow(mi);
-    }
-    if (returnTimings) {
-      times[1] = System.currentTimeMillis();
-    }
-    /*
-     * Extract heap, convert weights back to original values, and return as integers.
-     */
-    Object[] heap = pq.getHeap();
-    for (int si = 0; si < sampleSize; si++) {
-      sample[si] = (int)(((MI) heap[si+1]).value * PHI_32I) & 0x7FFFFFFF;
-    }
-    if (returnTimings) {
-      times[2] = System.currentTimeMillis();
-    }
-  }
-  
-  /**
-   * A mutable integer that lets queue objects be reused once they start overflowing.
-   */
-  private static class MI {
-    MI() { }
-    public int value;
-  }
-
-  /**
-   * A bounded priority queue for Integers, to retain a specified number of
-   * the highest-weighted values for return as a random sample.
-   */
-  private static class IntPriorityQueue extends PriorityQueue<MI> {
-
-    /**
-     * Creates a bounded PQ of size <code>size</code>.
-     * @param size The number of elements to retain.
-     */
-    public IntPriorityQueue(int size) {
-      super(size);
-    }
-
-    /**
-     * Returns the underlying data structure for faster access. Extracting elements
-     * one at a time would require N logN time, and since we want the elements sorted
-     * in ascending order by value (not weight), the array is useful as-is.
-     * @return The underlying heap array.
-     */
-    public Object[] getHeap() {
-      return getHeapArray();
-    }
-
-    /**
-     * Returns true if <code>o1<code>'s weight is less than that of <code>o2</code>, for
-     * ordering in the PQ.
-     * @return True if <code>o1</code> weighs less than <code>o2</code>.
-     */
-    @Override
-    public boolean lessThan(MI o1, MI o2) {
-      return o1.value < o2.value;
-    }
-
-  }
-
-  /**
-   * For specifying which sampling algorithm to use.
-   */
-  private enum Algorithm {
-
-    /**
-     * Specifies a methodical traversal algorithm, which is guaranteed to span the collection
-     * at least once, and never to return duplicates. Faster than the hashing algorithm and
-     * uses much less space, but the randomness of the sample may be affected by systematic
-     * variations in the collection. Requires only an array for the sample, and visits only
-     * the number of elements in the sample set, not the full set.
-     */
-    // TODO (Facet): This one produces a bimodal distribution (very flat around
-    // each peak!) for collection size 10M and sample sizes 10k and 10544.
-    // Figure out why.
-    TRAVERSAL,
-
-    /**
-     * Specifies a Fibonacci-style hash algorithm (see Knuth, S&S), which generates a less
-     * systematically distributed subset of the sampled collection than the traversal method,
-     * but requires a bounded priority queue the size of the sample, and creates an object
-     * containing a sampled value and its hash, for every element in the full set. 
-     */
-    HASHING
-  }
-
-  /**
-   * For specifying whether to sort the sample.
-   */
-  private enum Sorted {
-
-    /**
-     * Sort resulting sample before returning.
-     */
-    YES,
-
-    /**
-     *Do not sort the resulting sample. 
-     */
-    NO
-  }
-
-  /**
-   * Magic number 1: prime closest to phi, in 32 bits.
-   */
-  private static final long PHI_32 = 2654435769L;
-
-  /**
-   * Magic number 2: multiplicative inverse of PHI_32, modulo 2**32.
-   */
-  private static final long PHI_32I = 340573321L;
-
-  /**
-   * Switch to cause methods to return timings.
-   */
-  private static boolean returnTimings = false;
-
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/sampling/SampleFixer.java b/lucene/facet/src/java/org/apache/lucene/facet/search/sampling/SampleFixer.java
deleted file mode 100644
index 5b992d3..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/sampling/SampleFixer.java
+++ /dev/null
@@ -1,44 +0,0 @@
-package org.apache.lucene.facet.search.sampling;
-
-import java.io.IOException;
-
-import org.apache.lucene.facet.search.ScoredDocIDs;
-import org.apache.lucene.facet.search.results.FacetResult;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Fixer of sample facet accumulation results
- * 
- * @lucene.experimental
- */
-public interface SampleFixer {
-  
-  /**
-   * Alter the input result, fixing it to account for the sampling. This
-   * implementation can compute accurate or estimated counts for the sampled facets. 
-   * For example, a faster correction could just multiply by a compensating factor.
-   * 
-   * @param origDocIds
-   *          full set of matching documents.
-   * @param fres
-   *          sample result to be fixed.
-   * @throws IOException If there is a low-level I/O error.
-   */
-  public void fixResult(ScoredDocIDs origDocIds, FacetResult fres) throws IOException; 
-}
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/sampling/Sampler.java b/lucene/facet/src/java/org/apache/lucene/facet/search/sampling/Sampler.java
deleted file mode 100644
index 345677f..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/sampling/Sampler.java
+++ /dev/null
@@ -1,246 +0,0 @@
-package org.apache.lucene.facet.search.sampling;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.lucene.facet.search.FacetArrays;
-import org.apache.lucene.facet.search.ScoredDocIDs;
-import org.apache.lucene.facet.search.aggregator.Aggregator;
-import org.apache.lucene.facet.search.params.FacetRequest;
-import org.apache.lucene.facet.search.params.FacetSearchParams;
-import org.apache.lucene.facet.search.results.FacetResult;
-import org.apache.lucene.facet.search.results.FacetResultNode;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.index.IndexReader;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Sampling definition for facets accumulation
- * <p>
- * The Sampler uses TAKMI style counting to provide a 'best guess' top-K result
- * set of the facets accumulated.
- * <p>
- * Note: Sampling accumulation (Accumulation over a sampled-set of the results),
- * does not guarantee accurate values for
- * {@link FacetResult#getNumValidDescendants()}.
- * 
- * @lucene.experimental
- */
-public abstract class Sampler {
-
-  protected final SamplingParams samplingParams;
-  
-  /**
-   * Construct with {@link SamplingParams}
-   */
-  public Sampler() {
-    this(new SamplingParams()); 
-  }
-  
-  /**
-   * Construct with certain {@link SamplingParams}
-   * 
-   * @param params sampling params in effect
-   * @throws IllegalArgumentException if the provided SamplingParams are not valid 
-   */
-  public Sampler(SamplingParams params) throws IllegalArgumentException {
-    if (!params.validate()) {
-      throw new IllegalArgumentException("The provided SamplingParams are not valid!!");
-    }
-    this.samplingParams = params;
-  }
-
-  /**
-   * Check if this sampler would complement for the input docIds
-   */
-  public boolean shouldSample(ScoredDocIDs docIds) {
-    return docIds.size() > samplingParams.getSamplingThreshold();
-  }
-  
-  /**
-   * Compute a sample set out of the input set, based on the {@link SamplingParams#getSampleRatio()}
-   * in effect. Sub classes can override to alter how the sample set is
-   * computed.
-   * <p> 
-   * If the input set is of size smaller than {@link SamplingParams#getMinSampleSize()}, 
-   * the input set is returned (no sampling takes place).
-   * <p>
-   * Other than that, the returned set size will not be larger than {@link SamplingParams#getMaxSampleSize()} 
-   * nor smaller than {@link SamplingParams#getMinSampleSize()}.  
-   * @param docids
-   *          full set of matching documents out of which a sample is needed.
-   */
-  public SampleResult getSampleSet(ScoredDocIDs docids) throws IOException {
-    if (!shouldSample(docids)) {
-      return new SampleResult(docids, 1d);
-    }
-
-    int actualSize = docids.size();
-    int sampleSetSize = (int) (actualSize * samplingParams.getSampleRatio());
-    sampleSetSize = Math.max(sampleSetSize, samplingParams.getMinSampleSize());
-    sampleSetSize = Math.min(sampleSetSize, samplingParams.getMaxSampleSize());
-
-    return createSample(docids, actualSize, sampleSetSize);
-  }
-
-  /**
-   * Create and return a sample of the input set
-   * @param docids input set out of which a sample is to be created 
-   * @param actualSize original size of set, prior to sampling
-   * @param sampleSetSize required size of sample set
-   * @return sample of the input set in the required size
-   */
-  protected abstract SampleResult createSample(ScoredDocIDs docids, int actualSize, int sampleSetSize) 
-      throws IOException;
-
-  /**
-   * Get a fixer of sample facet accumulation results. Default implementation
-   * returns a <code>TakmiSampleFixer</code> which is adequate only for
-   * counting. For any other accumulator, provide a different fixer.
-   */
-  public SampleFixer getSampleFixer(IndexReader indexReader, TaxonomyReader taxonomyReader,
-      FacetSearchParams searchParams) {
-    return new TakmiSampleFixer(indexReader, taxonomyReader, searchParams);
-  }
-  
-  /**
-   * Result of sample computation
-   */
-  public final static class SampleResult {
-    public final ScoredDocIDs docids;
-    public final double actualSampleRatio;
-    protected SampleResult(ScoredDocIDs docids, double actualSampleRatio) {
-      this.docids = docids;
-      this.actualSampleRatio = actualSampleRatio;
-    }
-  }
-  
-  /**
-   * Return the sampling params in effect
-   */
-  public final SamplingParams getSamplingParams() {
-    return samplingParams;
-  }
-
-  /**
-   * Trim the input facet result.<br>
-   * Note: It is only valid to call this method with result obtained for a
-   * facet request created through {@link #overSampledSearchParams(FacetSearchParams)}.
-   * 
-   * @throws IllegalArgumentException
-   *             if called with results not obtained for requests created
-   *             through {@link #overSampledSearchParams(FacetSearchParams)}
-   */
-  public FacetResult trimResult(FacetResult facetResult) throws IllegalArgumentException {
-    double overSampleFactor = getSamplingParams().getOversampleFactor();
-    if (overSampleFactor <= 1) { // no factoring done?
-      return facetResult;
-    }
-    
-    OverSampledFacetRequest sampledFreq = null;
-    
-    try {
-      sampledFreq = (OverSampledFacetRequest) facetResult.getFacetRequest();
-    } catch (ClassCastException e) {
-      throw new IllegalArgumentException(
-          "It is only valid to call this method with result obtained for a " +
-          "facet request created through sampler.overSamlpingSearchParams()",
-          e);
-    }
-    
-    FacetRequest origFrq = sampledFreq.orig;
-
-    FacetResultNode trimmedRootNode = facetResult.getFacetResultNode();
-    trimSubResults(trimmedRootNode, origFrq.numResults);
-    
-    return new FacetResult(origFrq, trimmedRootNode, facetResult.getNumValidDescendants());
-  }
-  
-  /** Trim sub results to a given size. */
-  private void trimSubResults(FacetResultNode node, int size) {
-    if (node.subResults == FacetResultNode.EMPTY_SUB_RESULTS || node.subResults.size() == 0) {
-      return;
-    }
-
-    ArrayList<FacetResultNode> trimmed = new ArrayList<FacetResultNode>(size);
-    for (int i = 0; i < node.subResults.size() && i < size; i++) {
-      FacetResultNode trimmedNode = node.subResults.get(i);
-      trimSubResults(trimmedNode, size);
-      trimmed.add(trimmedNode);
-    }
-    
-    node.subResults = trimmed;
-  }
-
-  /**
-   * Over-sampled search params, wrapping each request with an over-sampled one.
-   */
-  public FacetSearchParams overSampledSearchParams(FacetSearchParams original) {
-    FacetSearchParams res = original;
-    // So now we can sample -> altering the searchParams to accommodate for the statistical error for the sampling
-    double overSampleFactor = getSamplingParams().getOversampleFactor();
-    if (overSampleFactor > 1) { // any factoring to do?
-      List<FacetRequest> facetRequests = new ArrayList<FacetRequest>();
-      for (FacetRequest frq : original.facetRequests) {
-        int overSampledNumResults = (int) Math.ceil(frq.numResults * overSampleFactor);
-        facetRequests.add(new OverSampledFacetRequest(frq, overSampledNumResults));
-      }
-      res = new FacetSearchParams(original.indexingParams, facetRequests);
-    }
-    return res;
-  }
-  
-  /**
-   * Wrapping a facet request for over sampling.
-   * Implementation detail: even if the original request is a count request, no 
-   * statistics will be computed for it as the wrapping is not a count request.
-   * This is ok, as the sampling accumulator is later computing the statistics
-   * over the original requests.
-   */
-  private static class OverSampledFacetRequest extends FacetRequest {
-    final FacetRequest orig;
-    public OverSampledFacetRequest(FacetRequest orig, int num) {
-      super(orig.categoryPath, num);
-      this.orig = orig;
-      setDepth(orig.getDepth());
-      setNumLabel(orig.getNumLabel());
-      setResultMode(orig.getResultMode());
-      setSortBy(orig.getSortBy());
-      setSortOrder(orig.getSortOrder());
-    }
-    
-    @Override
-    public Aggregator createAggregator(boolean useComplements, FacetArrays arrays, TaxonomyReader taxonomy) 
-        throws IOException {
-      return orig.createAggregator(useComplements, arrays, taxonomy);
-    }
-
-    @Override
-    public FacetArraysSource getFacetArraysSource() {
-      return orig.getFacetArraysSource();
-    }
-
-    @Override
-    public double getValueOf(FacetArrays arrays, int idx) {
-      return orig.getValueOf(arrays, idx);
-    }
-  }
-
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/sampling/SamplingAccumulator.java b/lucene/facet/src/java/org/apache/lucene/facet/search/sampling/SamplingAccumulator.java
deleted file mode 100644
index a2b7e40..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/sampling/SamplingAccumulator.java
+++ /dev/null
@@ -1,123 +0,0 @@
-package org.apache.lucene.facet.search.sampling;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.lucene.facet.partitions.search.PartitionsFacetResultsHandler;
-import org.apache.lucene.facet.search.FacetArrays;
-import org.apache.lucene.facet.search.FacetsAccumulator;
-import org.apache.lucene.facet.search.SamplingWrapper;
-import org.apache.lucene.facet.search.ScoredDocIDs;
-import org.apache.lucene.facet.search.StandardFacetsAccumulator;
-import org.apache.lucene.facet.search.params.FacetSearchParams;
-import org.apache.lucene.facet.search.results.FacetResult;
-import org.apache.lucene.facet.search.sampling.Sampler.SampleResult;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.index.IndexReader;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Facets accumulation with sampling.<br>
- * <p>
- * Note two major differences between this class and {@link SamplingWrapper}:
- * <ol>
- * <li>Latter can wrap any other {@link FacetsAccumulator} while this class
- * directly extends {@link StandardFacetsAccumulator}.</li>
- * <li>This class can effectively apply sampling on the complement set of
- * matching document, thereby working efficiently with the complement
- * optimization - see {@link StandardFacetsAccumulator#getComplementThreshold()}
- * .</li>
- * </ol>
- * <p>
- * Note: Sampling accumulation (Accumulation over a sampled-set of the results),
- * does not guarantee accurate values for
- * {@link FacetResult#getNumValidDescendants()}.
- * 
- * @see Sampler
- * @lucene.experimental
- */
-public class SamplingAccumulator extends StandardFacetsAccumulator {
-  
-  private double samplingRatio = -1d;
-  private final Sampler sampler;
-  
-  public SamplingAccumulator(Sampler sampler, FacetSearchParams searchParams,
-      IndexReader indexReader, TaxonomyReader taxonomyReader,
-      FacetArrays facetArrays) {
-    super(searchParams, indexReader, taxonomyReader, facetArrays);
-    this.sampler = sampler;
-  }
-
-  /**
-   * Constructor...
-   */
-  public SamplingAccumulator(
-      Sampler sampler,
-      FacetSearchParams searchParams,
-      IndexReader indexReader, TaxonomyReader taxonomyReader) {
-    super(searchParams, indexReader, taxonomyReader);
-    this.sampler = sampler;
-  }
-
-  @Override
-  public List<FacetResult> accumulate(ScoredDocIDs docids) throws IOException {
-    // Replacing the original searchParams with the over-sampled
-    FacetSearchParams original = searchParams;
-    searchParams = sampler.overSampledSearchParams(original);
-    
-    List<FacetResult> sampleRes = super.accumulate(docids);
-    
-    List<FacetResult> fixedRes = new ArrayList<FacetResult>();
-    for (FacetResult fres : sampleRes) {
-      // for sure fres is not null because this is guaranteed by the delegee.
-      PartitionsFacetResultsHandler frh = createFacetResultsHandler(fres.getFacetRequest());
-      // fix the result of current request
-      sampler.getSampleFixer(indexReader, taxonomyReader, searchParams).fixResult(docids, fres);
-      
-      fres = frh.rearrangeFacetResult(fres); // let delegee's handler do any arranging it needs to
-
-      // Using the sampler to trim the extra (over-sampled) results
-      fres = sampler.trimResult(fres);
-
-      // final labeling if allowed (because labeling is a costly operation)
-      frh.labelResult(fres);
-      fixedRes.add(fres); // add to final results
-    }
-    
-    searchParams = original; // Back to original params
-    
-    return fixedRes; 
-  }
-
-  @Override
-  protected ScoredDocIDs actualDocsToAccumulate(ScoredDocIDs docids) throws IOException {
-    SampleResult sampleRes = sampler.getSampleSet(docids);
-    samplingRatio = sampleRes.actualSampleRatio;
-    return sampleRes.docids;
-  }
-  
-  @Override
-  protected double getTotalCountsFactor() {
-    if (samplingRatio<0) {
-      throw new IllegalStateException("Total counts ratio unavailable because actualDocsToAccumulate() was not invoked");
-    }
-    return samplingRatio;
-  }
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/sampling/SamplingParams.java b/lucene/facet/src/java/org/apache/lucene/facet/search/sampling/SamplingParams.java
deleted file mode 100644
index fc509ca..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/sampling/SamplingParams.java
+++ /dev/null
@@ -1,169 +0,0 @@
-package org.apache.lucene.facet.search.sampling;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Parameters for sampling, dictating whether sampling is to take place and how. 
- * 
- * @lucene.experimental
- */
-public class SamplingParams {
-
-  /**
-   * Default factor by which more results are requested over the sample set.
-   * @see SamplingParams#getOversampleFactor()
-   */
-  public static final double DEFAULT_OVERSAMPLE_FACTOR = 2d;
-  
-  /**
-   * Default ratio between size of sample to original size of document set.
-   * @see Sampler#getSampleSet(org.apache.lucene.facet.search.ScoredDocIDs)
-   */
-  public static final double DEFAULT_SAMPLE_RATIO = 0.01;
-  
-  /**
-   * Default maximum size of sample.
-   * @see Sampler#getSampleSet(org.apache.lucene.facet.search.ScoredDocIDs)
-   */
-  public static final int DEFAULT_MAX_SAMPLE_SIZE = 10000;
-  
-  /**
-   * Default minimum size of sample.
-   * @see Sampler#getSampleSet(org.apache.lucene.facet.search.ScoredDocIDs)
-   */
-  public static final int DEFAULT_MIN_SAMPLE_SIZE = 100;
-  
-  /**
-   * Default sampling threshold, if number of results is less than this number - no sampling will take place
-   * @see SamplingParams#getSampleRatio()
-   */
-  public static final int DEFAULT_SAMPLING_THRESHOLD = 75000;
-
-  private int maxSampleSize = DEFAULT_MAX_SAMPLE_SIZE;
-  private int minSampleSize = DEFAULT_MIN_SAMPLE_SIZE;
-  private double sampleRatio = DEFAULT_SAMPLE_RATIO;
-  private int samplingThreshold = DEFAULT_SAMPLING_THRESHOLD;
-  private double oversampleFactor = DEFAULT_OVERSAMPLE_FACTOR;
-  
-  /**
-   * Return the maxSampleSize.
-   * In no case should the resulting sample size exceed this value.  
-   * @see Sampler#getSampleSet(org.apache.lucene.facet.search.ScoredDocIDs)
-   */
-  public final int getMaxSampleSize() {
-    return maxSampleSize;
-  }
-
-  /**
-   * Return the minSampleSize.
-   * In no case should the resulting sample size be smaller than this value.  
-   * @see Sampler#getSampleSet(org.apache.lucene.facet.search.ScoredDocIDs)
-   */
-  public final int getMinSampleSize() {
-    return minSampleSize;
-  }
-
-  /**
-   * @return the sampleRatio
-   * @see Sampler#getSampleSet(org.apache.lucene.facet.search.ScoredDocIDs)
-   */
-  public final double getSampleRatio() {
-    return sampleRatio;
-  }
-  
-  /**
-   * Return the samplingThreshold.
-   * Sampling would be performed only for document sets larger than this.  
-   */
-  public final int getSamplingThreshold() {
-    return samplingThreshold;
-  }
-
-  /**
-   * @param maxSampleSize
-   *          the maxSampleSize to set
-   * @see #getMaxSampleSize()
-   */
-  public void setMaxSampleSize(int maxSampleSize) {
-    this.maxSampleSize = maxSampleSize;
-  }
-
-  /**
-   * @param minSampleSize
-   *          the minSampleSize to set
-   * @see #getMinSampleSize()
-   */
-  public void setMinSampleSize(int minSampleSize) {
-    this.minSampleSize = minSampleSize;
-  }
-
-  /**
-   * @param sampleRatio
-   *          the sampleRatio to set
-   * @see #getSampleRatio()
-   */
-  public void setSampleRatio(double sampleRatio) {
-    this.sampleRatio = sampleRatio;
-  }
-
-  /**
-   * Set a sampling-threshold
-   * @see #getSamplingThreshold()
-   */
-  public void setSamplingThreshold(int samplingThreshold) {
-    this.samplingThreshold = samplingThreshold;
-  }
-
-  /**
-   * Check validity of sampling settings, making sure that
-   * <ul>
-   * <li> <code>minSampleSize <= maxSampleSize <= samplingThreshold </code></li>
-   * <li> <code>0 < samplingRatio <= 1 </code></li>
-   * </ul> 
-   * 
-   * @return true if valid, false otherwise
-   */
-  public boolean validate() {
-    return 
-      samplingThreshold >= maxSampleSize && 
-      maxSampleSize >= minSampleSize && 
-      sampleRatio > 0 &&
-      sampleRatio < 1;
-  }
-
-  /**
-   * Return the oversampleFactor. When sampling, we would collect that much more
-   * results, so that later, when selecting top out of these, chances are higher
-   * to get actual best results. Note that having this value larger than 1 only
-   * makes sense when using a SampleFixer which finds accurate results, such as
-   * <code>TakmiSampleFixer</code>. When this value is smaller than 1, it is
-   * ignored and no oversampling takes place.
-   */
-  public final double getOversampleFactor() {
-    return oversampleFactor;
-  }
-
-  /**
-   * @param oversampleFactor the oversampleFactor to set
-   * @see #getOversampleFactor()
-   */
-  public void setOversampleFactor(double oversampleFactor) {
-    this.oversampleFactor = oversampleFactor;
-  }
-
-}
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/sampling/TakmiSampleFixer.java b/lucene/facet/src/java/org/apache/lucene/facet/search/sampling/TakmiSampleFixer.java
deleted file mode 100644
index ddfb66f..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/sampling/TakmiSampleFixer.java
+++ /dev/null
@@ -1,182 +0,0 @@
-package org.apache.lucene.facet.search.sampling;
-
-import java.io.IOException;
-
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.MultiFields;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.search.DocIdSetIterator;
-import org.apache.lucene.util.Bits;
-
-import org.apache.lucene.facet.search.DrillDown;
-import org.apache.lucene.facet.search.ScoredDocIDs;
-import org.apache.lucene.facet.search.ScoredDocIDsIterator;
-import org.apache.lucene.facet.search.params.FacetSearchParams;
-import org.apache.lucene.facet.search.results.FacetResult;
-import org.apache.lucene.facet.search.results.FacetResultNode;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Fix sampling results by counting the intersection between two lists: a
- * TermDocs (list of documents in a certain category) and a DocIdSetIterator
- * (list of documents matching the query).
- * 
- * 
- * @lucene.experimental
- */
-// TODO (Facet): implement also an estimated fixing by ratio (taking into
-// account "translation" of counts!)
-class TakmiSampleFixer implements SampleFixer {
-  
-  private TaxonomyReader taxonomyReader;
-  private IndexReader indexReader;
-  private FacetSearchParams searchParams;
-  
-  public TakmiSampleFixer(IndexReader indexReader,
-      TaxonomyReader taxonomyReader, FacetSearchParams searchParams) {
-    this.indexReader = indexReader;
-    this.taxonomyReader = taxonomyReader;
-    this.searchParams = searchParams;
-  }
-
-  @Override
-  public void fixResult(ScoredDocIDs origDocIds, FacetResult fres)
-      throws IOException {
-    FacetResultNode topRes = fres.getFacetResultNode();
-    fixResultNode(topRes, origDocIds);
-  }
-  
-  /**
-   * Fix result node count, and, recursively, fix all its children
-   * 
-   * @param facetResNode
-   *          result node to be fixed
-   * @param docIds
-   *          docids in effect
-   * @throws IOException If there is a low-level I/O error.
-   */
-  private void fixResultNode(FacetResultNode facetResNode, ScoredDocIDs docIds) throws IOException {
-    recount(facetResNode, docIds);
-    for (FacetResultNode frn : facetResNode.subResults) {
-      fixResultNode(frn, docIds);
-    }
-  }
-
-  /**
-   * Internal utility: recount for a facet result node
-   * 
-   * @param fresNode
-   *          result node to be recounted
-   * @param docIds
-   *          full set of matching documents.
-   * @throws IOException If there is a low-level I/O error.
-   */
-  private void recount(FacetResultNode fresNode, ScoredDocIDs docIds) throws IOException {
-    // TODO (Facet): change from void to return the new, smaller docSet, and use
-    // that for the children, as this will make their intersection ops faster.
-    // can do this only when the new set is "sufficiently" smaller.
-    
-    /* We need the category's path name in order to do its recounting.
-     * If it is missing, because the option to label only part of the
-     * facet results was exercise, we need to calculate them anyway, so
-     * in essence sampling with recounting spends some extra cycles for
-     * labeling results for which labels are not required. */
-    if (fresNode.label == null) {
-      fresNode.label = taxonomyReader.getPath(fresNode.ordinal);
-    }
-    CategoryPath catPath = fresNode.label;
-
-    Term drillDownTerm = DrillDown.term(searchParams, catPath);
-    // TODO (Facet): avoid Multi*?
-    Bits liveDocs = MultiFields.getLiveDocs(indexReader);
-    int updatedCount = countIntersection(MultiFields.getTermDocsEnum(indexReader, liveDocs,
-                                                                     drillDownTerm.field(), drillDownTerm.bytes(),
-                                                                     0), docIds.iterator());
-    fresNode.value = updatedCount;
-  }
-
-  /**
-   * Count the size of the intersection between two lists: a TermDocs (list of
-   * documents in a certain category) and a DocIdSetIterator (list of documents
-   * matching a query).
-   */
-  private static int countIntersection(DocsEnum p1, ScoredDocIDsIterator p2)
-      throws IOException {
-    // The documentation of of both TermDocs and DocIdSetIterator claim
-    // that we must do next() before doc(). So we do, and if one of the
-    // lists is empty, obviously return 0;
-    if (p1 == null || p1.nextDoc() == DocIdSetIterator.NO_MORE_DOCS) {
-      return 0;
-    }
-    if (!p2.next()) {
-      return 0;
-    }
-    
-    int d1 = p1.docID();
-    int d2 = p2.getDocID();
-
-    int count = 0;
-    for (;;) {
-      if (d1 == d2) {
-        ++count;
-        if (p1.nextDoc() == DocIdSetIterator.NO_MORE_DOCS) {
-          break; // end of list 1, nothing more in intersection
-        }
-        d1 = p1.docID();
-        if (!advance(p2, d1)) {
-          break; // end of list 2, nothing more in intersection
-        }
-        d2 = p2.getDocID();
-      } else if (d1 < d2) {
-        if (p1.advance(d2) == DocIdSetIterator.NO_MORE_DOCS) {
-          break; // end of list 1, nothing more in intersection
-        }
-        d1 = p1.docID();
-      } else /* d1>d2 */ {
-        if (!advance(p2, d1)) {
-          break; // end of list 2, nothing more in intersection
-        }
-        d2 = p2.getDocID();
-      }
-    }
-    return count;
-  }
-
-  /**
-   * utility: advance the iterator until finding (or exceeding) specific
-   * document
-   * 
-   * @param iterator
-   *          iterator being advanced
-   * @param targetDoc
-   *          target of advancing
-   * @return false if iterator exhausted, true otherwise.
-   */
-  private static boolean advance(ScoredDocIDsIterator iterator, int targetDoc) {
-    while (iterator.next()) {
-      if (iterator.getDocID() >= targetDoc) {
-        return true; // target reached
-      }
-    }
-    return false; // exhausted
-  }
-}
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/sampling/package.html b/lucene/facet/src/java/org/apache/lucene/facet/search/sampling/package.html
deleted file mode 100644
index 90e033c..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/sampling/package.html
+++ /dev/null
@@ -1,24 +0,0 @@
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-  <head>
-    <title>Sampling for facets accumulation</title>
-  </head>
-  <body>
-    <h1>Sampling for facets accumulation</h1>
-  </body>
-</html>
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyReader.java b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyReader.java
index 9bca90e..2f1fb30 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyReader.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyReader.java
@@ -5,6 +5,7 @@ import java.util.Map;
 import java.util.logging.Level;
 import java.util.logging.Logger;
 
+import org.apache.lucene.facet.collections.LRUHashMap;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.facet.taxonomy.directory.Consts.LoadFullPathOnly;
@@ -17,7 +18,6 @@ import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.collections.LRUHashMap;
 
 /*
  * Licensed to the Apache Software Foundation (ASF) under one or more
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/package.html b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/package.html
index 82d2331..edbec74 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/package.html
+++ b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/package.html
@@ -16,10 +16,9 @@
 -->
 <html>
 <head>
-<title>Taxonomy implemented using a Lucene-Index</title>
+<title>Taxonomy index implementation using on top of a Directory</title>
 </head>
 <body>
-	<h1>Taxonomy implemented using a Lucene-Index</h1>
-	
+Taxonomy index implementation using on top of a Directory.
 </body>
 </html>
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/package.html b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/package.html
index e04e585..36f13e8 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/package.html
+++ b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/package.html
@@ -22,7 +22,7 @@
 	<h1>Taxonomy of Categories</h1>
 	
 	Facets are defined using a hierarchy of categories, known as a
-	<i>Taxonomy</i>
+	<i>Taxonomy</i>.
 	
 	<br>
 	For example, in a book store application, a Taxonomy could have the
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/package.html b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/package.html
index 64e6ff3..72bae6f 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/package.html
+++ b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/package.html
@@ -16,10 +16,9 @@
 -->
 <html>
 <head>
-<title>Improves indexing time by caching a map of CategoryPath to their Ordinal</title>
+<title>Taxonomy index cache</title>
 </head>
 <body>
-	<h1>Improves indexing time by caching a map of CategoryPath to their Ordinal</h1>
-	
+Improves indexing time by caching a map of CategoryPath to their Ordinal.	
 </body>
 </html>
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/util/FacetsPayloadMigrationReader.java b/lucene/facet/src/java/org/apache/lucene/facet/util/FacetsPayloadMigrationReader.java
new file mode 100644
index 0000000..4ce34b1
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/util/FacetsPayloadMigrationReader.java
@@ -0,0 +1,252 @@
+package org.apache.lucene.facet.util;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.Map;
+
+import org.apache.lucene.facet.params.CategoryListParams;
+import org.apache.lucene.facet.params.FacetIndexingParams;
+import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.BinaryDocValues;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.DocsAndPositionsEnum;
+import org.apache.lucene.index.FieldInfo.DocValuesType;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.Fields;
+import org.apache.lucene.index.FilterAtomicReader;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.index.Terms;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.search.DocIdSetIterator;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.BytesRef;
+
+/**
+ * A {@link FilterAtomicReader} for migrating a facets index which encodes
+ * category ordinals in a payload to {@link BinaryDocValues}. To migrate the index,
+ * you should build a mapping from a field (String) to term ({@link Term}),
+ * which denotes under which BinaryDocValues field to put the data encoded in the
+ * matching term's payload. You can follow the code example below to migrate an
+ * existing index:
+ * 
+ * <pre class="prettyprint">
+ * // Add the index and migrate payload to DocValues on the go
+ * DirectoryReader reader = DirectoryReader.open(oldDir);
+ * IndexWriterConfig conf = new IndexWriterConfig(VER, ANALYZER);
+ * IndexWriter writer = new IndexWriter(newDir, conf);
+ * List&lt;AtomicReaderContext&gt; leaves = reader.leaves();
+ * AtomicReader wrappedLeaves[] = new AtomicReader[leaves.size()];
+ * for (int i = 0; i &lt; leaves.size(); i++) {
+ *   wrappedLeaves[i] = new FacetPayloadMigrationReader(leaves.get(i).reader(),
+ *       fieldTerms);
+ * }
+ * writer.addIndexes(new MultiReader(wrappedLeaves));
+ * writer.commit();
+ * </pre>
+ * 
+ * <p>
+ * <b>NOTE:</b> to build the field-to-term map you can use
+ * {@link #buildFieldTermsMap(Directory, FacetIndexingParams)}, as long as the
+ * index to migrate contains the ordinals payload under
+ * {@link #PAYLOAD_TERM_TEXT}.
+ * 
+ * @lucene.experimental
+ */
+public class FacetsPayloadMigrationReader extends FilterAtomicReader {  
+
+  private class PayloadMigratingBinaryDocValues extends BinaryDocValues {
+
+    private Fields fields;
+    private Term term;
+    private DocsAndPositionsEnum dpe;
+    private int curDocID = -1;
+    private int lastRequestedDocID;
+
+    private DocsAndPositionsEnum getDPE() {
+      try {
+        DocsAndPositionsEnum dpe = null;
+        if (fields != null) {
+          Terms terms = fields.terms(term.field());
+          if (terms != null) {
+            TermsEnum te = terms.iterator(null); // no use for reusing
+            if (te.seekExact(term.bytes(), true)) {
+              // we're not expected to be called for deleted documents
+              dpe = te.docsAndPositions(null, null, DocsAndPositionsEnum.FLAG_PAYLOADS);
+            }
+          }
+        }
+        return dpe;
+      } catch (IOException ioe) {
+        throw new RuntimeException(ioe);
+      }
+    }
+    
+    protected PayloadMigratingBinaryDocValues(Fields fields, Term term) {
+      this.fields = fields;
+      this.term = term;
+      this.dpe = getDPE();
+      if (dpe == null) {
+        curDocID = DocIdSetIterator.NO_MORE_DOCS;
+      } else {
+        try {
+          curDocID = dpe.nextDoc();
+        } catch (IOException e) {
+          throw new RuntimeException(e);
+        }
+      }
+    }
+    
+    @Override
+    public void get(int docID, BytesRef result) {
+      try {
+        // If caller is moving backwards (eg, during merge,
+        // the consuming DV format is free to iterate over
+        // our values as many times as it wants), we must
+        // re-init the dpe:
+        if (docID <= lastRequestedDocID) {
+          dpe = getDPE();
+          if (dpe == null) {
+            curDocID = DocIdSetIterator.NO_MORE_DOCS;
+          } else{
+            curDocID = dpe.nextDoc();
+          }
+        }
+        lastRequestedDocID = docID;
+        if (curDocID > docID) {
+          // document does not exist
+          result.length = 0;
+          return;
+        }
+      
+        if (curDocID < docID) {
+          curDocID = dpe.advance(docID);
+          if (curDocID != docID) { // requested document does not have a payload
+            result.length = 0;
+            return;
+          }
+        }
+        
+        dpe.nextPosition();
+        result.copyBytes(dpe.getPayload());
+      } catch (IOException e) {
+        throw new RuntimeException(e);
+      }
+    }
+  }
+  
+  /** The {@link Term} text of the ordinals payload. */
+  public static final String PAYLOAD_TERM_TEXT = "$fulltree$";
+
+  /**
+   * A utility method for building the field-to-Term map, given the
+   * {@link FacetIndexingParams} and the directory of the index to migrate. The
+   * map that will be built will correspond to partitions as well as multiple
+   * {@link CategoryListParams}.
+   * <p>
+   * <b>NOTE:</b> since {@link CategoryListParams} no longer define a
+   * {@link Term}, this method assumes that the term used by the different
+   * {@link CategoryListParams} is {@link #PAYLOAD_TERM_TEXT}. If this is not
+   * the case, then you should build the map yourself, using the terms in your
+   * index.
+   */
+  public static Map<String,Term> buildFieldTermsMap(Directory dir, FacetIndexingParams fip) throws IOException {
+    // only add field-Term mapping that will actually have DocValues in the end.
+    // therefore traverse the index terms and add what exists. this pertains to
+    // multiple CLPs, as well as partitions
+    DirectoryReader reader = DirectoryReader.open(dir);
+    final Map<String,Term> fieldTerms = new HashMap<String,Term>();
+    for (AtomicReaderContext context : reader.leaves()) {
+      for (CategoryListParams clp : fip.getAllCategoryListParams()) {
+        Terms terms = context.reader().terms(clp.field);
+        if (terms != null) {
+          TermsEnum te = terms.iterator(null);
+          BytesRef termBytes = null;
+          while ((termBytes = te.next()) != null) {
+            String term = termBytes.utf8ToString();
+            if (term.startsWith(PAYLOAD_TERM_TEXT )) {
+              if (term.equals(PAYLOAD_TERM_TEXT)) {
+                fieldTerms.put(clp.field, new Term(clp.field, term));
+              } else {
+                fieldTerms.put(clp.field + term.substring(PAYLOAD_TERM_TEXT.length()), new Term(clp.field, term));
+              }
+            }
+          }
+        }        
+      }
+    }
+    reader.close();
+    return fieldTerms;
+  }
+  
+  private final Map<String,Term> fieldTerms;
+  
+  /**
+   * Wraps an {@link AtomicReader} and migrates the payload to {@link BinaryDocValues}
+   * fields by using the given mapping.
+   */
+  public FacetsPayloadMigrationReader(AtomicReader in, Map<String,Term> fieldTerms) {
+    super(in);
+    this.fieldTerms = fieldTerms;
+  }
+  
+  @Override
+  public BinaryDocValues getBinaryDocValues(String field) throws IOException {
+    Term term = fieldTerms.get(field);
+    if (term == null) {
+      return super.getBinaryDocValues(field);
+    } else {
+      // we shouldn't return null, even if the term does not exist or has no
+      // payloads, since we already marked the field as having DocValues.
+      return new PayloadMigratingBinaryDocValues(fields(), term);
+    }
+  }
+
+  @Override
+  public FieldInfos getFieldInfos() {
+    FieldInfos innerInfos = super.getFieldInfos();
+    ArrayList<FieldInfo> infos = new ArrayList<FieldInfo>(innerInfos.size());
+    // if there are partitions, then the source index contains one field for all their terms
+    // while with DocValues, we simulate that by multiple fields.
+    HashSet<String> leftoverFields = new HashSet<String>(fieldTerms.keySet());
+    int number = -1;
+    for (FieldInfo info : innerInfos) {
+      if (fieldTerms.containsKey(info.name)) {
+        // mark this field as having a DocValues
+        infos.add(new FieldInfo(info.name, true, info.number,
+            info.hasVectors(), info.omitsNorms(), info.hasPayloads(),
+            info.getIndexOptions(), DocValuesType.BINARY,
+            info.getNormType(), info.attributes()));
+        leftoverFields.remove(info.name);
+      } else {
+        infos.add(info);
+      }
+      number = Math.max(number, info.number);
+    }
+    for (String field : leftoverFields) {
+      infos.add(new FieldInfo(field, false, ++number, false, false, false,
+          null, DocValuesType.BINARY, null, null));
+    }
+    return new FieldInfos(infos.toArray(new FieldInfo[infos.size()]));
+  }
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/util/OrdinalMappingAtomicReader.java b/lucene/facet/src/java/org/apache/lucene/facet/util/OrdinalMappingAtomicReader.java
new file mode 100644
index 0000000..ad0a102
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/util/OrdinalMappingAtomicReader.java
@@ -0,0 +1,145 @@
+package org.apache.lucene.facet.util;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.lucene.facet.encoding.IntDecoder;
+import org.apache.lucene.facet.encoding.IntEncoder;
+import org.apache.lucene.facet.params.CategoryListParams;
+import org.apache.lucene.facet.params.FacetIndexingParams;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter.OrdinalMap;
+import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.BinaryDocValues;
+import org.apache.lucene.index.FilterAtomicReader;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IntsRef;
+
+/**
+ * A {@link FilterAtomicReader} for updating facets ordinal references,
+ * based on an ordinal map. You should use this code in conjunction with merging
+ * taxonomies - after you merge taxonomies, you receive an {@link OrdinalMap}
+ * which maps the 'old' ordinals to the 'new' ones. You can use that map to
+ * re-map the doc values which contain the facets information (ordinals) either
+ * before or while merging the indexes.
+ * <p>
+ * For re-mapping the ordinals during index merge, do the following:
+ * 
+ * <pre class="prettyprint">
+ * // merge the old taxonomy with the new one.
+ * OrdinalMap map = DirectoryTaxonomyWriter.addTaxonomies();
+ * int[] ordmap = map.getMap();
+ * 
+ * // Add the index and re-map ordinals on the go
+ * DirectoryReader reader = DirectoryReader.open(oldDir);
+ * IndexWriterConfig conf = new IndexWriterConfig(VER, ANALYZER);
+ * IndexWriter writer = new IndexWriter(newDir, conf);
+ * List&lt;AtomicReaderContext&gt; leaves = reader.leaves();
+ *   AtomicReader wrappedLeaves[] = new AtomicReader[leaves.size()];
+ *   for (int i = 0; i < leaves.size(); i++) {
+ *     wrappedLeaves[i] = new OrdinalMappingAtomicReader(leaves.get(i).reader(), ordmap);
+ *   }
+ * writer.addIndexes(new MultiReader(wrappedLeaves));
+ * writer.commit();
+ * </pre>
+ * 
+ * @lucene.experimental
+ */
+public class OrdinalMappingAtomicReader extends FilterAtomicReader {
+  
+  private final int[] ordinalMap;
+  
+  private final Map<String,CategoryListParams> dvFieldMap = new HashMap<String,CategoryListParams>();
+  
+  /**
+   * Wraps an AtomicReader, mapping ordinals according to the ordinalMap.
+   * Calls {@link #OrdinalMappingAtomicReader(AtomicReader, int[], FacetIndexingParams)
+   * OrdinalMappingAtomicReader(in, ordinalMap, new DefaultFacetIndexingParams())}
+   */
+  public OrdinalMappingAtomicReader(AtomicReader in, int[] ordinalMap) {
+    this(in, ordinalMap, FacetIndexingParams.ALL_PARENTS);
+  }
+  
+  /**
+   * Wraps an AtomicReader, mapping ordinals according to the ordinalMap,
+   * using the provided indexingParams.
+   */
+  public OrdinalMappingAtomicReader(AtomicReader in, int[] ordinalMap, FacetIndexingParams indexingParams) {
+    super(in);
+    this.ordinalMap = ordinalMap;
+    for (CategoryListParams params: indexingParams.getAllCategoryListParams()) {
+      dvFieldMap.put(params.field, params);
+    }
+  }
+
+  @Override
+  public BinaryDocValues getBinaryDocValues(String field) throws IOException {
+    BinaryDocValues inner = super.getBinaryDocValues(field);
+    if (inner == null) {
+      return inner;
+    }
+    
+    CategoryListParams clp = dvFieldMap.get(field);
+    if (clp == null) {
+      return inner;
+    } else {
+      return new OrdinalMappingBinaryDocValues(clp, inner);
+    }
+  }
+  
+  private class OrdinalMappingBinaryDocValues extends BinaryDocValues {
+
+    private final IntEncoder encoder;
+    private final IntDecoder decoder;
+    private final IntsRef ordinals = new IntsRef(32);
+    private final BinaryDocValues delegate;
+    private final BytesRef scratch = new BytesRef();
+    
+    protected OrdinalMappingBinaryDocValues(CategoryListParams clp, BinaryDocValues delegate) {
+      this.delegate = delegate;
+      encoder = clp.createEncoder();
+      decoder = encoder.createMatchingDecoder();
+    }
+    
+    @SuppressWarnings("synthetic-access")
+    @Override
+    public void get(int docID, BytesRef result) {
+      // NOTE: this isn't quite koscher, because in general
+      // multiple threads can call BinaryDV.get which would
+      // then conflict on the single scratch instance, but
+      // because this impl is only used for merging, we know
+      // only 1 thread calls us:
+      delegate.get(docID, scratch);
+      if (scratch.length > 0) {
+        // We must use scratch (and not re-use result) here,
+        // else encoder may overwrite the DV provider's
+        // private byte[]:
+        decoder.decode(scratch, ordinals);
+        
+        // map the ordinals
+        for (int i = 0; i < ordinals.length; i++) {
+          ordinals.ints[i] = ordinalMap[ordinals.ints[i]];
+        }
+        
+        encoder.encode(ordinals, result);
+      }
+    }
+  }
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/util/PartitionsUtils.java b/lucene/facet/src/java/org/apache/lucene/facet/util/PartitionsUtils.java
index a16a472..99f66df 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/util/PartitionsUtils.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/util/PartitionsUtils.java
@@ -1,6 +1,6 @@
 package org.apache.lucene.facet.util;
 
-import org.apache.lucene.facet.index.params.FacetIndexingParams;
+import org.apache.lucene.facet.params.FacetIndexingParams;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 
 /*
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/util/ResultSortUtils.java b/lucene/facet/src/java/org/apache/lucene/facet/util/ResultSortUtils.java
index 4c61d08..7a328a6 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/util/ResultSortUtils.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/util/ResultSortUtils.java
@@ -6,10 +6,10 @@ import java.util.Comparator;
 
 import org.apache.lucene.util.PriorityQueue;
 
+import org.apache.lucene.facet.search.FacetRequest;
+import org.apache.lucene.facet.search.FacetResultNode;
 import org.apache.lucene.facet.search.Heap;
-import org.apache.lucene.facet.search.params.FacetRequest;
-import org.apache.lucene.facet.search.params.FacetRequest.SortOrder;
-import org.apache.lucene.facet.search.results.FacetResultNode;
+import org.apache.lucene.facet.search.FacetRequest.SortOrder;
 
 /*
  * Licensed to the Apache Software Foundation (ASF) under one or more
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/util/TaxonomyMergeUtils.java b/lucene/facet/src/java/org/apache/lucene/facet/util/TaxonomyMergeUtils.java
index ea01728..3a6c75b 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/util/TaxonomyMergeUtils.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/util/TaxonomyMergeUtils.java
@@ -3,8 +3,7 @@ package org.apache.lucene.facet.util;
 import java.io.IOException;
 import java.util.List;
 
-import org.apache.lucene.facet.index.OrdinalMappingAtomicReader;
-import org.apache.lucene.facet.index.params.FacetIndexingParams;
+import org.apache.lucene.facet.params.FacetIndexingParams;
 import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
 import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter.OrdinalMap;
 import org.apache.lucene.index.AtomicReader;
diff --git a/lucene/facet/src/java/org/apache/lucene/util/UnsafeByteArrayInputStream.java b/lucene/facet/src/java/org/apache/lucene/util/UnsafeByteArrayInputStream.java
deleted file mode 100644
index 10b3ccb..0000000
--- a/lucene/facet/src/java/org/apache/lucene/util/UnsafeByteArrayInputStream.java
+++ /dev/null
@@ -1,147 +0,0 @@
-package org.apache.lucene.util;
-
-import java.io.ByteArrayInputStream;
-import java.io.IOException;
-import java.io.InputStream;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * This class, much like {@link ByteArrayInputStream} uses a given buffer as a
- * source of an InputStream. Unlike ByteArrayInputStream, this class does not
- * "waste" memory by creating a local copy of the given buffer, but rather uses
- * the given buffer as is. Hence the name Unsafe. While using this class one
- * should remember that the byte[] buffer memory is shared and might be changed
- * from outside.
- * 
- * For reuse-ability, a call for {@link #reInit(byte[])} can be called, and
- * initialize the stream with a new buffer.
- * 
- * @lucene.experimental
- */
-public class UnsafeByteArrayInputStream extends InputStream {
-
-  private byte[] buffer;
-  private int markIndex;
-  private int upperLimit;
-  private int index;
-
-  /**
-   * Creates a new instance by not using any byte[] up front. If you use this
-   * constructor, you MUST call either of the {@link #reInit(byte[]) reInit}
-   * methods before you consume any byte from this instance.<br>
-   * This constructor is for convenience purposes only, so that if one does not
-   * have the byte[] at the moment of creation, one is not forced to pass a
-   * <code>new byte[0]</code> or something. Obviously in that case, one will
-   * call either {@link #reInit(byte[]) reInit} methods before using the class.
-   */
-  public UnsafeByteArrayInputStream() {
-    markIndex = upperLimit = index = 0;
-  }
-
-  /**
-   * Creates an UnsafeByteArrayInputStream which uses a given byte array as
-   * the source of the stream. Default range is [0 , buffer.length)
-   * 
-   * @param buffer
-   *            byte array used as the source of this stream
-   */
-  public UnsafeByteArrayInputStream(byte[] buffer) {
-    reInit(buffer, 0, buffer.length);
-  }
-
-  /**
-   * Creates an UnsafeByteArrayInputStream which uses a given byte array as
-   * the source of the stream, at the specific range: [startPos, endPos)
-   * 
-   * @param buffer
-   *            byte array used as the source of this stream
-   * @param startPos
-   *            first index (inclusive) to the data lying in the given buffer
-   * @param endPos
-   *            an index (exclusive) where the data ends. data @
-   *            buffer[endPos] will never be read
-   */
-  public UnsafeByteArrayInputStream(byte[] buffer, int startPos, int endPos) {
-    reInit(buffer, startPos, endPos);
-  }
-
-  @Override
-  public void mark(int readlimit) {
-    markIndex = index;
-  }
-
-  @Override
-  public boolean markSupported() {
-    return true;
-  }
-
-  /**
-   * Initialize the stream with a given buffer, using the default limits of
-   * [0, buffer.length)
-   * 
-   * @param buffer
-   *            byte array used as the source of this stream
-   */
-  public void reInit(byte[] buffer) {
-    reInit(buffer, 0, buffer.length);
-  }
-
-  /**
-   * Initialize the stream with a given byte array as the source of the
-   * stream, at the specific range: [startPos, endPos)
-   * 
-   * @param buffer
-   *            byte array used as the source of this stream
-   * @param startPos
-   *            first index (inclusive) to the data lying in the given buffer
-   * @param endPos
-   *            an index (exclusive) where the data ends. data @
-   *            buffer[endPos] will never be read
-   */
-  public void reInit(byte[] buffer, int startPos, int endPos) {
-    this.buffer = buffer;
-    markIndex = startPos;
-    upperLimit = endPos;
-    index = markIndex;
-  }
-
-  @Override
-  public int available() throws IOException {
-    return upperLimit - index;
-  }
-
-  /**
-   * Read a byte. Data returned as an integer [0,255] If end of stream
-   * reached, returns -1
-   */
-  @Override
-  public int read() throws IOException {
-    return index < upperLimit ? buffer[index++] & 0xff : -1;
-  }
-
-  /**
-   * Resets the stream back to its original state. Basically - moving the
-   * index back to start position.
-   */
-  @Override
-  public void reset() throws IOException {
-    index = markIndex;
-  }
-
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/util/UnsafeByteArrayOutputStream.java b/lucene/facet/src/java/org/apache/lucene/util/UnsafeByteArrayOutputStream.java
deleted file mode 100644
index 80e5c8c..0000000
--- a/lucene/facet/src/java/org/apache/lucene/util/UnsafeByteArrayOutputStream.java
+++ /dev/null
@@ -1,184 +0,0 @@
-package org.apache.lucene.util;
-
-import java.io.IOException;
-import java.io.OutputStream;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * This class is used as a wrapper to a byte array, extending
- * {@link OutputStream}. Data is written in the given byte[] buffer, until its
- * length is insufficient. Than the buffer size is doubled and the data is
- * written.
- * 
- * This class is Unsafe as it is using a buffer which potentially can be changed
- * from the outside. Moreover, when {@link #toByteArray()} is called, the buffer
- * itself is returned, and not a copy.
- * 
- * @lucene.experimental
- */
-public class UnsafeByteArrayOutputStream extends OutputStream {
-
-  private byte[] buffer;
-  private int index;
-  private int startIndex;
-
-  /**
-   * Constructs a new output stream, with a default allocated buffer which can
-   * later be obtained via {@link #toByteArray()}.
-   */
-  public UnsafeByteArrayOutputStream() {
-    reInit(new byte[32], 0);
-  }
-
-  /**
-   * Constructs a new output stream, with a given buffer. Writing will start
-   * at index 0 as a default.
-   * 
-   * @param buffer
-   *            some space to which writing will be made
-   */
-  public UnsafeByteArrayOutputStream(byte[] buffer) {
-    reInit(buffer, 0);
-  }
-
-  /**
-   * Constructs a new output stream, with a given buffer. Writing will start
-   * at a given index.
-   * 
-   * @param buffer
-   *            some space to which writing will be made.
-   * @param startPos
-   *            an index (inclusive) from white data will be written.
-   */
-  public UnsafeByteArrayOutputStream(byte[] buffer, int startPos) {
-    reInit(buffer, startPos);
-  }
-
-  private void grow(int newLength) {
-    // It actually should be: (Java 1.7, when its intrinsic on all machines)
-    // buffer = Arrays.copyOf(buffer, newLength);
-    byte[] newBuffer = new byte[newLength];
-    System.arraycopy(buffer, 0, newBuffer, 0, buffer.length);
-    buffer = newBuffer;
-  }
-
-  /**
-   * For reuse-ability, this stream object can be re-initialized with another
-   * given buffer and starting position.
-   * 
-   * @param buffer some space to which writing will be made.
-   * @param startPos an index (inclusive) from white data will be written.
-   */
-  public void reInit(byte[] buffer, int startPos) {
-    if (buffer.length == 0) {
-      throw new IllegalArgumentException("initial buffer length must be greater than 0.");
-    }
-    this.buffer = buffer;
-    startIndex = startPos;
-    index = startIndex;
-  }
-
-  /**
-   * For reuse-ability, this stream object can be re-initialized with another
-   * given buffer, using 0 as default starting position.
-   * 
-   * @param buffer some space to which writing will be made.
-   */
-  public void reInit(byte[] buffer) {
-    reInit(buffer, 0);
-  }
-
-  /**
-   * writes a given byte(at the form of an int) to the buffer. If the buffer's
-   * empty space is insufficient, the buffer is doubled.
-   * 
-   * @param value byte value to be written
-   */
-  @Override
-  public void write(int value) throws IOException {
-    if (index >= buffer.length) {
-      grow(buffer.length << 1);
-    }
-    buffer[index++] = (byte) value;
-  }
-
-  /**
-   * writes a given byte[], with offset and length to the buffer. If the
-   * buffer's empty space is insufficient, the buffer is doubled until it
-   * could contain all the data.
-   * 
-   * @param b
-   *            byte buffer, containing the source data to be written
-   * @param off
-   *            index from which data from the buffer b should be written
-   * @param len
-   *            number of bytes that should be written
-   */
-  @Override
-  public void write(byte[] b, int off, int len) throws IOException {
-    // If there's not enough space for the data
-    int targetLength = index + len;
-    if (targetLength >= buffer.length) {
-      // Calculating the new required length of the array, keeping the array
-      // size a power of 2 if it was initialized like that.
-      int newlen = buffer.length;
-      while ((newlen <<= 1) < targetLength) {}
-      grow(newlen);
-    }
-
-    // Now that we have enough spare space, we could copy the rest of the
-    // data
-    System.arraycopy(b, off, buffer, index, len);
-
-    // Updating the index to next available index.
-    index += len;
-  }
-
-  /**
-   * Returns the byte array saved within the buffer AS IS.
-   * 
-   * @return the actual inner buffer - not a copy of it.
-   */
-  public byte[] toByteArray() {
-    return buffer;
-  }
-
-  /**
-   * Returns the number of relevant bytes. This objects makes sure the buffer
-   * is at least the size of it's data. But it can also be twice as big. The
-   * user would want to process the relevant bytes only. For that he would
-   * need the count.
-   * 
-   * @return number of relevant bytes
-   */
-  public int length() {
-    return index;
-  }
-
-  /**
-   * Returns the start position data was written to. This is useful in case you
-   * used {@link #reInit(byte[], int)} or
-   * {@link #UnsafeByteArrayOutputStream(byte[], int)} and passed a start
-   * position which is not 0.
-   */
-  public int getStartPos() {
-    return startIndex;
-  }
-
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/util/collections/ArrayHashMap.java b/lucene/facet/src/java/org/apache/lucene/util/collections/ArrayHashMap.java
deleted file mode 100644
index cce3808..0000000
--- a/lucene/facet/src/java/org/apache/lucene/util/collections/ArrayHashMap.java
+++ /dev/null
@@ -1,554 +0,0 @@
-package org.apache.lucene.util.collections;
-
-import java.util.Arrays;
-import java.util.Iterator;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * An Array-based hashtable which maps, similar to Java's HashMap, only
- * performance tests showed it performs better.
- * <p>
- * The hashtable is constructed with a given capacity, or 16 as a default. In
- * case there's not enough room for new pairs, the hashtable grows. Capacity is
- * adjusted to a power of 2, and there are 2 * capacity entries for the hash.
- * The pre allocated arrays (for keys, values) are at length of capacity + 1,
- * where index 0 is used as 'Ground' or 'NULL'.
- * <p>
- * The arrays are allocated ahead of hash operations, and form an 'empty space'
- * list, to which the &lt;key,value&gt; pair is allocated.
- * 
- * @lucene.experimental
- */
-public class ArrayHashMap<K,V> implements Iterable<V> {
-
-  /** Implements an IntIterator which iterates over all the allocated indexes. */
-  private final class IndexIterator implements IntIterator {
-    /**
-     * The last used baseHashIndex. Needed for "jumping" from one hash entry
-     * to another.
-     */
-    private int baseHashIndex = 0;
-
-    /** The next not-yet-visited index. */
-    private int index = 0;
-
-    /** Index of the last visited pair. Used in {@link #remove()}. */
-    private int lastIndex = 0;
-
-    /**
-     * Create the Iterator, make <code>index</code> point to the "first"
-     * index which is not empty. If such does not exist (eg. the map is
-     * empty) it would be zero.
-     */
-    public IndexIterator() {
-      for (baseHashIndex = 0; baseHashIndex < baseHash.length; ++baseHashIndex) {
-        index = baseHash[baseHashIndex];
-        if (index != 0) {
-          break;
-        }
-      }
-    }
-
-    @Override
-    public boolean hasNext() {
-      return index != 0;
-    }
-
-    @Override
-    public int next() {
-      // Save the last index visited
-      lastIndex = index;
-
-      // next the index
-      index = next[index];
-
-      // if the next index points to the 'Ground' it means we're done with
-      // the current hash entry and we need to jump to the next one. This
-      // is done until all the hash entries had been visited.
-      while (index == 0 && ++baseHashIndex < baseHash.length) {
-        index = baseHash[baseHashIndex];
-      }
-
-      return lastIndex;
-    }
-
-    @Override
-    @SuppressWarnings("unchecked")
-    public void remove() {
-      ArrayHashMap.this.remove((K) keys[lastIndex]);
-    }
-
-  }
-
-  /** Implements an Iterator, used for iteration over the map's keys. */
-  private final class KeyIterator implements Iterator<K> {
-    private IntIterator iterator = new IndexIterator();
-
-    KeyIterator() { }
-
-    @Override
-    public boolean hasNext() {
-      return iterator.hasNext();
-    }
-
-    @Override
-    @SuppressWarnings("unchecked")
-    public K next() {
-      return (K) keys[iterator.next()];
-    }
-
-    @Override
-    public void remove() {
-      iterator.remove();
-    }
-  }
-
-  /** Implements an Iterator, used for iteration over the map's values. */
-  private final class ValueIterator implements Iterator<V> {
-    private IntIterator iterator = new IndexIterator();
-
-    ValueIterator() { }
-
-    @Override
-    public boolean hasNext() {
-      return iterator.hasNext();
-    }
-
-    @Override
-    @SuppressWarnings("unchecked")
-    public V next() {
-      return (V) values[iterator.next()];
-    }
-
-    @Override
-    public void remove() {
-      iterator.remove();
-    }
-  }
-
-  /** Default capacity - in case no capacity was specified in the constructor */
-  private static final int DEFAULT_CAPACITY = 16;
-
-  /**
-   * Holds the base hash entries. if the capacity is 2^N, than the base hash
-   * holds 2^(N+1).
-   */
-  int[] baseHash;
-
-  /**
-   * The current capacity of the map. Always 2^N and never less than 16. We
-   * never use the zero index. It is needed to improve performance and is also
-   * used as "ground".
-   */
-  private int capacity;
-
-  /**
-   * All objects are being allocated at map creation. Those objects are "free"
-   * or empty. Whenever a new pair comes along, a pair is being "allocated" or
-   * taken from the free-linked list. as this is just a free list.
-   */
-  private int firstEmpty;
-
-  /** hashFactor is always (2^(N+1)) - 1. Used for faster hashing. */
-  private int hashFactor;
-
-  /** Holds the unique keys. */
-  Object[] keys;
-
-  /**
-   * In case of collisions, we implement a double linked list of the colliding
-   * hash's with the following next[] and prev[]. Those are also used to store
-   * the "empty" list.
-   */
-  int[] next;
-
-  private int prev;
-
-  /** Number of currently stored objects in the map. */
-  private int size;
-
-  /** Holds the values. */
-  Object[] values;
-
-  /** Constructs a map with default capacity. */
-  public ArrayHashMap() {
-    this(DEFAULT_CAPACITY);
-  }
-
-  /**
-   * Constructs a map with given capacity. Capacity is adjusted to a native
-   * power of 2, with minimum of 16.
-   * 
-   * @param capacity minimum capacity for the map.
-   */
-  public ArrayHashMap(int capacity) {
-    this.capacity = 16;
-    while (this.capacity < capacity) {
-      // Multiply by 2 as long as we're still under the requested capacity
-      this.capacity <<= 1;
-    }
-
-    // As mentioned, we use the first index (0) as 'Ground', so we need the
-    // length of the arrays to be one more than the capacity
-    int arrayLength = this.capacity + 1;
-
-    values = new Object[arrayLength];
-    keys = new Object[arrayLength];
-    next = new int[arrayLength];
-
-    // Hash entries are twice as big as the capacity.
-    int baseHashSize = this.capacity << 1;
-
-    baseHash = new int[baseHashSize];
-
-    // The has factor is 2^M - 1 which is used as an "AND" hashing operator.
-    // {@link #calcBaseHash()}
-    hashFactor = baseHashSize - 1;
-
-    size = 0;
-
-    clear();
-  }
-
-  /**
-   * Adds a pair to the map. Takes the first empty position from the
-   * empty-linked-list's head - {@link #firstEmpty}. New pairs are always
-   * inserted to baseHash, and are followed by the old colliding pair.
-   */
-  private void prvt_put(K key, V value) {
-    // Hash entry to which the new pair would be inserted
-    int hashIndex = calcBaseHashIndex(key);
-
-    // 'Allocating' a pair from the "Empty" list.
-    int objectIndex = firstEmpty;
-
-    // Setting data
-    firstEmpty = next[firstEmpty];
-    values[objectIndex] = value;
-    keys[objectIndex] = key;
-
-    // Inserting the new pair as the first node in the specific hash entry
-    next[objectIndex] = baseHash[hashIndex];
-    baseHash[hashIndex] = objectIndex;
-
-    // Announcing a new pair was added!
-    ++size;
-  }
-
-  /** Calculating the baseHash index using the internal internal <code>hashFactor</code>. */
-  protected int calcBaseHashIndex(K key) {
-    return key.hashCode() & hashFactor;
-  }
-
-  /** Empties the map. Generates the "Empty" space list for later allocation. */
-  public void clear() {
-    // Clears the hash entries
-    Arrays.fill(baseHash, 0);
-
-    // Set size to zero
-    size = 0;
-
-    // Mark all array entries as empty. This is done with
-    // <code>firstEmpty</code> pointing to the first valid index (1 as 0 is
-    // used as 'Ground').
-    firstEmpty = 1;
-
-    // And setting all the <code>next[i]</code> to point at
-    // <code>i+1</code>.
-    for (int i = 1; i < capacity;) {
-      next[i] = ++i;
-    }
-
-    // Surly, the last one should point to the 'Ground'.
-    next[capacity] = 0;
-  }
-
-  /** Returns true iff the key exists in the map. */
-  public boolean containsKey(K key) {
-    return find(key) != 0;
-  }
-
-  /** Returns true iff the object exists in the map. */
-  public boolean containsValue(Object o) {
-    for (Iterator<V> iterator = iterator(); iterator.hasNext();) {
-      V object = iterator.next();
-      if (object.equals(o)) {
-        return true;
-      }
-    }
-    return false;
-  }
-
-  /** Returns the index of the given key, or zero if the key wasn't found. */
-  protected int find(K key) {
-    // Calculate the hash entry.
-    int baseHashIndex = calcBaseHashIndex(key);
-
-    // Start from the hash entry.
-    int localIndex = baseHash[baseHashIndex];
-
-    // while the index does not point to the 'Ground'
-    while (localIndex != 0) {
-      // returns the index found in case of of a matching key.
-      if (keys[localIndex].equals(key)) {
-        return localIndex;
-      }
-
-      // next the local index
-      localIndex = next[localIndex];
-    }
-
-    // If we got this far, it could only mean we did not find the key we
-    // were asked for. return 'Ground' index.
-    return 0;
-  }
-
-  /**
-   * Finds the actual index of a given key with it's baseHashIndex. Some methods
-   * use the baseHashIndex. If those call {@link #find} there's no need to
-   * re-calculate that hash.
-   * 
-   * @return the index of the given key, or 0 if the key wasn't found.
-   */
-  private int findForRemove(K key, int baseHashIndex) {
-    // Start from the hash entry.
-    prev = 0;
-    int index = baseHash[baseHashIndex];
-
-    // while the index does not point to the 'Ground'
-    while (index != 0) {
-      // returns the index found in case of of a matching key.
-      if (keys[index].equals(key)) {
-        return index;
-      }
-
-      // next the local index
-      prev = index;
-      index = next[index];
-    }
-
-    // If we got thus far, it could only mean we did not find the key we
-    // were asked for. return 'Ground' index.
-    return prev = 0;
-  }
-
-  /** Returns the object mapped with the given key, or null if the key wasn't found. */
-  @SuppressWarnings("unchecked")
-  public V get(K key) {
-    return (V) values[find(key)];
-  }
-
-  /**
-   * Allocates a new map of double the capacity, and fast-insert the old
-   * key-value pairs.
-   */
-  @SuppressWarnings("unchecked")
-  protected void grow() {
-    ArrayHashMap<K,V> newmap = new ArrayHashMap<K,V>(capacity * 2);
-
-    // Iterates fast over the collection. Any valid pair is put into the new
-    // map without checking for duplicates or if there's enough space for
-    // it.
-    for (IndexIterator iterator = new IndexIterator(); iterator.hasNext();) {
-      int index = iterator.next();
-      newmap.prvt_put((K) keys[index], (V) values[index]);
-    }
-
-    // Copy that's data into this.
-    capacity = newmap.capacity;
-    size = newmap.size;
-    firstEmpty = newmap.firstEmpty;
-    values = newmap.values;
-    keys = newmap.keys;
-    next = newmap.next;
-    baseHash = newmap.baseHash;
-    hashFactor = newmap.hashFactor;
-  }
-
-  /** Returns true iff the map is empty. */
-  public boolean isEmpty() {
-    return size == 0;
-  }
-
-  /** Returns an iterator on the mapped objects. */
-  @Override
-  public Iterator<V> iterator() {
-    return new ValueIterator();
-  }
-
-  /** Returns an iterator on the map keys. */
-  public Iterator<K> keyIterator() {
-    return new KeyIterator();
-  }
-
-  /** Prints the baseHash array, used for debugging purposes. */
-  @SuppressWarnings("unused")
-  private String getBaseHashAsString() {
-    return Arrays.toString(this.baseHash);
-  }
-
-  /**
-   * Inserts the &lt;key,value&gt; pair into the map. If the key already exists,
-   * this method updates the mapped value to the given one, returning the old
-   * mapped value.
-   * 
-   * @return the old mapped value, or null if the key didn't exist.
-   */
-  @SuppressWarnings("unchecked")
-  public V put(K key, V e) {
-    // Does key exists?
-    int index = find(key);
-
-    // Yes!
-    if (index != 0) {
-      // Set new data and exit.
-      V old = (V) values[index];
-      values[index] = e;
-      return old;
-    }
-
-    // Is there enough room for a new pair?
-    if (size == capacity) {
-      // No? Than grow up!
-      grow();
-    }
-
-    // Now that everything is set, the pair can be just put inside with no
-    // worries.
-    prvt_put(key, e);
-
-    return null;
-  }
-
-  /**
-   * Removes a &lt;key,value&gt; pair from the map and returns the mapped value,
-   * or null if the none existed.
-   * 
-   * @param key used to find the value to remove
-   * @return the removed value or null if none existed.
-   */
-  @SuppressWarnings("unchecked")
-  public V remove(K key) {
-    int baseHashIndex = calcBaseHashIndex(key);
-    int index = findForRemove(key, baseHashIndex);
-    if (index != 0) {
-      // If it is the first in the collision list, we should promote its
-      // next colliding element.
-      if (prev == 0) {
-        baseHash[baseHashIndex] = next[index];
-      }
-
-      next[prev] = next[index];
-      next[index] = firstEmpty;
-      firstEmpty = index;
-      --size;
-      return (V) values[index];
-    }
-
-    return null;
-  }
-
-  /** Returns number of pairs currently in the map. */
-  public int size() {
-    return this.size;
-  }
-
-  /**
-   * Translates the mapped pairs' values into an array of Objects
-   * 
-   * @return an object array of all the values currently in the map.
-   */
-  public Object[] toArray() {
-    int j = -1;
-    Object[] array = new Object[size];
-
-    // Iterates over the values, adding them to the array.
-    for (Iterator<V> iterator = iterator(); iterator.hasNext();) {
-      array[++j] = iterator.next();
-    }
-    return array;
-  }
-
-  /**
-   * Translates the mapped pairs' values into an array of V
-   * 
-   * @param a the array into which the elements of the list are to be stored, if
-   *        it is big enough; otherwise, use as much space as it can.
-   * @return an array containing the elements of the list
-   */
-  public V[] toArray(V[] a) {
-    int j = 0;
-    // Iterates over the values, adding them to the array.
-    for (Iterator<V> iterator = iterator(); j < a.length
-    && iterator.hasNext(); ++j) {
-      a[j] = iterator.next();
-    }
-    if (j < a.length) {
-      a[j] = null;
-    }
-
-    return a;
-  }
-
-  @Override
-  public String toString() {
-    StringBuffer sb = new StringBuffer();
-    sb.append('{');
-    Iterator<K> keyIterator = keyIterator();
-    while (keyIterator.hasNext()) {
-      K key = keyIterator.next();
-      sb.append(key);
-      sb.append('=');
-      sb.append(get(key));
-      if (keyIterator.hasNext()) {
-        sb.append(',');
-        sb.append(' ');
-      }
-    }
-    sb.append('}');
-    return sb.toString();
-  }
-
-  @Override
-  public int hashCode() {
-    return getClass().hashCode() ^ size();
-  }
-
-  @SuppressWarnings("unchecked")
-  @Override
-  public boolean equals(Object o) {
-    ArrayHashMap<K, V> that = (ArrayHashMap<K,V>)o;
-    if (that.size() != this.size()) {
-      return false;
-    }
-
-    Iterator<K> it = keyIterator();
-    while (it.hasNext()) {
-      K key = it.next();
-      V v1 = this.get(key);
-      V v2 = that.get(key);
-      if ((v1 == null && v2 != null) ||
-          (v1 != null && v2 == null) ||
-          (!v1.equals(v2))) {
-        return false;
-      }
-    }
-    return true;
-  }
-}
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/util/collections/DoubleIterator.java b/lucene/facet/src/java/org/apache/lucene/util/collections/DoubleIterator.java
deleted file mode 100644
index c3c9261..0000000
--- a/lucene/facet/src/java/org/apache/lucene/util/collections/DoubleIterator.java
+++ /dev/null
@@ -1,31 +0,0 @@
-package org.apache.lucene.util.collections;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Iterator interface for primitive double iteration. *
- * 
- * @lucene.experimental
- */
-public interface DoubleIterator {
-
-  boolean hasNext();
-  double next();
-  void remove();
-
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/util/collections/FloatIterator.java b/lucene/facet/src/java/org/apache/lucene/util/collections/FloatIterator.java
deleted file mode 100644
index 7e6a728..0000000
--- a/lucene/facet/src/java/org/apache/lucene/util/collections/FloatIterator.java
+++ /dev/null
@@ -1,31 +0,0 @@
-package org.apache.lucene.util.collections;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Iterator interface for primitive int iteration. *
- * 
- * @lucene.experimental
- */
-public interface FloatIterator {
-
-  boolean hasNext();
-  float next();
-  void remove();
-
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/util/collections/FloatToObjectMap.java b/lucene/facet/src/java/org/apache/lucene/util/collections/FloatToObjectMap.java
deleted file mode 100644
index c096fe4..0000000
--- a/lucene/facet/src/java/org/apache/lucene/util/collections/FloatToObjectMap.java
+++ /dev/null
@@ -1,634 +0,0 @@
-package org.apache.lucene.util.collections;
-
-import java.util.Arrays;
-import java.util.Iterator;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
-
- * An Array-based hashtable which maps primitive float to Objects of generic type
- * T.<br>
- * The hashtable is constracted with a given capacity, or 16 as a default. In
- * case there's not enough room for new pairs, the hashtable grows. <br>
- * Capacity is adjusted to a power of 2, and there are 2 * capacity entries for
- * the hash.
- * 
- * The pre allocated arrays (for keys, values) are at length of capacity + 1,
- * when index 0 is used as 'Ground' or 'NULL'.<br>
- * 
- * The arrays are allocated ahead of hash operations, and form an 'empty space'
- * list, to which the key,value pair is allocated.
- * 
- * @lucene.experimental
- */
-public class FloatToObjectMap<T> implements Iterable<T> {
-
-  /**
-   * Implements an IntIterator which iterates over all the allocated indexes.
-   */
-  private final class IndexIterator implements IntIterator {
-    /**
-     * The last used baseHashIndex. Needed for "jumping" from one hash entry
-     * to another.
-     */
-    private int baseHashIndex = 0;
-
-    /**
-     * The next not-yet-visited index.
-     */
-    private int index = 0;
-
-    /**
-     * Index of the last visited pair. Used in {@link #remove()}.
-     */
-    private int lastIndex = 0;
-
-    /**
-     * Create the Iterator, make <code>index</code> point to the "first"
-     * index which is not empty. If such does not exist (eg. the map is
-     * empty) it would be zero.
-     */
-    public IndexIterator() {
-      for (baseHashIndex = 0; baseHashIndex < baseHash.length; ++baseHashIndex) {
-        index = baseHash[baseHashIndex];
-        if (index != 0) {
-          break;
-        }
-      }
-    }
-
-    @Override
-    public boolean hasNext() {
-      return (index != 0);
-    }
-
-    @Override
-    public int next() {
-      // Save the last index visited
-      lastIndex = index;
-
-      // next the index
-      index = next[index];
-
-      // if the next index points to the 'Ground' it means we're done with
-      // the current hash entry and we need to jump to the next one. This
-      // is done until all the hash entries had been visited.
-      while (index == 0 && ++baseHashIndex < baseHash.length) {
-        index = baseHash[baseHashIndex];
-      }
-
-      return lastIndex;
-    }
-
-    @Override
-    public void remove() {
-      FloatToObjectMap.this.remove(keys[lastIndex]);
-    }
-
-  }
-
-  /**
-   * Implements an IntIterator, used for iteration over the map's keys.
-   */
-  private final class KeyIterator implements FloatIterator {
-    private IntIterator iterator = new IndexIterator();
-
-    KeyIterator() { }
-
-    @Override
-    public boolean hasNext() {
-      return iterator.hasNext();
-    }
-
-    @Override
-    public float next() {
-      return keys[iterator.next()];
-    }
-
-    @Override
-    public void remove() {
-      iterator.remove();
-    }
-  }
-
-  /**
-   * Implements an Iterator of a generic type T used for iteration over the
-   * map's values.
-   */
-  private final class ValueIterator implements Iterator<T> {
-    private IntIterator iterator = new IndexIterator();
-
-    ValueIterator() { }
-
-    @Override
-    public boolean hasNext() {
-      return iterator.hasNext();
-    }
-
-    @Override
-    @SuppressWarnings("unchecked")
-    public T next() {
-      return (T) values[iterator.next()];
-    }
-
-    @Override
-    public void remove() {
-      iterator.remove();
-    }
-  }
-
-  /**
-   * Default capacity - in case no capacity was specified in the constructor
-   */
-  private static int defaultCapacity = 16;
-
-  /**
-   * Holds the base hash entries. if the capacity is 2^N, than the base hash
-   * holds 2^(N+1). It can hold
-   */
-  int[] baseHash;
-
-  /**
-   * The current capacity of the map. Always 2^N and never less than 16. We
-   * never use the zero index. It is needed to improve performance and is also
-   * used as "ground".
-   */
-  private int capacity;
-  /**
-   * All objects are being allocated at map creation. Those objects are "free"
-   * or empty. Whenever a new pair comes along, a pair is being "allocated" or
-   * taken from the free-linked list. as this is just a free list.
-   */
-  private int firstEmpty;
-
-  /**
-   * hashFactor is always (2^(N+1)) - 1. Used for faster hashing.
-   */
-  private int hashFactor;
-
-  /**
-   * This array holds the unique keys
-   */
-  float[] keys;
-
-  /**
-   * In case of collisions, we implement a double linked list of the colliding
-   * hash's with the following next[] and prev[]. Those are also used to store
-   * the "empty" list.
-   */
-  int[] next;
-
-  private int prev;
-
-  /**
-   * Number of currently objects in the map.
-   */
-  private int size;
-
-  /**
-   * This array holds the values
-   */
-  Object[] values;
-
-  /**
-   * Constructs a map with default capacity.
-   */
-  public FloatToObjectMap() {
-    this(defaultCapacity);
-  }
-
-  /**
-   * Constructs a map with given capacity. Capacity is adjusted to a native
-   * power of 2, with minimum of 16.
-   * 
-   * @param capacity
-   *            minimum capacity for the map.
-   */
-  public FloatToObjectMap(int capacity) {
-    this.capacity = 16;
-    // Minimum capacity is 16..
-    while (this.capacity < capacity) {
-      // Multiply by 2 as long as we're still under the requested capacity
-      this.capacity <<= 1;
-    }
-
-    // As mentioned, we use the first index (0) as 'Ground', so we need the
-    // length of the arrays to be one more than the capacity
-    int arrayLength = this.capacity + 1;
-
-    this.values = new Object[arrayLength];
-    this.keys = new float[arrayLength];
-    this.next = new int[arrayLength];
-
-    // Hash entries are twice as big as the capacity.
-    int baseHashSize = this.capacity << 1;
-
-    this.baseHash = new int[baseHashSize];
-
-    // The has factor is 2^M - 1 which is used as an "AND" hashing operator.
-    // {@link #calcBaseHash()}
-    this.hashFactor = baseHashSize - 1;
-
-    this.size = 0;
-
-    clear();
-  }
-
-  /**
-   * Adds a pair to the map. Takes the first empty position from the
-   * empty-linked-list's head - {@link #firstEmpty}.
-   * 
-   * New pairs are always inserted to baseHash, and are followed by the old
-   * colliding pair.
-   * 
-   * @param key
-   *            integer which maps the given Object
-   * @param e
-   *            element which is being mapped using the given key
-   */
-  private void prvt_put(float key, T e) {
-    // Hash entry to which the new pair would be inserted
-    int hashIndex = calcBaseHashIndex(key);
-
-    // 'Allocating' a pair from the "Empty" list.
-    int objectIndex = firstEmpty;
-
-    // Setting data
-    firstEmpty = next[firstEmpty];
-    values[objectIndex] = e;
-    keys[objectIndex] = key;
-
-    // Inserting the new pair as the first node in the specific hash entry
-    next[objectIndex] = baseHash[hashIndex];
-    baseHash[hashIndex] = objectIndex;
-
-    // Announcing a new pair was added!
-    ++size;
-  }
-
-  /**
-   * Calculating the baseHash index using the internal <code>hashFactor</code>.
-   */
-  protected int calcBaseHashIndex(float key) {
-    return Float.floatToIntBits(key) & hashFactor;
-  }
-
-  /**
-   * Empties the map. Generates the "Empty" space list for later allocation.
-   */
-  public void clear() {
-    // Clears the hash entries
-    Arrays.fill(this.baseHash, 0);
-
-    // Set size to zero
-    size = 0;
-
-    // Mark all array entries as empty. This is done with
-    // <code>firstEmpty</code> pointing to the first valid index (1 as 0 is
-    // used as 'Ground').
-    firstEmpty = 1;
-
-    // And setting all the <code>next[i]</code> to point at
-    // <code>i+1</code>.
-    for (int i = 1; i < this.capacity;) {
-      next[i] = ++i;
-    }
-
-    // Surly, the last one should point to the 'Ground'.
-    next[this.capacity] = 0;
-  }
-
-  /**
-   * Checks if a given key exists in the map.
-   * 
-   * @param key
-   *            that is checked against the map data.
-   * @return true if the key exists in the map. false otherwise.
-   */
-  public boolean containsKey(float key) {
-    return find(key) != 0;
-  }
-
-  /**
-   * Checks if the given object exists in the map.<br>
-   * This method iterates over the collection, trying to find an equal object.
-   * 
-   * @param o
-   *            object that is checked against the map data.
-   * @return true if the object exists in the map (in .equals() meaning).
-   *         false otherwise.
-   */
-  public boolean containsValue(Object o) {
-    for (Iterator<T> iterator = iterator(); iterator.hasNext();) {
-      T object = iterator.next();
-      if (object.equals(o)) {
-        return true;
-      }
-    }
-    return false;
-  }
-
-  /**
-   * Find the actual index of a given key.
-   * 
-   * @return index of the key. zero if the key wasn't found.
-   */
-  protected int find(float key) {
-    // Calculate the hash entry.
-    int baseHashIndex = calcBaseHashIndex(key);
-
-    // Start from the hash entry.
-    int localIndex = baseHash[baseHashIndex];
-
-    // while the index does not point to the 'Ground'
-    while (localIndex != 0) {
-      // returns the index found in case of of a matching key.
-      if (keys[localIndex] == key) {
-        return localIndex;
-      }
-
-      // next the local index
-      localIndex = next[localIndex];
-    }
-
-    // If we got this far, it could only mean we did not find the key we
-    // were asked for. return 'Ground' index.
-    return 0;
-  }
-
-  /**
-   * Find the actual index of a given key with it's baseHashIndex.<br>
-   * Some methods use the baseHashIndex. If those call {@link #find} there's
-   * no need to re-calculate that hash.
-   * 
-   * @return the index of the given key, or 0 as 'Ground' if the key wasn't
-   *         found.
-   */
-  private int findForRemove(float key, int baseHashIndex) {
-    // Start from the hash entry.
-    this.prev = 0;
-    int index = baseHash[baseHashIndex];
-
-    // while the index does not point to the 'Ground'
-    while (index != 0) {
-      // returns the index found in case of of a matching key.
-      if (keys[index] == key) {
-        return index;
-      }
-
-      // next the local index
-      prev = index;
-      index = next[index];
-    }
-
-    // If we got this far, it could only mean we did not find the key we
-    // were asked for. return 'Ground' index.
-    this.prev = 0;
-    return 0;
-  }
-
-  /**
-   * Returns the object mapped with the given key.
-   * 
-   * @param key
-   *            int who's mapped object we're interested in.
-   * @return an object mapped by the given key. null if the key wasn't found.
-   */
-  @SuppressWarnings("unchecked")
-  public T get(float key) {
-    return (T) values[find(key)];
-  }
-
-  /**
-   * Grows the map. Allocates a new map of double the capacity, and
-   * fast-insert the old key-value pairs.
-   */
-  @SuppressWarnings("unchecked")
-  protected void grow() {
-    FloatToObjectMap<T> that = new FloatToObjectMap<T>(
-        this.capacity * 2);
-
-    // Iterates fast over the collection. Any valid pair is put into the new
-    // map without checking for duplicates or if there's enough space for
-    // it.
-    for (IndexIterator iterator = new IndexIterator(); iterator.hasNext();) {
-      int index = iterator.next();
-      that.prvt_put(this.keys[index], (T) this.values[index]);
-    }
-
-    // Copy that's data into this.
-    this.capacity = that.capacity;
-    this.size = that.size;
-    this.firstEmpty = that.firstEmpty;
-    this.values = that.values;
-    this.keys = that.keys;
-    this.next = that.next;
-    this.baseHash = that.baseHash;
-    this.hashFactor = that.hashFactor;
-  }
-
-  /**
-   * 
-   * @return true if the map is empty. false otherwise.
-   */
-  public boolean isEmpty() {
-    return size == 0;
-  }
-
-  /**
-   * Returns a new iterator for the mapped objects.
-   */
-  @Override
-  public Iterator<T> iterator() {
-    return new ValueIterator();
-  }
-
-  /** Returns an iterator on the map keys. */
-  public FloatIterator keyIterator() {
-    return new KeyIterator();
-  }
-
-  /**
-   * Prints the baseHash array, used for DEBUG purposes.
-   */
-  @SuppressWarnings("unused")
-  private String getBaseHashAsString() {
-    return Arrays.toString(this.baseHash);
-  }
-
-  /**
-   * Inserts the &lt;key,value&gt; pair into the map. If the key already exists,
-   * this method updates the mapped value to the given one, returning the old
-   * mapped value.
-   * 
-   * @return the old mapped value, or null if the key didn't exist.
-   */
-  @SuppressWarnings("unchecked")
-  public T put(float key, T e) {
-    // Does key exists?
-    int index = find(key);
-
-    // Yes!
-    if (index != 0) {
-      // Set new data and exit.
-      T old = (T) values[index];
-      values[index] = e;
-      return old;
-    }
-
-    // Is there enough room for a new pair?
-    if (size == capacity) {
-      // No? Than grow up!
-      grow();
-    }
-
-    // Now that everything is set, the pair can be just put inside with no
-    // worries.
-    prvt_put(key, e);
-
-    return null;
-  }
-
-  /**
-   * Removes a &lt;key,value&gt; pair from the map and returns the mapped value,
-   * or null if the none existed.
-   * 
-   * @param key used to find the value to remove
-   * @return the removed value or null if none existed.
-   */
-  @SuppressWarnings("unchecked")
-  public T remove(float key) {
-    int baseHashIndex = calcBaseHashIndex(key);
-    int index = findForRemove(key, baseHashIndex);
-    if (index != 0) {
-      // If it is the first in the collision list, we should promote its
-      // next colliding element.
-      if (prev == 0) {
-        baseHash[baseHashIndex] = next[index];
-      }
-
-      next[prev] = next[index];
-      next[index] = firstEmpty;
-      firstEmpty = index;
-      --size;
-      return (T) values[index];
-    }
-
-    return null;
-  }
-
-  /**
-   * @return number of pairs currently in the map
-   */
-  public int size() {
-    return this.size;
-  }
-
-  /**
-   * Translates the mapped pairs' values into an array of Objects
-   * 
-   * @return an object array of all the values currently in the map.
-   */
-  public Object[] toArray() {
-    int j = -1;
-    Object[] array = new Object[size];
-
-    // Iterates over the values, adding them to the array.
-    for (Iterator<T> iterator = iterator(); iterator.hasNext();) {
-      array[++j] = iterator.next();
-    }
-    return array;
-  }
-
-  /**
-   * Translates the mapped pairs' values into an array of T
-   * 
-   * @param a
-   *            the array into which the elements of the list are to be
-   *            stored, if it is big enough; otherwise, use whatever space we
-   *            have, setting the one after the true data as null.
-   * 
-   * @return an array containing the elements of the list
-   * 
-   */
-  public T[] toArray(T[] a) {
-    int j = 0;
-    // Iterates over the values, adding them to the array.
-    for (Iterator<T> iterator = iterator(); j < a.length
-    && iterator.hasNext(); ++j) {
-      a[j] = iterator.next();
-    }
-
-    if (j < a.length) {
-      a[j] = null;
-    }
-
-    return a;
-  }
-
-  @Override
-  public String toString() {
-    StringBuffer sb = new StringBuffer();
-    sb.append('{');
-    FloatIterator keyIterator = keyIterator();
-    while (keyIterator.hasNext()) {
-      float key = keyIterator.next();
-      sb.append(key);
-      sb.append('=');
-      sb.append(get(key));
-      if (keyIterator.hasNext()) {
-        sb.append(',');
-        sb.append(' ');
-      }
-    }
-    sb.append('}');
-    return sb.toString();
-  }
-
-  @Override
-  public int hashCode() {
-    return getClass().hashCode() ^ size();
-  }
-
-  @SuppressWarnings("unchecked")
-  @Override
-  public boolean equals(Object o) {
-    FloatToObjectMap<T> that = (FloatToObjectMap<T>)o;
-    if (that.size() != this.size()) {
-      return false;
-    }
-
-    FloatIterator it = keyIterator();
-    while (it.hasNext()) {
-      float key = it.next();
-      if (!that.containsKey(key)) {
-        return false;
-      }
-
-      T v1 = this.get(key);
-      T v2 = that.get(key);
-      if ((v1 == null && v2 != null) ||
-          (v1 != null && v2 == null) ||
-          (!v1.equals(v2))) {
-        return false;
-      }
-    }
-    return true;
-  }
-}
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/util/collections/IntArray.java b/lucene/facet/src/java/org/apache/lucene/util/collections/IntArray.java
deleted file mode 100644
index d1d59ef..0000000
--- a/lucene/facet/src/java/org/apache/lucene/util/collections/IntArray.java
+++ /dev/null
@@ -1,252 +0,0 @@
-package org.apache.lucene.util.collections;
-
-import java.util.Arrays;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * A Class wrapper for a grow-able int[] which can be sorted and intersect with
- * other IntArrays.
- * 
- * @lucene.experimental
- */
-public class IntArray {
-
-  /**
-   * The int[] which holds the data
-   */
-  private int[] data;
-
-  /**
-   * Holds the number of items in the array.
-   */
-  private int size;
-
-  /**
-   * A flag which indicates whether a sort should occur of the array is
-   * already sorted.
-   */
-  private boolean shouldSort;
-
-  /**
-   * Construct a default IntArray, size 0 and surly a sort should not occur.
-   */
-  public IntArray() {
-    init(true);
-  }
-
-  private void init(boolean realloc) {
-    size = 0;
-    if (realloc) {
-      data = new int[0];
-    }
-    shouldSort = false;
-  }
-
-  /**
-   * Intersects the data with a given {@link IntHashSet}.
-   * 
-   * @param set
-   *            A given ArrayHashSetInt which holds the data to be intersected
-   *            against
-   */
-  public void intersect(IntHashSet set) {
-    int newSize = 0;
-    for (int i = 0; i < size; ++i) {
-      if (set.contains(data[i])) {
-        data[newSize] = data[i];
-        ++newSize;
-      }
-    }
-    this.size = newSize;
-  }
-
-  /**
-   * Intersects the data with a given IntArray
-   * 
-   * @param other
-   *            A given IntArray which holds the data to be intersected agains
-   */
-  public void intersect(IntArray other) {
-    sort();
-    other.sort();
-
-    int myIndex = 0;
-    int otherIndex = 0;
-    int newSize = 0;
-    if (this.size > other.size) {
-      while (otherIndex < other.size && myIndex < size) {
-        while (otherIndex < other.size
-            && other.data[otherIndex] < data[myIndex]) {
-          ++otherIndex;
-        }
-        if (otherIndex == other.size) {
-          break;
-        }
-        while (myIndex < size && other.data[otherIndex] > data[myIndex]) {
-          ++myIndex;
-        }
-        if (other.data[otherIndex] == data[myIndex]) {
-          data[newSize++] = data[myIndex];
-          ++otherIndex;
-          ++myIndex;
-        }
-      }
-    } else {
-      while (otherIndex < other.size && myIndex < size) {
-        while (myIndex < size && other.data[otherIndex] > data[myIndex]) {
-          ++myIndex;
-        }
-        if (myIndex == size) {
-          break;
-        }
-        while (otherIndex < other.size
-            && other.data[otherIndex] < data[myIndex]) {
-          ++otherIndex;
-        }
-        if (other.data[otherIndex] == data[myIndex]) {
-          data[newSize++] = data[myIndex];
-          ++otherIndex;
-          ++myIndex;
-        }
-      }
-    }
-    this.size = newSize;
-  }
-
-  /**
-   * Return the size of the Array. Not the allocated size, but the number of
-   * values actually set.
-   * 
-   * @return the (filled) size of the array
-   */
-  public int size() {
-    return size;
-  }
-
-  /**
-   * Adds a value to the array.
-   * 
-   * @param value
-   *            value to be added
-   */
-  public void addToArray(int value) {
-    if (size == data.length) {
-      int[] newArray = new int[2 * size + 1];
-      System.arraycopy(data, 0, newArray, 0, size);
-      data = newArray;
-    }
-    data[size] = value;
-    ++size;
-    shouldSort = true;
-  }
-
-  /**
-   * Equals method. Checking the sizes, than the values from the last index to
-   * the first (Statistically for random should be the same but for our
-   * specific use would find differences faster).
-   */
-  @Override
-  public boolean equals(Object o) {
-    if (!(o instanceof IntArray)) {
-      return false;
-    }
-
-    IntArray array = (IntArray) o;
-    if (array.size != size) {
-      return false;
-    }
-
-    sort();
-    array.sort();
-
-    boolean equal = true;
-
-    for (int i = size; i > 0 && equal;) {
-      --i;
-      equal = (array.data[i] == this.data[i]);
-    }
-
-    return equal;
-  }
-
-  /**
-   * Sorts the data. If it is needed.
-   */
-  public void sort() {
-    if (shouldSort) {
-      shouldSort = false;
-      Arrays.sort(data, 0, size);
-    }
-  }
-
-  /**
-   * Calculates a hash-code for HashTables
-   */
-  @Override
-  public int hashCode() {
-    int hash = 0;
-    for (int i = 0; i < size; ++i) {
-      hash = data[i] ^ (hash * 31);
-    }
-    return hash;
-  }
-
-  /**
-   * Get an element from a specific index.
-   * 
-   * @param i
-   *            index of which element should be retrieved.
-   */
-  public int get(int i) {
-    if (i >= size) {
-      throw new ArrayIndexOutOfBoundsException(i);
-    }
-    return this.data[i];
-  }
-
-  public void set(int idx, int value) {
-    if (idx >= size) {
-      throw new ArrayIndexOutOfBoundsException(idx);
-    }
-    this.data[idx] = value;
-  }
-
-  /**
-   * toString or not toString. That is the question!
-   */
-  @Override
-  public String toString() {
-    String s = "(" + size + ") ";
-    for (int i = 0; i < size; ++i) {
-      s += "" + data[i] + ", ";
-    }
-    return s;
-  }
-
-  /**
-   * Clear the IntArray (set all elements to zero).
-   * @param resize - if resize is true, then clear actually allocates
-   * a new array of size 0, essentially 'clearing' the array and freeing
-   * memory.
-   */
-  public void clear(boolean resize) {
-    init(resize);
-  }
-
-}
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/util/collections/IntHashSet.java b/lucene/facet/src/java/org/apache/lucene/util/collections/IntHashSet.java
deleted file mode 100644
index b9f383d..0000000
--- a/lucene/facet/src/java/org/apache/lucene/util/collections/IntHashSet.java
+++ /dev/null
@@ -1,548 +0,0 @@
-package org.apache.lucene.util.collections;
-
-import java.util.Arrays;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * A Set or primitive int. Implemented as a HashMap of int->int. *
- * 
- * @lucene.experimental
- */
-public class IntHashSet {
-  
-  // TODO (Facet): This is wasteful as the "values" are actually the "keys" and
-  // we could spare this amount of space (capacity * sizeof(int)). Perhaps even
-  // though it is not OOP, we should re-implement the hash for just that cause.
-
-  /**
-   * Implements an IntIterator which iterates over all the allocated indexes.
-   */
-  private final class IndexIterator implements IntIterator {
-    /**
-     * The last used baseHashIndex. Needed for "jumping" from one hash entry
-     * to another.
-     */
-    private int baseHashIndex = 0;
-
-    /**
-     * The next not-yet-visited index.
-     */
-    private int index = 0;
-
-    /**
-     * Index of the last visited pair. Used in {@link #remove()}.
-     */
-    private int lastIndex = 0;
-
-    /**
-     * Create the Iterator, make <code>index</code> point to the "first"
-     * index which is not empty. If such does not exist (eg. the map is
-     * empty) it would be zero.
-     */
-    public IndexIterator() {
-      for (baseHashIndex = 0; baseHashIndex < baseHash.length; ++baseHashIndex) {
-        index = baseHash[baseHashIndex];
-        if (index != 0) {
-          break;
-        }
-      }
-    }
-
-    @Override
-    public boolean hasNext() {
-      return (index != 0);
-    }
-
-    @Override
-    public int next() {
-      // Save the last index visited
-      lastIndex = index;
-
-      // next the index
-      index = next[index];
-
-      // if the next index points to the 'Ground' it means we're done with
-      // the current hash entry and we need to jump to the next one. This
-      // is done until all the hash entries had been visited.
-      while (index == 0 && ++baseHashIndex < baseHash.length) {
-        index = baseHash[baseHashIndex];
-      }
-
-      return lastIndex;
-    }
-
-    @Override
-    public void remove() {
-      IntHashSet.this.remove(keys[lastIndex]);
-    }
-
-  }
-
-  /**
-   * Implements an IntIterator, used for iteration over the map's keys.
-   */
-  private final class KeyIterator implements IntIterator {
-    private IntIterator iterator = new IndexIterator();
-
-    KeyIterator() { }
-    
-    @Override
-    public boolean hasNext() {
-      return iterator.hasNext();
-    }
-
-    @Override
-    public int next() {
-      return keys[iterator.next()];
-    }
-
-    @Override
-    public void remove() {
-      iterator.remove();
-    }
-  }
-
-  /**
-   * Default capacity - in case no capacity was specified in the constructor
-   */
-  private static int defaultCapacity = 16;
-
-  /**
-   * Holds the base hash entries. if the capacity is 2^N, than the base hash
-   * holds 2^(N+1). It can hold
-   */
-  int[] baseHash;
-
-  /**
-   * The current capacity of the map. Always 2^N and never less than 16. We
-   * never use the zero index. It is needed to improve performance and is also
-   * used as "ground".
-   */
-  private int capacity;
-  
-  /**
-   * All objects are being allocated at map creation. Those objects are "free"
-   * or empty. Whenever a new pair comes along, a pair is being "allocated" or
-   * taken from the free-linked list. as this is just a free list.
-   */
-  private int firstEmpty;
-
-  /**
-   * hashFactor is always (2^(N+1)) - 1. Used for faster hashing.
-   */
-  private int hashFactor;
-
-  /**
-   * This array holds the unique keys
-   */
-  int[] keys;
-
-  /**
-   * In case of collisions, we implement a double linked list of the colliding
-   * hash's with the following next[] and prev[]. Those are also used to store
-   * the "empty" list.
-   */
-  int[] next;
-
-  private int prev;
-
-  /**
-   * Number of currently objects in the map.
-   */
-  private int size;
-
-  /**
-   * Constructs a map with default capacity.
-   */
-  public IntHashSet() {
-    this(defaultCapacity);
-  }
-
-  /**
-   * Constructs a map with given capacity. Capacity is adjusted to a native
-   * power of 2, with minimum of 16.
-   * 
-   * @param capacity
-   *            minimum capacity for the map.
-   */
-  public IntHashSet(int capacity) {
-    this.capacity = 16;
-    // Minimum capacity is 16..
-    while (this.capacity < capacity) {
-      // Multiply by 2 as long as we're still under the requested capacity
-      this.capacity <<= 1;
-    }
-
-    // As mentioned, we use the first index (0) as 'Ground', so we need the
-    // length of the arrays to be one more than the capacity
-    int arrayLength = this.capacity + 1;
-
-    this.keys = new int[arrayLength];
-    this.next = new int[arrayLength];
-
-    // Hash entries are twice as big as the capacity.
-    int baseHashSize = this.capacity << 1;
-
-    this.baseHash = new int[baseHashSize];
-
-    // The has factor is 2^M - 1 which is used as an "AND" hashing operator.
-    // {@link #calcBaseHash()}
-    this.hashFactor = baseHashSize - 1;
-
-    this.size = 0;
-
-    clear();
-  }
-
-  /**
-   * Adds a pair to the map. Takes the first empty position from the
-   * empty-linked-list's head - {@link #firstEmpty}.
-   * 
-   * New pairs are always inserted to baseHash, and are followed by the old
-   * colliding pair.
-   * 
-   * @param key
-   *            integer which maps the given value
-   */
-  private void prvt_add(int key) {
-    // Hash entry to which the new pair would be inserted
-    int hashIndex = calcBaseHashIndex(key);
-
-    // 'Allocating' a pair from the "Empty" list.
-    int objectIndex = firstEmpty;
-
-    // Setting data
-    firstEmpty = next[firstEmpty];
-    keys[objectIndex] = key;
-
-    // Inserting the new pair as the first node in the specific hash entry
-    next[objectIndex] = baseHash[hashIndex];
-    baseHash[hashIndex] = objectIndex;
-
-    // Announcing a new pair was added!
-    ++size;
-  }
-
-  /**
-   * Calculating the baseHash index using the internal <code>hashFactor</code>
-   * .
-   */
-  protected int calcBaseHashIndex(int key) {
-    return key & hashFactor;
-  }
-
-  /**
-   * Empties the map. Generates the "Empty" space list for later allocation.
-   */
-  public void clear() {
-    // Clears the hash entries
-    Arrays.fill(this.baseHash, 0);
-
-    // Set size to zero
-    size = 0;
-
-    // Mark all array entries as empty. This is done with
-    // <code>firstEmpty</code> pointing to the first valid index (1 as 0 is
-    // used as 'Ground').
-    firstEmpty = 1;
-
-    // And setting all the <code>next[i]</code> to point at
-    // <code>i+1</code>.
-    for (int i = 1; i < this.capacity;) {
-      next[i] = ++i;
-    }
-
-    // Surly, the last one should point to the 'Ground'.
-    next[this.capacity] = 0;
-  }
-
-  /**
-   * Checks if a given key exists in the map.
-   * 
-   * @param value
-   *            that is checked against the map data.
-   * @return true if the key exists in the map. false otherwise.
-   */
-  public boolean contains(int value) {
-    return find(value) != 0;
-  }
-
-  /**
-   * Find the actual index of a given key.
-   * 
-   * @return index of the key. zero if the key wasn't found.
-   */
-  protected int find(int key) {
-    // Calculate the hash entry.
-    int baseHashIndex = calcBaseHashIndex(key);
-
-    // Start from the hash entry.
-    int localIndex = baseHash[baseHashIndex];
-
-    // while the index does not point to the 'Ground'
-    while (localIndex != 0) {
-      // returns the index found in case of of a matching key.
-      if (keys[localIndex] == key) {
-        return localIndex;
-      }
-
-      // next the local index
-      localIndex = next[localIndex];
-    }
-
-    // If we got this far, it could only mean we did not find the key we
-    // were asked for. return 'Ground' index.
-    return 0;
-  }
-
-  /**
-   * Find the actual index of a given key with it's baseHashIndex.<br>
-   * Some methods use the baseHashIndex. If those call {@link #find} there's
-   * no need to re-calculate that hash.
-   * 
-   * @return the index of the given key, or 0 as 'Ground' if the key wasn't
-   *         found.
-   */
-  private int findForRemove(int key, int baseHashIndex) {
-    // Start from the hash entry.
-    this.prev = 0;
-    int index = baseHash[baseHashIndex];
-
-    // while the index does not point to the 'Ground'
-    while (index != 0) {
-      // returns the index found in case of of a matching key.
-      if (keys[index] == key) {
-        return index;
-      }
-
-      // next the local index
-      prev = index;
-      index = next[index];
-    }
-
-    // If we got this far, it could only mean we did not find the key we
-    // were asked for. return 'Ground' index.
-    this.prev = 0;
-    return 0;
-  }
-
-  /**
-   * Grows the map. Allocates a new map of double the capacity, and
-   * fast-insert the old key-value pairs.
-   */
-  protected void grow() {
-    IntHashSet that = new IntHashSet(this.capacity * 2);
-
-    // Iterates fast over the collection. Any valid pair is put into the new
-    // map without checking for duplicates or if there's enough space for
-    // it.
-    for (IndexIterator iterator = new IndexIterator(); iterator.hasNext();) {
-      int index = iterator.next();
-      that.prvt_add(this.keys[index]);
-    }
-    // for (int i = capacity; i > 0; --i) {
-    //
-    // that._add(this.keys[i]);
-    //
-    // }
-
-    // Copy that's data into this.
-    this.capacity = that.capacity;
-    this.size = that.size;
-    this.firstEmpty = that.firstEmpty;
-    this.keys = that.keys;
-    this.next = that.next;
-    this.baseHash = that.baseHash;
-    this.hashFactor = that.hashFactor;
-  }
-
-  /**
-   * 
-   * @return true if the map is empty. false otherwise.
-   */
-  public boolean isEmpty() {
-    return size == 0;
-  }
-
-  /**
-   * Returns a new iterator for the mapped objects.
-   */
-  public IntIterator iterator() {
-    return new KeyIterator();
-  }
-
-  /**
-   * Prints the baseHash array, used for debug purposes.
-   */
-  @SuppressWarnings("unused")
-  private String getBaseHashAsString() {
-    return Arrays.toString(this.baseHash);
-  }
-
-  /**
-   * Add a mapping int key -> int value.
-   * <p>
-   * If the key was already inside just
-   * updating the value it refers to as the given object.
-   * <p>
-   * Otherwise if the map is full, first {@link #grow()} the map.
-   * 
-   * @param value
-   *            integer which maps the given value
-   * @return true always.
-   */
-  public boolean add(int value) {
-    // Does key exists?
-    int index = find(value);
-
-    // Yes!
-    if (index != 0) {
-      return true;
-    }
-
-    // Is there enough room for a new pair?
-    if (size == capacity) {
-      // No? Than grow up!
-      grow();
-    }
-
-    // Now that everything is set, the pair can be just put inside with no
-    // worries.
-    prvt_add(value);
-
-    return true;
-  }
-
-  /**
-   * Remove a pair from the map, specified by it's key.
-   * 
-   * @param value
-   *            specify the value to be removed
-   * 
-   * @return true if the map was changed (the key was found and removed).
-   *         false otherwise.
-   */
-  public boolean remove(int value) {
-    int baseHashIndex = calcBaseHashIndex(value);
-    int index = findForRemove(value, baseHashIndex);
-    if (index != 0) {
-      // If it is the first in the collision list, we should promote its
-      // next colliding element.
-      if (prev == 0) {
-        baseHash[baseHashIndex] = next[index];
-      }
-
-      next[prev] = next[index];
-      next[index] = firstEmpty;
-      firstEmpty = index;
-      --size;
-      return true;
-    }
-
-    return false;
-  }
-
-  /**
-   * @return number of pairs currently in the map
-   */
-  public int size() {
-    return this.size;
-  }
-
-  /**
-   * Translates the mapped pairs' values into an array of Objects
-   * 
-   * @return an object array of all the values currently in the map.
-   */
-  public int[] toArray() {
-    int j = -1;
-    int[] array = new int[size];
-
-    // Iterates over the values, adding them to the array.
-    for (IntIterator iterator = iterator(); iterator.hasNext();) {
-      array[++j] = iterator.next();
-    }
-    return array;
-  }
-
-  /**
-   * Translates the mapped pairs' values into an array of ints
-   * 
-   * @param a
-   *            the array into which the elements of the map are to be stored,
-   *            if it is big enough; otherwise, a new array of the same
-   *            runtime type is allocated for this purpose.
-   * 
-   * @return an array containing the values stored in the map
-   * 
-   */
-  public int[] toArray(int[] a) {
-    int j = 0;
-    if (a.length < size) {
-      a = new int[size];
-    }
-    // Iterates over the values, adding them to the array.
-    for (IntIterator iterator = iterator(); j < a.length
-        && iterator.hasNext(); ++j) {
-      a[j] = iterator.next();
-    }
-    return a;
-  }
-
-  /**
-   * I have no idea why would anyone call it - but for debug purposes.<br>
-   * Prints the entire map, including the index, key, object, next and prev.
-   */
-  @Override
-  public String toString() {
-    StringBuffer sb = new StringBuffer();
-    sb.append('{');
-    IntIterator iterator = iterator();
-    while (iterator.hasNext()) {
-      sb.append(iterator.next());
-      if (iterator.hasNext()) {
-        sb.append(',');
-        sb.append(' ');
-      }
-    }
-    sb.append('}');
-    return sb.toString();
-  }
-
-  public String toHashString() {
-    String string = "\n";
-    StringBuffer sb = new StringBuffer();
-
-    for (int i = 0; i < this.baseHash.length; i++) {
-      StringBuffer sb2 = new StringBuffer();
-      boolean shouldAppend = false;
-      sb2.append(i + ".\t");
-      for (int index = baseHash[i]; index != 0; index = next[index]) {
-        sb2.append(" -> " + keys[index] + "@" + index);
-        shouldAppend = true;
-      }
-      if (shouldAppend) {
-        sb.append(sb2);
-        sb.append(string);
-      }
-    }
-
-    return sb.toString();
-  }
-}
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/util/collections/IntIterator.java b/lucene/facet/src/java/org/apache/lucene/util/collections/IntIterator.java
deleted file mode 100644
index 9bd9ea3..0000000
--- a/lucene/facet/src/java/org/apache/lucene/util/collections/IntIterator.java
+++ /dev/null
@@ -1,31 +0,0 @@
-package org.apache.lucene.util.collections;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Iterator interface for primitive int iteration. *
- * 
- * @lucene.experimental
- */
-public interface IntIterator {
-
-  boolean hasNext();
-  int next();
-  void remove();
-
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/util/collections/IntToDoubleMap.java b/lucene/facet/src/java/org/apache/lucene/util/collections/IntToDoubleMap.java
deleted file mode 100644
index c7b4064..0000000
--- a/lucene/facet/src/java/org/apache/lucene/util/collections/IntToDoubleMap.java
+++ /dev/null
@@ -1,631 +0,0 @@
-package org.apache.lucene.util.collections;
-
-import java.util.Arrays;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * An Array-based hashtable which maps primitive int to a primitive double.<br>
- * The hashtable is constracted with a given capacity, or 16 as a default. In
- * case there's not enough room for new pairs, the hashtable grows. <br>
- * Capacity is adjusted to a power of 2, and there are 2 * capacity entries for
- * the hash.
- * 
- * The pre allocated arrays (for keys, values) are at length of capacity + 1,
- * when index 0 is used as 'Ground' or 'NULL'.<br>
- * 
- * The arrays are allocated ahead of hash operations, and form an 'empty space'
- * list, to which the key,value pair is allocated.
- * 
- * @lucene.experimental
- */
-public class IntToDoubleMap {
-
-  public static final double GROUND = Double.NaN;
-
-  /**
-   * Implements an IntIterator which iterates over all the allocated indexes.
-   */
-  private final class IndexIterator implements IntIterator {
-    /**
-     * The last used baseHashIndex. Needed for "jumping" from one hash entry
-     * to another.
-     */
-    private int baseHashIndex = 0;
-
-    /**
-     * The next not-yet-visited index.
-     */
-    private int index = 0;
-
-    /**
-     * Index of the last visited pair. Used in {@link #remove()}.
-     */
-    private int lastIndex = 0;
-
-    /**
-     * Create the Iterator, make <code>index</code> point to the "first"
-     * index which is not empty. If such does not exist (eg. the map is
-     * empty) it would be zero.
-     */
-    public IndexIterator() {
-      for (baseHashIndex = 0; baseHashIndex < baseHash.length; ++baseHashIndex) {
-        index = baseHash[baseHashIndex];
-        if (index != 0) {
-          break;
-        }
-      }
-    }
-
-    @Override
-    public boolean hasNext() {
-      return (index != 0);
-    }
-
-    @Override
-    public int next() {
-      // Save the last index visited
-      lastIndex = index;
-
-      // next the index
-      index = next[index];
-
-      // if the next index points to the 'Ground' it means we're done with
-      // the current hash entry and we need to jump to the next one. This
-      // is done until all the hash entries had been visited.
-      while (index == 0 && ++baseHashIndex < baseHash.length) {
-        index = baseHash[baseHashIndex];
-      }
-
-      return lastIndex;
-    }
-
-    @Override
-    public void remove() {
-      IntToDoubleMap.this.remove(keys[lastIndex]);
-    }
-
-  }
-
-  /**
-   * Implements an IntIterator, used for iteration over the map's keys.
-   */
-  private final class KeyIterator implements IntIterator {
-    private IntIterator iterator = new IndexIterator();
-
-    KeyIterator() { }
-    
-    @Override
-    public boolean hasNext() {
-      return iterator.hasNext();
-    }
-
-    @Override
-    public int next() {
-      return keys[iterator.next()];
-    }
-
-    @Override
-    public void remove() {
-      iterator.remove();
-    }
-  }
-
-  /**
-   * Implements an Iterator of a generic type T used for iteration over the
-   * map's values.
-   */
-  private final class ValueIterator implements DoubleIterator {
-    private IntIterator iterator = new IndexIterator();
-
-    ValueIterator() { }
-    
-    @Override
-    public boolean hasNext() {
-      return iterator.hasNext();
-    }
-
-    @Override
-    public double next() {
-      return values[iterator.next()];
-    }
-
-    @Override
-    public void remove() {
-      iterator.remove();
-    }
-  }
-
-  /**
-   * Default capacity - in case no capacity was specified in the constructor
-   */
-  private static int defaultCapacity = 16;
-
-  /**
-   * Holds the base hash entries. if the capacity is 2^N, than the base hash
-   * holds 2^(N+1). It can hold
-   */
-  int[] baseHash;
-
-  /**
-   * The current capacity of the map. Always 2^N and never less than 16. We
-   * never use the zero index. It is needed to improve performance and is also
-   * used as "ground".
-   */
-  private int capacity;
-  /**
-   * All objects are being allocated at map creation. Those objects are "free"
-   * or empty. Whenever a new pair comes along, a pair is being "allocated" or
-   * taken from the free-linked list. as this is just a free list.
-   */
-  private int firstEmpty;
-
-  /**
-   * hashFactor is always (2^(N+1)) - 1. Used for faster hashing.
-   */
-  private int hashFactor;
-
-  /**
-   * This array holds the unique keys
-   */
-  int[] keys;
-
-  /**
-   * In case of collisions, we implement a double linked list of the colliding
-   * hash's with the following next[] and prev[]. Those are also used to store
-   * the "empty" list.
-   */
-  int[] next;
-
-  private int prev;
-
-  /**
-   * Number of currently objects in the map.
-   */
-  private int size;
-
-  /**
-   * This array holds the values
-   */
-  double[] values;
-
-  /**
-   * Constructs a map with default capacity.
-   */
-  public IntToDoubleMap() {
-    this(defaultCapacity);
-  }
-
-  /**
-   * Constructs a map with given capacity. Capacity is adjusted to a native
-   * power of 2, with minimum of 16.
-   * 
-   * @param capacity
-   *            minimum capacity for the map.
-   */
-  public IntToDoubleMap(int capacity) {
-    this.capacity = 16;
-    // Minimum capacity is 16..
-    while (this.capacity < capacity) {
-      // Multiply by 2 as long as we're still under the requested capacity
-      this.capacity <<= 1;
-    }
-
-    // As mentioned, we use the first index (0) as 'Ground', so we need the
-    // length of the arrays to be one more than the capacity
-    int arrayLength = this.capacity + 1;
-
-    this.values = new double[arrayLength];
-    this.keys = new int[arrayLength];
-    this.next = new int[arrayLength];
-
-    // Hash entries are twice as big as the capacity.
-    int baseHashSize = this.capacity << 1;
-
-    this.baseHash = new int[baseHashSize];
-
-    this.values[0] = GROUND;
-
-    // The has factor is 2^M - 1 which is used as an "AND" hashing operator.
-    // {@link #calcBaseHash()}
-    this.hashFactor = baseHashSize - 1;
-
-    this.size = 0;
-
-    clear();
-  }
-
-  /**
-   * Adds a pair to the map. Takes the first empty position from the
-   * empty-linked-list's head - {@link #firstEmpty}.
-   * 
-   * New pairs are always inserted to baseHash, and are followed by the old
-   * colliding pair.
-   * 
-   * @param key
-   *            integer which maps the given Object
-   * @param v
-   *            double value which is being mapped using the given key
-   */
-  private void prvt_put(int key, double v) {
-    // Hash entry to which the new pair would be inserted
-    int hashIndex = calcBaseHashIndex(key);
-
-    // 'Allocating' a pair from the "Empty" list.
-    int objectIndex = firstEmpty;
-
-    // Setting data
-    firstEmpty = next[firstEmpty];
-    values[objectIndex] = v;
-    keys[objectIndex] = key;
-
-    // Inserting the new pair as the first node in the specific hash entry
-    next[objectIndex] = baseHash[hashIndex];
-    baseHash[hashIndex] = objectIndex;
-
-    // Announcing a new pair was added!
-    ++size;
-  }
-
-  /**
-   * Calculating the baseHash index using the internal <code>hashFactor</code>
-   * .
-   */
-  protected int calcBaseHashIndex(int key) {
-    return key & hashFactor;
-  }
-
-  /**
-   * Empties the map. Generates the "Empty" space list for later allocation.
-   */
-  public void clear() {
-    // Clears the hash entries
-    Arrays.fill(this.baseHash, 0);
-
-    // Set size to zero
-    size = 0;
-
-    // Mark all array entries as empty. This is done with
-    // <code>firstEmpty</code> pointing to the first valid index (1 as 0 is
-    // used as 'Ground').
-    firstEmpty = 1;
-
-    // And setting all the <code>next[i]</code> to point at
-    // <code>i+1</code>.
-    for (int i = 1; i < this.capacity;) {
-      next[i] = ++i;
-    }
-
-    // Surly, the last one should point to the 'Ground'.
-    next[this.capacity] = 0;
-  }
-
-  /**
-   * Checks if a given key exists in the map.
-   * 
-   * @param key
-   *            that is checked against the map data.
-   * @return true if the key exists in the map. false otherwise.
-   */
-  public boolean containsKey(int key) {
-    return find(key) != 0;
-  }
-
-  /**
-   * Checks if the given value exists in the map.<br>
-   * This method iterates over the collection, trying to find an equal object.
-   * 
-   * @param value
-   *            double value that is checked against the map data.
-   * @return true if the value exists in the map, false otherwise.
-   */
-  public boolean containsValue(double value) {
-    for (DoubleIterator iterator = iterator(); iterator.hasNext();) {
-      double d = iterator.next();
-      if (d == value) {
-        return true;
-      }
-    }
-    return false;
-  }
-
-  /**
-   * Find the actual index of a given key.
-   * 
-   * @return index of the key. zero if the key wasn't found.
-   */
-  protected int find(int key) {
-    // Calculate the hash entry.
-    int baseHashIndex = calcBaseHashIndex(key);
-
-    // Start from the hash entry.
-    int localIndex = baseHash[baseHashIndex];
-
-    // while the index does not point to the 'Ground'
-    while (localIndex != 0) {
-      // returns the index found in case of of a matching key.
-      if (keys[localIndex] == key) {
-        return localIndex;
-      }
-        
-      // next the local index
-      localIndex = next[localIndex];
-    }
-
-    // If we got this far, it could only mean we did not find the key we
-    // were asked for. return 'Ground' index.
-    return 0;
-  }
-
-  /**
-   * Find the actual index of a given key with it's baseHashIndex.<br>
-   * Some methods use the baseHashIndex. If those call {@link #find} there's
-   * no need to re-calculate that hash.
-   * 
-   * @return the index of the given key, or 0 as 'Ground' if the key wasn't
-   *         found.
-   */
-  private int findForRemove(int key, int baseHashIndex) {
-    // Start from the hash entry.
-    this.prev = 0;
-    int index = baseHash[baseHashIndex];
-
-    // while the index does not point to the 'Ground'
-    while (index != 0) {
-      // returns the index found in case of of a matching key.
-      if (keys[index] == key) {
-        return index;
-      }
-
-      // next the local index
-      prev = index;
-      index = next[index];
-    }
-
-    // If we got this far, it could only mean we did not find the key we
-    // were asked for. return 'Ground' index.
-    this.prev = 0;
-    return 0;
-  }
-
-  /**
-   * Returns the value mapped with the given key.
-   * 
-   * @param key
-   *            int who's mapped object we're interested in.
-   * @return a double value mapped by the given key. Double.NaN if the key wasn't found.
-   */
-  public double get(int key) {
-    return values[find(key)];
-  }
-
-  /**
-   * Grows the map. Allocates a new map of double the capacity, and
-   * fast-insert the old key-value pairs.
-   */
-  protected void grow() {
-    IntToDoubleMap that = new IntToDoubleMap(
-        this.capacity * 2);
-
-    // Iterates fast over the collection. Any valid pair is put into the new
-    // map without checking for duplicates or if there's enough space for
-    // it.
-    for (IndexIterator iterator = new IndexIterator(); iterator.hasNext();) {
-      int index = iterator.next();
-      that.prvt_put(this.keys[index], this.values[index]);
-    }
-
-    // Copy that's data into this.
-    this.capacity = that.capacity;
-    this.size = that.size;
-    this.firstEmpty = that.firstEmpty;
-    this.values = that.values;
-    this.keys = that.keys;
-    this.next = that.next;
-    this.baseHash = that.baseHash;
-    this.hashFactor = that.hashFactor;
-  }
-
-  /**
-   * 
-   * @return true if the map is empty. false otherwise.
-   */
-  public boolean isEmpty() {
-    return size == 0;
-  }
-
-  /**
-   * Returns a new iterator for the mapped double values.
-   */
-  public DoubleIterator iterator() {
-    return new ValueIterator();
-  }
-
-  /** Returns an iterator on the map keys. */
-  public IntIterator keyIterator() {
-    return new KeyIterator();
-  }
-
-  /**
-   * Prints the baseHash array, used for debug purposes.
-   */
-  @SuppressWarnings("unused")
-  private String getBaseHashAsString() {
-    return Arrays.toString(this.baseHash);
-  }
-
-  /**
-   * Inserts the &lt;key,value&gt; pair into the map. If the key already exists,
-   * this method updates the mapped value to the given one, returning the old
-   * mapped value.
-   * 
-   * @return the old mapped value, or {@link Double#NaN} if the key didn't exist.
-   */
-  public double put(int key, double v) {
-    // Does key exists?
-    int index = find(key);
-
-    // Yes!
-    if (index != 0) {
-      // Set new data and exit.
-      double old = values[index];
-      values[index] = v;
-      return old;
-    }
-
-    // Is there enough room for a new pair?
-    if (size == capacity) {
-      // No? Than grow up!
-      grow();
-    }
-
-    // Now that everything is set, the pair can be just put inside with no
-    // worries.
-    prvt_put(key, v);
-
-    return Double.NaN;
-  }
-
-  /**
-   * Removes a &lt;key,value&gt; pair from the map and returns the mapped value,
-   * or {@link Double#NaN} if the none existed.
-   * 
-   * @param key used to find the value to remove
-   * @return the removed value or {@link Double#NaN} if none existed.
-   */
-  public double remove(int key) {
-    int baseHashIndex = calcBaseHashIndex(key);
-    int index = findForRemove(key, baseHashIndex);
-    if (index != 0) {
-      // If it is the first in the collision list, we should promote its
-      // next colliding element.
-      if (prev == 0) {
-        baseHash[baseHashIndex] = next[index];
-      }
-
-      next[prev] = next[index];
-      next[index] = firstEmpty;
-      firstEmpty = index;
-      --size;
-      return values[index];
-    }
-
-    return Double.NaN;
-  }
-
-  /**
-   * @return number of pairs currently in the map
-   */
-  public int size() {
-    return this.size;
-  }
-
-  /**
-   * Translates the mapped pairs' values into an array of Objects
-   * 
-   * @return a double array of all the values currently in the map.
-   */
-  public double[] toArray() {
-    int j = -1;
-    double[] array = new double[size];
-
-    // Iterates over the values, adding them to the array.
-    for (DoubleIterator iterator = iterator(); iterator.hasNext();) {
-      array[++j] = iterator.next();
-    }
-    return array;
-  }
-
-  /**
-   * Translates the mapped pairs' values into an array of T
-   * 
-   * @param a
-   *            the array into which the elements of the list are to be
-   *            stored. If it is big enough use whatever space we need,
-   *            setting the one after the true data as {@link Double#NaN}.
-   * 
-   * @return an array containing the elements of the list, using the given
-   *         parameter if big enough, otherwise allocate an appropriate array
-   *         and return it.
-   * 
-   */
-  public double[] toArray(double[] a) {
-    int j = 0;
-    if (a.length < this.size()) {
-      a = new double[this.size()];
-    }
-
-    // Iterates over the values, adding them to the array.
-    for (DoubleIterator iterator = iterator(); iterator.hasNext(); ++j) {
-      a[j] = iterator.next();
-    }
-
-    if (j < a.length) {
-      a[j] = Double.NaN;
-    }
-
-    return a;
-  }
-
-  @Override
-  public String toString() {
-    StringBuffer sb = new StringBuffer();
-    sb.append('{');
-    IntIterator keyIterator = keyIterator();
-    while (keyIterator.hasNext()) {
-      int key = keyIterator.next();
-      sb.append(key);
-      sb.append('=');
-      sb.append(get(key));
-      if (keyIterator.hasNext()) {
-        sb.append(',');
-        sb.append(' ');
-      }
-    }
-    sb.append('}');
-    return sb.toString();
-  }
-  
-  @Override
-  public int hashCode() {
-    return getClass().hashCode() ^ size();
-  }
-  
-  @Override
-  public boolean equals(Object o) {
-    IntToDoubleMap that = (IntToDoubleMap)o;
-    if (that.size() != this.size()) {
-      return false;
-    }
-    
-    IntIterator it = keyIterator();
-    while (it.hasNext()) {
-      int key = it.next();
-      if (!that.containsKey(key)) {
-        return false;
-      }
-
-      double v1 = this.get(key);
-      double v2 = that.get(key);
-      if (Double.compare(v1, v2) != 0) {
-        return false;
-      }
-    }
-    return true;
-  }
-}
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/util/collections/IntToFloatMap.java b/lucene/facet/src/java/org/apache/lucene/util/collections/IntToFloatMap.java
deleted file mode 100644
index 1f257bc..0000000
--- a/lucene/facet/src/java/org/apache/lucene/util/collections/IntToFloatMap.java
+++ /dev/null
@@ -1,631 +0,0 @@
-package org.apache.lucene.util.collections;
-
-import java.util.Arrays;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * An Array-based hashtable which maps primitive int to a primitive float.<br>
- * The hashtable is constracted with a given capacity, or 16 as a default. In
- * case there's not enough room for new pairs, the hashtable grows. <br>
- * Capacity is adjusted to a power of 2, and there are 2 * capacity entries for
- * the hash.
- * 
- * The pre allocated arrays (for keys, values) are at length of capacity + 1,
- * when index 0 is used as 'Ground' or 'NULL'.<br>
- * 
- * The arrays are allocated ahead of hash operations, and form an 'empty space'
- * list, to which the key,value pair is allocated.
- * 
- * @lucene.experimental
- */
-public class IntToFloatMap {
-
-  public static final float GROUND = Float.NaN;
-
-  /**
-   * Implements an IntIterator which iterates over all the allocated indexes.
-   */
-  private final class IndexIterator implements IntIterator {
-    /**
-     * The last used baseHashIndex. Needed for "jumping" from one hash entry
-     * to another.
-     */
-    private int baseHashIndex = 0;
-
-    /**
-     * The next not-yet-visited index.
-     */
-    private int index = 0;
-
-    /**
-     * Index of the last visited pair. Used in {@link #remove()}.
-     */
-    private int lastIndex = 0;
-
-    /**
-     * Create the Iterator, make <code>index</code> point to the "first"
-     * index which is not empty. If such does not exist (eg. the map is
-     * empty) it would be zero.
-     */
-    public IndexIterator() {
-      for (baseHashIndex = 0; baseHashIndex < baseHash.length; ++baseHashIndex) {
-        index = baseHash[baseHashIndex];
-        if (index != 0) {
-          break;
-        }
-      }
-    }
-
-    @Override
-    public boolean hasNext() {
-      return (index != 0);
-    }
-
-    @Override
-    public int next() {
-      // Save the last index visited
-      lastIndex = index;
-
-      // next the index
-      index = next[index];
-
-      // if the next index points to the 'Ground' it means we're done with
-      // the current hash entry and we need to jump to the next one. This
-      // is done until all the hash entries had been visited.
-      while (index == 0 && ++baseHashIndex < baseHash.length) {
-        index = baseHash[baseHashIndex];
-      }
-
-      return lastIndex;
-    }
-
-    @Override
-    public void remove() {
-      IntToFloatMap.this.remove(keys[lastIndex]);
-    }
-
-  }
-
-  /**
-   * Implements an IntIterator, used for iteration over the map's keys.
-   */
-  private final class KeyIterator implements IntIterator {
-    private IntIterator iterator = new IndexIterator();
-
-    KeyIterator() { }
-    
-    @Override
-    public boolean hasNext() {
-      return iterator.hasNext();
-    }
-
-    @Override
-    public int next() {
-      return keys[iterator.next()];
-    }
-
-    @Override
-    public void remove() {
-      iterator.remove();
-    }
-  }
-
-  /**
-   * Implements an Iterator of a generic type T used for iteration over the
-   * map's values.
-   */
-  private final class ValueIterator implements FloatIterator {
-    private IntIterator iterator = new IndexIterator();
-
-    ValueIterator() { }
-    
-    @Override
-    public boolean hasNext() {
-      return iterator.hasNext();
-    }
-
-    @Override
-    public float next() {
-      return values[iterator.next()];
-    }
-
-    @Override
-    public void remove() {
-      iterator.remove();
-    }
-  }
-
-  /**
-   * Default capacity - in case no capacity was specified in the constructor
-   */
-  private static int defaultCapacity = 16;
-
-  /**
-   * Holds the base hash entries. if the capacity is 2^N, than the base hash
-   * holds 2^(N+1). It can hold
-   */
-  int[] baseHash;
-
-  /**
-   * The current capacity of the map. Always 2^N and never less than 16. We
-   * never use the zero index. It is needed to improve performance and is also
-   * used as "ground".
-   */
-  private int capacity;
-  /**
-   * All objects are being allocated at map creation. Those objects are "free"
-   * or empty. Whenever a new pair comes along, a pair is being "allocated" or
-   * taken from the free-linked list. as this is just a free list.
-   */
-  private int firstEmpty;
-
-  /**
-   * hashFactor is always (2^(N+1)) - 1. Used for faster hashing.
-   */
-  private int hashFactor;
-
-  /**
-   * This array holds the unique keys
-   */
-  int[] keys;
-
-  /**
-   * In case of collisions, we implement a float linked list of the colliding
-   * hash's with the following next[] and prev[]. Those are also used to store
-   * the "empty" list.
-   */
-  int[] next;
-
-  private int prev;
-
-  /**
-   * Number of currently objects in the map.
-   */
-  private int size;
-
-  /**
-   * This array holds the values
-   */
-  float[] values;
-
-  /**
-   * Constructs a map with default capacity.
-   */
-  public IntToFloatMap() {
-    this(defaultCapacity);
-  }
-
-  /**
-   * Constructs a map with given capacity. Capacity is adjusted to a native
-   * power of 2, with minimum of 16.
-   * 
-   * @param capacity
-   *            minimum capacity for the map.
-   */
-  public IntToFloatMap(int capacity) {
-    this.capacity = 16;
-    // Minimum capacity is 16..
-    while (this.capacity < capacity) {
-      // Multiply by 2 as long as we're still under the requested capacity
-      this.capacity <<= 1;
-    }
-
-    // As mentioned, we use the first index (0) as 'Ground', so we need the
-    // length of the arrays to be one more than the capacity
-    int arrayLength = this.capacity + 1;
-
-    this.values = new float[arrayLength];
-    this.keys = new int[arrayLength];
-    this.next = new int[arrayLength];
-
-    // Hash entries are twice as big as the capacity.
-    int baseHashSize = this.capacity << 1;
-
-    this.baseHash = new int[baseHashSize];
-
-    this.values[0] = GROUND;
-
-    // The has factor is 2^M - 1 which is used as an "AND" hashing operator.
-    // {@link #calcBaseHash()}
-    this.hashFactor = baseHashSize - 1;
-
-    this.size = 0;
-
-    clear();
-  }
-
-  /**
-   * Adds a pair to the map. Takes the first empty position from the
-   * empty-linked-list's head - {@link #firstEmpty}.
-   * 
-   * New pairs are always inserted to baseHash, and are followed by the old
-   * colliding pair.
-   * 
-   * @param key
-   *            integer which maps the given Object
-   * @param v
-   *            float value which is being mapped using the given key
-   */
-  private void prvt_put(int key, float v) {
-    // Hash entry to which the new pair would be inserted
-    int hashIndex = calcBaseHashIndex(key);
-
-    // 'Allocating' a pair from the "Empty" list.
-    int objectIndex = firstEmpty;
-
-    // Setting data
-    firstEmpty = next[firstEmpty];
-    values[objectIndex] = v;
-    keys[objectIndex] = key;
-
-    // Inserting the new pair as the first node in the specific hash entry
-    next[objectIndex] = baseHash[hashIndex];
-    baseHash[hashIndex] = objectIndex;
-
-    // Announcing a new pair was added!
-    ++size;
-  }
-
-  /**
-   * Calculating the baseHash index using the internal <code>hashFactor</code>
-   * .
-   */
-  protected int calcBaseHashIndex(int key) {
-    return key & hashFactor;
-  }
-
-  /**
-   * Empties the map. Generates the "Empty" space list for later allocation.
-   */
-  public void clear() {
-    // Clears the hash entries
-    Arrays.fill(this.baseHash, 0);
-
-    // Set size to zero
-    size = 0;
-
-    // Mark all array entries as empty. This is done with
-    // <code>firstEmpty</code> pointing to the first valid index (1 as 0 is
-    // used as 'Ground').
-    firstEmpty = 1;
-
-    // And setting all the <code>next[i]</code> to point at
-    // <code>i+1</code>.
-    for (int i = 1; i < this.capacity;) {
-      next[i] = ++i;
-    }
-
-    // Surly, the last one should point to the 'Ground'.
-    next[this.capacity] = 0;
-  }
-
-  /**
-   * Checks if a given key exists in the map.
-   * 
-   * @param key
-   *            that is checked against the map data.
-   * @return true if the key exists in the map. false otherwise.
-   */
-  public boolean containsKey(int key) {
-    return find(key) != 0;
-  }
-
-  /**
-   * Checks if the given value exists in the map.<br>
-   * This method iterates over the collection, trying to find an equal object.
-   * 
-   * @param value
-   *            float value that is checked against the map data.
-   * @return true if the value exists in the map, false otherwise.
-   */
-  public boolean containsValue(float value) {
-    for (FloatIterator iterator = iterator(); iterator.hasNext();) {
-      float d = iterator.next();
-      if (d == value) {
-        return true;
-      }
-    }
-    return false;
-  }
-
-  /**
-   * Find the actual index of a given key.
-   * 
-   * @return index of the key. zero if the key wasn't found.
-   */
-  protected int find(int key) {
-    // Calculate the hash entry.
-    int baseHashIndex = calcBaseHashIndex(key);
-
-    // Start from the hash entry.
-    int localIndex = baseHash[baseHashIndex];
-
-    // while the index does not point to the 'Ground'
-    while (localIndex != 0) {
-      // returns the index found in case of of a matching key.
-      if (keys[localIndex] == key) {
-        return localIndex;
-      }
-        
-      // next the local index
-      localIndex = next[localIndex];
-    }
-
-    // If we got this far, it could only mean we did not find the key we
-    // were asked for. return 'Ground' index.
-    return 0;
-  }
-
-  /**
-   * Find the actual index of a given key with it's baseHashIndex.<br>
-   * Some methods use the baseHashIndex. If those call {@link #find} there's
-   * no need to re-calculate that hash.
-   * 
-   * @return the index of the given key, or 0 as 'Ground' if the key wasn't
-   *         found.
-   */
-  private int findForRemove(int key, int baseHashIndex) {
-    // Start from the hash entry.
-    this.prev = 0;
-    int index = baseHash[baseHashIndex];
-
-    // while the index does not point to the 'Ground'
-    while (index != 0) {
-      // returns the index found in case of of a matching key.
-      if (keys[index] == key) {
-        return index;
-      }
-
-      // next the local index
-      prev = index;
-      index = next[index];
-    }
-
-    // If we got this far, it could only mean we did not find the key we
-    // were asked for. return 'Ground' index.
-    this.prev = 0;
-    return 0;
-  }
-
-  /**
-   * Returns the value mapped with the given key.
-   * 
-   * @param key
-   *            int who's mapped object we're interested in.
-   * @return a float value mapped by the given key. float.NaN if the key wasn't found.
-   */
-  public float get(int key) {
-    return values[find(key)];
-  }
-
-  /**
-   * Grows the map. Allocates a new map of float the capacity, and
-   * fast-insert the old key-value pairs.
-   */
-  protected void grow() {
-    IntToFloatMap that = new IntToFloatMap(
-        this.capacity * 2);
-
-    // Iterates fast over the collection. Any valid pair is put into the new
-    // map without checking for duplicates or if there's enough space for
-    // it.
-    for (IndexIterator iterator = new IndexIterator(); iterator.hasNext();) {
-      int index = iterator.next();
-      that.prvt_put(this.keys[index], this.values[index]);
-    }
-
-    // Copy that's data into this.
-    this.capacity = that.capacity;
-    this.size = that.size;
-    this.firstEmpty = that.firstEmpty;
-    this.values = that.values;
-    this.keys = that.keys;
-    this.next = that.next;
-    this.baseHash = that.baseHash;
-    this.hashFactor = that.hashFactor;
-  }
-
-  /**
-   * 
-   * @return true if the map is empty. false otherwise.
-   */
-  public boolean isEmpty() {
-    return size == 0;
-  }
-
-  /**
-   * Returns a new iterator for the mapped float values.
-   */
-  public FloatIterator iterator() {
-    return new ValueIterator();
-  }
-
-  /** Returns an iterator on the map keys. */
-  public IntIterator keyIterator() {
-    return new KeyIterator();
-  }
-
-  /**
-   * Prints the baseHash array, used for debug purposes.
-   */
-  @SuppressWarnings("unused")
-  private String getBaseHashAsString() {
-    return Arrays.toString(this.baseHash);
-  }
-
-  /**
-   * Inserts the &lt;key,value&gt; pair into the map. If the key already exists,
-   * this method updates the mapped value to the given one, returning the old
-   * mapped value.
-   * 
-   * @return the old mapped value, or {@link Float#NaN} if the key didn't exist.
-   */
-  public float put(int key, float v) {
-    // Does key exists?
-    int index = find(key);
-
-    // Yes!
-    if (index != 0) {
-      // Set new data and exit.
-      float old = values[index];
-      values[index] = v;
-      return old;
-    }
-
-    // Is there enough room for a new pair?
-    if (size == capacity) {
-      // No? Than grow up!
-      grow();
-    }
-
-    // Now that everything is set, the pair can be just put inside with no
-    // worries.
-    prvt_put(key, v);
-
-    return Float.NaN;
-  }
-
-  /**
-   * Removes a &lt;key,value&gt; pair from the map and returns the mapped value,
-   * or {@link Float#NaN} if the none existed.
-   * 
-   * @param key used to find the value to remove
-   * @return the removed value or {@link Float#NaN} if none existed.
-   */
-  public float remove(int key) {
-    int baseHashIndex = calcBaseHashIndex(key);
-    int index = findForRemove(key, baseHashIndex);
-    if (index != 0) {
-      // If it is the first in the collision list, we should promote its
-      // next colliding element.
-      if (prev == 0) {
-        baseHash[baseHashIndex] = next[index];
-      }
-
-      next[prev] = next[index];
-      next[index] = firstEmpty;
-      firstEmpty = index;
-      --size;
-      return values[index];
-    }
-
-    return Float.NaN;
-  }
-
-  /**
-   * @return number of pairs currently in the map
-   */
-  public int size() {
-    return this.size;
-  }
-
-  /**
-   * Translates the mapped pairs' values into an array of Objects
-   * 
-   * @return a float array of all the values currently in the map.
-   */
-  public float[] toArray() {
-    int j = -1;
-    float[] array = new float[size];
-
-    // Iterates over the values, adding them to the array.
-    for (FloatIterator iterator = iterator(); iterator.hasNext();) {
-      array[++j] = iterator.next();
-    }
-    return array;
-  }
-
-  /**
-   * Translates the mapped pairs' values into an array of T
-   * 
-   * @param a
-   *            the array into which the elements of the list are to be
-   *            stored. If it is big enough use whatever space we need,
-   *            setting the one after the true data as {@link Float#NaN}.
-   * 
-   * @return an array containing the elements of the list, using the given
-   *         parameter if big enough, otherwise allocate an appropriate array
-   *         and return it.
-   * 
-   */
-  public float[] toArray(float[] a) {
-    int j = 0;
-    if (a.length < this.size()) {
-      a = new float[this.size()];
-    }
-
-    // Iterates over the values, adding them to the array.
-    for (FloatIterator iterator = iterator(); iterator.hasNext(); ++j) {
-      a[j] = iterator.next();
-    }
-
-    if (j < a.length) {
-      a[j] = Float.NaN;
-    }
-
-    return a;
-  }
-
-  @Override
-  public String toString() {
-    StringBuffer sb = new StringBuffer();
-    sb.append('{');
-    IntIterator keyIterator = keyIterator();
-    while (keyIterator.hasNext()) {
-      int key = keyIterator.next();
-      sb.append(key);
-      sb.append('=');
-      sb.append(get(key));
-      if (keyIterator.hasNext()) {
-        sb.append(',');
-        sb.append(' ');
-      }
-    }
-    sb.append('}');
-    return sb.toString();
-  }
-  
-  @Override
-  public int hashCode() {
-    return getClass().hashCode() ^ size();
-  }
-  
-  @Override
-  public boolean equals(Object o) {
-    IntToFloatMap that = (IntToFloatMap)o;
-    if (that.size() != this.size()) {
-      return false;
-    }
-    
-    IntIterator it = keyIterator();
-    while (it.hasNext()) {
-      int key = it.next();
-      if (!that.containsKey(key)) {
-        return false;
-      }
-
-      float v1 = this.get(key);
-      float v2 = that.get(key);
-      if (Float.compare(v1, v2) != 0) {
-        return false;
-      }
-    }
-    return true;
-  }
-}
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/util/collections/IntToIntMap.java b/lucene/facet/src/java/org/apache/lucene/util/collections/IntToIntMap.java
deleted file mode 100644
index 76155a5..0000000
--- a/lucene/facet/src/java/org/apache/lucene/util/collections/IntToIntMap.java
+++ /dev/null
@@ -1,622 +0,0 @@
-package org.apache.lucene.util.collections;
-
-import java.util.Arrays;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * An Array-based hashtable which maps primitive int to primitive int.<br>
- * The hashtable is constracted with a given capacity, or 16 as a default. In
- * case there's not enough room for new pairs, the hashtable grows. <br>
- * Capacity is adjusted to a power of 2, and there are 2 * capacity entries for
- * the hash.
- * 
- * The pre allocated arrays (for keys, values) are at length of capacity + 1,
- * when index 0 is used as 'Ground' or 'NULL'.<br>
- * 
- * The arrays are allocated ahead of hash operations, and form an 'empty space'
- * list, to which the key,value pair is allocated.
- * 
- * @lucene.experimental
- */
-public class IntToIntMap {
-
-  public static final int GROUD = -1;
-  /**
-   * Implements an IntIterator which iterates over all the allocated indexes.
-   */
-  private final class IndexIterator implements IntIterator {
-    /**
-     * The last used baseHashIndex. Needed for "jumping" from one hash entry
-     * to another.
-     */
-    private int baseHashIndex = 0;
-
-    /**
-     * The next not-yet-visited index.
-     */
-    private int index = 0;
-
-    /**
-     * Index of the last visited pair. Used in {@link #remove()}.
-     */
-    private int lastIndex = 0;
-
-    /**
-     * Create the Iterator, make <code>index</code> point to the "first"
-     * index which is not empty. If such does not exist (eg. the map is
-     * empty) it would be zero.
-     */
-    public IndexIterator() {
-      for (baseHashIndex = 0; baseHashIndex < baseHash.length; ++baseHashIndex) {
-        index = baseHash[baseHashIndex];
-        if (index != 0) {
-          break;
-        }
-      }
-    }
-
-    @Override
-    public boolean hasNext() {
-      return (index != 0);
-    }
-
-    @Override
-    public int next() {
-      // Save the last index visited
-      lastIndex = index;
-
-      // next the index
-      index = next[index];
-
-      // if the next index points to the 'Ground' it means we're done with
-      // the current hash entry and we need to jump to the next one. This
-      // is done until all the hash entries had been visited.
-      while (index == 0 && ++baseHashIndex < baseHash.length) {
-        index = baseHash[baseHashIndex];
-      }
-
-      return lastIndex;
-    }
-
-    @Override
-    public void remove() {
-      IntToIntMap.this.remove(keys[lastIndex]);
-    }
-
-  }
-
-  /**
-   * Implements an IntIterator, used for iteration over the map's keys.
-   */
-  private final class KeyIterator implements IntIterator {
-    private IntIterator iterator = new IndexIterator();
-
-    KeyIterator() { }
-    
-    @Override
-    public boolean hasNext() {
-      return iterator.hasNext();
-    }
-
-    @Override
-    public int next() {
-      return keys[iterator.next()];
-    }
-
-    @Override
-    public void remove() {
-      iterator.remove();
-    }
-  }
-
-  /**
-   * Implements an IntIterator used for iteration over the map's values.
-   */
-  private final class ValueIterator implements IntIterator {
-    private IntIterator iterator = new IndexIterator();
-
-    ValueIterator() { }
-    
-    @Override
-    public boolean hasNext() {
-      return iterator.hasNext();
-    }
-
-    @Override
-    public int next() {
-      return values[iterator.next()];
-    }
-
-    @Override
-    public void remove() {
-      iterator.remove();
-    }
-  }
-
-  /**
-   * Default capacity - in case no capacity was specified in the constructor
-   */
-  private static int defaultCapacity = 16;
-
-  /**
-   * Holds the base hash entries. if the capacity is 2^N, than the base hash
-   * holds 2^(N+1). It can hold
-   */
-  int[] baseHash;
-
-  /**
-   * The current capacity of the map. Always 2^N and never less than 16. We
-   * never use the zero index. It is needed to improve performance and is also
-   * used as "ground".
-   */
-  private int capacity;
-  /**
-   * All objects are being allocated at map creation. Those objects are "free"
-   * or empty. Whenever a new pair comes along, a pair is being "allocated" or
-   * taken from the free-linked list. as this is just a free list.
-   */
-  private int firstEmpty;
-
-  /**
-   * hashFactor is always (2^(N+1)) - 1. Used for faster hashing.
-   */
-  private int hashFactor;
-
-  /**
-   * This array holds the unique keys
-   */
-  int[] keys;
-
-  /**
-   * In case of collisions, we implement a double linked list of the colliding
-   * hash's with the following next[] and prev[]. Those are also used to store
-   * the "empty" list.
-   */
-  int[] next;
-
-  private int prev;
-
-  /**
-   * Number of currently objects in the map.
-   */
-  private int size;
-
-  /**
-   * This array holds the values
-   */
-  int[] values;
-
-  /**
-   * Constructs a map with default capacity.
-   */
-  public IntToIntMap() {
-    this(defaultCapacity);
-  }
-
-  /**
-   * Constructs a map with given capacity. Capacity is adjusted to a native
-   * power of 2, with minimum of 16.
-   * 
-   * @param capacity
-   *            minimum capacity for the map.
-   */
-  public IntToIntMap(int capacity) {
-    this.capacity = 16;
-    // Minimum capacity is 16..
-    while (this.capacity < capacity) {
-      // Multiply by 2 as long as we're still under the requested capacity
-      this.capacity <<= 1;
-    }
-
-    // As mentioned, we use the first index (0) as 'Ground', so we need the
-    // length of the arrays to be one more than the capacity
-    int arrayLength = this.capacity + 1;
-
-    this.values = new int[arrayLength];
-    this.keys = new int[arrayLength];
-    this.next = new int[arrayLength];
-
-    this.values[0] = GROUD;
-
-    // Hash entries are twice as big as the capacity.
-    int baseHashSize = this.capacity << 1;
-
-    this.baseHash = new int[baseHashSize];
-
-    // The has factor is 2^M - 1 which is used as an "AND" hashing operator.
-    // {@link #calcBaseHash()}
-    this.hashFactor = baseHashSize - 1;
-
-    this.size = 0;
-
-    clear();
-  }
-
-  /**
-   * Adds a pair to the map. Takes the first empty position from the
-   * empty-linked-list's head - {@link #firstEmpty}.
-   * 
-   * New pairs are always inserted to baseHash, and are followed by the old
-   * colliding pair.
-   * 
-   * @param key
-   *            integer which maps the given value
-   * @param e
-   *            value which is being mapped using the given key
-   */
-  private void prvt_put(int key, int e) {
-    // Hash entry to which the new pair would be inserted
-    int hashIndex = calcBaseHashIndex(key);
-
-    // 'Allocating' a pair from the "Empty" list.
-    int objectIndex = firstEmpty;
-
-    // Setting data
-    firstEmpty = next[firstEmpty];
-    values[objectIndex] = e;
-    keys[objectIndex] = key;
-
-    // Inserting the new pair as the first node in the specific hash entry
-    next[objectIndex] = baseHash[hashIndex];
-    baseHash[hashIndex] = objectIndex;
-
-    // Announcing a new pair was added!
-    ++size;
-  }
-
-  /**
-   * Calculating the baseHash index using the internal <code>hashFactor</code>.
-   */
-  protected int calcBaseHashIndex(int key) {
-    return key & hashFactor;
-  }
-
-  /**
-   * Empties the map. Generates the "Empty" space list for later allocation.
-   */
-  public void clear() {
-    // Clears the hash entries
-    Arrays.fill(this.baseHash, 0);
-
-    // Set size to zero
-    size = 0;
-
-    // Mark all array entries as empty. This is done with
-    // <code>firstEmpty</code> pointing to the first valid index (1 as 0 is
-    // used as 'Ground').
-    firstEmpty = 1;
-
-    // And setting all the <code>next[i]</code> to point at
-    // <code>i+1</code>.
-    for (int i = 1; i < this.capacity;) {
-      next[i] = ++i;
-    }
-
-    // Surly, the last one should point to the 'Ground'.
-    next[this.capacity] = 0;
-  }
-
-  /**
-   * Checks if a given key exists in the map.
-   * 
-   * @param key
-   *            that is checked against the map data.
-   * @return true if the key exists in the map. false otherwise.
-   */
-  public boolean containsKey(int key) {
-    return find(key) != 0;
-  }
-
-  /**
-   * Checks if the given object exists in the map.<br>
-   * This method iterates over the collection, trying to find an equal object.
-   * 
-   * @param v
-   *            value that is checked against the map data.
-   * @return true if the value exists in the map (in .equals() meaning).
-   *         false otherwise.
-   */
-  public boolean containsValue(int v) {
-    for (IntIterator iterator = iterator(); iterator.hasNext();) {
-      if (v == iterator.next()) {
-        return true;
-      }
-    }
-    return false;
-  }
-
-  /**
-   * Find the actual index of a given key.
-   * 
-   * @return index of the key. zero if the key wasn't found.
-   */
-  protected int find(int key) {
-    // Calculate the hash entry.
-    int baseHashIndex = calcBaseHashIndex(key);
-
-    // Start from the hash entry.
-    int localIndex = baseHash[baseHashIndex];
-
-    // while the index does not point to the 'Ground'
-    while (localIndex != 0) {
-      // returns the index found in case of of a matching key.
-      if (keys[localIndex] == key) {
-        return localIndex;
-      }
-      
-      // next the local index
-      localIndex = next[localIndex];
-    }
-
-    // If we got this far, it could only mean we did not find the key we
-    // were asked for. return 'Ground' index.
-    return 0;
-  }
-
-  /**
-   * Find the actual index of a given key with it's baseHashIndex.<br>
-   * Some methods use the baseHashIndex. If those call {@link #find} there's
-   * no need to re-calculate that hash.
-   * 
-   * @return the index of the given key, or 0 as 'Ground' if the key wasn't
-   *         found.
-   */
-  private int findForRemove(int key, int baseHashIndex) {
-    // Start from the hash entry.
-    this.prev = 0;
-    int index = baseHash[baseHashIndex];
-
-    // while the index does not point to the 'Ground'
-    while (index != 0) {
-      // returns the index found in case of of a matching key.
-      if (keys[index] == key) {
-        return index;
-      }
-
-      // next the local index
-      prev = index;
-      index = next[index];
-    }
-
-    // If we got this far, it could only mean we did not find the key we
-    // were asked for. return 'Ground' index.
-    this.prev = 0;
-    return 0;
-  }
-
-  /**
-   * Returns the object mapped with the given key.
-   * 
-   * @param key
-   *            int who's mapped object we're interested in.
-   * @return an object mapped by the given key. null if the key wasn't found.
-   */
-  public int get(int key) {
-    return values[find(key)];
-  }
-
-  /**
-   * Grows the map. Allocates a new map of double the capacity, and
-   * fast-insert the old key-value pairs.
-   */
-  protected void grow() {
-    IntToIntMap that = new IntToIntMap(
-        this.capacity * 2);
-
-    // Iterates fast over the collection. Any valid pair is put into the new
-    // map without checking for duplicates or if there's enough space for
-    // it.
-    for (IndexIterator iterator = new IndexIterator(); iterator.hasNext();) {
-      int index = iterator.next();
-      that.prvt_put(this.keys[index], this.values[index]);
-    }
-
-    // Copy that's data into this.
-    this.capacity = that.capacity;
-    this.size = that.size;
-    this.firstEmpty = that.firstEmpty;
-    this.values = that.values;
-    this.keys = that.keys;
-    this.next = that.next;
-    this.baseHash = that.baseHash;
-    this.hashFactor = that.hashFactor;
-  }
-
-  /**
-   * 
-   * @return true if the map is empty. false otherwise.
-   */
-  public boolean isEmpty() {
-    return size == 0;
-  }
-
-  /**
-   * Returns a new iterator for the mapped objects.
-   */
-  public IntIterator iterator() {
-    return new ValueIterator();
-  }
-
-  /** Returns an iterator on the map keys. */
-  public IntIterator keyIterator() {
-    return new KeyIterator();
-  }
-
-  /**
-   * Prints the baseHash array, used for debug purposes.
-   */
-  @SuppressWarnings("unused")
-  private String getBaseHashAsString() {
-    return Arrays.toString(this.baseHash);
-  }
-
-  /**
-   * Inserts the &lt;key,value&gt; pair into the map. If the key already exists,
-   * this method updates the mapped value to the given one, returning the old
-   * mapped value.
-   * 
-   * @return the old mapped value, or 0 if the key didn't exist.
-   */
-  public int put(int key, int e) {
-    // Does key exists?
-    int index = find(key);
-
-    // Yes!
-    if (index != 0) {
-      // Set new data and exit.
-      int old = values[index];
-      values[index] = e;
-      return old;
-    }
-
-    // Is there enough room for a new pair?
-    if (size == capacity) {
-      // No? Than grow up!
-      grow();
-    }
-
-    // Now that everything is set, the pair can be just put inside with no
-    // worries.
-    prvt_put(key, e);
-
-    return 0;
-  }
-
-  /**
-   * Removes a &lt;key,value&gt; pair from the map and returns the mapped value,
-   * or 0 if the none existed.
-   * 
-   * @param key used to find the value to remove
-   * @return the removed value or 0 if none existed.
-   */
-  public int remove(int key) {
-    int baseHashIndex = calcBaseHashIndex(key);
-    int index = findForRemove(key, baseHashIndex);
-    if (index != 0) {
-      // If it is the first in the collision list, we should promote its
-      // next colliding element.
-      if (prev == 0) {
-        baseHash[baseHashIndex] = next[index];
-      }
-
-      next[prev] = next[index];
-      next[index] = firstEmpty;
-      firstEmpty = index;
-      --size;
-      return values[index];
-    }
-
-    return 0;
-  }
-
-  /**
-   * @return number of pairs currently in the map
-   */
-  public int size() {
-    return this.size;
-  }
-
-  /**
-   * Translates the mapped pairs' values into an array of Objects
-   * 
-   * @return an object array of all the values currently in the map.
-   */
-  public int[] toArray() {
-    int j = -1;
-    int[] array = new int[size];
-
-    // Iterates over the values, adding them to the array.
-    for (IntIterator iterator = iterator(); iterator.hasNext();) {
-      array[++j] = iterator.next();
-    }
-    return array;
-  }
-
-  /**
-   * Translates the mapped pairs' values into an array of ints
-   * 
-   * @param a
-   *            the array into which the elements of the map are to be
-   *            stored, if it is big enough; otherwise, a new array of the
-   *            same runtime type is allocated for this purpose.
-   * 
-   * @return an array containing the values stored in the map
-   * 
-   */
-  public int[] toArray(int[] a) {
-    int j = 0;
-    if (a.length < size) {
-      a = new int[size];
-    }
-    // Iterates over the values, adding them to the array.
-    for (IntIterator iterator = iterator(); j < a.length
-      && iterator.hasNext(); ++j) {
-      a[j] = iterator.next();
-    }
-    return a;
-  }
-
-  @Override
-  public String toString() {
-    StringBuffer sb = new StringBuffer();
-    sb.append('{');
-    IntIterator keyIterator = keyIterator();
-    while (keyIterator.hasNext()) {
-      int key = keyIterator.next();
-      sb.append(key);
-      sb.append('=');
-      sb.append(get(key));
-      if (keyIterator.hasNext()) {
-        sb.append(',');
-        sb.append(' ');
-      }
-    }
-    sb.append('}');
-    return sb.toString();
-  }
-  
-  @Override
-  public int hashCode() {
-    return getClass().hashCode() ^ size();
-  }
-  
-  @Override
-  public boolean equals(Object o) {
-    IntToIntMap that = (IntToIntMap)o;
-    if (that.size() != this.size()) {
-      return false;
-    }
-    
-    IntIterator it = keyIterator();
-    while (it.hasNext()) {
-      int key = it.next();
-      
-      if (!that.containsKey(key)) {
-        return false;
-      }
-      
-      int v1 = this.get(key);
-      int v2 = that.get(key);
-      if (v1 != v2) {
-        return false;
-      }
-    }
-    return true;
-  }
-}
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/util/collections/IntToObjectMap.java b/lucene/facet/src/java/org/apache/lucene/util/collections/IntToObjectMap.java
deleted file mode 100644
index 42d429b..0000000
--- a/lucene/facet/src/java/org/apache/lucene/util/collections/IntToObjectMap.java
+++ /dev/null
@@ -1,634 +0,0 @@
-package org.apache.lucene.util.collections;
-
-import java.util.Arrays;
-import java.util.Iterator;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * An Array-based hashtable which maps primitive int to Objects of generic type
- * T.<br>
- * The hashtable is constracted with a given capacity, or 16 as a default. In
- * case there's not enough room for new pairs, the hashtable grows. <br>
- * Capacity is adjusted to a power of 2, and there are 2 * capacity entries for
- * the hash.
- * 
- * The pre allocated arrays (for keys, values) are at length of capacity + 1,
- * when index 0 is used as 'Ground' or 'NULL'.<br>
- * 
- * The arrays are allocated ahead of hash operations, and form an 'empty space'
- * list, to which the key,value pair is allocated.
- * 
- * @lucene.experimental
- */
-public class IntToObjectMap<T> implements Iterable<T> {
-
-  /**
-   * Implements an IntIterator which iterates over all the allocated indexes.
-   */
-  private final class IndexIterator implements IntIterator {
-    /**
-     * The last used baseHashIndex. Needed for "jumping" from one hash entry
-     * to another.
-     */
-    private int baseHashIndex = 0;
-
-    /**
-     * The next not-yet-visited index.
-     */
-    private int index = 0;
-
-    /**
-     * Index of the last visited pair. Used in {@link #remove()}.
-     */
-    private int lastIndex = 0;
-
-    /**
-     * Create the Iterator, make <code>index</code> point to the "first"
-     * index which is not empty. If such does not exist (eg. the map is
-     * empty) it would be zero.
-     */
-    public IndexIterator() {
-      for (baseHashIndex = 0; baseHashIndex < baseHash.length; ++baseHashIndex) {
-        index = baseHash[baseHashIndex];
-        if (index != 0) {
-          break;
-        }
-      }
-    }
-
-    @Override
-    public boolean hasNext() {
-      return (index != 0);
-    }
-
-    @Override
-    public int next() {
-      // Save the last index visited
-      lastIndex = index;
-
-      // next the index
-      index = next[index];
-
-      // if the next index points to the 'Ground' it means we're done with
-      // the current hash entry and we need to jump to the next one. This
-      // is done until all the hash entries had been visited.
-      while (index == 0 && ++baseHashIndex < baseHash.length) {
-        index = baseHash[baseHashIndex];
-      }
-
-      return lastIndex;
-    }
-
-    @Override
-    public void remove() {
-      IntToObjectMap.this.remove(keys[lastIndex]);
-    }
-
-  }
-
-  /**
-   * Implements an IntIterator, used for iteration over the map's keys.
-   */
-  private final class KeyIterator implements IntIterator {
-    private IntIterator iterator = new IndexIterator();
-
-    KeyIterator() { }
-    
-    @Override
-    public boolean hasNext() {
-      return iterator.hasNext();
-    }
-
-    @Override
-    public int next() {
-      return keys[iterator.next()];
-    }
-
-    @Override
-    public void remove() {
-      iterator.remove();
-    }
-  }
-
-  /**
-   * Implements an Iterator of a generic type T used for iteration over the
-   * map's values.
-   */
-  private final class ValueIterator implements Iterator<T> {
-    private IntIterator iterator = new IndexIterator();
-
-    ValueIterator() { }
-    
-    @Override
-    public boolean hasNext() {
-      return iterator.hasNext();
-    }
-
-    @Override
-    @SuppressWarnings("unchecked")
-    public T next() {
-      return (T) values[iterator.next()];
-    }
-
-    @Override
-    public void remove() {
-      iterator.remove();
-    }
-  }
-
-  /**
-   * Default capacity - in case no capacity was specified in the constructor
-   */
-  private static int defaultCapacity = 16;
-
-  /**
-   * Holds the base hash entries. if the capacity is 2^N, than the base hash
-   * holds 2^(N+1). It can hold
-   */
-  int[] baseHash;
-
-  /**
-   * The current capacity of the map. Always 2^N and never less than 16. We
-   * never use the zero index. It is needed to improve performance and is also
-   * used as "ground".
-   */
-  private int capacity;
-  /**
-   * All objects are being allocated at map creation. Those objects are "free"
-   * or empty. Whenever a new pair comes along, a pair is being "allocated" or
-   * taken from the free-linked list. as this is just a free list.
-   */
-  private int firstEmpty;
-
-  /**
-   * hashFactor is always (2^(N+1)) - 1. Used for faster hashing.
-   */
-  private int hashFactor;
-
-  /**
-   * This array holds the unique keys
-   */
-  int[] keys;
-
-  /**
-   * In case of collisions, we implement a double linked list of the colliding
-   * hash's with the following next[] and prev[]. Those are also used to store
-   * the "empty" list.
-   */
-  int[] next;
-
-  private int prev;
-
-  /**
-   * Number of currently objects in the map.
-   */
-  private int size;
-
-  /**
-   * This array holds the values
-   */
-  Object[] values;
-
-  /**
-   * Constructs a map with default capacity.
-   */
-  public IntToObjectMap() {
-    this(defaultCapacity);
-  }
-
-  /**
-   * Constructs a map with given capacity. Capacity is adjusted to a native
-   * power of 2, with minimum of 16.
-   * 
-   * @param capacity
-   *            minimum capacity for the map.
-   */
-  public IntToObjectMap(int capacity) {
-    this.capacity = 16;
-    // Minimum capacity is 16..
-    while (this.capacity < capacity) {
-      // Multiply by 2 as long as we're still under the requested capacity
-      this.capacity <<= 1;
-    }
-
-    // As mentioned, we use the first index (0) as 'Ground', so we need the
-    // length of the arrays to be one more than the capacity
-    int arrayLength = this.capacity + 1;
-
-    this.values = new Object[arrayLength];
-    this.keys = new int[arrayLength];
-    this.next = new int[arrayLength];
-
-    // Hash entries are twice as big as the capacity.
-    int baseHashSize = this.capacity << 1;
-
-    this.baseHash = new int[baseHashSize];
-
-    // The has factor is 2^M - 1 which is used as an "AND" hashing operator.
-    // {@link #calcBaseHash()}
-    this.hashFactor = baseHashSize - 1;
-
-    this.size = 0;
-
-    clear();
-  }
-
-  /**
-   * Adds a pair to the map. Takes the first empty position from the
-   * empty-linked-list's head - {@link #firstEmpty}.
-   * 
-   * New pairs are always inserted to baseHash, and are followed by the old
-   * colliding pair.
-   * 
-   * @param key
-   *            integer which maps the given Object
-   * @param e
-   *            element which is being mapped using the given key
-   */
-  private void prvt_put(int key, T e) {
-    // Hash entry to which the new pair would be inserted
-    int hashIndex = calcBaseHashIndex(key);
-
-    // 'Allocating' a pair from the "Empty" list.
-    int objectIndex = firstEmpty;
-
-    // Setting data
-    firstEmpty = next[firstEmpty];
-    values[objectIndex] = e;
-    keys[objectIndex] = key;
-
-    // Inserting the new pair as the first node in the specific hash entry
-    next[objectIndex] = baseHash[hashIndex];
-    baseHash[hashIndex] = objectIndex;
-
-    // Announcing a new pair was added!
-    ++size;
-  }
-
-  /**
-   * Calculating the baseHash index using the internal <code>hashFactor</code>.
-   * 
-   */
-  protected int calcBaseHashIndex(int key) {
-    return key & hashFactor;
-  }
-
-  /**
-   * Empties the map. Generates the "Empty" space list for later allocation.
-   */
-  public void clear() {
-    // Clears the hash entries
-    Arrays.fill(this.baseHash, 0);
-
-    // Set size to zero
-    size = 0;
-
-    // Mark all array entries as empty. This is done with
-    // <code>firstEmpty</code> pointing to the first valid index (1 as 0 is
-    // used as 'Ground').
-    firstEmpty = 1;
-
-    // And setting all the <code>next[i]</code> to point at
-    // <code>i+1</code>.
-    for (int i = 1; i < this.capacity;) {
-      next[i] = ++i;
-    }
-
-    // Surly, the last one should point to the 'Ground'.
-    next[this.capacity] = 0;
-  }
-
-  /**
-   * Checks if a given key exists in the map.
-   * 
-   * @param key
-   *            that is checked against the map data.
-   * @return true if the key exists in the map. false otherwise.
-   */
-  public boolean containsKey(int key) {
-    return find(key) != 0;
-  }
-
-  /**
-   * Checks if the given object exists in the map.<br>
-   * This method iterates over the collection, trying to find an equal object.
-   * 
-   * @param o
-   *            object that is checked against the map data.
-   * @return true if the object exists in the map (in .equals() meaning).
-   *         false otherwise.
-   */
-  public boolean containsValue(Object o) {
-    for (Iterator<T> iterator = iterator(); iterator.hasNext();) {
-      T object = iterator.next();
-      if (object.equals(o)) {
-        return true;
-      }
-    }
-    return false;
-  }
-
-  /**
-   * Find the actual index of a given key.
-   * 
-   * @return index of the key. zero if the key wasn't found.
-   */
-  protected int find(int key) {
-    // Calculate the hash entry.
-    int baseHashIndex = calcBaseHashIndex(key);
-
-    // Start from the hash entry.
-    int localIndex = baseHash[baseHashIndex];
-
-    // while the index does not point to the 'Ground'
-    while (localIndex != 0) {
-      // returns the index found in case of of a matching key.
-      if (keys[localIndex] == key) {
-        return localIndex;
-      }
-
-      // next the local index
-      localIndex = next[localIndex];
-    }
-
-    // If we got this far, it could only mean we did not find the key we
-    // were asked for. return 'Ground' index.
-    return 0;
-  }
-
-  /**
-   * Find the actual index of a given key with it's baseHashIndex.<br>
-   * Some methods use the baseHashIndex. If those call {@link #find} there's
-   * no need to re-calculate that hash.
-   * 
-   * @return the index of the given key, or 0 as 'Ground' if the key wasn't
-   *         found.
-   */
-  private int findForRemove(int key, int baseHashIndex) {
-    // Start from the hash entry.
-    this.prev = 0;
-    int index = baseHash[baseHashIndex];
-
-    // while the index does not point to the 'Ground'
-    while (index != 0) {
-      // returns the index found in case of of a matching key.
-      if (keys[index] == key) {
-        return index;
-      }
-
-      // next the local index
-      prev = index;
-      index = next[index];
-    }
-
-    // If we got this far, it could only mean we did not find the key we
-    // were asked for. return 'Ground' index.
-    this.prev = 0;
-    return 0;
-  }
-
-  /**
-   * Returns the object mapped with the given key.
-   * 
-   * @param key
-   *            int who's mapped object we're interested in.
-   * @return an object mapped by the given key. null if the key wasn't found.
-   */
-  @SuppressWarnings("unchecked")
-  public T get(int key) {
-    return (T) values[find(key)];
-  }
-
-  /**
-   * Grows the map. Allocates a new map of double the capacity, and
-   * fast-insert the old key-value pairs.
-   */
-  @SuppressWarnings("unchecked")
-  protected void grow() {
-    IntToObjectMap<T> that = new IntToObjectMap<T>(
-        this.capacity * 2);
-
-    // Iterates fast over the collection. Any valid pair is put into the new
-    // map without checking for duplicates or if there's enough space for
-    // it.
-    for (IndexIterator iterator = new IndexIterator(); iterator.hasNext();) {
-      int index = iterator.next();
-      that.prvt_put(this.keys[index], (T) this.values[index]);
-    }
-
-    // Copy that's data into this.
-    this.capacity = that.capacity;
-    this.size = that.size;
-    this.firstEmpty = that.firstEmpty;
-    this.values = that.values;
-    this.keys = that.keys;
-    this.next = that.next;
-    this.baseHash = that.baseHash;
-    this.hashFactor = that.hashFactor;
-  }
-
-  /**
-   * 
-   * @return true if the map is empty. false otherwise.
-   */
-  public boolean isEmpty() {
-    return size == 0;
-  }
-
-  /**
-   * Returns a new iterator for the mapped objects.
-   */
-  @Override
-  public Iterator<T> iterator() {
-    return new ValueIterator();
-  }
-
-  /** Returns an iterator on the map keys. */
-  public IntIterator keyIterator() {
-    return new KeyIterator();
-  }
-
-  /**
-   * Prints the baseHash array, used for debug purposes.
-   */
-  @SuppressWarnings("unused")
-  private String getBaseHashAsString() {
-    return Arrays.toString(baseHash);
-  }
-
-  /**
-   * Inserts the &lt;key,value&gt; pair into the map. If the key already exists,
-   * this method updates the mapped value to the given one, returning the old
-   * mapped value.
-   * 
-   * @return the old mapped value, or null if the key didn't exist.
-   */
-  @SuppressWarnings("unchecked")
-  public T put(int key, T e) {
-    // Does key exists?
-    int index = find(key);
-
-    // Yes!
-    if (index != 0) {
-      // Set new data and exit.
-      T old = (T) values[index];
-      values[index] = e;
-      return old;
-    }
-
-    // Is there enough room for a new pair?
-    if (size == capacity) {
-      // No? Than grow up!
-      grow();
-    }
-
-    // Now that everything is set, the pair can be just put inside with no
-    // worries.
-    prvt_put(key, e);
-
-    return null;
-  }
-
-  /**
-   * Removes a &lt;key,value&gt; pair from the map and returns the mapped value,
-   * or null if the none existed.
-   * 
-   * @param key used to find the value to remove
-   * @return the removed value or null if none existed.
-   */
-  @SuppressWarnings("unchecked")
-  public T remove(int key) {
-    int baseHashIndex = calcBaseHashIndex(key);
-    int index = findForRemove(key, baseHashIndex);
-    if (index != 0) {
-      // If it is the first in the collision list, we should promote its
-      // next colliding element.
-      if (prev == 0) {
-        baseHash[baseHashIndex] = next[index];
-      }
-
-      next[prev] = next[index];
-      next[index] = firstEmpty;
-      firstEmpty = index;
-      --size;
-      return (T) values[index];
-    }
-
-    return null;
-  }
-
-  /**
-   * @return number of pairs currently in the map
-   */
-  public int size() {
-    return this.size;
-  }
-
-  /**
-   * Translates the mapped pairs' values into an array of Objects
-   * 
-   * @return an object array of all the values currently in the map.
-   */
-  public Object[] toArray() {
-    int j = -1;
-    Object[] array = new Object[size];
-
-    // Iterates over the values, adding them to the array.
-    for (Iterator<T> iterator = iterator(); iterator.hasNext();) {
-      array[++j] = iterator.next();
-    }
-    return array;
-  }
-
-  /**
-   * Translates the mapped pairs' values into an array of T
-   * 
-   * @param a
-   *            the array into which the elements of the list are to be
-   *            stored, if it is big enough; otherwise, use whatever space we
-   *            have, setting the one after the true data as null.
-   * 
-   * @return an array containing the elements of the list
-   * 
-   */
-  public T[] toArray(T[] a) {
-    int j = 0;
-    // Iterates over the values, adding them to the array.
-    for (Iterator<T> iterator = iterator(); j < a.length
-    && iterator.hasNext(); ++j) {
-      a[j] = iterator.next();
-    }
-
-    if (j < a.length) {
-      a[j] = null;
-    }
-
-    return a;
-  }
-
-  @Override
-  public String toString() {
-    StringBuffer sb = new StringBuffer();
-    sb.append('{');
-    IntIterator keyIterator = keyIterator();
-    while (keyIterator.hasNext()) {
-      int key = keyIterator.next();
-      sb.append(key);
-      sb.append('=');
-      sb.append(get(key));
-      if (keyIterator.hasNext()) {
-        sb.append(',');
-        sb.append(' ');
-      }
-    }
-    sb.append('}');
-    return sb.toString();
-  }
-  
-  @Override
-  public int hashCode() {
-    return getClass().hashCode() ^ size();
-  }
-  
-  @SuppressWarnings("unchecked")
-  @Override
-  public boolean equals(Object o) {
-    IntToObjectMap<T> that = (IntToObjectMap<T>)o;
-    if (that.size() != this.size()) {
-      return false;
-    }
-    
-    IntIterator it = keyIterator();
-    while (it.hasNext()) {
-      int key = it.next();
-      if (!that.containsKey(key)) {
-        return false;
-      }
-
-      T v1 = this.get(key);
-      T v2 = that.get(key);
-      if ((v1 == null && v2 != null) ||
-          (v1 != null && v2 == null) ||
-          (!v1.equals(v2))) {
-        return false;
-      }
-    }
-    return true;
-  }
-}
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/util/collections/LRUHashMap.java b/lucene/facet/src/java/org/apache/lucene/util/collections/LRUHashMap.java
deleted file mode 100644
index fe084dc..0000000
--- a/lucene/facet/src/java/org/apache/lucene/util/collections/LRUHashMap.java
+++ /dev/null
@@ -1,111 +0,0 @@
-package org.apache.lucene.util.collections;
-
-import java.util.LinkedHashMap;
-import java.util.Map;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * LRUHashMap is an extension of Java's HashMap, which has a bounded size();
- * When it reaches that size, each time a new element is added, the least
- * recently used (LRU) entry is removed.
- * <p>
- * Java makes it very easy to implement LRUHashMap - all its functionality is
- * already available from {@link java.util.LinkedHashMap}, and we just need to
- * configure that properly.
- * <p>
- * Note that like HashMap, LRUHashMap is unsynchronized, and the user MUST
- * synchronize the access to it if used from several threads. Moreover, while
- * with HashMap this is only a concern if one of the threads is modifies the
- * map, with LURHashMap every read is a modification (because the LRU order
- * needs to be remembered) so proper synchronization is always necessary.
- * <p>
- * With the usual synchronization mechanisms available to the user, this
- * unfortunately means that LRUHashMap will probably perform sub-optimally under
- * heavy contention: while one thread uses the hash table (reads or writes), any
- * other thread will be blocked from using it - or even just starting to use it
- * (e.g., calculating the hash function). A more efficient approach would be not
- * to use LinkedHashMap at all, but rather to use a non-locking (as much as
- * possible) thread-safe solution, something along the lines of
- * java.util.concurrent.ConcurrentHashMap (though that particular class does not
- * support the additional LRU semantics, which will need to be added separately
- * using a concurrent linked list or additional storage of timestamps (in an
- * array or inside the entry objects), or whatever).
- * 
- * @lucene.experimental
- */
-public class LRUHashMap<K,V> extends LinkedHashMap<K,V> {
-
-  private int maxSize;
-
-  /**
-   * Create a new hash map with a bounded size and with least recently
-   * used entries removed.
-   * @param maxSize
-   *     the maximum size (in number of entries) to which the map can grow
-   *     before the least recently used entries start being removed.<BR>
-   *      Setting maxSize to a very large value, like
-   *      {@link Integer#MAX_VALUE} is allowed, but is less efficient than
-   *      using {@link java.util.HashMap} because our class needs
-   *      to keep track of the use order (via an additional doubly-linked
-   *      list) which is not used when the map's size is always below the
-   *      maximum size. 
-   */
-  public LRUHashMap(int maxSize) {
-    super(16, 0.75f, true);
-    this.maxSize = maxSize;
-  }
-
-  /**
-   * Return the max size
-   */
-  public int getMaxSize() {
-    return maxSize;
-  }
-
-  /**
-   * setMaxSize() allows changing the map's maximal number of elements
-   * which was defined at construction time.
-   * <P>
-   * Note that if the map is already larger than maxSize, the current 
-   * implementation does not shrink it (by removing the oldest elements);
-   * Rather, the map remains in its current size as new elements are
-   * added, and will only start shrinking (until settling again on the
-   * give maxSize) if existing elements are explicitly deleted.  
-   */
-  public void setMaxSize(int maxSize) {
-    this.maxSize = maxSize;
-  }
-
-  // We override LinkedHashMap's removeEldestEntry() method. This method
-  // is called every time a new entry is added, and if we return true
-  // here, the eldest element will be deleted automatically. In our case,
-  // we return true if the size of the map grew beyond our limit - ignoring
-  // what is that eldest element that we'll be deleting.
-  @Override
-  protected boolean removeEldestEntry(Map.Entry<K, V> eldest) {
-    return size() > maxSize;
-  }
-
-  @SuppressWarnings("unchecked")
-  @Override
-  public LRUHashMap<K,V> clone() {
-    return (LRUHashMap<K,V>) super.clone();
-  }
-  
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/util/collections/ObjectToFloatMap.java b/lucene/facet/src/java/org/apache/lucene/util/collections/ObjectToFloatMap.java
deleted file mode 100644
index 602c26e..0000000
--- a/lucene/facet/src/java/org/apache/lucene/util/collections/ObjectToFloatMap.java
+++ /dev/null
@@ -1,623 +0,0 @@
-package org.apache.lucene.util.collections;
-
-import java.util.Arrays;
-import java.util.Iterator;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * An Array-based hashtable which maps Objects of generic type
- * T to primitive float values.<br>
- * The hashtable is constructed with a given capacity, or 16 as a default. In
- * case there's not enough room for new pairs, the hashtable grows. <br>
- * Capacity is adjusted to a power of 2, and there are 2 * capacity entries for
- * the hash.
- * 
- * The pre allocated arrays (for keys, values) are at length of capacity + 1,
- * when index 0 is used as 'Ground' or 'NULL'.<br>
- * 
- * The arrays are allocated ahead of hash operations, and form an 'empty space'
- * list, to which the key,value pair is allocated.
- * 
- * @lucene.experimental
- */
-public class ObjectToFloatMap<K> {
-
-  /**
-   * Implements an IntIterator which iterates over all the allocated indexes.
-   */
-  private final class IndexIterator implements IntIterator {
-    /**
-     * The last used baseHashIndex. Needed for "jumping" from one hash entry
-     * to another.
-     */
-    private int baseHashIndex = 0;
-
-    /**
-     * The next not-yet-visited index.
-     */
-    private int index = 0;
-
-    /**
-     * Index of the last visited pair. Used in {@link #remove()}.
-     */
-    private int lastIndex = 0;
-
-    /**
-     * Create the Iterator, make <code>index</code> point to the "first"
-     * index which is not empty. If such does not exist (eg. the map is
-     * empty) it would be zero.
-     */
-    public IndexIterator() {
-      for (baseHashIndex = 0; baseHashIndex < baseHash.length; ++baseHashIndex) {
-        index = baseHash[baseHashIndex];
-        if (index != 0) {
-          break;
-        }
-      }
-    }
-
-    @Override
-    public boolean hasNext() {
-      return (index != 0);
-    }
-
-    @Override
-    public int next() {
-      // Save the last index visited
-      lastIndex = index;
-
-      // next the index
-      index = next[index];
-
-      // if the next index points to the 'Ground' it means we're done with
-      // the current hash entry and we need to jump to the next one. This
-      // is done until all the hash entries had been visited.
-      while (index == 0 && ++baseHashIndex < baseHash.length) {
-        index = baseHash[baseHashIndex];
-      }
-
-      return lastIndex;
-    }
-
-    @Override
-    @SuppressWarnings("unchecked")
-    public void remove() {
-      ObjectToFloatMap.this.remove((K) keys[lastIndex]);
-    }
-
-  }
-
-  /**
-   * Implements an IntIterator, used for iteration over the map's keys.
-   */
-  private final class KeyIterator implements Iterator<K> {
-    private IntIterator iterator = new IndexIterator();
-
-    KeyIterator() { }
-    
-    @Override
-    public boolean hasNext() {
-      return iterator.hasNext();
-    }
-
-    @Override
-    @SuppressWarnings("unchecked")
-    public K next() {
-      return (K) keys[iterator.next()];
-    }
-
-    @Override
-    public void remove() {
-      iterator.remove();
-    }
-  }
-
-  /**
-   * Implements an Iterator of a generic type T used for iteration over the
-   * map's values.
-   */
-  private final class ValueIterator implements FloatIterator {
-    private IntIterator iterator = new IndexIterator();
-
-    ValueIterator() { }
-    
-    @Override
-    public boolean hasNext() {
-      return iterator.hasNext();
-    }
-
-    @Override
-    public float next() {
-      return values[iterator.next()];
-    }
-
-    @Override
-    public void remove() {
-      iterator.remove();
-    }
-  }
-
-  /**
-   * Default capacity - in case no capacity was specified in the constructor
-   */
-  private static int defaultCapacity = 16;
-
-  /**
-   * Holds the base hash entries. if the capacity is 2^N, than the base hash
-   * holds 2^(N+1). It can hold
-   */
-  int[] baseHash;
-
-  /**
-   * The current capacity of the map. Always 2^N and never less than 16. We
-   * never use the zero index. It is needed to improve performance and is also
-   * used as "ground".
-   */
-  private int capacity;
-  /**
-   * All objects are being allocated at map creation. Those objects are "free"
-   * or empty. Whenever a new pair comes along, a pair is being "allocated" or
-   * taken from the free-linked list. as this is just a free list.
-   */
-  private int firstEmpty;
-
-  /**
-   * hashFactor is always (2^(N+1)) - 1. Used for faster hashing.
-   */
-  private int hashFactor;
-
-  /**
-   * This array holds the unique keys
-   */
-  Object[] keys;
-
-  /**
-   * In case of collisions, we implement a double linked list of the colliding
-   * hash's with the following next[] and prev[]. Those are also used to store
-   * the "empty" list.
-   */
-  int[] next;
-
-  private int prev;
-
-  /**
-   * Number of currently objects in the map.
-   */
-  private int size;
-
-  /**
-   * This array holds the values
-   */
-  float[] values;
-
-  /**
-   * Constructs a map with default capacity.
-   */
-  public ObjectToFloatMap() {
-    this(defaultCapacity);
-  }
-
-  /**
-   * Constructs a map with given capacity. Capacity is adjusted to a native
-   * power of 2, with minimum of 16.
-   * 
-   * @param capacity
-   *            minimum capacity for the map.
-   */
-  public ObjectToFloatMap(int capacity) {
-    this.capacity = 16;
-    // Minimum capacity is 16..
-    while (this.capacity < capacity) {
-      // Multiply by 2 as long as we're still under the requested capacity
-      this.capacity <<= 1;
-    }
-
-    // As mentioned, we use the first index (0) as 'Ground', so we need the
-    // length of the arrays to be one more than the capacity
-    int arrayLength = this.capacity + 1;
-
-    this.values = new float[arrayLength];
-    this.keys = new Object[arrayLength];
-    this.next = new int[arrayLength];
-
-    // Hash entries are twice as big as the capacity.
-    int baseHashSize = this.capacity << 1;
-
-    this.baseHash = new int[baseHashSize];
-
-    // The has factor is 2^M - 1 which is used as an "AND" hashing operator.
-    // {@link #calcBaseHash()}
-    this.hashFactor = baseHashSize - 1;
-
-    this.size = 0;
-
-    clear();
-  }
-
-  /**
-   * Adds a pair to the map. Takes the first empty position from the
-   * empty-linked-list's head - {@link #firstEmpty}.
-   * 
-   * New pairs are always inserted to baseHash, and are followed by the old
-   * colliding pair.
-   * 
-   * @param key
-   *            integer which maps the given Object
-   * @param e
-   *            element which is being mapped using the given key
-   */
-  private void prvt_put(K key, float e) {
-    // Hash entry to which the new pair would be inserted
-    int hashIndex = calcBaseHashIndex(key);
-
-    // 'Allocating' a pair from the "Empty" list.
-    int objectIndex = firstEmpty;
-
-    // Setting data
-    firstEmpty = next[firstEmpty];
-    values[objectIndex] = e;
-    keys[objectIndex] = key;
-
-    // Inserting the new pair as the first node in the specific hash entry
-    next[objectIndex] = baseHash[hashIndex];
-    baseHash[hashIndex] = objectIndex;
-
-    // Announcing a new pair was added!
-    ++size;
-  }
-
-  /**
-   * Calculating the baseHash index using the internal <code>hashFactor</code>.
-   */
-  protected int calcBaseHashIndex(K key) {
-    return key.hashCode() & hashFactor;
-  }
-
-  /**
-   * Empties the map. Generates the "Empty" space list for later allocation.
-   */
-  public void clear() {
-    // Clears the hash entries
-    Arrays.fill(this.baseHash, 0);
-
-    // Set size to zero
-    size = 0;
-
-    values[0] = Float.NaN;
-
-    // Mark all array entries as empty. This is done with
-    // <code>firstEmpty</code> pointing to the first valid index (1 as 0 is
-    // used as 'Ground').
-    firstEmpty = 1;
-
-    // And setting all the <code>next[i]</code> to point at
-    // <code>i+1</code>.
-    for (int i = 1; i < this.capacity;) {
-      next[i] = ++i;
-    }
-
-    // Surly, the last one should point to the 'Ground'.
-    next[this.capacity] = 0;
-  }
-
-  /**
-   * Checks if a given key exists in the map.
-   * 
-   * @param key
-   *            that is checked against the map data.
-   * @return true if the key exists in the map. false otherwise.
-   */
-  public boolean containsKey(K key) {
-    return find(key) != 0;
-  }
-
-  /**
-   * Checks if the given object exists in the map.<br>
-   * This method iterates over the collection, trying to find an equal object.
-   * 
-   * @param o
-   *            object that is checked against the map data.
-   * @return true if the object exists in the map (in .equals() meaning).
-   *         false otherwise.
-   */
-  public boolean containsValue(float o) {
-    for (FloatIterator iterator = iterator(); iterator.hasNext();) {
-      if (o == iterator.next()) {
-        return true;
-      }
-    }
-    return false;
-  }
-
-  /**
-   * Find the actual index of a given key.
-   * 
-   * @return index of the key. zero if the key wasn't found.
-   */
-  protected int find(K key) {
-    // Calculate the hash entry.
-    int baseHashIndex = calcBaseHashIndex(key);
-
-    // Start from the hash entry.
-    int localIndex = baseHash[baseHashIndex];
-
-    // while the index does not point to the 'Ground'
-    while (localIndex != 0) {
-      // returns the index found in case of of a matching key.
-      if (keys[localIndex].equals(key)) {
-        return localIndex;
-      }
-
-      // next the local index
-      localIndex = next[localIndex];
-    }
-
-    // If we got this far, it could only mean we did not find the key we
-    // were asked for. return 'Ground' index.
-    return 0;
-  }
-
-  /**
-   * Find the actual index of a given key with it's baseHashIndex.<br>
-   * Some methods use the baseHashIndex. If those call {@link #find} there's
-   * no need to re-calculate that hash.
-   * 
-   * @return the index of the given key, or 0 as 'Ground' if the key wasn't
-   *         found.
-   */
-  private int findForRemove(K key, int baseHashIndex) {
-    // Start from the hash entry.
-    this.prev = 0;
-    int index = baseHash[baseHashIndex];
-
-    // while the index does not point to the 'Ground'
-    while (index != 0) {
-      // returns the index found in case of of a matching key.
-      if (keys[index].equals(key)) {
-        return index;
-      }
-
-      // next the local index
-      prev = index;
-      index = next[index];
-    }
-
-    // If we got this far, it could only mean we did not find the key we
-    // were asked for. return 'Ground' index.
-    this.prev = 0;
-    return 0;
-  }
-
-  /**
-   * Returns the float mapped with the given key.
-   * 
-   * @param key
-   *            object who's mapped float we're interested in.
-   * @return a float mapped by the given key. Float.NaN if the key wasn't found.
-   */
-  public float get(K key) {
-    return values[find(key)];
-  }
-
-  /**
-   * Grows the map. Allocates a new map of double the capacity, and
-   * fast-insert the old key-value pairs.
-   */
-  @SuppressWarnings("unchecked")
-  protected void grow() {
-    ObjectToFloatMap<K> that = new ObjectToFloatMap<K>(
-        this.capacity * 2);
-
-    // Iterates fast over the collection. Any valid pair is put into the new
-    // map without checking for duplicates or if there's enough space for
-    // it.
-    for (IndexIterator iterator = new IndexIterator(); iterator.hasNext();) {
-      int index = iterator.next();
-      that.prvt_put((K) this.keys[index], this.values[index]);
-    }
-
-    // Copy that's data into this.
-    this.capacity = that.capacity;
-    this.size = that.size;
-    this.firstEmpty = that.firstEmpty;
-    this.values = that.values;
-    this.keys = that.keys;
-    this.next = that.next;
-    this.baseHash = that.baseHash;
-    this.hashFactor = that.hashFactor;
-  }
-
-  /**
-   * 
-   * @return true if the map is empty. false otherwise.
-   */
-  public boolean isEmpty() {
-    return size == 0;
-  }
-
-  /**
-   * Returns a new iterator for the mapped floats.
-   */
-  public FloatIterator iterator() {
-    return new ValueIterator();
-  }
-
-  /** Returns an iterator on the map keys. */
-  public Iterator<K> keyIterator() {
-    return new KeyIterator();
-  }
-
-  /**
-   * Prints the baseHash array, used for debug purposes.
-   */
-  @SuppressWarnings("unused")
-  private String getBaseHashAsString() {
-    return Arrays.toString(baseHash);
-  }
-
-  /**
-   * Inserts the &lt;key,value&gt; pair into the map. If the key already exists,
-   * this method updates the mapped value to the given one, returning the old
-   * mapped value.
-   * 
-   * @return the old mapped value, or {@link Float#NaN} if the key didn't exist.
-   */
-  public float put(K key, float e) {
-    // Does key exists?
-    int index = find(key);
-
-    // Yes!
-    if (index != 0) {
-      // Set new data and exit.
-      float old = values[index];
-      values[index] = e;
-      return old;
-    }
-
-    // Is there enough room for a new pair?
-    if (size == capacity) {
-      // No? Than grow up!
-      grow();
-    }
-    
-    // Now that everything is set, the pair can be just put inside with no
-    // worries.
-    prvt_put(key, e);
-
-    return Float.NaN;
-  }
-
-  /**
-   * Removes a &lt;key,value&gt; pair from the map and returns the mapped value,
-   * or {@link Float#NaN} if the none existed.
-   * 
-   * @param key used to find the value to remove
-   * @return the removed value or {@link Float#NaN} if none existed.
-   */
-  public float remove(K key) {
-    int baseHashIndex = calcBaseHashIndex(key);
-    int index = findForRemove(key, baseHashIndex);
-    if (index != 0) {
-      // If it is the first in the collision list, we should promote its
-      // next colliding element.
-      if (prev == 0) {
-        baseHash[baseHashIndex] = next[index];
-      }
-
-      next[prev] = next[index];
-      next[index] = firstEmpty;
-      firstEmpty = index;
-      --size;
-      return values[index];
-    }
-
-    return Float.NaN;
-  }
-
-  /**
-   * @return number of pairs currently in the map
-   */
-  public int size() {
-    return this.size;
-  }
-
-  /**
-   * Translates the mapped pairs' values into an array of Objects
-   * 
-   * @return an object array of all the values currently in the map.
-   */
-  public float[] toArray() {
-    int j = -1;
-    float[] array = new float[size];
-
-    // Iterates over the values, adding them to the array.
-    for (FloatIterator iterator = iterator(); iterator.hasNext();) {
-      array[++j] = iterator.next();
-    }
-    return array;
-  }
-
-  /**
-   * Translates the mapped pairs' values into an array of T
-   * 
-   * @param a
-   *            the array into which the elements of the list are to be
-   *            stored, if it is big enough; otherwise, use as much space as it can.
-   * 
-   * @return an array containing the elements of the list
-   * 
-   */
-  public float[] toArray(float[] a) {
-    int j = 0;
-    // Iterates over the values, adding them to the array.
-    for (FloatIterator iterator = iterator(); j < a.length
-    && iterator.hasNext(); ++j) {
-      a[j] = iterator.next();
-    }
-    if (j < a.length) {
-      a[j] = Float.NaN;
-    }
-
-    return a;
-  }
-
-  @Override
-  public String toString() {
-    StringBuffer sb = new StringBuffer();
-    sb.append('{');
-    Iterator<K> keyIterator = keyIterator();
-    while (keyIterator.hasNext()) {
-      K key = keyIterator.next();
-      sb.append(key);
-      sb.append('=');
-      sb.append(get(key));
-      if (keyIterator.hasNext()) {
-        sb.append(',');
-        sb.append(' ');
-      }
-    }
-    sb.append('}');
-    return sb.toString();
-  }
-  
-  @Override
-  public int hashCode() {
-    return getClass().hashCode() ^ size();
-  }
-  
-  @SuppressWarnings("unchecked")
-  @Override
-  public boolean equals(Object o) {
-    ObjectToFloatMap<K> that = (ObjectToFloatMap<K>)o;
-    if (that.size() != this.size()) {
-      return false;
-    }
-    
-    Iterator<K> it = keyIterator();
-    while (it.hasNext()) {
-      K key = it.next();
-      float v1 = this.get(key);
-      float v2 = that.get(key);
-      if (Float.compare(v1, v2) != 0) {
-        return false;
-      }
-    }
-    return true;
-  }
-}
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/util/collections/ObjectToIntMap.java b/lucene/facet/src/java/org/apache/lucene/util/collections/ObjectToIntMap.java
deleted file mode 100644
index 673f347..0000000
--- a/lucene/facet/src/java/org/apache/lucene/util/collections/ObjectToIntMap.java
+++ /dev/null
@@ -1,622 +0,0 @@
-package org.apache.lucene.util.collections;
-
-import java.util.Arrays;
-import java.util.Iterator;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * An Array-based hashtable which maps Objects of generic type
- * T to primitive int values.<br>
- * The hashtable is constructed with a given capacity, or 16 as a default. In
- * case there's not enough room for new pairs, the hashtable grows. <br>
- * Capacity is adjusted to a power of 2, and there are 2 * capacity entries for
- * the hash.
- * 
- * The pre allocated arrays (for keys, values) are at length of capacity + 1,
- * when index 0 is used as 'Ground' or 'NULL'.<br>
- * 
- * The arrays are allocated ahead of hash operations, and form an 'empty space'
- * list, to which the key,value pair is allocated.
- * 
- * @lucene.experimental
- */
-public class ObjectToIntMap<K> {
-
-  /**
-   * Implements an IntIterator which iterates over all the allocated indexes.
-   */
-  private final class IndexIterator implements IntIterator {
-    /**
-     * The last used baseHashIndex. Needed for "jumping" from one hash entry
-     * to another.
-     */
-    private int baseHashIndex = 0;
-
-    /**
-     * The next not-yet-visited index.
-     */
-    private int index = 0;
-
-    /**
-     * Index of the last visited pair. Used in {@link #remove()}.
-     */
-    private int lastIndex = 0;
-
-    /**
-     * Create the Iterator, make <code>index</code> point to the "first"
-     * index which is not empty. If such does not exist (eg. the map is
-     * empty) it would be zero.
-     */
-    public IndexIterator() {
-      for (baseHashIndex = 0; baseHashIndex < baseHash.length; ++baseHashIndex) {
-        index = baseHash[baseHashIndex];
-        if (index != 0) {
-          break;
-        }
-      }
-    }
-
-    @Override
-    public boolean hasNext() {
-      return (index != 0);
-    }
-
-    @Override
-    public int next() {
-      // Save the last index visited
-      lastIndex = index;
-
-      // next the index
-      index = next[index];
-
-      // if the next index points to the 'Ground' it means we're done with
-      // the current hash entry and we need to jump to the next one. This
-      // is done until all the hash entries had been visited.
-      while (index == 0 && ++baseHashIndex < baseHash.length) {
-        index = baseHash[baseHashIndex];
-      }
-
-      return lastIndex;
-    }
-
-    @Override
-    @SuppressWarnings("unchecked")
-    public void remove() {
-      ObjectToIntMap.this.remove((K) keys[lastIndex]);
-    }
-
-  }
-
-  /**
-   * Implements an IntIterator, used for iteration over the map's keys.
-   */
-  private final class KeyIterator implements Iterator<K> {
-    private IntIterator iterator = new IndexIterator();
-
-    KeyIterator() { }
-    
-    @Override
-    public boolean hasNext() {
-      return iterator.hasNext();
-    }
-
-    @Override
-    @SuppressWarnings("unchecked")
-    public K next() {
-      return (K) keys[iterator.next()];
-    }
-
-    @Override
-    public void remove() {
-      iterator.remove();
-    }
-  }
-
-  /**
-   * Implements an Iterator of a generic type T used for iteration over the
-   * map's values.
-   */
-  private final class ValueIterator implements IntIterator {
-    private IntIterator iterator = new IndexIterator();
-
-    ValueIterator() {}
-    
-    @Override
-    public boolean hasNext() {
-      return iterator.hasNext();
-    }
-
-    @Override
-    public int next() {
-      return values[iterator.next()];
-    }
-
-    @Override
-    public void remove() {
-      iterator.remove();
-    }
-  }
-
-  /**
-   * Default capacity - in case no capacity was specified in the constructor
-   */
-  private static int defaultCapacity = 16;
-
-  /**
-   * Holds the base hash entries. if the capacity is 2^N, than the base hash
-   * holds 2^(N+1). It can hold
-   */
-  int[] baseHash;
-
-  /**
-   * The current capacity of the map. Always 2^N and never less than 16. We
-   * never use the zero index. It is needed to improve performance and is also
-   * used as "ground".
-   */
-  private int capacity;
-  /**
-   * All objects are being allocated at map creation. Those objects are "free"
-   * or empty. Whenever a new pair comes along, a pair is being "allocated" or
-   * taken from the free-linked list. as this is just a free list.
-   */
-  private int firstEmpty;
-
-  /**
-   * hashFactor is always (2^(N+1)) - 1. Used for faster hashing.
-   */
-  private int hashFactor;
-
-  /**
-   * This array holds the unique keys
-   */
-  Object[] keys;
-
-  /**
-   * In case of collisions, we implement a double linked list of the colliding
-   * hash's with the following next[] and prev[]. Those are also used to store
-   * the "empty" list.
-   */
-  int[] next;
-
-  private int prev;
-
-  /**
-   * Number of currently objects in the map.
-   */
-  private int size;
-
-  /**
-   * This array holds the values
-   */
-  int[] values;
-
-  /**
-   * Constructs a map with default capacity.
-   */
-  public ObjectToIntMap() {
-    this(defaultCapacity);
-  }
-
-  /**
-   * Constructs a map with given capacity. Capacity is adjusted to a native
-   * power of 2, with minimum of 16.
-   * 
-   * @param capacity
-   *            minimum capacity for the map.
-   */
-  public ObjectToIntMap(int capacity) {
-    this.capacity = 16;
-    // Minimum capacity is 16..
-    while (this.capacity < capacity) {
-      // Multiply by 2 as long as we're still under the requested capacity
-      this.capacity <<= 1;
-    }
-
-    // As mentioned, we use the first index (0) as 'Ground', so we need the
-    // length of the arrays to be one more than the capacity
-    int arrayLength = this.capacity + 1;
-
-    this.values = new int[arrayLength];
-    this.keys = new Object[arrayLength];
-    this.next = new int[arrayLength];
-
-    // Hash entries are twice as big as the capacity.
-    int baseHashSize = this.capacity << 1;
-
-    this.baseHash = new int[baseHashSize];
-
-    // The has factor is 2^M - 1 which is used as an "AND" hashing operator.
-    // {@link #calcBaseHash()}
-    this.hashFactor = baseHashSize - 1;
-
-    this.size = 0;
-
-    clear();
-  }
-
-  /**
-   * Adds a pair to the map. Takes the first empty position from the
-   * empty-linked-list's head - {@link #firstEmpty}.
-   * 
-   * New pairs are always inserted to baseHash, and are followed by the old
-   * colliding pair.
-   * 
-   * @param key
-   *            integer which maps the given Object
-   * @param e
-   *            element which is being mapped using the given key
-   */
-  private void prvt_put(K key, int e) {
-    // Hash entry to which the new pair would be inserted
-    int hashIndex = calcBaseHashIndex(key);
-
-    // 'Allocating' a pair from the "Empty" list.
-    int objectIndex = firstEmpty;
-
-    // Setting data
-    firstEmpty = next[firstEmpty];
-    values[objectIndex] = e;
-    keys[objectIndex] = key;
-
-    // Inserting the new pair as the first node in the specific hash entry
-    next[objectIndex] = baseHash[hashIndex];
-    baseHash[hashIndex] = objectIndex;
-
-    // Announcing a new pair was added!
-    ++size;
-  }
-
-  /**
-   * Calculating the baseHash index using the internal <code>hashFactor</code>.
-   */
-  protected int calcBaseHashIndex(K key) {
-    return key.hashCode() & hashFactor;
-  }
-
-  /**
-   * Empties the map. Generates the "Empty" space list for later allocation.
-   */
-  public void clear() {
-    // Clears the hash entries
-    Arrays.fill(this.baseHash, 0);
-
-    // Set size to zero
-    size = 0;
-
-    values[0] = Integer.MAX_VALUE;
-
-    // Mark all array entries as empty. This is done with
-    // <code>firstEmpty</code> pointing to the first valid index (1 as 0 is
-    // used as 'Ground').
-    firstEmpty = 1;
-
-    // And setting all the <code>next[i]</code> to point at
-    // <code>i+1</code>.
-    for (int i = 1; i < this.capacity;) {
-      next[i] = ++i;
-    }
-
-    // Surly, the last one should point to the 'Ground'.
-    next[this.capacity] = 0;
-  }
-
-  /**
-   * Checks if a given key exists in the map.
-   * 
-   * @param key
-   *            that is checked against the map data.
-   * @return true if the key exists in the map. false otherwise.
-   */
-  public boolean containsKey(K key) {
-    return find(key) != 0;
-  }
-
-  /**
-   * Checks if the given object exists in the map.<br>
-   * This method iterates over the collection, trying to find an equal object.
-   * 
-   * @param o
-   *            object that is checked against the map data.
-   * @return true if the object exists in the map (in .equals() meaning).
-   *         false otherwise.
-   */
-  public boolean containsValue(int o) {
-    for (IntIterator iterator = iterator(); iterator.hasNext();) {
-      if (o == iterator.next()) {
-        return true;
-      }
-    }
-    return false;
-  }
-
-  /**
-   * Find the actual index of a given key.
-   * 
-   * @return index of the key. zero if the key wasn't found.
-   */
-  protected int find(K key) {
-    // Calculate the hash entry.
-    int baseHashIndex = calcBaseHashIndex(key);
-
-    // Start from the hash entry.
-    int localIndex = baseHash[baseHashIndex];
-
-    // while the index does not point to the 'Ground'
-    while (localIndex != 0) {
-      // returns the index found in case of of a matching key.
-      if (keys[localIndex].equals(key)) {
-        return localIndex;
-      }
-
-      // next the local index
-      localIndex = next[localIndex];
-    }
-
-    // If we got this far, it could only mean we did not find the key we
-    // were asked for. return 'Ground' index.
-    return 0;
-  }
-
-  /**
-   * Find the actual index of a given key with it's baseHashIndex.<br>
-   * Some methods use the baseHashIndex. If those call {@link #find} there's
-   * no need to re-calculate that hash.
-   * 
-   * @return the index of the given key, or 0 as 'Ground' if the key wasn't
-   *         found.
-   */
-  private int findForRemove(K key, int baseHashIndex) {
-    // Start from the hash entry.
-    this.prev = 0;
-    int index = baseHash[baseHashIndex];
-
-    // while the index does not point to the 'Ground'
-    while (index != 0) {
-      // returns the index found in case of of a matching key.
-      if (keys[index].equals(key)) {
-        return index;
-      }
-
-      // next the local index
-      prev = index;
-      index = next[index];
-    }
-
-    // If we got this far, it could only mean we did not find the key we
-    // were asked for. return 'Ground' index.
-    this.prev = 0;
-    return 0;
-  }
-
-  /**
-   * Returns the int mapped with the given key.
-   * 
-   * @param key
-   *            int who's mapped object we're interested in.
-   * @return an object mapped by the given key. null if the key wasn't found.
-   */
-  public int get(K key) {
-    return values[find(key)];
-  }
-
-  /**
-   * Grows the map. Allocates a new map of double the capacity, and
-   * fast-insert the old key-value pairs.
-   */
-  @SuppressWarnings("unchecked")
-  protected void grow() {
-    ObjectToIntMap<K> that = new ObjectToIntMap<K>(
-        this.capacity * 2);
-
-    // Iterates fast over the collection. Any valid pair is put into the new
-    // map without checking for duplicates or if there's enough space for
-    // it.
-    for (IndexIterator iterator = new IndexIterator(); iterator.hasNext();) {
-      int index = iterator.next();
-      that.prvt_put((K) this.keys[index], this.values[index]);
-    }
-
-    // Copy that's data into this.
-    this.capacity = that.capacity;
-    this.size = that.size;
-    this.firstEmpty = that.firstEmpty;
-    this.values = that.values;
-    this.keys = that.keys;
-    this.next = that.next;
-    this.baseHash = that.baseHash;
-    this.hashFactor = that.hashFactor;
-  }
-
-  /**
-   * 
-   * @return true if the map is empty. false otherwise.
-   */
-  public boolean isEmpty() {
-    return size == 0;
-  }
-
-  /**
-   * Returns a new iterator for the mapped objects.
-   */
-  public IntIterator iterator() {
-    return new ValueIterator();
-  }
-
-  public Iterator<K> keyIterator() {
-    return new KeyIterator();
-  }
-
-  /**
-   * Prints the baseHash array, used for debug purposes.
-   */
-  @SuppressWarnings("unused")
-  private String getBaseHashAsString() {
-    return Arrays.toString(baseHash);
-  }
-
-  /**
-   * Inserts the &lt;key,value&gt; pair into the map. If the key already exists,
-   * this method updates the mapped value to the given one, returning the old
-   * mapped value.
-   * 
-   * @return the old mapped value, or 0 if the key didn't exist.
-   */
-  public int put(K key, int e) {
-    // Does key exists?
-    int index = find(key);
-
-    // Yes!
-    if (index != 0) {
-      // Set new data and exit.
-      int old = values[index];
-      values[index] = e;
-      return old;
-    }
-
-    // Is there enough room for a new pair?
-    if (size == capacity) {
-      // No? Than grow up!
-      grow();
-    }
-
-    // Now that everything is set, the pair can be just put inside with no
-    // worries.
-    prvt_put(key, e);
-
-    return 0;
-  }
-
-  /**
-   * Removes a &lt;key,value&gt; pair from the map and returns the mapped value,
-   * or 0 if the none existed.
-   * 
-   * @param key used to find the value to remove
-   * @return the removed value or 0 if none existed.
-   */
-  public int remove(K key) {
-    int baseHashIndex = calcBaseHashIndex(key);
-    int index = findForRemove(key, baseHashIndex);
-    if (index != 0) {
-      // If it is the first in the collision list, we should promote its
-      // next colliding element.
-      if (prev == 0) {
-        baseHash[baseHashIndex] = next[index];
-      }
-
-      next[prev] = next[index];
-      next[index] = firstEmpty;
-      firstEmpty = index;
-      --size;
-      return values[index];
-    }
-
-    return 0;
-  }
-
-  /**
-   * @return number of pairs currently in the map
-   */
-  public int size() {
-    return this.size;
-  }
-
-  /**
-   * Translates the mapped pairs' values into an array of Objects
-   * 
-   * @return an object array of all the values currently in the map.
-   */
-  public int[] toArray() {
-    int j = -1;
-    int[] array = new int[size];
-
-    // Iterates over the values, adding them to the array.
-    for (IntIterator iterator = iterator(); iterator.hasNext();) {
-      array[++j] = iterator.next();
-    }
-    return array;
-  }
-
-  /**
-   * Translates the mapped pairs' values into an array of T
-   * 
-   * @param a
-   *            the array into which the elements of the list are to be
-   *            stored, if it is big enough; otherwise, use as much space as it can.
-   * 
-   * @return an array containing the elements of the list
-   * 
-   */
-  public int[] toArray(int[] a) {
-    int j = 0;
-    // Iterates over the values, adding them to the array.
-    for (IntIterator iterator = iterator(); j < a.length
-    && iterator.hasNext(); ++j) {
-      a[j] = iterator.next();
-    }
-    if (j < a.length) {
-      a[j] = Integer.MAX_VALUE;
-    }
-
-    return a;
-  }
-
-  @Override
-  public String toString() {
-    StringBuffer sb = new StringBuffer();
-    sb.append('{');
-    Iterator<K> keyIterator = keyIterator();
-    while (keyIterator.hasNext()) {
-      K key = keyIterator.next();
-      sb.append(key);
-      sb.append('=');
-      sb.append(get(key));
-      if (keyIterator.hasNext()) {
-        sb.append(',');
-        sb.append(' ');
-      }
-    }
-    sb.append('}');
-    return sb.toString();
-  }
-  
-  @Override
-  public int hashCode() {
-    return getClass().hashCode() ^ size();
-  }
-  
-  @SuppressWarnings("unchecked")
-  @Override
-  public boolean equals(Object o) {
-    ObjectToIntMap<K> that = (ObjectToIntMap<K>)o;
-    if (that.size() != this.size()) {
-      return false;
-    }
-    
-    Iterator<K> it = keyIterator();
-    while (it.hasNext()) {
-      K key = it.next();
-      int v1 = this.get(key);
-      int v2 = that.get(key);
-      if (Float.compare(v1, v2) != 0) {
-        return false;
-      }
-    }
-    return true;
-  }
-}
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/util/collections/package.html b/lucene/facet/src/java/org/apache/lucene/util/collections/package.html
deleted file mode 100644
index 4686096..0000000
--- a/lucene/facet/src/java/org/apache/lucene/util/collections/package.html
+++ /dev/null
@@ -1,24 +0,0 @@
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-<title>Collections</title>
-</head>
-<body>
-	Various optimized Collections implementations.
-</body>
-</html>
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/util/encoding/ChunksIntEncoder.java b/lucene/facet/src/java/org/apache/lucene/util/encoding/ChunksIntEncoder.java
deleted file mode 100644
index 8829f2b..0000000
--- a/lucene/facet/src/java/org/apache/lucene/util/encoding/ChunksIntEncoder.java
+++ /dev/null
@@ -1,115 +0,0 @@
-package org.apache.lucene.util.encoding;
-
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IntsRef;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * An {@link IntEncoder} which encodes values in chunks. Implementations of this
- * class assume the data which needs encoding consists of small, consecutive
- * values, and therefore the encoder is able to compress them better. You can
- * read more on the two implementations {@link FourFlagsIntEncoder} and
- * {@link EightFlagsIntEncoder}.
- * <p>
- * Extensions of this class need to implement {@link #encode(IntsRef, BytesRef)}
- * in order to build the proper indicator (flags). When enough values were
- * accumulated (typically the batch size), extensions can call
- * {@link #encodeChunk(BytesRef)} to flush the indicator and the rest of the
- * values.
- * <p>
- * <b>NOTE:</b> flags encoders do not accept values &le; 0 (zero) in their
- * {@link #encode(IntsRef, BytesRef)}. For performance reasons they do not check
- * that condition, however if such value is passed the result stream may be
- * corrupt or an exception will be thrown. Also, these encoders perform the best
- * when there are many consecutive small values (depends on the encoder
- * implementation). If that is not the case, the encoder will occupy 1 more byte
- * for every <i>batch</i> number of integers, over whatever
- * {@link VInt8IntEncoder} would have occupied. Therefore make sure to check
- * whether your data fits into the conditions of the specific encoder.
- * <p>
- * For the reasons mentioned above, these encoders are usually chained with
- * {@link UniqueValuesIntEncoder} and {@link DGapIntEncoder}.
- * 
- * @lucene.experimental
- */
-public abstract class ChunksIntEncoder extends IntEncoder {
-  
-  /** Holds the values which must be encoded, outside the indicator. */
-  protected final IntsRef encodeQueue;
-  
-  /** Represents bits flag byte. */
-  protected int indicator = 0;
-  
-  /** Counts the current ordinal of the encoded value. */
-  protected byte ordinal = 0;
-  
-  protected ChunksIntEncoder(int chunkSize) {
-    encodeQueue = new IntsRef(chunkSize);
-  }
-  
-  /**
-   * Encodes the values of the current chunk. First it writes the indicator, and
-   * then it encodes the values outside the indicator.
-   */
-  protected void encodeChunk(BytesRef buf) {
-    // ensure there's enough room in the buffer
-    int maxBytesRequired = buf.length + 1 + encodeQueue.length * 4; /* indicator + at most 4 bytes per positive VInt */
-    if (buf.bytes.length < maxBytesRequired) {
-      buf.grow(maxBytesRequired);
-    }
-    
-    buf.bytes[buf.length++] = ((byte) indicator);
-    for (int i = 0; i < encodeQueue.length; i++) {
-      // it is better if the encoding is inlined like so, and not e.g.
-      // in a utility method
-      int value = encodeQueue.ints[i];
-      if ((value & ~0x7F) == 0) {
-        buf.bytes[buf.length] = (byte) value;
-        buf.length++;
-      } else if ((value & ~0x3FFF) == 0) {
-        buf.bytes[buf.length] = (byte) (0x80 | ((value & 0x3F80) >> 7));
-        buf.bytes[buf.length + 1] = (byte) (value & 0x7F);
-        buf.length += 2;
-      } else if ((value & ~0x1FFFFF) == 0) {
-        buf.bytes[buf.length] = (byte) (0x80 | ((value & 0x1FC000) >> 14));
-        buf.bytes[buf.length + 1] = (byte) (0x80 | ((value & 0x3F80) >> 7));
-        buf.bytes[buf.length + 2] = (byte) (value & 0x7F);
-        buf.length += 3;
-      } else if ((value & ~0xFFFFFFF) == 0) {
-        buf.bytes[buf.length] = (byte) (0x80 | ((value & 0xFE00000) >> 21));
-        buf.bytes[buf.length + 1] = (byte) (0x80 | ((value & 0x1FC000) >> 14));
-        buf.bytes[buf.length + 2] = (byte) (0x80 | ((value & 0x3F80) >> 7));
-        buf.bytes[buf.length + 3] = (byte) (value & 0x7F);
-        buf.length += 4;
-      } else {
-        buf.bytes[buf.length] = (byte) (0x80 | ((value & 0xF0000000) >> 28));
-        buf.bytes[buf.length + 1] = (byte) (0x80 | ((value & 0xFE00000) >> 21));
-        buf.bytes[buf.length + 2] = (byte) (0x80 | ((value & 0x1FC000) >> 14));
-        buf.bytes[buf.length + 3] = (byte) (0x80 | ((value & 0x3F80) >> 7));
-        buf.bytes[buf.length + 4] = (byte) (value & 0x7F);
-        buf.length += 5;
-      }
-    }
-    
-    ordinal = 0;
-    indicator = 0;
-    encodeQueue.length = 0;
-  }
-  
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/util/encoding/DGapIntDecoder.java b/lucene/facet/src/java/org/apache/lucene/util/encoding/DGapIntDecoder.java
deleted file mode 100644
index 5e90820..0000000
--- a/lucene/facet/src/java/org/apache/lucene/util/encoding/DGapIntDecoder.java
+++ /dev/null
@@ -1,52 +0,0 @@
-package org.apache.lucene.util.encoding;
-
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IntsRef;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * An {@link IntDecoder} which wraps another decoder and reverts the d-gap that
- * was encoded by {@link DGapIntEncoder}.
- * 
- * @lucene.experimental
- */
-public final class DGapIntDecoder extends IntDecoder {
-
-  private final IntDecoder decoder;
-
-  public DGapIntDecoder(IntDecoder decoder) {
-    this.decoder = decoder;
-  }
-
-  @Override
-  public void decode(BytesRef buf, IntsRef values) {
-    decoder.decode(buf, values);
-    int prev = 0;
-    for (int i = 0; i < values.length; i++) {
-      values.ints[i] += prev;
-      prev = values.ints[i];
-    }
-  }
-
-  @Override
-  public String toString() {
-    return "DGap(" + decoder.toString() + ")";
-  }
-
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/util/encoding/DGapIntEncoder.java b/lucene/facet/src/java/org/apache/lucene/util/encoding/DGapIntEncoder.java
deleted file mode 100644
index 5e8ca5d..0000000
--- a/lucene/facet/src/java/org/apache/lucene/util/encoding/DGapIntEncoder.java
+++ /dev/null
@@ -1,67 +0,0 @@
-package org.apache.lucene.util.encoding;
-
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IntsRef;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * An {@link IntEncoderFilter} which encodes the gap between the given values,
- * rather than the values themselves. This encoder usually yields better
- * encoding performance space-wise (i.e., the final encoded values consume less
- * space) if the values are 'close' to each other.
- * <p>
- * <b>NOTE:</b> this encoder assumes the values are given to
- * {@link #encode(IntsRef, BytesRef)} in an ascending sorted manner, which ensures only
- * positive values are encoded and thus yields better performance. If you are
- * not sure whether the values are sorted or not, it is possible to chain this
- * encoder with {@link SortingIntEncoder} to ensure the values will be
- * sorted before encoding.
- * 
- * @lucene.experimental
- */
-public final class DGapIntEncoder extends IntEncoderFilter {
-
-  /** Initializes with the given encoder. */
-  public DGapIntEncoder(IntEncoder encoder) {
-    super(encoder);
-  }
-
-  @Override
-  public void encode(IntsRef values, BytesRef buf) {
-    int prev = 0;
-    int upto = values.offset + values.length;
-    for (int i = values.offset; i < upto; i++) {
-      int tmp = values.ints[i];
-      values.ints[i] -= prev;
-      prev = tmp;
-    }
-    encoder.encode(values, buf);
-  }
-
-  @Override
-  public IntDecoder createMatchingDecoder() {
-    return new DGapIntDecoder(encoder.createMatchingDecoder());
-  }
-  
-  @Override
-  public String toString() {
-    return "DGap(" + encoder.toString() + ")";
-  }
-  
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/util/encoding/DGapVInt8IntDecoder.java b/lucene/facet/src/java/org/apache/lucene/util/encoding/DGapVInt8IntDecoder.java
deleted file mode 100644
index d5a5332..0000000
--- a/lucene/facet/src/java/org/apache/lucene/util/encoding/DGapVInt8IntDecoder.java
+++ /dev/null
@@ -1,67 +0,0 @@
-package org.apache.lucene.util.encoding;
-
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IntsRef;
-import org.apache.lucene.util.RamUsageEstimator;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Decodes values encoded by {@link DGapVInt8IntDecoder}.
- * 
- * @lucene.experimental
- */
-public final class DGapVInt8IntDecoder extends IntDecoder {
-
-  @Override
-  public void decode(BytesRef buf, IntsRef values) {
-    values.offset = values.length = 0;
-
-    // grow the buffer up front, even if by a large number of values (buf.length)
-    // that saves the need to check inside the loop for every decoded value if
-    // the buffer needs to grow.
-    if (values.ints.length < buf.length) {
-      values.ints = new int[ArrayUtil.oversize(buf.length, RamUsageEstimator.NUM_BYTES_INT)];
-    }
-
-    // it is better if the decoding is inlined like so, and not e.g.
-    // in a utility method
-    int upto = buf.offset + buf.length;
-    int value = 0;
-    int offset = buf.offset;
-    int prev = 0;
-    while (offset < upto) {
-      byte b = buf.bytes[offset++];
-      if (b >= 0) {
-        values.ints[values.length] = ((value << 7) | b) + prev;
-        value = 0;
-        prev = values.ints[values.length];
-        values.length++;
-      } else {
-        value = (value << 7) | (b & 0x7F);
-      }
-    }
-  }
-
-  @Override
-  public String toString() {
-    return "DGapVInt8";
-  }
-
-} 
diff --git a/lucene/facet/src/java/org/apache/lucene/util/encoding/DGapVInt8IntEncoder.java b/lucene/facet/src/java/org/apache/lucene/util/encoding/DGapVInt8IntEncoder.java
deleted file mode 100644
index a28328e..0000000
--- a/lucene/facet/src/java/org/apache/lucene/util/encoding/DGapVInt8IntEncoder.java
+++ /dev/null
@@ -1,89 +0,0 @@
-package org.apache.lucene.util.encoding;
-
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IntsRef;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * An {@link IntEncoder} which implements variable length encoding for the gap
- * between values. It's a specialized form of the combination of
- * {@link DGapIntEncoder} and {@link VInt8IntEncoder}.
- * 
- * @see VInt8IntEncoder
- * @see DGapIntEncoder
- * 
- * @lucene.experimental
- */
-public final class DGapVInt8IntEncoder extends IntEncoder {
-
-  @Override
-  public void encode(IntsRef values, BytesRef buf) {
-    buf.offset = buf.length = 0;
-    int maxBytesNeeded = 5 * values.length; // at most 5 bytes per VInt
-    if (buf.bytes.length < maxBytesNeeded) {
-      buf.grow(maxBytesNeeded);
-    }
-    
-    int upto = values.offset + values.length;
-    int prev = 0;
-    for (int i = values.offset; i < upto; i++) {
-      // it is better if the encoding is inlined like so, and not e.g.
-      // in a utility method
-      int value = values.ints[i] - prev;
-      if ((value & ~0x7F) == 0) {
-        buf.bytes[buf.length] = (byte) value;
-        buf.length++;
-      } else if ((value & ~0x3FFF) == 0) {
-        buf.bytes[buf.length] = (byte) (0x80 | ((value & 0x3F80) >> 7));
-        buf.bytes[buf.length + 1] = (byte) (value & 0x7F);
-        buf.length += 2;
-      } else if ((value & ~0x1FFFFF) == 0) {
-        buf.bytes[buf.length] = (byte) (0x80 | ((value & 0x1FC000) >> 14));
-        buf.bytes[buf.length + 1] = (byte) (0x80 | ((value & 0x3F80) >> 7));
-        buf.bytes[buf.length + 2] = (byte) (value & 0x7F);
-        buf.length += 3;
-      } else if ((value & ~0xFFFFFFF) == 0) {
-        buf.bytes[buf.length] = (byte) (0x80 | ((value & 0xFE00000) >> 21));
-        buf.bytes[buf.length + 1] = (byte) (0x80 | ((value & 0x1FC000) >> 14));
-        buf.bytes[buf.length + 2] = (byte) (0x80 | ((value & 0x3F80) >> 7));
-        buf.bytes[buf.length + 3] = (byte) (value & 0x7F);
-        buf.length += 4;
-      } else {
-        buf.bytes[buf.length] = (byte) (0x80 | ((value & 0xF0000000) >> 28));
-        buf.bytes[buf.length + 1] = (byte) (0x80 | ((value & 0xFE00000) >> 21));
-        buf.bytes[buf.length + 2] = (byte) (0x80 | ((value & 0x1FC000) >> 14));
-        buf.bytes[buf.length + 3] = (byte) (0x80 | ((value & 0x3F80) >> 7));
-        buf.bytes[buf.length + 4] = (byte) (value & 0x7F);
-        buf.length += 5;
-      }
-      prev = values.ints[i];
-    }
-  }
-
-  @Override
-  public IntDecoder createMatchingDecoder() {
-    return new DGapVInt8IntDecoder();
-  }
-
-  @Override
-  public String toString() {
-    return "DGapVInt8";
-  }
-
-} 
diff --git a/lucene/facet/src/java/org/apache/lucene/util/encoding/EightFlagsIntDecoder.java b/lucene/facet/src/java/org/apache/lucene/util/encoding/EightFlagsIntDecoder.java
deleted file mode 100644
index 317185f..0000000
--- a/lucene/facet/src/java/org/apache/lucene/util/encoding/EightFlagsIntDecoder.java
+++ /dev/null
@@ -1,92 +0,0 @@
-package org.apache.lucene.util.encoding;
-
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IntsRef;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Decodes values encoded with {@link EightFlagsIntEncoder}.
- * 
- * @lucene.experimental
- */
-public class EightFlagsIntDecoder extends IntDecoder {
-
-  /*
-   * Holds all combinations of <i>indicator</i> for fast decoding (saves time
-   * on real-time bit manipulation)
-   */
-  private static final byte[][] DECODE_TABLE = new byte[256][8];
-
-  /** Generating all combinations of <i>indicator</i> into separate flags. */
-  static {
-    for (int i = 256; i != 0;) {
-      --i;
-      for (int j = 8; j != 0;) {
-        --j;
-        DECODE_TABLE[i][j] = (byte) ((i >>> j) & 0x1);
-      }
-    }
-  }
-
-  @Override
-  public void decode(BytesRef buf, IntsRef values) {
-    values.offset = values.length = 0;
-    int upto = buf.offset + buf.length;
-    int offset = buf.offset;
-    while (offset < upto) {
-      // read indicator
-      int indicator = buf.bytes[offset++] & 0xFF;
-      int ordinal = 0;
-
-      int capacityNeeded = values.length + 8;
-      if (values.ints.length < capacityNeeded) {
-        values.grow(capacityNeeded);
-      }
-
-      // process indicator, until we read 8 values, or end-of-buffer
-      while (ordinal != 8) {
-        if (DECODE_TABLE[indicator][ordinal++] == 0) {
-          if (offset == upto) { // end of buffer
-            return;
-          }
-          // it is better if the decoding is inlined like so, and not e.g.
-          // in a utility method
-          int value = 0;
-          while (true) {
-            byte b = buf.bytes[offset++];
-            if (b >= 0) {
-              values.ints[values.length++] = ((value << 7) | b) + 2;
-              break;
-            } else {
-              value = (value << 7) | (b & 0x7F);
-            }
-          }
-        } else {
-          values.ints[values.length++] = 1;
-        }
-      }
-    }
-  }
-
-  @Override
-  public String toString() {
-    return "EightFlags(VInt8)";
-  }
-
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/util/encoding/EightFlagsIntEncoder.java b/lucene/facet/src/java/org/apache/lucene/util/encoding/EightFlagsIntEncoder.java
deleted file mode 100644
index 812d86d..0000000
--- a/lucene/facet/src/java/org/apache/lucene/util/encoding/EightFlagsIntEncoder.java
+++ /dev/null
@@ -1,96 +0,0 @@
-package org.apache.lucene.util.encoding;
-
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IntsRef;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * A {@link ChunksIntEncoder} which encodes data in chunks of 8. Every group
- * starts with a single byte (called indicator) which represents 8 - 1 bit
- * flags, where the value:
- * <ul>
- * <li>1 means the encoded value is '1'
- * <li>0 means the value is encoded using {@link VInt8IntEncoder}, and the
- * encoded bytes follow the indicator.<br>
- * Since value 0 is illegal, and 1 is encoded in the indicator, the actual value
- * that is encoded is <code>value-2</code>, which saves some more bits.
- * </ul>
- * Encoding example:
- * <ul>
- * <li>Original values: 6, 16, 5, 9, 7, 1
- * <li>After sorting: 1, 5, 6, 7, 9, 16
- * <li>D-Gap computing: 1, 4, 1, 1, 2, 5 (so far - done by
- * {@link DGapIntEncoder})
- * <li>Encoding: 1,0,1,1,0,0,0,0 as the indicator, by 2 (4-2), 0 (2-2), 3 (5-2).
- * <li>Binary encode: <u>0 | 0 | 0 | 0 | 1 | 1 | 0 | 1</u> 00000010 00000000
- * 00000011 (indicator is <u>underlined</u>).<br>
- * <b>NOTE:</b> the order of the values in the indicator is lsb &rArr; msb,
- * which allows for more efficient decoding.
- * </ul>
- * 
- * @lucene.experimental
- */
-public class EightFlagsIntEncoder extends ChunksIntEncoder {
-
-  /*
-   * Holds all combinations of <i>indicator</i> flags for fast encoding (saves
-   * time on bit manipulation at encode time)
-   */
-  private static final byte[] ENCODE_TABLE = new byte[] { 0x1, 0x2, 0x4, 0x8, 0x10, 0x20, 0x40, (byte) 0x80 };
-
-  public EightFlagsIntEncoder() {
-    super(8);
-  }
-
-  @Override
-  public void encode(IntsRef values, BytesRef buf) {
-    buf.offset = buf.length = 0;
-    int upto = values.offset + values.length;
-    for (int i = values.offset; i < upto; i++) {
-      int value = values.ints[i];
-      if (value == 1) {
-        indicator |= ENCODE_TABLE[ordinal];
-      } else {
-        encodeQueue.ints[encodeQueue.length++] = value - 2;
-      }
-      ++ordinal;
-      
-      // encode the chunk and the indicator
-      if (ordinal == 8) {
-        encodeChunk(buf);
-      }
-    }
-    
-    // encode remaining values
-    if (ordinal != 0) {
-      encodeChunk(buf);
-    }
-  }
-
-  @Override
-  public IntDecoder createMatchingDecoder() {
-    return new EightFlagsIntDecoder();
-  }
-
-  @Override
-  public String toString() {
-    return "EightFlags(VInt)";
-  }
-
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/util/encoding/FourFlagsIntDecoder.java b/lucene/facet/src/java/org/apache/lucene/util/encoding/FourFlagsIntDecoder.java
deleted file mode 100644
index 216b806..0000000
--- a/lucene/facet/src/java/org/apache/lucene/util/encoding/FourFlagsIntDecoder.java
+++ /dev/null
@@ -1,92 +0,0 @@
-package org.apache.lucene.util.encoding;
-
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IntsRef;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Decodes values encoded with {@link FourFlagsIntEncoder}.
- * 
- * @lucene.experimental
- */
-public class FourFlagsIntDecoder extends IntDecoder {
-
-  /**
-   * Holds all combinations of <i>indicator</i> for fast decoding (saves time
-   * on real-time bit manipulation)
-   */
-  private final static byte[][] DECODE_TABLE = new byte[256][4];
-
-  /** Generating all combinations of <i>indicator</i> into separate flags. */
-  static {
-    for (int i = 256; i != 0;) {
-      --i;
-      for (int j = 4; j != 0;) {
-        --j;
-        DECODE_TABLE[i][j] = (byte) ((i >>> (j << 1)) & 0x3);
-      }
-    }
-  }
-
-  @Override
-  public void decode(BytesRef buf, IntsRef values) {
-    values.offset = values.length = 0;
-    int upto = buf.offset + buf.length;
-    int offset = buf.offset;
-    while (offset < upto) {
-      // read indicator
-      int indicator = buf.bytes[offset++] & 0xFF;
-      int ordinal = 0;
-      
-      int capacityNeeded = values.length + 4;
-      if (values.ints.length < capacityNeeded) {
-        values.grow(capacityNeeded);
-      }
-      
-      while (ordinal != 4) {
-        byte decodeVal = DECODE_TABLE[indicator][ordinal++];
-        if (decodeVal == 0) {
-          if (offset == upto) { // end of buffer
-            return;
-          }
-          // it is better if the decoding is inlined like so, and not e.g.
-          // in a utility method
-          int value = 0;
-          while (true) {
-            byte b = buf.bytes[offset++];
-            if (b >= 0) {
-              values.ints[values.length++] = ((value << 7) | b) + 4;
-              break;
-            } else {
-              value = (value << 7) | (b & 0x7F);
-            }
-          }
-        } else {
-          values.ints[values.length++] = decodeVal;
-        }
-      }
-    }
-  }
-
-  @Override
-  public String toString() {
-    return "FourFlags(VInt)";
-  }
-
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/util/encoding/FourFlagsIntEncoder.java b/lucene/facet/src/java/org/apache/lucene/util/encoding/FourFlagsIntEncoder.java
deleted file mode 100644
index fd82265..0000000
--- a/lucene/facet/src/java/org/apache/lucene/util/encoding/FourFlagsIntEncoder.java
+++ /dev/null
@@ -1,102 +0,0 @@
-package org.apache.lucene.util.encoding;
-
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IntsRef;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * A {@link ChunksIntEncoder} which encodes values in chunks of 4. Every group
- * starts with a single byte (called indicator) which represents 4 - 2 bit
- * flags, where the values:
- * <ul>
- * <li>1, 2 or 3 mean the encoded value is '1', '2' or '3' respectively.
- * <li>0 means the value is encoded using {@link VInt8IntEncoder}, and the
- * encoded bytes follow the indicator.<br>
- * Since value 0 is illegal, and 1-3 are encoded in the indicator, the actual
- * value that is encoded is <code>value-4</code>, which saves some more bits.
- * </ul>
- * Encoding example:
- * <ul>
- * <li>Original values: 6, 16, 5, 9, 7, 1, 11
- * <li>After sorting: 1, 5, 6, 7, 9, 11, 16
- * <li>D-Gap computing: 1, 4, 1, 1, 2, 5 (so far - done by
- * {@link DGapIntEncoder})
- * <li>Encoding: 1,0,1,1 as the first indicator, followed by 0 (4-4), than
- * 2,0,0,0 as the second indicator, followed by 1 (5-4) encoded with.
- * <li>Binary encode: <u>01 | 01 | 00 | 01</u> 00000000 <u>00 | 00 | 00 | 10</u>
- * 00000001 (indicators are <u>underlined</u>).<br>
- * <b>NOTE:</b> the order of the values in the indicator is lsb &rArr; msb,
- * which allows for more efficient decoding.
- * </ul>
- * 
- * @lucene.experimental
- */
-public class FourFlagsIntEncoder extends ChunksIntEncoder {
-
-  /*
-   * Holds all combinations of <i>indicator</i> flags for fast encoding (saves
-   * time on bit manipulation @ encode time)
-   */
-  private static final byte[][] ENCODE_TABLE = new byte[][] {
-    new byte[] { 0x00, 0x00, 0x00, 0x00 },
-    new byte[] { 0x01, 0x04, 0x10, 0x40 },
-    new byte[] { 0x02, 0x08, 0x20, (byte) 0x80 },
-    new byte[] { 0x03, 0x0C, 0x30, (byte) 0xC0 },
-  };
-
-  public FourFlagsIntEncoder() {
-    super(4);
-  }
-
-  @Override
-  public void encode(IntsRef values, BytesRef buf) {
-    buf.offset = buf.length = 0;
-    int upto = values.offset + values.length;
-    for (int i = values.offset; i < upto; i++) {
-      int value = values.ints[i];
-      if (value <= 3) {
-        indicator |= ENCODE_TABLE[value][ordinal];
-      } else {
-        encodeQueue.ints[encodeQueue.length++] = value - 4;
-      }
-      ++ordinal;
-      
-      // encode the chunk and the indicator
-      if (ordinal == 4) {
-        encodeChunk(buf);
-      }
-    }
-    
-    // encode remaining values
-    if (ordinal != 0) {
-      encodeChunk(buf);
-    }
-  }
-
-  @Override
-  public IntDecoder createMatchingDecoder() {
-    return new FourFlagsIntDecoder();
-  }
-
-  @Override
-  public String toString() {
-    return "FourFlags(VInt)";
-  }
-
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/util/encoding/IntDecoder.java b/lucene/facet/src/java/org/apache/lucene/util/encoding/IntDecoder.java
deleted file mode 100644
index e89c755..0000000
--- a/lucene/facet/src/java/org/apache/lucene/util/encoding/IntDecoder.java
+++ /dev/null
@@ -1,36 +0,0 @@
-package org.apache.lucene.util.encoding;
-
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IntsRef;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Decodes integers from a set {@link BytesRef}.
- * 
- * @lucene.experimental
- */
-public abstract class IntDecoder {
-  
-  /**
-   * Decodes the values from the buffer into the given {@link IntsRef}. Note
-   * that {@code values.offset} and {@code values.length} are set to 0.
-   */
-  public abstract void decode(BytesRef buf, IntsRef values);
-
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/util/encoding/IntEncoder.java b/lucene/facet/src/java/org/apache/lucene/util/encoding/IntEncoder.java
deleted file mode 100644
index 64e2878..0000000
--- a/lucene/facet/src/java/org/apache/lucene/util/encoding/IntEncoder.java
+++ /dev/null
@@ -1,46 +0,0 @@
-package org.apache.lucene.util.encoding;
-
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IntsRef;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Encodes integers to a set {@link BytesRef}. For convenience, each encoder
- * implements {@link #createMatchingDecoder()} for easy access to the matching
- * decoder.
- * 
- * @lucene.experimental
- */
-public abstract class IntEncoder {
-
-  public IntEncoder() {}
-
-  /**
-   * Encodes the values to the given buffer. Note that the buffer's offset and
-   * length are set to 0.
-   */
-  public abstract void encode(IntsRef values, BytesRef buf);
-
-  /**
-   * Returns an {@link IntDecoder} which can decode the values that were encoded
-   * with this encoder.
-   */
-  public abstract IntDecoder createMatchingDecoder();
-  
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/util/encoding/IntEncoderFilter.java b/lucene/facet/src/java/org/apache/lucene/util/encoding/IntEncoderFilter.java
deleted file mode 100644
index c1fa04b..0000000
--- a/lucene/facet/src/java/org/apache/lucene/util/encoding/IntEncoderFilter.java
+++ /dev/null
@@ -1,34 +0,0 @@
-package org.apache.lucene.util.encoding;
-
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * An abstract implementation of {@link IntEncoder} which wraps another encoder.
- * 
- * @lucene.experimental
- */
-public abstract class IntEncoderFilter extends IntEncoder {
-
-  protected final IntEncoder encoder;
-
-  protected IntEncoderFilter(IntEncoder encoder) {
-    this.encoder = encoder;
-  }
-
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/util/encoding/NOnesIntDecoder.java b/lucene/facet/src/java/org/apache/lucene/util/encoding/NOnesIntDecoder.java
deleted file mode 100644
index 37e52e9..0000000
--- a/lucene/facet/src/java/org/apache/lucene/util/encoding/NOnesIntDecoder.java
+++ /dev/null
@@ -1,86 +0,0 @@
-package org.apache.lucene.util.encoding;
-
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IntsRef;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Decodes values encoded encoded with {@link NOnesIntEncoder}.
- * 
- * @lucene.experimental
- */
-public class NOnesIntDecoder extends FourFlagsIntDecoder {
-
-  // Number of consecutive '1's to generate upon decoding a '2'
-  private final int n;
-  private final IntsRef internalBuffer;
-  
-  /**
-   * Constructs a decoder with a given N (Number of consecutive '1's which are
-   * translated into a single target value '2'.
-   */
-  public NOnesIntDecoder(int n) {
-    this.n = n;
-    // initial size (room for 100 integers)
-    internalBuffer = new IntsRef(100);
-  }
-
-  @Override
-  public void decode(BytesRef buf, IntsRef values) {
-    values.offset = values.length = 0;
-    internalBuffer.length = 0;
-    super.decode(buf, internalBuffer);
-    if (values.ints.length < internalBuffer.length) {
-      // need space for internalBuffer.length to internalBuffer.length*N,
-      // grow mildly at first
-      values.grow(internalBuffer.length * n/2);
-    }
-    
-    for (int i = 0; i < internalBuffer.length; i++) {
-      int decode = internalBuffer.ints[i];
-      if (decode == 1) {
-        if (values.length == values.ints.length) {
-          values.grow(values.length + 10); // grow by few items, however not too many
-        }
-        // 1 is 1
-        values.ints[values.length++] = 1;
-      } else if (decode == 2) {
-        if (values.length + n >= values.ints.length) {
-          values.grow(values.length + n); // grow by few items, however not too many
-        }
-        // '2' means N 1's
-        for (int j = 0; j < n; j++) {
-          values.ints[values.length++] = 1;
-        }
-      } else {
-        if (values.length == values.ints.length) {
-          values.grow(values.length + 10); // grow by few items, however not too many
-        }
-        // any other value is val-1
-        values.ints[values.length++] = decode - 1;
-      }
-    }
-  }
-
-  @Override
-  public String toString() {
-    return "NOnes(" + n + ") (" + super.toString() + ")";
-  }
-
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/util/encoding/NOnesIntEncoder.java b/lucene/facet/src/java/org/apache/lucene/util/encoding/NOnesIntEncoder.java
deleted file mode 100644
index 5a705c5..0000000
--- a/lucene/facet/src/java/org/apache/lucene/util/encoding/NOnesIntEncoder.java
+++ /dev/null
@@ -1,114 +0,0 @@
-package org.apache.lucene.util.encoding;
-
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IntsRef;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * A variation of {@link FourFlagsIntEncoder} which translates the data as
- * follows:
- * <ul>
- * <li>Values &ge; 2 are trnalsated to <code>value+1</code> (2 &rArr; 3, 3
- * &rArr; 4 and so forth).
- * <li>Any <code>N</code> occurrences of 1 are encoded as a single 2.
- * <li>Otherwise, each 1 is encoded as 1.
- * </ul>
- * <p>
- * Encoding examples:
- * <ul>
- * <li>N = 4: the data 1,1,1,1,1 is translated to: 2, 1
- * <li>N = 3: the data 1,2,3,4,1,1,1,1,5 is translated to 1,3,4,5,2,1,6
- * </ul>
- * <b>NOTE:</b> this encoder does not support values &le; 0 and
- * {@link Integer#MAX_VALUE}. 0 is not supported because it's not supported by
- * {@link FourFlagsIntEncoder} and {@link Integer#MAX_VALUE} because this
- * encoder translates N to N+1, which will cause an overflow and
- * {@link Integer#MAX_VALUE} will become a negative number, which is not
- * supported as well.<br>
- * This does not mean you cannot encode {@link Integer#MAX_VALUE}. If it is not
- * the first value to encode, and you wrap this encoder with
- * {@link DGapIntEncoder}, then the value that will be sent to this encoder will
- * be <code>MAX_VAL - prev</code>.
- * 
- * @lucene.experimental
- */
-public class NOnesIntEncoder extends FourFlagsIntEncoder {
-
-  private final IntsRef internalBuffer;
-  
-  /** Number of consecutive '1's to be translated into single target value '2'. */
-  private final int n;
-
-  /**
-   * Constructs an encoder with a given value of N (N: Number of consecutive
-   * '1's to be translated into single target value '2').
-   */
-  public NOnesIntEncoder(int n) {
-    this.n = n;
-    internalBuffer = new IntsRef(n);
-  }
-
-  @Override
-  public void encode(IntsRef values, BytesRef buf) {
-    internalBuffer.length = 0;
-    // make sure the internal buffer is large enough
-    if (values.length > internalBuffer.ints.length) {
-      internalBuffer.grow(values.length);
-    }
-    
-    int onesCounter = 0;
-    int upto = values.offset + values.length;
-    for (int i = values.offset; i < upto; i++) {
-      int value = values.ints[i];
-      if (value == 1) {
-        // every N 1's should be encoded as '2'
-        if (++onesCounter == n) {
-          internalBuffer.ints[internalBuffer.length++] = 2;
-          onesCounter = 0;
-        }
-      } else {
-        // there might have been 1's that we need to encode
-        while (onesCounter > 0) {
-          --onesCounter;
-          internalBuffer.ints[internalBuffer.length++] = 1;
-        }
-        
-        // encode value as value+1
-        internalBuffer.ints[internalBuffer.length++] = value + 1;
-      }
-    }
-    // there might have been 1's that we need to encode
-    while (onesCounter > 0) {
-      --onesCounter;
-      internalBuffer.ints[internalBuffer.length++] = 1;
-    }
-    super.encode(internalBuffer, buf);
-  }
-
-  @Override
-  public IntDecoder createMatchingDecoder() {
-    return new NOnesIntDecoder(n);
-  }
-
-  @Override
-  public String toString() {
-    return "NOnes(" + n + ") (" + super.toString() + ")";
-  }
-
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/util/encoding/SimpleIntDecoder.java b/lucene/facet/src/java/org/apache/lucene/util/encoding/SimpleIntDecoder.java
deleted file mode 100644
index fd7a79e..0000000
--- a/lucene/facet/src/java/org/apache/lucene/util/encoding/SimpleIntDecoder.java
+++ /dev/null
@@ -1,56 +0,0 @@
-package org.apache.lucene.util.encoding;
-
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IntsRef;
-import org.apache.lucene.util.RamUsageEstimator;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Decodes values encoded with {@link SimpleIntEncoder}.
- * 
- * @lucene.experimental
- */
-public final class SimpleIntDecoder extends IntDecoder {
-
-  @Override
-  public void decode(BytesRef buf, IntsRef values) {
-    values.offset = values.length = 0;
-    int numValues = buf.length / 4; // every value is 4 bytes
-    if (values.ints.length < numValues) { // offset and length are 0
-      values.ints = new int[ArrayUtil.oversize(numValues, RamUsageEstimator.NUM_BYTES_INT)];
-    }
-    
-    int offset = buf.offset;
-    int upto = buf.offset + buf.length;
-    while (offset < upto) {
-      values.ints[values.length++] = 
-          ((buf.bytes[offset++] & 0xFF) << 24) | 
-          ((buf.bytes[offset++] & 0xFF) << 16) | 
-          ((buf.bytes[offset++] & 0xFF) <<  8) | 
-          (buf.bytes[offset++] & 0xFF);
-    }
-  }
-
-  @Override
-  public String toString() {
-    return "Simple";
-  }
-
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/util/encoding/SimpleIntEncoder.java b/lucene/facet/src/java/org/apache/lucene/util/encoding/SimpleIntEncoder.java
deleted file mode 100644
index ae0b295..0000000
--- a/lucene/facet/src/java/org/apache/lucene/util/encoding/SimpleIntEncoder.java
+++ /dev/null
@@ -1,59 +0,0 @@
-package org.apache.lucene.util.encoding;
-
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IntsRef;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * A simple {@link IntEncoder}, writing an integer as 4 raw bytes. *
- * 
- * @lucene.experimental
- */
-public final class SimpleIntEncoder extends IntEncoder {
-
-  @Override
-  public void encode(IntsRef values, BytesRef buf) {
-    buf.offset = buf.length = 0;
-    // ensure there's enough room in the buffer
-    int bytesNeeded = values.length * 4;
-    if (buf.bytes.length < bytesNeeded) {
-      buf.grow(bytesNeeded);
-    }
-    
-    int upto = values.offset + values.length;
-    for (int i = values.offset; i < upto; i++) {
-      int value = values.ints[i];
-      buf.bytes[buf.length++] = (byte) (value >>> 24);
-      buf.bytes[buf.length++] = (byte) ((value >> 16) & 0xFF);
-      buf.bytes[buf.length++] = (byte) ((value >> 8) & 0xFF);
-      buf.bytes[buf.length++] = (byte) (value & 0xFF);
-    }
-  }
-
-  @Override
-  public IntDecoder createMatchingDecoder() {
-    return new SimpleIntDecoder();
-  }
-
-  @Override
-  public String toString() {
-    return "Simple";
-  }
-
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/util/encoding/SortingIntEncoder.java b/lucene/facet/src/java/org/apache/lucene/util/encoding/SortingIntEncoder.java
deleted file mode 100644
index 1285420..0000000
--- a/lucene/facet/src/java/org/apache/lucene/util/encoding/SortingIntEncoder.java
+++ /dev/null
@@ -1,54 +0,0 @@
-package org.apache.lucene.util.encoding;
-
-import java.util.Arrays;
-
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IntsRef;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * An {@link IntEncoderFilter} which sorts the values to encode in ascending
- * order before encoding them.
- * 
- * @lucene.experimental
- */
-public final class SortingIntEncoder extends IntEncoderFilter {
-
-  /** Initializes with the given encoder. */
-  public SortingIntEncoder(IntEncoder encoder) {
-    super(encoder);
-  }
-
-  @Override
-  public void encode(IntsRef values, BytesRef buf) {
-    Arrays.sort(values.ints, values.offset, values.offset + values.length);
-    encoder.encode(values, buf);
-  }
-
-  @Override
-  public IntDecoder createMatchingDecoder() {
-    return encoder.createMatchingDecoder();
-  }
-  
-  @Override
-  public String toString() {
-    return "Sorting(" + encoder.toString() + ")";
-  }
-  
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/util/encoding/UniqueValuesIntEncoder.java b/lucene/facet/src/java/org/apache/lucene/util/encoding/UniqueValuesIntEncoder.java
deleted file mode 100644
index 2612c23..0000000
--- a/lucene/facet/src/java/org/apache/lucene/util/encoding/UniqueValuesIntEncoder.java
+++ /dev/null
@@ -1,63 +0,0 @@
-package org.apache.lucene.util.encoding;
-
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IntsRef;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * An {@link IntEncoderFilter} which ensures only unique values are encoded. The
- * implementation assumes the values given to {@link #encode(IntsRef, BytesRef)} are sorted.
- * If this is not the case, you can chain this encoder with
- * {@link SortingIntEncoder}.
- * 
- * @lucene.experimental
- */
-public final class UniqueValuesIntEncoder extends IntEncoderFilter {
-
-  /** Constructs a new instance with the given encoder. */
-  public UniqueValuesIntEncoder(IntEncoder encoder) {
-    super(encoder);
-  }
-
-  @Override
-  public void encode(IntsRef values, BytesRef buf) {
-    int prev = values.ints[values.offset];
-    int idx = values.offset + 1;
-    int upto = values.offset + values.length;
-    for (int i = idx; i < upto; i++) {
-      if (values.ints[i] != prev) {
-        values.ints[idx++] = values.ints[i];
-        prev = values.ints[i];
-      }
-    }
-    values.length = idx - values.offset;
-    encoder.encode(values, buf);
-  }
-
-  @Override
-  public IntDecoder createMatchingDecoder() {
-    return encoder.createMatchingDecoder();
-  }
-  
-  @Override
-  public String toString() {
-    return "Unique(" + encoder.toString() + ")";
-  }
-  
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/util/encoding/VInt8IntDecoder.java b/lucene/facet/src/java/org/apache/lucene/util/encoding/VInt8IntDecoder.java
deleted file mode 100644
index 65a122a..0000000
--- a/lucene/facet/src/java/org/apache/lucene/util/encoding/VInt8IntDecoder.java
+++ /dev/null
@@ -1,64 +0,0 @@
-package org.apache.lucene.util.encoding;
-
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IntsRef;
-import org.apache.lucene.util.RamUsageEstimator;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Decodes values encoded by {@link VInt8IntEncoder}.
- * 
- * @lucene.experimental
- */
-public final class VInt8IntDecoder extends IntDecoder {
-
-  @Override
-  public void decode(BytesRef buf, IntsRef values) {
-    values.offset = values.length = 0;
-
-    // grow the buffer up front, even if by a large number of values (buf.length)
-    // that saves the need to check inside the loop for every decoded value if
-    // the buffer needs to grow.
-    if (values.ints.length < buf.length) {
-      values.ints = new int[ArrayUtil.oversize(buf.length, RamUsageEstimator.NUM_BYTES_INT)];
-    }
-
-    // it is better if the decoding is inlined like so, and not e.g.
-    // in a utility method
-    int upto = buf.offset + buf.length;
-    int value = 0;
-    int offset = buf.offset;
-    while (offset < upto) {
-      byte b = buf.bytes[offset++];
-      if (b >= 0) {
-        values.ints[values.length++] = (value << 7) | b;
-        value = 0;
-      } else {
-        value = (value << 7) | (b & 0x7F);
-      }
-    }
-  }
-
-  @Override
-  public String toString() {
-    return "VInt8";
-  }
-
-} 
diff --git a/lucene/facet/src/java/org/apache/lucene/util/encoding/VInt8IntEncoder.java b/lucene/facet/src/java/org/apache/lucene/util/encoding/VInt8IntEncoder.java
deleted file mode 100644
index cc4bc13..0000000
--- a/lucene/facet/src/java/org/apache/lucene/util/encoding/VInt8IntEncoder.java
+++ /dev/null
@@ -1,104 +0,0 @@
-package org.apache.lucene.util.encoding;
-
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IntsRef;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * An {@link IntEncoder} which implements variable length encoding. A number is
- * encoded as follows:
- * <ul>
- * <li>If it is less than 127 and non-negative, i.e. uses only 7 bits, it is
- * encoded as a single byte: 0bbbbbbb.
- * <li>If it occupies more than 7 bits, it is represented as a series of bytes,
- * each byte carrying 7 bits. All but the last byte have the MSB set, the last
- * one has it unset.
- * </ul>
- * Example:
- * <ol>
- * <li>n = 117 = 01110101: This has less than 8 significant bits, therefore is
- * encoded as 01110101 = 0x75.
- * <li>n = 100000 = (binary) 11000011010100000. This has 17 significant bits,
- * thus needs three Vint8 bytes. Pad it to a multiple of 7 bits, then split it
- * into chunks of 7 and add an MSB, 0 for the last byte, 1 for the others:
- * 1|0000110 1|0001101 0|0100000 = 0x86 0x8D 0x20.
- * </ol>
- * <b>NOTE:</b> although this encoder is not limited to values &ge; 0, it is not
- * recommended for use with negative values, as their encoding will result in 5
- * bytes written to the output stream, rather than 4. For such values, either
- * use {@link SimpleIntEncoder} or write your own version of variable length
- * encoding, which can better handle negative values.
- * 
- * @lucene.experimental
- */
-public final class VInt8IntEncoder extends IntEncoder {
-
-  @Override
-  public void encode(IntsRef values, BytesRef buf) {
-    buf.offset = buf.length = 0;
-    int maxBytesNeeded = 5 * values.length; // at most 5 bytes per VInt
-    if (buf.bytes.length < maxBytesNeeded) {
-      buf.grow(maxBytesNeeded);
-    }
-    
-    int upto = values.offset + values.length;
-    for (int i = values.offset; i < upto; i++) {
-      // it is better if the encoding is inlined like so, and not e.g.
-      // in a utility method
-      int value = values.ints[i];
-      if ((value & ~0x7F) == 0) {
-        buf.bytes[buf.length] = (byte) value;
-        buf.length++;
-      } else if ((value & ~0x3FFF) == 0) {
-        buf.bytes[buf.length] = (byte) (0x80 | ((value & 0x3F80) >> 7));
-        buf.bytes[buf.length + 1] = (byte) (value & 0x7F);
-        buf.length += 2;
-      } else if ((value & ~0x1FFFFF) == 0) {
-        buf.bytes[buf.length] = (byte) (0x80 | ((value & 0x1FC000) >> 14));
-        buf.bytes[buf.length + 1] = (byte) (0x80 | ((value & 0x3F80) >> 7));
-        buf.bytes[buf.length + 2] = (byte) (value & 0x7F);
-        buf.length += 3;
-      } else if ((value & ~0xFFFFFFF) == 0) {
-        buf.bytes[buf.length] = (byte) (0x80 | ((value & 0xFE00000) >> 21));
-        buf.bytes[buf.length + 1] = (byte) (0x80 | ((value & 0x1FC000) >> 14));
-        buf.bytes[buf.length + 2] = (byte) (0x80 | ((value & 0x3F80) >> 7));
-        buf.bytes[buf.length + 3] = (byte) (value & 0x7F);
-        buf.length += 4;
-      } else {
-        buf.bytes[buf.length] = (byte) (0x80 | ((value & 0xF0000000) >> 28));
-        buf.bytes[buf.length + 1] = (byte) (0x80 | ((value & 0xFE00000) >> 21));
-        buf.bytes[buf.length + 2] = (byte) (0x80 | ((value & 0x1FC000) >> 14));
-        buf.bytes[buf.length + 3] = (byte) (0x80 | ((value & 0x3F80) >> 7));
-        buf.bytes[buf.length + 4] = (byte) (value & 0x7F);
-        buf.length += 5;
-      }
-    }
-  }
-
-  @Override
-  public IntDecoder createMatchingDecoder() {
-    return new VInt8IntDecoder();
-  }
-
-  @Override
-  public String toString() {
-    return "VInt8";
-  }
-
-} 
diff --git a/lucene/facet/src/java/org/apache/lucene/util/encoding/package.html b/lucene/facet/src/java/org/apache/lucene/util/encoding/package.html
deleted file mode 100644
index 8a81b25..0000000
--- a/lucene/facet/src/java/org/apache/lucene/util/encoding/package.html
+++ /dev/null
@@ -1,40 +0,0 @@
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-<title>Encoding</title>
-</head>
-<body>
-Offers various encoders and decoders for integers, as well as the
-mechanisms to create new ones. The super class for all encoders is
-{@link org.apache.lucene.util.encoding.IntEncoder} and for most of the
-encoders there is a matching {@link
-org.apache.lucene.util.encoding.IntDecoder} implementation (not all
-encoders need a decoder).
-<p>
-Some encoders don't perform any encoding at all, or do not include an
-encoding logic. Those are called {@link
-org.apache.lucene.util.encoding.IntEncoderFilter}s. A filter is an
-encoder which delegates the encoding task to a given encoder, however
-performs additional logic before the values are sent for encoding. An
-example is {@link org.apache.lucene.util.encoding.DGapIntEncoder}
-which encodes the gaps between values rather than the values themselves.
-Another example is {@link
-org.apache.lucene.util.encoding.SortingIntEncoder} which sorts all the
-values in ascending order before they are sent for encoding.
-</body>
-</html>
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/util/package.html b/lucene/facet/src/java/org/apache/lucene/util/package.html
deleted file mode 100644
index ae35259..0000000
--- a/lucene/facet/src/java/org/apache/lucene/util/package.html
+++ /dev/null
@@ -1,21 +0,0 @@
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<body>
-Low-level faceting utilities.
-</body>
-</html>
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/FacetTestBase.java b/lucene/facet/src/test/org/apache/lucene/facet/FacetTestBase.java
index d770546..46321c3 100644
--- a/lucene/facet/src/test/org/apache/lucene/facet/FacetTestBase.java
+++ b/lucene/facet/src/test/org/apache/lucene/facet/FacetTestBase.java
@@ -16,14 +16,15 @@ import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
 import org.apache.lucene.document.TextField;
+import org.apache.lucene.facet.collections.IntToObjectMap;
 import org.apache.lucene.facet.index.FacetFields;
-import org.apache.lucene.facet.index.params.CategoryListParams;
-import org.apache.lucene.facet.index.params.CategoryListParams.OrdinalPolicy;
-import org.apache.lucene.facet.index.params.FacetIndexingParams;
-import org.apache.lucene.facet.search.params.FacetRequest;
-import org.apache.lucene.facet.search.params.FacetSearchParams;
-import org.apache.lucene.facet.search.results.FacetResult;
-import org.apache.lucene.facet.search.results.FacetResultNode;
+import org.apache.lucene.facet.params.CategoryListParams;
+import org.apache.lucene.facet.params.FacetIndexingParams;
+import org.apache.lucene.facet.params.FacetSearchParams;
+import org.apache.lucene.facet.params.CategoryListParams.OrdinalPolicy;
+import org.apache.lucene.facet.search.FacetRequest;
+import org.apache.lucene.facet.search.FacetResult;
+import org.apache.lucene.facet.search.FacetResultNode;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
@@ -46,7 +47,6 @@ import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.IOUtils;
 import org.apache.lucene.util.LuceneTestCase.SuppressCodecs;
 import org.apache.lucene.util._TestUtil;
-import org.apache.lucene.util.collections.IntToObjectMap;
 import org.junit.AfterClass;
 import org.junit.BeforeClass;
 
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/FacetTestCase.java b/lucene/facet/src/test/org/apache/lucene/facet/FacetTestCase.java
index a6cf3b8..bbff743 100644
--- a/lucene/facet/src/test/org/apache/lucene/facet/FacetTestCase.java
+++ b/lucene/facet/src/test/org/apache/lucene/facet/FacetTestCase.java
@@ -2,17 +2,17 @@ package org.apache.lucene.facet;
 
 import java.util.Random;
 
-import org.apache.lucene.facet.index.params.CategoryListParams;
+import org.apache.lucene.facet.encoding.DGapIntEncoder;
+import org.apache.lucene.facet.encoding.DGapVInt8IntEncoder;
+import org.apache.lucene.facet.encoding.EightFlagsIntEncoder;
+import org.apache.lucene.facet.encoding.FourFlagsIntEncoder;
+import org.apache.lucene.facet.encoding.IntEncoder;
+import org.apache.lucene.facet.encoding.NOnesIntEncoder;
+import org.apache.lucene.facet.encoding.SortingIntEncoder;
+import org.apache.lucene.facet.encoding.UniqueValuesIntEncoder;
+import org.apache.lucene.facet.encoding.VInt8IntEncoder;
+import org.apache.lucene.facet.params.CategoryListParams;
 import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util.encoding.DGapIntEncoder;
-import org.apache.lucene.util.encoding.DGapVInt8IntEncoder;
-import org.apache.lucene.util.encoding.EightFlagsIntEncoder;
-import org.apache.lucene.util.encoding.FourFlagsIntEncoder;
-import org.apache.lucene.util.encoding.IntEncoder;
-import org.apache.lucene.util.encoding.NOnesIntEncoder;
-import org.apache.lucene.util.encoding.SortingIntEncoder;
-import org.apache.lucene.util.encoding.UniqueValuesIntEncoder;
-import org.apache.lucene.util.encoding.VInt8IntEncoder;
 
 /*
  * Licensed to the Apache Software Foundation (ASF) under one or more
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/FacetTestUtils.java b/lucene/facet/src/test/org/apache/lucene/facet/FacetTestUtils.java
index aef675e..66c6d5a 100644
--- a/lucene/facet/src/test/org/apache/lucene/facet/FacetTestUtils.java
+++ b/lucene/facet/src/test/org/apache/lucene/facet/FacetTestUtils.java
@@ -1,7 +1,7 @@
 package org.apache.lucene.facet;
 
-import org.apache.lucene.facet.search.results.FacetResult;
-import org.apache.lucene.facet.search.results.FacetResultNode;
+import org.apache.lucene.facet.search.FacetResult;
+import org.apache.lucene.facet.search.FacetResultNode;
 
 /*
  * Licensed to the Apache Software Foundation (ASF) under one or more
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/SlowRAMDirectory.java b/lucene/facet/src/test/org/apache/lucene/facet/SlowRAMDirectory.java
new file mode 100644
index 0000000..ee21a8c
--- /dev/null
+++ b/lucene/facet/src/test/org/apache/lucene/facet/SlowRAMDirectory.java
@@ -0,0 +1,172 @@
+package org.apache.lucene.facet;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Random;
+
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.store.RAMDirectory;
+import org.apache.lucene.util.ThreadInterruptedException;
+
+/**
+ * Test utility - slow directory
+ */
+// TODO: move to test-framework and sometimes use in tests?
+public class SlowRAMDirectory extends RAMDirectory {
+
+  private static final int IO_SLEEP_THRESHOLD = 50;
+  
+  Random random;
+  private int sleepMillis;
+
+  public void setSleepMillis(int sleepMillis) {
+    this.sleepMillis = sleepMillis;
+  }
+  
+  public SlowRAMDirectory(int sleepMillis, Random random) {
+    this.sleepMillis = sleepMillis;
+    this.random = random;
+  }
+
+  @Override
+  public IndexOutput createOutput(String name, IOContext context) throws IOException {
+    if (sleepMillis != -1) {
+      return new SlowIndexOutput(super.createOutput(name, context));
+    } 
+
+    return super.createOutput(name, context);
+  }
+
+  @Override
+  public IndexInput openInput(String name, IOContext context) throws IOException {
+    if (sleepMillis != -1) {
+      return new SlowIndexInput(super.openInput(name, context));
+    } 
+    return super.openInput(name, context);
+  }
+
+  void doSleep(Random random, int length) {
+    int sTime = length<10 ? sleepMillis : (int) (sleepMillis * Math.log(length));
+    if (random!=null) {
+      sTime = random.nextInt(sTime);
+    }
+    try {
+      Thread.sleep(sTime);
+    } catch (InterruptedException e) {
+      throw new ThreadInterruptedException(e);
+    }
+  }
+
+  /** Make a private random. */
+  Random forkRandom() {
+    if (random == null) {
+      return null;
+    }
+    return new Random(random.nextLong());
+  }
+
+  /**
+   * Delegate class to wrap an IndexInput and delay reading bytes by some
+   * specified time.
+   */
+  private class SlowIndexInput extends IndexInput {
+    private IndexInput ii;
+    private int numRead = 0;
+    private Random rand;
+    
+    public SlowIndexInput(IndexInput ii) {
+      super("SlowIndexInput(" + ii + ")");
+      this.rand = forkRandom();
+      this.ii = ii;
+    }
+    
+    @Override
+    public byte readByte() throws IOException {
+      if (numRead >= IO_SLEEP_THRESHOLD) {
+        doSleep(rand, 0);
+        numRead = 0;
+      }
+      ++numRead;
+      return ii.readByte();
+    }
+    
+    @Override
+    public void readBytes(byte[] b, int offset, int len) throws IOException {
+      if (numRead >= IO_SLEEP_THRESHOLD) {
+        doSleep(rand, len);
+        numRead = 0;
+      }
+      numRead += len;
+      ii.readBytes(b, offset, len);
+    }
+    
+    @Override public IndexInput clone() { return ii.clone(); }
+    @Override public void close() throws IOException { ii.close(); }
+    @Override public boolean equals(Object o) { return ii.equals(o); }
+    @Override public long getFilePointer() { return ii.getFilePointer(); }
+    @Override public int hashCode() { return ii.hashCode(); }
+    @Override public long length() { return ii.length(); }
+    @Override public void seek(long pos) throws IOException { ii.seek(pos); }
+    
+  }
+  
+  /**
+   * Delegate class to wrap an IndexOutput and delay writing bytes by some
+   * specified time.
+   */
+  private class SlowIndexOutput extends IndexOutput {
+    
+    private IndexOutput io;
+    private int numWrote;
+    private final Random rand;
+    
+    public SlowIndexOutput(IndexOutput io) {
+      this.io = io;
+      this.rand = forkRandom();
+    }
+    
+    @Override
+    public void writeByte(byte b) throws IOException {
+      if (numWrote >= IO_SLEEP_THRESHOLD) {
+        doSleep(rand, 0);
+        numWrote = 0;
+      }
+      ++numWrote;
+      io.writeByte(b);
+    }
+    
+    @Override
+    public void writeBytes(byte[] b, int offset, int length) throws IOException {
+      if (numWrote >= IO_SLEEP_THRESHOLD) {
+        doSleep(rand, length);
+        numWrote = 0;
+      }
+      numWrote += length;
+      io.writeBytes(b, offset, length);
+    }
+    
+    @Override public void close() throws IOException { io.close(); }
+    @Override public void flush() throws IOException { io.flush(); }
+    @Override public long getFilePointer() { return io.getFilePointer(); }
+    @Override public long length() throws IOException { return io.length(); }
+  }
+  
+}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/associations/AssociationsFacetRequestTest.java b/lucene/facet/src/test/org/apache/lucene/facet/associations/AssociationsFacetRequestTest.java
new file mode 100644
index 0000000..b65e511
--- /dev/null
+++ b/lucene/facet/src/test/org/apache/lucene/facet/associations/AssociationsFacetRequestTest.java
@@ -0,0 +1,181 @@
+package org.apache.lucene.facet.associations;
+
+import java.util.List;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.analysis.MockTokenizer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.facet.FacetTestCase;
+import org.apache.lucene.facet.associations.AssociationFloatSumFacetRequest;
+import org.apache.lucene.facet.associations.AssociationIntSumFacetRequest;
+import org.apache.lucene.facet.associations.AssociationsFacetFields;
+import org.apache.lucene.facet.associations.CategoryAssociationsContainer;
+import org.apache.lucene.facet.associations.CategoryFloatAssociation;
+import org.apache.lucene.facet.associations.CategoryIntAssociation;
+import org.apache.lucene.facet.params.FacetSearchParams;
+import org.apache.lucene.facet.search.FacetResult;
+import org.apache.lucene.facet.search.FacetsCollector;
+import org.apache.lucene.facet.taxonomy.CategoryPath;
+import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.store.Directory;
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/** Test for associations */
+public class AssociationsFacetRequestTest extends FacetTestCase {
+
+  private static Directory dir;
+  private static IndexReader reader;
+  private static Directory taxoDir;
+  
+  private static final CategoryPath aint = new CategoryPath("int", "a");
+  private static final CategoryPath bint = new CategoryPath("int", "b");
+  private static final CategoryPath afloat = new CategoryPath("float", "a");
+  private static final CategoryPath bfloat = new CategoryPath("float", "b");
+  
+  @BeforeClass
+  public static void beforeClassAssociationsFacetRequestTest() throws Exception {
+    dir = newDirectory();
+    taxoDir = newDirectory();
+    // preparations - index, taxonomy, content
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, newIndexWriterConfig(TEST_VERSION_CURRENT, 
+        new MockAnalyzer(random(), MockTokenizer.KEYWORD, false)));
+    
+    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
+    
+    AssociationsFacetFields assocFacetFields = new AssociationsFacetFields(taxoWriter);
+    
+    // index documents, 50% have only 'b' and all have 'a'
+    for (int i = 0; i < 100; i++) {
+      Document doc = new Document();
+      CategoryAssociationsContainer associations = new CategoryAssociationsContainer();
+      associations.setAssociation(aint, new CategoryIntAssociation(2));
+      associations.setAssociation(afloat, new CategoryFloatAssociation(0.5f));
+      if (i % 2 == 0) { // 50
+        associations.setAssociation(bint, new CategoryIntAssociation(3));
+        associations.setAssociation(bfloat, new CategoryFloatAssociation(0.2f));
+      }
+      assocFacetFields.addFields(doc, associations);
+      writer.addDocument(doc);
+    }
+    
+    taxoWriter.close();
+    reader = writer.getReader();
+    writer.close();
+  }
+  
+  @AfterClass
+  public static void afterClassAssociationsFacetRequestTest() throws Exception {
+    reader.close();
+    reader = null;
+    dir.close();
+    dir = null;
+    taxoDir.close();
+    taxoDir = null;
+  }
+  
+  @Test
+  public void testIntSumAssociation() throws Exception {
+    DirectoryTaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);
+
+    // facet requests for two facets
+    FacetSearchParams fsp = new FacetSearchParams(
+        new AssociationIntSumFacetRequest(aint, 10),
+        new AssociationIntSumFacetRequest(bint, 10));
+    
+    Query q = new MatchAllDocsQuery();
+
+    FacetsCollector fc = FacetsCollector.create(fsp, reader, taxo);
+    
+    IndexSearcher searcher = newSearcher(reader);
+    searcher.search(q, fc);
+    List<FacetResult> res = fc.getFacetResults();
+    
+    assertNotNull("No results!",res);
+    assertEquals("Wrong number of results!",2, res.size());
+    assertEquals("Wrong count for category 'a'!", 200, (int) res.get(0).getFacetResultNode().value);
+    assertEquals("Wrong count for category 'b'!", 150, (int) res.get(1).getFacetResultNode().value);
+    
+    taxo.close();
+  }
+  
+  @Test
+  public void testFloatSumAssociation() throws Exception {
+    DirectoryTaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);
+
+    // facet requests for two facets
+    FacetSearchParams fsp = new FacetSearchParams(
+        new AssociationFloatSumFacetRequest(afloat, 10),
+        new AssociationFloatSumFacetRequest(bfloat, 10));
+    
+    Query q = new MatchAllDocsQuery();
+
+    FacetsCollector fc = FacetsCollector.create(fsp, reader, taxo);
+    
+    IndexSearcher searcher = newSearcher(reader);
+    searcher.search(q, fc);
+    List<FacetResult> res = fc.getFacetResults();
+    
+    assertNotNull("No results!",res);
+    assertEquals("Wrong number of results!",2, res.size());
+    assertEquals("Wrong count for category 'a'!",50f, (float) res.get(0).getFacetResultNode().value, 0.00001);
+    assertEquals("Wrong count for category 'b'!",10f, (float) res.get(1).getFacetResultNode().value, 0.00001);
+    
+    taxo.close();
+  }  
+    
+  @Test
+  public void testDifferentAggregatorsSameCategoryList() throws Exception {
+    // Same category list cannot be aggregated by two different aggregators. If
+    // you want to do that, you need to separate the categories into two
+    // category list (you'll still have one association list).
+    DirectoryTaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);
+
+    // facet requests for two facets
+    FacetSearchParams fsp = new FacetSearchParams(
+        new AssociationIntSumFacetRequest(aint, 10),
+        new AssociationIntSumFacetRequest(bint, 10),
+        new AssociationFloatSumFacetRequest(afloat, 10),
+        new AssociationFloatSumFacetRequest(bfloat, 10));
+    
+    Query q = new MatchAllDocsQuery();
+
+    FacetsCollector fc = FacetsCollector.create(fsp, reader, taxo);
+    
+    IndexSearcher searcher = newSearcher(reader);
+    searcher.search(q, fc);
+    try {
+      fc.getFacetResults();
+      fail("different aggregators for same category list should not be supported");
+    } catch (RuntimeException e) {
+      // ok - expected
+    }
+    taxo.close();
+  }  
+
+}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/collections/ArrayHashMapTest.java b/lucene/facet/src/test/org/apache/lucene/facet/collections/ArrayHashMapTest.java
new file mode 100644
index 0000000..ade3bc8
--- /dev/null
+++ b/lucene/facet/src/test/org/apache/lucene/facet/collections/ArrayHashMapTest.java
@@ -0,0 +1,268 @@
+package org.apache.lucene.facet.collections;
+
+import java.util.HashSet;
+import java.util.Iterator;
+import java.util.Random;
+
+import org.apache.lucene.facet.FacetTestCase;
+import org.apache.lucene.facet.collections.ArrayHashMap;
+import org.junit.Test;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+public class ArrayHashMapTest extends FacetTestCase {
+
+  public static final int RANDOM_TEST_NUM_ITERATIONS = 100; // set to 100,000 for deeper test
+  
+  @Test
+  public void test0() {
+    ArrayHashMap<Integer,Integer> map = new ArrayHashMap<Integer,Integer>();
+
+    assertNull(map.get(0));
+
+    for (int i = 0; i < 100; ++i) {
+      int value = 100 + i;
+      assertFalse(map.containsValue(value));
+      map.put(i, value);
+      assertTrue(map.containsValue(value));
+      assertNotNull(map.get(i));
+    }
+
+    assertEquals(100, map.size());
+    for (int i = 0; i < 100; ++i) {
+      assertTrue(map.containsKey(i));
+      assertEquals(100 + i, map.get(i).intValue());
+
+    }
+
+    for (int i = 10; i < 90; ++i) {
+      map.remove(i);
+      assertNull(map.get(i));
+    }
+
+    assertEquals(20, map.size());
+    for (int i = 0; i < 100; ++i) {
+      assertEquals(map.containsKey(i), !(i >= 10 && i < 90));
+    }
+
+    for (int i = 5; i < 85; ++i) {
+      map.put(i, Integer.valueOf(5 + i));
+    }
+    assertEquals(95, map.size());
+    for (int i = 0; i < 100; ++i) {
+      assertEquals(map.containsKey(i), !(i >= 85 && i < 90));
+    }
+    for (int i = 0; i < 5; ++i) {
+      assertEquals(map.get(i).intValue(), (100 + i));
+    }
+    for (int i = 5; i < 85; ++i) {
+      assertEquals(map.get(i).intValue(), (5 + i));
+    }
+    for (int i = 90; i < 100; ++i) {
+      assertEquals(map.get(i).intValue(), (100 + i));
+    }
+  }
+
+  @Test
+  public void test1() {
+    ArrayHashMap<Integer,Integer> map = new ArrayHashMap<Integer,Integer>();
+
+    for (int i = 0; i < 100; ++i) {
+      map.put(i, Integer.valueOf(100 + i));
+    }
+
+    HashSet<Integer> set = new HashSet<Integer>();
+
+    for (Iterator<Integer> iterator = map.iterator(); iterator.hasNext();) {
+      set.add(iterator.next());
+    }
+
+    assertEquals(set.size(), map.size());
+    for (int i = 0; i < 100; ++i) {
+      assertTrue(set.contains(Integer.valueOf(100 + i)));
+    }
+
+    set.clear();
+    for (Iterator<Integer> iterator = map.iterator(); iterator.hasNext();) {
+      Integer integer = iterator.next();
+      if (integer % 2 == 1) {
+        iterator.remove();
+        continue;
+      }
+      set.add(integer);
+    }
+    assertEquals(set.size(), map.size());
+    for (int i = 0; i < 100; i += 2) {
+      assertTrue(set.contains(Integer.valueOf(100 + i)));
+    }
+  }
+
+  @Test
+  public void test2() {
+    ArrayHashMap<Integer,Integer> map = new ArrayHashMap<Integer,Integer>();
+
+    assertTrue(map.isEmpty());
+    assertNull(map.get(0));
+    for (int i = 0; i < 128; ++i) {
+      int value = i * 4096;
+      assertFalse(map.containsValue(value));
+      map.put(i, value);
+      assertTrue(map.containsValue(value));
+      assertNotNull(map.get(i));
+      assertFalse(map.isEmpty());
+    }
+
+    assertEquals(128, map.size());
+    for (int i = 0; i < 128; ++i) {
+      assertTrue(map.containsKey(i));
+      assertEquals(i * 4096, map.get(i).intValue());
+    }
+
+    for (int i = 0; i < 200; i += 2) {
+      map.remove(i);
+    }
+    assertEquals(64, map.size());
+    for (int i = 1; i < 128; i += 2) {
+      assertTrue(map.containsKey(i));
+      assertEquals(i * 4096, map.get(i).intValue());
+      map.remove(i);
+    }
+    assertTrue(map.isEmpty());
+  }
+
+  @Test
+  public void test3() {
+    ArrayHashMap<Integer,Integer> map = new ArrayHashMap<Integer,Integer>();
+    int length = 100;
+    for (int i = 0; i < length; ++i) {
+      map.put(i * 64, 100 + i);
+    }
+    HashSet<Integer> keySet = new HashSet<Integer>();
+    for (Iterator<Integer> iit = map.keyIterator(); iit.hasNext();) {
+      keySet.add(iit.next());
+    }
+    assertEquals(length, keySet.size());
+    for (int i = 0; i < length; ++i) {
+      assertTrue(keySet.contains(i * 64));
+    }
+
+    HashSet<Integer> valueSet = new HashSet<Integer>();
+    for (Iterator<Integer> iit = map.iterator(); iit.hasNext();) {
+      valueSet.add(iit.next());
+    }
+    assertEquals(length, valueSet.size());
+    Object[] array = map.toArray();
+    assertEquals(length, array.length);
+    for (Object value : array) {
+      assertTrue(valueSet.contains(value));
+    }
+
+    Integer[] array2 = new Integer[80];
+    array2 = map.toArray(array2);
+    for (int value : array2) {
+      assertTrue(valueSet.contains(value));
+    }
+    Integer[] array3 = new Integer[120];
+    array3 = map.toArray(array3);
+    for (int i = 0; i < length; ++i) {
+      assertTrue(valueSet.contains(array3[i]));
+    }
+    assertNull(array3[length]);
+
+    for (int i = 0; i < length; ++i) {
+      assertTrue(map.containsValue(i + 100));
+      assertTrue(map.containsKey(i * 64));
+    }
+
+    for (Iterator<Integer> iit = map.keyIterator(); iit.hasNext();) {
+      iit.next();
+      iit.remove();
+    }
+    assertTrue(map.isEmpty());
+    assertEquals(0, map.size());
+
+  }
+
+  // now with random data.. and lots of it
+  @Test
+  public void test4() {
+    ArrayHashMap<Integer,Integer> map = new ArrayHashMap<Integer,Integer>();
+    int length = RANDOM_TEST_NUM_ITERATIONS;
+    
+    // for a repeatable random sequence
+    long seed = random().nextLong();
+    Random random = new Random(seed);
+
+    for (int i = 0; i < length; ++i) {
+      int value = random.nextInt(Integer.MAX_VALUE);
+      map.put(i * 128, value);
+    }
+
+    assertEquals(length, map.size());
+    
+    // now repeat
+    random.setSeed(seed);
+
+    for (int i = 0; i < length; ++i) {
+      int value = random.nextInt(Integer.MAX_VALUE);
+      assertTrue(map.containsValue(value));
+      assertTrue(map.containsKey(i * 128));
+      assertEquals(Integer.valueOf(value), map.remove(i * 128));
+    }
+    assertEquals(0, map.size());
+    assertTrue(map.isEmpty());
+  }
+
+  @Test
+  public void testEquals() {
+    ArrayHashMap<Integer,Float> map1 = new ArrayHashMap<Integer,Float>(100);
+    ArrayHashMap<Integer,Float> map2 = new ArrayHashMap<Integer,Float>(100);
+    assertEquals("Empty maps should be equal", map1, map2);
+    assertEquals("hashCode() for empty maps should be equal", 
+        map1.hashCode(), map2.hashCode());
+    
+    for (int i = 0; i < 100; ++i) {
+      map1.put(i, Float.valueOf(1f/i));
+      map2.put(i, Float.valueOf(1f/i));
+    }
+    assertEquals("Identical maps should be equal", map1, map2);
+    assertEquals("hashCode() for identical maps should be equal", 
+        map1.hashCode(), map2.hashCode());
+
+    for (int i = 10; i < 20; i++) {
+      map1.remove(i);
+    }
+    assertFalse("Different maps should not be equal", map1.equals(map2));
+    
+    for (int i = 19; i >=10; --i) {
+      map2.remove(i);
+    }
+    assertEquals("Identical maps should be equal", map1, map2);
+    assertEquals("hashCode() for identical maps should be equal", 
+        map1.hashCode(), map2.hashCode());
+    
+    map1.put(-1,-1f);
+    map2.put(-1,-1.1f);
+    assertFalse("Different maps should not be equal", map1.equals(map2));
+    
+    map2.put(-1,-1f);
+    assertEquals("Identical maps should be equal", map1, map2);
+    assertEquals("hashCode() for identical maps should be equal", 
+        map1.hashCode(), map2.hashCode());
+  }
+}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/collections/FloatToObjectMapTest.java b/lucene/facet/src/test/org/apache/lucene/facet/collections/FloatToObjectMapTest.java
new file mode 100644
index 0000000..c3cb26f
--- /dev/null
+++ b/lucene/facet/src/test/org/apache/lucene/facet/collections/FloatToObjectMapTest.java
@@ -0,0 +1,267 @@
+package org.apache.lucene.facet.collections;
+
+import java.util.HashSet;
+import java.util.Iterator;
+import java.util.Random;
+
+import org.apache.lucene.facet.FacetTestCase;
+import org.apache.lucene.facet.collections.FloatIterator;
+import org.apache.lucene.facet.collections.FloatToObjectMap;
+import org.junit.Test;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+public class FloatToObjectMapTest extends FacetTestCase {
+
+  @Test
+  public void test0() {
+    FloatToObjectMap<Integer> map = new FloatToObjectMap<Integer>();
+
+    assertNull(map.get(0));
+
+    for (int i = 0; i < 100; ++i) {
+      int value = 100 + i;
+      assertFalse(map.containsValue(value));
+      map.put(i * 1.1f, value);
+      assertTrue(map.containsValue(value));
+      assertNotNull(map.get(i * 1.1f));
+    }
+
+    assertEquals(100, map.size());
+    for (int i = 0; i < 100; ++i) {
+      assertTrue(map.containsKey(i*1.1f));
+      assertEquals(100 + i, map.get(i*1.1f).intValue());
+
+    }
+
+    for (int i = 10; i < 90; ++i) {
+      map.remove(i*1.1f);
+      assertNull(map.get(i*1.1f));
+    }
+
+    assertEquals(20, map.size());
+    for (int i = 0; i < 100; ++i) {
+      assertEquals(map.containsKey(i*1.1f), !(i >= 10 && i < 90));
+    }
+
+    for (int i = 5; i < 85; ++i) {
+      map.put(i*1.1f, Integer.valueOf(5 + i));
+    }
+    assertEquals(95, map.size());
+    for (int i = 0; i < 100; ++i) {
+      assertEquals(map.containsKey(i*1.1f), !(i >= 85 && i < 90));
+    }
+    for (int i = 0; i < 5; ++i) {
+      assertEquals(map.get(i*1.1f).intValue(), (100 + i));
+    }
+    for (int i = 5; i < 85; ++i) {
+      assertEquals(map.get(i*1.1f).intValue(), (5 + i));
+    }
+    for (int i = 90; i < 100; ++i) {
+      assertEquals(map.get(i*1.1f).intValue(), (100 + i));
+    }
+  }
+
+  @Test
+  public void test1() {
+    FloatToObjectMap<Integer> map = new FloatToObjectMap<Integer>();
+
+    for (int i = 0; i < 100; ++i) {
+      map.put(i*1.1f, Integer.valueOf(100 + i));
+    }
+
+    HashSet<Integer> set = new HashSet<Integer>();
+
+    for (Iterator<Integer> iterator = map.iterator(); iterator.hasNext();) {
+      set.add(iterator.next());
+    }
+
+    assertEquals(set.size(), map.size());
+    for (int i = 0; i < 100; ++i) {
+      assertTrue(set.contains(Integer.valueOf(100 + i)));
+    }
+
+    set.clear();
+    for (Iterator<Integer> iterator = map.iterator(); iterator.hasNext();) {
+      Integer integer = iterator.next();
+      if (integer % 2 == 1) {
+        iterator.remove();
+        continue;
+      }
+      set.add(integer);
+    }
+    assertEquals(set.size(), map.size());
+    for (int i = 0; i < 100; i += 2) {
+      assertTrue(set.contains(Integer.valueOf(100 + i)));
+    }
+  }
+
+  @Test
+  public void test2() {
+    FloatToObjectMap<Integer> map = new FloatToObjectMap<Integer>();
+
+    assertTrue(map.isEmpty());
+    assertNull(map.get(0));
+    for (int i = 0; i < 128; ++i) {
+      int value = i * 4096;
+      assertFalse(map.containsValue(value));
+      map.put(i*1.1f, value);
+      assertTrue(map.containsValue(value));
+      assertNotNull(map.get(i*1.1f));
+      assertFalse(map.isEmpty());
+    }
+
+    assertEquals(128, map.size());
+    for (int i = 0; i < 128; ++i) {
+      assertTrue(map.containsKey(i*1.1f));
+      assertEquals(i * 4096, map.get(i*1.1f).intValue());
+    }
+
+    for (int i = 0; i < 200; i += 2) {
+      map.remove(i*1.1f);
+    }
+    assertEquals(64, map.size());
+    for (int i = 1; i < 128; i += 2) {
+      assertTrue(map.containsKey(i*1.1f));
+      assertEquals(i * 4096, map.get(i*1.1f).intValue());
+      map.remove(i*1.1f);
+    }
+    assertTrue(map.isEmpty());
+  }
+
+  @Test
+  public void test3() {
+    FloatToObjectMap<Integer> map = new FloatToObjectMap<Integer>();
+    int length = 100;
+    for (int i = 0; i < length; ++i) {
+      map.put(i * 64*1.1f, 100 + i);
+    }
+    HashSet<Float> keySet = new HashSet<Float>();
+    for (FloatIterator iit = map.keyIterator(); iit.hasNext();) {
+      keySet.add(iit.next());
+    }
+    assertEquals(length, keySet.size());
+    for (int i = 0; i < length; ++i) {
+      assertTrue(keySet.contains(i * 64*1.1f));
+    }
+
+    HashSet<Integer> valueSet = new HashSet<Integer>();
+    for (Iterator<Integer> iit = map.iterator(); iit.hasNext();) {
+      valueSet.add(iit.next());
+    }
+    assertEquals(length, valueSet.size());
+    Object[] array = map.toArray();
+    assertEquals(length, array.length);
+    for (Object value : array) {
+      assertTrue(valueSet.contains(value));
+    }
+
+    Integer[] array2 = new Integer[80];
+    array2 = map.toArray(array2);
+    for (int value : array2) {
+      assertTrue(valueSet.contains(value));
+    }
+    Integer[] array3 = new Integer[120];
+    array3 = map.toArray(array3);
+    for (int i = 0; i < length; ++i) {
+      assertTrue(valueSet.contains(array3[i]));
+    }
+    assertNull(array3[length]);
+
+    for (int i = 0; i < length; ++i) {
+      assertTrue(map.containsValue(i + 100));
+      assertTrue(map.containsKey(i * 64*1.1f));
+    }
+
+    for (FloatIterator iit = map.keyIterator(); iit.hasNext();) {
+      iit.next();
+      iit.remove();
+    }
+    assertTrue(map.isEmpty());
+    assertEquals(0, map.size());
+
+  }
+
+  // now with random data.. and lots of it
+  @Test
+  public void test4() {
+    FloatToObjectMap<Integer> map = new FloatToObjectMap<Integer>();
+    int length = ArrayHashMapTest.RANDOM_TEST_NUM_ITERATIONS;
+    
+    // for a repeatable random sequence
+    long seed = random().nextLong();
+    Random random = new Random(seed);
+
+    for (int i = 0; i < length; ++i) {
+      int value = random.nextInt(Integer.MAX_VALUE);
+      map.put(i * 128*1.1f, value);
+    }
+
+    assertEquals(length, map.size());
+
+    // now repeat
+    random.setSeed(seed);
+
+    for (int i = 0; i < length; ++i) {
+      int value = random.nextInt(Integer.MAX_VALUE);
+      assertTrue(map.containsValue(value));
+      assertTrue(map.containsKey(i * 128*1.1f));
+      assertEquals(Integer.valueOf(value), map.remove(i * 128*1.1f));
+    }
+    assertEquals(0, map.size());
+    assertTrue(map.isEmpty());
+  }
+
+  @Test
+  public void testEquals() {
+    FloatToObjectMap<Integer> map1 = new FloatToObjectMap<Integer>(100);
+    FloatToObjectMap<Integer> map2 = new FloatToObjectMap<Integer>(100);
+    assertEquals("Empty maps should be equal", map1, map2);
+    assertEquals("hashCode() for empty maps should be equal", 
+        map1.hashCode(), map2.hashCode());
+    
+    for (int i = 0; i < 100; ++i) {
+      map1.put(i * 1.1f, 100 + i);
+      map2.put(i * 1.1f, 100 + i);
+    }
+    assertEquals("Identical maps should be equal", map1, map2);
+    assertEquals("hashCode() for identical maps should be equal", 
+        map1.hashCode(), map2.hashCode());
+
+    for (int i = 10; i < 20; i++) {
+      map1.remove(i * 1.1f);
+    }
+    assertFalse("Different maps should not be equal", map1.equals(map2));
+    
+    for (int i = 19; i >=10; --i) {
+      map2.remove(i * 1.1f);
+    }
+    assertEquals("Identical maps should be equal", map1, map2);
+    assertEquals("hashCode() for identical maps should be equal", 
+        map1.hashCode(), map2.hashCode());
+    
+    map1.put(-1.1f,-1);
+    map2.put(-1.1f,-2);
+    assertFalse("Different maps should not be equal", map1.equals(map2));
+    
+    map2.put(-1.1f,-1);
+    assertEquals("Identical maps should be equal", map1, map2);
+    assertEquals("hashCode() for identical maps should be equal", 
+        map1.hashCode(), map2.hashCode());
+  }
+}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/collections/IntArrayTest.java b/lucene/facet/src/test/org/apache/lucene/facet/collections/IntArrayTest.java
new file mode 100644
index 0000000..e15ecf7
--- /dev/null
+++ b/lucene/facet/src/test/org/apache/lucene/facet/collections/IntArrayTest.java
@@ -0,0 +1,125 @@
+package org.apache.lucene.facet.collections;
+
+import org.apache.lucene.facet.FacetTestCase;
+import org.apache.lucene.facet.collections.IntArray;
+import org.junit.Test;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+public class IntArrayTest extends FacetTestCase {
+  
+  @Test
+  public void test0() {
+    IntArray array = new IntArray();
+    
+    assertEquals(0, array.size());
+    
+    for (int i = 0; i < 100; ++i) {
+      array.addToArray(i);
+    }
+    
+    assertEquals(100, array.size());
+    for (int i = 0; i < 100; ++i) {
+      assertEquals(i, array.get(i));
+    }
+    
+    assertTrue(array.equals(array));
+  }
+  
+  @Test
+  public void test1() {
+    IntArray array = new IntArray();
+    IntArray array2 = new IntArray();
+    
+    assertEquals(0, array.size());
+    
+    for (int i = 0; i < 100; ++i) {
+      array.addToArray(99-i);
+      array2.addToArray(99-i);
+    }
+    
+    assertEquals(100, array.size());
+    for (int i = 0; i < 100; ++i) {
+      assertEquals(i, array.get(99-i));
+    }
+    
+    array.sort();
+    for (int i = 0; i < 100; ++i) {
+      assertEquals(i, array.get(i));
+    }
+
+    assertTrue(array.equals(array2));
+  }
+  
+  @Test
+  public void test2() {
+    IntArray array = new IntArray();
+    IntArray array2 = new IntArray();
+    IntArray array3 = new IntArray();
+    
+    for (int i = 0; i < 100; ++i) {
+      array.addToArray(i);
+    }
+
+    for (int i = 0; i < 100; ++i) {
+      array2.addToArray(i*2);
+    }
+
+    for (int i = 0; i < 50; ++i) {
+      array3.addToArray(i*2);
+    }
+
+    assertFalse(array.equals(array2));
+    
+    array.intersect(array2);
+    assertTrue(array.equals(array3));
+    assertFalse(array.equals(array2));
+  }
+  
+  @Test
+  public void testSet() {
+    int[] original = new int[] { 2,4,6,8,10,12,14 };
+    int[] toSet = new int[] { 1,3,5,7,9,11};
+    
+    IntArray arr = new IntArray();
+    for (int val : original) {
+      arr.addToArray(val);
+    }
+    
+    for (int i = 0; i < toSet.length; i++ ) {
+      int val = toSet[i];
+      arr.set(i, val);
+    }
+    
+    // Test to see if the set worked correctly
+    for (int i = 0; i < toSet.length; i++ ) {
+      assertEquals(toSet[i], arr.get(i));
+    }
+    
+    // Now attempt to set something outside of the array
+    try {
+      arr.set(100, 99);
+      fail("IntArray.set should have thrown an exception for attempting to set outside the array");
+    } catch (ArrayIndexOutOfBoundsException e) {
+      // We expected this to happen so let it fall through
+      // silently
+    }
+    
+  }
+
+}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/collections/IntHashSetTest.java b/lucene/facet/src/test/org/apache/lucene/facet/collections/IntHashSetTest.java
new file mode 100644
index 0000000..4545bb3
--- /dev/null
+++ b/lucene/facet/src/test/org/apache/lucene/facet/collections/IntHashSetTest.java
@@ -0,0 +1,223 @@
+package org.apache.lucene.facet.collections;
+
+import java.util.HashSet;
+
+import org.apache.lucene.facet.FacetTestCase;
+import org.apache.lucene.facet.collections.IntHashSet;
+import org.junit.Test;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+public class IntHashSetTest extends FacetTestCase {
+
+  @Test
+  public void test0() {
+    IntHashSet set0 = new IntHashSet();
+
+    assertEquals(0, set0.size());
+    assertTrue(set0.isEmpty());
+    set0.add(0);
+    assertEquals(1, set0.size());
+    assertFalse(set0.isEmpty());
+    set0.remove(0);
+    assertEquals(0, set0.size());
+    assertTrue(set0.isEmpty());
+  }
+
+  @Test
+  public void test1() {
+    IntHashSet set0 = new IntHashSet();
+
+    assertEquals(0, set0.size());
+    assertTrue(set0.isEmpty());
+    for (int i = 0; i < 1000; ++i) {
+      set0.add(i);
+    }
+    assertEquals(1000, set0.size());
+    assertFalse(set0.isEmpty());
+    for (int i = 0; i < 1000; ++i) {
+      assertTrue(set0.contains(i));
+    }
+
+    set0.clear();
+    assertEquals(0, set0.size());
+    assertTrue(set0.isEmpty());
+
+  }
+
+  @Test
+  public void test2() {
+    IntHashSet set0 = new IntHashSet();
+
+    assertEquals(0, set0.size());
+    assertTrue(set0.isEmpty());
+    for (int i = 0; i < 1000; ++i) {
+      set0.add(1);
+      set0.add(-382);
+    }
+    assertEquals(2, set0.size());
+    assertFalse(set0.isEmpty());
+    set0.remove(-382);
+    set0.remove(1);
+    assertEquals(0, set0.size());
+    assertTrue(set0.isEmpty());
+
+  }
+
+  @Test
+  public void test3() {
+    IntHashSet set0 = new IntHashSet();
+
+    assertEquals(0, set0.size());
+    assertTrue(set0.isEmpty());
+    for (int i = 0; i < 1000; ++i) {
+      set0.add(i);
+    }
+
+    for (int i = 0; i < 1000; i += 2) {
+      set0.remove(i);
+    }
+
+    assertEquals(500, set0.size());
+    for (int i = 0; i < 1000; ++i) {
+      if (i % 2 == 0) {
+        assertFalse(set0.contains(i));
+      } else {
+        assertTrue(set0.contains(i));
+      }
+    }
+
+  }
+
+  @Test
+  public void test4() {
+    IntHashSet set1 = new IntHashSet();
+    HashSet<Integer> set2 = new HashSet<Integer>();
+    for (int i = 0; i < ArrayHashMapTest.RANDOM_TEST_NUM_ITERATIONS; ++i) {
+      int value = random().nextInt() % 500;
+      boolean shouldAdd = random().nextBoolean();
+      if (shouldAdd) {
+        set1.add(value);
+        set2.add(value);
+      } else {
+        set1.remove(value);
+        set2.remove(value);
+      }
+    }
+    assertEquals(set2.size(), set1.size());
+    for (int value : set2) {
+      assertTrue(set1.contains(value));
+    }
+  }
+
+  @Test
+  public void testRegularJavaSet() {
+    HashSet<Integer> set = new HashSet<Integer>();
+    for (int j = 0; j < 100; ++j) {
+      for (int i = 0; i < ArrayHashMapTest.RANDOM_TEST_NUM_ITERATIONS; ++i) {
+        int value = random().nextInt() % 5000;
+        boolean shouldAdd = random().nextBoolean();
+        if (shouldAdd) {
+          set.add(value);
+        } else {
+          set.remove(value);
+        }
+      }
+      set.clear();
+    }
+  }
+
+  @Test
+  public void testMySet() {
+    IntHashSet set = new IntHashSet();
+    for (int j = 0; j < 100; ++j) {
+      for (int i = 0; i < ArrayHashMapTest.RANDOM_TEST_NUM_ITERATIONS; ++i) {
+        int value = random().nextInt() % 5000;
+        boolean shouldAdd = random().nextBoolean();
+        if (shouldAdd) {
+          set.add(value);
+        } else {
+          set.remove(value);
+        }
+      }
+      set.clear();
+    }
+  }
+
+  @Test
+  public void testToArray() {
+    IntHashSet set = new IntHashSet();
+    for (int j = 0; j < 100; ++j) {
+      for (int i = 0; i < ArrayHashMapTest.RANDOM_TEST_NUM_ITERATIONS; ++i) {
+        int value = random().nextInt() % 5000;
+        boolean shouldAdd = random().nextBoolean();
+        if (shouldAdd) {
+          set.add(value);
+        } else {
+          set.remove(value);
+        }
+      }
+      int[] vals = set.toArray();
+      assertEquals(set.size(), vals.length);
+
+      int[] realValues = new int[set.size()];
+      int[] unrealValues = set.toArray(realValues);
+      assertEquals(realValues, unrealValues);
+      for (int value : vals) {
+        assertTrue(set.remove(value));
+      }
+      for (int i = 0; i < vals.length; ++i) {
+        assertEquals(vals[i], realValues[i]);
+      }
+    }
+  }
+
+  @Test
+  public void testZZRegularJavaSet() {
+    HashSet<Integer> set = new HashSet<Integer>();
+    for (int j = 0; j < 100; ++j) {
+      for (int i = 0; i < ArrayHashMapTest.RANDOM_TEST_NUM_ITERATIONS; ++i) {
+        int value = random().nextInt() % 5000;
+        boolean shouldAdd = random().nextBoolean();
+        if (shouldAdd) {
+          set.add(value);
+        } else {
+          set.remove(value);
+        }
+      }
+      set.clear();
+    }
+  }
+
+  @Test
+  public void testZZMySet() {
+    IntHashSet set = new IntHashSet();
+    for (int j = 0; j < 100; ++j) {
+      for (int i = 0; i < ArrayHashMapTest.RANDOM_TEST_NUM_ITERATIONS; ++i) {
+        int value = random().nextInt() % 5000;
+        boolean shouldAdd = random().nextBoolean();
+        if (shouldAdd) {
+          set.add(value);
+        } else {
+          set.remove(value);
+        }
+      }
+      set.clear();
+    }
+  }
+}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/collections/IntToDoubleMapTest.java b/lucene/facet/src/test/org/apache/lucene/facet/collections/IntToDoubleMapTest.java
new file mode 100644
index 0000000..a75e965
--- /dev/null
+++ b/lucene/facet/src/test/org/apache/lucene/facet/collections/IntToDoubleMapTest.java
@@ -0,0 +1,272 @@
+package org.apache.lucene.facet.collections;
+
+import java.util.HashSet;
+import java.util.Random;
+
+import org.apache.lucene.facet.FacetTestCase;
+import org.apache.lucene.facet.collections.DoubleIterator;
+import org.apache.lucene.facet.collections.IntIterator;
+import org.apache.lucene.facet.collections.IntToDoubleMap;
+import org.junit.Test;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+public class IntToDoubleMapTest extends FacetTestCase {
+  
+  private static void assertGround(double value) {
+    assertEquals(IntToDoubleMap.GROUND, value, Double.MAX_VALUE);
+  }
+  
+  @Test
+  public void test0() {
+    IntToDoubleMap map = new IntToDoubleMap();
+
+    assertGround(map.get(0));
+    
+    for (int i = 0; i < 100; ++i) {
+      int value = 100 + i;
+      assertFalse(map.containsValue(value));
+      map.put(i, value);
+      assertTrue(map.containsValue(value));
+      assertNotNull(map.get(i));
+    }
+
+    assertEquals(100, map.size());
+    for (int i = 0; i < 100; ++i) {
+      assertTrue(map.containsKey(i));
+      assertEquals(100 + i, map.get(i), Double.MAX_VALUE);
+
+    }
+
+    for (int i = 10; i < 90; ++i) {
+      map.remove(i);
+      assertGround(map.get(i));
+    }
+
+    assertEquals(20, map.size());
+    for (int i = 0; i < 100; ++i) {
+      assertEquals(map.containsKey(i), !(i >= 10 && i < 90));
+    }
+
+    for (int i = 5; i < 85; ++i) {
+      map.put(i, Integer.valueOf(5 + i));
+    }
+    assertEquals(95, map.size());
+    for (int i = 0; i < 100; ++i) {
+      assertEquals(map.containsKey(i), !(i >= 85 && i < 90));
+    }
+    for (int i = 0; i < 5; ++i) {
+      assertEquals(map.get(i), (100 + i), Double.MAX_VALUE);
+    }
+    for (int i = 5; i < 85; ++i) {
+      assertEquals(map.get(i), (5 + i), Double.MAX_VALUE);
+    }
+    for (int i = 90; i < 100; ++i) {
+      assertEquals(map.get(i), (100 + i), Double.MAX_VALUE);
+    }
+  }
+
+  @Test
+  public void test1() {
+    IntToDoubleMap map = new IntToDoubleMap();
+
+    for (int i = 0; i < 100; ++i) {
+      map.put(i, Integer.valueOf(100 + i));
+    }
+    
+    HashSet<Double> set = new HashSet<Double>();
+    
+    for (DoubleIterator iterator = map.iterator(); iterator.hasNext();) {
+      set.add(iterator.next());
+    }
+
+    assertEquals(set.size(), map.size());
+    for (int i = 0; i < 100; ++i) {
+      assertTrue(set.contains(Double.valueOf(100+i)));
+    }
+
+    set.clear();
+    for (DoubleIterator iterator = map.iterator(); iterator.hasNext();) {
+      double d = iterator.next();
+      if (d % 2 == 1) {
+        iterator.remove();
+        continue;
+      }
+      set.add(d);
+    }
+    assertEquals(set.size(), map.size());
+    for (int i = 0; i < 100; i+=2) {
+      assertTrue(set.contains(Double.valueOf(100+i)));
+    }
+  }
+  
+  @Test
+  public void test2() {
+    IntToDoubleMap map = new IntToDoubleMap();
+
+    assertTrue(map.isEmpty());
+    assertGround(map.get(0));
+    for (int i = 0; i < 128; ++i) {
+      int value = i * 4096;
+      assertFalse(map.containsValue(value));
+      map.put(i, value);
+      assertTrue(map.containsValue(value));
+      assertNotNull(map.get(i));
+      assertFalse(map.isEmpty());
+    }
+
+    assertEquals(128, map.size());
+    for (int i = 0; i < 128; ++i) {
+      assertTrue(map.containsKey(i));
+      assertEquals(i * 4096, map.get(i), Double.MAX_VALUE);
+    }
+    
+    for (int i = 0 ; i < 200; i+=2) {
+      map.remove(i);
+    }
+    assertEquals(64, map.size());
+    for (int i = 1; i < 128; i+=2) {
+      assertTrue(map.containsKey(i));
+      assertEquals(i * 4096, map.get(i), Double.MAX_VALUE);
+      map.remove(i);
+    }
+    assertTrue(map.isEmpty());
+  }
+  
+  @Test
+  public void test3() {
+    IntToDoubleMap map = new IntToDoubleMap();
+    int length = 100;
+    for (int i = 0; i < length; ++i) {
+      map.put(i*64, 100 + i);
+    }
+    HashSet<Integer> keySet = new HashSet<Integer>();
+    for (IntIterator iit = map.keyIterator(); iit.hasNext(); ) {
+      keySet.add(iit.next());
+    }
+    assertEquals(length, keySet.size());
+    for (int i = 0; i < length; ++i) {
+      assertTrue(keySet.contains(i * 64));
+    }
+    
+    HashSet<Double> valueSet = new HashSet<Double>();
+    for (DoubleIterator iit = map.iterator(); iit.hasNext(); ) {
+      valueSet.add(iit.next());
+    }
+    assertEquals(length, valueSet.size());
+    double[] array = map.toArray();
+    assertEquals(length, array.length);
+    for (double value: array) {
+      assertTrue(valueSet.contains(value));
+    }
+    
+    double[] array2 = new double[80];
+    array2 = map.toArray(array2);
+    assertEquals(length, array2.length);
+    for (double value: array2) {
+      assertTrue(valueSet.contains(value));
+    }
+    
+    double[] array3 = new double[120];
+    array3 = map.toArray(array3);
+    for (int i = 0 ;i < length; ++i) {
+      assertTrue(valueSet.contains(array3[i]));
+    }
+    
+    for (int i = 0; i < length; ++i) {
+      assertTrue(map.containsValue(i + 100));
+      assertTrue(map.containsKey(i*64));
+    }
+    
+    for (IntIterator iit = map.keyIterator(); iit.hasNext(); ) {
+      iit.next();
+      iit.remove();
+    }
+    assertTrue(map.isEmpty());
+    assertEquals(0, map.size());
+    
+  }
+
+  // now with random data.. and lots of it
+  @Test
+  public void test4() {
+    IntToDoubleMap map = new IntToDoubleMap();
+    int length = ArrayHashMapTest.RANDOM_TEST_NUM_ITERATIONS;
+    // for a repeatable random sequence
+    long seed = random().nextLong();
+    Random random = new Random(seed);
+    
+    for (int i = 0; i < length; ++i) {
+      int value = random.nextInt(Integer.MAX_VALUE);
+      map.put(i*128, value);
+    }
+
+    assertEquals(length, map.size());
+
+    // now repeat
+    random.setSeed(seed);
+
+    for (int i = 0; i < length; ++i) {
+      int value = random.nextInt(Integer.MAX_VALUE);
+      assertTrue(map.containsValue(value));
+      assertTrue(map.containsKey(i*128));
+      assertEquals(0, Double.compare(value, map.remove(i*128)));
+    }
+    assertEquals(0, map.size());
+    assertTrue(map.isEmpty());
+  }
+  
+  @Test
+  public void testEquals() {
+    IntToDoubleMap map1 = new IntToDoubleMap(100);
+    IntToDoubleMap map2 = new IntToDoubleMap(100);
+    assertEquals("Empty maps should be equal", map1, map2);
+    assertEquals("hashCode() for empty maps should be equal", 
+        map1.hashCode(), map2.hashCode());
+    
+    for (int i = 0; i < 100; ++i) {
+      map1.put(i, Float.valueOf(1f/i));
+      map2.put(i, Float.valueOf(1f/i));
+    }
+    assertEquals("Identical maps should be equal", map1, map2);
+    assertEquals("hashCode() for identical maps should be equal", 
+        map1.hashCode(), map2.hashCode());
+
+    for (int i = 10; i < 20; i++) {
+      map1.remove(i);
+    }
+    assertFalse("Different maps should not be equal", map1.equals(map2));
+    
+    for (int i = 19; i >=10; --i) {
+      map2.remove(i);
+    }
+    assertEquals("Identical maps should be equal", map1, map2);
+    assertEquals("hashCode() for identical maps should be equal", 
+        map1.hashCode(), map2.hashCode());
+    
+    map1.put(-1,-1f);
+    map2.put(-1,-1.1f);
+    assertFalse("Different maps should not be equal", map1.equals(map2));
+    
+    map2.put(-1,-1f);
+    assertEquals("Identical maps should be equal", map1, map2);
+    assertEquals("hashCode() for identical maps should be equal", 
+        map1.hashCode(), map2.hashCode());
+  }
+  
+}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/collections/IntToFloatMapTest.java b/lucene/facet/src/test/org/apache/lucene/facet/collections/IntToFloatMapTest.java
new file mode 100644
index 0000000..272c8ae
--- /dev/null
+++ b/lucene/facet/src/test/org/apache/lucene/facet/collections/IntToFloatMapTest.java
@@ -0,0 +1,272 @@
+package org.apache.lucene.facet.collections;
+
+import java.util.HashSet;
+import java.util.Random;
+
+import org.apache.lucene.facet.FacetTestCase;
+import org.apache.lucene.facet.collections.FloatIterator;
+import org.apache.lucene.facet.collections.IntIterator;
+import org.apache.lucene.facet.collections.IntToFloatMap;
+import org.junit.Test;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+public class IntToFloatMapTest extends FacetTestCase {
+  
+  private static void assertGround(float value) {
+    assertEquals(IntToFloatMap.GROUND, value, Float.MAX_VALUE);
+  }
+  
+  @Test
+  public void test0() {
+    IntToFloatMap map = new IntToFloatMap();
+
+    assertGround(map.get(0));
+    
+    for (int i = 0; i < 100; ++i) {
+      int value = 100 + i;
+      assertFalse(map.containsValue(value));
+      map.put(i, value);
+      assertTrue(map.containsValue(value));
+      assertNotNull(map.get(i));
+    }
+
+    assertEquals(100, map.size());
+    for (int i = 0; i < 100; ++i) {
+      assertTrue(map.containsKey(i));
+      assertEquals(100 + i, map.get(i), Float.MAX_VALUE);
+
+    }
+
+    for (int i = 10; i < 90; ++i) {
+      map.remove(i);
+      assertGround(map.get(i));
+    }
+
+    assertEquals(20, map.size());
+    for (int i = 0; i < 100; ++i) {
+      assertEquals(map.containsKey(i), !(i >= 10 && i < 90));
+    }
+
+    for (int i = 5; i < 85; ++i) {
+      map.put(i, Integer.valueOf(5 + i));
+    }
+    assertEquals(95, map.size());
+    for (int i = 0; i < 100; ++i) {
+      assertEquals(map.containsKey(i), !(i >= 85 && i < 90));
+    }
+    for (int i = 0; i < 5; ++i) {
+      assertEquals(map.get(i), (100 + i), Float.MAX_VALUE);
+    }
+    for (int i = 5; i < 85; ++i) {
+      assertEquals(map.get(i), (5 + i), Float.MAX_VALUE);
+    }
+    for (int i = 90; i < 100; ++i) {
+      assertEquals(map.get(i), (100 + i), Float.MAX_VALUE);
+    }
+  }
+
+  @Test
+  public void test1() {
+    IntToFloatMap map = new IntToFloatMap();
+
+    for (int i = 0; i < 100; ++i) {
+      map.put(i, Integer.valueOf(100 + i));
+    }
+    
+    HashSet<Float> set = new HashSet<Float>();
+    
+    for (FloatIterator iterator = map.iterator(); iterator.hasNext();) {
+      set.add(iterator.next());
+    }
+
+    assertEquals(set.size(), map.size());
+    for (int i = 0; i < 100; ++i) {
+      assertTrue(set.contains(Float.valueOf(100+i)));
+    }
+
+    set.clear();
+    for (FloatIterator iterator = map.iterator(); iterator.hasNext();) {
+      float d = iterator.next();
+      if (d % 2 == 1) {
+        iterator.remove();
+        continue;
+      }
+      set.add(d);
+    }
+    assertEquals(set.size(), map.size());
+    for (int i = 0; i < 100; i+=2) {
+      assertTrue(set.contains(Float.valueOf(100+i)));
+    }
+  }
+  
+  @Test
+  public void test2() {
+    IntToFloatMap map = new IntToFloatMap();
+
+    assertTrue(map.isEmpty());
+    assertGround(map.get(0));
+    for (int i = 0; i < 128; ++i) {
+      int value = i * 4096;
+      assertFalse(map.containsValue(value));
+      map.put(i, value);
+      assertTrue(map.containsValue(value));
+      assertNotNull(map.get(i));
+      assertFalse(map.isEmpty());
+    }
+
+    assertEquals(128, map.size());
+    for (int i = 0; i < 128; ++i) {
+      assertTrue(map.containsKey(i));
+      assertEquals(i * 4096, map.get(i), Float.MAX_VALUE);
+    }
+    
+    for (int i = 0 ; i < 200; i+=2) {
+      map.remove(i);
+    }
+    assertEquals(64, map.size());
+    for (int i = 1; i < 128; i+=2) {
+      assertTrue(map.containsKey(i));
+      assertEquals(i * 4096, map.get(i), Float.MAX_VALUE);
+      map.remove(i);
+    }
+    assertTrue(map.isEmpty());
+  }
+  
+  @Test
+  public void test3() {
+    IntToFloatMap map = new IntToFloatMap();
+    int length = 100;
+    for (int i = 0; i < length; ++i) {
+      map.put(i*64, 100 + i);
+    }
+    HashSet<Integer> keySet = new HashSet<Integer>();
+    for (IntIterator iit = map.keyIterator(); iit.hasNext(); ) {
+      keySet.add(iit.next());
+    }
+    assertEquals(length, keySet.size());
+    for (int i = 0; i < length; ++i) {
+      assertTrue(keySet.contains(i * 64));
+    }
+    
+    HashSet<Float> valueSet = new HashSet<Float>();
+    for (FloatIterator iit = map.iterator(); iit.hasNext(); ) {
+      valueSet.add(iit.next());
+    }
+    assertEquals(length, valueSet.size());
+    float[] array = map.toArray();
+    assertEquals(length, array.length);
+    for (float value: array) {
+      assertTrue(valueSet.contains(value));
+    }
+    
+    float[] array2 = new float[80];
+    array2 = map.toArray(array2);
+    assertEquals(length, array2.length);
+    for (float value: array2) {
+      assertTrue(valueSet.contains(value));
+    }
+    
+    float[] array3 = new float[120];
+    array3 = map.toArray(array3);
+    for (int i = 0 ;i < length; ++i) {
+      assertTrue(valueSet.contains(array3[i]));
+    }
+    
+    for (int i = 0; i < length; ++i) {
+      assertTrue(map.containsValue(i + 100));
+      assertTrue(map.containsKey(i*64));
+    }
+    
+    for (IntIterator iit = map.keyIterator(); iit.hasNext(); ) {
+      iit.next();
+      iit.remove();
+    }
+    assertTrue(map.isEmpty());
+    assertEquals(0, map.size());
+    
+  }
+
+  // now with random data.. and lots of it
+  @Test
+  public void test4() {
+    IntToFloatMap map = new IntToFloatMap();
+    int length = ArrayHashMapTest.RANDOM_TEST_NUM_ITERATIONS;
+    // for a repeatable random sequence
+    long seed = random().nextLong();
+    Random random = new Random(seed);
+    
+    for (int i = 0; i < length; ++i) {
+      int value = random.nextInt(Integer.MAX_VALUE);
+      map.put(i*128, value);
+    }
+
+    assertEquals(length, map.size());
+
+    // now repeat
+    random.setSeed(seed);
+
+    for (int i = 0; i < length; ++i) {
+      int value = random.nextInt(Integer.MAX_VALUE);
+      assertTrue(map.containsValue(value));
+      assertTrue(map.containsKey(i*128));
+      assertEquals(0, Float.compare(value, map.remove(i*128)));
+    }
+    assertEquals(0, map.size());
+    assertTrue(map.isEmpty());
+  }
+  
+  @Test
+  public void testEquals() {
+    IntToFloatMap map1 = new IntToFloatMap(100);
+    IntToFloatMap map2 = new IntToFloatMap(100);
+    assertEquals("Empty maps should be equal", map1, map2);
+    assertEquals("hashCode() for empty maps should be equal", 
+        map1.hashCode(), map2.hashCode());
+    
+    for (int i = 0; i < 100; ++i) {
+      map1.put(i, Float.valueOf(1f/i));
+      map2.put(i, Float.valueOf(1f/i));
+    }
+    assertEquals("Identical maps should be equal", map1, map2);
+    assertEquals("hashCode() for identical maps should be equal", 
+        map1.hashCode(), map2.hashCode());
+
+    for (int i = 10; i < 20; i++) {
+      map1.remove(i);
+    }
+    assertFalse("Different maps should not be equal", map1.equals(map2));
+    
+    for (int i = 19; i >=10; --i) {
+      map2.remove(i);
+    }
+    assertEquals("Identical maps should be equal", map1, map2);
+    assertEquals("hashCode() for identical maps should be equal", 
+        map1.hashCode(), map2.hashCode());
+    
+    map1.put(-1,-1f);
+    map2.put(-1,-1.1f);
+    assertFalse("Different maps should not be equal", map1.equals(map2));
+    
+    map2.put(-1,-1f);
+    assertEquals("Identical maps should be equal", map1, map2);
+    assertEquals("hashCode() for identical maps should be equal", 
+        map1.hashCode(), map2.hashCode());
+  }
+  
+}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/collections/IntToIntMapTest.java b/lucene/facet/src/test/org/apache/lucene/facet/collections/IntToIntMapTest.java
new file mode 100644
index 0000000..06988b9
--- /dev/null
+++ b/lucene/facet/src/test/org/apache/lucene/facet/collections/IntToIntMapTest.java
@@ -0,0 +1,272 @@
+package org.apache.lucene.facet.collections;
+
+import java.util.HashSet;
+import java.util.Random;
+
+import org.apache.lucene.facet.FacetTestCase;
+import org.apache.lucene.facet.collections.IntIterator;
+import org.apache.lucene.facet.collections.IntToIntMap;
+import org.junit.Test;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+public class IntToIntMapTest extends FacetTestCase {
+  
+  private static void assertGround(int value) {
+    assertEquals(IntToIntMap.GROUD, value);
+  }
+  
+  @Test
+  public void test0() {
+    IntToIntMap map = new IntToIntMap();
+
+    assertGround(map.get(0));
+    
+    for (int i = 0; i < 100; ++i) {
+      int value = 100 + i;
+      assertFalse(map.containsValue(value));
+      map.put(i, value);
+      assertTrue(map.containsValue(value));
+      assertNotNull(map.get(i));
+    }
+
+    assertEquals(100, map.size());
+    for (int i = 0; i < 100; ++i) {
+      assertTrue(map.containsKey(i));
+      assertEquals(100 + i, map.get(i));
+
+    }
+
+    for (int i = 10; i < 90; ++i) {
+      map.remove(i);
+      assertGround(map.get(i));
+    }
+
+    assertEquals(20, map.size());
+    for (int i = 0; i < 100; ++i) {
+      assertEquals(map.containsKey(i), !(i >= 10 && i < 90));
+    }
+
+    for (int i = 5; i < 85; ++i) {
+      map.put(i, Integer.valueOf(5 + i));
+    }
+    assertEquals(95, map.size());
+    for (int i = 0; i < 100; ++i) {
+      assertEquals(map.containsKey(i), !(i >= 85 && i < 90));
+    }
+    for (int i = 0; i < 5; ++i) {
+      assertEquals(map.get(i), (100 + i));
+    }
+    for (int i = 5; i < 85; ++i) {
+      assertEquals(map.get(i), (5 + i));
+    }
+    for (int i = 90; i < 100; ++i) {
+      assertEquals(map.get(i), (100 + i));
+    }
+  }
+
+  @Test
+  public void test1() {
+    IntToIntMap map = new IntToIntMap();
+
+    for (int i = 0; i < 100; ++i) {
+      map.put(i, Integer.valueOf(100 + i));
+    }
+    
+    HashSet<Integer> set = new HashSet<Integer>();
+    
+    for (IntIterator iterator = map.iterator(); iterator.hasNext();) {
+      set.add(iterator.next());
+    }
+
+    assertEquals(set.size(), map.size());
+    for (int i = 0; i < 100; ++i) {
+      assertTrue(set.contains(Integer.valueOf(100+i)));
+    }
+
+    set.clear();
+    for (IntIterator iterator = map.iterator(); iterator.hasNext();) {
+      Integer integer = iterator.next();
+      if (integer % 2 == 1) {
+        iterator.remove();
+        continue;
+      }
+      set.add(integer);
+    }
+    assertEquals(set.size(), map.size());
+    for (int i = 0; i < 100; i+=2) {
+      assertTrue(set.contains(Integer.valueOf(100+i)));
+    }
+  }
+  
+  @Test
+  public void test2() {
+    IntToIntMap map = new IntToIntMap();
+
+    assertTrue(map.isEmpty());
+    assertGround(map.get(0));
+    for (int i = 0; i < 128; ++i) {
+      int value = i * 4096;
+      assertFalse(map.containsValue(value));
+      map.put(i, value);
+      assertTrue(map.containsValue(value));
+      assertNotNull(map.get(i));
+      assertFalse(map.isEmpty());
+    }
+
+    assertEquals(128, map.size());
+    for (int i = 0; i < 128; ++i) {
+      assertTrue(map.containsKey(i));
+      assertEquals(i * 4096, map.get(i));
+    }
+    
+    for (int i = 0 ; i < 200; i+=2) {
+      map.remove(i);
+    }
+    assertEquals(64, map.size());
+    for (int i = 1; i < 128; i+=2) {
+      assertTrue(map.containsKey(i));
+      assertEquals(i * 4096, map.get(i));
+      map.remove(i);
+    }
+    assertTrue(map.isEmpty());
+  }
+  
+  @Test
+  public void test3() {
+    IntToIntMap map = new IntToIntMap();
+    int length = 100;
+    for (int i = 0; i < length; ++i) {
+      map.put(i*64, 100 + i);
+    }
+    HashSet<Integer> keySet = new HashSet<Integer>();
+    for (IntIterator iit = map.keyIterator(); iit.hasNext(); ) {
+      keySet.add(iit.next());
+    }
+    assertEquals(length, keySet.size());
+    for (int i = 0; i < length; ++i) {
+      assertTrue(keySet.contains(i * 64));
+    }
+    
+    HashSet<Integer> valueSet = new HashSet<Integer>();
+    for (IntIterator iit = map.iterator(); iit.hasNext(); ) {
+      valueSet.add(iit.next());
+    }
+    assertEquals(length, valueSet.size());
+    int[] array = map.toArray();
+    assertEquals(length, array.length);
+    for (int value: array) {
+      assertTrue(valueSet.contains(value));
+    }
+    
+    int[] array2 = new int[80];
+    array2 = map.toArray(array2);
+    assertEquals(length, array2.length);
+    for (int value: array2) {
+      assertTrue(valueSet.contains(value));
+    }
+    
+    int[] array3 = new int[120];
+    array3 = map.toArray(array3);
+    for (int i = 0 ;i < length; ++i) {
+      assertTrue(valueSet.contains(array3[i]));
+    }
+    
+    for (int i = 0; i < length; ++i) {
+      assertTrue(map.containsValue(i + 100));
+      assertTrue(map.containsKey(i*64));
+    }
+    
+    for (IntIterator iit = map.keyIterator(); iit.hasNext(); ) {
+      iit.next();
+      iit.remove();
+    }
+    assertTrue(map.isEmpty());
+    assertEquals(0, map.size());
+    
+  }
+
+  // now with random data.. and lots of it
+  @Test
+  public void test4() {
+    IntToIntMap map = new IntToIntMap();
+    int length = ArrayHashMapTest.RANDOM_TEST_NUM_ITERATIONS;
+    
+    // for a repeatable random sequence
+    long seed = random().nextLong();
+    Random random = new Random(seed);
+    
+    for (int i = 0; i < length; ++i) {
+      int value = random.nextInt(Integer.MAX_VALUE);
+      map.put(i*128, value);
+    }
+
+    assertEquals(length, map.size());
+
+    // now repeat
+    random.setSeed(seed);
+
+    for (int i = 0; i < length; ++i) {
+      int value = random.nextInt(Integer.MAX_VALUE);
+      
+      assertTrue(map.containsValue(value));
+      assertTrue(map.containsKey(i*128));
+      assertEquals(value, map.remove(i*128));
+    }
+    assertEquals(0, map.size());
+    assertTrue(map.isEmpty());
+  }
+  
+  @Test
+  public void testEquals() {
+    IntToIntMap map1 = new IntToIntMap(100);
+    IntToIntMap map2 = new IntToIntMap(100);
+    assertEquals("Empty maps should be equal", map1, map2);
+    assertEquals("hashCode() for empty maps should be equal", 
+        map1.hashCode(), map2.hashCode());
+    
+    for (int i = 0; i < 100; ++i) {
+      map1.put(i, 100 + i);
+      map2.put(i, 100 + i);
+    }
+    assertEquals("Identical maps should be equal", map1, map2);
+    assertEquals("hashCode() for identical maps should be equal", 
+        map1.hashCode(), map2.hashCode());
+
+    for (int i = 10; i < 20; i++) {
+      map1.remove(i);
+    }
+    assertFalse("Different maps should not be equal", map1.equals(map2));
+    
+    for (int i = 19; i >=10; --i) {
+      map2.remove(i);
+    }
+    assertEquals("Identical maps should be equal", map1, map2);
+    assertEquals("hashCode() for identical maps should be equal", 
+        map1.hashCode(), map2.hashCode());
+    
+    map1.put(-1,-1);
+    map2.put(-1,-2);
+    assertFalse("Different maps should not be equal", map1.equals(map2));
+    
+    map2.put(-1,-1);
+    assertEquals("Identical maps should be equal", map1, map2);
+    assertEquals("hashCode() for identical maps should be equal", 
+        map1.hashCode(), map2.hashCode());
+  }
+}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/collections/IntToObjectMapTest.java b/lucene/facet/src/test/org/apache/lucene/facet/collections/IntToObjectMapTest.java
new file mode 100644
index 0000000..d505155
--- /dev/null
+++ b/lucene/facet/src/test/org/apache/lucene/facet/collections/IntToObjectMapTest.java
@@ -0,0 +1,267 @@
+package org.apache.lucene.facet.collections;
+
+import java.util.HashSet;
+import java.util.Iterator;
+import java.util.Random;
+
+import org.apache.lucene.facet.FacetTestCase;
+import org.apache.lucene.facet.collections.IntIterator;
+import org.apache.lucene.facet.collections.IntToObjectMap;
+import org.junit.Test;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+public class IntToObjectMapTest extends FacetTestCase {
+
+  @Test
+  public void test0() {
+    IntToObjectMap<Integer> map = new IntToObjectMap<Integer>();
+
+    assertNull(map.get(0));
+
+    for (int i = 0; i < 100; ++i) {
+      int value = 100 + i;
+      assertFalse(map.containsValue(value));
+      map.put(i, value);
+      assertTrue(map.containsValue(value));
+      assertNotNull(map.get(i));
+    }
+
+    assertEquals(100, map.size());
+    for (int i = 0; i < 100; ++i) {
+      assertTrue(map.containsKey(i));
+      assertEquals(100 + i, map.get(i).intValue());
+
+    }
+
+    for (int i = 10; i < 90; ++i) {
+      map.remove(i);
+      assertNull(map.get(i));
+    }
+
+    assertEquals(20, map.size());
+    for (int i = 0; i < 100; ++i) {
+      assertEquals(map.containsKey(i), !(i >= 10 && i < 90));
+    }
+
+    for (int i = 5; i < 85; ++i) {
+      map.put(i, Integer.valueOf(5 + i));
+    }
+    assertEquals(95, map.size());
+    for (int i = 0; i < 100; ++i) {
+      assertEquals(map.containsKey(i), !(i >= 85 && i < 90));
+    }
+    for (int i = 0; i < 5; ++i) {
+      assertEquals(map.get(i).intValue(), (100 + i));
+    }
+    for (int i = 5; i < 85; ++i) {
+      assertEquals(map.get(i).intValue(), (5 + i));
+    }
+    for (int i = 90; i < 100; ++i) {
+      assertEquals(map.get(i).intValue(), (100 + i));
+    }
+  }
+
+  @Test
+  public void test1() {
+    IntToObjectMap<Integer> map = new IntToObjectMap<Integer>();
+
+    for (int i = 0; i < 100; ++i) {
+      map.put(i, Integer.valueOf(100 + i));
+    }
+
+    HashSet<Integer> set = new HashSet<Integer>();
+
+    for (Iterator<Integer> iterator = map.iterator(); iterator.hasNext();) {
+      set.add(iterator.next());
+    }
+
+    assertEquals(set.size(), map.size());
+    for (int i = 0; i < 100; ++i) {
+      assertTrue(set.contains(Integer.valueOf(100 + i)));
+    }
+
+    set.clear();
+    for (Iterator<Integer> iterator = map.iterator(); iterator.hasNext();) {
+      Integer integer = iterator.next();
+      if (integer % 2 == 1) {
+        iterator.remove();
+        continue;
+      }
+      set.add(integer);
+    }
+    assertEquals(set.size(), map.size());
+    for (int i = 0; i < 100; i += 2) {
+      assertTrue(set.contains(Integer.valueOf(100 + i)));
+    }
+  }
+
+  @Test
+  public void test2() {
+    IntToObjectMap<Integer> map = new IntToObjectMap<Integer>();
+
+    assertTrue(map.isEmpty());
+    assertNull(map.get(0));
+    for (int i = 0; i < 128; ++i) {
+      int value = i * 4096;
+      assertFalse(map.containsValue(value));
+      map.put(i, value);
+      assertTrue(map.containsValue(value));
+      assertNotNull(map.get(i));
+      assertFalse(map.isEmpty());
+    }
+
+    assertEquals(128, map.size());
+    for (int i = 0; i < 128; ++i) {
+      assertTrue(map.containsKey(i));
+      assertEquals(i * 4096, map.get(i).intValue());
+    }
+
+    for (int i = 0; i < 200; i += 2) {
+      map.remove(i);
+    }
+    assertEquals(64, map.size());
+    for (int i = 1; i < 128; i += 2) {
+      assertTrue(map.containsKey(i));
+      assertEquals(i * 4096, map.get(i).intValue());
+      map.remove(i);
+    }
+    assertTrue(map.isEmpty());
+  }
+
+  @Test
+  public void test3() {
+    IntToObjectMap<Integer> map = new IntToObjectMap<Integer>();
+    int length = 100;
+    for (int i = 0; i < length; ++i) {
+      map.put(i * 64, 100 + i);
+    }
+    HashSet<Integer> keySet = new HashSet<Integer>();
+    for (IntIterator iit = map.keyIterator(); iit.hasNext();) {
+      keySet.add(iit.next());
+    }
+    assertEquals(length, keySet.size());
+    for (int i = 0; i < length; ++i) {
+      assertTrue(keySet.contains(i * 64));
+    }
+
+    HashSet<Integer> valueSet = new HashSet<Integer>();
+    for (Iterator<Integer> iit = map.iterator(); iit.hasNext();) {
+      valueSet.add(iit.next());
+    }
+    assertEquals(length, valueSet.size());
+    Object[] array = map.toArray();
+    assertEquals(length, array.length);
+    for (Object value : array) {
+      assertTrue(valueSet.contains(value));
+    }
+
+    Integer[] array2 = new Integer[80];
+    array2 = map.toArray(array2);
+    for (int value : array2) {
+      assertTrue(valueSet.contains(value));
+    }
+    Integer[] array3 = new Integer[120];
+    array3 = map.toArray(array3);
+    for (int i = 0; i < length; ++i) {
+      assertTrue(valueSet.contains(array3[i]));
+    }
+    assertNull(array3[length]);
+
+    for (int i = 0; i < length; ++i) {
+      assertTrue(map.containsValue(i + 100));
+      assertTrue(map.containsKey(i * 64));
+    }
+
+    for (IntIterator iit = map.keyIterator(); iit.hasNext();) {
+      iit.next();
+      iit.remove();
+    }
+    assertTrue(map.isEmpty());
+    assertEquals(0, map.size());
+
+  }
+
+  // now with random data.. and lots of it
+  @Test
+  public void test4() {
+    IntToObjectMap<Integer> map = new IntToObjectMap<Integer>();
+    int length = ArrayHashMapTest.RANDOM_TEST_NUM_ITERATIONS;
+    
+    // for a repeatable random sequence
+    long seed = random().nextLong();
+    Random random = new Random(seed);
+    
+    for (int i = 0; i < length; ++i) {
+      int value = random.nextInt(Integer.MAX_VALUE);
+      map.put(i * 128, value);
+    }
+
+    assertEquals(length, map.size());
+
+    // now repeat
+    random.setSeed(seed);
+
+    for (int i = 0; i < length; ++i) {
+      int value = random.nextInt(Integer.MAX_VALUE);
+      assertTrue(map.containsValue(value));
+      assertTrue(map.containsKey(i * 128));
+      assertEquals(Integer.valueOf(value), map.remove(i * 128));
+    }
+    assertEquals(0, map.size());
+    assertTrue(map.isEmpty());
+  }
+  
+  @Test
+  public void testEquals() {
+    IntToObjectMap<Double> map1 = new IntToObjectMap<Double>(100);
+    IntToObjectMap<Double> map2 = new IntToObjectMap<Double>(100);
+    assertEquals("Empty maps should be equal", map1, map2);
+    assertEquals("hashCode() for empty maps should be equal", 
+        map1.hashCode(), map2.hashCode());
+    
+    for (int i = 0; i < 100; ++i) {
+      map1.put(i, Double.valueOf(1f/i));
+      map2.put(i, Double.valueOf(1f/i));
+    }
+    assertEquals("Identical maps should be equal", map1, map2);
+    assertEquals("hashCode() for identical maps should be equal", 
+        map1.hashCode(), map2.hashCode());
+  
+    for (int i = 10; i < 20; i++) {
+      map1.remove(i);
+    }
+    assertFalse("Different maps should not be equal", map1.equals(map2));
+    
+    for (int i = 19; i >=10; --i) {
+      map2.remove(i);
+    }
+    assertEquals("Identical maps should be equal", map1, map2);
+    assertEquals("hashCode() for identical maps should be equal", 
+        map1.hashCode(), map2.hashCode());
+    
+    map1.put(-1,-1d);
+    map2.put(-1,-1.1d);
+    assertFalse("Different maps should not be equal", map1.equals(map2));
+    
+    map2.put(-1,-1d);
+    assertEquals("Identical maps should be equal", map1, map2);
+    assertEquals("hashCode() for identical maps should be equal", 
+        map1.hashCode(), map2.hashCode());
+  }
+}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/collections/ObjectToFloatMapTest.java b/lucene/facet/src/test/org/apache/lucene/facet/collections/ObjectToFloatMapTest.java
new file mode 100644
index 0000000..b291cc9
--- /dev/null
+++ b/lucene/facet/src/test/org/apache/lucene/facet/collections/ObjectToFloatMapTest.java
@@ -0,0 +1,279 @@
+package org.apache.lucene.facet.collections;
+
+import java.util.HashSet;
+import java.util.Iterator;
+import java.util.Random;
+
+import org.apache.lucene.facet.FacetTestCase;
+import org.apache.lucene.facet.collections.FloatIterator;
+import org.apache.lucene.facet.collections.ObjectToFloatMap;
+import org.junit.Test;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+public class ObjectToFloatMapTest extends FacetTestCase {
+
+  @Test
+  public void test0() {
+    ObjectToFloatMap<Integer> map = new ObjectToFloatMap<Integer>();
+
+    assertNaN(map.get(0));
+    
+    for (int i = 0; i < 100; ++i) {
+      int value = 100 + i;
+      assertFalse(map.containsValue(value));
+      map.put(i, value);
+      assertTrue(map.containsValue(value));
+      assertNotNaN(map.get(i));
+    }
+
+    assertEquals(100, map.size());
+    for (int i = 0; i < 100; ++i) {
+      assertTrue(map.containsKey(i));
+      assertEquals(100 + i, map.get(i), 1E-5);
+
+    }
+
+    for (int i = 10; i < 90; ++i) {
+      map.remove(i);
+      assertNaN(map.get(i));
+    }
+
+    assertEquals(20, map.size());
+    for (int i = 0; i < 100; ++i) {
+      assertEquals(map.containsKey(i), !(i >= 10 && i < 90));
+    }
+
+    for (int i = 5; i < 85; ++i) {
+      map.put(i, Integer.valueOf(5 + i));
+    }
+    assertEquals(95, map.size());
+    for (int i = 0; i < 100; ++i) {
+      assertEquals(map.containsKey(i), !(i >= 85 && i < 90));
+    }
+    for (int i = 0; i < 5; ++i) {
+      assertEquals(map.get(i), (100 + i), 1E-5);
+    }
+    for (int i = 5; i < 85; ++i) {
+      assertEquals(map.get(i), (5 + i), 1E-5);
+    }
+    for (int i = 90; i < 100; ++i) {
+      assertEquals(map.get(i), (100 + i), 1E-5);
+    }
+  }
+
+  private static void assertNaN(float f) {
+    assertTrue(Float.isNaN(f));
+  }
+  
+  private static void assertNotNaN(float f) {
+    assertFalse(Float.isNaN(f));
+  }
+
+  @Test
+  public void test1() {
+    ObjectToFloatMap<Integer> map = new ObjectToFloatMap<Integer>();
+
+    for (int i = 0; i < 100; ++i) {
+      map.put(i, Integer.valueOf(100 + i));
+    }
+    
+    HashSet<Float> set = new HashSet<Float>();
+    
+    for (FloatIterator iterator = map.iterator(); iterator.hasNext();) {
+      set.add(iterator.next());
+    }
+
+    assertEquals(set.size(), map.size());
+    for (int i = 0; i < 100; ++i) {
+      assertTrue(set.contains(Float.valueOf(100+i)));
+    }
+
+    set.clear();
+    for (FloatIterator iterator = map.iterator(); iterator.hasNext();) {
+      Float value = iterator.next();
+      if (value % 2 == 1) {
+        iterator.remove();
+        continue;
+      }
+      set.add(value);
+    }
+    assertEquals(set.size(), map.size());
+    for (int i = 0; i < 100; i+=2) {
+      assertTrue(set.contains(Float.valueOf(100+i)));
+    }
+  }
+  
+  @Test
+  public void test2() {
+    ObjectToFloatMap<Integer> map = new ObjectToFloatMap<Integer>();
+
+    assertTrue(map.isEmpty());
+    assertNaN(map.get(0));
+    for (int i = 0; i < 128; ++i) {
+      int value = i * 4096;
+      assertFalse(map.containsValue(value));
+      map.put(i, value);
+      assertTrue(map.containsValue(value));
+      assertNotNaN(map.get(i));
+      assertFalse(map.isEmpty());
+    }
+
+    assertEquals(128, map.size());
+    for (int i = 0; i < 128; ++i) {
+      assertTrue(map.containsKey(i));
+      assertEquals(i * 4096, map.get(i), 1E-5);
+    }
+    
+    for (int i = 0 ; i < 200; i+=2) {
+      map.remove(i);
+    }
+    assertEquals(64, map.size());
+    for (int i = 1; i < 128; i+=2) {
+      assertTrue(map.containsKey(i));
+      assertEquals(i * 4096, map.get(i), 1E-5);
+      map.remove(i);
+    }
+    assertTrue(map.isEmpty());
+  }
+  
+  @Test
+  public void test3() {
+    ObjectToFloatMap<Integer> map = new ObjectToFloatMap<Integer>();
+    int length = 100;
+    for (int i = 0; i < length; ++i) {
+      map.put(i*64, 100 + i);
+    }
+    HashSet<Integer> keySet = new HashSet<Integer>();
+    for (Iterator<Integer> iit = map.keyIterator(); iit.hasNext(); ) {
+      keySet.add(iit.next());
+    }
+    assertEquals(length, keySet.size());
+    for (int i = 0; i < length; ++i) {
+      assertTrue(keySet.contains(i * 64));
+    }
+    
+    HashSet<Float> valueSet = new HashSet<Float>();
+    for (FloatIterator iit = map.iterator(); iit.hasNext(); ) {
+      valueSet.add(iit.next());
+    }
+    assertEquals(length, valueSet.size());
+    float[] array = map.toArray();
+    assertEquals(length, array.length);
+    for (float value: array) {
+      assertTrue(valueSet.contains(value));
+    }
+    
+    float[] array2 = new float[80];
+    array2 = map.toArray(array2);
+    assertEquals(80, array2.length);
+    for (float value: array2) {
+      assertTrue(valueSet.contains(value));
+    }
+    
+    float[] array3 = new float[120];
+    array3 = map.toArray(array3);
+    for (int i = 0 ;i < length; ++i) {
+      assertTrue(valueSet.contains(array3[i]));
+    }
+    assertNaN(array3[length]);
+    
+    for (int i = 0; i < length; ++i) {
+      assertTrue(map.containsValue(i + 100));
+      assertTrue(map.containsKey(i*64));
+    }
+    
+    for (Iterator<Integer> iit = map.keyIterator(); iit.hasNext(); ) {
+      iit.next();
+      iit.remove();
+    }
+    assertTrue(map.isEmpty());
+    assertEquals(0, map.size());
+    
+  }
+
+  // now with random data.. and lots of it
+  @Test
+  public void test4() {
+    ObjectToFloatMap<Integer> map = new ObjectToFloatMap<Integer>();
+    int length = ArrayHashMapTest.RANDOM_TEST_NUM_ITERATIONS;
+    
+    // for a repeatable random sequence
+    long seed = random().nextLong();
+    Random random = new Random(seed);
+
+    for (int i = 0; i < length; ++i) {
+      int value = random.nextInt(Integer.MAX_VALUE);
+      map.put(i*128, value);
+    }
+
+    assertEquals(length, map.size());
+
+    // now repeat
+    random.setSeed(seed);
+
+    for (int i = 0; i < length; ++i) {
+      int value = random.nextInt(Integer.MAX_VALUE);
+      
+      assertTrue(map.containsValue(value));
+      assertTrue(map.containsKey(i*128));
+      assertEquals(0, Float.compare(value, map.remove(i*128)));
+    }
+    assertEquals(0, map.size());
+    assertTrue(map.isEmpty());
+  }
+  
+  @Test
+  public void testEquals() {
+    ObjectToFloatMap<Integer> map1 = new ObjectToFloatMap<Integer>(100);
+    ObjectToFloatMap<Integer> map2 = new ObjectToFloatMap<Integer>(100);
+    assertEquals("Empty maps should be equal", map1, map2);
+    assertEquals("hashCode() for empty maps should be equal", 
+        map1.hashCode(), map2.hashCode());
+    
+    for (int i = 0; i < 100; ++i) {
+      map1.put(i, Float.valueOf(1f/i));
+      map2.put(i, Float.valueOf(1f/i));
+    }
+    assertEquals("Identical maps should be equal", map1, map2);
+    assertEquals("hashCode() for identical maps should be equal", 
+        map1.hashCode(), map2.hashCode());
+
+    for (int i = 10; i < 20; i++) {
+      map1.remove(i);
+    }
+    assertFalse("Different maps should not be equal", map1.equals(map2));
+    
+    for (int i = 19; i >=10; --i) {
+      map2.remove(i);
+    }
+    assertEquals("Identical maps should be equal", map1, map2);
+    assertEquals("hashCode() for identical maps should be equal", 
+        map1.hashCode(), map2.hashCode());
+    
+    map1.put(-1,-1f);
+    map2.put(-1,-1.1f);
+    assertFalse("Different maps should not be equal", map1.equals(map2));
+    
+    map2.put(-1,-1f);
+    assertEquals("Identical maps should be equal", map1, map2);
+    assertEquals("hashCode() for identical maps should be equal", 
+        map1.hashCode(), map2.hashCode());
+  }
+  
+}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/collections/ObjectToIntMapTest.java b/lucene/facet/src/test/org/apache/lucene/facet/collections/ObjectToIntMapTest.java
new file mode 100644
index 0000000..8963128
--- /dev/null
+++ b/lucene/facet/src/test/org/apache/lucene/facet/collections/ObjectToIntMapTest.java
@@ -0,0 +1,277 @@
+package org.apache.lucene.facet.collections;
+
+import java.util.HashSet;
+import java.util.Iterator;
+import java.util.Random;
+
+import org.apache.lucene.facet.FacetTestCase;
+import org.apache.lucene.facet.collections.IntIterator;
+import org.apache.lucene.facet.collections.ObjectToIntMap;
+import org.junit.Test;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+public class ObjectToIntMapTest extends FacetTestCase {
+
+  @Test
+  public void test0() {
+    ObjectToIntMap<Integer> map = new ObjectToIntMap<Integer>();
+
+    assertIntegerMaxValue(map.get(0));
+    
+    for (int i = 0; i < 100; ++i) {
+      int value = 100 + i;
+      assertFalse(map.containsValue(value));
+      map.put(i, value);
+      assertTrue(map.containsValue(value));
+      assertNotIntegerMaxValue(map.get(i));
+    }
+
+    assertEquals(100, map.size());
+    for (int i = 0; i < 100; ++i) {
+      assertTrue(map.containsKey(i));
+      assertEquals(100 + i, map.get(i), 1E-5);
+
+    }
+
+    for (int i = 10; i < 90; ++i) {
+      map.remove(i);
+      assertIntegerMaxValue(map.get(i));
+    }
+
+    assertEquals(20, map.size());
+    for (int i = 0; i < 100; ++i) {
+      assertEquals(map.containsKey(i), !(i >= 10 && i < 90));
+    }
+
+    for (int i = 5; i < 85; ++i) {
+      map.put(i, Integer.valueOf(5 + i));
+    }
+    assertEquals(95, map.size());
+    for (int i = 0; i < 100; ++i) {
+      assertEquals(map.containsKey(i), !(i >= 85 && i < 90));
+    }
+    for (int i = 0; i < 5; ++i) {
+      assertEquals(map.get(i), (100 + i), 1E-5);
+    }
+    for (int i = 5; i < 85; ++i) {
+      assertEquals(map.get(i), (5 + i), 1E-5);
+    }
+    for (int i = 90; i < 100; ++i) {
+      assertEquals(map.get(i), (100 + i), 1E-5);
+    }
+  }
+
+  private static void assertIntegerMaxValue(int i) {
+    assertTrue(i == Integer.MAX_VALUE);
+  }
+  
+  private static void assertNotIntegerMaxValue(int i) {
+    assertFalse(i == Integer.MAX_VALUE);
+  }
+
+  @Test
+  public void test1() {
+    ObjectToIntMap<Integer> map = new ObjectToIntMap<Integer>();
+
+    for (int i = 0; i < 100; ++i) {
+      map.put(i, Integer.valueOf(100 + i));
+    }
+    
+    HashSet<Integer> set = new HashSet<Integer>();
+    
+    for (IntIterator iterator = map.iterator(); iterator.hasNext();) {
+      set.add(iterator.next());
+    }
+
+    assertEquals(set.size(), map.size());
+    for (int i = 0; i < 100; ++i) {
+      assertTrue(set.contains(Integer.valueOf(100+i)));
+    }
+
+    set.clear();
+    for (IntIterator iterator = map.iterator(); iterator.hasNext();) {
+      Integer value = iterator.next();
+      if (value % 2 == 1) {
+        iterator.remove();
+        continue;
+      }
+      set.add(value);
+    }
+    assertEquals(set.size(), map.size());
+    for (int i = 0; i < 100; i+=2) {
+      assertTrue(set.contains(Integer.valueOf(100+i)));
+    }
+  }
+  
+  @Test
+  public void test2() {
+    ObjectToIntMap<Integer> map = new ObjectToIntMap<Integer>();
+
+    assertTrue(map.isEmpty());
+    assertIntegerMaxValue(map.get(0));
+    for (int i = 0; i < 128; ++i) {
+      int value = i * 4096;
+      assertFalse(map.containsValue(value));
+      map.put(i, value);
+      assertTrue(map.containsValue(value));
+      assertNotIntegerMaxValue(map.get(i));
+      assertFalse(map.isEmpty());
+    }
+
+    assertEquals(128, map.size());
+    for (int i = 0; i < 128; ++i) {
+      assertTrue(map.containsKey(i));
+      assertEquals(i * 4096, map.get(i), 1E-5);
+    }
+    
+    for (int i = 0 ; i < 200; i+=2) {
+      map.remove(i);
+    }
+    assertEquals(64, map.size());
+    for (int i = 1; i < 128; i+=2) {
+      assertTrue(map.containsKey(i));
+      assertEquals(i * 4096, map.get(i), 1E-5);
+      map.remove(i);
+    }
+    assertTrue(map.isEmpty());
+  }
+  
+  @Test
+  public void test3() {
+    ObjectToIntMap<Integer> map = new ObjectToIntMap<Integer>();
+    int length = 100;
+    for (int i = 0; i < length; ++i) {
+      map.put(i*64, 100 + i);
+    }
+    HashSet<Integer> keySet = new HashSet<Integer>();
+    for (Iterator<Integer> iit = map.keyIterator(); iit.hasNext(); ) {
+      keySet.add(iit.next());
+    }
+    assertEquals(length, keySet.size());
+    for (int i = 0; i < length; ++i) {
+      assertTrue(keySet.contains(i * 64));
+    }
+    
+    HashSet<Integer> valueSet = new HashSet<Integer>();
+    for (IntIterator iit = map.iterator(); iit.hasNext(); ) {
+      valueSet.add(iit.next());
+    }
+    assertEquals(length, valueSet.size());
+    int[] array = map.toArray();
+    assertEquals(length, array.length);
+    for (int value: array) {
+      assertTrue(valueSet.contains(value));
+    }
+    
+    int[] array2 = new int[80];
+    array2 = map.toArray(array2);
+    assertEquals(80, array2.length);
+    for (int value: array2) {
+      assertTrue(valueSet.contains(value));
+    }
+    
+    int[] array3 = new int[120];
+    array3 = map.toArray(array3);
+    for (int i = 0 ;i < length; ++i) {
+      assertTrue(valueSet.contains(array3[i]));
+    }
+    assertIntegerMaxValue(array3[length]);
+    
+    for (int i = 0; i < length; ++i) {
+      assertTrue(map.containsValue(i + 100));
+      assertTrue(map.containsKey(i*64));
+    }
+    
+    for (Iterator<Integer> iit = map.keyIterator(); iit.hasNext(); ) {
+      iit.next();
+      iit.remove();
+    }
+    assertTrue(map.isEmpty());
+    assertEquals(0, map.size());
+    
+  }
+
+  // now with random data.. and lots of it
+  @Test
+  public void test4() {
+    ObjectToIntMap<Integer> map = new ObjectToIntMap<Integer>();
+    int length = ArrayHashMapTest.RANDOM_TEST_NUM_ITERATIONS;
+    
+    // for a repeatable random sequence
+    long seed = random().nextLong();
+    Random random = new Random(seed);
+    
+    for (int i = 0; i < length; ++i) {
+      int value = random.nextInt(Integer.MAX_VALUE);
+      map.put(i*128, value);
+    }
+
+    assertEquals(length, map.size());
+    
+    // now repeat
+    random.setSeed(seed);
+
+    for (int i = 0; i < length; ++i) {
+      int value = random.nextInt(Integer.MAX_VALUE);
+      assertTrue(map.containsValue(value));
+      assertTrue(map.containsKey(i*128));
+      assertEquals(value, map.remove(i*128));
+    }
+    assertEquals(0, map.size());
+    assertTrue(map.isEmpty());
+  }
+  
+  @Test
+  public void testEquals() {
+    ObjectToIntMap<Float> map1 = new ObjectToIntMap<Float>(100);
+    ObjectToIntMap<Float> map2 = new ObjectToIntMap<Float>(100);
+    assertEquals("Empty maps should be equal", map1, map2);
+    assertEquals("hashCode() for empty maps should be equal", 
+        map1.hashCode(), map2.hashCode());
+    
+    for (int i = 0; i < 100; ++i) {
+      map1.put(i * 1.1f, 100 + i);
+      map2.put(i * 1.1f, 100 + i);
+    }
+    assertEquals("Identical maps should be equal", map1, map2);
+    assertEquals("hashCode() for identical maps should be equal", 
+        map1.hashCode(), map2.hashCode());
+
+    for (int i = 10; i < 20; i++) {
+      map1.remove(i * 1.1f);
+    }
+    assertFalse("Different maps should not be equal", map1.equals(map2));
+    
+    for (int i = 19; i >=10; --i) {
+      map2.remove(i * 1.1f);
+    }
+    assertEquals("Identical maps should be equal", map1, map2);
+    assertEquals("hashCode() for identical maps should be equal", 
+        map1.hashCode(), map2.hashCode());
+    
+    map1.put(-1.1f,-1);
+    map2.put(-1.1f,-2);
+    assertFalse("Different maps should not be equal", map1.equals(map2));
+    
+    map2.put(-1.1f,-1);
+    assertEquals("Identical maps should be equal", map1, map2);
+    assertEquals("hashCode() for identical maps should be equal", 
+        map1.hashCode(), map2.hashCode());
+  }
+}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/collections/TestLRUHashMap.java b/lucene/facet/src/test/org/apache/lucene/facet/collections/TestLRUHashMap.java
new file mode 100644
index 0000000..d61bf71
--- /dev/null
+++ b/lucene/facet/src/test/org/apache/lucene/facet/collections/TestLRUHashMap.java
@@ -0,0 +1,60 @@
+package org.apache.lucene.facet.collections;
+
+import org.apache.lucene.facet.FacetTestCase;
+import org.apache.lucene.facet.collections.LRUHashMap;
+import org.junit.Test;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+public class TestLRUHashMap extends FacetTestCase {
+  // testLRU() tests that the specified size limit is indeed honored, and
+  // the remaining objects in the map are indeed those that have been most
+  // recently used
+  @Test
+  public void testLRU() throws Exception {
+    LRUHashMap<String, String> lru = new LRUHashMap<String, String>(3);
+    assertEquals(0, lru.size());
+    lru.put("one", "Hello world");
+    assertEquals(1, lru.size());
+    lru.put("two", "Hi man");
+    assertEquals(2, lru.size());
+    lru.put("three", "Bonjour");
+    assertEquals(3, lru.size());
+    lru.put("four", "Shalom");
+    assertEquals(3, lru.size());
+    assertNotNull(lru.get("three"));
+    assertNotNull(lru.get("two"));
+    assertNotNull(lru.get("four"));
+    assertNull(lru.get("one"));
+    lru.put("five", "Yo!");
+    assertEquals(3, lru.size());
+    assertNull(lru.get("three")); // three was last used, so it got removed
+    assertNotNull(lru.get("five"));
+    lru.get("four");
+    lru.put("six", "hi");
+    lru.put("seven", "hey dude");
+    assertEquals(3, lru.size());
+    assertNull(lru.get("one"));
+    assertNull(lru.get("two"));
+    assertNull(lru.get("three"));
+    assertNotNull(lru.get("four"));
+    assertNull(lru.get("five"));
+    assertNotNull(lru.get("six"));
+    assertNotNull(lru.get("seven"));
+  }
+}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/complements/TestFacetsAccumulatorWithComplement.java b/lucene/facet/src/test/org/apache/lucene/facet/complements/TestFacetsAccumulatorWithComplement.java
new file mode 100644
index 0000000..8cb229a
--- /dev/null
+++ b/lucene/facet/src/test/org/apache/lucene/facet/complements/TestFacetsAccumulatorWithComplement.java
@@ -0,0 +1,135 @@
+package org.apache.lucene.facet.complements;
+
+import java.io.IOException;
+import java.util.List;
+
+import org.apache.lucene.facet.FacetTestBase;
+import org.apache.lucene.facet.params.FacetIndexingParams;
+import org.apache.lucene.facet.params.FacetSearchParams;
+import org.apache.lucene.facet.search.CountFacetRequest;
+import org.apache.lucene.facet.search.FacetResult;
+import org.apache.lucene.facet.search.FacetResultNode;
+import org.apache.lucene.facet.search.FacetsCollector;
+import org.apache.lucene.facet.search.StandardFacetsAccumulator;
+import org.apache.lucene.facet.taxonomy.CategoryPath;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.MultiReader;
+import org.apache.lucene.index.ParallelAtomicReader;
+import org.apache.lucene.index.SlowCompositeReaderWrapper;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.Test;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+public class TestFacetsAccumulatorWithComplement extends FacetTestBase {
+  
+  private FacetIndexingParams fip;
+  
+  @Override
+  @Before
+  public void setUp() throws Exception {
+    super.setUp();
+    fip = getFacetIndexingParams(Integer.MAX_VALUE);
+    initIndex(fip);
+  }
+  
+  @Override
+  @After
+  public void tearDown() throws Exception {
+    closeAll();
+    super.tearDown();
+  }
+  
+  /**
+   * Test that complements does not cause a failure when using a parallel reader
+   */
+  @Test
+  public void testComplementsWithParallerReader() throws Exception {
+    IndexReader origReader = indexReader; 
+    ParallelAtomicReader pr = new ParallelAtomicReader(SlowCompositeReaderWrapper.wrap(origReader));
+    indexReader = pr;
+    try {
+      doTestComplements();
+    } finally {
+      indexReader = origReader;
+    }
+  }
+
+  /**
+   * Test that complements works with MultiReader
+   */
+  @Test
+  public void testComplementsWithMultiReader() throws Exception {
+    final IndexReader origReader = indexReader; 
+    indexReader = new MultiReader(origReader);
+    try {
+      doTestComplements();
+    } finally {
+      indexReader = origReader;
+    }
+  }
+  
+  /**
+   * Test that score is indeed constant when using a constant score
+   */
+  @Test
+  public void testComplements() throws Exception {
+    doTestComplements();
+  }
+  
+  private void doTestComplements() throws Exception {
+    // verify by facet values
+    List<FacetResult> countResWithComplement = findFacets(true);
+    List<FacetResult> countResNoComplement = findFacets(false);
+    
+    assertEquals("Wrong number of facet count results with complement!",1,countResWithComplement.size());
+    assertEquals("Wrong number of facet count results no complement!",1,countResNoComplement.size());
+    
+    FacetResultNode parentResWithComp = countResWithComplement.get(0).getFacetResultNode();
+    FacetResultNode parentResNoComp = countResWithComplement.get(0).getFacetResultNode();
+    
+    assertEquals("Wrong number of top count aggregated categories with complement!",3,parentResWithComp.subResults.size());
+    assertEquals("Wrong number of top count aggregated categories no complement!",3,parentResNoComp.subResults.size());
+  }
+  
+  /** compute facets with certain facet requests and docs */
+  private List<FacetResult> findFacets(boolean withComplement) throws IOException {
+    FacetSearchParams fsp = new FacetSearchParams(fip, new CountFacetRequest(new CategoryPath("root","a"), 10));
+    StandardFacetsAccumulator sfa = new StandardFacetsAccumulator(fsp, indexReader, taxoReader);
+    sfa.setComplementThreshold(withComplement ? StandardFacetsAccumulator.FORCE_COMPLEMENT : StandardFacetsAccumulator.DISABLE_COMPLEMENT);
+    FacetsCollector fc = FacetsCollector.create(sfa);
+    searcher.search(new MatchAllDocsQuery(), fc);
+    
+    List<FacetResult> res = fc.getFacetResults();
+    
+    // Results are ready, printing them...
+    int i = 0;
+    for (FacetResult facetResult : res) {
+      if (VERBOSE) {
+        System.out.println("Res "+(i++)+": "+facetResult);
+      }
+    }
+    
+    assertEquals(withComplement, sfa.isUsingComplements());
+    
+    return res;
+  }
+  
+}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/complements/TestTotalFacetCounts.java b/lucene/facet/src/test/org/apache/lucene/facet/complements/TestTotalFacetCounts.java
new file mode 100644
index 0000000..144356b
--- /dev/null
+++ b/lucene/facet/src/test/org/apache/lucene/facet/complements/TestTotalFacetCounts.java
@@ -0,0 +1,130 @@
+package org.apache.lucene.facet.complements;
+
+import java.io.File;
+import java.io.IOException;
+import java.util.Arrays;
+import java.util.Collections;
+
+import org.apache.lucene.document.Document;
+import org.apache.lucene.facet.FacetTestCase;
+import org.apache.lucene.facet.complements.TotalFacetCounts;
+import org.apache.lucene.facet.complements.TotalFacetCountsCache;
+import org.apache.lucene.facet.index.FacetFields;
+import org.apache.lucene.facet.params.CategoryListParams;
+import org.apache.lucene.facet.params.FacetIndexingParams;
+import org.apache.lucene.facet.taxonomy.CategoryPath;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util._TestUtil;
+import org.junit.Test;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+public class TestTotalFacetCounts extends FacetTestCase {
+
+  private static void initCache() {
+    TotalFacetCountsCache.getSingleton().clear();
+    TotalFacetCountsCache.getSingleton().setCacheSize(1); // Set to keep one in mem
+  }
+
+  @Test
+  public void testWriteRead() throws IOException {
+    doTestWriteRead(14);
+    doTestWriteRead(100);
+    doTestWriteRead(7);
+    doTestWriteRead(3);
+    doTestWriteRead(1);
+  }
+
+  private void doTestWriteRead(final int partitionSize) throws IOException {
+    initCache();
+
+    Directory indexDir = newDirectory();
+    Directory taxoDir = newDirectory();
+    IndexWriter indexWriter = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, null));
+    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
+    
+    FacetIndexingParams iParams = new FacetIndexingParams() {
+      @Override
+      public int getPartitionSize() {
+        return partitionSize;
+      }
+      
+      @Override
+      public CategoryListParams getCategoryListParams(CategoryPath category) {
+        return new CategoryListParams() {
+          @Override
+          public OrdinalPolicy getOrdinalPolicy(String dimension) {
+            return OrdinalPolicy.ALL_PARENTS;
+          }
+        };
+      }
+    };
+    // The counts that the TotalFacetCountsArray should have after adding
+    // the below facets to the index.
+    int[] expectedCounts = new int[] { 0, 3, 1, 3, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1 };
+    String[] categories = new String[] { "a/b", "c/d", "a/e", "a/d", "c/g", "c/z", "b/a", "1/2", "b/c" };
+
+    FacetFields facetFields = new FacetFields(taxoWriter, iParams);
+    for (String cat : categories) {
+      Document doc = new Document();
+      facetFields.addFields(doc, Collections.singletonList(new CategoryPath(cat, '/')));
+      indexWriter.addDocument(doc);
+    }
+
+    // Commit Changes
+    IOUtils.close(indexWriter, taxoWriter);
+
+    DirectoryReader indexReader = DirectoryReader.open(indexDir);
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoDir);
+    
+    int[] intArray = new int[iParams.getPartitionSize()];
+
+    TotalFacetCountsCache tfcc = TotalFacetCountsCache.getSingleton();
+    File tmpFile = _TestUtil.createTempFile("test", "tmp", TEMP_DIR);
+    tfcc.store(tmpFile, indexReader, taxoReader, iParams);
+    tfcc.clear(); // not really required because TFCC overrides on load(), but in the test we need not rely on this.
+    tfcc.load(tmpFile, indexReader, taxoReader, iParams);
+    
+    // now retrieve the one just loaded
+    TotalFacetCounts totalCounts = tfcc.getTotalCounts(indexReader, taxoReader, iParams);
+
+    int partition = 0;
+    for (int i = 0; i < expectedCounts.length; i += partitionSize) {
+      totalCounts.fillTotalCountsForPartition(intArray, partition);
+      int[] partitionExpectedCounts = new int[partitionSize];
+      int nToCopy = Math.min(partitionSize,expectedCounts.length-i);
+      System.arraycopy(expectedCounts, i, partitionExpectedCounts, 0, nToCopy);
+      assertTrue("Wrong counts! for partition "+partition+
+          "\nExpected:\n" + Arrays.toString(partitionExpectedCounts)+
+          "\nActual:\n" + Arrays.toString(intArray),
+          Arrays.equals(partitionExpectedCounts, intArray));
+      ++partition;
+    }
+    IOUtils.close(indexReader, taxoReader);
+    IOUtils.close(indexDir, taxoDir);
+    tmpFile.delete();
+  }
+
+}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/complements/TestTotalFacetCountsCache.java b/lucene/facet/src/test/org/apache/lucene/facet/complements/TestTotalFacetCountsCache.java
new file mode 100644
index 0000000..d70e608
--- /dev/null
+++ b/lucene/facet/src/test/org/apache/lucene/facet/complements/TestTotalFacetCountsCache.java
@@ -0,0 +1,503 @@
+package org.apache.lucene.facet.complements;
+
+import java.io.File;
+import java.io.IOException;
+import java.util.Collections;
+import java.util.List;
+import java.util.Random;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.analysis.MockTokenizer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.facet.FacetTestCase;
+import org.apache.lucene.facet.SlowRAMDirectory;
+import org.apache.lucene.facet.complements.TotalFacetCounts;
+import org.apache.lucene.facet.complements.TotalFacetCountsCache;
+import org.apache.lucene.facet.complements.TotalFacetCounts.CreationType;
+import org.apache.lucene.facet.index.FacetFields;
+import org.apache.lucene.facet.params.FacetIndexingParams;
+import org.apache.lucene.facet.params.FacetSearchParams;
+import org.apache.lucene.facet.search.CountFacetRequest;
+import org.apache.lucene.facet.search.FacetResult;
+import org.apache.lucene.facet.search.FacetResultNode;
+import org.apache.lucene.facet.search.FacetsCollector;
+import org.apache.lucene.facet.taxonomy.CategoryPath;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.MockDirectoryWrapper;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util._TestUtil;
+import org.junit.Before;
+import org.junit.Test;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+public class TestTotalFacetCountsCache extends FacetTestCase {
+
+  static final TotalFacetCountsCache TFC = TotalFacetCountsCache.getSingleton();
+
+  /**
+   * Thread class to be used in tests for this method. This thread gets a TFC
+   * and records times.
+   */
+  private static class TFCThread extends Thread {
+    private final IndexReader r;
+    private final DirectoryTaxonomyReader tr;
+    private final FacetIndexingParams iParams;
+    
+    TotalFacetCounts tfc;
+
+    public TFCThread(IndexReader r, DirectoryTaxonomyReader tr, FacetIndexingParams iParams) {
+      this.r = r;
+      this.tr = tr;
+      this.iParams = iParams;
+    }
+    @Override
+    public void run() {
+      try {
+        tfc = TFC.getTotalCounts(r, tr, iParams);
+      } catch (Exception e) {
+        throw new RuntimeException(e);
+      }
+    }
+  }
+
+  /** Utility method to add a document and facets to an index/taxonomy. */
+  private static void addFacets(FacetIndexingParams iParams, IndexWriter iw,
+      TaxonomyWriter tw, String... strings) throws IOException {
+    Document doc = new Document();
+    FacetFields facetFields = new FacetFields(tw, iParams);
+    facetFields.addFields(doc, Collections.singletonList(new CategoryPath(strings)));
+    iw.addDocument(doc);
+  }
+
+  /** Clears the cache and sets its size to one. */
+  private static void initCache() {
+    TFC.clear();
+    TFC.setCacheSize(1); // Set to keep one in memory
+  }
+
+  @Override
+  @Before
+  public void setUp() throws Exception {
+    super.setUp();
+    initCache();
+  }
+
+  /** runs few searches in parallel */
+  public void testGeneralSynchronization() throws Exception {
+    int numIters = atLeast(4);
+    Random random = random();
+    for (int i = 0; i < numIters; i++) {
+      int numThreads = random.nextInt(3) + 2; // 2-4
+      int sleepMillis = random.nextBoolean() ? -1 : random.nextInt(10) + 1 /*1-10*/;
+      int cacheSize = random.nextInt(4); // 0-3
+      doTestGeneralSynchronization(numThreads, sleepMillis, cacheSize);
+    }
+  }
+
+  private static final String[] CATEGORIES = new String[] { "a/b", "c/d", "a/e", "a/d", "c/g", "c/z", "b/a", "1/2", "b/c" };
+
+  private void index(Directory indexDir, Directory taxoDir) throws IOException {
+    IndexWriter indexWriter = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, null));
+    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
+    FacetFields facetFields = new FacetFields(taxoWriter);
+    
+    for (String cat : CATEGORIES) {
+      Document doc = new Document();
+      facetFields.addFields(doc, Collections.singletonList(new CategoryPath(cat, '/')));
+      indexWriter.addDocument(doc);
+    }
+    
+    IOUtils.close(indexWriter, taxoWriter);
+  }
+  
+  private void doTestGeneralSynchronization(int numThreads, int sleepMillis, int cacheSize) throws Exception {
+    TFC.setCacheSize(cacheSize);
+    SlowRAMDirectory slowIndexDir = new SlowRAMDirectory(-1, random());
+    MockDirectoryWrapper indexDir = new MockDirectoryWrapper(random(), slowIndexDir);
+    SlowRAMDirectory slowTaxoDir = new SlowRAMDirectory(-1, random());
+    MockDirectoryWrapper taxoDir = new MockDirectoryWrapper(random(), slowTaxoDir);
+
+    // Index documents without the "slowness"
+    index(indexDir, taxoDir);
+
+    slowIndexDir.setSleepMillis(sleepMillis);
+    slowTaxoDir.setSleepMillis(sleepMillis);
+    
+    // Open the slow readers
+    IndexReader slowIndexReader = DirectoryReader.open(indexDir);
+    TaxonomyReader slowTaxoReader = new DirectoryTaxonomyReader(taxoDir);
+
+    // Class to perform search and return results as threads
+    class Multi extends Thread {
+      private List<FacetResult> results;
+      private FacetIndexingParams iParams;
+      private IndexReader indexReader;
+      private TaxonomyReader taxoReader;
+
+      public Multi(IndexReader indexReader, TaxonomyReader taxoReader, FacetIndexingParams iParams) {
+        this.indexReader = indexReader;
+        this.taxoReader = taxoReader;
+        this.iParams = iParams;
+      }
+
+      public List<FacetResult> getResults() {
+        return results;
+      }
+
+      @Override
+      public void run() {
+        try {
+          FacetSearchParams fsp = new FacetSearchParams(iParams, new CountFacetRequest(new CategoryPath("a"), 10),
+              new CountFacetRequest(new CategoryPath("b"), 10));
+          IndexSearcher searcher = new IndexSearcher(indexReader);
+          FacetsCollector fc = FacetsCollector.create(fsp, indexReader, taxoReader);
+          searcher.search(new MatchAllDocsQuery(), fc);
+          results = fc.getFacetResults();
+        } catch (Exception e) {
+          throw new RuntimeException(e);
+        }
+      }
+    }
+
+    Multi[] multis = new Multi[numThreads];
+    for (int i = 0; i < numThreads; i++) {
+      multis[i] = new Multi(slowIndexReader, slowTaxoReader, FacetIndexingParams.ALL_PARENTS);
+    }
+
+    for (Multi m : multis) {
+      m.start();
+    }
+
+    // Wait for threads and get results
+    String[] expLabelsA = new String[] { "a/d", "a/e", "a/b" };
+    String[] expLabelsB = new String[] { "b/c", "b/a" };
+    for (Multi m : multis) {
+      m.join();
+      List<FacetResult> facetResults = m.getResults();
+      assertEquals("expected two results", 2, facetResults.size());
+      
+      FacetResultNode nodeA = facetResults.get(0).getFacetResultNode();
+      int i = 0;
+      for (FacetResultNode node : nodeA.subResults) {
+        assertEquals("wrong count", 1, (int) node.value);
+        assertEquals(expLabelsA[i++], node.label.toString('/'));
+      }
+      
+      FacetResultNode nodeB = facetResults.get(1).getFacetResultNode();
+      i = 0;
+      for (FacetResultNode node : nodeB.subResults) {
+        assertEquals("wrong count", 1, (int) node.value);
+        assertEquals(expLabelsB[i++], node.label.toString('/'));
+      }
+    }
+    
+    IOUtils.close(slowIndexReader, slowTaxoReader, indexDir, taxoDir);
+  }
+
+  /**
+   * Simple test to make sure the TotalFacetCountsManager updates the
+   * TotalFacetCounts array only when it is supposed to, and whether it
+   * is recomputed or read from disk.
+   */
+  @Test
+  public void testGenerationalConsistency() throws Exception {
+    // Create temporary RAMDirectories
+    Directory indexDir = newDirectory();
+    Directory taxoDir = newDirectory();
+
+    // Create our index/taxonomy writers
+    IndexWriter indexWriter = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, null));
+    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
+    FacetIndexingParams iParams = FacetIndexingParams.ALL_PARENTS;
+
+    // Add a facet to the index
+    addFacets(iParams, indexWriter, taxoWriter, "a", "b");
+
+    // Commit Changes
+    indexWriter.commit();
+    taxoWriter.commit();
+
+    // Open readers
+    DirectoryReader indexReader = DirectoryReader.open(indexDir);
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoDir);
+
+    // As this is the first time we have invoked the TotalFacetCountsManager, 
+    // we should expect to compute and not read from disk.
+    TotalFacetCounts totalCounts = TFC.getTotalCounts(indexReader, taxoReader, iParams);
+    int prevGen = assertRecomputed(totalCounts, 0, "after first attempt to get it!");
+
+    // Repeating same operation should pull from the cache - not recomputed. 
+    assertTrue("Should be obtained from cache at 2nd attempt",totalCounts == 
+      TFC.getTotalCounts(indexReader, taxoReader, iParams));
+
+    // Repeat the same operation as above. but clear first - now should recompute again
+    initCache();
+    totalCounts = TFC.getTotalCounts(indexReader, taxoReader, iParams);
+    prevGen = assertRecomputed(totalCounts, prevGen, "after cache clear, 3rd attempt to get it!");
+    
+    //store to file
+    File outputFile = _TestUtil.createTempFile("test", "tmp", TEMP_DIR);
+    initCache();
+    TFC.store(outputFile, indexReader, taxoReader, iParams);
+    totalCounts = TFC.getTotalCounts(indexReader, taxoReader, iParams);
+    prevGen = assertRecomputed(totalCounts, prevGen, "after cache clear, 4th attempt to get it!");
+
+    //clear and load
+    initCache();
+    TFC.load(outputFile, indexReader, taxoReader, iParams);
+    totalCounts = TFC.getTotalCounts(indexReader, taxoReader, iParams);
+    prevGen = assertReadFromDisc(totalCounts, prevGen, "after 5th attempt to get it!");
+
+    // Add a new facet to the index, commit and refresh readers
+    addFacets(iParams, indexWriter, taxoWriter, "c", "d");
+    IOUtils.close(indexWriter, taxoWriter);
+
+    TaxonomyReader newTaxoReader = TaxonomyReader.openIfChanged(taxoReader);
+    assertNotNull(newTaxoReader);
+    assertTrue("should have received more cagtegories in updated taxonomy", newTaxoReader.getSize() > taxoReader.getSize());
+    taxoReader.close();
+    taxoReader = newTaxoReader;
+    
+    DirectoryReader r2 = DirectoryReader.openIfChanged(indexReader);
+    assertNotNull(r2);
+    indexReader.close();
+    indexReader = r2;
+
+    // now use the new reader - should recompute
+    totalCounts = TFC.getTotalCounts(indexReader, taxoReader, iParams);
+    prevGen = assertRecomputed(totalCounts, prevGen, "after updating the index - 7th attempt!");
+
+    // try again - should not recompute
+    assertTrue("Should be obtained from cache at 8th attempt",totalCounts == 
+      TFC.getTotalCounts(indexReader, taxoReader, iParams));
+    
+    IOUtils.close(indexReader, taxoReader);
+    outputFile.delete();
+    IOUtils.close(indexDir, taxoDir);
+  }
+
+  private int assertReadFromDisc(TotalFacetCounts totalCounts, int prevGen, String errMsg) {
+    assertEquals("should read from disk "+errMsg, CreationType.Loaded, totalCounts.createType4test);
+    int gen4test = totalCounts.gen4test;
+    assertTrue("should read from disk "+errMsg, gen4test > prevGen);
+    return gen4test;
+  }
+  
+  private int assertRecomputed(TotalFacetCounts totalCounts, int prevGen, String errMsg) {
+    assertEquals("should recompute "+errMsg, CreationType.Computed, totalCounts.createType4test);
+    int gen4test = totalCounts.gen4test;
+    assertTrue("should recompute "+errMsg, gen4test > prevGen);
+    return gen4test;
+  }
+
+  /**
+   * This test is to address a bug in a previous version.  If a TFC cache is
+   * written to disk, and then the taxonomy grows (but the index does not change),
+   * and then the TFC cache is re-read from disk, there will be an exception
+   * thrown, as the integers are read off of the disk according to taxonomy
+   * size, which has changed.
+   */
+  @Test
+  public void testGrowingTaxonomy() throws Exception {
+    Directory indexDir = newDirectory();
+    Directory taxoDir = newDirectory();
+    
+    // Create our index/taxonomy writers
+    IndexWriter indexWriter = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, null));
+    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
+    FacetIndexingParams iParams = new FacetIndexingParams() {
+      @Override
+      public int getPartitionSize() {
+        return 2;
+      }
+    };
+    // Add a facet to the index
+    addFacets(iParams, indexWriter, taxoWriter, "a", "b");
+    // Commit Changes
+    indexWriter.commit();
+    taxoWriter.commit();
+
+    DirectoryReader indexReader = DirectoryReader.open(indexDir);
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoDir);
+
+    // Create TFC and write cache to disk
+    File outputFile = _TestUtil.createTempFile("test", "tmp", TEMP_DIR);
+    TFC.store(outputFile, indexReader, taxoReader, iParams);
+    
+    // Make the taxonomy grow without touching the index
+    for (int i = 0; i < 10; i++) {
+      taxoWriter.addCategory(new CategoryPath("foo", Integer.toString(i)));
+    }
+    taxoWriter.commit();
+    TaxonomyReader newTaxoReader = TaxonomyReader.openIfChanged(taxoReader);
+    assertNotNull(newTaxoReader);
+    taxoReader.close();
+    taxoReader = newTaxoReader;
+
+    initCache();
+
+    // With the bug, this next call should result in an exception
+    TFC.load(outputFile, indexReader, taxoReader, iParams);
+    TotalFacetCounts totalCounts = TFC.getTotalCounts(indexReader, taxoReader, iParams);
+    assertReadFromDisc(totalCounts, 0, "after reading from disk.");
+    
+    outputFile.delete();
+    IOUtils.close(indexWriter, taxoWriter, indexReader, taxoReader);
+    IOUtils.close(indexDir, taxoDir);
+  }
+
+  /**
+   * Test that a new TFC is only calculated and placed in memory (by two
+   * threads who want it at the same time) only once.
+   */
+  @Test
+  public void testMemoryCacheSynchronization() throws Exception {
+    SlowRAMDirectory indexDir = new SlowRAMDirectory(-1, null);
+    SlowRAMDirectory taxoDir = new SlowRAMDirectory(-1, null);
+
+    // Write index using 'normal' directories
+    IndexWriter w = new IndexWriter(indexDir, new IndexWriterConfig(
+        TEST_VERSION_CURRENT, new MockAnalyzer(random(), MockTokenizer.WHITESPACE, false)));
+    DirectoryTaxonomyWriter tw = new DirectoryTaxonomyWriter(taxoDir);
+    FacetIndexingParams iParams = FacetIndexingParams.ALL_PARENTS;
+    // Add documents and facets
+    for (int i = 0; i < 1000; i++) {
+      addFacets(iParams, w, tw, "facet", Integer.toString(i));
+    }
+    w.close();
+    tw.close();
+
+    indexDir.setSleepMillis(1);
+    taxoDir.setSleepMillis(1);
+
+    IndexReader r = DirectoryReader.open(indexDir);
+    DirectoryTaxonomyReader tr = new DirectoryTaxonomyReader(taxoDir);
+
+    // Create and start threads. Thread1 should lock the cache and calculate
+    // the TFC array. The second thread should block until the first is
+    // done, then successfully retrieve from the cache without recalculating
+    // or reading from disk.
+    TFCThread tfcCalc1 = new TFCThread(r, tr, iParams);
+    TFCThread tfcCalc2 = new TFCThread(r, tr, iParams);
+    tfcCalc1.start();
+    // Give thread 1 a head start to ensure correct sequencing for testing
+    Thread.sleep(5);
+    tfcCalc2.start();
+
+    tfcCalc1.join();
+    tfcCalc2.join();
+
+    // Since this test ends up with references to the same TFC object, we
+    // can only test the times to make sure that they are the same.
+    assertRecomputed(tfcCalc1.tfc, 0, "thread 1 should recompute");
+    assertRecomputed(tfcCalc2.tfc, 0, "thread 2 should recompute");
+    assertTrue("Both results should be the same (as their inputs are the same objects)",
+        tfcCalc1.tfc == tfcCalc2.tfc);
+
+    r.close();
+    tr.close();
+  }
+
+  /**
+   * Simple test to make sure the TotalFacetCountsManager updates the
+   * TotalFacetCounts array only when it is supposed to, and whether it
+   * is recomputed or read from disk, but this time with TWO different
+   * TotalFacetCounts
+   */
+  @Test
+  public void testMultipleIndices() throws IOException {
+    Directory indexDir1 = newDirectory(), indexDir2 = newDirectory();
+    Directory taxoDir1 = newDirectory(), taxoDir2 = newDirectory();
+    
+    // Create our index/taxonomy writers
+    IndexWriter indexWriter1 = new IndexWriter(indexDir1, newIndexWriterConfig(TEST_VERSION_CURRENT, null));
+    IndexWriter indexWriter2 = new IndexWriter(indexDir2, newIndexWriterConfig(TEST_VERSION_CURRENT, null));
+    TaxonomyWriter taxoWriter1 = new DirectoryTaxonomyWriter(taxoDir1);
+    TaxonomyWriter taxoWriter2 = new DirectoryTaxonomyWriter(taxoDir2);
+    FacetIndexingParams iParams = FacetIndexingParams.ALL_PARENTS;
+
+    // Add a facet to the index
+    addFacets(iParams, indexWriter1, taxoWriter1, "a", "b");
+    addFacets(iParams, indexWriter1, taxoWriter1, "d", "e");
+    // Commit Changes
+    indexWriter1.commit();
+    indexWriter2.commit();
+    taxoWriter1.commit();
+    taxoWriter2.commit();
+
+    // Open two readers
+    DirectoryReader indexReader1 = DirectoryReader.open(indexDir1);
+    DirectoryReader indexReader2 = DirectoryReader.open(indexDir2);
+    TaxonomyReader taxoReader1 = new DirectoryTaxonomyReader(taxoDir1);
+    TaxonomyReader taxoReader2 = new DirectoryTaxonomyReader(taxoDir2);
+
+    // As this is the first time we have invoked the TotalFacetCountsManager, we
+    // should expect to compute.
+    TotalFacetCounts totalCounts0 = TFC.getTotalCounts(indexReader1, taxoReader1, iParams);
+    int prevGen = -1;
+    prevGen = assertRecomputed(totalCounts0, prevGen, "after attempt 1");
+    assertTrue("attempt 1b for same input [0] shout find it in cache",
+        totalCounts0 == TFC.getTotalCounts(indexReader1, taxoReader1, iParams));
+    
+    // 2nd Reader - As this is the first time we have invoked the
+    // TotalFacetCountsManager, we should expect a state of NEW to be returned.
+    TotalFacetCounts totalCounts1 = TFC.getTotalCounts(indexReader2, taxoReader2, iParams);
+    prevGen = assertRecomputed(totalCounts1, prevGen, "after attempt 2");
+    assertTrue("attempt 2b for same input [1] shout find it in cache",
+        totalCounts1 == TFC.getTotalCounts(indexReader2, taxoReader2, iParams));
+
+    // Right now cache size is one, so first TFC is gone and should be recomputed  
+    totalCounts0 = TFC.getTotalCounts(indexReader1, taxoReader1, iParams);
+    prevGen = assertRecomputed(totalCounts0, prevGen, "after attempt 3");
+    
+    // Similarly will recompute the second result  
+    totalCounts1 = TFC.getTotalCounts(indexReader2, taxoReader2, iParams);
+    prevGen = assertRecomputed(totalCounts1, prevGen, "after attempt 4");
+
+    // Now we set the cache size to two, meaning both should exist in the
+    // cache simultaneously
+    TFC.setCacheSize(2);
+
+    // Re-compute totalCounts0 (was evicted from the cache when the cache was smaller)
+    totalCounts0 = TFC.getTotalCounts(indexReader1, taxoReader1, iParams);
+    prevGen = assertRecomputed(totalCounts0, prevGen, "after attempt 5");
+
+    // now both are in the larger cache and should not be recomputed 
+    totalCounts1 = TFC.getTotalCounts(indexReader2, taxoReader2, iParams);
+    assertTrue("with cache of size 2 res no. 0 should come from cache",
+        totalCounts0 == TFC.getTotalCounts(indexReader1, taxoReader1, iParams));
+    assertTrue("with cache of size 2 res no. 1 should come from cache",
+        totalCounts1 == TFC.getTotalCounts(indexReader2, taxoReader2, iParams));
+    
+    IOUtils.close(indexWriter1, indexWriter2, taxoWriter1, taxoWriter2);
+    IOUtils.close(indexReader1, indexReader2, taxoReader1, taxoReader2);
+    IOUtils.close(indexDir1, indexDir2, taxoDir1, taxoDir2);
+  }
+
+}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/encoding/EncodingSpeed.java b/lucene/facet/src/test/org/apache/lucene/facet/encoding/EncodingSpeed.java
new file mode 100644
index 0000000..dee37ea
--- /dev/null
+++ b/lucene/facet/src/test/org/apache/lucene/facet/encoding/EncodingSpeed.java
@@ -0,0 +1,649 @@
+package org.apache.lucene.facet.encoding;
+
+import java.io.IOException;
+import java.text.NumberFormat;
+import java.util.Arrays;
+import java.util.Locale;
+
+import org.apache.lucene.facet.encoding.DGapIntEncoder;
+import org.apache.lucene.facet.encoding.DGapVInt8IntEncoder;
+import org.apache.lucene.facet.encoding.EightFlagsIntEncoder;
+import org.apache.lucene.facet.encoding.FourFlagsIntEncoder;
+import org.apache.lucene.facet.encoding.IntDecoder;
+import org.apache.lucene.facet.encoding.IntEncoder;
+import org.apache.lucene.facet.encoding.NOnesIntEncoder;
+import org.apache.lucene.facet.encoding.SortingIntEncoder;
+import org.apache.lucene.facet.encoding.UniqueValuesIntEncoder;
+import org.apache.lucene.facet.encoding.VInt8IntEncoder;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IntsRef;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+public class EncodingSpeed {
+
+  private static int[] data3630 = null;
+  private static int[] data9910 = null;
+  private static int[] data501871 = null;
+  private static int[] data10k = null;
+  private static String resultsFormat = "%-60s %10s %20d %26s %20d %26s";
+  private static String headerFormat = "%-60s %10s %20s %26s %20s %26s";
+  private static int integers = 100000000;
+
+  private static NumberFormat nf;
+
+  public static void main(String[] args) throws IOException {
+    testFacetIDs(data3630, 3630);
+    testFacetIDs(data9910, 9910);
+    testFacetIDs(data10k, 10000);
+    testFacetIDs(data501871, 501871);
+  }
+
+  private static IntsRef newIntsRef(int[] data) {
+    IntsRef res = new IntsRef(data.length);
+    System.arraycopy(data, 0, res.ints, 0, data.length);
+    res.length = data.length;
+    return res;
+  }
+  
+  private static void testFacetIDs(int[] facetIDs, int docID) throws IOException {
+    int loopFactor = integers / facetIDs.length;
+    System.out
+        .println("\nEstimating ~"
+            + integers
+            + " Integers compression time by\nEncoding/decoding facets' ID payload of docID = "
+            + docID + " (unsorted, length of: " + facetIDs.length
+            + ") " + loopFactor + " times.");
+
+    System.out.println();
+    String header = String.format(Locale.ROOT, headerFormat, "Encoder", "Bits/Int",
+        "Encode Time", "Encode Time", "Decode Time", "Decode Time");
+
+    System.out.println(header);
+    String header2 = String.format(Locale.ROOT, headerFormat, "", "", "[milliseconds]",
+        "[microsecond / int]", "[milliseconds]", "[microsecond / int]");
+
+    System.out.println(header2);
+
+    char[] separator = header.toCharArray();
+    Arrays.fill(separator, '-');
+    System.out.println(separator);
+
+    encoderTest(new VInt8IntEncoder(), facetIDs, loopFactor);
+    encoderTest(new SortingIntEncoder(new UniqueValuesIntEncoder(new VInt8IntEncoder())), facetIDs, loopFactor);
+    encoderTest(new SortingIntEncoder(new UniqueValuesIntEncoder(new DGapIntEncoder(new VInt8IntEncoder()))), facetIDs, loopFactor);
+    encoderTest(new SortingIntEncoder(new UniqueValuesIntEncoder(new DGapVInt8IntEncoder())), facetIDs, loopFactor);
+    encoderTest(new SortingIntEncoder(new UniqueValuesIntEncoder(new DGapIntEncoder(new EightFlagsIntEncoder()))), facetIDs, loopFactor);
+    encoderTest(new SortingIntEncoder(new UniqueValuesIntEncoder(new DGapIntEncoder(new FourFlagsIntEncoder()))), facetIDs, loopFactor);
+    encoderTest(new SortingIntEncoder(new UniqueValuesIntEncoder(new DGapIntEncoder(new NOnesIntEncoder(3)))), facetIDs, loopFactor);
+    encoderTest(new SortingIntEncoder(new UniqueValuesIntEncoder(new DGapIntEncoder(new NOnesIntEncoder(4)))), facetIDs, loopFactor);
+
+    System.out.println();
+  }
+
+  private static void encoderTest(IntEncoder encoder, int[] values, int loopFactor) throws IOException {
+
+    BytesRef bytes = new BytesRef(values.length); // at least one byte per value
+
+    // -- Looping 100 times as a warm up --------------------------
+    for (int i = 100; i != 0; --i) {
+      IntsRef data = newIntsRef(values);
+      encoder.encode(data, bytes);
+    }
+    // -----------------------------------------------------------
+
+    long encodeTime = 0;
+    for (int factor = loopFactor; factor > 0; --factor) {
+      IntsRef data = newIntsRef(values);
+      long start = System.currentTimeMillis();
+      encoder.encode(data, bytes);
+      encodeTime += System.currentTimeMillis() - start;
+    }
+
+    IntsRef decoded = new IntsRef(values.length);
+    int encodedSize = bytes.length;
+    IntDecoder decoder = encoder.createMatchingDecoder();
+    
+    // -- Looping 100 times as a warm up --------------------------
+    for (int i = 100; i != 0; --i) {
+      decoder.decode(bytes, decoded);
+    }
+    // -----------------------------------------------------------
+
+    long decodeTime = 0;
+    for (int i = loopFactor; i > 0; --i) {
+      long start = System.currentTimeMillis();
+      decoder.decode(bytes, decoded);
+      decodeTime += System.currentTimeMillis() - start;
+    }
+    
+    if (decoded.length != values.length) {
+      throw new RuntimeException("wrong num values. expected=" + values.length + " actual=" + decoded.length + 
+          " decoder=" + decoder);
+    }
+
+    System.out.println(String.format(Locale.ROOT, resultsFormat, encoder, 
+        nf.format(encodedSize * 8.0 / values.length), 
+        encodeTime, 
+        nf.format(encodeTime * 1000000.0 / (loopFactor * values.length)), 
+        decodeTime, 
+        nf.format(decodeTime * 1000000.0 / (loopFactor * values.length))));
+  }
+
+  static {
+    nf = NumberFormat.getInstance(Locale.ROOT);
+    nf.setMaximumFractionDigits(4);
+    nf.setMinimumFractionDigits(4);
+
+    data9910 = new int[] { 2, 4, 149085, 11, 12292, 69060, 69061, 149309,
+        99090, 568, 5395, 149310, 3911, 149311, 149312, 148752, 1408,
+        1410, 1411, 1412, 4807, 1413, 1414, 1417, 1415, 1416, 1418,
+        1420, 470, 4808, 1422, 1423, 1424, 4809, 4810, 1427, 1429,
+        1430, 4811, 1432, 1433, 3752, 1435, 3753, 1437, 1439, 1440,
+        4812, 1442, 1443, 4813, 1445, 1446, 1447, 4814, 4815, 1450,
+        4816, 353, 1452, 89004, 1624, 1625, 2052, 1626, 1627, 63991,
+        725, 726, 727, 728, 35543, 729, 730, 731, 1633, 733, 734, 735,
+        37954, 737, 738, 76315, 23068, 76316, 1634, 740, 741, 742, 744,
+        745, 76317, 15645, 748, 17488, 2904, 89005, 752, 753, 89006,
+        754, 755, 756, 757, 41, 261, 758, 89007, 760, 762, 763, 89008,
+        764, 765, 766, 85930, 165, 768, 149313, 33593, 149314, 149315,
+        81589, 39456, 15467, 1296, 149316, 39457, 2235, 144, 2236,
+        2309, 3050, 2237, 2311, 89692, 2240, 2241, 2243, 2244, 2245,
+        2246, 2314, 12856, 2248, 2250, 2251, 2253, 2254, 12857, 7677,
+        12858, 39149, 2257, 23147, 3303, 2258, 7422, 2322, 2262, 2317,
+        2263, 7423, 24264, 2232, 89693, 12862, 89694, 12863, 12864,
+        23201, 2329, 33019, 2255, 12865, 3517, 2492, 2277, 2280, 2267,
+        2260, 25368, 12866, 2281, 2282, 2283, 12867, 2284, 9055, 2287,
+        125133, 2337, 2286, 2288, 2338, 125134, 2290, 125135, 12869,
+        965, 966, 1298, 17945, 1300, 970, 971, 972, 973, 974, 296,
+        17946, 1303, 1391, 902, 1304, 1395, 1308, 1309, 1310, 1312,
+        967, 9414, 1315, 1317, 1318, 9415, 1321, 23592, 1322, 22433,
+        1323, 1324, 1326, 109241, 31225, 1330, 1331, 2540, 27196, 1332,
+        1334, 1335, 11999, 414, 340, 3651, 44040, 31995, 1344, 1343,
+        4618, 116770, 116771, 1474, 1349, 42122, 14199, 149317, 451,
+        149318, 29, 14200, 14198, 14201, 1979, 1980, 1981, 3132, 3147,
+        34090, 1987, 12770, 1329, 80818, 80819, 1988, 23522, 1986,
+        15880, 1985, 32975, 1992, 1993, 7165, 3141, 3143, 86346, 1982,
+        1984, 3145, 86347, 78064, 23456, 29578, 3136, 17752, 4710,
+        4711, 4712, 149319, 424, 4713, 95735, 4715, 149320, 4717, 4718,
+        149321, 192, 149322, 108126, 29976, 5404, 38059, 5406, 2030,
+        289, 1804, 1557, 1558, 94080, 29651, 94317, 1561, 1562, 1563,
+        1565, 24632, 1927, 1928, 1566, 1570, 1571, 1572, 1573, 1574,
+        1575, 94318, 1576, 2674, 9351, 94319, 94320, 2677, 2678, 29654,
+        2946, 2945, 2682, 2683, 2947, 3102, 3402, 3104, 4780, 3106,
+        3107, 3108, 3109, 3110, 3111, 3112, 3113, 3114, 3116, 3117,
+        3118, 19610, 44805, 3119, 3407, 3121, 3122, 3124, 3126, 3127,
+        41745, 41746, 3130, 459, 460, 461, 462, 463, 464, 466, 467,
+        40306, 468, 471, 472, 40307, 4467, 475, 476, 477, 478, 479,
+        40308, 481, 482, 20129, 483, 484, 485, 486, 4473, 488, 489,
+        458, 491, 40309, 494, 495, 496, 497, 499, 500, 501, 502, 355,
+        356, 1549, 358, 359, 360, 37971, 362, 2579, 2581, 24578, 2583,
+        24579, 2586, 2587, 2588, 2590, 2591, 24580, 24581, 3666, 24582,
+        2594, 24583, 2596, 2597, 24584, 2599, 18013, 24585, 2601,
+        49361, 280, 3969, 11651, 11652, 3926, 5103, 11653, 11654,
+        11655, 6896, 417, 168, 149323, 11268, 11657, 38089, 59517,
+        149324, 38092, 149325, 5110, 38094, 59520, 38096, 38097, 28916,
+        59703, 4992, 149326, 32383, 2478, 3985, 2479, 2480, 2481, 2482,
+        2483, 2484, 2485, 2486, 24146, 22184, 2488, 2489, 2490, 2494,
+        2493, 18043, 2495, 2542, 2497, 5062, 2499, 2501, 24147, 24148,
+        2504, 2505, 2506, 2507, 2508, 394, 2660, 2509, 2511, 24149,
+        2512, 2513, 2514, 3988, 4410, 3989, 2518, 2522, 2521, 24150,
+        12082, 2524, 3990, 24151, 387, 24152, 2529, 2530, 2528, 3991,
+        24153, 2534, 24154, 2536, 24155, 2538, 22510, 6332, 3554, 5309,
+        7700, 6333, 6334, 6335, 6336, 6337, 5693, 117020, 6339, 149327,
+        149328, 149329, 6340, 6343, 117022, 4324, 283, 284, 285, 286,
+        2688, 287, 2689, 288, 8880, 290, 291, 2690, 292, 295, 294,
+        24543, 13899, 297, 298, 299, 300, 303, 301, 59178, 302, 8881,
+        34403, 13900, 17811, 305, 307, 306, 308, 2727, 368, 364,
+        110416, 1587, 366, 367, 2692, 26624, 7233, 9082, 35684, 7250,
+        13902, 304, 13903, 991, 110417, 273, 274, 275, 276, 277, 278,
+        41095, 281, 282, 4419, 2768, 229, 230, 231, 232, 233, 234, 235,
+        236, 237, 1065, 239, 2745, 2746, 240, 9250, 241, 242, 244, 245,
+        9251, 246, 247, 248, 249, 250, 251, 253, 254, 255, 9252, 257,
+        258, 259, 9253, 9254, 2751, 265, 266, 267, 268, 9255, 9256,
+        270, 271, 9257, 238, 1024, 829, 1025, 1026, 1028, 1029, 1030,
+        9258, 1032, 1033, 1034, 1027, 1035, 1036, 9259, 1037, 1038,
+        1039, 4594, 4429, 1041, 1042, 1043, 70332, 1045, 1046, 1047,
+        1048, 21128, 1050, 122787, 72433, 1052, 2762, 1054, 1055, 1056,
+        9548, 1057, 71311, 1058, 1059, 1060, 61413, 2765, 4436, 1064,
+        1066, 11610, 3485, 22357, 104580, 149330, 149331, 15471, 5679,
+        5680, 687, 5683, 5684, 953, 8849, 102120, 149332, 5688, 5689,
+        149333, 6920, 60202, 33855, 33856, 33857, 19163, 33858, 3491,
+        149334, 914, 2202, 916, 917, 919, 920, 921, 922, 3568, 924,
+        925, 926, 927, 928, 929, 8752, 931, 932, 933, 934, 3570, 1876,
+        9138, 1877, 1878, 2210, 1880, 1881, 3571, 1883, 1884, 2212,
+        1886, 2214, 1888, 1889, 1890, 8753, 1891, 1892, 1893, 1894,
+        1895, 1896, 1898, 2217, 3572, 1901, 1902, 688, 2219, 107, 1904,
+        1905, 3573, 1907, 3323, 1909, 1910, 1911, 8754, 1912, 55911,
+        1913, 1914, 3574, 1741, 3575, 1916, 2226, 3576, 1919, 2227,
+        1920, 3577, 3578, 2229, 1923, 85396, 174, 175, 114875, 178,
+        180, 181, 182, 1477, 185, 186, 172, 187, 188, 85397, 85398,
+        190, 191, 891, 893, 19778, 18068, 895, 897, 896, 25985, 894,
+        900, 361, 1206, 193, 194, 195, 196, 197, 198, 199, 200, 55009,
+        201, 33266, 29064, 204, 205, 40129, 206, 207, 208, 2842, 209,
+        210, 211, 212, 149335, 870, 871, 18005, 872, 18006, 874, 875,
+        876, 1479, 1480, 1481, 879, 881, 57212, 2779, 57213, 886, 887,
+        57214, 57215, 889, 890, 806, 69305, 808, 809, 86327, 812, 813,
+        814, 815, 26724, 816, 69307, 43484, 818, 819, 63904, 820, 821,
+        822, 86328, 13498, 824, 825, 12218, 149336, 49042, 4464, 4466,
+        35536, 73245, 73246, 474, 73247, 480, 46247, 29624, 21086,
+        73248, 490, 493, 73249, 73250, 401, 403, 405, 2860, 15483,
+        74826, 408, 409, 74827, 410, 411, 413, 74828, 415, 2863, 68707,
+        33284, 2865, 2866, 2867, 2868, 2869, 2870, 17976, 3032, 38498,
+        7350, 2876, 2874, 24506, 918, 923, 64562, 64563, 32648, 930,
+        1875, 32649, 1879, 32650, 1882, 1887, 32651, 64564, 32652,
+        1897, 32653, 18170, 1900, 32654, 1906, 1915, 64565, 1921, 1922,
+        90662, 2234, 37449, 8886, 37450, 7418, 37451, 37452, 37453,
+        37454, 1609, 1610, 1611, 1612, 113456, 1212, 1616, 1617,
+        113457, 1615, 1619, 113458, 1620, 8747, 113459, 8748, 42233,
+        78065, 42235, 2149, 42236, 78066, 42237, 42238, 4335, 42239,
+        78067, 42241, 78068, 42243, 78069, 42244, 78070, 54587, 12993,
+        2040, 1130, 1131, 51172, 1133, 1134, 1135, 1136, 1137, 1138,
+        1139, 1140, 1141, 149337, 1115, 5178, 149338, 452, 7784, 21522,
+        1361, 103718, 149339, 15990, 79432, 149340, 4232, 149341,
+        15998, 53917, 15996, 53918, 149342, 149343, 97544, 53920,
+        97546, 841, 1954, 842, 41926, 844, 2589, 845, 846, 27370, 848,
+        849, 41927, 25165, 852, 1956, 854, 856, 1957, 855, 1959, 35170,
+        23055, 75673, 116783, 857, 116784, 851, 116785, 858, 859, 860,
+        861, 57422, 1964, 864, 866, 867, 1965, 1966, 1968, 1969, 2989,
+        116786, 1972, 1973, 116787, 1975, 1976, 1977, 2580, 39540,
+        2585, 39541, 21755, 39542, 2592, 34859, 2593, 39543, 38540,
+        2595, 39544, 149344, 35433, 81849, 35434, 40257, 873, 877,
+        2778, 32040, 882, 883, 884, 885, 888, 3358, 1559, 1560, 1438,
+        25387, 1569, 38135, 66925, 2673, 3095, 2679, 59053, 25443,
+        34369, 1983, 17749, 9343, 1989, 13565, 31525, 61690, 18165,
+        17751, 78234, 26506, 9348, 20307, 18154, 3133, 2572, 3134,
+        12131, 19770, 48724, 25759, 13549, 65465, 19936, 13545, 25645,
+        4786, 15756, 19547, 1581, 92226, 1362, 21524, 13059, 23717,
+        149345, 20198, 27123, 149346, 149347, 26030, 27126, 27652,
+        10538, 1667, 40282, 14134, 40284, 16368, 149348, 40287, 8870,
+        40288, 149349, 40289, 149350, 149351, 40295, 10424, 7012,
+        13178, 45608, 10423, 13181, 4201, 672, 13182, 10174, 10607,
+        13183, 580, 149352, 149353, 96298, 53691, 3721, 66048, 21584,
+        149354, 48206, 48207, 149355, 1405, 1406, 1407, 11162, 577,
+        149356, 6941, 6942, 16583, 1284, 10511, 16584, 16585, 422, 423,
+        1249, 1244, 1245, 1247, 2544, 1248, 1250, 2545, 1252, 2547,
+        1253, 2549, 1259, 1257, 1258, 1260, 1261, 2551, 1262, 1263,
+        1264, 1265, 2553, 1266, 17795, 2554, 17796, 1270, 1271, 1273,
+        17797, 2556, 1275, 1276, 2557, 1277, 1278, 1279, 1280, 1282,
+        68, 69, 5080, 5256, 6869, 10148, 6960, 10150, 149357, 10152,
+        14413, 149358, 14414, 56037, 651, 56038, 131797, 555, 14415,
+        14416, 149359, 149360, 56042, 14418, 149361, 149, 56043, 97512,
+        34512, 797, 7396, 9395, 9396, 9397, 63417, 805, 23984, 13665,
+        10452, 55147, 5656, 53, 4348, 4349, 4350, 148488, 13669, 6527,
+        149362, 11374, 11376, 11377, 8092, 11378, 11380, 152, 5013,
+        8093, 561, 11381, 5623, 4176, 26840, 3564, 3565, 3708, 3567,
+        18783, 18784, 4039, 10540, 18786, 30100, 30101, 1528, 149363,
+        19561, 19562, 19563, 19564, 1110, 134146, 10600, 149364, 10602,
+        149365, 149366, 10603, 10604, 4981, 57075, 37508, 149367,
+        34589, 1209, 149368, 19592, 19593, 7620, 9674, 3481, 10240,
+        81835, 8001, 33872, 8907, 55155, 1585, 31731, 49694, 25760,
+        31733, 903, 904, 2539, 49695, 1194, 1195, 1196, 31734, 1197,
+        1198, 1199, 1593, 899, 1200, 1201, 9276, 1202, 40181, 40482,
+        55718, 80833, 24596, 3669, 15699, 55720, 55721, 40481, 3672,
+        39826, 80363, 2602, 2603, 2604, 62126, 2605, 2606, 2607, 8714,
+        2608, 2609, 2610, 2612, 149369, 2894, 15241, 15242, 15262,
+        5384, 20290, 20291, 7792, 20295, 64413, 39236, 18011, 71494,
+        898, 51015, 19782, 105107, 149370, 7634, 149371, 149372,
+        115458, 22821, 19894, 2213, 66926 };
+
+    data3630 = new int[] { 2, 4, 86133, 11, 16505, 86134, 86135, 86136,
+        1290, 86137, 86138, 32473, 19346, 32474, 4922, 32475, 86139,
+        16914, 86140, 86141, 86142, 86143, 32478, 86144, 86145, 32480,
+        4884, 4887, 32481, 86146, 16572, 86147, 16295, 165, 86148,
+        3183, 21920, 21921, 21922, 555, 4006, 32484, 21925, 21926,
+        13775, 86149, 13777, 85833, 85834, 13779, 13773, 13780, 75266,
+        17674, 13784, 13785, 13786, 13787, 13788, 6258, 86150, 13790,
+        75267, 13793, 13794, 13795, 312, 4914, 4915, 6222, 86151, 4845,
+        4883, 4918, 4894, 4919, 86152, 4921, 6223, 6224, 6225, 6226,
+        67909, 6229, 18170, 6230, 5198, 25625, 6231, 6232, 6233, 1808,
+        6234, 6235, 6236, 41376, 6238, 6239, 67911, 6240, 86153, 6243,
+        6244, 83549, 6246, 6247, 6248, 6249, 782, 444, 6251, 6250,
+        19863, 28963, 310, 2234, 144, 2236, 2309, 69437, 2311, 2325,
+        2241, 69438, 69439, 2244, 2245, 2246, 23504, 2314, 69440,
+        36603, 2250, 2268, 2271, 2251, 2254, 2255, 2257, 2240, 36604,
+        84726, 36605, 84727, 2262, 2263, 18431, 38853, 2317, 2149,
+        2326, 2327, 2329, 3980, 2275, 2277, 2258, 84728, 2260, 84729,
+        84730, 13766, 36607, 2282, 2283, 84731, 2284, 2286, 2287, 2337,
+        7424, 2288, 2338, 3522, 2290, 84733, 32902, 371, 37708, 2096,
+        3065, 3066, 375, 377, 374, 378, 2100, 86154, 381, 382, 58795,
+        379, 383, 384, 385, 4449, 387, 388, 389, 390, 9052, 391, 18358,
+        2107, 394, 2111, 2108, 393, 2109, 395, 86155, 86156, 397, 2113,
+        398, 399, 400, 273, 274, 275, 40980, 276, 277, 31716, 279, 280,
+        31717, 281, 282, 1628, 1623, 1624, 1625, 2052, 1626, 725, 727,
+        728, 729, 730, 731, 1633, 733, 734, 735, 86157, 737, 738, 739,
+        1634, 3563, 3564, 3565, 1667, 12461, 76276, 3567, 5413, 77622,
+        5415, 5416, 5417, 5418, 107, 86158, 7784, 15363, 153, 3723,
+        2713, 7786, 3835, 7787, 86159, 7789, 7791, 7792, 7794, 86160,
+        7796, 86161, 6708, 7798, 7799, 7800, 7801, 7802, 7803, 1665,
+        43150, 15365, 1581, 5656, 43152, 80258, 7450, 39922, 86162,
+        51587, 9059, 4606, 396, 86163, 86164, 7250, 401, 403, 2860,
+        33281, 2964, 408, 9119, 409, 86165, 7669, 2861, 410, 413,
+        86166, 414, 415, 33282, 405, 33283, 7498, 2865, 7230, 33284,
+        2866, 86167, 2867, 47518, 2868, 86168, 2869, 2870, 4712, 7096,
+        28484, 6913, 6914, 6915, 6916, 37169, 37170, 7103, 28269, 6919,
+        86169, 45431, 6922, 7104, 6923, 7108, 6924, 6925, 6926, 6927,
+        6928, 86170, 86171, 86172, 6930, 6931, 6932, 6934, 6935, 6936,
+        451, 6937, 6938, 4756, 3554, 5309, 8145, 3586, 16417, 9767,
+        14126, 25854, 6580, 10174, 86173, 5519, 21309, 8561, 20938,
+        10386, 86174, 781, 2030, 16419, 30323, 16420, 16421, 16424,
+        86175, 86176, 86177, 28871, 86178, 28872, 63980, 6329, 49561,
+        4271, 38778, 86179, 86180, 20126, 16245, 193, 195, 196, 197,
+        56973, 199, 200, 201, 202, 203, 204, 56974, 56975, 205, 206,
+        4662, 207, 208, 209, 210, 211, 212, 47901, 641, 642, 643, 1380,
+        1079, 47902, 1381, 1081, 1082, 1083, 47903, 1382, 47904, 1087,
+        47905, 965, 966, 1298, 968, 1387, 1300, 50288, 971, 972, 973,
+        974, 23974, 22183, 1390, 23313, 1389, 1391, 902, 23029, 296,
+        1304, 1395, 1303, 1309, 1308, 50289, 1312, 50290, 50291, 1315,
+        1317, 9270, 19796, 3605, 1320, 1321, 44946, 1322, 1323, 50292,
+        967, 1587, 1326, 1331, 17482, 633, 29115, 53858, 29118, 29119,
+        62624, 44494, 6965, 6966, 6959, 6967, 71562, 6969, 23459,
+        23460, 17464, 4225, 23461, 23462, 23463, 5893, 23464, 17467,
+        17468, 23465, 12562, 1405, 1406, 1407, 960, 961, 962, 687, 963,
+        86181, 86182, 5997, 10812, 11976, 11977, 1850, 577, 13393,
+        10810, 13394, 65040, 86183, 3935, 3936, 3937, 710, 86184, 5785,
+        5786, 29949, 5787, 5788, 283, 284, 2687, 285, 286, 287, 2689,
+        288, 289, 8880, 290, 2690, 13899, 991, 292, 295, 42007, 35616,
+        63103, 298, 299, 3520, 297, 9024, 303, 301, 302, 300, 31345,
+        3719, 304, 305, 306, 307, 308, 368, 364, 85002, 9026, 63105,
+        367, 39596, 25835, 19746, 293, 294, 26505, 85003, 18377, 56785,
+        10122, 10123, 10124, 86185, 39863, 86186, 10125, 39865, 4066,
+        4067, 24257, 4068, 4070, 86187, 4073, 4074, 86188, 4076, 7538,
+        4077, 86189, 4078, 4079, 7540, 7541, 4084, 4085, 7542, 86190,
+        4086, 86191, 4087, 4088, 86192, 7545, 44874, 7821, 44875,
+        86193, 4286, 86194, 51470, 17609, 1408, 47486, 1411, 1412,
+        47487, 1413, 1414, 1417, 1415, 47488, 1416, 1418, 1420, 470,
+        1422, 1423, 1424, 5001, 5002, 47489, 1427, 1429, 1430, 31811,
+        1432, 1433, 47490, 1435, 3753, 1437, 1439, 1440, 47491, 1443,
+        47492, 1446, 5004, 5005, 1450, 47493, 353, 1452, 42145, 3103,
+        3402, 3104, 3105, 4780, 3106, 3107, 3108, 12157, 3111, 42146,
+        42147, 3114, 4782, 42148, 3116, 3117, 42149, 42150, 3407, 3121,
+        3122, 18154, 3126, 3127, 3128, 3410, 3130, 3411, 3412, 3415,
+        24241, 3417, 3418, 3449, 42151, 3421, 3422, 7587, 42152, 3424,
+        3427, 3428, 3448, 3430, 3432, 42153, 42154, 41648, 1991, 407,
+        57234, 411, 2862, 57235, 2863, 18368, 57236, 2874, 7350, 4115,
+        2876, 2877, 17975, 86195, 4116, 2881, 2882, 2883, 2886, 463,
+        870, 872, 873, 874, 875, 8783, 8784, 877, 1480, 1481, 459,
+        2778, 881, 8785, 2779, 8786, 8787, 8788, 886, 887, 8789, 889,
+        8790, 86196, 6920, 86197, 5080, 5081, 7395, 7396, 9395, 9396,
+        1528, 42737, 805, 86198, 1209, 13595, 4126, 9680, 34368, 9682,
+        86199, 86200, 174, 175, 176, 177, 178, 179, 180, 182, 183,
+        1477, 31138, 186, 172, 187, 188, 189, 190, 191, 458, 871,
+        31294, 31295, 27604, 31296, 31297, 882, 883, 884, 31298, 890,
+        1089, 1488, 1489, 1092, 1093, 1094, 1095, 1096, 1097, 1490,
+        1098, 1495, 1502, 1099, 1100, 1101, 1493, 2997, 12223, 1103,
+        2654, 1498, 1499, 1500, 80615, 80616, 80617, 33359, 86201,
+        9294, 1501, 86202, 1506, 1507, 23454, 38802, 38803, 1014,
+        86203, 5583, 5584, 651, 74717, 5586, 5587, 5588, 5589, 74720,
+        5590, 38808, 33527, 78330, 10930, 5119, 10931, 1000, 10928,
+        10932, 10933, 10934, 10935, 5863, 10936, 86204, 10938, 10939,
+        86205, 192, 194, 38754, 38755, 198, 38756, 38757, 38758, 2842,
+        640, 22780, 22781, 1080, 86206, 86207, 1084, 1086, 1088, 63916,
+        9412, 970, 9413, 9414, 9415, 9416, 9417, 1310, 7168, 7169,
+        1318, 9418, 1324, 39159, 1804, 1557, 24850, 41499, 1560, 41500,
+        1562, 1563, 1565, 1927, 1928, 1566, 1569, 1570, 1571, 1572,
+        1573, 1574, 1575, 1576, 2674, 2677, 2678, 2679, 2946, 2682,
+        2676, 2683, 2947, 1156, 1157, 1158, 1467, 1160, 1468, 1469,
+        1161, 1162, 1163, 4369, 1165, 1166, 1167, 12923, 2917, 1169,
+        1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 18153, 8359,
+        1178, 1164, 1191, 1180, 12924, 86208, 86209, 54817, 66962,
+        2476, 86210, 86211, 41820, 41821, 41822, 41824, 1130, 1131,
+        1132, 32692, 1134, 34848, 1136, 1133, 1137, 1138, 1139, 1140,
+        1141, 1143, 1144, 1145, 34849, 2639, 34850, 1146, 1147, 1148,
+        34851, 1150, 1151, 1152, 1153, 1154, 1155, 1678, 1679, 1680,
+        1681, 40870, 2059, 1685, 1686, 32686, 14970, 1688, 1689, 86212,
+        1692, 1682, 1693, 1695, 1696, 1698, 12955, 8909, 41690, 1700,
+        41691, 86213, 30949, 41692, 1703, 1704, 1705, 41693, 14976,
+        1708, 2071, 1709, 1710, 1711, 1712, 1727, 86214, 86215, 86216,
+        1715, 86217, 1714, 1717, 1690, 41697, 86218, 1720, 86219, 2073,
+        41699, 1724, 2075, 1726, 1729, 1730, 1732, 2078, 2223, 1735,
+        1713, 41700, 1737, 14977, 1739, 1740, 1741, 2080, 1743, 1744,
+        1745, 1746, 1747, 1748, 1749, 1750, 1751, 41701, 1752, 1753,
+        1909, 86220, 2085, 1754, 19548, 86221, 19551, 5733, 3856, 5190,
+        4581, 25145, 86222, 86223, 4846, 86224, 4861, 86225, 86226,
+        86227, 25150, 86228, 86229, 13820, 2027, 4898, 4899, 4901,
+        2135, 4902, 4868, 4904, 86230, 4905, 25155, 4907, 86231, 4909,
+        4910, 4911, 4912, 86232, 6220, 81357, 86233, 2589, 73877,
+        29706, 6227, 6228, 86234, 6237, 86235, 6241, 6242, 1812, 13808,
+        13809, 70908, 2293, 2294, 86236, 2295, 2296, 2297, 22947,
+        16511, 2299, 2300, 2301, 13097, 73079, 86237, 13099, 50121,
+        86238, 86239, 13101, 86240, 2424, 4725, 4726, 4727, 4728, 4729,
+        4730, 86241, 26881, 10944, 4734, 4735, 4736, 26239, 26240,
+        71408, 86242, 57401, 71410, 26244, 5344, 26245, 86243, 4102,
+        71414, 11091, 6736, 86244, 6737, 6738, 38152, 6740, 6741, 6742,
+        6298, 6743, 6745, 6746, 20867, 6749, 20616, 86245, 9801, 65297,
+        20617, 65298, 20619, 5629, 65299, 20621, 20622, 8385, 20623,
+        20624, 5191, 20625, 20626, 442, 443, 445, 27837, 77681, 86246,
+        27839, 86247, 86248, 41435, 66511, 2478, 2479, 2480, 2481,
+        2482, 2483, 2484, 2485, 2486, 2487, 2488, 2489, 2490, 2494,
+        2493, 33025, 12084, 2542, 2497, 2499, 2501, 2503, 2504, 2505,
+        33026, 2506, 2507, 2508, 2509, 2511, 1787, 12080, 2513, 2514,
+        3988, 3176, 3989, 2518, 2521, 9285, 2522, 2524, 2525, 3990,
+        2527, 2528, 27499, 2529, 2530, 3991, 2532, 2534, 2535, 18038,
+        2536, 2538, 2495, 46077, 61493, 61494, 1006, 713, 4971, 4972,
+        4973, 4975, 4976, 650, 170, 7549, 7550, 7551, 7552, 7553,
+        86249, 7936, 956, 11169, 11170, 1249, 1244, 1245, 1247, 2544,
+        1250, 2545, 1252, 2547, 1253, 1254, 2549, 39636, 1259, 1257,
+        1258, 39637, 1260, 1261, 2551, 1262, 1263, 848, 86250, 86251,
+        854, 74596, 856, 1957, 86252, 855, 1959, 1961, 857, 86253, 851,
+        859, 860, 862, 1964, 864, 865, 866, 867, 1965, 1966, 1967,
+        1968, 1969, 86254, 1971, 1972, 1973, 1974, 1975, 1976, 1977,
+        841, 1954, 842, 2978, 846, 847, 849, 850, 852, 1956, 17452,
+        71941, 86255, 86256, 73665, 1471, 13690, 185, 503, 504, 2342,
+        505, 506, 4378, 508, 4379, 17313, 510, 511, 512, 520, 513,
+        4384, 17314, 514, 515, 46158, 17317, 518, 34269, 519, 4386,
+        523, 524, 525, 46159, 528, 529, 17319, 531, 532, 533, 534, 535,
+        7482, 537, 538, 5267, 536, 539, 541, 540, 19858, 17320, 17321,
+        906, 907, 908, 17322, 910, 17323, 912, 15850, 913, 4398, 17324,
+        86257, 278, 2948, 2949, 2950, 3007, 2951, 2952, 2953, 2954,
+        2955, 3013, 35352, 3014, 3015, 2962, 3016, 33505, 39118, 3017,
+        3018, 20492, 4000, 3021, 3022, 35353, 39293, 3024, 18443, 3029,
+        9467, 20529, 39119, 8380, 2965, 3030, 3043, 22714, 39120, 2956,
+        3035, 39121, 3037, 3038, 2688, 86258, 36675, 30894, 24505,
+        8888, 13541, 49728, 27660, 9082, 27661, 365, 366, 2232, 76098,
+        7233, 1494, 17391, 606, 607, 611, 610, 612, 614, 615, 613, 616,
+        9117, 617, 618, 21155, 1789, 619, 620, 7636, 12019, 621, 622,
+        1793, 623, 625, 624, 631, 626, 627, 21578, 21103, 628, 21579,
+        629, 9122, 9123, 12189, 9289, 3168, 3169, 630, 632, 634, 21580,
+        9121, 635, 636, 637, 21581, 12781, 1801, 638, 639, 1559, 24343,
+        9419, 9420, 795, 796, 1611, 86259, 1612, 21551, 21552, 3741,
+        1617, 3742, 1615, 1619, 1620, 6301, 3744, 1622, 67685, 8521,
+        55937, 9025, 27663, 8881, 13581, 86260, 11592, 44720, 86261,
+        63231, 50873, 42925, 52332, 86262, 72706, 17705, 17707, 17708,
+        3401, 40217, 1248, 40218, 86263, 7098, 86264, 86265, 1264,
+        86266, 1266, 1267, 1268, 1269, 86267, 1271, 1272, 1273, 1274,
+        2556, 1275, 1276, 1277, 1278, 1279, 1280, 1282, 1283, 22680,
+        11889, 86268, 45662, 7038, 86269, 19315, 45663, 45664, 86270,
+        5855, 34002, 49245, 10447, 5663, 86271, 15429, 53877, 49249,
+        86272, 86273, 86274, 60128, 60453, 60129, 5552, 31923, 43407,
+        4287, 17980, 64977, 86275, 86276, 8234, 86277, 3649, 8240,
+        1330, 11999, 1332, 27618, 1334, 1335, 340, 3651, 25640, 18165,
+        1343, 4618, 1474, 3653, 75921, 1349, 53519, 1779, 45454, 22778,
+        40153, 67677, 63826, 45455, 15128, 67678, 67679, 1792, 67680,
+        3171, 47816, 45457, 9288, 59891, 67681, 25703, 35731, 35732,
+        369, 35713, 35714, 35715, 34652, 35716, 31681, 35717, 12779,
+        35718, 35719, 11992, 806, 807, 808, 43499, 43500, 810, 776,
+        812, 813, 814, 241, 43501, 43502, 816, 755, 43503, 818, 819,
+        820, 43504, 821, 822, 823, 824, 825, 826, 43505, 43506, 43507,
+        828, 829, 20083, 43508, 43509, 832, 833, 834, 835, 86278,
+        19984, 19985, 86279, 24125, 19986, 86280, 19988, 86281, 5414,
+        86282, 85808, 5479, 5420, 5421, 5422, 5423, 63800, 86283,
+        86284, 30965, 86285, 416, 1510, 5740, 5741, 81991, 86286,
+        28938, 50149, 1003, 55512, 14306, 6960, 688, 86287, 14307,
+        5399, 5400, 17783, 24118, 720, 86288, 44913, 24557, 667, 24876,
+        6529, 24877, 24878, 24879, 24880, 31847, 20671, 4011, 171, 580,
+        86289, 3863, 914, 2202, 916, 917, 918, 919, 921, 922, 923,
+        7585, 925, 7586, 926, 927, 928, 7588, 929, 930, 931, 932, 933,
+        934, 1875, 1876, 7589, 7590, 1878, 1879, 7591, 7592, 1882,
+        1883, 1884, 2212, 7593, 1887, 1888, 1889, 1890, 1891, 1892,
+        1893, 1894, 1895, 1896, 1897, 1898, 2217, 1900, 7594, 1902,
+        2219, 7595, 1905, 1906, 1907, 3323, 7596, 1911, 1912, 7597,
+        1914, 1915, 1916, 2226, 1919, 7598, 2227, 1920, 1921, 7599,
+        7600, 4708, 1923, 355, 356, 1549, 358, 32077, 360, 32078,
+        21117, 362, 19043, 71677, 5716, 86290, 49790, 86291, 86292,
+        86293, 49792, 86294, 86295, 49794, 86296, 86297, 86298, 86299,
+        11882, 86300, 49798, 86301, 49800, 49801, 49802, 49803, 453,
+        49804, 8591, 6794, 49806, 18989, 49807, 49808, 16308, 49809,
+        86302, 86303, 10105, 86304, 5285, 10106, 10107, 6557, 86305,
+        23571, 10109, 38883, 10110, 5401, 86306, 67557, 16430, 67558,
+        40171, 16433, 25878, 86307, 21762, 23, 86308, 86309, 21766,
+        86310, 86311, 5149, 3926, 21768, 21769, 47826, 942, 46985,
+        6588, 58867, 6589, 6590, 86312, 6592, 6006, 53855, 9565, 359,
+        86313, 2845, 876, 879, 27556, 27557, 885, 27558, 888, 2847,
+        27559, 2115, 2116, 2117, 53962, 57839, 315, 316, 317, 318, 319,
+        86314, 321, 322, 2122, 323, 2123, 324, 325, 328, 326, 327,
+        40542, 329, 330, 18079, 18080, 331, 1790, 7382, 332, 7380,
+        7236, 23413, 23414, 18924, 18925, 333, 335, 336, 39750, 337,
+        86315, 339, 341, 342, 343, 16264, 16265, 6615, 86316, 86317,
+        86318, 86319, 16269, 10538, 33226, 86320, 16272, 5824, 16273,
+        16274, 16276, 16277, 16278, 16279, 16280, 14517, 1547, 6463,
+        3394, 49677, 659, 10380, 30013, 10382, 10378, 10379, 10383,
+        10384, 10385, 86321, 4139, 13370, 13371, 86322, 86323, 11878,
+        64509, 15141, 15142, 15143, 32737, 14183, 15144, 39101, 42768,
+        5645, 32738, 801, 803, 804, 86324, 14707, 86325, 6601, 12402,
+        712, 12403, 2936, 1447, 15477, 1410, 44872, 1550, 8614, 15478,
+        15479, 15480, 15481, 4811, 3752, 1442, 15482, 8818, 1445, 5006,
+        16304, 32277, 16305, 16306, 86326, 16307, 53691, 69305, 809,
+        86327, 815, 26724, 69307, 43484, 63904, 86328, 13498, 827,
+        86329, 831, 2857, 836, 86330, 86331, 837, 838, 839, 840, 228,
+        229, 43722, 230, 231, 43723, 234, 235, 236, 237, 238, 239,
+        2745, 2746, 240, 242, 243, 244, 43724, 19788, 246, 247, 21134,
+        248, 250, 251, 252, 253, 254, 255, 256, 257, 258, 43725, 43726,
+        41, 43727, 262, 43728, 2751, 264, 265, 266, 267, 268, 269, 270,
+        271, 272, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032,
+        1033, 1034, 43729, 1035, 43730, 1037, 21821, 2926, 14388,
+        10432, 14389, 14390, 14391, 14392, 86332, 14394, 14395, 2035,
+        2169, 86333, 14397, 14398, 14399, 14400, 52, 14401, 14402,
+        7077, 21822, 14405, 14406, 14396, 86334, 17356, 17357, 84679,
+        84680, 76383, 17360, 17361, 86335, 38801, 2060, 30850, 12963,
+        1684, 1687, 2061, 14978, 1694, 43387, 1697, 1699, 2067, 1701,
+        1702, 1706, 43388, 43389, 76325, 1716, 1718, 26832, 1719, 1723,
+        2081, 2063, 1728, 39059, 76326, 1731, 86336, 1736, 76327, 1738,
+        19657, 6579, 6581, 6582, 6583, 6584, 6585, 29979, 1818, 28239,
+        68, 69, 3391, 86337, 10266, 63528, 86338, 10269, 10270, 10271,
+        10272, 86339, 86340, 63530, 63531, 63532, 63533, 10273, 63534,
+        86341, 10681, 10682, 86342, 9673, 86343, 10683, 460, 461, 462,
+        467, 4464, 4466, 3729, 471, 472, 468, 81634, 474, 81635, 475,
+        476, 477, 479, 480, 81636, 81637, 482, 17442, 81638, 81639,
+        484, 485, 486, 4473, 488, 489, 490, 493, 466, 494, 495, 496,
+        497, 499, 500, 501, 502, 34376, 86344, 63836, 56281, 1707,
+        20416, 61452, 56282, 1755, 56283, 56284, 18508, 53650, 63444,
+        86345, 3579, 63445, 3677, 1979, 1980, 1981, 3132, 3147, 34090,
+        1987, 12770, 1329, 80818, 80819, 1988, 23522, 1986, 15880,
+        1985, 32975, 1992, 1993, 7165, 3141, 3143, 86346, 1982, 1984,
+        3145, 86347, 78064, 55453, 2656, 2657, 35634, 35635, 2167,
+        43479 };
+
+    data10k = new int[] { 2, 4, 149900, 11, 70236, 149901, 149902, 6721,
+        149929, 29212, 34600, 149930, 149931, 149932, 141696, 149908,
+        149909, 149910 };
+
+    data501871 = new int[] { 1368366, 1368367, 1817408, 11, 2513, 1817409,
+        1817410, 1817411, 1382349, 126700, 1817412, 5539, 21862, 21863,
+        21864, 1233, 1127, 121, 15254, 15255, 357, 449, 15256, 8817,
+        15257, 15258, 1406, 1096, 281, 4826, 4827, 223, 166, 2372, 168,
+        169, 2219, 170, 171, 1176, 172, 173, 2222, 3035, 177, 178, 179,
+        180, 181, 183, 3036, 2378, 1157, 1158, 2380, 1160, 1161, 1162,
+        2384, 1164, 1165, 1166, 1167, 1168, 2385, 3037, 1171, 1172,
+        1173, 2238, 1175, 1177, 1178, 1179, 1180, 1181, 2243, 3038,
+        1182, 2244, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190,
+        59766, 471, 7349, 3599, 2847, 59767, 59768, 59769, 59770,
+        59771, 59772, 59773, 59774, 59775, 2625, 852, 853, 2632, 854,
+        855, 856, 2284, 857, 862, 1031, 859, 860, 861, 866, 1033, 867,
+        1035, 868, 870, 2294, 871, 2295, 873, 874, 875, 876, 877, 878,
+        879, 66632, 66633, 66634, 66635, 14823, 66636, 66637, 3763,
+        77345, 1370, 3764, 3765, 3766, 5666, 3768, 3770, 16892, 3771,
+        3772, 3773, 3244, 3246, 3247, 1504, 266, 29250, 24764, 29251,
+        689, 12844, 8068, 29252, 38918, 750, 751, 770, 3704, 753, 754,
+        765, 755, 3708, 757, 758, 759, 760, 3710, 761, 762, 763, 3712,
+        766, 767, 768, 769, 771, 3719, 4380, 3722, 3723, 3725, 4381,
+        3727, 3728, 3731, 3732, 764, 4382, 2316, 334, 1637, 4383, 4384,
+        4385, 4386, 4387, 184, 185, 1134, 186, 1135, 187, 188, 1138,
+        197, 191, 3517, 193, 194, 195, 196, 208, 3519, 198, 9210, 937,
+        9211, 9212, 916, 917, 117, 118, 919, 122, 921, 123, 124, 125,
+        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 924, 137,
+        138, 139, 140, 141, 588, 928, 142, 143, 144, 929, 146, 147,
+        148, 149, 150, 151, 3775, 3776, 3777, 3778, 3780, 3781, 3783,
+        3784, 3785, 3796, 4169, 3788, 4170, 3790, 3791, 3793, 3803,
+        3794, 3797, 4171, 3799, 3800, 3801, 3802, 3804, 4172, 3806,
+        4173, 4174, 3811, 4175, 3813, 3829, 3815, 3816, 3817, 4176,
+        4177, 3820, 3821, 3822, 2168, 3039, 2460, 2170, 2459, 2174,
+        2175, 2176, 2461, 2462, 2463, 3040, 2466, 2467, 2469, 2468,
+        2470, 3041, 2472, 3042, 3043, 3044, 3045, 231, 881, 882, 1219,
+        884, 2038, 886, 887, 888, 891, 892, 1221, 894, 895, 1222, 2039,
+        899, 1225, 900, 901, 902, 2492, 2494, 2495, 2496, 4052, 2498,
+        2502, 2500, 2501, 2503, 2504, 4653, 5514, 18671, 10350, 1122,
+        44317, 44318, 44319, 44320, 44321, 44322, 44323, 44324, 7923,
+        1422, 10284, 10285, 6146, 9803, 10286, 466, 5998, 696, 3257,
+        6043, 6195, 6196, 6197, 6198, 6199, 6200, 6201, 7029, 4405,
+        4864, 450, 349, 11214, 3548, 1092, 5728, 7395, 6533, 1123,
+        5736, 1115, 6535, 6536, 2739, 2832, 2833, 2834, 2835, 2836,
+        23972, 2837, 23973, 2839, 2840, 2691, 1339, 20116, 3219, 8210,
+        3170, 3171, 3172, 3173, 2094, 2095, 2096, 2097, 2099, 2100,
+        2102, 3174, 2104, 1372, 2105, 2107, 2108, 2109, 2110, 2113,
+        2114, 2115, 2117, 2118, 3221, 3222, 2122, 2123, 2124, 4611,
+        2125, 2126, 2127, 2128, 2129, 2130, 2131, 575, 576, 2132, 4612,
+        2134, 2135, 2136, 4368, 5931, 5932, 5933, 5934, 5935, 5936,
+        5937, 5938, 5939, 2902, 4057, 4058, 4059, 4060, 4062, 4063,
+        4064, 4654, 4655, 4067, 4068, 4069, 4656, 4657, 4073, 4658,
+        4074, 4075, 4659, 4660, 4661, 4076, 4662, 4663, 4664, 4078,
+        4079, 4080, 4665, 4082, 4083, 4084, 4666, 4086, 4087, 4088,
+        544, 545, 546, 547, 548, 549, 550, 559, 1227, 552, 553, 5035,
+        555, 554, 1228, 556, 1229, 557, 558, 560, 561, 562, 563, 564,
+        565, 1230, 566, 567, 568, 569, 570, 572, 573, 222, 7461, 2059,
+        2060, 2061, 5664, 2062, 7463, 16997, 2065, 2066, 2067, 2068,
+        2069, 2070, 2072, 2073, 2074, 2075, 2076, 2077, 2078, 7464,
+        2079, 2080, 2081, 7465, 2082, 2083, 2084, 2085, 2086, 2087,
+        199, 206, 200, 203, 205, 211, 1140, 3699, 209, 214, 215, 216,
+        217, 218, 777, 778, 779, 780, 2298, 781, 782, 783, 784, 785,
+        787, 788, 384, 789, 790, 791, 2677, 793, 794, 795, 796, 797,
+        2307, 798, 799, 801, 802, 3645, 803, 4337, 805, 3648, 3649,
+        807, 808, 3651, 810, 812, 813, 814, 815, 816, 3654, 818, 819,
+        13780, 930, 932, 4221, 935, 936, 938, 2197, 939, 940, 941,
+        2200, 943, 1591, 1952, 2630, 1592, 2631, 1602, 1607, 1595,
+        1596, 1597, 1598, 1599, 1955, 1601, 1603, 1956, 1605, 1606,
+        1608, 1610, 1638, 20608, 968, 969, 970, 971, 972, 973, 974,
+        975, 2729, 2730, 977, 2731, 979, 980, 981, 982, 983, 984, 3506,
+        987, 989, 990, 991, 2732, 2733, 6051, 6053, 6055, 910, 6056,
+        4339, 4340, 577, 4341, 579, 580, 581, 616, 584, 585, 586, 4342,
+        4343, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 5046,
+        599, 600, 5047, 601, 602, 603, 604, 605, 5053, 608, 609, 610,
+        5055, 612, 613, 5056, 615, 617, 618, 619, 620, 621, 622, 623,
+        624, 6882, 627, 628, 629, 630, 631, 5330, 633, 634, 635, 636,
+        637, 639, 640, 7870, 632, 34480, 13118, 903, 904, 905, 907,
+        2616, 2617, 2618, 2619, 2620, 2621, 2622, 2623, 2624, 2643,
+        1685, 1686, 1687, 1688, 1690, 1691, 2644, 2645, 1695, 2646,
+        1699, 2647, 2648, 1702, 2649, 2650, 1706, 22082, 5516, 4307,
+        2203, 1995, 1996, 1998, 1999, 2206, 2002, 2003, 4407, 2005,
+        4408, 2007, 2008, 2009, 2010, 2011, 4409, 2013, 2014, 2015,
+        2017, 3227, 3149, 6025, 22913, 22914, 3228, 7925, 10123, 10124,
+        10125, 10127, 16978, 14094, 1593, 4869, 4870, 3477, 3844, 3845,
+        9923, 3846, 3847, 39767, 39768, 39769, 3541, 39770, 39771,
+        14179, 39772, 39773, 39774, 42558, 1043, 4203, 42559, 42560,
+        42561, 42562, 42563, 42564, 11018, 42565, 42566, 4589, 4590,
+        4591, 4312, 18283, 4317, 4318, 4319, 12659, 11706, 11707,
+        53395, 53396, 29410, 8040, 8041, 915, 20105, 22952, 22953,
+        20596, 4161, 3047, 3048, 3049, 3050, 3051, 3052, 3053, 3054,
+        3055, 1474, 3056, 3057, 3058, 3059, 3060, 3061, 2549, 2551,
+        3062, 3063, 3064, 3065, 3066, 3067, 3068, 3069, 515, 3070,
+        3071, 3072, 3073, 3074, 3075, 3076, 3077, 3078, 3079, 3080,
+        3081, 3082, 506, 3083, 3084, 3085, 3086, 3087, 3088, 3089,
+        3090, 3091, 527, 528, 2995, 530, 531, 533, 534, 535, 537, 538 };
+  }
+
+}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/encoding/EncodingTest.java b/lucene/facet/src/test/org/apache/lucene/facet/encoding/EncodingTest.java
new file mode 100644
index 0000000..77521f8
--- /dev/null
+++ b/lucene/facet/src/test/org/apache/lucene/facet/encoding/EncodingTest.java
@@ -0,0 +1,170 @@
+package org.apache.lucene.facet.encoding;
+
+import java.io.IOException;
+import java.util.Arrays;
+
+import org.apache.lucene.facet.FacetTestCase;
+import org.apache.lucene.facet.encoding.DGapIntEncoder;
+import org.apache.lucene.facet.encoding.DGapVInt8IntEncoder;
+import org.apache.lucene.facet.encoding.EightFlagsIntEncoder;
+import org.apache.lucene.facet.encoding.FourFlagsIntEncoder;
+import org.apache.lucene.facet.encoding.IntDecoder;
+import org.apache.lucene.facet.encoding.IntEncoder;
+import org.apache.lucene.facet.encoding.NOnesIntEncoder;
+import org.apache.lucene.facet.encoding.SimpleIntEncoder;
+import org.apache.lucene.facet.encoding.SortingIntEncoder;
+import org.apache.lucene.facet.encoding.UniqueValuesIntEncoder;
+import org.apache.lucene.facet.encoding.VInt8IntEncoder;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IntsRef;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+public class EncodingTest extends FacetTestCase {
+
+  private static IntsRef uniqueSortedData, data;
+  
+  @BeforeClass
+  public static void beforeClassEncodingTest() throws Exception {
+    int capacity = atLeast(10000);
+    data = new IntsRef(capacity);
+    for (int i = 0; i < 10; i++) {
+      data.ints[i] = i + 1; // small values
+    }
+    for (int i = 10; i < data.ints.length; i++) {
+      data.ints[i] = random().nextInt(Integer.MAX_VALUE - 1) + 1; // some encoders don't allow 0
+    }
+    data.length = data.ints.length;
+    
+    uniqueSortedData = IntsRef.deepCopyOf(data);
+    Arrays.sort(uniqueSortedData.ints);
+    uniqueSortedData.length = 0;
+    int prev = -1;
+    for (int i = 0; i < uniqueSortedData.ints.length; i++) {
+      if (uniqueSortedData.ints[i] != prev) {
+        uniqueSortedData.ints[uniqueSortedData.length++] = uniqueSortedData.ints[i];
+        prev = uniqueSortedData.ints[i];
+      }
+    }
+  }
+  
+  private static void encoderTest(IntEncoder encoder, IntsRef data, IntsRef expected) throws IOException {
+    // ensure toString is implemented
+    String toString = encoder.toString();
+    assertFalse(toString.startsWith(encoder.getClass().getName() + "@"));
+    IntDecoder decoder = encoder.createMatchingDecoder();
+    toString = decoder.toString();
+    assertFalse(toString.startsWith(decoder.getClass().getName() + "@"));
+    
+    BytesRef bytes = new BytesRef(100); // some initial capacity - encoders should grow the byte[]
+    IntsRef values = new IntsRef(100); // some initial capacity - decoders should grow the int[]
+    for (int i = 0; i < 2; i++) {
+      // run 2 iterations to catch encoders/decoders which don't reset properly
+      encoding(encoder, data, bytes);
+      decoding(bytes, values, encoder.createMatchingDecoder());
+      assertTrue(expected.intsEquals(values));
+    }
+  }
+
+  private static void encoding(IntEncoder encoder, IntsRef data, BytesRef bytes) throws IOException {
+    final IntsRef values;
+    if (random().nextBoolean()) { // randomly set the offset
+      values = new IntsRef(data.length + 1);
+      System.arraycopy(data.ints, 0, values.ints, 1, data.length);
+      values.offset = 1; // ints start at index 1
+      values.length = data.length;
+    } else {
+      // need to copy the array because it may be modified by encoders (e.g. sorting)
+      values = IntsRef.deepCopyOf(data);
+    }
+    encoder.encode(values, bytes);
+  }
+
+  private static void decoding(BytesRef bytes, IntsRef values, IntDecoder decoder) throws IOException {
+    int offset = 0;
+    if (random().nextBoolean()) { // randomly set the offset and length to other than 0,0
+      bytes.grow(bytes.length + 1); // ensure that we have enough capacity to shift values by 1
+      bytes.offset = 1; // bytes start at index 1 (must do that after grow)
+      System.arraycopy(bytes.bytes, 0, bytes.bytes, 1, bytes.length);
+      offset = 1;
+    }
+    decoder.decode(bytes, values);
+    assertEquals(offset, bytes.offset); // decoders should not mess with offsets
+  }
+
+  @Test
+  public void testVInt8() throws Exception {
+    encoderTest(new VInt8IntEncoder(), data, data);
+    
+    // cover negative numbers;
+    BytesRef bytes = new BytesRef(5);
+    IntEncoder enc = new VInt8IntEncoder();
+    IntsRef values = new IntsRef(1);
+    values.ints[values.length++] = -1;
+    enc.encode(values, bytes);
+    
+    IntDecoder dec = enc.createMatchingDecoder();
+    values.length = 0;
+    dec.decode(bytes, values);
+    assertEquals(1, values.length);
+    assertEquals(-1, values.ints[0]);
+  }
+  
+  @Test
+  public void testSimpleInt() throws Exception {
+    encoderTest(new SimpleIntEncoder(), data, data);
+  }
+  
+  @Test
+  public void testSortingUniqueValues() throws Exception {
+    encoderTest(new SortingIntEncoder(new UniqueValuesIntEncoder(new VInt8IntEncoder())), data, uniqueSortedData);
+  }
+
+  @Test
+  public void testSortingUniqueDGap() throws Exception {
+    encoderTest(new SortingIntEncoder(new UniqueValuesIntEncoder(new DGapIntEncoder(new VInt8IntEncoder()))), data, uniqueSortedData);
+  }
+
+  @Test
+  public void testSortingUniqueDGapEightFlags() throws Exception {
+    encoderTest(new SortingIntEncoder(new UniqueValuesIntEncoder(new DGapIntEncoder(new EightFlagsIntEncoder()))), data, uniqueSortedData);
+  }
+
+  @Test
+  public void testSortingUniqueDGapFourFlags() throws Exception {
+    encoderTest(new SortingIntEncoder(new UniqueValuesIntEncoder(new DGapIntEncoder(new FourFlagsIntEncoder()))), data, uniqueSortedData);
+  }
+
+  @Test
+  public void testSortingUniqueDGapNOnes4() throws Exception {
+    encoderTest(new SortingIntEncoder(new UniqueValuesIntEncoder(new DGapIntEncoder(new NOnesIntEncoder(4)))), data, uniqueSortedData);
+  }
+  
+  @Test
+  public void testSortingUniqueDGapNOnes3() throws Exception {
+    encoderTest(new SortingIntEncoder(new UniqueValuesIntEncoder(new DGapIntEncoder(new NOnesIntEncoder(3)))), data, uniqueSortedData);
+  }
+  
+  @Test
+  public void testSortingUniqueDGapVInt() throws Exception {
+    encoderTest(new SortingIntEncoder(new UniqueValuesIntEncoder(new DGapVInt8IntEncoder())), data, uniqueSortedData);
+  }
+
+}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/index/OrdinalMappingReaderTest.java b/lucene/facet/src/test/org/apache/lucene/facet/index/OrdinalMappingReaderTest.java
deleted file mode 100644
index b224d69..0000000
--- a/lucene/facet/src/test/org/apache/lucene/facet/index/OrdinalMappingReaderTest.java
+++ /dev/null
@@ -1,122 +0,0 @@
-package org.apache.lucene.facet.index;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.analysis.MockTokenizer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.index.params.FacetIndexingParams;
-import org.apache.lucene.facet.search.FacetsCollector;
-import org.apache.lucene.facet.search.params.CountFacetRequest;
-import org.apache.lucene.facet.search.params.FacetSearchParams;
-import org.apache.lucene.facet.search.results.FacetResult;
-import org.apache.lucene.facet.search.results.FacetResultNode;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter.MemoryOrdinalMap;
-import org.apache.lucene.facet.util.TaxonomyMergeUtils;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.IOUtils;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class OrdinalMappingReaderTest extends FacetTestCase {
-  
-  private static final int NUM_DOCS = 100;
-  
-  @Test
-  public void testTaxonomyMergeUtils() throws Exception {
-    Directory dir = newDirectory();
-    Directory taxDir = newDirectory();
-    FacetIndexingParams fip = new FacetIndexingParams(randomCategoryListParams());
-    buildIndexWithFacets(dir, taxDir, true, fip);
-    
-    Directory dir1 = newDirectory();
-    Directory taxDir1 = newDirectory();
-    buildIndexWithFacets(dir1, taxDir1, false, fip);
-    
-    IndexWriter destIndexWriter = new IndexWriter(dir1, new IndexWriterConfig(TEST_VERSION_CURRENT, null));
-    DirectoryTaxonomyWriter destTaxWriter = new DirectoryTaxonomyWriter(taxDir1);
-    try {
-      TaxonomyMergeUtils.merge(dir, taxDir, new MemoryOrdinalMap(), destIndexWriter, destTaxWriter, fip);
-    } finally {
-      IOUtils.close(destIndexWriter, destTaxWriter);
-    }
-    
-    verifyResults(dir1, taxDir1, fip);
-    dir1.close();
-    taxDir1.close();
-    dir.close();
-    taxDir.close();
-  }
-
-  private void verifyResults(Directory dir, Directory taxDir, FacetIndexingParams fip) throws IOException {
-    DirectoryReader reader1 = DirectoryReader.open(dir);
-    DirectoryTaxonomyReader taxReader = new DirectoryTaxonomyReader(taxDir);
-    IndexSearcher searcher = newSearcher(reader1);
-    FacetSearchParams fsp = new FacetSearchParams(fip, new CountFacetRequest(new CategoryPath("tag"), NUM_DOCS));
-    FacetsCollector collector = FacetsCollector.create(fsp, reader1, taxReader);
-    searcher.search(new MatchAllDocsQuery(), collector);
-    FacetResult result = collector.getFacetResults().get(0);
-    FacetResultNode node = result.getFacetResultNode();
-    for (FacetResultNode facet: node.subResults) {
-      int weight = (int)facet.value;
-      int label = Integer.parseInt(facet.label.components[1]);
-      //System.out.println(label + ": " + weight);
-      if (VERBOSE) {
-        System.out.println(label + ": " + weight);
-      }
-      assertEquals(NUM_DOCS ,weight);
-    }
-    reader1.close();
-    taxReader.close();
-  }
-
-  private void buildIndexWithFacets(Directory dir, Directory taxDir, boolean asc, FacetIndexingParams fip) throws IOException {
-    IndexWriterConfig config = newIndexWriterConfig(TEST_VERSION_CURRENT, 
-        new MockAnalyzer(random(), MockTokenizer.WHITESPACE, false));
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, config);
-    
-    DirectoryTaxonomyWriter taxonomyWriter = new DirectoryTaxonomyWriter(taxDir);
-    for (int i = 1; i <= NUM_DOCS; i++) {
-      Document doc = new Document();
-      List<CategoryPath> categoryPaths = new ArrayList<CategoryPath>(i + 1);
-      for (int j = i; j <= NUM_DOCS; j++) {
-        int facetValue = asc? j: NUM_DOCS - j;
-        categoryPaths.add(new CategoryPath("tag", Integer.toString(facetValue)));
-      }
-      FacetFields facetFields = new FacetFields(taxonomyWriter, fip);
-      facetFields.addFields(doc, categoryPaths);
-      writer.addDocument(doc);
-    }    
-    taxonomyWriter.close();
-    writer.close();
-  }  
-
-}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/index/TestFacetsPayloadMigrationReader.java b/lucene/facet/src/test/org/apache/lucene/facet/index/TestFacetsPayloadMigrationReader.java
deleted file mode 100644
index 2fd14d1..0000000
--- a/lucene/facet/src/test/org/apache/lucene/facet/index/TestFacetsPayloadMigrationReader.java
+++ /dev/null
@@ -1,413 +0,0 @@
-package org.apache.lucene.facet.index;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map;
-import java.util.Map.Entry;
-import java.util.Random;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.Field.Store;
-import org.apache.lucene.document.FieldType;
-import org.apache.lucene.document.StringField;
-import org.apache.lucene.document.TextField;
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.index.params.CategoryListParams;
-import org.apache.lucene.facet.index.params.FacetIndexingParams;
-import org.apache.lucene.facet.index.params.PerDimensionIndexingParams;
-import org.apache.lucene.facet.index.params.CategoryListParams.OrdinalPolicy;
-import org.apache.lucene.facet.index.params.PerDimensionOrdinalPolicy;
-import org.apache.lucene.facet.search.CategoryListIterator;
-import org.apache.lucene.facet.search.DrillDown;
-import org.apache.lucene.facet.search.FacetsCollector;
-import org.apache.lucene.facet.search.params.CountFacetRequest;
-import org.apache.lucene.facet.search.params.FacetRequest;
-import org.apache.lucene.facet.search.params.FacetSearchParams;
-import org.apache.lucene.facet.search.results.FacetResult;
-import org.apache.lucene.facet.search.results.FacetResultNode;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
-import org.apache.lucene.facet.util.PartitionsUtils;
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.MultiReader;
-import org.apache.lucene.index.NoMergePolicy;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.index.Terms;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.MultiCollector;
-import org.apache.lucene.search.PrefixQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TotalHitCountCollector;
-import org.apache.lucene.search.BooleanClause.Occur;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.IntsRef;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/** Tests facets index migration from payload to DocValues.*/
-public class TestFacetsPayloadMigrationReader extends FacetTestCase {
-  
-  private static class PayloadFacetFields extends FacetFields {
-
-    private static final class CountingListStream extends TokenStream {
-      private final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);
-      private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
-      private final Iterator<Entry<String,BytesRef>> categoriesData;
-      
-      CountingListStream(Map<String,BytesRef> categoriesData) {
-        this.categoriesData = categoriesData.entrySet().iterator();
-      }
-      
-      @Override
-      public boolean incrementToken() throws IOException {
-        if (!categoriesData.hasNext()) {
-          return false;
-        }
-        
-        Entry<String,BytesRef> entry = categoriesData.next();
-        termAtt.setEmpty().append(FacetsPayloadMigrationReader.PAYLOAD_TERM_TEXT + entry.getKey());
-        payloadAtt.setPayload(entry.getValue());
-        return true;
-      }
-      
-    }
-
-    private static final FieldType COUNTING_LIST_PAYLOAD_TYPE = new FieldType();
-    static {
-      COUNTING_LIST_PAYLOAD_TYPE.setIndexed(true);
-      COUNTING_LIST_PAYLOAD_TYPE.setTokenized(true);
-      COUNTING_LIST_PAYLOAD_TYPE.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);
-      COUNTING_LIST_PAYLOAD_TYPE.setStored(false);
-      COUNTING_LIST_PAYLOAD_TYPE.setOmitNorms(true);
-      COUNTING_LIST_PAYLOAD_TYPE.freeze();
-    }
-    
-    public PayloadFacetFields(TaxonomyWriter taxonomyWriter, FacetIndexingParams params) {
-      super(taxonomyWriter, params);
-    }
-
-    @Override
-    protected FieldType drillDownFieldType() {
-      // Since the payload is indexed in the same field as the drill-down terms,
-      // we must set IndexOptions to DOCS_AND_FREQS_AND_POSITIONS
-      final FieldType type = new FieldType(TextField.TYPE_NOT_STORED);
-      type.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);
-      type.freeze();
-      return type;
-    }
-
-    @Override
-    protected void addCountingListData(Document doc, Map<String,BytesRef> categoriesData, String field) {
-      CountingListStream ts = new CountingListStream(categoriesData);
-      doc.add(new Field(field, ts, COUNTING_LIST_PAYLOAD_TYPE));
-    }
-  }
-
-  private static final String[] DIMENSIONS = new String[] { "dim1", "dim2", "dim3.1", "dim3.2" };
-  
-  private HashMap<String,Integer> createIndex(Directory indexDir, Directory taxoDir, FacetIndexingParams fip) 
-      throws Exception {
-    Random random = random();
-    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));
-    conf.setMaxBufferedDocs(2); // force few segments
-    conf.setMergePolicy(NoMergePolicy.COMPOUND_FILES); // avoid merges so that we're left with few segments
-    IndexWriter indexWriter = new IndexWriter(indexDir, conf);
-    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
-    
-    FacetFields facetFields = new PayloadFacetFields(taxoWriter, fip);
-    
-    HashMap<String,Integer> expectedCounts = new HashMap<String,Integer>(DIMENSIONS.length);
-    int numDocs = atLeast(10);
-    for (int i = 0; i < numDocs; i++) {
-      Document doc = new Document();
-      int numCategories = random.nextInt(3) + 1;
-      ArrayList<CategoryPath> categories = new ArrayList<CategoryPath>(numCategories);
-      HashSet<String> docDimensions = new HashSet<String>();
-      while (numCategories-- > 0) {
-        String dim = DIMENSIONS[random.nextInt(DIMENSIONS.length)];
-        // we should only increment the expected count by 1 per document
-        docDimensions.add(dim);
-        categories.add(new CategoryPath(dim, Integer.toString(i), Integer.toString(numCategories)));
-      }
-      facetFields.addFields(doc, categories);
-      doc.add(new StringField("docid", Integer.toString(i), Store.YES));
-      doc.add(new TextField("foo", "content" + i, Store.YES));
-      indexWriter.addDocument(doc);
-
-      // update expected count per dimension
-      for (String dim : docDimensions) {
-        Integer val = expectedCounts.get(dim);
-        if (val == null) {
-          expectedCounts.put(dim, Integer.valueOf(1));
-        } else {
-          expectedCounts.put(dim, Integer.valueOf(val.intValue() + 1));
-        }
-      }
-      
-      if (random.nextDouble() < 0.2) { // add some documents that will be deleted
-        doc = new Document();
-        doc.add(new StringField("del", "key", Store.NO));
-        facetFields.addFields(doc, Collections.singletonList(new CategoryPath("dummy")));
-        indexWriter.addDocument(doc);
-      }
-    }
-    
-    indexWriter.commit();
-    taxoWriter.commit();
-
-    // delete the docs that were marked for deletion. note that the 'dummy'
-    // category is not removed from the taxonomy, so must account for it when we
-    // verify the migrated index.
-    indexWriter.deleteDocuments(new Term("del", "key"));
-    indexWriter.commit();
-    
-    IOUtils.close(indexWriter, taxoWriter);
-    
-    return expectedCounts;
-  }
-  
-  private void migrateIndex(Directory indexDir, FacetIndexingParams fip) throws Exception {
-    final Map<String,Term> fieldTerms = FacetsPayloadMigrationReader.buildFieldTermsMap(indexDir, fip);
-    DirectoryReader reader = DirectoryReader.open(indexDir);
-    List<AtomicReaderContext> leaves = reader.leaves();
-    int numReaders = leaves.size();
-    AtomicReader wrappedLeaves[] = new AtomicReader[numReaders];
-    for (int i = 0; i < numReaders; i++) {
-      wrappedLeaves[i] = new FacetsPayloadMigrationReader(leaves.get(i).reader(), fieldTerms);
-    }
-    
-    IndexWriter writer = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, null));
-    writer.deleteAll();
-    try {
-      writer.addIndexes(new MultiReader(wrappedLeaves));
-      writer.commit();
-    } finally {
-      reader.close();
-      writer.close();
-    }
-  }
-  
-  private void verifyMigratedIndex(Directory indexDir, Directory taxoDir, HashMap<String,Integer> expectedCounts, 
-      FacetIndexingParams fip) throws Exception {
-    DirectoryReader indexReader = DirectoryReader.open(indexDir);
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoDir);
-    IndexSearcher searcher = new IndexSearcher(indexReader);
-
-    assertFalse("index should not have deletions", indexReader.hasDeletions());
-    
-    verifyNotFacetsData(indexReader, searcher);
-    verifyFacetedSearch(expectedCounts, fip, indexReader, taxoReader, searcher);
-    verifyDrillDown(expectedCounts, fip, indexReader, taxoReader, searcher);
-    verifyIndexOrdinals(indexReader, taxoReader, fip);
-    
-    IOUtils.close(indexReader, taxoReader);
-  }
-  
-  private void verifyNotFacetsData(DirectoryReader indexReader, IndexSearcher searcher) throws IOException {
-    // verify that non facets data was not damaged
-    TotalHitCountCollector total = new TotalHitCountCollector();
-    searcher.search(new PrefixQuery(new Term("foo", "content")), total);
-    assertEquals("invalid number of results for content query", total.getTotalHits(), indexReader.maxDoc());
-    
-    int numDocIDs = 0;
-    for (AtomicReaderContext context : indexReader.leaves()) {
-      Terms docIDs = context.reader().terms("docid");
-      assertNotNull(docIDs);
-      TermsEnum te = docIDs.iterator(null);
-      while (te.next() != null) {
-        ++numDocIDs;
-      }
-    }
-    assertEquals("invalid number of docid terms", indexReader.maxDoc(), numDocIDs);
-  }
-  
-  private void verifyFacetedSearch(Map<String,Integer> expectedCounts, FacetIndexingParams fip, 
-      DirectoryReader indexReader, TaxonomyReader taxoReader, IndexSearcher searcher) throws IOException {
-    // run faceted search and assert expected counts
-    ArrayList<FacetRequest> requests = new ArrayList<FacetRequest>(expectedCounts.size());
-    for (String dim : expectedCounts.keySet()) {
-      requests.add(new CountFacetRequest(new CategoryPath(dim), 5));
-    }
-    FacetSearchParams fsp = new FacetSearchParams(fip, requests);
-    FacetsCollector fc = FacetsCollector.create(fsp, indexReader, taxoReader);
-    MatchAllDocsQuery base = new MatchAllDocsQuery();
-    searcher.search(base, fc);
-    List<FacetResult> facetResults = fc.getFacetResults();
-    assertEquals(requests.size(), facetResults.size());
-    for (FacetResult res : facetResults) {
-      FacetResultNode node = res.getFacetResultNode();
-      String dim = node.label.components[0];
-      assertEquals("wrong count for " + dim, expectedCounts.get(dim).intValue(), (int) node.value);
-    }
-  }
-  
-  private void verifyDrillDown(Map<String,Integer> expectedCounts, FacetIndexingParams fip, DirectoryReader indexReader, 
-      TaxonomyReader taxoReader, IndexSearcher searcher) throws IOException {
-    // verify drill-down
-    for (String dim : expectedCounts.keySet()) {
-      CategoryPath drillDownCP = new CategoryPath(dim);
-      FacetSearchParams fsp = new FacetSearchParams(fip, new CountFacetRequest(drillDownCP, 10));
-      Query drillDown = DrillDown.query(fsp, new MatchAllDocsQuery(), Occur.MUST, drillDownCP);
-      TotalHitCountCollector total = new TotalHitCountCollector();
-      FacetsCollector fc = FacetsCollector.create(fsp, indexReader, taxoReader);
-      searcher.search(drillDown, MultiCollector.wrap(fc, total));
-      assertTrue("no results for drill-down query " + drillDown, total.getTotalHits() > 0);
-      List<FacetResult> facetResults = fc.getFacetResults();
-      assertEquals(1, facetResults.size());
-      FacetResultNode rootNode = facetResults.get(0).getFacetResultNode();
-      assertEquals("wrong count for " + dim, expectedCounts.get(dim).intValue(), (int) rootNode.value);
-    }
-  }
-  
-  private void verifyIndexOrdinals(DirectoryReader indexReader, TaxonomyReader taxoReader, FacetIndexingParams fip) 
-      throws IOException {
-    // verify that the ordinals in the index match the ones in the taxonomy, and vice versa
-    
-    // collect all fields which have DocValues, to assert later that all were
-    // visited i.e. that during migration we didn't add FieldInfos with no
-    // DocValues
-    HashSet<String> docValuesFields = new HashSet<String>();
-    for (AtomicReaderContext context : indexReader.leaves()) {
-      FieldInfos infos = context.reader().getFieldInfos();
-      for (FieldInfo info : infos) {
-        if (info.hasDocValues()) {
-          docValuesFields.add(info.name);
-        }
-      }
-    }
-    
-    // check that all visited ordinals are found in the taxonomy and vice versa
-    boolean[] foundOrdinals = new boolean[taxoReader.getSize()];
-    for (int i = 0; i < foundOrdinals.length; i++) {
-      foundOrdinals[i] = false; // init to be on the safe side
-    }
-    foundOrdinals[0] = true; // ROOT ordinals isn't indexed
-    // mark 'dummy' category ordinal as seen
-    int dummyOrdinal = taxoReader.getOrdinal(new CategoryPath("dummy"));
-    if (dummyOrdinal > 0) {
-      foundOrdinals[dummyOrdinal] = true;
-    }
-    
-    int partitionSize = fip.getPartitionSize();
-    int numPartitions = (int) Math.ceil(taxoReader.getSize() / (double) partitionSize);
-    final IntsRef ordinals = new IntsRef(32);
-    for (String dim : DIMENSIONS) {
-      CategoryListParams clp = fip.getCategoryListParams(new CategoryPath(dim));
-      int partitionOffset = 0;
-      for (int partition = 0; partition < numPartitions; partition++, partitionOffset += partitionSize) {
-        final CategoryListIterator cli = clp.createCategoryListIterator(partition);
-        for (AtomicReaderContext context : indexReader.leaves()) {
-          if (cli.setNextReader(context)) { // not all fields may exist in all segments
-            // remove that field from the list of DocValues fields
-            docValuesFields.remove(clp.field + PartitionsUtils.partitionName(partition));
-            int maxDoc = context.reader().maxDoc();
-            for (int doc = 0; doc < maxDoc; doc++) {
-              cli.getOrdinals(doc, ordinals);
-              for (int j = 0; j < ordinals.length; j++) {
-                // verify that the ordinal is recognized by the taxonomy
-                int ordinal = ordinals.ints[j] + partitionOffset;
-                assertTrue("should not have received dummy ordinal (" + dummyOrdinal + ")", dummyOrdinal != ordinal);
-                assertNotNull("missing category for ordinal " + ordinal, taxoReader.getPath(ordinal));
-                foundOrdinals[ordinal] = true;
-              }
-            }
-          }
-        }
-      }
-    }
-    
-    assertTrue("some fields which have docValues were not visited: " + docValuesFields, docValuesFields.isEmpty());
-    
-    for (int i = 0; i < foundOrdinals.length; i++) {
-      assertTrue("ordinal " + i + " not visited", foundOrdinals[i]);
-    }
-  }
-  
-  private void doTestMigration(final int partitionSize) throws Exception {
-    // create a facets index with PayloadFacetFields and check it after migration
-    Directory indexDir = newDirectory();
-    Directory taxoDir = newDirectory();
-    
-    // set custom CLP fields for two dimensions and use the default ($facets) for the other two
-    HashMap<CategoryPath,CategoryListParams> params = new HashMap<CategoryPath,CategoryListParams>();
-    params.put(new CategoryPath(DIMENSIONS[0]), new CategoryListParams(DIMENSIONS[0]) {
-      @Override
-      public OrdinalPolicy getOrdinalPolicy(String dimension) {
-        return OrdinalPolicy.ALL_PARENTS;
-      }
-    });
-    params.put(new CategoryPath(DIMENSIONS[1]), new CategoryListParams(DIMENSIONS[1]) {
-      @Override
-      public OrdinalPolicy getOrdinalPolicy(String dimension) {
-        return OrdinalPolicy.ALL_PARENTS;
-      }
-    });
-    
-    HashMap<String,OrdinalPolicy> policies = new HashMap<String,CategoryListParams.OrdinalPolicy>();
-    policies.put(DIMENSIONS[2], OrdinalPolicy.ALL_PARENTS);
-    policies.put(DIMENSIONS[3], OrdinalPolicy.ALL_PARENTS);
-    FacetIndexingParams fip = new PerDimensionIndexingParams(params, new PerDimensionOrdinalPolicy(policies)) {
-      @Override
-      public int getPartitionSize() {
-        return partitionSize;
-      }
-    };
-    
-    HashMap<String,Integer> expectedCounts = createIndex(indexDir, taxoDir, fip);
-    migrateIndex(indexDir, fip);
-    verifyMigratedIndex(indexDir, taxoDir, expectedCounts, fip);
-    
-    IOUtils.close(indexDir, taxoDir);
-  }
-  
-  @Test
-  public void testMigration() throws Exception {
-    doTestMigration(Integer.MAX_VALUE);
-  }
-  
-  @Test
-  public void testMigrationWithPartitions() throws Exception {
-    doTestMigration(2);
-  }
-  
-}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/index/params/CategoryListParamsTest.java b/lucene/facet/src/test/org/apache/lucene/facet/index/params/CategoryListParamsTest.java
deleted file mode 100644
index 49378e4..0000000
--- a/lucene/facet/src/test/org/apache/lucene/facet/index/params/CategoryListParamsTest.java
+++ /dev/null
@@ -1,90 +0,0 @@
-package org.apache.lucene.facet.index.params;
-
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.util.encoding.DGapVInt8IntEncoder;
-import org.apache.lucene.util.encoding.IntDecoder;
-import org.apache.lucene.util.encoding.IntEncoder;
-import org.apache.lucene.util.encoding.SortingIntEncoder;
-import org.apache.lucene.util.encoding.UniqueValuesIntEncoder;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class CategoryListParamsTest extends FacetTestCase {
-
-  @Test
-  public void testDefaultSettings() {
-    CategoryListParams clp = new CategoryListParams();
-    assertEquals("wrong default field", "$facets", clp.field);
-    IntEncoder encoder = new SortingIntEncoder(new UniqueValuesIntEncoder(new DGapVInt8IntEncoder()));
-    IntDecoder decoder = encoder.createMatchingDecoder();
-    assertEquals("unexpected default encoder", encoder.toString(), clp.createEncoder().toString());
-    assertEquals("unexpected default decoder", decoder.toString(), clp.createEncoder().createMatchingDecoder().toString());
-  }
-  
-  /**
-   * Test that the {@link CategoryListParams#hashCode()} and
-   * {@link CategoryListParams#equals(Object)} are consistent.
-   */
-  @Test
-  public void testIdentity() {
-    CategoryListParams clParams1 = new CategoryListParams();
-    // Assert identity is correct - a CategoryListParams equals itself.
-    assertEquals("A CategoryListParams object does not equal itself.",
-        clParams1, clParams1);
-    // For completeness, the object's hashcode equals itself
-    assertEquals("A CategoryListParams object's hashCode does not equal itself.",
-        clParams1.hashCode(), clParams1.hashCode());
-  }
-
-  /**
-   * Test that CategoryListParams behave correctly when compared against each
-   * other.
-   */
-  @Test
-  public void testIdentityConsistency() {
-    // Test 2 CategoryListParams with the default parameter
-    CategoryListParams clParams1 = new CategoryListParams();
-    CategoryListParams clParams2 = new CategoryListParams();
-    assertEquals(
-        "2 CategoryListParams with the same default term should equal each other.",
-        clParams1, clParams2);
-    assertEquals("2 CategoryListParams with the same default term should have the same hashcode",
-        clParams1.hashCode(), clParams2.hashCode());
-
-    // Test 2 CategoryListParams with the same specified Term
-    clParams1 = new CategoryListParams("test");
-    clParams2 = new CategoryListParams("test");
-    assertEquals(
-        "2 CategoryListParams with the same term should equal each other.",
-        clParams1, clParams2);
-    assertEquals("2 CategoryListParams with the same term should have the same hashcode",
-        clParams1.hashCode(), clParams2.hashCode());
-    
-    // Test 2 CategoryListParams with DIFFERENT terms
-    clParams1 = new CategoryListParams("test1");
-    clParams2 = new CategoryListParams("test2");
-    assertFalse(
-        "2 CategoryListParams with the different terms should NOT equal each other.",
-        clParams1.equals(clParams2));
-    assertFalse(
-        "2 CategoryListParams with the different terms should NOT have the same hashcode.",
-        clParams1.hashCode() == clParams2.hashCode());
-  }
-
-}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/index/params/FacetIndexingParamsTest.java b/lucene/facet/src/test/org/apache/lucene/facet/index/params/FacetIndexingParamsTest.java
deleted file mode 100644
index f24fec8..0000000
--- a/lucene/facet/src/test/org/apache/lucene/facet/index/params/FacetIndexingParamsTest.java
+++ /dev/null
@@ -1,66 +0,0 @@
-package org.apache.lucene.facet.index.params;
-
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.search.DrillDown;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.util.PartitionsUtils;
-import org.apache.lucene.index.Term;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class FacetIndexingParamsTest extends FacetTestCase {
-
-  @Test
-  public void testDefaultSettings() {
-    FacetIndexingParams dfip = FacetIndexingParams.ALL_PARENTS;
-    assertNotNull("Missing default category list", dfip.getAllCategoryListParams());
-    assertEquals("all categories have the same CategoryListParams by default",
-        dfip.getCategoryListParams(null), dfip.getCategoryListParams(new CategoryPath("a")));
-    assertEquals("Expected default category list field is $facets", "$facets", dfip.getCategoryListParams(null).field);
-    String expectedDDText = "a"
-        + dfip.getFacetDelimChar() + "b";
-    CategoryPath cp = new CategoryPath("a", "b");
-    assertEquals("wrong drill-down term", new Term("$facets",
-        expectedDDText), DrillDown.term(dfip,cp));
-    char[] buf = new char[20];
-    int numchars = dfip.drillDownTermText(cp, buf);
-    assertEquals("3 characters should be written", 3, numchars);
-    assertEquals("wrong drill-down term text", expectedDDText, new String(
-        buf, 0, numchars));
-    assertEquals("partition for all ordinals is the first", "", 
-        PartitionsUtils.partitionNameByOrdinal(dfip, 250));
-    assertEquals("for partition 0, the same name should be returned",
-        "", PartitionsUtils.partitionName(0));
-    assertEquals(
-        "for any other, it's the concatenation of name + partition",
-        PartitionsUtils.PART_NAME_PREFIX + "1", PartitionsUtils.partitionName(1));
-    assertEquals("default partition number is always 0", 0, 
-        PartitionsUtils.partitionNumber(dfip,100));
-    assertEquals("default partition size is unbounded", Integer.MAX_VALUE,
-        dfip.getPartitionSize());
-  }
-
-  @Test
-  public void testCategoryListParamsWithDefaultIndexingParams() {
-    CategoryListParams clp = new CategoryListParams("clp");
-    FacetIndexingParams dfip = new FacetIndexingParams(clp);
-    assertEquals("Expected default category list field is " + clp.field, clp.field, dfip.getCategoryListParams(null).field);
-  }
-
-}
\ No newline at end of file
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/index/params/PerDimensionIndexingParamsTest.java b/lucene/facet/src/test/org/apache/lucene/facet/index/params/PerDimensionIndexingParamsTest.java
deleted file mode 100644
index ef8ec97..0000000
--- a/lucene/facet/src/test/org/apache/lucene/facet/index/params/PerDimensionIndexingParamsTest.java
+++ /dev/null
@@ -1,61 +0,0 @@
-package org.apache.lucene.facet.index.params;
-
-import java.util.Collections;
-
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.search.DrillDown;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.util.PartitionsUtils;
-import org.apache.lucene.index.Term;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class PerDimensionIndexingParamsTest extends FacetTestCase {
-
-  @Test
-  public void testTopLevelSettings() {
-    FacetIndexingParams ifip = new PerDimensionIndexingParams(Collections.<CategoryPath, CategoryListParams>emptyMap());
-    assertNotNull("Missing default category list", ifip.getAllCategoryListParams());
-    assertEquals("Expected default category list field is $facets", "$facets", ifip.getCategoryListParams(null).field);
-    String expectedDDText = "a" + ifip.getFacetDelimChar() + "b";
-    CategoryPath cp = new CategoryPath("a", "b");
-    assertEquals("wrong drill-down term", new Term("$facets", expectedDDText), DrillDown.term(ifip,cp));
-    char[] buf = new char[20];
-    int numchars = ifip.drillDownTermText(cp, buf);
-    assertEquals("3 characters should be written", 3, numchars);
-    assertEquals("wrong drill-down term text", expectedDDText, new String(buf, 0, numchars));
-    
-    assertEquals("partition for all ordinals is the first", "", PartitionsUtils.partitionNameByOrdinal(ifip, 250));
-    assertEquals("for partition 0, the same name should be returned", "", PartitionsUtils.partitionName(0));
-    assertEquals("for any other, it's the concatenation of name + partition", PartitionsUtils.PART_NAME_PREFIX + "1", PartitionsUtils.partitionName(1));
-    assertEquals("default partition number is always 0", 0, PartitionsUtils.partitionNumber(ifip,100));
-    assertEquals("default partition size is unbounded", Integer.MAX_VALUE, ifip.getPartitionSize());
-  }
-
-  @Test
-  public void testCategoryListParamsAddition() {
-    CategoryListParams clp = new CategoryListParams("clp");
-    PerDimensionIndexingParams tlfip = new PerDimensionIndexingParams(
-        Collections.<CategoryPath,CategoryListParams> singletonMap(new CategoryPath("a"), clp));
-    assertEquals("Expected category list field is " + clp.field, 
-        clp.field, tlfip.getCategoryListParams(new CategoryPath("a")).field);
-    assertNotSame("Unexpected default category list " + clp.field, clp, tlfip.getCategoryListParams(null));
-  }
-
-}
\ No newline at end of file
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/params/CategoryListParamsTest.java b/lucene/facet/src/test/org/apache/lucene/facet/params/CategoryListParamsTest.java
new file mode 100644
index 0000000..6fe71c2
--- /dev/null
+++ b/lucene/facet/src/test/org/apache/lucene/facet/params/CategoryListParamsTest.java
@@ -0,0 +1,91 @@
+package org.apache.lucene.facet.params;
+
+import org.apache.lucene.facet.FacetTestCase;
+import org.apache.lucene.facet.encoding.DGapVInt8IntEncoder;
+import org.apache.lucene.facet.encoding.IntDecoder;
+import org.apache.lucene.facet.encoding.IntEncoder;
+import org.apache.lucene.facet.encoding.SortingIntEncoder;
+import org.apache.lucene.facet.encoding.UniqueValuesIntEncoder;
+import org.apache.lucene.facet.params.CategoryListParams;
+import org.junit.Test;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+public class CategoryListParamsTest extends FacetTestCase {
+
+  @Test
+  public void testDefaultSettings() {
+    CategoryListParams clp = new CategoryListParams();
+    assertEquals("wrong default field", "$facets", clp.field);
+    IntEncoder encoder = new SortingIntEncoder(new UniqueValuesIntEncoder(new DGapVInt8IntEncoder()));
+    IntDecoder decoder = encoder.createMatchingDecoder();
+    assertEquals("unexpected default encoder", encoder.toString(), clp.createEncoder().toString());
+    assertEquals("unexpected default decoder", decoder.toString(), clp.createEncoder().createMatchingDecoder().toString());
+  }
+  
+  /**
+   * Test that the {@link CategoryListParams#hashCode()} and
+   * {@link CategoryListParams#equals(Object)} are consistent.
+   */
+  @Test
+  public void testIdentity() {
+    CategoryListParams clParams1 = new CategoryListParams();
+    // Assert identity is correct - a CategoryListParams equals itself.
+    assertEquals("A CategoryListParams object does not equal itself.",
+        clParams1, clParams1);
+    // For completeness, the object's hashcode equals itself
+    assertEquals("A CategoryListParams object's hashCode does not equal itself.",
+        clParams1.hashCode(), clParams1.hashCode());
+  }
+
+  /**
+   * Test that CategoryListParams behave correctly when compared against each
+   * other.
+   */
+  @Test
+  public void testIdentityConsistency() {
+    // Test 2 CategoryListParams with the default parameter
+    CategoryListParams clParams1 = new CategoryListParams();
+    CategoryListParams clParams2 = new CategoryListParams();
+    assertEquals(
+        "2 CategoryListParams with the same default term should equal each other.",
+        clParams1, clParams2);
+    assertEquals("2 CategoryListParams with the same default term should have the same hashcode",
+        clParams1.hashCode(), clParams2.hashCode());
+
+    // Test 2 CategoryListParams with the same specified Term
+    clParams1 = new CategoryListParams("test");
+    clParams2 = new CategoryListParams("test");
+    assertEquals(
+        "2 CategoryListParams with the same term should equal each other.",
+        clParams1, clParams2);
+    assertEquals("2 CategoryListParams with the same term should have the same hashcode",
+        clParams1.hashCode(), clParams2.hashCode());
+    
+    // Test 2 CategoryListParams with DIFFERENT terms
+    clParams1 = new CategoryListParams("test1");
+    clParams2 = new CategoryListParams("test2");
+    assertFalse(
+        "2 CategoryListParams with the different terms should NOT equal each other.",
+        clParams1.equals(clParams2));
+    assertFalse(
+        "2 CategoryListParams with the different terms should NOT have the same hashcode.",
+        clParams1.hashCode() == clParams2.hashCode());
+  }
+
+}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/params/FacetIndexingParamsTest.java b/lucene/facet/src/test/org/apache/lucene/facet/params/FacetIndexingParamsTest.java
new file mode 100644
index 0000000..3f58bb5
--- /dev/null
+++ b/lucene/facet/src/test/org/apache/lucene/facet/params/FacetIndexingParamsTest.java
@@ -0,0 +1,68 @@
+package org.apache.lucene.facet.params;
+
+import org.apache.lucene.facet.FacetTestCase;
+import org.apache.lucene.facet.params.CategoryListParams;
+import org.apache.lucene.facet.params.FacetIndexingParams;
+import org.apache.lucene.facet.search.DrillDown;
+import org.apache.lucene.facet.taxonomy.CategoryPath;
+import org.apache.lucene.facet.util.PartitionsUtils;
+import org.apache.lucene.index.Term;
+import org.junit.Test;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+public class FacetIndexingParamsTest extends FacetTestCase {
+
+  @Test
+  public void testDefaultSettings() {
+    FacetIndexingParams dfip = FacetIndexingParams.ALL_PARENTS;
+    assertNotNull("Missing default category list", dfip.getAllCategoryListParams());
+    assertEquals("all categories have the same CategoryListParams by default",
+        dfip.getCategoryListParams(null), dfip.getCategoryListParams(new CategoryPath("a")));
+    assertEquals("Expected default category list field is $facets", "$facets", dfip.getCategoryListParams(null).field);
+    String expectedDDText = "a"
+        + dfip.getFacetDelimChar() + "b";
+    CategoryPath cp = new CategoryPath("a", "b");
+    assertEquals("wrong drill-down term", new Term("$facets",
+        expectedDDText), DrillDown.term(dfip,cp));
+    char[] buf = new char[20];
+    int numchars = dfip.drillDownTermText(cp, buf);
+    assertEquals("3 characters should be written", 3, numchars);
+    assertEquals("wrong drill-down term text", expectedDDText, new String(
+        buf, 0, numchars));
+    assertEquals("partition for all ordinals is the first", "", 
+        PartitionsUtils.partitionNameByOrdinal(dfip, 250));
+    assertEquals("for partition 0, the same name should be returned",
+        "", PartitionsUtils.partitionName(0));
+    assertEquals(
+        "for any other, it's the concatenation of name + partition",
+        PartitionsUtils.PART_NAME_PREFIX + "1", PartitionsUtils.partitionName(1));
+    assertEquals("default partition number is always 0", 0, 
+        PartitionsUtils.partitionNumber(dfip,100));
+    assertEquals("default partition size is unbounded", Integer.MAX_VALUE,
+        dfip.getPartitionSize());
+  }
+
+  @Test
+  public void testCategoryListParamsWithDefaultIndexingParams() {
+    CategoryListParams clp = new CategoryListParams("clp");
+    FacetIndexingParams dfip = new FacetIndexingParams(clp);
+    assertEquals("Expected default category list field is " + clp.field, clp.field, dfip.getCategoryListParams(null).field);
+  }
+
+}
\ No newline at end of file
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/params/FacetSearchParamsTest.java b/lucene/facet/src/test/org/apache/lucene/facet/params/FacetSearchParamsTest.java
new file mode 100644
index 0000000..d0fba73
--- /dev/null
+++ b/lucene/facet/src/test/org/apache/lucene/facet/params/FacetSearchParamsTest.java
@@ -0,0 +1,34 @@
+package org.apache.lucene.facet.params;
+
+import org.apache.lucene.facet.FacetTestCase;
+import org.apache.lucene.facet.params.FacetSearchParams;
+import org.junit.Test;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+public class FacetSearchParamsTest extends FacetTestCase {
+
+  @Test
+  public void testSearchParamsWithNullRequest() throws Exception {
+    try {
+      assertNull(new FacetSearchParams());
+      fail("FacetSearchParams should throw IllegalArgumentException when not adding requests");
+    } catch (IllegalArgumentException e) {
+    }
+  }
+}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/params/PerDimensionIndexingParamsTest.java b/lucene/facet/src/test/org/apache/lucene/facet/params/PerDimensionIndexingParamsTest.java
new file mode 100644
index 0000000..62d9cfe
--- /dev/null
+++ b/lucene/facet/src/test/org/apache/lucene/facet/params/PerDimensionIndexingParamsTest.java
@@ -0,0 +1,64 @@
+package org.apache.lucene.facet.params;
+
+import java.util.Collections;
+
+import org.apache.lucene.facet.FacetTestCase;
+import org.apache.lucene.facet.params.CategoryListParams;
+import org.apache.lucene.facet.params.FacetIndexingParams;
+import org.apache.lucene.facet.params.PerDimensionIndexingParams;
+import org.apache.lucene.facet.search.DrillDown;
+import org.apache.lucene.facet.taxonomy.CategoryPath;
+import org.apache.lucene.facet.util.PartitionsUtils;
+import org.apache.lucene.index.Term;
+import org.junit.Test;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+public class PerDimensionIndexingParamsTest extends FacetTestCase {
+
+  @Test
+  public void testTopLevelSettings() {
+    FacetIndexingParams ifip = new PerDimensionIndexingParams(Collections.<CategoryPath, CategoryListParams>emptyMap());
+    assertNotNull("Missing default category list", ifip.getAllCategoryListParams());
+    assertEquals("Expected default category list field is $facets", "$facets", ifip.getCategoryListParams(null).field);
+    String expectedDDText = "a" + ifip.getFacetDelimChar() + "b";
+    CategoryPath cp = new CategoryPath("a", "b");
+    assertEquals("wrong drill-down term", new Term("$facets", expectedDDText), DrillDown.term(ifip,cp));
+    char[] buf = new char[20];
+    int numchars = ifip.drillDownTermText(cp, buf);
+    assertEquals("3 characters should be written", 3, numchars);
+    assertEquals("wrong drill-down term text", expectedDDText, new String(buf, 0, numchars));
+    
+    assertEquals("partition for all ordinals is the first", "", PartitionsUtils.partitionNameByOrdinal(ifip, 250));
+    assertEquals("for partition 0, the same name should be returned", "", PartitionsUtils.partitionName(0));
+    assertEquals("for any other, it's the concatenation of name + partition", PartitionsUtils.PART_NAME_PREFIX + "1", PartitionsUtils.partitionName(1));
+    assertEquals("default partition number is always 0", 0, PartitionsUtils.partitionNumber(ifip,100));
+    assertEquals("default partition size is unbounded", Integer.MAX_VALUE, ifip.getPartitionSize());
+  }
+
+  @Test
+  public void testCategoryListParamsAddition() {
+    CategoryListParams clp = new CategoryListParams("clp");
+    PerDimensionIndexingParams tlfip = new PerDimensionIndexingParams(
+        Collections.<CategoryPath,CategoryListParams> singletonMap(new CategoryPath("a"), clp));
+    assertEquals("Expected category list field is " + clp.field, 
+        clp.field, tlfip.getCategoryListParams(new CategoryPath("a")).field);
+    assertNotSame("Unexpected default category list " + clp.field, clp, tlfip.getCategoryListParams(null));
+  }
+
+}
\ No newline at end of file
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/sampling/BaseSampleTestTopK.java b/lucene/facet/src/test/org/apache/lucene/facet/sampling/BaseSampleTestTopK.java
new file mode 100644
index 0000000..8214160
--- /dev/null
+++ b/lucene/facet/src/test/org/apache/lucene/facet/sampling/BaseSampleTestTopK.java
@@ -0,0 +1,147 @@
+package org.apache.lucene.facet.sampling;
+
+import java.util.List;
+import java.util.Random;
+
+import org.apache.lucene.facet.params.FacetIndexingParams;
+import org.apache.lucene.facet.params.FacetSearchParams;
+import org.apache.lucene.facet.sampling.RandomSampler;
+import org.apache.lucene.facet.sampling.RepeatableSampler;
+import org.apache.lucene.facet.sampling.Sampler;
+import org.apache.lucene.facet.sampling.SamplingParams;
+import org.apache.lucene.facet.search.BaseTestTopK;
+import org.apache.lucene.facet.search.FacetRequest;
+import org.apache.lucene.facet.search.FacetResult;
+import org.apache.lucene.facet.search.FacetsCollector;
+import org.apache.lucene.facet.search.StandardFacetsAccumulator;
+import org.apache.lucene.facet.search.FacetRequest.ResultMode;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.TermQuery;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+public abstract class BaseSampleTestTopK extends BaseTestTopK {
+  
+  /** Number of top results */
+  protected static final int K = 2; 
+  
+  /** since there is a chance that this test would fail even if the code is correct, retry the sampling */
+  protected static final int RETRIES = 10;
+  
+  @Override
+  protected FacetSearchParams searchParamsWithRequests(int numResults, FacetIndexingParams fip) {
+    FacetSearchParams res = super.searchParamsWithRequests(numResults, fip);
+    for (FacetRequest req : res.facetRequests) {
+      // randomize the way we aggregate results
+      if (random().nextBoolean()) {
+        req.setResultMode(ResultMode.GLOBAL_FLAT);
+      } else {
+        req.setResultMode(ResultMode.PER_NODE_IN_TREE);
+      }
+    }
+    return res;
+  }
+  
+  protected abstract StandardFacetsAccumulator getSamplingAccumulator(Sampler sampler, TaxonomyReader taxoReader, 
+      IndexReader indexReader, FacetSearchParams searchParams);
+  
+  /**
+   * Try out faceted search with sampling enabled and complements either disabled or enforced
+   * Lots of randomly generated data is being indexed, and later on a "90% docs" faceted search
+   * is performed. The results are compared to non-sampled ones.
+   */
+  public void testCountUsingSampling() throws Exception {
+    boolean useRandomSampler = random().nextBoolean();
+    for (int partitionSize : partitionSizes) {
+      try {
+        // complements return counts for all ordinals, so force ALL_PARENTS indexing
+        // so that it's easier to compare
+        FacetIndexingParams fip = getFacetIndexingParams(partitionSize, true);
+        initIndex(fip);
+        // Get all of the documents and run the query, then do different
+        // facet counts and compare to control
+        Query q = new TermQuery(new Term(CONTENT_FIELD, BETA)); // 90% of the docs
+        
+        FacetSearchParams expectedSearchParams = searchParamsWithRequests(K, fip); 
+        FacetsCollector fc = FacetsCollector.create(expectedSearchParams, indexReader, taxoReader);
+        
+        searcher.search(q, fc);
+        
+        List<FacetResult> expectedResults = fc.getFacetResults();
+        
+        FacetSearchParams samplingSearchParams = searchParamsWithRequests(K, fip); 
+        
+        // try several times in case of failure, because the test has a chance to fail 
+        // if the top K facets are not sufficiently common with the sample set
+        for (int nTrial = 0; nTrial < RETRIES; nTrial++) {
+          try {
+            // complement with sampling!
+            final Sampler sampler = createSampler(nTrial, useRandomSampler);
+            
+            assertSampling(expectedResults, q, sampler, samplingSearchParams, false);
+            assertSampling(expectedResults, q, sampler, samplingSearchParams, true);
+            
+            break; // succeeded
+          } catch (AssertionError e) {
+            if (nTrial >= RETRIES - 1) {
+              throw e; // no more retries allowed, must fail
+            }
+          }
+        }
+      } finally { 
+        closeAll();
+      }
+    }
+  }
+  
+  private void assertSampling(List<FacetResult> expected, Query q, Sampler sampler, FacetSearchParams params, boolean complement) throws Exception {
+    FacetsCollector samplingFC = samplingCollector(complement, sampler, params);
+    
+    searcher.search(q, samplingFC);
+    List<FacetResult> sampledResults = samplingFC.getFacetResults();
+    
+    assertSameResults(expected, sampledResults);
+  }
+  
+  private FacetsCollector samplingCollector(final boolean complement, final Sampler sampler,
+      FacetSearchParams samplingSearchParams) {
+    StandardFacetsAccumulator sfa = getSamplingAccumulator(sampler, taxoReader, indexReader, samplingSearchParams);
+    sfa.setComplementThreshold(complement ? StandardFacetsAccumulator.FORCE_COMPLEMENT : StandardFacetsAccumulator.DISABLE_COMPLEMENT);
+    return FacetsCollector.create(sfa);
+  }
+  
+  private Sampler createSampler(int nTrial, boolean useRandomSampler) {
+    SamplingParams samplingParams = new SamplingParams();
+    
+    final double retryFactor = Math.pow(1.01, nTrial);
+    samplingParams.setSampleRatio(0.8 * retryFactor);
+    samplingParams.setMinSampleSize((int) (100 * retryFactor));
+    samplingParams.setMaxSampleSize((int) (10000 * retryFactor));
+    samplingParams.setOversampleFactor(5.0 * retryFactor);
+    samplingParams.setSamplingThreshold(11000); //force sampling
+
+    Sampler sampler = useRandomSampler ? 
+        new RandomSampler(samplingParams, new Random(random().nextLong())) :
+          new RepeatableSampler(samplingParams);
+    return sampler;
+  }
+  
+}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/sampling/OversampleWithDepthTest.java b/lucene/facet/src/test/org/apache/lucene/facet/sampling/OversampleWithDepthTest.java
new file mode 100644
index 0000000..3a3bdd4
--- /dev/null
+++ b/lucene/facet/src/test/org/apache/lucene/facet/sampling/OversampleWithDepthTest.java
@@ -0,0 +1,128 @@
+package org.apache.lucene.facet.sampling;
+
+import java.io.IOException;
+import java.util.Collections;
+
+import org.apache.lucene.document.Document;
+import org.apache.lucene.facet.FacetTestCase;
+import org.apache.lucene.facet.index.FacetFields;
+import org.apache.lucene.facet.params.FacetIndexingParams;
+import org.apache.lucene.facet.params.FacetSearchParams;
+import org.apache.lucene.facet.sampling.RandomSampler;
+import org.apache.lucene.facet.sampling.Sampler;
+import org.apache.lucene.facet.sampling.SamplingAccumulator;
+import org.apache.lucene.facet.sampling.SamplingParams;
+import org.apache.lucene.facet.search.CountFacetRequest;
+import org.apache.lucene.facet.search.FacetRequest;
+import org.apache.lucene.facet.search.FacetResult;
+import org.apache.lucene.facet.search.FacetResultNode;
+import org.apache.lucene.facet.search.FacetsCollector;
+import org.apache.lucene.facet.search.StandardFacetsAccumulator;
+import org.apache.lucene.facet.search.FacetRequest.ResultMode;
+import org.apache.lucene.facet.taxonomy.CategoryPath;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.IOUtils;
+import org.junit.Test;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+public class OversampleWithDepthTest extends FacetTestCase {
+  
+  @Test
+  public void testCountWithdepthUsingSampling() throws Exception, IOException {
+    Directory indexDir = newDirectory();
+    Directory taxoDir = newDirectory();
+    
+    FacetIndexingParams fip = new FacetIndexingParams(randomCategoryListParams());
+    
+    // index 100 docs, each with one category: ["root", docnum/10, docnum]
+    // e.g. root/8/87
+    index100Docs(indexDir, taxoDir, fip);
+    
+    DirectoryReader r = DirectoryReader.open(indexDir);
+    TaxonomyReader tr = new DirectoryTaxonomyReader(taxoDir);
+    
+    CountFacetRequest facetRequest = new CountFacetRequest(new CategoryPath("root"), 10);
+    // Setting the depth to '2', should potentially get all categories
+    facetRequest.setDepth(2);
+    facetRequest.setResultMode(ResultMode.PER_NODE_IN_TREE);
+
+    FacetSearchParams fsp = new FacetSearchParams(fip, facetRequest);
+    
+    // Craft sampling params to enforce sampling
+    final SamplingParams params = new SamplingParams();
+    params.setMinSampleSize(2);
+    params.setMaxSampleSize(50);
+    params.setOversampleFactor(5);
+    params.setSamplingThreshold(60);
+    params.setSampleRatio(0.1);
+    
+    FacetResult res = searchWithFacets(r, tr, fsp, params);
+    FacetRequest req = res.getFacetRequest();
+    assertEquals(facetRequest, req);
+    
+    FacetResultNode rootNode = res.getFacetResultNode();
+    
+    // Each node below root should also have sub-results as the requested depth was '2'
+    for (FacetResultNode node : rootNode.subResults) {
+      assertTrue("node " + node.label + " should have had children as the requested depth was '2'", node.subResults.size() > 0);
+    }
+    
+    IOUtils.close(r, tr, indexDir, taxoDir);
+  }
+
+  private void index100Docs(Directory indexDir, Directory taxoDir, FacetIndexingParams fip) throws IOException {
+    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, null);
+    IndexWriter w = new IndexWriter(indexDir, iwc);
+    TaxonomyWriter tw = new DirectoryTaxonomyWriter(taxoDir);
+    
+    FacetFields facetFields = new FacetFields(tw, fip);
+    for (int i = 0; i < 100; i++) {
+      Document doc = new Document();
+      CategoryPath cp = new CategoryPath("root",Integer.toString(i / 10), Integer.toString(i));
+      facetFields.addFields(doc, Collections.singletonList(cp));
+      w.addDocument(doc);
+    }
+    IOUtils.close(tw, w);
+  }
+
+  /** search reader <code>r</code>*/
+  private FacetResult searchWithFacets(IndexReader r, TaxonomyReader tr, FacetSearchParams fsp, 
+      final SamplingParams params) throws IOException {
+    // a FacetsCollector with a sampling accumulator
+    Sampler sampler = new RandomSampler(params, random());
+    StandardFacetsAccumulator sfa = new SamplingAccumulator(sampler, fsp, r, tr);
+    FacetsCollector fcWithSampling = FacetsCollector.create(sfa);
+    
+    IndexSearcher s = new IndexSearcher(r);
+    s.search(new MatchAllDocsQuery(), fcWithSampling);
+    
+    // there's only one expected result, return just it.
+    return fcWithSampling.getFacetResults().get(0);
+  }
+}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/sampling/SamplingAccumulatorTest.java b/lucene/facet/src/test/org/apache/lucene/facet/sampling/SamplingAccumulatorTest.java
new file mode 100644
index 0000000..9950904
--- /dev/null
+++ b/lucene/facet/src/test/org/apache/lucene/facet/sampling/SamplingAccumulatorTest.java
@@ -0,0 +1,36 @@
+package org.apache.lucene.facet.sampling;
+
+import org.apache.lucene.facet.params.FacetSearchParams;
+import org.apache.lucene.facet.sampling.Sampler;
+import org.apache.lucene.facet.sampling.SamplingAccumulator;
+import org.apache.lucene.facet.search.StandardFacetsAccumulator;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.util.LuceneTestCase.Slow;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+@Slow
+public class SamplingAccumulatorTest extends BaseSampleTestTopK {
+
+  @Override
+  protected StandardFacetsAccumulator getSamplingAccumulator(Sampler sampler, TaxonomyReader taxoReader, 
+      IndexReader indexReader, FacetSearchParams searchParams) {
+    return new SamplingAccumulator(sampler, searchParams, indexReader, taxoReader);
+  }
+}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/sampling/SamplingWrapperTest.java b/lucene/facet/src/test/org/apache/lucene/facet/sampling/SamplingWrapperTest.java
new file mode 100644
index 0000000..4eddb07
--- /dev/null
+++ b/lucene/facet/src/test/org/apache/lucene/facet/sampling/SamplingWrapperTest.java
@@ -0,0 +1,38 @@
+package org.apache.lucene.facet.sampling;
+
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.util.LuceneTestCase.Slow;
+
+import org.apache.lucene.facet.params.FacetSearchParams;
+import org.apache.lucene.facet.sampling.Sampler;
+import org.apache.lucene.facet.sampling.SamplingWrapper;
+import org.apache.lucene.facet.search.StandardFacetsAccumulator;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+@Slow
+public class SamplingWrapperTest extends BaseSampleTestTopK {
+
+  @Override
+  protected StandardFacetsAccumulator getSamplingAccumulator(Sampler sampler, TaxonomyReader taxoReader, 
+      IndexReader indexReader, FacetSearchParams searchParams) {
+    return new SamplingWrapper(new StandardFacetsAccumulator(searchParams, indexReader, taxoReader), sampler);
+  }
+  
+}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/search/AdaptiveAccumulatorTest.java b/lucene/facet/src/test/org/apache/lucene/facet/search/AdaptiveAccumulatorTest.java
index 8775052..15bce9c 100644
--- a/lucene/facet/src/test/org/apache/lucene/facet/search/AdaptiveAccumulatorTest.java
+++ b/lucene/facet/src/test/org/apache/lucene/facet/search/AdaptiveAccumulatorTest.java
@@ -3,9 +3,9 @@ package org.apache.lucene.facet.search;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.util.LuceneTestCase.Slow;
 
-import org.apache.lucene.facet.search.params.FacetSearchParams;
-import org.apache.lucene.facet.search.sampling.BaseSampleTestTopK;
-import org.apache.lucene.facet.search.sampling.Sampler;
+import org.apache.lucene.facet.params.FacetSearchParams;
+import org.apache.lucene.facet.sampling.BaseSampleTestTopK;
+import org.apache.lucene.facet.sampling.Sampler;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 
 /*
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/search/BaseTestTopK.java b/lucene/facet/src/test/org/apache/lucene/facet/search/BaseTestTopK.java
index f9f58d9..e717647 100644
--- a/lucene/facet/src/test/org/apache/lucene/facet/search/BaseTestTopK.java
+++ b/lucene/facet/src/test/org/apache/lucene/facet/search/BaseTestTopK.java
@@ -11,10 +11,8 @@ import org.apache.lucene.util._TestUtil;
 
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.facet.FacetTestBase;
-import org.apache.lucene.facet.index.params.FacetIndexingParams;
-import org.apache.lucene.facet.search.params.CountFacetRequest;
-import org.apache.lucene.facet.search.params.FacetRequest;
-import org.apache.lucene.facet.search.params.FacetSearchParams;
+import org.apache.lucene.facet.params.FacetIndexingParams;
+import org.apache.lucene.facet.params.FacetSearchParams;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
 
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/search/CategoryListIteratorTest.java b/lucene/facet/src/test/org/apache/lucene/facet/search/CategoryListIteratorTest.java
index a8d9d8b..e4fbc3e 100644
--- a/lucene/facet/src/test/org/apache/lucene/facet/search/CategoryListIteratorTest.java
+++ b/lucene/facet/src/test/org/apache/lucene/facet/search/CategoryListIteratorTest.java
@@ -8,17 +8,17 @@ import org.apache.lucene.analysis.MockTokenizer;
 import org.apache.lucene.document.BinaryDocValuesField;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.facet.FacetTestCase;
+import org.apache.lucene.facet.encoding.DGapIntEncoder;
+import org.apache.lucene.facet.encoding.IntEncoder;
+import org.apache.lucene.facet.encoding.SortingIntEncoder;
+import org.apache.lucene.facet.encoding.UniqueValuesIntEncoder;
+import org.apache.lucene.facet.encoding.VInt8IntEncoder;
 import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.RandomIndexWriter;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.IntsRef;
-import org.apache.lucene.util.encoding.DGapIntEncoder;
-import org.apache.lucene.util.encoding.IntEncoder;
-import org.apache.lucene.util.encoding.SortingIntEncoder;
-import org.apache.lucene.util.encoding.UniqueValuesIntEncoder;
-import org.apache.lucene.util.encoding.VInt8IntEncoder;
 import org.junit.Test;
 
 /*
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/search/CountingFacetsAggregatorTest.java b/lucene/facet/src/test/org/apache/lucene/facet/search/CountingFacetsAggregatorTest.java
index 383e667..f009536 100644
--- a/lucene/facet/src/test/org/apache/lucene/facet/search/CountingFacetsAggregatorTest.java
+++ b/lucene/facet/src/test/org/apache/lucene/facet/search/CountingFacetsAggregatorTest.java
@@ -14,15 +14,13 @@ import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field.Store;
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.facet.FacetTestCase;
+import org.apache.lucene.facet.collections.ObjectToIntMap;
 import org.apache.lucene.facet.index.FacetFields;
-import org.apache.lucene.facet.index.params.CategoryListParams;
-import org.apache.lucene.facet.index.params.CategoryListParams.OrdinalPolicy;
-import org.apache.lucene.facet.index.params.FacetIndexingParams;
-import org.apache.lucene.facet.index.params.PerDimensionOrdinalPolicy;
-import org.apache.lucene.facet.search.params.CountFacetRequest;
-import org.apache.lucene.facet.search.params.FacetSearchParams;
-import org.apache.lucene.facet.search.results.FacetResult;
-import org.apache.lucene.facet.search.results.FacetResultNode;
+import org.apache.lucene.facet.params.CategoryListParams;
+import org.apache.lucene.facet.params.FacetIndexingParams;
+import org.apache.lucene.facet.params.FacetSearchParams;
+import org.apache.lucene.facet.params.PerDimensionOrdinalPolicy;
+import org.apache.lucene.facet.params.CategoryListParams.OrdinalPolicy;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
@@ -39,7 +37,6 @@ import org.apache.lucene.search.MatchAllDocsQuery;
 import org.apache.lucene.search.TermQuery;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.collections.ObjectToIntMap;
 import org.junit.AfterClass;
 import org.junit.BeforeClass;
 import org.junit.Test;
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/search/DrillDownTest.java b/lucene/facet/src/test/org/apache/lucene/facet/search/DrillDownTest.java
index 16846d6..1c62048 100644
--- a/lucene/facet/src/test/org/apache/lucene/facet/search/DrillDownTest.java
+++ b/lucene/facet/src/test/org/apache/lucene/facet/search/DrillDownTest.java
@@ -12,9 +12,9 @@ import org.apache.lucene.document.Field;
 import org.apache.lucene.document.TextField;
 import org.apache.lucene.facet.FacetTestCase;
 import org.apache.lucene.facet.index.FacetFields;
-import org.apache.lucene.facet.index.params.CategoryListParams;
-import org.apache.lucene.facet.index.params.FacetIndexingParams;
-import org.apache.lucene.facet.index.params.PerDimensionIndexingParams;
+import org.apache.lucene.facet.params.CategoryListParams;
+import org.apache.lucene.facet.params.FacetIndexingParams;
+import org.apache.lucene.facet.params.PerDimensionIndexingParams;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
 import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/search/FacetRequestTest.java b/lucene/facet/src/test/org/apache/lucene/facet/search/FacetRequestTest.java
new file mode 100644
index 0000000..c68ca4e
--- /dev/null
+++ b/lucene/facet/src/test/org/apache/lucene/facet/search/FacetRequestTest.java
@@ -0,0 +1,47 @@
+package org.apache.lucene.facet.search;
+
+import org.apache.lucene.facet.FacetTestCase;
+import org.apache.lucene.facet.search.CountFacetRequest;
+import org.apache.lucene.facet.taxonomy.CategoryPath;
+import org.junit.Test;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+public class FacetRequestTest extends FacetTestCase {
+
+  @Test(expected=IllegalArgumentException.class)
+  public void testIllegalNumResults() throws Exception {
+    assertNotNull(new CountFacetRequest(new CategoryPath("a", "b"), 0));
+  }
+  
+  @Test(expected=IllegalArgumentException.class)
+  public void testIllegalCategoryPath() throws Exception {
+    assertNotNull(new CountFacetRequest(null, 1));
+  }
+
+  @Test
+  public void testHashAndEquals() {
+    CountFacetRequest fr1 = new CountFacetRequest(new CategoryPath("a"), 8);
+    CountFacetRequest fr2 = new CountFacetRequest(new CategoryPath("a"), 8);
+    assertEquals("hashCode() should agree on both objects", fr1.hashCode(), fr2.hashCode());
+    assertTrue("equals() should return true", fr1.equals(fr2));
+    fr1.setDepth(10);
+    assertFalse("equals() should return false as fr1.depth != fr2.depth", fr1.equals(fr2));
+  }
+  
+}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/search/MultiCategoryListIteratorTest.java b/lucene/facet/src/test/org/apache/lucene/facet/search/MultiCategoryListIteratorTest.java
new file mode 100644
index 0000000..14ced14
--- /dev/null
+++ b/lucene/facet/src/test/org/apache/lucene/facet/search/MultiCategoryListIteratorTest.java
@@ -0,0 +1,118 @@
+package org.apache.lucene.facet.search;
+
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.Random;
+
+import org.apache.lucene.document.Document;
+import org.apache.lucene.facet.FacetTestCase;
+import org.apache.lucene.facet.encoding.IntDecoder;
+import org.apache.lucene.facet.index.FacetFields;
+import org.apache.lucene.facet.params.CategoryListParams;
+import org.apache.lucene.facet.params.PerDimensionIndexingParams;
+import org.apache.lucene.facet.search.CategoryListIterator;
+import org.apache.lucene.facet.search.DocValuesCategoryListIterator;
+import org.apache.lucene.facet.taxonomy.CategoryPath;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
+import org.apache.lucene.facet.util.MultiCategoryListIterator;
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.IntsRef;
+import org.junit.Test;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+public class MultiCategoryListIteratorTest extends FacetTestCase {
+
+  @Test
+  public void testMultipleCategoryLists() throws Exception {
+    Random random = random();
+    int numDimensions = atLeast(random, 2); // at least 2 dimensions
+    String[] dimensions = new String[numDimensions];
+    for (int i = 0; i < numDimensions; i++) {
+      dimensions[i] = "dim" + i;
+    }
+    
+    // build the PerDimensionIndexingParams
+    HashMap<CategoryPath,CategoryListParams> clps = new HashMap<CategoryPath,CategoryListParams>();
+    for (String dim : dimensions) {
+      CategoryPath cp = new CategoryPath(dim);
+      CategoryListParams clp = randomCategoryListParams("$" + dim);
+      clps.put(cp, clp);
+    }
+    PerDimensionIndexingParams indexingParams = new PerDimensionIndexingParams(clps);
+    
+    // index some documents
+    Directory indexDir = newDirectory();
+    Directory taxoDir = newDirectory();
+    IndexWriter indexWriter = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, null).setMaxBufferedDocs(2));
+    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
+    FacetFields facetFields = new FacetFields(taxoWriter, indexingParams);
+    int ndocs = atLeast(random, 10);
+    for (int i = 0; i < ndocs; i++) {
+      Document doc = new Document();
+      int numCategories = random.nextInt(numDimensions) + 1;
+      ArrayList<CategoryPath> categories = new ArrayList<CategoryPath>();
+      for (int j = 0; j < numCategories; j++) {
+        String dimension = dimensions[random.nextInt(dimensions.length)];
+        categories.add(new CategoryPath(dimension, Integer.toString(i)));
+      }
+      facetFields.addFields(doc, categories);
+      indexWriter.addDocument(doc);
+    }
+    IOUtils.close(indexWriter, taxoWriter);
+    
+    // test the multi iterator
+    DirectoryReader indexReader = DirectoryReader.open(indexDir);
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoDir);
+    CategoryListIterator[] iterators = new CategoryListIterator[numDimensions];
+    for (int i = 0; i < iterators.length; i++) {
+      CategoryListParams clp = indexingParams.getCategoryListParams(new CategoryPath(dimensions[i]));
+      IntDecoder decoder = clp.createEncoder().createMatchingDecoder();
+      iterators[i] = new DocValuesCategoryListIterator(clp.field, decoder);
+    }
+    MultiCategoryListIterator cli = new MultiCategoryListIterator(iterators);
+    for (AtomicReaderContext context : indexReader.leaves()) {
+      assertTrue("failed to init multi-iterator", cli.setNextReader(context));
+      IntsRef ordinals = new IntsRef();
+      final int maxDoc = context.reader().maxDoc();
+      for (int i = 0; i < maxDoc; i++) {
+        cli.getOrdinals(i, ordinals);
+        assertTrue("document " + i + " does not have categories", ordinals.length > 0);
+        for (int j = 0; j < ordinals.length; j++) {
+          CategoryPath cp = taxoReader.getPath(ordinals.ints[j]);
+          assertNotNull("ordinal " + ordinals.ints[j] + " not found in taxonomy", cp);
+          if (cp.length == 2) {
+            int globalDoc = i + context.docBase;
+            assertEquals("invalid category for document " + globalDoc, globalDoc, Integer.parseInt(cp.components[1]));
+          }
+        }
+      }
+    }
+    
+    IOUtils.close(indexReader, taxoReader);
+    IOUtils.close(indexDir, taxoDir);
+  }
+  
+}
\ No newline at end of file
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/search/SamplingWrapperTest.java b/lucene/facet/src/test/org/apache/lucene/facet/search/SamplingWrapperTest.java
deleted file mode 100644
index 058e664..0000000
--- a/lucene/facet/src/test/org/apache/lucene/facet/search/SamplingWrapperTest.java
+++ /dev/null
@@ -1,37 +0,0 @@
-package org.apache.lucene.facet.search;
-
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.util.LuceneTestCase.Slow;
-
-import org.apache.lucene.facet.search.params.FacetSearchParams;
-import org.apache.lucene.facet.search.sampling.BaseSampleTestTopK;
-import org.apache.lucene.facet.search.sampling.Sampler;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-@Slow
-public class SamplingWrapperTest extends BaseSampleTestTopK {
-
-  @Override
-  protected StandardFacetsAccumulator getSamplingAccumulator(Sampler sampler, TaxonomyReader taxoReader, 
-      IndexReader indexReader, FacetSearchParams searchParams) {
-    return new SamplingWrapper(new StandardFacetsAccumulator(searchParams, indexReader, taxoReader), sampler);
-  }
-  
-}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/search/TestDemoFacets.java b/lucene/facet/src/test/org/apache/lucene/facet/search/TestDemoFacets.java
index 1e402cc..ac70655 100644
--- a/lucene/facet/src/test/org/apache/lucene/facet/search/TestDemoFacets.java
+++ b/lucene/facet/src/test/org/apache/lucene/facet/search/TestDemoFacets.java
@@ -27,9 +27,7 @@ import org.apache.lucene.document.Document;
 import org.apache.lucene.facet.FacetTestCase;
 import org.apache.lucene.facet.FacetTestUtils;
 import org.apache.lucene.facet.index.FacetFields;
-import org.apache.lucene.facet.search.params.CountFacetRequest;
-import org.apache.lucene.facet.search.params.FacetSearchParams;
-import org.apache.lucene.facet.search.results.FacetResult;
+import org.apache.lucene.facet.params.FacetSearchParams;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/search/TestFacetsAccumulatorWithComplement.java b/lucene/facet/src/test/org/apache/lucene/facet/search/TestFacetsAccumulatorWithComplement.java
deleted file mode 100644
index ef45025..0000000
--- a/lucene/facet/src/test/org/apache/lucene/facet/search/TestFacetsAccumulatorWithComplement.java
+++ /dev/null
@@ -1,138 +0,0 @@
-package org.apache.lucene.facet.search;
-
-import java.io.IOException;
-import java.util.List;
-
-import org.apache.lucene.facet.FacetTestBase;
-import org.apache.lucene.facet.index.params.FacetIndexingParams;
-import org.apache.lucene.facet.search.params.CountFacetRequest;
-import org.apache.lucene.facet.search.params.FacetSearchParams;
-import org.apache.lucene.facet.search.results.FacetResult;
-import org.apache.lucene.facet.search.results.FacetResultNode;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.MultiReader;
-import org.apache.lucene.index.ParallelAtomicReader;
-import org.apache.lucene.index.SlowCompositeReaderWrapper;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.junit.After;
-import org.junit.Before;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Test that complementsworks as expected.
- * We place this test under *.facet.search rather than *.search
- * because the test actually does faceted search.
- */
-public class TestFacetsAccumulatorWithComplement extends FacetTestBase {
-  
-  private FacetIndexingParams fip;
-  
-  @Override
-  @Before
-  public void setUp() throws Exception {
-    super.setUp();
-    fip = getFacetIndexingParams(Integer.MAX_VALUE);
-    initIndex(fip);
-  }
-  
-  @Override
-  @After
-  public void tearDown() throws Exception {
-    closeAll();
-    super.tearDown();
-  }
-  
-  /**
-   * Test that complements does not cause a failure when using a parallel reader
-   */
-  @Test
-  public void testComplementsWithParallerReader() throws Exception {
-    IndexReader origReader = indexReader; 
-    ParallelAtomicReader pr = new ParallelAtomicReader(SlowCompositeReaderWrapper.wrap(origReader));
-    indexReader = pr;
-    try {
-      doTestComplements();
-    } finally {
-      indexReader = origReader;
-    }
-  }
-
-  /**
-   * Test that complements works with MultiReader
-   */
-  @Test
-  public void testComplementsWithMultiReader() throws Exception {
-    final IndexReader origReader = indexReader; 
-    indexReader = new MultiReader(origReader);
-    try {
-      doTestComplements();
-    } finally {
-      indexReader = origReader;
-    }
-  }
-  
-  /**
-   * Test that score is indeed constant when using a constant score
-   */
-  @Test
-  public void testComplements() throws Exception {
-    doTestComplements();
-  }
-  
-  private void doTestComplements() throws Exception {
-    // verify by facet values
-    List<FacetResult> countResWithComplement = findFacets(true);
-    List<FacetResult> countResNoComplement = findFacets(false);
-    
-    assertEquals("Wrong number of facet count results with complement!",1,countResWithComplement.size());
-    assertEquals("Wrong number of facet count results no complement!",1,countResNoComplement.size());
-    
-    FacetResultNode parentResWithComp = countResWithComplement.get(0).getFacetResultNode();
-    FacetResultNode parentResNoComp = countResWithComplement.get(0).getFacetResultNode();
-    
-    assertEquals("Wrong number of top count aggregated categories with complement!",3,parentResWithComp.subResults.size());
-    assertEquals("Wrong number of top count aggregated categories no complement!",3,parentResNoComp.subResults.size());
-  }
-  
-  /** compute facets with certain facet requests and docs */
-  private List<FacetResult> findFacets(boolean withComplement) throws IOException {
-    FacetSearchParams fsp = new FacetSearchParams(fip, new CountFacetRequest(new CategoryPath("root","a"), 10));
-    StandardFacetsAccumulator sfa = new StandardFacetsAccumulator(fsp, indexReader, taxoReader);
-    sfa.setComplementThreshold(withComplement ? StandardFacetsAccumulator.FORCE_COMPLEMENT : StandardFacetsAccumulator.DISABLE_COMPLEMENT);
-    FacetsCollector fc = FacetsCollector.create(sfa);
-    searcher.search(new MatchAllDocsQuery(), fc);
-    
-    List<FacetResult> res = fc.getFacetResults();
-    
-    // Results are ready, printing them...
-    int i = 0;
-    for (FacetResult facetResult : res) {
-      if (VERBOSE) {
-        System.out.println("Res "+(i++)+": "+facetResult);
-      }
-    }
-    
-    assertEquals(withComplement, sfa.isUsingComplements);
-    
-    return res;
-  }
-  
-}
\ No newline at end of file
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/search/TestFacetsCollector.java b/lucene/facet/src/test/org/apache/lucene/facet/search/TestFacetsCollector.java
index d368266..53c5188 100644
--- a/lucene/facet/src/test/org/apache/lucene/facet/search/TestFacetsCollector.java
+++ b/lucene/facet/src/test/org/apache/lucene/facet/search/TestFacetsCollector.java
@@ -1,6 +1,5 @@
 package org.apache.lucene.facet.search;
 
-import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Collections;
 import java.util.HashMap;
@@ -13,14 +12,10 @@ import org.apache.lucene.document.Field.Store;
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.facet.FacetTestCase;
 import org.apache.lucene.facet.index.FacetFields;
-import org.apache.lucene.facet.index.params.CategoryListParams;
-import org.apache.lucene.facet.index.params.FacetIndexingParams;
-import org.apache.lucene.facet.index.params.PerDimensionIndexingParams;
-import org.apache.lucene.facet.search.FacetsCollector.MatchingDocs;
-import org.apache.lucene.facet.search.params.CountFacetRequest;
-import org.apache.lucene.facet.search.params.FacetSearchParams;
-import org.apache.lucene.facet.search.params.SumScoreFacetRequest;
-import org.apache.lucene.facet.search.results.FacetResult;
+import org.apache.lucene.facet.params.CategoryListParams;
+import org.apache.lucene.facet.params.FacetIndexingParams;
+import org.apache.lucene.facet.params.FacetSearchParams;
+import org.apache.lucene.facet.params.PerDimensionIndexingParams;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
 import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
@@ -164,34 +159,14 @@ public class TestFacetsCollector extends FacetTestCase {
         new CountFacetRequest(new CategoryPath("a"), 10), 
         new SumScoreFacetRequest(new CategoryPath("b"), 10));
     
-    final Map<CategoryListParams,FacetsAggregator> clpAggregator = new HashMap<CategoryListParams,FacetsAggregator>();
-    clpAggregator.put(fip.getCategoryListParams(new CategoryPath("a")), new FastCountingFacetsAggregator());
-    clpAggregator.put(fip.getCategoryListParams(new CategoryPath("b")), new SumScoreFacetsAggregator());
+    Map<CategoryListParams,FacetsAggregator> aggregators = new HashMap<CategoryListParams,FacetsAggregator>();
+    aggregators.put(fip.getCategoryListParams(new CategoryPath("a")), new FastCountingFacetsAggregator());
+    aggregators.put(fip.getCategoryListParams(new CategoryPath("b")), new SumScoreFacetsAggregator());
+    final FacetsAggregator aggregator = new PerCategoryListAggregator(aggregators, fip);
     FacetsAccumulator fa = new FacetsAccumulator(sParams, r, taxo) {
       @Override
       public FacetsAggregator getAggregator() {
-        return new FacetsAggregator() {
-          
-          @Override
-          public void rollupValues(int ordinal, int[] children, int[] siblings, FacetArrays facetArrays) {
-            throw new UnsupportedOperationException("not supported yet");
-          }
-          
-          @Override
-          public boolean requiresDocScores() {
-            for (FacetsAggregator aggregator : clpAggregator.values()) {
-              if (aggregator.requiresDocScores()) {
-                return true;
-              }
-            }
-            return false;
-          }
-          
-          @Override
-          public void aggregate(MatchingDocs matchingDocs, CategoryListParams clp, FacetArrays facetArrays) throws IOException {
-            clpAggregator.get(clp).aggregate(matchingDocs, clp, facetArrays);
-          }
-        };
+        return aggregator;
       }
     };
     
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/search/TestMultipleCategoryLists.java b/lucene/facet/src/test/org/apache/lucene/facet/search/TestMultipleCategoryLists.java
index b407c05..b69c433 100644
--- a/lucene/facet/src/test/org/apache/lucene/facet/search/TestMultipleCategoryLists.java
+++ b/lucene/facet/src/test/org/apache/lucene/facet/search/TestMultipleCategoryLists.java
@@ -15,15 +15,11 @@ import org.apache.lucene.document.Field;
 import org.apache.lucene.document.TextField;
 import org.apache.lucene.facet.FacetTestCase;
 import org.apache.lucene.facet.index.FacetFields;
-import org.apache.lucene.facet.index.params.CategoryListParams;
-import org.apache.lucene.facet.index.params.FacetIndexingParams;
-import org.apache.lucene.facet.index.params.PerDimensionIndexingParams;
-import org.apache.lucene.facet.search.params.CountFacetRequest;
-import org.apache.lucene.facet.search.params.FacetRequest;
-import org.apache.lucene.facet.search.params.FacetRequest.ResultMode;
-import org.apache.lucene.facet.search.params.FacetSearchParams;
-import org.apache.lucene.facet.search.results.FacetResult;
-import org.apache.lucene.facet.search.results.FacetResultNode;
+import org.apache.lucene.facet.params.CategoryListParams;
+import org.apache.lucene.facet.params.FacetIndexingParams;
+import org.apache.lucene.facet.params.FacetSearchParams;
+import org.apache.lucene.facet.params.PerDimensionIndexingParams;
+import org.apache.lucene.facet.search.FacetRequest.ResultMode;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/search/TestSameRequestAccumulation.java b/lucene/facet/src/test/org/apache/lucene/facet/search/TestSameRequestAccumulation.java
index 8575d32..83c2859 100644
--- a/lucene/facet/src/test/org/apache/lucene/facet/search/TestSameRequestAccumulation.java
+++ b/lucene/facet/src/test/org/apache/lucene/facet/search/TestSameRequestAccumulation.java
@@ -3,11 +3,9 @@ package org.apache.lucene.facet.search;
 import java.util.List;
 
 import org.apache.lucene.facet.FacetTestBase;
-import org.apache.lucene.facet.index.params.FacetIndexingParams;
+import org.apache.lucene.facet.params.FacetIndexingParams;
+import org.apache.lucene.facet.params.FacetSearchParams;
 import org.apache.lucene.facet.search.FacetsCollector;
-import org.apache.lucene.facet.search.params.CountFacetRequest;
-import org.apache.lucene.facet.search.params.FacetSearchParams;
-import org.apache.lucene.facet.search.results.FacetResult;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.search.MatchAllDocsQuery;
 import org.junit.After;
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/search/TestStandardFacetsAccumulator.java b/lucene/facet/src/test/org/apache/lucene/facet/search/TestStandardFacetsAccumulator.java
index 2707476..a348fd3 100644
--- a/lucene/facet/src/test/org/apache/lucene/facet/search/TestStandardFacetsAccumulator.java
+++ b/lucene/facet/src/test/org/apache/lucene/facet/search/TestStandardFacetsAccumulator.java
@@ -10,13 +10,9 @@ import org.apache.lucene.document.Field.Store;
 import org.apache.lucene.document.StringField;
 import org.apache.lucene.facet.FacetTestCase;
 import org.apache.lucene.facet.index.FacetFields;
-import org.apache.lucene.facet.index.params.CategoryListParams;
-import org.apache.lucene.facet.index.params.FacetIndexingParams;
-import org.apache.lucene.facet.search.params.CountFacetRequest;
-import org.apache.lucene.facet.search.params.FacetRequest;
-import org.apache.lucene.facet.search.params.FacetSearchParams;
-import org.apache.lucene.facet.search.results.FacetResult;
-import org.apache.lucene.facet.search.results.FacetResultNode;
+import org.apache.lucene.facet.params.CategoryListParams;
+import org.apache.lucene.facet.params.FacetIndexingParams;
+import org.apache.lucene.facet.params.FacetSearchParams;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/search/TestTopKInEachNodeResultHandler.java b/lucene/facet/src/test/org/apache/lucene/facet/search/TestTopKInEachNodeResultHandler.java
index 7a17bfe..a192bb4 100644
--- a/lucene/facet/src/test/org/apache/lucene/facet/search/TestTopKInEachNodeResultHandler.java
+++ b/lucene/facet/src/test/org/apache/lucene/facet/search/TestTopKInEachNodeResultHandler.java
@@ -11,13 +11,9 @@ import org.apache.lucene.document.Field;
 import org.apache.lucene.document.TextField;
 import org.apache.lucene.facet.FacetTestCase;
 import org.apache.lucene.facet.index.FacetFields;
-import org.apache.lucene.facet.index.params.FacetIndexingParams;
-import org.apache.lucene.facet.search.params.CountFacetRequest;
-import org.apache.lucene.facet.search.params.FacetRequest;
-import org.apache.lucene.facet.search.params.FacetRequest.ResultMode;
-import org.apache.lucene.facet.search.params.FacetSearchParams;
-import org.apache.lucene.facet.search.results.FacetResult;
-import org.apache.lucene.facet.search.results.FacetResultNode;
+import org.apache.lucene.facet.params.FacetIndexingParams;
+import org.apache.lucene.facet.params.FacetSearchParams;
+import org.apache.lucene.facet.search.FacetRequest.ResultMode;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
 import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/search/TestTopKResultsHandler.java b/lucene/facet/src/test/org/apache/lucene/facet/search/TestTopKResultsHandler.java
index df9f646..70c03ea 100644
--- a/lucene/facet/src/test/org/apache/lucene/facet/search/TestTopKResultsHandler.java
+++ b/lucene/facet/src/test/org/apache/lucene/facet/search/TestTopKResultsHandler.java
@@ -4,14 +4,10 @@ import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.List;
 
-import org.apache.lucene.facet.index.params.CategoryListParams.OrdinalPolicy;
-import org.apache.lucene.facet.index.params.FacetIndexingParams;
-import org.apache.lucene.facet.search.params.CountFacetRequest;
-import org.apache.lucene.facet.search.params.FacetRequest;
-import org.apache.lucene.facet.search.params.FacetRequest.ResultMode;
-import org.apache.lucene.facet.search.params.FacetSearchParams;
-import org.apache.lucene.facet.search.results.FacetResult;
-import org.apache.lucene.facet.search.results.FacetResultNode;
+import org.apache.lucene.facet.params.FacetIndexingParams;
+import org.apache.lucene.facet.params.FacetSearchParams;
+import org.apache.lucene.facet.params.CategoryListParams.OrdinalPolicy;
+import org.apache.lucene.facet.search.FacetRequest.ResultMode;
 import org.apache.lucene.facet.taxonomy.CategoryPath;
 import org.apache.lucene.search.MatchAllDocsQuery;
 import org.junit.Test;
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/search/TestTopKResultsHandlerRandom.java b/lucene/facet/src/test/org/apache/lucene/facet/search/TestTopKResultsHandlerRandom.java
index ced4df0..51e673a 100644
--- a/lucene/facet/src/test/org/apache/lucene/facet/search/TestTopKResultsHandlerRandom.java
+++ b/lucene/facet/src/test/org/apache/lucene/facet/search/TestTopKResultsHandlerRandom.java
@@ -4,10 +4,8 @@ import java.io.IOException;
 import java.util.HashMap;
 import java.util.List;
 
-import org.apache.lucene.facet.index.params.FacetIndexingParams;
-import org.apache.lucene.facet.search.params.FacetSearchParams;
-import org.apache.lucene.facet.search.results.FacetResult;
-import org.apache.lucene.facet.search.results.FacetResultNode;
+import org.apache.lucene.facet.params.FacetIndexingParams;
+import org.apache.lucene.facet.params.FacetSearchParams;
 import org.apache.lucene.search.MatchAllDocsQuery;
 import org.apache.lucene.search.Query;
 import org.junit.Test;
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/search/TestTotalFacetCounts.java b/lucene/facet/src/test/org/apache/lucene/facet/search/TestTotalFacetCounts.java
deleted file mode 100644
index f84678b..0000000
--- a/lucene/facet/src/test/org/apache/lucene/facet/search/TestTotalFacetCounts.java
+++ /dev/null
@@ -1,128 +0,0 @@
-package org.apache.lucene.facet.search;
-
-import java.io.File;
-import java.io.IOException;
-import java.util.Arrays;
-import java.util.Collections;
-
-import org.apache.lucene.document.Document;
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.index.FacetFields;
-import org.apache.lucene.facet.index.params.CategoryListParams;
-import org.apache.lucene.facet.index.params.FacetIndexingParams;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util._TestUtil;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class TestTotalFacetCounts extends FacetTestCase {
-
-  private static void initCache() {
-    TotalFacetCountsCache.getSingleton().clear();
-    TotalFacetCountsCache.getSingleton().setCacheSize(1); // Set to keep one in mem
-  }
-
-  @Test
-  public void testWriteRead() throws IOException {
-    doTestWriteRead(14);
-    doTestWriteRead(100);
-    doTestWriteRead(7);
-    doTestWriteRead(3);
-    doTestWriteRead(1);
-  }
-
-  private void doTestWriteRead(final int partitionSize) throws IOException {
-    initCache();
-
-    Directory indexDir = newDirectory();
-    Directory taxoDir = newDirectory();
-    IndexWriter indexWriter = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, null));
-    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
-    
-    FacetIndexingParams iParams = new FacetIndexingParams() {
-      @Override
-      public int getPartitionSize() {
-        return partitionSize;
-      }
-      
-      @Override
-      public CategoryListParams getCategoryListParams(CategoryPath category) {
-        return new CategoryListParams() {
-          @Override
-          public OrdinalPolicy getOrdinalPolicy(String dimension) {
-            return OrdinalPolicy.ALL_PARENTS;
-          }
-        };
-      }
-    };
-    // The counts that the TotalFacetCountsArray should have after adding
-    // the below facets to the index.
-    int[] expectedCounts = new int[] { 0, 3, 1, 3, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1 };
-    String[] categories = new String[] { "a/b", "c/d", "a/e", "a/d", "c/g", "c/z", "b/a", "1/2", "b/c" };
-
-    FacetFields facetFields = new FacetFields(taxoWriter, iParams);
-    for (String cat : categories) {
-      Document doc = new Document();
-      facetFields.addFields(doc, Collections.singletonList(new CategoryPath(cat, '/')));
-      indexWriter.addDocument(doc);
-    }
-
-    // Commit Changes
-    IOUtils.close(indexWriter, taxoWriter);
-
-    DirectoryReader indexReader = DirectoryReader.open(indexDir);
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoDir);
-    
-    int[] intArray = new int[iParams.getPartitionSize()];
-
-    TotalFacetCountsCache tfcc = TotalFacetCountsCache.getSingleton();
-    File tmpFile = _TestUtil.createTempFile("test", "tmp", TEMP_DIR);
-    tfcc.store(tmpFile, indexReader, taxoReader, iParams);
-    tfcc.clear(); // not really required because TFCC overrides on load(), but in the test we need not rely on this.
-    tfcc.load(tmpFile, indexReader, taxoReader, iParams);
-    
-    // now retrieve the one just loaded
-    TotalFacetCounts totalCounts = tfcc.getTotalCounts(indexReader, taxoReader, iParams);
-
-    int partition = 0;
-    for (int i = 0; i < expectedCounts.length; i += partitionSize) {
-      totalCounts.fillTotalCountsForPartition(intArray, partition);
-      int[] partitionExpectedCounts = new int[partitionSize];
-      int nToCopy = Math.min(partitionSize,expectedCounts.length-i);
-      System.arraycopy(expectedCounts, i, partitionExpectedCounts, 0, nToCopy);
-      assertTrue("Wrong counts! for partition "+partition+
-          "\nExpected:\n" + Arrays.toString(partitionExpectedCounts)+
-          "\nActual:\n" + Arrays.toString(intArray),
-          Arrays.equals(partitionExpectedCounts, intArray));
-      ++partition;
-    }
-    IOUtils.close(indexReader, taxoReader);
-    IOUtils.close(indexDir, taxoDir);
-    tmpFile.delete();
-  }
-
-}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/search/TestTotalFacetCountsCache.java b/lucene/facet/src/test/org/apache/lucene/facet/search/TestTotalFacetCountsCache.java
deleted file mode 100644
index 8e88dbb..0000000
--- a/lucene/facet/src/test/org/apache/lucene/facet/search/TestTotalFacetCountsCache.java
+++ /dev/null
@@ -1,500 +0,0 @@
-package org.apache.lucene.facet.search;
-
-import java.io.File;
-import java.io.IOException;
-import java.util.Collections;
-import java.util.List;
-import java.util.Random;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.analysis.MockTokenizer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.index.FacetFields;
-import org.apache.lucene.facet.index.params.FacetIndexingParams;
-import org.apache.lucene.facet.search.TotalFacetCounts.CreationType;
-import org.apache.lucene.facet.search.params.CountFacetRequest;
-import org.apache.lucene.facet.search.params.FacetSearchParams;
-import org.apache.lucene.facet.search.results.FacetResult;
-import org.apache.lucene.facet.search.results.FacetResultNode;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.MockDirectoryWrapper;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.SlowRAMDirectory;
-import org.apache.lucene.util._TestUtil;
-import org.junit.Before;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class TestTotalFacetCountsCache extends FacetTestCase {
-
-  static final TotalFacetCountsCache TFC = TotalFacetCountsCache.getSingleton();
-
-  /**
-   * Thread class to be used in tests for this method. This thread gets a TFC
-   * and records times.
-   */
-  private static class TFCThread extends Thread {
-    private final IndexReader r;
-    private final DirectoryTaxonomyReader tr;
-    private final FacetIndexingParams iParams;
-    
-    TotalFacetCounts tfc;
-
-    public TFCThread(IndexReader r, DirectoryTaxonomyReader tr, FacetIndexingParams iParams) {
-      this.r = r;
-      this.tr = tr;
-      this.iParams = iParams;
-    }
-    @Override
-    public void run() {
-      try {
-        tfc = TFC.getTotalCounts(r, tr, iParams);
-      } catch (Exception e) {
-        throw new RuntimeException(e);
-      }
-    }
-  }
-
-  /** Utility method to add a document and facets to an index/taxonomy. */
-  private static void addFacets(FacetIndexingParams iParams, IndexWriter iw,
-      TaxonomyWriter tw, String... strings) throws IOException {
-    Document doc = new Document();
-    FacetFields facetFields = new FacetFields(tw, iParams);
-    facetFields.addFields(doc, Collections.singletonList(new CategoryPath(strings)));
-    iw.addDocument(doc);
-  }
-
-  /** Clears the cache and sets its size to one. */
-  private static void initCache() {
-    TFC.clear();
-    TFC.setCacheSize(1); // Set to keep one in memory
-  }
-
-  @Override
-  @Before
-  public void setUp() throws Exception {
-    super.setUp();
-    initCache();
-  }
-
-  /** runs few searches in parallel */
-  public void testGeneralSynchronization() throws Exception {
-    int numIters = atLeast(4);
-    Random random = random();
-    for (int i = 0; i < numIters; i++) {
-      int numThreads = random.nextInt(3) + 2; // 2-4
-      int sleepMillis = random.nextBoolean() ? -1 : random.nextInt(10) + 1 /*1-10*/;
-      int cacheSize = random.nextInt(4); // 0-3
-      doTestGeneralSynchronization(numThreads, sleepMillis, cacheSize);
-    }
-  }
-
-  private static final String[] CATEGORIES = new String[] { "a/b", "c/d", "a/e", "a/d", "c/g", "c/z", "b/a", "1/2", "b/c" };
-
-  private void index(Directory indexDir, Directory taxoDir) throws IOException {
-    IndexWriter indexWriter = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, null));
-    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
-    FacetFields facetFields = new FacetFields(taxoWriter);
-    
-    for (String cat : CATEGORIES) {
-      Document doc = new Document();
-      facetFields.addFields(doc, Collections.singletonList(new CategoryPath(cat, '/')));
-      indexWriter.addDocument(doc);
-    }
-    
-    IOUtils.close(indexWriter, taxoWriter);
-  }
-  
-  private void doTestGeneralSynchronization(int numThreads, int sleepMillis, int cacheSize) throws Exception {
-    TFC.setCacheSize(cacheSize);
-    SlowRAMDirectory slowIndexDir = new SlowRAMDirectory(-1, random());
-    MockDirectoryWrapper indexDir = new MockDirectoryWrapper(random(), slowIndexDir);
-    SlowRAMDirectory slowTaxoDir = new SlowRAMDirectory(-1, random());
-    MockDirectoryWrapper taxoDir = new MockDirectoryWrapper(random(), slowTaxoDir);
-
-    // Index documents without the "slowness"
-    index(indexDir, taxoDir);
-
-    slowIndexDir.setSleepMillis(sleepMillis);
-    slowTaxoDir.setSleepMillis(sleepMillis);
-    
-    // Open the slow readers
-    IndexReader slowIndexReader = DirectoryReader.open(indexDir);
-    TaxonomyReader slowTaxoReader = new DirectoryTaxonomyReader(taxoDir);
-
-    // Class to perform search and return results as threads
-    class Multi extends Thread {
-      private List<FacetResult> results;
-      private FacetIndexingParams iParams;
-      private IndexReader indexReader;
-      private TaxonomyReader taxoReader;
-
-      public Multi(IndexReader indexReader, TaxonomyReader taxoReader, FacetIndexingParams iParams) {
-        this.indexReader = indexReader;
-        this.taxoReader = taxoReader;
-        this.iParams = iParams;
-      }
-
-      public List<FacetResult> getResults() {
-        return results;
-      }
-
-      @Override
-      public void run() {
-        try {
-          FacetSearchParams fsp = new FacetSearchParams(iParams, new CountFacetRequest(new CategoryPath("a"), 10),
-              new CountFacetRequest(new CategoryPath("b"), 10));
-          IndexSearcher searcher = new IndexSearcher(indexReader);
-          FacetsCollector fc = FacetsCollector.create(fsp, indexReader, taxoReader);
-          searcher.search(new MatchAllDocsQuery(), fc);
-          results = fc.getFacetResults();
-        } catch (Exception e) {
-          throw new RuntimeException(e);
-        }
-      }
-    }
-
-    Multi[] multis = new Multi[numThreads];
-    for (int i = 0; i < numThreads; i++) {
-      multis[i] = new Multi(slowIndexReader, slowTaxoReader, FacetIndexingParams.ALL_PARENTS);
-    }
-
-    for (Multi m : multis) {
-      m.start();
-    }
-
-    // Wait for threads and get results
-    String[] expLabelsA = new String[] { "a/d", "a/e", "a/b" };
-    String[] expLabelsB = new String[] { "b/c", "b/a" };
-    for (Multi m : multis) {
-      m.join();
-      List<FacetResult> facetResults = m.getResults();
-      assertEquals("expected two results", 2, facetResults.size());
-      
-      FacetResultNode nodeA = facetResults.get(0).getFacetResultNode();
-      int i = 0;
-      for (FacetResultNode node : nodeA.subResults) {
-        assertEquals("wrong count", 1, (int) node.value);
-        assertEquals(expLabelsA[i++], node.label.toString('/'));
-      }
-      
-      FacetResultNode nodeB = facetResults.get(1).getFacetResultNode();
-      i = 0;
-      for (FacetResultNode node : nodeB.subResults) {
-        assertEquals("wrong count", 1, (int) node.value);
-        assertEquals(expLabelsB[i++], node.label.toString('/'));
-      }
-    }
-    
-    IOUtils.close(slowIndexReader, slowTaxoReader, indexDir, taxoDir);
-  }
-
-  /**
-   * Simple test to make sure the TotalFacetCountsManager updates the
-   * TotalFacetCounts array only when it is supposed to, and whether it
-   * is recomputed or read from disk.
-   */
-  @Test
-  public void testGenerationalConsistency() throws Exception {
-    // Create temporary RAMDirectories
-    Directory indexDir = newDirectory();
-    Directory taxoDir = newDirectory();
-
-    // Create our index/taxonomy writers
-    IndexWriter indexWriter = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, null));
-    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
-    FacetIndexingParams iParams = FacetIndexingParams.ALL_PARENTS;
-
-    // Add a facet to the index
-    addFacets(iParams, indexWriter, taxoWriter, "a", "b");
-
-    // Commit Changes
-    indexWriter.commit();
-    taxoWriter.commit();
-
-    // Open readers
-    DirectoryReader indexReader = DirectoryReader.open(indexDir);
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoDir);
-
-    // As this is the first time we have invoked the TotalFacetCountsManager, 
-    // we should expect to compute and not read from disk.
-    TotalFacetCounts totalCounts = TFC.getTotalCounts(indexReader, taxoReader, iParams);
-    int prevGen = assertRecomputed(totalCounts, 0, "after first attempt to get it!");
-
-    // Repeating same operation should pull from the cache - not recomputed. 
-    assertTrue("Should be obtained from cache at 2nd attempt",totalCounts == 
-      TFC.getTotalCounts(indexReader, taxoReader, iParams));
-
-    // Repeat the same operation as above. but clear first - now should recompute again
-    initCache();
-    totalCounts = TFC.getTotalCounts(indexReader, taxoReader, iParams);
-    prevGen = assertRecomputed(totalCounts, prevGen, "after cache clear, 3rd attempt to get it!");
-    
-    //store to file
-    File outputFile = _TestUtil.createTempFile("test", "tmp", TEMP_DIR);
-    initCache();
-    TFC.store(outputFile, indexReader, taxoReader, iParams);
-    totalCounts = TFC.getTotalCounts(indexReader, taxoReader, iParams);
-    prevGen = assertRecomputed(totalCounts, prevGen, "after cache clear, 4th attempt to get it!");
-
-    //clear and load
-    initCache();
-    TFC.load(outputFile, indexReader, taxoReader, iParams);
-    totalCounts = TFC.getTotalCounts(indexReader, taxoReader, iParams);
-    prevGen = assertReadFromDisc(totalCounts, prevGen, "after 5th attempt to get it!");
-
-    // Add a new facet to the index, commit and refresh readers
-    addFacets(iParams, indexWriter, taxoWriter, "c", "d");
-    IOUtils.close(indexWriter, taxoWriter);
-
-    TaxonomyReader newTaxoReader = TaxonomyReader.openIfChanged(taxoReader);
-    assertNotNull(newTaxoReader);
-    assertTrue("should have received more cagtegories in updated taxonomy", newTaxoReader.getSize() > taxoReader.getSize());
-    taxoReader.close();
-    taxoReader = newTaxoReader;
-    
-    DirectoryReader r2 = DirectoryReader.openIfChanged(indexReader);
-    assertNotNull(r2);
-    indexReader.close();
-    indexReader = r2;
-
-    // now use the new reader - should recompute
-    totalCounts = TFC.getTotalCounts(indexReader, taxoReader, iParams);
-    prevGen = assertRecomputed(totalCounts, prevGen, "after updating the index - 7th attempt!");
-
-    // try again - should not recompute
-    assertTrue("Should be obtained from cache at 8th attempt",totalCounts == 
-      TFC.getTotalCounts(indexReader, taxoReader, iParams));
-    
-    IOUtils.close(indexReader, taxoReader);
-    outputFile.delete();
-    IOUtils.close(indexDir, taxoDir);
-  }
-
-  private int assertReadFromDisc(TotalFacetCounts totalCounts, int prevGen, String errMsg) {
-    assertEquals("should read from disk "+errMsg, CreationType.Loaded, totalCounts.createType4test);
-    int gen4test = totalCounts.gen4test;
-    assertTrue("should read from disk "+errMsg, gen4test > prevGen);
-    return gen4test;
-  }
-  
-  private int assertRecomputed(TotalFacetCounts totalCounts, int prevGen, String errMsg) {
-    assertEquals("should recompute "+errMsg, CreationType.Computed, totalCounts.createType4test);
-    int gen4test = totalCounts.gen4test;
-    assertTrue("should recompute "+errMsg, gen4test > prevGen);
-    return gen4test;
-  }
-
-  /**
-   * This test is to address a bug in a previous version.  If a TFC cache is
-   * written to disk, and then the taxonomy grows (but the index does not change),
-   * and then the TFC cache is re-read from disk, there will be an exception
-   * thrown, as the integers are read off of the disk according to taxonomy
-   * size, which has changed.
-   */
-  @Test
-  public void testGrowingTaxonomy() throws Exception {
-    Directory indexDir = newDirectory();
-    Directory taxoDir = newDirectory();
-    
-    // Create our index/taxonomy writers
-    IndexWriter indexWriter = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, null));
-    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
-    FacetIndexingParams iParams = new FacetIndexingParams() {
-      @Override
-      public int getPartitionSize() {
-        return 2;
-      }
-    };
-    // Add a facet to the index
-    addFacets(iParams, indexWriter, taxoWriter, "a", "b");
-    // Commit Changes
-    indexWriter.commit();
-    taxoWriter.commit();
-
-    DirectoryReader indexReader = DirectoryReader.open(indexDir);
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoDir);
-
-    // Create TFC and write cache to disk
-    File outputFile = _TestUtil.createTempFile("test", "tmp", TEMP_DIR);
-    TFC.store(outputFile, indexReader, taxoReader, iParams);
-    
-    // Make the taxonomy grow without touching the index
-    for (int i = 0; i < 10; i++) {
-      taxoWriter.addCategory(new CategoryPath("foo", Integer.toString(i)));
-    }
-    taxoWriter.commit();
-    TaxonomyReader newTaxoReader = TaxonomyReader.openIfChanged(taxoReader);
-    assertNotNull(newTaxoReader);
-    taxoReader.close();
-    taxoReader = newTaxoReader;
-
-    initCache();
-
-    // With the bug, this next call should result in an exception
-    TFC.load(outputFile, indexReader, taxoReader, iParams);
-    TotalFacetCounts totalCounts = TFC.getTotalCounts(indexReader, taxoReader, iParams);
-    assertReadFromDisc(totalCounts, 0, "after reading from disk.");
-    
-    outputFile.delete();
-    IOUtils.close(indexWriter, taxoWriter, indexReader, taxoReader);
-    IOUtils.close(indexDir, taxoDir);
-  }
-
-  /**
-   * Test that a new TFC is only calculated and placed in memory (by two
-   * threads who want it at the same time) only once.
-   */
-  @Test
-  public void testMemoryCacheSynchronization() throws Exception {
-    SlowRAMDirectory indexDir = new SlowRAMDirectory(-1, null);
-    SlowRAMDirectory taxoDir = new SlowRAMDirectory(-1, null);
-
-    // Write index using 'normal' directories
-    IndexWriter w = new IndexWriter(indexDir, new IndexWriterConfig(
-        TEST_VERSION_CURRENT, new MockAnalyzer(random(), MockTokenizer.WHITESPACE, false)));
-    DirectoryTaxonomyWriter tw = new DirectoryTaxonomyWriter(taxoDir);
-    FacetIndexingParams iParams = FacetIndexingParams.ALL_PARENTS;
-    // Add documents and facets
-    for (int i = 0; i < 1000; i++) {
-      addFacets(iParams, w, tw, "facet", Integer.toString(i));
-    }
-    w.close();
-    tw.close();
-
-    indexDir.setSleepMillis(1);
-    taxoDir.setSleepMillis(1);
-
-    IndexReader r = DirectoryReader.open(indexDir);
-    DirectoryTaxonomyReader tr = new DirectoryTaxonomyReader(taxoDir);
-
-    // Create and start threads. Thread1 should lock the cache and calculate
-    // the TFC array. The second thread should block until the first is
-    // done, then successfully retrieve from the cache without recalculating
-    // or reading from disk.
-    TFCThread tfcCalc1 = new TFCThread(r, tr, iParams);
-    TFCThread tfcCalc2 = new TFCThread(r, tr, iParams);
-    tfcCalc1.start();
-    // Give thread 1 a head start to ensure correct sequencing for testing
-    Thread.sleep(5);
-    tfcCalc2.start();
-
-    tfcCalc1.join();
-    tfcCalc2.join();
-
-    // Since this test ends up with references to the same TFC object, we
-    // can only test the times to make sure that they are the same.
-    assertRecomputed(tfcCalc1.tfc, 0, "thread 1 should recompute");
-    assertRecomputed(tfcCalc2.tfc, 0, "thread 2 should recompute");
-    assertTrue("Both results should be the same (as their inputs are the same objects)",
-        tfcCalc1.tfc == tfcCalc2.tfc);
-
-    r.close();
-    tr.close();
-  }
-
-  /**
-   * Simple test to make sure the TotalFacetCountsManager updates the
-   * TotalFacetCounts array only when it is supposed to, and whether it
-   * is recomputed or read from disk, but this time with TWO different
-   * TotalFacetCounts
-   */
-  @Test
-  public void testMultipleIndices() throws IOException {
-    Directory indexDir1 = newDirectory(), indexDir2 = newDirectory();
-    Directory taxoDir1 = newDirectory(), taxoDir2 = newDirectory();
-    
-    // Create our index/taxonomy writers
-    IndexWriter indexWriter1 = new IndexWriter(indexDir1, newIndexWriterConfig(TEST_VERSION_CURRENT, null));
-    IndexWriter indexWriter2 = new IndexWriter(indexDir2, newIndexWriterConfig(TEST_VERSION_CURRENT, null));
-    TaxonomyWriter taxoWriter1 = new DirectoryTaxonomyWriter(taxoDir1);
-    TaxonomyWriter taxoWriter2 = new DirectoryTaxonomyWriter(taxoDir2);
-    FacetIndexingParams iParams = FacetIndexingParams.ALL_PARENTS;
-
-    // Add a facet to the index
-    addFacets(iParams, indexWriter1, taxoWriter1, "a", "b");
-    addFacets(iParams, indexWriter1, taxoWriter1, "d", "e");
-    // Commit Changes
-    indexWriter1.commit();
-    indexWriter2.commit();
-    taxoWriter1.commit();
-    taxoWriter2.commit();
-
-    // Open two readers
-    DirectoryReader indexReader1 = DirectoryReader.open(indexDir1);
-    DirectoryReader indexReader2 = DirectoryReader.open(indexDir2);
-    TaxonomyReader taxoReader1 = new DirectoryTaxonomyReader(taxoDir1);
-    TaxonomyReader taxoReader2 = new DirectoryTaxonomyReader(taxoDir2);
-
-    // As this is the first time we have invoked the TotalFacetCountsManager, we
-    // should expect to compute.
-    TotalFacetCounts totalCounts0 = TFC.getTotalCounts(indexReader1, taxoReader1, iParams);
-    int prevGen = -1;
-    prevGen = assertRecomputed(totalCounts0, prevGen, "after attempt 1");
-    assertTrue("attempt 1b for same input [0] shout find it in cache",
-        totalCounts0 == TFC.getTotalCounts(indexReader1, taxoReader1, iParams));
-    
-    // 2nd Reader - As this is the first time we have invoked the
-    // TotalFacetCountsManager, we should expect a state of NEW to be returned.
-    TotalFacetCounts totalCounts1 = TFC.getTotalCounts(indexReader2, taxoReader2, iParams);
-    prevGen = assertRecomputed(totalCounts1, prevGen, "after attempt 2");
-    assertTrue("attempt 2b for same input [1] shout find it in cache",
-        totalCounts1 == TFC.getTotalCounts(indexReader2, taxoReader2, iParams));
-
-    // Right now cache size is one, so first TFC is gone and should be recomputed  
-    totalCounts0 = TFC.getTotalCounts(indexReader1, taxoReader1, iParams);
-    prevGen = assertRecomputed(totalCounts0, prevGen, "after attempt 3");
-    
-    // Similarly will recompute the second result  
-    totalCounts1 = TFC.getTotalCounts(indexReader2, taxoReader2, iParams);
-    prevGen = assertRecomputed(totalCounts1, prevGen, "after attempt 4");
-
-    // Now we set the cache size to two, meaning both should exist in the
-    // cache simultaneously
-    TFC.setCacheSize(2);
-
-    // Re-compute totalCounts0 (was evicted from the cache when the cache was smaller)
-    totalCounts0 = TFC.getTotalCounts(indexReader1, taxoReader1, iParams);
-    prevGen = assertRecomputed(totalCounts0, prevGen, "after attempt 5");
-
-    // now both are in the larger cache and should not be recomputed 
-    totalCounts1 = TFC.getTotalCounts(indexReader2, taxoReader2, iParams);
-    assertTrue("with cache of size 2 res no. 0 should come from cache",
-        totalCounts0 == TFC.getTotalCounts(indexReader1, taxoReader1, iParams));
-    assertTrue("with cache of size 2 res no. 1 should come from cache",
-        totalCounts1 == TFC.getTotalCounts(indexReader2, taxoReader2, iParams));
-    
-    IOUtils.close(indexWriter1, indexWriter2, taxoWriter1, taxoWriter2);
-    IOUtils.close(indexReader1, indexReader2, taxoReader1, taxoReader2);
-    IOUtils.close(indexDir1, indexDir2, taxoDir1, taxoDir2);
-  }
-
-}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/search/associations/AssociationsFacetRequestTest.java b/lucene/facet/src/test/org/apache/lucene/facet/search/associations/AssociationsFacetRequestTest.java
deleted file mode 100644
index 7bb650b..0000000
--- a/lucene/facet/src/test/org/apache/lucene/facet/search/associations/AssociationsFacetRequestTest.java
+++ /dev/null
@@ -1,181 +0,0 @@
-package org.apache.lucene.facet.search.associations;
-
-import java.util.List;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.analysis.MockTokenizer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.associations.AssociationsFacetFields;
-import org.apache.lucene.facet.associations.CategoryAssociationsContainer;
-import org.apache.lucene.facet.associations.CategoryFloatAssociation;
-import org.apache.lucene.facet.associations.CategoryIntAssociation;
-import org.apache.lucene.facet.search.FacetsCollector;
-import org.apache.lucene.facet.search.params.FacetSearchParams;
-import org.apache.lucene.facet.search.params.associations.AssociationFloatSumFacetRequest;
-import org.apache.lucene.facet.search.params.associations.AssociationIntSumFacetRequest;
-import org.apache.lucene.facet.search.results.FacetResult;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.store.Directory;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/** Test for associations */
-public class AssociationsFacetRequestTest extends FacetTestCase {
-
-  private static Directory dir;
-  private static IndexReader reader;
-  private static Directory taxoDir;
-  
-  private static final CategoryPath aint = new CategoryPath("int", "a");
-  private static final CategoryPath bint = new CategoryPath("int", "b");
-  private static final CategoryPath afloat = new CategoryPath("float", "a");
-  private static final CategoryPath bfloat = new CategoryPath("float", "b");
-  
-  @BeforeClass
-  public static void beforeClassAssociationsFacetRequestTest() throws Exception {
-    dir = newDirectory();
-    taxoDir = newDirectory();
-    // preparations - index, taxonomy, content
-    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, newIndexWriterConfig(TEST_VERSION_CURRENT, 
-        new MockAnalyzer(random(), MockTokenizer.KEYWORD, false)));
-    
-    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
-    
-    AssociationsFacetFields assocFacetFields = new AssociationsFacetFields(taxoWriter);
-    
-    // index documents, 50% have only 'b' and all have 'a'
-    for (int i = 0; i < 100; i++) {
-      Document doc = new Document();
-      CategoryAssociationsContainer associations = new CategoryAssociationsContainer();
-      associations.setAssociation(aint, new CategoryIntAssociation(2));
-      associations.setAssociation(afloat, new CategoryFloatAssociation(0.5f));
-      if (i % 2 == 0) { // 50
-        associations.setAssociation(bint, new CategoryIntAssociation(3));
-        associations.setAssociation(bfloat, new CategoryFloatAssociation(0.2f));
-      }
-      assocFacetFields.addFields(doc, associations);
-      writer.addDocument(doc);
-    }
-    
-    taxoWriter.close();
-    reader = writer.getReader();
-    writer.close();
-  }
-  
-  @AfterClass
-  public static void afterClassAssociationsFacetRequestTest() throws Exception {
-    reader.close();
-    reader = null;
-    dir.close();
-    dir = null;
-    taxoDir.close();
-    taxoDir = null;
-  }
-  
-  @Test
-  public void testIntSumAssociation() throws Exception {
-    DirectoryTaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);
-
-    // facet requests for two facets
-    FacetSearchParams fsp = new FacetSearchParams(
-        new AssociationIntSumFacetRequest(aint, 10),
-        new AssociationIntSumFacetRequest(bint, 10));
-    
-    Query q = new MatchAllDocsQuery();
-
-    FacetsCollector fc = FacetsCollector.create(fsp, reader, taxo);
-    
-    IndexSearcher searcher = newSearcher(reader);
-    searcher.search(q, fc);
-    List<FacetResult> res = fc.getFacetResults();
-    
-    assertNotNull("No results!",res);
-    assertEquals("Wrong number of results!",2, res.size());
-    assertEquals("Wrong count for category 'a'!", 200, (int) res.get(0).getFacetResultNode().value);
-    assertEquals("Wrong count for category 'b'!", 150, (int) res.get(1).getFacetResultNode().value);
-    
-    taxo.close();
-  }
-  
-  @Test
-  public void testFloatSumAssociation() throws Exception {
-    DirectoryTaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);
-
-    // facet requests for two facets
-    FacetSearchParams fsp = new FacetSearchParams(
-        new AssociationFloatSumFacetRequest(afloat, 10),
-        new AssociationFloatSumFacetRequest(bfloat, 10));
-    
-    Query q = new MatchAllDocsQuery();
-
-    FacetsCollector fc = FacetsCollector.create(fsp, reader, taxo);
-    
-    IndexSearcher searcher = newSearcher(reader);
-    searcher.search(q, fc);
-    List<FacetResult> res = fc.getFacetResults();
-    
-    assertNotNull("No results!",res);
-    assertEquals("Wrong number of results!",2, res.size());
-    assertEquals("Wrong count for category 'a'!",50f, (float) res.get(0).getFacetResultNode().value, 0.00001);
-    assertEquals("Wrong count for category 'b'!",10f, (float) res.get(1).getFacetResultNode().value, 0.00001);
-    
-    taxo.close();
-  }  
-    
-  @Test
-  public void testDifferentAggregatorsSameCategoryList() throws Exception {
-    // Same category list cannot be aggregated by two different aggregators. If
-    // you want to do that, you need to separate the categories into two
-    // category list (you'll still have one association list).
-    DirectoryTaxonomyReader taxo = new DirectoryTaxonomyReader(taxoDir);
-
-    // facet requests for two facets
-    FacetSearchParams fsp = new FacetSearchParams(
-        new AssociationIntSumFacetRequest(aint, 10),
-        new AssociationIntSumFacetRequest(bint, 10),
-        new AssociationFloatSumFacetRequest(afloat, 10),
-        new AssociationFloatSumFacetRequest(bfloat, 10));
-    
-    Query q = new MatchAllDocsQuery();
-
-    FacetsCollector fc = FacetsCollector.create(fsp, reader, taxo);
-    
-    IndexSearcher searcher = newSearcher(reader);
-    searcher.search(q, fc);
-    try {
-      fc.getFacetResults();
-      fail("different aggregators for same category list should not be supported");
-    } catch (RuntimeException e) {
-      // ok - expected
-    }
-    taxo.close();
-  }  
-
-}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/search/params/FacetRequestTest.java b/lucene/facet/src/test/org/apache/lucene/facet/search/params/FacetRequestTest.java
deleted file mode 100644
index 46dabe4..0000000
--- a/lucene/facet/src/test/org/apache/lucene/facet/search/params/FacetRequestTest.java
+++ /dev/null
@@ -1,46 +0,0 @@
-package org.apache.lucene.facet.search.params;
-
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class FacetRequestTest extends FacetTestCase {
-
-  @Test(expected=IllegalArgumentException.class)
-  public void testIllegalNumResults() throws Exception {
-    assertNotNull(new CountFacetRequest(new CategoryPath("a", "b"), 0));
-  }
-  
-  @Test(expected=IllegalArgumentException.class)
-  public void testIllegalCategoryPath() throws Exception {
-    assertNotNull(new CountFacetRequest(null, 1));
-  }
-
-  @Test
-  public void testHashAndEquals() {
-    CountFacetRequest fr1 = new CountFacetRequest(new CategoryPath("a"), 8);
-    CountFacetRequest fr2 = new CountFacetRequest(new CategoryPath("a"), 8);
-    assertEquals("hashCode() should agree on both objects", fr1.hashCode(), fr2.hashCode());
-    assertTrue("equals() should return true", fr1.equals(fr2));
-    fr1.setDepth(10);
-    assertFalse("equals() should return false as fr1.depth != fr2.depth", fr1.equals(fr2));
-  }
-  
-}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/search/params/FacetSearchParamsTest.java b/lucene/facet/src/test/org/apache/lucene/facet/search/params/FacetSearchParamsTest.java
deleted file mode 100644
index e75d176..0000000
--- a/lucene/facet/src/test/org/apache/lucene/facet/search/params/FacetSearchParamsTest.java
+++ /dev/null
@@ -1,33 +0,0 @@
-package org.apache.lucene.facet.search.params;
-
-import org.apache.lucene.facet.FacetTestCase;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class FacetSearchParamsTest extends FacetTestCase {
-
-  @Test
-  public void testSearchParamsWithNullRequest() throws Exception {
-    try {
-      assertNull(new FacetSearchParams());
-      fail("FacetSearchParams should throw IllegalArgumentException when not adding requests");
-    } catch (IllegalArgumentException e) {
-    }
-  }
-}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/search/params/MultiCategoryListIteratorTest.java b/lucene/facet/src/test/org/apache/lucene/facet/search/params/MultiCategoryListIteratorTest.java
deleted file mode 100644
index 861e607..0000000
--- a/lucene/facet/src/test/org/apache/lucene/facet/search/params/MultiCategoryListIteratorTest.java
+++ /dev/null
@@ -1,118 +0,0 @@
-package org.apache.lucene.facet.search.params;
-
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.Random;
-
-import org.apache.lucene.document.Document;
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.index.FacetFields;
-import org.apache.lucene.facet.index.params.CategoryListParams;
-import org.apache.lucene.facet.index.params.PerDimensionIndexingParams;
-import org.apache.lucene.facet.search.CategoryListIterator;
-import org.apache.lucene.facet.search.DocValuesCategoryListIterator;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
-import org.apache.lucene.facet.util.MultiCategoryListIterator;
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.IntsRef;
-import org.apache.lucene.util.encoding.IntDecoder;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class MultiCategoryListIteratorTest extends FacetTestCase {
-
-  @Test
-  public void testMultipleCategoryLists() throws Exception {
-    Random random = random();
-    int numDimensions = atLeast(random, 2); // at least 2 dimensions
-    String[] dimensions = new String[numDimensions];
-    for (int i = 0; i < numDimensions; i++) {
-      dimensions[i] = "dim" + i;
-    }
-    
-    // build the PerDimensionIndexingParams
-    HashMap<CategoryPath,CategoryListParams> clps = new HashMap<CategoryPath,CategoryListParams>();
-    for (String dim : dimensions) {
-      CategoryPath cp = new CategoryPath(dim);
-      CategoryListParams clp = randomCategoryListParams("$" + dim);
-      clps.put(cp, clp);
-    }
-    PerDimensionIndexingParams indexingParams = new PerDimensionIndexingParams(clps);
-    
-    // index some documents
-    Directory indexDir = newDirectory();
-    Directory taxoDir = newDirectory();
-    IndexWriter indexWriter = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, null).setMaxBufferedDocs(2));
-    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
-    FacetFields facetFields = new FacetFields(taxoWriter, indexingParams);
-    int ndocs = atLeast(random, 10);
-    for (int i = 0; i < ndocs; i++) {
-      Document doc = new Document();
-      int numCategories = random.nextInt(numDimensions) + 1;
-      ArrayList<CategoryPath> categories = new ArrayList<CategoryPath>();
-      for (int j = 0; j < numCategories; j++) {
-        String dimension = dimensions[random.nextInt(dimensions.length)];
-        categories.add(new CategoryPath(dimension, Integer.toString(i)));
-      }
-      facetFields.addFields(doc, categories);
-      indexWriter.addDocument(doc);
-    }
-    IOUtils.close(indexWriter, taxoWriter);
-    
-    // test the multi iterator
-    DirectoryReader indexReader = DirectoryReader.open(indexDir);
-    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoDir);
-    CategoryListIterator[] iterators = new CategoryListIterator[numDimensions];
-    for (int i = 0; i < iterators.length; i++) {
-      CategoryListParams clp = indexingParams.getCategoryListParams(new CategoryPath(dimensions[i]));
-      IntDecoder decoder = clp.createEncoder().createMatchingDecoder();
-      iterators[i] = new DocValuesCategoryListIterator(clp.field, decoder);
-    }
-    MultiCategoryListIterator cli = new MultiCategoryListIterator(iterators);
-    for (AtomicReaderContext context : indexReader.leaves()) {
-      assertTrue("failed to init multi-iterator", cli.setNextReader(context));
-      IntsRef ordinals = new IntsRef();
-      final int maxDoc = context.reader().maxDoc();
-      for (int i = 0; i < maxDoc; i++) {
-        cli.getOrdinals(i, ordinals);
-        assertTrue("document " + i + " does not have categories", ordinals.length > 0);
-        for (int j = 0; j < ordinals.length; j++) {
-          CategoryPath cp = taxoReader.getPath(ordinals.ints[j]);
-          assertNotNull("ordinal " + ordinals.ints[j] + " not found in taxonomy", cp);
-          if (cp.length == 2) {
-            int globalDoc = i + context.docBase;
-            assertEquals("invalid category for document " + globalDoc, globalDoc, Integer.parseInt(cp.components[1]));
-          }
-        }
-      }
-    }
-    
-    IOUtils.close(indexReader, taxoReader);
-    IOUtils.close(indexDir, taxoDir);
-  }
-  
-}
\ No newline at end of file
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/search/sampling/BaseSampleTestTopK.java b/lucene/facet/src/test/org/apache/lucene/facet/search/sampling/BaseSampleTestTopK.java
deleted file mode 100644
index d10ce54..0000000
--- a/lucene/facet/src/test/org/apache/lucene/facet/search/sampling/BaseSampleTestTopK.java
+++ /dev/null
@@ -1,143 +0,0 @@
-package org.apache.lucene.facet.search.sampling;
-
-import java.util.List;
-import java.util.Random;
-
-import org.apache.lucene.facet.index.params.FacetIndexingParams;
-import org.apache.lucene.facet.search.BaseTestTopK;
-import org.apache.lucene.facet.search.FacetsCollector;
-import org.apache.lucene.facet.search.StandardFacetsAccumulator;
-import org.apache.lucene.facet.search.params.FacetRequest;
-import org.apache.lucene.facet.search.params.FacetRequest.ResultMode;
-import org.apache.lucene.facet.search.params.FacetSearchParams;
-import org.apache.lucene.facet.search.results.FacetResult;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TermQuery;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public abstract class BaseSampleTestTopK extends BaseTestTopK {
-  
-  /** Number of top results */
-  protected static final int K = 2; 
-  
-  /** since there is a chance that this test would fail even if the code is correct, retry the sampling */
-  protected static final int RETRIES = 10;
-  
-  @Override
-  protected FacetSearchParams searchParamsWithRequests(int numResults, FacetIndexingParams fip) {
-    FacetSearchParams res = super.searchParamsWithRequests(numResults, fip);
-    for (FacetRequest req : res.facetRequests) {
-      // randomize the way we aggregate results
-      if (random().nextBoolean()) {
-        req.setResultMode(ResultMode.GLOBAL_FLAT);
-      } else {
-        req.setResultMode(ResultMode.PER_NODE_IN_TREE);
-      }
-    }
-    return res;
-  }
-  
-  protected abstract StandardFacetsAccumulator getSamplingAccumulator(Sampler sampler, TaxonomyReader taxoReader, 
-      IndexReader indexReader, FacetSearchParams searchParams);
-  
-  /**
-   * Try out faceted search with sampling enabled and complements either disabled or enforced
-   * Lots of randomly generated data is being indexed, and later on a "90% docs" faceted search
-   * is performed. The results are compared to non-sampled ones.
-   */
-  public void testCountUsingSampling() throws Exception {
-    boolean useRandomSampler = random().nextBoolean();
-    for (int partitionSize : partitionSizes) {
-      try {
-        // complements return counts for all ordinals, so force ALL_PARENTS indexing
-        // so that it's easier to compare
-        FacetIndexingParams fip = getFacetIndexingParams(partitionSize, true);
-        initIndex(fip);
-        // Get all of the documents and run the query, then do different
-        // facet counts and compare to control
-        Query q = new TermQuery(new Term(CONTENT_FIELD, BETA)); // 90% of the docs
-        
-        FacetSearchParams expectedSearchParams = searchParamsWithRequests(K, fip); 
-        FacetsCollector fc = FacetsCollector.create(expectedSearchParams, indexReader, taxoReader);
-        
-        searcher.search(q, fc);
-        
-        List<FacetResult> expectedResults = fc.getFacetResults();
-        
-        FacetSearchParams samplingSearchParams = searchParamsWithRequests(K, fip); 
-        
-        // try several times in case of failure, because the test has a chance to fail 
-        // if the top K facets are not sufficiently common with the sample set
-        for (int nTrial = 0; nTrial < RETRIES; nTrial++) {
-          try {
-            // complement with sampling!
-            final Sampler sampler = createSampler(nTrial, useRandomSampler);
-            
-            assertSampling(expectedResults, q, sampler, samplingSearchParams, false);
-            assertSampling(expectedResults, q, sampler, samplingSearchParams, true);
-            
-            break; // succeeded
-          } catch (AssertionError e) {
-            if (nTrial >= RETRIES - 1) {
-              throw e; // no more retries allowed, must fail
-            }
-          }
-        }
-      } finally { 
-        closeAll();
-      }
-    }
-  }
-  
-  private void assertSampling(List<FacetResult> expected, Query q, Sampler sampler, FacetSearchParams params, boolean complement) throws Exception {
-    FacetsCollector samplingFC = samplingCollector(complement, sampler, params);
-    
-    searcher.search(q, samplingFC);
-    List<FacetResult> sampledResults = samplingFC.getFacetResults();
-    
-    assertSameResults(expected, sampledResults);
-  }
-  
-  private FacetsCollector samplingCollector(final boolean complement, final Sampler sampler,
-      FacetSearchParams samplingSearchParams) {
-    StandardFacetsAccumulator sfa = getSamplingAccumulator(sampler, taxoReader, indexReader, samplingSearchParams);
-    sfa.setComplementThreshold(complement ? StandardFacetsAccumulator.FORCE_COMPLEMENT : StandardFacetsAccumulator.DISABLE_COMPLEMENT);
-    return FacetsCollector.create(sfa);
-  }
-  
-  private Sampler createSampler(int nTrial, boolean useRandomSampler) {
-    SamplingParams samplingParams = new SamplingParams();
-    
-    final double retryFactor = Math.pow(1.01, nTrial);
-    samplingParams.setSampleRatio(0.8 * retryFactor);
-    samplingParams.setMinSampleSize((int) (100 * retryFactor));
-    samplingParams.setMaxSampleSize((int) (10000 * retryFactor));
-    samplingParams.setOversampleFactor(5.0 * retryFactor);
-    samplingParams.setSamplingThreshold(11000); //force sampling
-
-    Sampler sampler = useRandomSampler ? 
-        new RandomSampler(samplingParams, new Random(random().nextLong())) :
-          new RepeatableSampler(samplingParams);
-    return sampler;
-  }
-  
-}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/search/sampling/OversampleWithDepthTest.java b/lucene/facet/src/test/org/apache/lucene/facet/search/sampling/OversampleWithDepthTest.java
deleted file mode 100644
index 9f51a0d..0000000
--- a/lucene/facet/src/test/org/apache/lucene/facet/search/sampling/OversampleWithDepthTest.java
+++ /dev/null
@@ -1,124 +0,0 @@
-package org.apache.lucene.facet.search.sampling;
-
-import java.io.IOException;
-import java.util.Collections;
-
-import org.apache.lucene.document.Document;
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.index.FacetFields;
-import org.apache.lucene.facet.index.params.FacetIndexingParams;
-import org.apache.lucene.facet.search.FacetsCollector;
-import org.apache.lucene.facet.search.StandardFacetsAccumulator;
-import org.apache.lucene.facet.search.params.CountFacetRequest;
-import org.apache.lucene.facet.search.params.FacetRequest;
-import org.apache.lucene.facet.search.params.FacetRequest.ResultMode;
-import org.apache.lucene.facet.search.params.FacetSearchParams;
-import org.apache.lucene.facet.search.results.FacetResult;
-import org.apache.lucene.facet.search.results.FacetResultNode;
-import org.apache.lucene.facet.taxonomy.CategoryPath;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.IOUtils;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class OversampleWithDepthTest extends FacetTestCase {
-  
-  @Test
-  public void testCountWithdepthUsingSampling() throws Exception, IOException {
-    Directory indexDir = newDirectory();
-    Directory taxoDir = newDirectory();
-    
-    FacetIndexingParams fip = new FacetIndexingParams(randomCategoryListParams());
-    
-    // index 100 docs, each with one category: ["root", docnum/10, docnum]
-    // e.g. root/8/87
-    index100Docs(indexDir, taxoDir, fip);
-    
-    DirectoryReader r = DirectoryReader.open(indexDir);
-    TaxonomyReader tr = new DirectoryTaxonomyReader(taxoDir);
-    
-    CountFacetRequest facetRequest = new CountFacetRequest(new CategoryPath("root"), 10);
-    // Setting the depth to '2', should potentially get all categories
-    facetRequest.setDepth(2);
-    facetRequest.setResultMode(ResultMode.PER_NODE_IN_TREE);
-
-    FacetSearchParams fsp = new FacetSearchParams(fip, facetRequest);
-    
-    // Craft sampling params to enforce sampling
-    final SamplingParams params = new SamplingParams();
-    params.setMinSampleSize(2);
-    params.setMaxSampleSize(50);
-    params.setOversampleFactor(5);
-    params.setSamplingThreshold(60);
-    params.setSampleRatio(0.1);
-    
-    FacetResult res = searchWithFacets(r, tr, fsp, params);
-    FacetRequest req = res.getFacetRequest();
-    assertEquals(facetRequest, req);
-    
-    FacetResultNode rootNode = res.getFacetResultNode();
-    
-    // Each node below root should also have sub-results as the requested depth was '2'
-    for (FacetResultNode node : rootNode.subResults) {
-      assertTrue("node " + node.label + " should have had children as the requested depth was '2'", node.subResults.size() > 0);
-    }
-    
-    IOUtils.close(r, tr, indexDir, taxoDir);
-  }
-
-  private void index100Docs(Directory indexDir, Directory taxoDir, FacetIndexingParams fip) throws IOException {
-    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, null);
-    IndexWriter w = new IndexWriter(indexDir, iwc);
-    TaxonomyWriter tw = new DirectoryTaxonomyWriter(taxoDir);
-    
-    FacetFields facetFields = new FacetFields(tw, fip);
-    for (int i = 0; i < 100; i++) {
-      Document doc = new Document();
-      CategoryPath cp = new CategoryPath("root",Integer.toString(i / 10), Integer.toString(i));
-      facetFields.addFields(doc, Collections.singletonList(cp));
-      w.addDocument(doc);
-    }
-    IOUtils.close(tw, w);
-  }
-
-  /** search reader <code>r</code>*/
-  private FacetResult searchWithFacets(IndexReader r, TaxonomyReader tr, FacetSearchParams fsp, 
-      final SamplingParams params) throws IOException {
-    // a FacetsCollector with a sampling accumulator
-    Sampler sampler = new RandomSampler(params, random());
-    StandardFacetsAccumulator sfa = new SamplingAccumulator(sampler, fsp, r, tr);
-    FacetsCollector fcWithSampling = FacetsCollector.create(sfa);
-    
-    IndexSearcher s = new IndexSearcher(r);
-    s.search(new MatchAllDocsQuery(), fcWithSampling);
-    
-    // there's only one expected result, return just it.
-    return fcWithSampling.getFacetResults().get(0);
-  }
-}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/search/sampling/SamplingAccumulatorTest.java b/lucene/facet/src/test/org/apache/lucene/facet/search/sampling/SamplingAccumulatorTest.java
deleted file mode 100644
index 621a292..0000000
--- a/lucene/facet/src/test/org/apache/lucene/facet/search/sampling/SamplingAccumulatorTest.java
+++ /dev/null
@@ -1,34 +0,0 @@
-package org.apache.lucene.facet.search.sampling;
-
-import org.apache.lucene.facet.search.StandardFacetsAccumulator;
-import org.apache.lucene.facet.search.params.FacetSearchParams;
-import org.apache.lucene.facet.taxonomy.TaxonomyReader;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.util.LuceneTestCase.Slow;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-@Slow
-public class SamplingAccumulatorTest extends BaseSampleTestTopK {
-
-  @Override
-  protected StandardFacetsAccumulator getSamplingAccumulator(Sampler sampler, TaxonomyReader taxoReader, 
-      IndexReader indexReader, FacetSearchParams searchParams) {
-    return new SamplingAccumulator(sampler, searchParams, indexReader, taxoReader);
-  }
-}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestTaxonomyCombined.java b/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestTaxonomyCombined.java
index cddee22..e7970d2 100644
--- a/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestTaxonomyCombined.java
+++ b/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestTaxonomyCombined.java
@@ -8,6 +8,7 @@ import java.util.Arrays;
 import java.util.concurrent.atomic.AtomicBoolean;
 
 import org.apache.lucene.facet.FacetTestCase;
+import org.apache.lucene.facet.SlowRAMDirectory;
 import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
 import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
 import org.apache.lucene.facet.taxonomy.directory.ParallelTaxonomyArrays;
@@ -15,7 +16,6 @@ import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.LockObtainFailedException;
 import org.apache.lucene.store.RAMDirectory;
 import org.apache.lucene.util.LuceneTestCase.SuppressCodecs;
-import org.apache.lucene.util.SlowRAMDirectory;
 import org.junit.Test;
 
 /*
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/util/OrdinalMappingReaderTest.java b/lucene/facet/src/test/org/apache/lucene/facet/util/OrdinalMappingReaderTest.java
new file mode 100644
index 0000000..7dea42b
--- /dev/null
+++ b/lucene/facet/src/test/org/apache/lucene/facet/util/OrdinalMappingReaderTest.java
@@ -0,0 +1,123 @@
+package org.apache.lucene.facet.util;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.analysis.MockTokenizer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.facet.FacetTestCase;
+import org.apache.lucene.facet.index.FacetFields;
+import org.apache.lucene.facet.params.FacetIndexingParams;
+import org.apache.lucene.facet.params.FacetSearchParams;
+import org.apache.lucene.facet.search.CountFacetRequest;
+import org.apache.lucene.facet.search.FacetResult;
+import org.apache.lucene.facet.search.FacetResultNode;
+import org.apache.lucene.facet.search.FacetsCollector;
+import org.apache.lucene.facet.taxonomy.CategoryPath;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter.MemoryOrdinalMap;
+import org.apache.lucene.facet.util.TaxonomyMergeUtils;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.IOUtils;
+import org.junit.Test;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+public class OrdinalMappingReaderTest extends FacetTestCase {
+  
+  private static final int NUM_DOCS = 100;
+  
+  @Test
+  public void testTaxonomyMergeUtils() throws Exception {
+    Directory dir = newDirectory();
+    Directory taxDir = newDirectory();
+    FacetIndexingParams fip = new FacetIndexingParams(randomCategoryListParams());
+    buildIndexWithFacets(dir, taxDir, true, fip);
+    
+    Directory dir1 = newDirectory();
+    Directory taxDir1 = newDirectory();
+    buildIndexWithFacets(dir1, taxDir1, false, fip);
+    
+    IndexWriter destIndexWriter = new IndexWriter(dir1, new IndexWriterConfig(TEST_VERSION_CURRENT, null));
+    DirectoryTaxonomyWriter destTaxWriter = new DirectoryTaxonomyWriter(taxDir1);
+    try {
+      TaxonomyMergeUtils.merge(dir, taxDir, new MemoryOrdinalMap(), destIndexWriter, destTaxWriter, fip);
+    } finally {
+      IOUtils.close(destIndexWriter, destTaxWriter);
+    }
+    
+    verifyResults(dir1, taxDir1, fip);
+    dir1.close();
+    taxDir1.close();
+    dir.close();
+    taxDir.close();
+  }
+
+  private void verifyResults(Directory dir, Directory taxDir, FacetIndexingParams fip) throws IOException {
+    DirectoryReader reader1 = DirectoryReader.open(dir);
+    DirectoryTaxonomyReader taxReader = new DirectoryTaxonomyReader(taxDir);
+    IndexSearcher searcher = newSearcher(reader1);
+    FacetSearchParams fsp = new FacetSearchParams(fip, new CountFacetRequest(new CategoryPath("tag"), NUM_DOCS));
+    FacetsCollector collector = FacetsCollector.create(fsp, reader1, taxReader);
+    searcher.search(new MatchAllDocsQuery(), collector);
+    FacetResult result = collector.getFacetResults().get(0);
+    FacetResultNode node = result.getFacetResultNode();
+    for (FacetResultNode facet: node.subResults) {
+      int weight = (int)facet.value;
+      int label = Integer.parseInt(facet.label.components[1]);
+      //System.out.println(label + ": " + weight);
+      if (VERBOSE) {
+        System.out.println(label + ": " + weight);
+      }
+      assertEquals(NUM_DOCS ,weight);
+    }
+    reader1.close();
+    taxReader.close();
+  }
+
+  private void buildIndexWithFacets(Directory dir, Directory taxDir, boolean asc, FacetIndexingParams fip) throws IOException {
+    IndexWriterConfig config = newIndexWriterConfig(TEST_VERSION_CURRENT, 
+        new MockAnalyzer(random(), MockTokenizer.WHITESPACE, false));
+    RandomIndexWriter writer = new RandomIndexWriter(random(), dir, config);
+    
+    DirectoryTaxonomyWriter taxonomyWriter = new DirectoryTaxonomyWriter(taxDir);
+    for (int i = 1; i <= NUM_DOCS; i++) {
+      Document doc = new Document();
+      List<CategoryPath> categoryPaths = new ArrayList<CategoryPath>(i + 1);
+      for (int j = i; j <= NUM_DOCS; j++) {
+        int facetValue = asc? j: NUM_DOCS - j;
+        categoryPaths.add(new CategoryPath("tag", Integer.toString(facetValue)));
+      }
+      FacetFields facetFields = new FacetFields(taxonomyWriter, fip);
+      facetFields.addFields(doc, categoryPaths);
+      writer.addDocument(doc);
+    }    
+    taxonomyWriter.close();
+    writer.close();
+  }  
+
+}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/util/TestFacetsPayloadMigrationReader.java b/lucene/facet/src/test/org/apache/lucene/facet/util/TestFacetsPayloadMigrationReader.java
new file mode 100644
index 0000000..71d3908
--- /dev/null
+++ b/lucene/facet/src/test/org/apache/lucene/facet/util/TestFacetsPayloadMigrationReader.java
@@ -0,0 +1,415 @@
+package org.apache.lucene.facet.util;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.Map.Entry;
+import java.util.Random;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.Field.Store;
+import org.apache.lucene.document.FieldType;
+import org.apache.lucene.document.StringField;
+import org.apache.lucene.document.TextField;
+import org.apache.lucene.facet.FacetTestCase;
+import org.apache.lucene.facet.index.FacetFields;
+import org.apache.lucene.facet.params.CategoryListParams;
+import org.apache.lucene.facet.params.FacetIndexingParams;
+import org.apache.lucene.facet.params.FacetSearchParams;
+import org.apache.lucene.facet.params.PerDimensionIndexingParams;
+import org.apache.lucene.facet.params.PerDimensionOrdinalPolicy;
+import org.apache.lucene.facet.params.CategoryListParams.OrdinalPolicy;
+import org.apache.lucene.facet.search.CategoryListIterator;
+import org.apache.lucene.facet.search.CountFacetRequest;
+import org.apache.lucene.facet.search.DrillDown;
+import org.apache.lucene.facet.search.FacetRequest;
+import org.apache.lucene.facet.search.FacetResult;
+import org.apache.lucene.facet.search.FacetResultNode;
+import org.apache.lucene.facet.search.FacetsCollector;
+import org.apache.lucene.facet.taxonomy.CategoryPath;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
+import org.apache.lucene.facet.util.FacetsPayloadMigrationReader;
+import org.apache.lucene.facet.util.PartitionsUtils;
+import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.MultiReader;
+import org.apache.lucene.index.NoMergePolicy;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.index.Terms;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.search.MultiCollector;
+import org.apache.lucene.search.PrefixQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.TotalHitCountCollector;
+import org.apache.lucene.search.BooleanClause.Occur;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.IntsRef;
+import org.junit.Test;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/** Tests facets index migration from payload to DocValues.*/
+public class TestFacetsPayloadMigrationReader extends FacetTestCase {
+  
+  private static class PayloadFacetFields extends FacetFields {
+
+    private static final class CountingListStream extends TokenStream {
+      private final PayloadAttribute payloadAtt = addAttribute(PayloadAttribute.class);
+      private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
+      private final Iterator<Entry<String,BytesRef>> categoriesData;
+      
+      CountingListStream(Map<String,BytesRef> categoriesData) {
+        this.categoriesData = categoriesData.entrySet().iterator();
+      }
+      
+      @Override
+      public boolean incrementToken() throws IOException {
+        if (!categoriesData.hasNext()) {
+          return false;
+        }
+        
+        Entry<String,BytesRef> entry = categoriesData.next();
+        termAtt.setEmpty().append(FacetsPayloadMigrationReader.PAYLOAD_TERM_TEXT + entry.getKey());
+        payloadAtt.setPayload(entry.getValue());
+        return true;
+      }
+      
+    }
+
+    private static final FieldType COUNTING_LIST_PAYLOAD_TYPE = new FieldType();
+    static {
+      COUNTING_LIST_PAYLOAD_TYPE.setIndexed(true);
+      COUNTING_LIST_PAYLOAD_TYPE.setTokenized(true);
+      COUNTING_LIST_PAYLOAD_TYPE.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);
+      COUNTING_LIST_PAYLOAD_TYPE.setStored(false);
+      COUNTING_LIST_PAYLOAD_TYPE.setOmitNorms(true);
+      COUNTING_LIST_PAYLOAD_TYPE.freeze();
+    }
+    
+    public PayloadFacetFields(TaxonomyWriter taxonomyWriter, FacetIndexingParams params) {
+      super(taxonomyWriter, params);
+    }
+
+    @Override
+    protected FieldType drillDownFieldType() {
+      // Since the payload is indexed in the same field as the drill-down terms,
+      // we must set IndexOptions to DOCS_AND_FREQS_AND_POSITIONS
+      final FieldType type = new FieldType(TextField.TYPE_NOT_STORED);
+      type.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);
+      type.freeze();
+      return type;
+    }
+
+    @Override
+    protected void addCountingListData(Document doc, Map<String,BytesRef> categoriesData, String field) {
+      CountingListStream ts = new CountingListStream(categoriesData);
+      doc.add(new Field(field, ts, COUNTING_LIST_PAYLOAD_TYPE));
+    }
+  }
+
+  private static final String[] DIMENSIONS = new String[] { "dim1", "dim2", "dim3.1", "dim3.2" };
+  
+  private HashMap<String,Integer> createIndex(Directory indexDir, Directory taxoDir, FacetIndexingParams fip) 
+      throws Exception {
+    Random random = random();
+    IndexWriterConfig conf = new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random));
+    conf.setMaxBufferedDocs(2); // force few segments
+    conf.setMergePolicy(NoMergePolicy.COMPOUND_FILES); // avoid merges so that we're left with few segments
+    IndexWriter indexWriter = new IndexWriter(indexDir, conf);
+    TaxonomyWriter taxoWriter = new DirectoryTaxonomyWriter(taxoDir);
+    
+    FacetFields facetFields = new PayloadFacetFields(taxoWriter, fip);
+    
+    HashMap<String,Integer> expectedCounts = new HashMap<String,Integer>(DIMENSIONS.length);
+    int numDocs = atLeast(10);
+    for (int i = 0; i < numDocs; i++) {
+      Document doc = new Document();
+      int numCategories = random.nextInt(3) + 1;
+      ArrayList<CategoryPath> categories = new ArrayList<CategoryPath>(numCategories);
+      HashSet<String> docDimensions = new HashSet<String>();
+      while (numCategories-- > 0) {
+        String dim = DIMENSIONS[random.nextInt(DIMENSIONS.length)];
+        // we should only increment the expected count by 1 per document
+        docDimensions.add(dim);
+        categories.add(new CategoryPath(dim, Integer.toString(i), Integer.toString(numCategories)));
+      }
+      facetFields.addFields(doc, categories);
+      doc.add(new StringField("docid", Integer.toString(i), Store.YES));
+      doc.add(new TextField("foo", "content" + i, Store.YES));
+      indexWriter.addDocument(doc);
+
+      // update expected count per dimension
+      for (String dim : docDimensions) {
+        Integer val = expectedCounts.get(dim);
+        if (val == null) {
+          expectedCounts.put(dim, Integer.valueOf(1));
+        } else {
+          expectedCounts.put(dim, Integer.valueOf(val.intValue() + 1));
+        }
+      }
+      
+      if (random.nextDouble() < 0.2) { // add some documents that will be deleted
+        doc = new Document();
+        doc.add(new StringField("del", "key", Store.NO));
+        facetFields.addFields(doc, Collections.singletonList(new CategoryPath("dummy")));
+        indexWriter.addDocument(doc);
+      }
+    }
+    
+    indexWriter.commit();
+    taxoWriter.commit();
+
+    // delete the docs that were marked for deletion. note that the 'dummy'
+    // category is not removed from the taxonomy, so must account for it when we
+    // verify the migrated index.
+    indexWriter.deleteDocuments(new Term("del", "key"));
+    indexWriter.commit();
+    
+    IOUtils.close(indexWriter, taxoWriter);
+    
+    return expectedCounts;
+  }
+  
+  private void migrateIndex(Directory indexDir, FacetIndexingParams fip) throws Exception {
+    final Map<String,Term> fieldTerms = FacetsPayloadMigrationReader.buildFieldTermsMap(indexDir, fip);
+    DirectoryReader reader = DirectoryReader.open(indexDir);
+    List<AtomicReaderContext> leaves = reader.leaves();
+    int numReaders = leaves.size();
+    AtomicReader wrappedLeaves[] = new AtomicReader[numReaders];
+    for (int i = 0; i < numReaders; i++) {
+      wrappedLeaves[i] = new FacetsPayloadMigrationReader(leaves.get(i).reader(), fieldTerms);
+    }
+    
+    IndexWriter writer = new IndexWriter(indexDir, newIndexWriterConfig(TEST_VERSION_CURRENT, null));
+    writer.deleteAll();
+    try {
+      writer.addIndexes(new MultiReader(wrappedLeaves));
+      writer.commit();
+    } finally {
+      reader.close();
+      writer.close();
+    }
+  }
+  
+  private void verifyMigratedIndex(Directory indexDir, Directory taxoDir, HashMap<String,Integer> expectedCounts, 
+      FacetIndexingParams fip) throws Exception {
+    DirectoryReader indexReader = DirectoryReader.open(indexDir);
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoDir);
+    IndexSearcher searcher = new IndexSearcher(indexReader);
+
+    assertFalse("index should not have deletions", indexReader.hasDeletions());
+    
+    verifyNotFacetsData(indexReader, searcher);
+    verifyFacetedSearch(expectedCounts, fip, indexReader, taxoReader, searcher);
+    verifyDrillDown(expectedCounts, fip, indexReader, taxoReader, searcher);
+    verifyIndexOrdinals(indexReader, taxoReader, fip);
+    
+    IOUtils.close(indexReader, taxoReader);
+  }
+  
+  private void verifyNotFacetsData(DirectoryReader indexReader, IndexSearcher searcher) throws IOException {
+    // verify that non facets data was not damaged
+    TotalHitCountCollector total = new TotalHitCountCollector();
+    searcher.search(new PrefixQuery(new Term("foo", "content")), total);
+    assertEquals("invalid number of results for content query", total.getTotalHits(), indexReader.maxDoc());
+    
+    int numDocIDs = 0;
+    for (AtomicReaderContext context : indexReader.leaves()) {
+      Terms docIDs = context.reader().terms("docid");
+      assertNotNull(docIDs);
+      TermsEnum te = docIDs.iterator(null);
+      while (te.next() != null) {
+        ++numDocIDs;
+      }
+    }
+    assertEquals("invalid number of docid terms", indexReader.maxDoc(), numDocIDs);
+  }
+  
+  private void verifyFacetedSearch(Map<String,Integer> expectedCounts, FacetIndexingParams fip, 
+      DirectoryReader indexReader, TaxonomyReader taxoReader, IndexSearcher searcher) throws IOException {
+    // run faceted search and assert expected counts
+    ArrayList<FacetRequest> requests = new ArrayList<FacetRequest>(expectedCounts.size());
+    for (String dim : expectedCounts.keySet()) {
+      requests.add(new CountFacetRequest(new CategoryPath(dim), 5));
+    }
+    FacetSearchParams fsp = new FacetSearchParams(fip, requests);
+    FacetsCollector fc = FacetsCollector.create(fsp, indexReader, taxoReader);
+    MatchAllDocsQuery base = new MatchAllDocsQuery();
+    searcher.search(base, fc);
+    List<FacetResult> facetResults = fc.getFacetResults();
+    assertEquals(requests.size(), facetResults.size());
+    for (FacetResult res : facetResults) {
+      FacetResultNode node = res.getFacetResultNode();
+      String dim = node.label.components[0];
+      assertEquals("wrong count for " + dim, expectedCounts.get(dim).intValue(), (int) node.value);
+    }
+  }
+  
+  private void verifyDrillDown(Map<String,Integer> expectedCounts, FacetIndexingParams fip, DirectoryReader indexReader, 
+      TaxonomyReader taxoReader, IndexSearcher searcher) throws IOException {
+    // verify drill-down
+    for (String dim : expectedCounts.keySet()) {
+      CategoryPath drillDownCP = new CategoryPath(dim);
+      FacetSearchParams fsp = new FacetSearchParams(fip, new CountFacetRequest(drillDownCP, 10));
+      Query drillDown = DrillDown.query(fsp, new MatchAllDocsQuery(), Occur.MUST, drillDownCP);
+      TotalHitCountCollector total = new TotalHitCountCollector();
+      FacetsCollector fc = FacetsCollector.create(fsp, indexReader, taxoReader);
+      searcher.search(drillDown, MultiCollector.wrap(fc, total));
+      assertTrue("no results for drill-down query " + drillDown, total.getTotalHits() > 0);
+      List<FacetResult> facetResults = fc.getFacetResults();
+      assertEquals(1, facetResults.size());
+      FacetResultNode rootNode = facetResults.get(0).getFacetResultNode();
+      assertEquals("wrong count for " + dim, expectedCounts.get(dim).intValue(), (int) rootNode.value);
+    }
+  }
+  
+  private void verifyIndexOrdinals(DirectoryReader indexReader, TaxonomyReader taxoReader, FacetIndexingParams fip) 
+      throws IOException {
+    // verify that the ordinals in the index match the ones in the taxonomy, and vice versa
+    
+    // collect all fields which have DocValues, to assert later that all were
+    // visited i.e. that during migration we didn't add FieldInfos with no
+    // DocValues
+    HashSet<String> docValuesFields = new HashSet<String>();
+    for (AtomicReaderContext context : indexReader.leaves()) {
+      FieldInfos infos = context.reader().getFieldInfos();
+      for (FieldInfo info : infos) {
+        if (info.hasDocValues()) {
+          docValuesFields.add(info.name);
+        }
+      }
+    }
+    
+    // check that all visited ordinals are found in the taxonomy and vice versa
+    boolean[] foundOrdinals = new boolean[taxoReader.getSize()];
+    for (int i = 0; i < foundOrdinals.length; i++) {
+      foundOrdinals[i] = false; // init to be on the safe side
+    }
+    foundOrdinals[0] = true; // ROOT ordinals isn't indexed
+    // mark 'dummy' category ordinal as seen
+    int dummyOrdinal = taxoReader.getOrdinal(new CategoryPath("dummy"));
+    if (dummyOrdinal > 0) {
+      foundOrdinals[dummyOrdinal] = true;
+    }
+    
+    int partitionSize = fip.getPartitionSize();
+    int numPartitions = (int) Math.ceil(taxoReader.getSize() / (double) partitionSize);
+    final IntsRef ordinals = new IntsRef(32);
+    for (String dim : DIMENSIONS) {
+      CategoryListParams clp = fip.getCategoryListParams(new CategoryPath(dim));
+      int partitionOffset = 0;
+      for (int partition = 0; partition < numPartitions; partition++, partitionOffset += partitionSize) {
+        final CategoryListIterator cli = clp.createCategoryListIterator(partition);
+        for (AtomicReaderContext context : indexReader.leaves()) {
+          if (cli.setNextReader(context)) { // not all fields may exist in all segments
+            // remove that field from the list of DocValues fields
+            docValuesFields.remove(clp.field + PartitionsUtils.partitionName(partition));
+            int maxDoc = context.reader().maxDoc();
+            for (int doc = 0; doc < maxDoc; doc++) {
+              cli.getOrdinals(doc, ordinals);
+              for (int j = 0; j < ordinals.length; j++) {
+                // verify that the ordinal is recognized by the taxonomy
+                int ordinal = ordinals.ints[j] + partitionOffset;
+                assertTrue("should not have received dummy ordinal (" + dummyOrdinal + ")", dummyOrdinal != ordinal);
+                assertNotNull("missing category for ordinal " + ordinal, taxoReader.getPath(ordinal));
+                foundOrdinals[ordinal] = true;
+              }
+            }
+          }
+        }
+      }
+    }
+    
+    assertTrue("some fields which have docValues were not visited: " + docValuesFields, docValuesFields.isEmpty());
+    
+    for (int i = 0; i < foundOrdinals.length; i++) {
+      assertTrue("ordinal " + i + " not visited", foundOrdinals[i]);
+    }
+  }
+  
+  private void doTestMigration(final int partitionSize) throws Exception {
+    // create a facets index with PayloadFacetFields and check it after migration
+    Directory indexDir = newDirectory();
+    Directory taxoDir = newDirectory();
+    
+    // set custom CLP fields for two dimensions and use the default ($facets) for the other two
+    HashMap<CategoryPath,CategoryListParams> params = new HashMap<CategoryPath,CategoryListParams>();
+    params.put(new CategoryPath(DIMENSIONS[0]), new CategoryListParams(DIMENSIONS[0]) {
+      @Override
+      public OrdinalPolicy getOrdinalPolicy(String dimension) {
+        return OrdinalPolicy.ALL_PARENTS;
+      }
+    });
+    params.put(new CategoryPath(DIMENSIONS[1]), new CategoryListParams(DIMENSIONS[1]) {
+      @Override
+      public OrdinalPolicy getOrdinalPolicy(String dimension) {
+        return OrdinalPolicy.ALL_PARENTS;
+      }
+    });
+    
+    HashMap<String,OrdinalPolicy> policies = new HashMap<String,CategoryListParams.OrdinalPolicy>();
+    policies.put(DIMENSIONS[2], OrdinalPolicy.ALL_PARENTS);
+    policies.put(DIMENSIONS[3], OrdinalPolicy.ALL_PARENTS);
+    FacetIndexingParams fip = new PerDimensionIndexingParams(params, new PerDimensionOrdinalPolicy(policies)) {
+      @Override
+      public int getPartitionSize() {
+        return partitionSize;
+      }
+    };
+    
+    HashMap<String,Integer> expectedCounts = createIndex(indexDir, taxoDir, fip);
+    migrateIndex(indexDir, fip);
+    verifyMigratedIndex(indexDir, taxoDir, expectedCounts, fip);
+    
+    IOUtils.close(indexDir, taxoDir);
+  }
+  
+  @Test
+  public void testMigration() throws Exception {
+    doTestMigration(Integer.MAX_VALUE);
+  }
+  
+  @Test
+  public void testMigrationWithPartitions() throws Exception {
+    doTestMigration(2);
+  }
+  
+}
diff --git a/lucene/facet/src/test/org/apache/lucene/util/SlowRAMDirectory.java b/lucene/facet/src/test/org/apache/lucene/util/SlowRAMDirectory.java
deleted file mode 100644
index 77c75ca..0000000
--- a/lucene/facet/src/test/org/apache/lucene/util/SlowRAMDirectory.java
+++ /dev/null
@@ -1,171 +0,0 @@
-package org.apache.lucene.util;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Random;
-
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.store.RAMDirectory;
-
-/**
- * Test utility - slow directory
- */
-// TODO: move to test-framework and sometimes use in tests?
-public class SlowRAMDirectory extends RAMDirectory {
-
-  private static final int IO_SLEEP_THRESHOLD = 50;
-  
-  Random random;
-  private int sleepMillis;
-
-  public void setSleepMillis(int sleepMillis) {
-    this.sleepMillis = sleepMillis;
-  }
-  
-  public SlowRAMDirectory(int sleepMillis, Random random) {
-    this.sleepMillis = sleepMillis;
-    this.random = random;
-  }
-
-  @Override
-  public IndexOutput createOutput(String name, IOContext context) throws IOException {
-    if (sleepMillis != -1) {
-      return new SlowIndexOutput(super.createOutput(name, context));
-    } 
-
-    return super.createOutput(name, context);
-  }
-
-  @Override
-  public IndexInput openInput(String name, IOContext context) throws IOException {
-    if (sleepMillis != -1) {
-      return new SlowIndexInput(super.openInput(name, context));
-    } 
-    return super.openInput(name, context);
-  }
-
-  void doSleep(Random random, int length) {
-    int sTime = length<10 ? sleepMillis : (int) (sleepMillis * Math.log(length));
-    if (random!=null) {
-      sTime = random.nextInt(sTime);
-    }
-    try {
-      Thread.sleep(sTime);
-    } catch (InterruptedException e) {
-      throw new ThreadInterruptedException(e);
-    }
-  }
-
-  /** Make a private random. */
-  Random forkRandom() {
-    if (random == null) {
-      return null;
-    }
-    return new Random(random.nextLong());
-  }
-
-  /**
-   * Delegate class to wrap an IndexInput and delay reading bytes by some
-   * specified time.
-   */
-  private class SlowIndexInput extends IndexInput {
-    private IndexInput ii;
-    private int numRead = 0;
-    private Random random;
-    
-    public SlowIndexInput(IndexInput ii) {
-      super("SlowIndexInput(" + ii + ")");
-      this.random = forkRandom();
-      this.ii = ii;
-    }
-    
-    @Override
-    public byte readByte() throws IOException {
-      if (numRead >= IO_SLEEP_THRESHOLD) {
-        doSleep(random, 0);
-        numRead = 0;
-      }
-      ++numRead;
-      return ii.readByte();
-    }
-    
-    @Override
-    public void readBytes(byte[] b, int offset, int len) throws IOException {
-      if (numRead >= IO_SLEEP_THRESHOLD) {
-        doSleep(random, len);
-        numRead = 0;
-      }
-      numRead += len;
-      ii.readBytes(b, offset, len);
-    }
-    
-    @Override public IndexInput clone() { return ii.clone(); }
-    @Override public void close() throws IOException { ii.close(); }
-    @Override public boolean equals(Object o) { return ii.equals(o); }
-    @Override public long getFilePointer() { return ii.getFilePointer(); }
-    @Override public int hashCode() { return ii.hashCode(); }
-    @Override public long length() { return ii.length(); }
-    @Override public void seek(long pos) throws IOException { ii.seek(pos); }
-    
-  }
-  
-  /**
-   * Delegate class to wrap an IndexOutput and delay writing bytes by some
-   * specified time.
-   */
-  private class SlowIndexOutput extends IndexOutput {
-    
-    private IndexOutput io;
-    private int numWrote;
-    private final Random random;
-    
-    public SlowIndexOutput(IndexOutput io) {
-      this.io = io;
-      this.random = forkRandom();
-    }
-    
-    @Override
-    public void writeByte(byte b) throws IOException {
-      if (numWrote >= IO_SLEEP_THRESHOLD) {
-        doSleep(random, 0);
-        numWrote = 0;
-      }
-      ++numWrote;
-      io.writeByte(b);
-    }
-    
-    @Override
-    public void writeBytes(byte[] b, int offset, int length) throws IOException {
-      if (numWrote >= IO_SLEEP_THRESHOLD) {
-        doSleep(random, length);
-        numWrote = 0;
-      }
-      numWrote += length;
-      io.writeBytes(b, offset, length);
-    }
-    
-    @Override public void close() throws IOException { io.close(); }
-    @Override public void flush() throws IOException { io.flush(); }
-    @Override public long getFilePointer() { return io.getFilePointer(); }
-    @Override public long length() throws IOException { return io.length(); }
-  }
-  
-}
diff --git a/lucene/facet/src/test/org/apache/lucene/util/UnsafeByteArrayInputStreamTest.java b/lucene/facet/src/test/org/apache/lucene/util/UnsafeByteArrayInputStreamTest.java
deleted file mode 100644
index e88bd22..0000000
--- a/lucene/facet/src/test/org/apache/lucene/util/UnsafeByteArrayInputStreamTest.java
+++ /dev/null
@@ -1,137 +0,0 @@
-package org.apache.lucene.util;
-
-import java.io.IOException;
-import java.util.Arrays;
-
-import org.apache.lucene.facet.FacetTestCase;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class UnsafeByteArrayInputStreamTest extends FacetTestCase {
-
-  @Test
-  public void testSimple() throws IOException {
-    int length = 256;
-    byte[] buffer = new byte[length];
-    for (int i = 0; i < length; ++i) {
-      buffer[i] = (byte) i;
-    }
-    byte[] result = new byte[buffer.length];
-    UnsafeByteArrayInputStream ubais = new UnsafeByteArrayInputStream(buffer);
-    
-    int index = 0;
-    int by = ubais.read();
-    while (by >= 0) {
-      result[index++] = (byte) (by);
-      by = ubais.read();
-    }
-    
-    assertEquals(length, index);
-    assertTrue(Arrays.equals(buffer, result));
-  }
-  
-  @Test
-  public void testStartPos() throws IOException {
-    int length = 100;
-    byte[] buffer = new byte[length];
-    for (int i = 0; i < length; ++i) {
-      buffer[i] = (byte) i;
-    }
-    int startPos = 5;
-    byte[] result = new byte[buffer.length];
-    UnsafeByteArrayInputStream ubais = new UnsafeByteArrayInputStream(buffer, startPos, length);
-    
-    int index = 0;
-    int by = ubais.read();
-    while (by >= 0) {
-      result[index++] = (byte) (by);
-      by = ubais.read();
-    }
-    
-    assertEquals(length - startPos, index);
-    for (int i = startPos; i < length; i++) {
-      assertEquals(buffer[i], result[i - startPos]);
-    }
-  }
-  
-  @Test
-  public void testReinit() throws IOException {
-    int length = 100;
-    byte[] buffer = new byte[length];
-    for (int i = 0; i < length; ++i) {
-      buffer[i] = (byte) i;
-    }
-    byte[] result = new byte[buffer.length];
-    UnsafeByteArrayInputStream ubais = new UnsafeByteArrayInputStream(buffer);
-
-    int index = 0;
-    int by = ubais.read();
-    while (by >= 0) {
-      result[index++] = (byte) (by);
-      by = ubais.read();
-    }
-
-    assertEquals(length, index);
-    assertTrue(Arrays.equals(buffer, result));
-
-    int length2 = 50;
-    byte[] buffer2 = new byte[length2];
-    for (int i = 0; i < length2; ++i) {
-      buffer2[i] = (byte) (90 + i);
-    }
-    byte[] result2 = new byte[buffer2.length];
-    ubais.reInit(buffer2);
-
-    int index2 = 0;
-    int by2 = ubais.read();
-    while (by2 >= 0) {
-      result2[index2++] = (byte) (by2);
-      by2 = ubais.read();
-    }
-
-    assertEquals(length2, index2);
-    assertTrue(Arrays.equals(buffer2, result2));
-  }
-
-  @Test
-  public void testDefaultCtor() throws Exception {
-    UnsafeByteArrayInputStream ubais = new UnsafeByteArrayInputStream();
-    assertEquals(0, ubais.available());
-    assertEquals(-1, ubais.read());
-  }
-
-  @Test
-  public void testMark() throws Exception {
-    byte[] bytes = new byte[] { 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 };
-    UnsafeByteArrayInputStream ubais = new UnsafeByteArrayInputStream(bytes);
-    assertTrue(ubais.markSupported());
-    int markIndex = 3;
-    // Advance the index
-    for (int i = 0; i < markIndex; i++) {
-      ubais.read();
-    }
-    ubais.mark(markIndex);
-    for (int i = markIndex; i < bytes.length; i++) {
-      ubais.read();
-    }
-    ubais.reset();
-    assertEquals(bytes.length - markIndex, ubais.available());
-  }
-  
-}
diff --git a/lucene/facet/src/test/org/apache/lucene/util/UnsafeByteArrayOutputStreamTest.java b/lucene/facet/src/test/org/apache/lucene/util/UnsafeByteArrayOutputStreamTest.java
deleted file mode 100644
index b7af76a..0000000
--- a/lucene/facet/src/test/org/apache/lucene/util/UnsafeByteArrayOutputStreamTest.java
+++ /dev/null
@@ -1,209 +0,0 @@
-package org.apache.lucene.util;
-
-import java.io.IOException;
-
-import org.apache.lucene.facet.FacetTestCase;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class UnsafeByteArrayOutputStreamTest extends FacetTestCase {
-
-  @Test
-  public void testSimpleWrite() throws IOException {
-    int length = 100;
-    byte[] buffer = new byte[length];
-    UnsafeByteArrayOutputStream ubaos = new UnsafeByteArrayOutputStream(buffer);
-
-    for (int i = 0; i < 100; i++) {
-      ubaos.write((byte) i);
-    }
-
-    byte[] result = ubaos.toByteArray();
-
-    assertEquals(length, ubaos.length());
-
-    for (int j = 0; j < length; ++j) {
-      assertEquals(result[j], j);
-    }
-  }
-
-  @Test
-  public void testArrayWrite() throws IOException {
-    int length = 100;
-    byte[] buffer = new byte[length];
-    UnsafeByteArrayOutputStream ubaos = new UnsafeByteArrayOutputStream(buffer);
-
-    for (int i = 0; i < 100; i++) {
-      ubaos.write((byte) i);
-    }
-
-    int length2 = 10;
-    byte[] buffer2 = new byte[length2];
-    for (int i = 0; i < length2; i++) {
-      buffer2[i] = (byte) (8 + i);
-    }
-
-    ubaos.write(buffer2);
-
-    byte[] result = ubaos.toByteArray();
-
-    assertEquals(length + length2, ubaos.length());
-
-    for (int j = 0; j < length; ++j) {
-      assertEquals(result[j], j);
-    }
-    for (int j = 0; j < length2; ++j) {
-      assertEquals(result[j + length], buffer2[j]);
-    }
-  }
-
-  @Test
-  public void testArrayWriteStartNotZero() throws IOException {
-    int length = 100;
-    byte[] buffer = new byte[length];
-    UnsafeByteArrayOutputStream ubaos = new UnsafeByteArrayOutputStream(buffer);
-
-    for (int i = 0; i < 100; i++) {
-      ubaos.write((byte) i);
-    }
-
-    int length2 = 1000;
-    byte[] buffer2 = new byte[length2];
-    for (int i = 0; i < length2; i++) {
-      buffer2[i] = (byte) (8 + i);
-    }
-
-    int length3 = 5;
-    int start = 2;
-    ubaos.write(buffer2, start, length3);
-
-    byte[] result = ubaos.toByteArray();
-
-    assertEquals(length + length3, ubaos.length());
-
-    for (int j = 0; j < length; ++j) {
-      assertEquals(result[j], j);
-    }
-    for (int j = 0; j < length3; ++j) {
-      assertEquals(result[j + length], buffer2[j + start]);
-    }
-  }
-
-  @Test
-  public void testBufferGrow() throws IOException {
-    int length = 100;
-    byte[] buffer = new byte[length / 10];
-    UnsafeByteArrayOutputStream ubaos = new UnsafeByteArrayOutputStream(buffer);
-
-    for (int i = 0; i < length; i++) {
-      ubaos.write((byte) i);
-    }
-
-    byte[] result = ubaos.toByteArray();
-
-    assertEquals(length, ubaos.length());
-
-    for (int j = 0; j < length; ++j) {
-      assertEquals(result[j], j);
-    }
-
-    buffer = ubaos.toByteArray();
-
-    int length2 = 10;
-    byte[] buffer2 = new byte[length2];
-    for (int i = 0; i < length2; i++) {
-      buffer2[i] = (byte) (8 + i);
-    }
-
-    ubaos.reInit(buffer2);
-    for (int i = 0; i < length2; i++) {
-      ubaos.write(7 + i);
-    }
-
-    byte[] result2 = ubaos.toByteArray();
-
-    assertEquals(length2, ubaos.length());
-
-    for (int j = 0; j < length2; ++j) {
-      assertEquals(result2[j], j + 7);
-    }
-
-    for (int i = 0; i < length; i++) {
-      assertEquals(buffer[i], i);
-    }
-  }
-  
-  @Test
-  public void testStartPos() throws Exception {
-    byte[] buf = new byte[10];
-    for (int i = 0; i < buf.length; i++) {
-      buf[i] = (byte) i;
-    }
-    
-    int startPos = 3;
-    UnsafeByteArrayOutputStream ubaos = new UnsafeByteArrayOutputStream(buf, startPos);
-    int numValues = 5;
-    for (int i = 0; i < numValues; i++) {
-      ubaos.write((i + 1) * 2);
-    }
-
-    // the length of the buffer should be whatever was written after startPos
-    // and before that.
-    assertEquals("invalid buffer length", startPos + numValues, ubaos.length());
-
-    assertEquals("invalid startPos", startPos, ubaos.getStartPos());
-
-    byte[] bytes = ubaos.toByteArray();
-    for (int i = 0; i < startPos; i++) {
-      assertEquals(i, bytes[i]);
-    }
-    
-    for (int i = startPos, j = 0; j < numValues; i++, j++) {
-      assertEquals((j + 1) * 2, bytes[i]);
-    }
-
-    for (int i = startPos + numValues; i < buf.length; i++) {
-      assertEquals(i, bytes[i]);
-    }
-
-  }
-
-  @Test
-  public void testDefaultCtor() throws Exception {
-    UnsafeByteArrayOutputStream ubaos = new UnsafeByteArrayOutputStream();
-    int numValues = 5;
-    for (int i = 0; i < numValues; i++) {
-      ubaos.write(i);
-    }
-
-    assertEquals("invalid buffer length", numValues, ubaos.length());
-    
-    byte[] bytes = ubaos.toByteArray();
-    for (int i = 0; i < numValues; i++) {
-      assertEquals(i, bytes[i]);
-    }
-  }
-  
-  @Test(expected=IllegalArgumentException.class)
-  public void testIllegalBufferSize() throws Exception {
-    UnsafeByteArrayOutputStream ubaos = new UnsafeByteArrayOutputStream();
-    ubaos.reInit(new byte[0]);
-  }
-
-}
diff --git a/lucene/facet/src/test/org/apache/lucene/util/collections/ArrayHashMapTest.java b/lucene/facet/src/test/org/apache/lucene/util/collections/ArrayHashMapTest.java
deleted file mode 100644
index 4b0c3ba..0000000
--- a/lucene/facet/src/test/org/apache/lucene/util/collections/ArrayHashMapTest.java
+++ /dev/null
@@ -1,267 +0,0 @@
-package org.apache.lucene.util.collections;
-
-import java.util.HashSet;
-import java.util.Iterator;
-import java.util.Random;
-
-import org.apache.lucene.facet.FacetTestCase;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class ArrayHashMapTest extends FacetTestCase {
-
-  public static final int RANDOM_TEST_NUM_ITERATIONS = 100; // set to 100,000 for deeper test
-  
-  @Test
-  public void test0() {
-    ArrayHashMap<Integer,Integer> map = new ArrayHashMap<Integer,Integer>();
-
-    assertNull(map.get(0));
-
-    for (int i = 0; i < 100; ++i) {
-      int value = 100 + i;
-      assertFalse(map.containsValue(value));
-      map.put(i, value);
-      assertTrue(map.containsValue(value));
-      assertNotNull(map.get(i));
-    }
-
-    assertEquals(100, map.size());
-    for (int i = 0; i < 100; ++i) {
-      assertTrue(map.containsKey(i));
-      assertEquals(100 + i, map.get(i).intValue());
-
-    }
-
-    for (int i = 10; i < 90; ++i) {
-      map.remove(i);
-      assertNull(map.get(i));
-    }
-
-    assertEquals(20, map.size());
-    for (int i = 0; i < 100; ++i) {
-      assertEquals(map.containsKey(i), !(i >= 10 && i < 90));
-    }
-
-    for (int i = 5; i < 85; ++i) {
-      map.put(i, Integer.valueOf(5 + i));
-    }
-    assertEquals(95, map.size());
-    for (int i = 0; i < 100; ++i) {
-      assertEquals(map.containsKey(i), !(i >= 85 && i < 90));
-    }
-    for (int i = 0; i < 5; ++i) {
-      assertEquals(map.get(i).intValue(), (100 + i));
-    }
-    for (int i = 5; i < 85; ++i) {
-      assertEquals(map.get(i).intValue(), (5 + i));
-    }
-    for (int i = 90; i < 100; ++i) {
-      assertEquals(map.get(i).intValue(), (100 + i));
-    }
-  }
-
-  @Test
-  public void test1() {
-    ArrayHashMap<Integer,Integer> map = new ArrayHashMap<Integer,Integer>();
-
-    for (int i = 0; i < 100; ++i) {
-      map.put(i, Integer.valueOf(100 + i));
-    }
-
-    HashSet<Integer> set = new HashSet<Integer>();
-
-    for (Iterator<Integer> iterator = map.iterator(); iterator.hasNext();) {
-      set.add(iterator.next());
-    }
-
-    assertEquals(set.size(), map.size());
-    for (int i = 0; i < 100; ++i) {
-      assertTrue(set.contains(Integer.valueOf(100 + i)));
-    }
-
-    set.clear();
-    for (Iterator<Integer> iterator = map.iterator(); iterator.hasNext();) {
-      Integer integer = iterator.next();
-      if (integer % 2 == 1) {
-        iterator.remove();
-        continue;
-      }
-      set.add(integer);
-    }
-    assertEquals(set.size(), map.size());
-    for (int i = 0; i < 100; i += 2) {
-      assertTrue(set.contains(Integer.valueOf(100 + i)));
-    }
-  }
-
-  @Test
-  public void test2() {
-    ArrayHashMap<Integer,Integer> map = new ArrayHashMap<Integer,Integer>();
-
-    assertTrue(map.isEmpty());
-    assertNull(map.get(0));
-    for (int i = 0; i < 128; ++i) {
-      int value = i * 4096;
-      assertFalse(map.containsValue(value));
-      map.put(i, value);
-      assertTrue(map.containsValue(value));
-      assertNotNull(map.get(i));
-      assertFalse(map.isEmpty());
-    }
-
-    assertEquals(128, map.size());
-    for (int i = 0; i < 128; ++i) {
-      assertTrue(map.containsKey(i));
-      assertEquals(i * 4096, map.get(i).intValue());
-    }
-
-    for (int i = 0; i < 200; i += 2) {
-      map.remove(i);
-    }
-    assertEquals(64, map.size());
-    for (int i = 1; i < 128; i += 2) {
-      assertTrue(map.containsKey(i));
-      assertEquals(i * 4096, map.get(i).intValue());
-      map.remove(i);
-    }
-    assertTrue(map.isEmpty());
-  }
-
-  @Test
-  public void test3() {
-    ArrayHashMap<Integer,Integer> map = new ArrayHashMap<Integer,Integer>();
-    int length = 100;
-    for (int i = 0; i < length; ++i) {
-      map.put(i * 64, 100 + i);
-    }
-    HashSet<Integer> keySet = new HashSet<Integer>();
-    for (Iterator<Integer> iit = map.keyIterator(); iit.hasNext();) {
-      keySet.add(iit.next());
-    }
-    assertEquals(length, keySet.size());
-    for (int i = 0; i < length; ++i) {
-      assertTrue(keySet.contains(i * 64));
-    }
-
-    HashSet<Integer> valueSet = new HashSet<Integer>();
-    for (Iterator<Integer> iit = map.iterator(); iit.hasNext();) {
-      valueSet.add(iit.next());
-    }
-    assertEquals(length, valueSet.size());
-    Object[] array = map.toArray();
-    assertEquals(length, array.length);
-    for (Object value : array) {
-      assertTrue(valueSet.contains(value));
-    }
-
-    Integer[] array2 = new Integer[80];
-    array2 = map.toArray(array2);
-    for (int value : array2) {
-      assertTrue(valueSet.contains(value));
-    }
-    Integer[] array3 = new Integer[120];
-    array3 = map.toArray(array3);
-    for (int i = 0; i < length; ++i) {
-      assertTrue(valueSet.contains(array3[i]));
-    }
-    assertNull(array3[length]);
-
-    for (int i = 0; i < length; ++i) {
-      assertTrue(map.containsValue(i + 100));
-      assertTrue(map.containsKey(i * 64));
-    }
-
-    for (Iterator<Integer> iit = map.keyIterator(); iit.hasNext();) {
-      iit.next();
-      iit.remove();
-    }
-    assertTrue(map.isEmpty());
-    assertEquals(0, map.size());
-
-  }
-
-  // now with random data.. and lots of it
-  @Test
-  public void test4() {
-    ArrayHashMap<Integer,Integer> map = new ArrayHashMap<Integer,Integer>();
-    int length = RANDOM_TEST_NUM_ITERATIONS;
-    
-    // for a repeatable random sequence
-    long seed = random().nextLong();
-    Random random = new Random(seed);
-
-    for (int i = 0; i < length; ++i) {
-      int value = random.nextInt(Integer.MAX_VALUE);
-      map.put(i * 128, value);
-    }
-
-    assertEquals(length, map.size());
-    
-    // now repeat
-    random.setSeed(seed);
-
-    for (int i = 0; i < length; ++i) {
-      int value = random.nextInt(Integer.MAX_VALUE);
-      assertTrue(map.containsValue(value));
-      assertTrue(map.containsKey(i * 128));
-      assertEquals(Integer.valueOf(value), map.remove(i * 128));
-    }
-    assertEquals(0, map.size());
-    assertTrue(map.isEmpty());
-  }
-
-  @Test
-  public void testEquals() {
-    ArrayHashMap<Integer,Float> map1 = new ArrayHashMap<Integer,Float>(100);
-    ArrayHashMap<Integer,Float> map2 = new ArrayHashMap<Integer,Float>(100);
-    assertEquals("Empty maps should be equal", map1, map2);
-    assertEquals("hashCode() for empty maps should be equal", 
-        map1.hashCode(), map2.hashCode());
-    
-    for (int i = 0; i < 100; ++i) {
-      map1.put(i, Float.valueOf(1f/i));
-      map2.put(i, Float.valueOf(1f/i));
-    }
-    assertEquals("Identical maps should be equal", map1, map2);
-    assertEquals("hashCode() for identical maps should be equal", 
-        map1.hashCode(), map2.hashCode());
-
-    for (int i = 10; i < 20; i++) {
-      map1.remove(i);
-    }
-    assertFalse("Different maps should not be equal", map1.equals(map2));
-    
-    for (int i = 19; i >=10; --i) {
-      map2.remove(i);
-    }
-    assertEquals("Identical maps should be equal", map1, map2);
-    assertEquals("hashCode() for identical maps should be equal", 
-        map1.hashCode(), map2.hashCode());
-    
-    map1.put(-1,-1f);
-    map2.put(-1,-1.1f);
-    assertFalse("Different maps should not be equal", map1.equals(map2));
-    
-    map2.put(-1,-1f);
-    assertEquals("Identical maps should be equal", map1, map2);
-    assertEquals("hashCode() for identical maps should be equal", 
-        map1.hashCode(), map2.hashCode());
-  }
-}
diff --git a/lucene/facet/src/test/org/apache/lucene/util/collections/FloatToObjectMapTest.java b/lucene/facet/src/test/org/apache/lucene/util/collections/FloatToObjectMapTest.java
deleted file mode 100644
index a627ea1..0000000
--- a/lucene/facet/src/test/org/apache/lucene/util/collections/FloatToObjectMapTest.java
+++ /dev/null
@@ -1,265 +0,0 @@
-package org.apache.lucene.util.collections;
-
-import java.util.HashSet;
-import java.util.Iterator;
-import java.util.Random;
-
-import org.apache.lucene.facet.FacetTestCase;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class FloatToObjectMapTest extends FacetTestCase {
-
-  @Test
-  public void test0() {
-    FloatToObjectMap<Integer> map = new FloatToObjectMap<Integer>();
-
-    assertNull(map.get(0));
-
-    for (int i = 0; i < 100; ++i) {
-      int value = 100 + i;
-      assertFalse(map.containsValue(value));
-      map.put(i * 1.1f, value);
-      assertTrue(map.containsValue(value));
-      assertNotNull(map.get(i * 1.1f));
-    }
-
-    assertEquals(100, map.size());
-    for (int i = 0; i < 100; ++i) {
-      assertTrue(map.containsKey(i*1.1f));
-      assertEquals(100 + i, map.get(i*1.1f).intValue());
-
-    }
-
-    for (int i = 10; i < 90; ++i) {
-      map.remove(i*1.1f);
-      assertNull(map.get(i*1.1f));
-    }
-
-    assertEquals(20, map.size());
-    for (int i = 0; i < 100; ++i) {
-      assertEquals(map.containsKey(i*1.1f), !(i >= 10 && i < 90));
-    }
-
-    for (int i = 5; i < 85; ++i) {
-      map.put(i*1.1f, Integer.valueOf(5 + i));
-    }
-    assertEquals(95, map.size());
-    for (int i = 0; i < 100; ++i) {
-      assertEquals(map.containsKey(i*1.1f), !(i >= 85 && i < 90));
-    }
-    for (int i = 0; i < 5; ++i) {
-      assertEquals(map.get(i*1.1f).intValue(), (100 + i));
-    }
-    for (int i = 5; i < 85; ++i) {
-      assertEquals(map.get(i*1.1f).intValue(), (5 + i));
-    }
-    for (int i = 90; i < 100; ++i) {
-      assertEquals(map.get(i*1.1f).intValue(), (100 + i));
-    }
-  }
-
-  @Test
-  public void test1() {
-    FloatToObjectMap<Integer> map = new FloatToObjectMap<Integer>();
-
-    for (int i = 0; i < 100; ++i) {
-      map.put(i*1.1f, Integer.valueOf(100 + i));
-    }
-
-    HashSet<Integer> set = new HashSet<Integer>();
-
-    for (Iterator<Integer> iterator = map.iterator(); iterator.hasNext();) {
-      set.add(iterator.next());
-    }
-
-    assertEquals(set.size(), map.size());
-    for (int i = 0; i < 100; ++i) {
-      assertTrue(set.contains(Integer.valueOf(100 + i)));
-    }
-
-    set.clear();
-    for (Iterator<Integer> iterator = map.iterator(); iterator.hasNext();) {
-      Integer integer = iterator.next();
-      if (integer % 2 == 1) {
-        iterator.remove();
-        continue;
-      }
-      set.add(integer);
-    }
-    assertEquals(set.size(), map.size());
-    for (int i = 0; i < 100; i += 2) {
-      assertTrue(set.contains(Integer.valueOf(100 + i)));
-    }
-  }
-
-  @Test
-  public void test2() {
-    FloatToObjectMap<Integer> map = new FloatToObjectMap<Integer>();
-
-    assertTrue(map.isEmpty());
-    assertNull(map.get(0));
-    for (int i = 0; i < 128; ++i) {
-      int value = i * 4096;
-      assertFalse(map.containsValue(value));
-      map.put(i*1.1f, value);
-      assertTrue(map.containsValue(value));
-      assertNotNull(map.get(i*1.1f));
-      assertFalse(map.isEmpty());
-    }
-
-    assertEquals(128, map.size());
-    for (int i = 0; i < 128; ++i) {
-      assertTrue(map.containsKey(i*1.1f));
-      assertEquals(i * 4096, map.get(i*1.1f).intValue());
-    }
-
-    for (int i = 0; i < 200; i += 2) {
-      map.remove(i*1.1f);
-    }
-    assertEquals(64, map.size());
-    for (int i = 1; i < 128; i += 2) {
-      assertTrue(map.containsKey(i*1.1f));
-      assertEquals(i * 4096, map.get(i*1.1f).intValue());
-      map.remove(i*1.1f);
-    }
-    assertTrue(map.isEmpty());
-  }
-
-  @Test
-  public void test3() {
-    FloatToObjectMap<Integer> map = new FloatToObjectMap<Integer>();
-    int length = 100;
-    for (int i = 0; i < length; ++i) {
-      map.put(i * 64*1.1f, 100 + i);
-    }
-    HashSet<Float> keySet = new HashSet<Float>();
-    for (FloatIterator iit = map.keyIterator(); iit.hasNext();) {
-      keySet.add(iit.next());
-    }
-    assertEquals(length, keySet.size());
-    for (int i = 0; i < length; ++i) {
-      assertTrue(keySet.contains(i * 64*1.1f));
-    }
-
-    HashSet<Integer> valueSet = new HashSet<Integer>();
-    for (Iterator<Integer> iit = map.iterator(); iit.hasNext();) {
-      valueSet.add(iit.next());
-    }
-    assertEquals(length, valueSet.size());
-    Object[] array = map.toArray();
-    assertEquals(length, array.length);
-    for (Object value : array) {
-      assertTrue(valueSet.contains(value));
-    }
-
-    Integer[] array2 = new Integer[80];
-    array2 = map.toArray(array2);
-    for (int value : array2) {
-      assertTrue(valueSet.contains(value));
-    }
-    Integer[] array3 = new Integer[120];
-    array3 = map.toArray(array3);
-    for (int i = 0; i < length; ++i) {
-      assertTrue(valueSet.contains(array3[i]));
-    }
-    assertNull(array3[length]);
-
-    for (int i = 0; i < length; ++i) {
-      assertTrue(map.containsValue(i + 100));
-      assertTrue(map.containsKey(i * 64*1.1f));
-    }
-
-    for (FloatIterator iit = map.keyIterator(); iit.hasNext();) {
-      iit.next();
-      iit.remove();
-    }
-    assertTrue(map.isEmpty());
-    assertEquals(0, map.size());
-
-  }
-
-  // now with random data.. and lots of it
-  @Test
-  public void test4() {
-    FloatToObjectMap<Integer> map = new FloatToObjectMap<Integer>();
-    int length = ArrayHashMapTest.RANDOM_TEST_NUM_ITERATIONS;
-    
-    // for a repeatable random sequence
-    long seed = random().nextLong();
-    Random random = new Random(seed);
-
-    for (int i = 0; i < length; ++i) {
-      int value = random.nextInt(Integer.MAX_VALUE);
-      map.put(i * 128*1.1f, value);
-    }
-
-    assertEquals(length, map.size());
-
-    // now repeat
-    random.setSeed(seed);
-
-    for (int i = 0; i < length; ++i) {
-      int value = random.nextInt(Integer.MAX_VALUE);
-      assertTrue(map.containsValue(value));
-      assertTrue(map.containsKey(i * 128*1.1f));
-      assertEquals(Integer.valueOf(value), map.remove(i * 128*1.1f));
-    }
-    assertEquals(0, map.size());
-    assertTrue(map.isEmpty());
-  }
-
-  @Test
-  public void testEquals() {
-    FloatToObjectMap<Integer> map1 = new FloatToObjectMap<Integer>(100);
-    FloatToObjectMap<Integer> map2 = new FloatToObjectMap<Integer>(100);
-    assertEquals("Empty maps should be equal", map1, map2);
-    assertEquals("hashCode() for empty maps should be equal", 
-        map1.hashCode(), map2.hashCode());
-    
-    for (int i = 0; i < 100; ++i) {
-      map1.put(i * 1.1f, 100 + i);
-      map2.put(i * 1.1f, 100 + i);
-    }
-    assertEquals("Identical maps should be equal", map1, map2);
-    assertEquals("hashCode() for identical maps should be equal", 
-        map1.hashCode(), map2.hashCode());
-
-    for (int i = 10; i < 20; i++) {
-      map1.remove(i * 1.1f);
-    }
-    assertFalse("Different maps should not be equal", map1.equals(map2));
-    
-    for (int i = 19; i >=10; --i) {
-      map2.remove(i * 1.1f);
-    }
-    assertEquals("Identical maps should be equal", map1, map2);
-    assertEquals("hashCode() for identical maps should be equal", 
-        map1.hashCode(), map2.hashCode());
-    
-    map1.put(-1.1f,-1);
-    map2.put(-1.1f,-2);
-    assertFalse("Different maps should not be equal", map1.equals(map2));
-    
-    map2.put(-1.1f,-1);
-    assertEquals("Identical maps should be equal", map1, map2);
-    assertEquals("hashCode() for identical maps should be equal", 
-        map1.hashCode(), map2.hashCode());
-  }
-}
diff --git a/lucene/facet/src/test/org/apache/lucene/util/collections/IntArrayTest.java b/lucene/facet/src/test/org/apache/lucene/util/collections/IntArrayTest.java
deleted file mode 100644
index 73b3346..0000000
--- a/lucene/facet/src/test/org/apache/lucene/util/collections/IntArrayTest.java
+++ /dev/null
@@ -1,124 +0,0 @@
-package org.apache.lucene.util.collections;
-
-import org.apache.lucene.facet.FacetTestCase;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class IntArrayTest extends FacetTestCase {
-  
-  @Test
-  public void test0() {
-    IntArray array = new IntArray();
-    
-    assertEquals(0, array.size());
-    
-    for (int i = 0; i < 100; ++i) {
-      array.addToArray(i);
-    }
-    
-    assertEquals(100, array.size());
-    for (int i = 0; i < 100; ++i) {
-      assertEquals(i, array.get(i));
-    }
-    
-    assertTrue(array.equals(array));
-  }
-  
-  @Test
-  public void test1() {
-    IntArray array = new IntArray();
-    IntArray array2 = new IntArray();
-    
-    assertEquals(0, array.size());
-    
-    for (int i = 0; i < 100; ++i) {
-      array.addToArray(99-i);
-      array2.addToArray(99-i);
-    }
-    
-    assertEquals(100, array.size());
-    for (int i = 0; i < 100; ++i) {
-      assertEquals(i, array.get(99-i));
-    }
-    
-    array.sort();
-    for (int i = 0; i < 100; ++i) {
-      assertEquals(i, array.get(i));
-    }
-
-    assertTrue(array.equals(array2));
-  }
-  
-  @Test
-  public void test2() {
-    IntArray array = new IntArray();
-    IntArray array2 = new IntArray();
-    IntArray array3 = new IntArray();
-    
-    for (int i = 0; i < 100; ++i) {
-      array.addToArray(i);
-    }
-
-    for (int i = 0; i < 100; ++i) {
-      array2.addToArray(i*2);
-    }
-
-    for (int i = 0; i < 50; ++i) {
-      array3.addToArray(i*2);
-    }
-
-    assertFalse(array.equals(array2));
-    
-    array.intersect(array2);
-    assertTrue(array.equals(array3));
-    assertFalse(array.equals(array2));
-  }
-  
-  @Test
-  public void testSet() {
-    int[] original = new int[] { 2,4,6,8,10,12,14 };
-    int[] toSet = new int[] { 1,3,5,7,9,11};
-    
-    IntArray arr = new IntArray();
-    for (int val : original) {
-      arr.addToArray(val);
-    }
-    
-    for (int i = 0; i < toSet.length; i++ ) {
-      int val = toSet[i];
-      arr.set(i, val);
-    }
-    
-    // Test to see if the set worked correctly
-    for (int i = 0; i < toSet.length; i++ ) {
-      assertEquals(toSet[i], arr.get(i));
-    }
-    
-    // Now attempt to set something outside of the array
-    try {
-      arr.set(100, 99);
-      fail("IntArray.set should have thrown an exception for attempting to set outside the array");
-    } catch (ArrayIndexOutOfBoundsException e) {
-      // We expected this to happen so let it fall through
-      // silently
-    }
-    
-  }
-
-}
diff --git a/lucene/facet/src/test/org/apache/lucene/util/collections/IntHashSetTest.java b/lucene/facet/src/test/org/apache/lucene/util/collections/IntHashSetTest.java
deleted file mode 100644
index 1440925..0000000
--- a/lucene/facet/src/test/org/apache/lucene/util/collections/IntHashSetTest.java
+++ /dev/null
@@ -1,222 +0,0 @@
-package org.apache.lucene.util.collections;
-
-import java.util.HashSet;
-
-import org.apache.lucene.facet.FacetTestCase;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class IntHashSetTest extends FacetTestCase {
-
-  @Test
-  public void test0() {
-    IntHashSet set0 = new IntHashSet();
-
-    assertEquals(0, set0.size());
-    assertTrue(set0.isEmpty());
-    set0.add(0);
-    assertEquals(1, set0.size());
-    assertFalse(set0.isEmpty());
-    set0.remove(0);
-    assertEquals(0, set0.size());
-    assertTrue(set0.isEmpty());
-  }
-
-  @Test
-  public void test1() {
-    IntHashSet set0 = new IntHashSet();
-
-    assertEquals(0, set0.size());
-    assertTrue(set0.isEmpty());
-    for (int i = 0; i < 1000; ++i) {
-      set0.add(i);
-    }
-    assertEquals(1000, set0.size());
-    assertFalse(set0.isEmpty());
-    for (int i = 0; i < 1000; ++i) {
-      assertTrue(set0.contains(i));
-    }
-
-    set0.clear();
-    assertEquals(0, set0.size());
-    assertTrue(set0.isEmpty());
-
-  }
-
-  @Test
-  public void test2() {
-    IntHashSet set0 = new IntHashSet();
-
-    assertEquals(0, set0.size());
-    assertTrue(set0.isEmpty());
-    for (int i = 0; i < 1000; ++i) {
-      set0.add(1);
-      set0.add(-382);
-    }
-    assertEquals(2, set0.size());
-    assertFalse(set0.isEmpty());
-    set0.remove(-382);
-    set0.remove(1);
-    assertEquals(0, set0.size());
-    assertTrue(set0.isEmpty());
-
-  }
-
-  @Test
-  public void test3() {
-    IntHashSet set0 = new IntHashSet();
-
-    assertEquals(0, set0.size());
-    assertTrue(set0.isEmpty());
-    for (int i = 0; i < 1000; ++i) {
-      set0.add(i);
-    }
-
-    for (int i = 0; i < 1000; i += 2) {
-      set0.remove(i);
-    }
-
-    assertEquals(500, set0.size());
-    for (int i = 0; i < 1000; ++i) {
-      if (i % 2 == 0) {
-        assertFalse(set0.contains(i));
-      } else {
-        assertTrue(set0.contains(i));
-      }
-    }
-
-  }
-
-  @Test
-  public void test4() {
-    IntHashSet set1 = new IntHashSet();
-    HashSet<Integer> set2 = new HashSet<Integer>();
-    for (int i = 0; i < ArrayHashMapTest.RANDOM_TEST_NUM_ITERATIONS; ++i) {
-      int value = random().nextInt() % 500;
-      boolean shouldAdd = random().nextBoolean();
-      if (shouldAdd) {
-        set1.add(value);
-        set2.add(value);
-      } else {
-        set1.remove(value);
-        set2.remove(value);
-      }
-    }
-    assertEquals(set2.size(), set1.size());
-    for (int value : set2) {
-      assertTrue(set1.contains(value));
-    }
-  }
-
-  @Test
-  public void testRegularJavaSet() {
-    HashSet<Integer> set = new HashSet<Integer>();
-    for (int j = 0; j < 100; ++j) {
-      for (int i = 0; i < ArrayHashMapTest.RANDOM_TEST_NUM_ITERATIONS; ++i) {
-        int value = random().nextInt() % 5000;
-        boolean shouldAdd = random().nextBoolean();
-        if (shouldAdd) {
-          set.add(value);
-        } else {
-          set.remove(value);
-        }
-      }
-      set.clear();
-    }
-  }
-
-  @Test
-  public void testMySet() {
-    IntHashSet set = new IntHashSet();
-    for (int j = 0; j < 100; ++j) {
-      for (int i = 0; i < ArrayHashMapTest.RANDOM_TEST_NUM_ITERATIONS; ++i) {
-        int value = random().nextInt() % 5000;
-        boolean shouldAdd = random().nextBoolean();
-        if (shouldAdd) {
-          set.add(value);
-        } else {
-          set.remove(value);
-        }
-      }
-      set.clear();
-    }
-  }
-
-  @Test
-  public void testToArray() {
-    IntHashSet set = new IntHashSet();
-    for (int j = 0; j < 100; ++j) {
-      for (int i = 0; i < ArrayHashMapTest.RANDOM_TEST_NUM_ITERATIONS; ++i) {
-        int value = random().nextInt() % 5000;
-        boolean shouldAdd = random().nextBoolean();
-        if (shouldAdd) {
-          set.add(value);
-        } else {
-          set.remove(value);
-        }
-      }
-      int[] vals = set.toArray();
-      assertEquals(set.size(), vals.length);
-
-      int[] realValues = new int[set.size()];
-      int[] unrealValues = set.toArray(realValues);
-      assertEquals(realValues, unrealValues);
-      for (int value : vals) {
-        assertTrue(set.remove(value));
-      }
-      for (int i = 0; i < vals.length; ++i) {
-        assertEquals(vals[i], realValues[i]);
-      }
-    }
-  }
-
-  @Test
-  public void testZZRegularJavaSet() {
-    HashSet<Integer> set = new HashSet<Integer>();
-    for (int j = 0; j < 100; ++j) {
-      for (int i = 0; i < ArrayHashMapTest.RANDOM_TEST_NUM_ITERATIONS; ++i) {
-        int value = random().nextInt() % 5000;
-        boolean shouldAdd = random().nextBoolean();
-        if (shouldAdd) {
-          set.add(value);
-        } else {
-          set.remove(value);
-        }
-      }
-      set.clear();
-    }
-  }
-
-  @Test
-  public void testZZMySet() {
-    IntHashSet set = new IntHashSet();
-    for (int j = 0; j < 100; ++j) {
-      for (int i = 0; i < ArrayHashMapTest.RANDOM_TEST_NUM_ITERATIONS; ++i) {
-        int value = random().nextInt() % 5000;
-        boolean shouldAdd = random().nextBoolean();
-        if (shouldAdd) {
-          set.add(value);
-        } else {
-          set.remove(value);
-        }
-      }
-      set.clear();
-    }
-  }
-}
diff --git a/lucene/facet/src/test/org/apache/lucene/util/collections/IntToDoubleMapTest.java b/lucene/facet/src/test/org/apache/lucene/util/collections/IntToDoubleMapTest.java
deleted file mode 100644
index 9380cc1..0000000
--- a/lucene/facet/src/test/org/apache/lucene/util/collections/IntToDoubleMapTest.java
+++ /dev/null
@@ -1,269 +0,0 @@
-package org.apache.lucene.util.collections;
-
-import java.util.HashSet;
-import java.util.Random;
-
-import org.apache.lucene.facet.FacetTestCase;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class IntToDoubleMapTest extends FacetTestCase {
-  
-  private static void assertGround(double value) {
-    assertEquals(IntToDoubleMap.GROUND, value, Double.MAX_VALUE);
-  }
-  
-  @Test
-  public void test0() {
-    IntToDoubleMap map = new IntToDoubleMap();
-
-    assertGround(map.get(0));
-    
-    for (int i = 0; i < 100; ++i) {
-      int value = 100 + i;
-      assertFalse(map.containsValue(value));
-      map.put(i, value);
-      assertTrue(map.containsValue(value));
-      assertNotNull(map.get(i));
-    }
-
-    assertEquals(100, map.size());
-    for (int i = 0; i < 100; ++i) {
-      assertTrue(map.containsKey(i));
-      assertEquals(100 + i, map.get(i), Double.MAX_VALUE);
-
-    }
-
-    for (int i = 10; i < 90; ++i) {
-      map.remove(i);
-      assertGround(map.get(i));
-    }
-
-    assertEquals(20, map.size());
-    for (int i = 0; i < 100; ++i) {
-      assertEquals(map.containsKey(i), !(i >= 10 && i < 90));
-    }
-
-    for (int i = 5; i < 85; ++i) {
-      map.put(i, Integer.valueOf(5 + i));
-    }
-    assertEquals(95, map.size());
-    for (int i = 0; i < 100; ++i) {
-      assertEquals(map.containsKey(i), !(i >= 85 && i < 90));
-    }
-    for (int i = 0; i < 5; ++i) {
-      assertEquals(map.get(i), (100 + i), Double.MAX_VALUE);
-    }
-    for (int i = 5; i < 85; ++i) {
-      assertEquals(map.get(i), (5 + i), Double.MAX_VALUE);
-    }
-    for (int i = 90; i < 100; ++i) {
-      assertEquals(map.get(i), (100 + i), Double.MAX_VALUE);
-    }
-  }
-
-  @Test
-  public void test1() {
-    IntToDoubleMap map = new IntToDoubleMap();
-
-    for (int i = 0; i < 100; ++i) {
-      map.put(i, Integer.valueOf(100 + i));
-    }
-    
-    HashSet<Double> set = new HashSet<Double>();
-    
-    for (DoubleIterator iterator = map.iterator(); iterator.hasNext();) {
-      set.add(iterator.next());
-    }
-
-    assertEquals(set.size(), map.size());
-    for (int i = 0; i < 100; ++i) {
-      assertTrue(set.contains(Double.valueOf(100+i)));
-    }
-
-    set.clear();
-    for (DoubleIterator iterator = map.iterator(); iterator.hasNext();) {
-      double d = iterator.next();
-      if (d % 2 == 1) {
-        iterator.remove();
-        continue;
-      }
-      set.add(d);
-    }
-    assertEquals(set.size(), map.size());
-    for (int i = 0; i < 100; i+=2) {
-      assertTrue(set.contains(Double.valueOf(100+i)));
-    }
-  }
-  
-  @Test
-  public void test2() {
-    IntToDoubleMap map = new IntToDoubleMap();
-
-    assertTrue(map.isEmpty());
-    assertGround(map.get(0));
-    for (int i = 0; i < 128; ++i) {
-      int value = i * 4096;
-      assertFalse(map.containsValue(value));
-      map.put(i, value);
-      assertTrue(map.containsValue(value));
-      assertNotNull(map.get(i));
-      assertFalse(map.isEmpty());
-    }
-
-    assertEquals(128, map.size());
-    for (int i = 0; i < 128; ++i) {
-      assertTrue(map.containsKey(i));
-      assertEquals(i * 4096, map.get(i), Double.MAX_VALUE);
-    }
-    
-    for (int i = 0 ; i < 200; i+=2) {
-      map.remove(i);
-    }
-    assertEquals(64, map.size());
-    for (int i = 1; i < 128; i+=2) {
-      assertTrue(map.containsKey(i));
-      assertEquals(i * 4096, map.get(i), Double.MAX_VALUE);
-      map.remove(i);
-    }
-    assertTrue(map.isEmpty());
-  }
-  
-  @Test
-  public void test3() {
-    IntToDoubleMap map = new IntToDoubleMap();
-    int length = 100;
-    for (int i = 0; i < length; ++i) {
-      map.put(i*64, 100 + i);
-    }
-    HashSet<Integer> keySet = new HashSet<Integer>();
-    for (IntIterator iit = map.keyIterator(); iit.hasNext(); ) {
-      keySet.add(iit.next());
-    }
-    assertEquals(length, keySet.size());
-    for (int i = 0; i < length; ++i) {
-      assertTrue(keySet.contains(i * 64));
-    }
-    
-    HashSet<Double> valueSet = new HashSet<Double>();
-    for (DoubleIterator iit = map.iterator(); iit.hasNext(); ) {
-      valueSet.add(iit.next());
-    }
-    assertEquals(length, valueSet.size());
-    double[] array = map.toArray();
-    assertEquals(length, array.length);
-    for (double value: array) {
-      assertTrue(valueSet.contains(value));
-    }
-    
-    double[] array2 = new double[80];
-    array2 = map.toArray(array2);
-    assertEquals(length, array2.length);
-    for (double value: array2) {
-      assertTrue(valueSet.contains(value));
-    }
-    
-    double[] array3 = new double[120];
-    array3 = map.toArray(array3);
-    for (int i = 0 ;i < length; ++i) {
-      assertTrue(valueSet.contains(array3[i]));
-    }
-    
-    for (int i = 0; i < length; ++i) {
-      assertTrue(map.containsValue(i + 100));
-      assertTrue(map.containsKey(i*64));
-    }
-    
-    for (IntIterator iit = map.keyIterator(); iit.hasNext(); ) {
-      iit.next();
-      iit.remove();
-    }
-    assertTrue(map.isEmpty());
-    assertEquals(0, map.size());
-    
-  }
-
-  // now with random data.. and lots of it
-  @Test
-  public void test4() {
-    IntToDoubleMap map = new IntToDoubleMap();
-    int length = ArrayHashMapTest.RANDOM_TEST_NUM_ITERATIONS;
-    // for a repeatable random sequence
-    long seed = random().nextLong();
-    Random random = new Random(seed);
-    
-    for (int i = 0; i < length; ++i) {
-      int value = random.nextInt(Integer.MAX_VALUE);
-      map.put(i*128, value);
-    }
-
-    assertEquals(length, map.size());
-
-    // now repeat
-    random.setSeed(seed);
-
-    for (int i = 0; i < length; ++i) {
-      int value = random.nextInt(Integer.MAX_VALUE);
-      assertTrue(map.containsValue(value));
-      assertTrue(map.containsKey(i*128));
-      assertEquals(0, Double.compare(value, map.remove(i*128)));
-    }
-    assertEquals(0, map.size());
-    assertTrue(map.isEmpty());
-  }
-  
-  @Test
-  public void testEquals() {
-    IntToDoubleMap map1 = new IntToDoubleMap(100);
-    IntToDoubleMap map2 = new IntToDoubleMap(100);
-    assertEquals("Empty maps should be equal", map1, map2);
-    assertEquals("hashCode() for empty maps should be equal", 
-        map1.hashCode(), map2.hashCode());
-    
-    for (int i = 0; i < 100; ++i) {
-      map1.put(i, Float.valueOf(1f/i));
-      map2.put(i, Float.valueOf(1f/i));
-    }
-    assertEquals("Identical maps should be equal", map1, map2);
-    assertEquals("hashCode() for identical maps should be equal", 
-        map1.hashCode(), map2.hashCode());
-
-    for (int i = 10; i < 20; i++) {
-      map1.remove(i);
-    }
-    assertFalse("Different maps should not be equal", map1.equals(map2));
-    
-    for (int i = 19; i >=10; --i) {
-      map2.remove(i);
-    }
-    assertEquals("Identical maps should be equal", map1, map2);
-    assertEquals("hashCode() for identical maps should be equal", 
-        map1.hashCode(), map2.hashCode());
-    
-    map1.put(-1,-1f);
-    map2.put(-1,-1.1f);
-    assertFalse("Different maps should not be equal", map1.equals(map2));
-    
-    map2.put(-1,-1f);
-    assertEquals("Identical maps should be equal", map1, map2);
-    assertEquals("hashCode() for identical maps should be equal", 
-        map1.hashCode(), map2.hashCode());
-  }
-  
-}
diff --git a/lucene/facet/src/test/org/apache/lucene/util/collections/IntToFloatMapTest.java b/lucene/facet/src/test/org/apache/lucene/util/collections/IntToFloatMapTest.java
deleted file mode 100644
index 4c9fc88..0000000
--- a/lucene/facet/src/test/org/apache/lucene/util/collections/IntToFloatMapTest.java
+++ /dev/null
@@ -1,269 +0,0 @@
-package org.apache.lucene.util.collections;
-
-import java.util.HashSet;
-import java.util.Random;
-
-import org.apache.lucene.facet.FacetTestCase;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class IntToFloatMapTest extends FacetTestCase {
-  
-  private static void assertGround(float value) {
-    assertEquals(IntToFloatMap.GROUND, value, Float.MAX_VALUE);
-  }
-  
-  @Test
-  public void test0() {
-    IntToFloatMap map = new IntToFloatMap();
-
-    assertGround(map.get(0));
-    
-    for (int i = 0; i < 100; ++i) {
-      int value = 100 + i;
-      assertFalse(map.containsValue(value));
-      map.put(i, value);
-      assertTrue(map.containsValue(value));
-      assertNotNull(map.get(i));
-    }
-
-    assertEquals(100, map.size());
-    for (int i = 0; i < 100; ++i) {
-      assertTrue(map.containsKey(i));
-      assertEquals(100 + i, map.get(i), Float.MAX_VALUE);
-
-    }
-
-    for (int i = 10; i < 90; ++i) {
-      map.remove(i);
-      assertGround(map.get(i));
-    }
-
-    assertEquals(20, map.size());
-    for (int i = 0; i < 100; ++i) {
-      assertEquals(map.containsKey(i), !(i >= 10 && i < 90));
-    }
-
-    for (int i = 5; i < 85; ++i) {
-      map.put(i, Integer.valueOf(5 + i));
-    }
-    assertEquals(95, map.size());
-    for (int i = 0; i < 100; ++i) {
-      assertEquals(map.containsKey(i), !(i >= 85 && i < 90));
-    }
-    for (int i = 0; i < 5; ++i) {
-      assertEquals(map.get(i), (100 + i), Float.MAX_VALUE);
-    }
-    for (int i = 5; i < 85; ++i) {
-      assertEquals(map.get(i), (5 + i), Float.MAX_VALUE);
-    }
-    for (int i = 90; i < 100; ++i) {
-      assertEquals(map.get(i), (100 + i), Float.MAX_VALUE);
-    }
-  }
-
-  @Test
-  public void test1() {
-    IntToFloatMap map = new IntToFloatMap();
-
-    for (int i = 0; i < 100; ++i) {
-      map.put(i, Integer.valueOf(100 + i));
-    }
-    
-    HashSet<Float> set = new HashSet<Float>();
-    
-    for (FloatIterator iterator = map.iterator(); iterator.hasNext();) {
-      set.add(iterator.next());
-    }
-
-    assertEquals(set.size(), map.size());
-    for (int i = 0; i < 100; ++i) {
-      assertTrue(set.contains(Float.valueOf(100+i)));
-    }
-
-    set.clear();
-    for (FloatIterator iterator = map.iterator(); iterator.hasNext();) {
-      float d = iterator.next();
-      if (d % 2 == 1) {
-        iterator.remove();
-        continue;
-      }
-      set.add(d);
-    }
-    assertEquals(set.size(), map.size());
-    for (int i = 0; i < 100; i+=2) {
-      assertTrue(set.contains(Float.valueOf(100+i)));
-    }
-  }
-  
-  @Test
-  public void test2() {
-    IntToFloatMap map = new IntToFloatMap();
-
-    assertTrue(map.isEmpty());
-    assertGround(map.get(0));
-    for (int i = 0; i < 128; ++i) {
-      int value = i * 4096;
-      assertFalse(map.containsValue(value));
-      map.put(i, value);
-      assertTrue(map.containsValue(value));
-      assertNotNull(map.get(i));
-      assertFalse(map.isEmpty());
-    }
-
-    assertEquals(128, map.size());
-    for (int i = 0; i < 128; ++i) {
-      assertTrue(map.containsKey(i));
-      assertEquals(i * 4096, map.get(i), Float.MAX_VALUE);
-    }
-    
-    for (int i = 0 ; i < 200; i+=2) {
-      map.remove(i);
-    }
-    assertEquals(64, map.size());
-    for (int i = 1; i < 128; i+=2) {
-      assertTrue(map.containsKey(i));
-      assertEquals(i * 4096, map.get(i), Float.MAX_VALUE);
-      map.remove(i);
-    }
-    assertTrue(map.isEmpty());
-  }
-  
-  @Test
-  public void test3() {
-    IntToFloatMap map = new IntToFloatMap();
-    int length = 100;
-    for (int i = 0; i < length; ++i) {
-      map.put(i*64, 100 + i);
-    }
-    HashSet<Integer> keySet = new HashSet<Integer>();
-    for (IntIterator iit = map.keyIterator(); iit.hasNext(); ) {
-      keySet.add(iit.next());
-    }
-    assertEquals(length, keySet.size());
-    for (int i = 0; i < length; ++i) {
-      assertTrue(keySet.contains(i * 64));
-    }
-    
-    HashSet<Float> valueSet = new HashSet<Float>();
-    for (FloatIterator iit = map.iterator(); iit.hasNext(); ) {
-      valueSet.add(iit.next());
-    }
-    assertEquals(length, valueSet.size());
-    float[] array = map.toArray();
-    assertEquals(length, array.length);
-    for (float value: array) {
-      assertTrue(valueSet.contains(value));
-    }
-    
-    float[] array2 = new float[80];
-    array2 = map.toArray(array2);
-    assertEquals(length, array2.length);
-    for (float value: array2) {
-      assertTrue(valueSet.contains(value));
-    }
-    
-    float[] array3 = new float[120];
-    array3 = map.toArray(array3);
-    for (int i = 0 ;i < length; ++i) {
-      assertTrue(valueSet.contains(array3[i]));
-    }
-    
-    for (int i = 0; i < length; ++i) {
-      assertTrue(map.containsValue(i + 100));
-      assertTrue(map.containsKey(i*64));
-    }
-    
-    for (IntIterator iit = map.keyIterator(); iit.hasNext(); ) {
-      iit.next();
-      iit.remove();
-    }
-    assertTrue(map.isEmpty());
-    assertEquals(0, map.size());
-    
-  }
-
-  // now with random data.. and lots of it
-  @Test
-  public void test4() {
-    IntToFloatMap map = new IntToFloatMap();
-    int length = ArrayHashMapTest.RANDOM_TEST_NUM_ITERATIONS;
-    // for a repeatable random sequence
-    long seed = random().nextLong();
-    Random random = new Random(seed);
-    
-    for (int i = 0; i < length; ++i) {
-      int value = random.nextInt(Integer.MAX_VALUE);
-      map.put(i*128, value);
-    }
-
-    assertEquals(length, map.size());
-
-    // now repeat
-    random.setSeed(seed);
-
-    for (int i = 0; i < length; ++i) {
-      int value = random.nextInt(Integer.MAX_VALUE);
-      assertTrue(map.containsValue(value));
-      assertTrue(map.containsKey(i*128));
-      assertEquals(0, Float.compare(value, map.remove(i*128)));
-    }
-    assertEquals(0, map.size());
-    assertTrue(map.isEmpty());
-  }
-  
-  @Test
-  public void testEquals() {
-    IntToFloatMap map1 = new IntToFloatMap(100);
-    IntToFloatMap map2 = new IntToFloatMap(100);
-    assertEquals("Empty maps should be equal", map1, map2);
-    assertEquals("hashCode() for empty maps should be equal", 
-        map1.hashCode(), map2.hashCode());
-    
-    for (int i = 0; i < 100; ++i) {
-      map1.put(i, Float.valueOf(1f/i));
-      map2.put(i, Float.valueOf(1f/i));
-    }
-    assertEquals("Identical maps should be equal", map1, map2);
-    assertEquals("hashCode() for identical maps should be equal", 
-        map1.hashCode(), map2.hashCode());
-
-    for (int i = 10; i < 20; i++) {
-      map1.remove(i);
-    }
-    assertFalse("Different maps should not be equal", map1.equals(map2));
-    
-    for (int i = 19; i >=10; --i) {
-      map2.remove(i);
-    }
-    assertEquals("Identical maps should be equal", map1, map2);
-    assertEquals("hashCode() for identical maps should be equal", 
-        map1.hashCode(), map2.hashCode());
-    
-    map1.put(-1,-1f);
-    map2.put(-1,-1.1f);
-    assertFalse("Different maps should not be equal", map1.equals(map2));
-    
-    map2.put(-1,-1f);
-    assertEquals("Identical maps should be equal", map1, map2);
-    assertEquals("hashCode() for identical maps should be equal", 
-        map1.hashCode(), map2.hashCode());
-  }
-  
-}
diff --git a/lucene/facet/src/test/org/apache/lucene/util/collections/IntToIntMapTest.java b/lucene/facet/src/test/org/apache/lucene/util/collections/IntToIntMapTest.java
deleted file mode 100644
index 6877a89..0000000
--- a/lucene/facet/src/test/org/apache/lucene/util/collections/IntToIntMapTest.java
+++ /dev/null
@@ -1,270 +0,0 @@
-package org.apache.lucene.util.collections;
-
-import java.util.HashSet;
-import java.util.Random;
-
-import org.apache.lucene.facet.FacetTestCase;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class IntToIntMapTest extends FacetTestCase {
-  
-  private static void assertGround(int value) {
-    assertEquals(IntToIntMap.GROUD, value);
-  }
-  
-  @Test
-  public void test0() {
-    IntToIntMap map = new IntToIntMap();
-
-    assertGround(map.get(0));
-    
-    for (int i = 0; i < 100; ++i) {
-      int value = 100 + i;
-      assertFalse(map.containsValue(value));
-      map.put(i, value);
-      assertTrue(map.containsValue(value));
-      assertNotNull(map.get(i));
-    }
-
-    assertEquals(100, map.size());
-    for (int i = 0; i < 100; ++i) {
-      assertTrue(map.containsKey(i));
-      assertEquals(100 + i, map.get(i));
-
-    }
-
-    for (int i = 10; i < 90; ++i) {
-      map.remove(i);
-      assertGround(map.get(i));
-    }
-
-    assertEquals(20, map.size());
-    for (int i = 0; i < 100; ++i) {
-      assertEquals(map.containsKey(i), !(i >= 10 && i < 90));
-    }
-
-    for (int i = 5; i < 85; ++i) {
-      map.put(i, Integer.valueOf(5 + i));
-    }
-    assertEquals(95, map.size());
-    for (int i = 0; i < 100; ++i) {
-      assertEquals(map.containsKey(i), !(i >= 85 && i < 90));
-    }
-    for (int i = 0; i < 5; ++i) {
-      assertEquals(map.get(i), (100 + i));
-    }
-    for (int i = 5; i < 85; ++i) {
-      assertEquals(map.get(i), (5 + i));
-    }
-    for (int i = 90; i < 100; ++i) {
-      assertEquals(map.get(i), (100 + i));
-    }
-  }
-
-  @Test
-  public void test1() {
-    IntToIntMap map = new IntToIntMap();
-
-    for (int i = 0; i < 100; ++i) {
-      map.put(i, Integer.valueOf(100 + i));
-    }
-    
-    HashSet<Integer> set = new HashSet<Integer>();
-    
-    for (IntIterator iterator = map.iterator(); iterator.hasNext();) {
-      set.add(iterator.next());
-    }
-
-    assertEquals(set.size(), map.size());
-    for (int i = 0; i < 100; ++i) {
-      assertTrue(set.contains(Integer.valueOf(100+i)));
-    }
-
-    set.clear();
-    for (IntIterator iterator = map.iterator(); iterator.hasNext();) {
-      Integer integer = iterator.next();
-      if (integer % 2 == 1) {
-        iterator.remove();
-        continue;
-      }
-      set.add(integer);
-    }
-    assertEquals(set.size(), map.size());
-    for (int i = 0; i < 100; i+=2) {
-      assertTrue(set.contains(Integer.valueOf(100+i)));
-    }
-  }
-  
-  @Test
-  public void test2() {
-    IntToIntMap map = new IntToIntMap();
-
-    assertTrue(map.isEmpty());
-    assertGround(map.get(0));
-    for (int i = 0; i < 128; ++i) {
-      int value = i * 4096;
-      assertFalse(map.containsValue(value));
-      map.put(i, value);
-      assertTrue(map.containsValue(value));
-      assertNotNull(map.get(i));
-      assertFalse(map.isEmpty());
-    }
-
-    assertEquals(128, map.size());
-    for (int i = 0; i < 128; ++i) {
-      assertTrue(map.containsKey(i));
-      assertEquals(i * 4096, map.get(i));
-    }
-    
-    for (int i = 0 ; i < 200; i+=2) {
-      map.remove(i);
-    }
-    assertEquals(64, map.size());
-    for (int i = 1; i < 128; i+=2) {
-      assertTrue(map.containsKey(i));
-      assertEquals(i * 4096, map.get(i));
-      map.remove(i);
-    }
-    assertTrue(map.isEmpty());
-  }
-  
-  @Test
-  public void test3() {
-    IntToIntMap map = new IntToIntMap();
-    int length = 100;
-    for (int i = 0; i < length; ++i) {
-      map.put(i*64, 100 + i);
-    }
-    HashSet<Integer> keySet = new HashSet<Integer>();
-    for (IntIterator iit = map.keyIterator(); iit.hasNext(); ) {
-      keySet.add(iit.next());
-    }
-    assertEquals(length, keySet.size());
-    for (int i = 0; i < length; ++i) {
-      assertTrue(keySet.contains(i * 64));
-    }
-    
-    HashSet<Integer> valueSet = new HashSet<Integer>();
-    for (IntIterator iit = map.iterator(); iit.hasNext(); ) {
-      valueSet.add(iit.next());
-    }
-    assertEquals(length, valueSet.size());
-    int[] array = map.toArray();
-    assertEquals(length, array.length);
-    for (int value: array) {
-      assertTrue(valueSet.contains(value));
-    }
-    
-    int[] array2 = new int[80];
-    array2 = map.toArray(array2);
-    assertEquals(length, array2.length);
-    for (int value: array2) {
-      assertTrue(valueSet.contains(value));
-    }
-    
-    int[] array3 = new int[120];
-    array3 = map.toArray(array3);
-    for (int i = 0 ;i < length; ++i) {
-      assertTrue(valueSet.contains(array3[i]));
-    }
-    
-    for (int i = 0; i < length; ++i) {
-      assertTrue(map.containsValue(i + 100));
-      assertTrue(map.containsKey(i*64));
-    }
-    
-    for (IntIterator iit = map.keyIterator(); iit.hasNext(); ) {
-      iit.next();
-      iit.remove();
-    }
-    assertTrue(map.isEmpty());
-    assertEquals(0, map.size());
-    
-  }
-
-  // now with random data.. and lots of it
-  @Test
-  public void test4() {
-    IntToIntMap map = new IntToIntMap();
-    int length = ArrayHashMapTest.RANDOM_TEST_NUM_ITERATIONS;
-    
-    // for a repeatable random sequence
-    long seed = random().nextLong();
-    Random random = new Random(seed);
-    
-    for (int i = 0; i < length; ++i) {
-      int value = random.nextInt(Integer.MAX_VALUE);
-      map.put(i*128, value);
-    }
-
-    assertEquals(length, map.size());
-
-    // now repeat
-    random.setSeed(seed);
-
-    for (int i = 0; i < length; ++i) {
-      int value = random.nextInt(Integer.MAX_VALUE);
-      
-      assertTrue(map.containsValue(value));
-      assertTrue(map.containsKey(i*128));
-      assertEquals(value, map.remove(i*128));
-    }
-    assertEquals(0, map.size());
-    assertTrue(map.isEmpty());
-  }
-  
-  @Test
-  public void testEquals() {
-    IntToIntMap map1 = new IntToIntMap(100);
-    IntToIntMap map2 = new IntToIntMap(100);
-    assertEquals("Empty maps should be equal", map1, map2);
-    assertEquals("hashCode() for empty maps should be equal", 
-        map1.hashCode(), map2.hashCode());
-    
-    for (int i = 0; i < 100; ++i) {
-      map1.put(i, 100 + i);
-      map2.put(i, 100 + i);
-    }
-    assertEquals("Identical maps should be equal", map1, map2);
-    assertEquals("hashCode() for identical maps should be equal", 
-        map1.hashCode(), map2.hashCode());
-
-    for (int i = 10; i < 20; i++) {
-      map1.remove(i);
-    }
-    assertFalse("Different maps should not be equal", map1.equals(map2));
-    
-    for (int i = 19; i >=10; --i) {
-      map2.remove(i);
-    }
-    assertEquals("Identical maps should be equal", map1, map2);
-    assertEquals("hashCode() for identical maps should be equal", 
-        map1.hashCode(), map2.hashCode());
-    
-    map1.put(-1,-1);
-    map2.put(-1,-2);
-    assertFalse("Different maps should not be equal", map1.equals(map2));
-    
-    map2.put(-1,-1);
-    assertEquals("Identical maps should be equal", map1, map2);
-    assertEquals("hashCode() for identical maps should be equal", 
-        map1.hashCode(), map2.hashCode());
-  }
-}
diff --git a/lucene/facet/src/test/org/apache/lucene/util/collections/IntToObjectMapTest.java b/lucene/facet/src/test/org/apache/lucene/util/collections/IntToObjectMapTest.java
deleted file mode 100644
index 9133745..0000000
--- a/lucene/facet/src/test/org/apache/lucene/util/collections/IntToObjectMapTest.java
+++ /dev/null
@@ -1,265 +0,0 @@
-package org.apache.lucene.util.collections;
-
-import java.util.HashSet;
-import java.util.Iterator;
-import java.util.Random;
-
-import org.apache.lucene.facet.FacetTestCase;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class IntToObjectMapTest extends FacetTestCase {
-
-  @Test
-  public void test0() {
-    IntToObjectMap<Integer> map = new IntToObjectMap<Integer>();
-
-    assertNull(map.get(0));
-
-    for (int i = 0; i < 100; ++i) {
-      int value = 100 + i;
-      assertFalse(map.containsValue(value));
-      map.put(i, value);
-      assertTrue(map.containsValue(value));
-      assertNotNull(map.get(i));
-    }
-
-    assertEquals(100, map.size());
-    for (int i = 0; i < 100; ++i) {
-      assertTrue(map.containsKey(i));
-      assertEquals(100 + i, map.get(i).intValue());
-
-    }
-
-    for (int i = 10; i < 90; ++i) {
-      map.remove(i);
-      assertNull(map.get(i));
-    }
-
-    assertEquals(20, map.size());
-    for (int i = 0; i < 100; ++i) {
-      assertEquals(map.containsKey(i), !(i >= 10 && i < 90));
-    }
-
-    for (int i = 5; i < 85; ++i) {
-      map.put(i, Integer.valueOf(5 + i));
-    }
-    assertEquals(95, map.size());
-    for (int i = 0; i < 100; ++i) {
-      assertEquals(map.containsKey(i), !(i >= 85 && i < 90));
-    }
-    for (int i = 0; i < 5; ++i) {
-      assertEquals(map.get(i).intValue(), (100 + i));
-    }
-    for (int i = 5; i < 85; ++i) {
-      assertEquals(map.get(i).intValue(), (5 + i));
-    }
-    for (int i = 90; i < 100; ++i) {
-      assertEquals(map.get(i).intValue(), (100 + i));
-    }
-  }
-
-  @Test
-  public void test1() {
-    IntToObjectMap<Integer> map = new IntToObjectMap<Integer>();
-
-    for (int i = 0; i < 100; ++i) {
-      map.put(i, Integer.valueOf(100 + i));
-    }
-
-    HashSet<Integer> set = new HashSet<Integer>();
-
-    for (Iterator<Integer> iterator = map.iterator(); iterator.hasNext();) {
-      set.add(iterator.next());
-    }
-
-    assertEquals(set.size(), map.size());
-    for (int i = 0; i < 100; ++i) {
-      assertTrue(set.contains(Integer.valueOf(100 + i)));
-    }
-
-    set.clear();
-    for (Iterator<Integer> iterator = map.iterator(); iterator.hasNext();) {
-      Integer integer = iterator.next();
-      if (integer % 2 == 1) {
-        iterator.remove();
-        continue;
-      }
-      set.add(integer);
-    }
-    assertEquals(set.size(), map.size());
-    for (int i = 0; i < 100; i += 2) {
-      assertTrue(set.contains(Integer.valueOf(100 + i)));
-    }
-  }
-
-  @Test
-  public void test2() {
-    IntToObjectMap<Integer> map = new IntToObjectMap<Integer>();
-
-    assertTrue(map.isEmpty());
-    assertNull(map.get(0));
-    for (int i = 0; i < 128; ++i) {
-      int value = i * 4096;
-      assertFalse(map.containsValue(value));
-      map.put(i, value);
-      assertTrue(map.containsValue(value));
-      assertNotNull(map.get(i));
-      assertFalse(map.isEmpty());
-    }
-
-    assertEquals(128, map.size());
-    for (int i = 0; i < 128; ++i) {
-      assertTrue(map.containsKey(i));
-      assertEquals(i * 4096, map.get(i).intValue());
-    }
-
-    for (int i = 0; i < 200; i += 2) {
-      map.remove(i);
-    }
-    assertEquals(64, map.size());
-    for (int i = 1; i < 128; i += 2) {
-      assertTrue(map.containsKey(i));
-      assertEquals(i * 4096, map.get(i).intValue());
-      map.remove(i);
-    }
-    assertTrue(map.isEmpty());
-  }
-
-  @Test
-  public void test3() {
-    IntToObjectMap<Integer> map = new IntToObjectMap<Integer>();
-    int length = 100;
-    for (int i = 0; i < length; ++i) {
-      map.put(i * 64, 100 + i);
-    }
-    HashSet<Integer> keySet = new HashSet<Integer>();
-    for (IntIterator iit = map.keyIterator(); iit.hasNext();) {
-      keySet.add(iit.next());
-    }
-    assertEquals(length, keySet.size());
-    for (int i = 0; i < length; ++i) {
-      assertTrue(keySet.contains(i * 64));
-    }
-
-    HashSet<Integer> valueSet = new HashSet<Integer>();
-    for (Iterator<Integer> iit = map.iterator(); iit.hasNext();) {
-      valueSet.add(iit.next());
-    }
-    assertEquals(length, valueSet.size());
-    Object[] array = map.toArray();
-    assertEquals(length, array.length);
-    for (Object value : array) {
-      assertTrue(valueSet.contains(value));
-    }
-
-    Integer[] array2 = new Integer[80];
-    array2 = map.toArray(array2);
-    for (int value : array2) {
-      assertTrue(valueSet.contains(value));
-    }
-    Integer[] array3 = new Integer[120];
-    array3 = map.toArray(array3);
-    for (int i = 0; i < length; ++i) {
-      assertTrue(valueSet.contains(array3[i]));
-    }
-    assertNull(array3[length]);
-
-    for (int i = 0; i < length; ++i) {
-      assertTrue(map.containsValue(i + 100));
-      assertTrue(map.containsKey(i * 64));
-    }
-
-    for (IntIterator iit = map.keyIterator(); iit.hasNext();) {
-      iit.next();
-      iit.remove();
-    }
-    assertTrue(map.isEmpty());
-    assertEquals(0, map.size());
-
-  }
-
-  // now with random data.. and lots of it
-  @Test
-  public void test4() {
-    IntToObjectMap<Integer> map = new IntToObjectMap<Integer>();
-    int length = ArrayHashMapTest.RANDOM_TEST_NUM_ITERATIONS;
-    
-    // for a repeatable random sequence
-    long seed = random().nextLong();
-    Random random = new Random(seed);
-    
-    for (int i = 0; i < length; ++i) {
-      int value = random.nextInt(Integer.MAX_VALUE);
-      map.put(i * 128, value);
-    }
-
-    assertEquals(length, map.size());
-
-    // now repeat
-    random.setSeed(seed);
-
-    for (int i = 0; i < length; ++i) {
-      int value = random.nextInt(Integer.MAX_VALUE);
-      assertTrue(map.containsValue(value));
-      assertTrue(map.containsKey(i * 128));
-      assertEquals(Integer.valueOf(value), map.remove(i * 128));
-    }
-    assertEquals(0, map.size());
-    assertTrue(map.isEmpty());
-  }
-  
-  @Test
-  public void testEquals() {
-    IntToObjectMap<Double> map1 = new IntToObjectMap<Double>(100);
-    IntToObjectMap<Double> map2 = new IntToObjectMap<Double>(100);
-    assertEquals("Empty maps should be equal", map1, map2);
-    assertEquals("hashCode() for empty maps should be equal", 
-        map1.hashCode(), map2.hashCode());
-    
-    for (int i = 0; i < 100; ++i) {
-      map1.put(i, Double.valueOf(1f/i));
-      map2.put(i, Double.valueOf(1f/i));
-    }
-    assertEquals("Identical maps should be equal", map1, map2);
-    assertEquals("hashCode() for identical maps should be equal", 
-        map1.hashCode(), map2.hashCode());
-  
-    for (int i = 10; i < 20; i++) {
-      map1.remove(i);
-    }
-    assertFalse("Different maps should not be equal", map1.equals(map2));
-    
-    for (int i = 19; i >=10; --i) {
-      map2.remove(i);
-    }
-    assertEquals("Identical maps should be equal", map1, map2);
-    assertEquals("hashCode() for identical maps should be equal", 
-        map1.hashCode(), map2.hashCode());
-    
-    map1.put(-1,-1d);
-    map2.put(-1,-1.1d);
-    assertFalse("Different maps should not be equal", map1.equals(map2));
-    
-    map2.put(-1,-1d);
-    assertEquals("Identical maps should be equal", map1, map2);
-    assertEquals("hashCode() for identical maps should be equal", 
-        map1.hashCode(), map2.hashCode());
-  }
-}
diff --git a/lucene/facet/src/test/org/apache/lucene/util/collections/ObjectToFloatMapTest.java b/lucene/facet/src/test/org/apache/lucene/util/collections/ObjectToFloatMapTest.java
deleted file mode 100644
index 7d00a16..0000000
--- a/lucene/facet/src/test/org/apache/lucene/util/collections/ObjectToFloatMapTest.java
+++ /dev/null
@@ -1,277 +0,0 @@
-package org.apache.lucene.util.collections;
-
-import java.util.HashSet;
-import java.util.Iterator;
-import java.util.Random;
-
-import org.apache.lucene.facet.FacetTestCase;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class ObjectToFloatMapTest extends FacetTestCase {
-
-  @Test
-  public void test0() {
-    ObjectToFloatMap<Integer> map = new ObjectToFloatMap<Integer>();
-
-    assertNaN(map.get(0));
-    
-    for (int i = 0; i < 100; ++i) {
-      int value = 100 + i;
-      assertFalse(map.containsValue(value));
-      map.put(i, value);
-      assertTrue(map.containsValue(value));
-      assertNotNaN(map.get(i));
-    }
-
-    assertEquals(100, map.size());
-    for (int i = 0; i < 100; ++i) {
-      assertTrue(map.containsKey(i));
-      assertEquals(100 + i, map.get(i), 1E-5);
-
-    }
-
-    for (int i = 10; i < 90; ++i) {
-      map.remove(i);
-      assertNaN(map.get(i));
-    }
-
-    assertEquals(20, map.size());
-    for (int i = 0; i < 100; ++i) {
-      assertEquals(map.containsKey(i), !(i >= 10 && i < 90));
-    }
-
-    for (int i = 5; i < 85; ++i) {
-      map.put(i, Integer.valueOf(5 + i));
-    }
-    assertEquals(95, map.size());
-    for (int i = 0; i < 100; ++i) {
-      assertEquals(map.containsKey(i), !(i >= 85 && i < 90));
-    }
-    for (int i = 0; i < 5; ++i) {
-      assertEquals(map.get(i), (100 + i), 1E-5);
-    }
-    for (int i = 5; i < 85; ++i) {
-      assertEquals(map.get(i), (5 + i), 1E-5);
-    }
-    for (int i = 90; i < 100; ++i) {
-      assertEquals(map.get(i), (100 + i), 1E-5);
-    }
-  }
-
-  private static void assertNaN(float f) {
-    assertTrue(Float.isNaN(f));
-  }
-  
-  private static void assertNotNaN(float f) {
-    assertFalse(Float.isNaN(f));
-  }
-
-  @Test
-  public void test1() {
-    ObjectToFloatMap<Integer> map = new ObjectToFloatMap<Integer>();
-
-    for (int i = 0; i < 100; ++i) {
-      map.put(i, Integer.valueOf(100 + i));
-    }
-    
-    HashSet<Float> set = new HashSet<Float>();
-    
-    for (FloatIterator iterator = map.iterator(); iterator.hasNext();) {
-      set.add(iterator.next());
-    }
-
-    assertEquals(set.size(), map.size());
-    for (int i = 0; i < 100; ++i) {
-      assertTrue(set.contains(Float.valueOf(100+i)));
-    }
-
-    set.clear();
-    for (FloatIterator iterator = map.iterator(); iterator.hasNext();) {
-      Float value = iterator.next();
-      if (value % 2 == 1) {
-        iterator.remove();
-        continue;
-      }
-      set.add(value);
-    }
-    assertEquals(set.size(), map.size());
-    for (int i = 0; i < 100; i+=2) {
-      assertTrue(set.contains(Float.valueOf(100+i)));
-    }
-  }
-  
-  @Test
-  public void test2() {
-    ObjectToFloatMap<Integer> map = new ObjectToFloatMap<Integer>();
-
-    assertTrue(map.isEmpty());
-    assertNaN(map.get(0));
-    for (int i = 0; i < 128; ++i) {
-      int value = i * 4096;
-      assertFalse(map.containsValue(value));
-      map.put(i, value);
-      assertTrue(map.containsValue(value));
-      assertNotNaN(map.get(i));
-      assertFalse(map.isEmpty());
-    }
-
-    assertEquals(128, map.size());
-    for (int i = 0; i < 128; ++i) {
-      assertTrue(map.containsKey(i));
-      assertEquals(i * 4096, map.get(i), 1E-5);
-    }
-    
-    for (int i = 0 ; i < 200; i+=2) {
-      map.remove(i);
-    }
-    assertEquals(64, map.size());
-    for (int i = 1; i < 128; i+=2) {
-      assertTrue(map.containsKey(i));
-      assertEquals(i * 4096, map.get(i), 1E-5);
-      map.remove(i);
-    }
-    assertTrue(map.isEmpty());
-  }
-  
-  @Test
-  public void test3() {
-    ObjectToFloatMap<Integer> map = new ObjectToFloatMap<Integer>();
-    int length = 100;
-    for (int i = 0; i < length; ++i) {
-      map.put(i*64, 100 + i);
-    }
-    HashSet<Integer> keySet = new HashSet<Integer>();
-    for (Iterator<Integer> iit = map.keyIterator(); iit.hasNext(); ) {
-      keySet.add(iit.next());
-    }
-    assertEquals(length, keySet.size());
-    for (int i = 0; i < length; ++i) {
-      assertTrue(keySet.contains(i * 64));
-    }
-    
-    HashSet<Float> valueSet = new HashSet<Float>();
-    for (FloatIterator iit = map.iterator(); iit.hasNext(); ) {
-      valueSet.add(iit.next());
-    }
-    assertEquals(length, valueSet.size());
-    float[] array = map.toArray();
-    assertEquals(length, array.length);
-    for (float value: array) {
-      assertTrue(valueSet.contains(value));
-    }
-    
-    float[] array2 = new float[80];
-    array2 = map.toArray(array2);
-    assertEquals(80, array2.length);
-    for (float value: array2) {
-      assertTrue(valueSet.contains(value));
-    }
-    
-    float[] array3 = new float[120];
-    array3 = map.toArray(array3);
-    for (int i = 0 ;i < length; ++i) {
-      assertTrue(valueSet.contains(array3[i]));
-    }
-    assertNaN(array3[length]);
-    
-    for (int i = 0; i < length; ++i) {
-      assertTrue(map.containsValue(i + 100));
-      assertTrue(map.containsKey(i*64));
-    }
-    
-    for (Iterator<Integer> iit = map.keyIterator(); iit.hasNext(); ) {
-      iit.next();
-      iit.remove();
-    }
-    assertTrue(map.isEmpty());
-    assertEquals(0, map.size());
-    
-  }
-
-  // now with random data.. and lots of it
-  @Test
-  public void test4() {
-    ObjectToFloatMap<Integer> map = new ObjectToFloatMap<Integer>();
-    int length = ArrayHashMapTest.RANDOM_TEST_NUM_ITERATIONS;
-    
-    // for a repeatable random sequence
-    long seed = random().nextLong();
-    Random random = new Random(seed);
-
-    for (int i = 0; i < length; ++i) {
-      int value = random.nextInt(Integer.MAX_VALUE);
-      map.put(i*128, value);
-    }
-
-    assertEquals(length, map.size());
-
-    // now repeat
-    random.setSeed(seed);
-
-    for (int i = 0; i < length; ++i) {
-      int value = random.nextInt(Integer.MAX_VALUE);
-      
-      assertTrue(map.containsValue(value));
-      assertTrue(map.containsKey(i*128));
-      assertEquals(0, Float.compare(value, map.remove(i*128)));
-    }
-    assertEquals(0, map.size());
-    assertTrue(map.isEmpty());
-  }
-  
-  @Test
-  public void testEquals() {
-    ObjectToFloatMap<Integer> map1 = new ObjectToFloatMap<Integer>(100);
-    ObjectToFloatMap<Integer> map2 = new ObjectToFloatMap<Integer>(100);
-    assertEquals("Empty maps should be equal", map1, map2);
-    assertEquals("hashCode() for empty maps should be equal", 
-        map1.hashCode(), map2.hashCode());
-    
-    for (int i = 0; i < 100; ++i) {
-      map1.put(i, Float.valueOf(1f/i));
-      map2.put(i, Float.valueOf(1f/i));
-    }
-    assertEquals("Identical maps should be equal", map1, map2);
-    assertEquals("hashCode() for identical maps should be equal", 
-        map1.hashCode(), map2.hashCode());
-
-    for (int i = 10; i < 20; i++) {
-      map1.remove(i);
-    }
-    assertFalse("Different maps should not be equal", map1.equals(map2));
-    
-    for (int i = 19; i >=10; --i) {
-      map2.remove(i);
-    }
-    assertEquals("Identical maps should be equal", map1, map2);
-    assertEquals("hashCode() for identical maps should be equal", 
-        map1.hashCode(), map2.hashCode());
-    
-    map1.put(-1,-1f);
-    map2.put(-1,-1.1f);
-    assertFalse("Different maps should not be equal", map1.equals(map2));
-    
-    map2.put(-1,-1f);
-    assertEquals("Identical maps should be equal", map1, map2);
-    assertEquals("hashCode() for identical maps should be equal", 
-        map1.hashCode(), map2.hashCode());
-  }
-  
-}
diff --git a/lucene/facet/src/test/org/apache/lucene/util/collections/ObjectToIntMapTest.java b/lucene/facet/src/test/org/apache/lucene/util/collections/ObjectToIntMapTest.java
deleted file mode 100644
index adb3d85..0000000
--- a/lucene/facet/src/test/org/apache/lucene/util/collections/ObjectToIntMapTest.java
+++ /dev/null
@@ -1,275 +0,0 @@
-package org.apache.lucene.util.collections;
-
-import java.util.HashSet;
-import java.util.Iterator;
-import java.util.Random;
-
-import org.apache.lucene.facet.FacetTestCase;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class ObjectToIntMapTest extends FacetTestCase {
-
-  @Test
-  public void test0() {
-    ObjectToIntMap<Integer> map = new ObjectToIntMap<Integer>();
-
-    assertIntegerMaxValue(map.get(0));
-    
-    for (int i = 0; i < 100; ++i) {
-      int value = 100 + i;
-      assertFalse(map.containsValue(value));
-      map.put(i, value);
-      assertTrue(map.containsValue(value));
-      assertNotIntegerMaxValue(map.get(i));
-    }
-
-    assertEquals(100, map.size());
-    for (int i = 0; i < 100; ++i) {
-      assertTrue(map.containsKey(i));
-      assertEquals(100 + i, map.get(i), 1E-5);
-
-    }
-
-    for (int i = 10; i < 90; ++i) {
-      map.remove(i);
-      assertIntegerMaxValue(map.get(i));
-    }
-
-    assertEquals(20, map.size());
-    for (int i = 0; i < 100; ++i) {
-      assertEquals(map.containsKey(i), !(i >= 10 && i < 90));
-    }
-
-    for (int i = 5; i < 85; ++i) {
-      map.put(i, Integer.valueOf(5 + i));
-    }
-    assertEquals(95, map.size());
-    for (int i = 0; i < 100; ++i) {
-      assertEquals(map.containsKey(i), !(i >= 85 && i < 90));
-    }
-    for (int i = 0; i < 5; ++i) {
-      assertEquals(map.get(i), (100 + i), 1E-5);
-    }
-    for (int i = 5; i < 85; ++i) {
-      assertEquals(map.get(i), (5 + i), 1E-5);
-    }
-    for (int i = 90; i < 100; ++i) {
-      assertEquals(map.get(i), (100 + i), 1E-5);
-    }
-  }
-
-  private static void assertIntegerMaxValue(int i) {
-    assertTrue(i == Integer.MAX_VALUE);
-  }
-  
-  private static void assertNotIntegerMaxValue(int i) {
-    assertFalse(i == Integer.MAX_VALUE);
-  }
-
-  @Test
-  public void test1() {
-    ObjectToIntMap<Integer> map = new ObjectToIntMap<Integer>();
-
-    for (int i = 0; i < 100; ++i) {
-      map.put(i, Integer.valueOf(100 + i));
-    }
-    
-    HashSet<Integer> set = new HashSet<Integer>();
-    
-    for (IntIterator iterator = map.iterator(); iterator.hasNext();) {
-      set.add(iterator.next());
-    }
-
-    assertEquals(set.size(), map.size());
-    for (int i = 0; i < 100; ++i) {
-      assertTrue(set.contains(Integer.valueOf(100+i)));
-    }
-
-    set.clear();
-    for (IntIterator iterator = map.iterator(); iterator.hasNext();) {
-      Integer value = iterator.next();
-      if (value % 2 == 1) {
-        iterator.remove();
-        continue;
-      }
-      set.add(value);
-    }
-    assertEquals(set.size(), map.size());
-    for (int i = 0; i < 100; i+=2) {
-      assertTrue(set.contains(Integer.valueOf(100+i)));
-    }
-  }
-  
-  @Test
-  public void test2() {
-    ObjectToIntMap<Integer> map = new ObjectToIntMap<Integer>();
-
-    assertTrue(map.isEmpty());
-    assertIntegerMaxValue(map.get(0));
-    for (int i = 0; i < 128; ++i) {
-      int value = i * 4096;
-      assertFalse(map.containsValue(value));
-      map.put(i, value);
-      assertTrue(map.containsValue(value));
-      assertNotIntegerMaxValue(map.get(i));
-      assertFalse(map.isEmpty());
-    }
-
-    assertEquals(128, map.size());
-    for (int i = 0; i < 128; ++i) {
-      assertTrue(map.containsKey(i));
-      assertEquals(i * 4096, map.get(i), 1E-5);
-    }
-    
-    for (int i = 0 ; i < 200; i+=2) {
-      map.remove(i);
-    }
-    assertEquals(64, map.size());
-    for (int i = 1; i < 128; i+=2) {
-      assertTrue(map.containsKey(i));
-      assertEquals(i * 4096, map.get(i), 1E-5);
-      map.remove(i);
-    }
-    assertTrue(map.isEmpty());
-  }
-  
-  @Test
-  public void test3() {
-    ObjectToIntMap<Integer> map = new ObjectToIntMap<Integer>();
-    int length = 100;
-    for (int i = 0; i < length; ++i) {
-      map.put(i*64, 100 + i);
-    }
-    HashSet<Integer> keySet = new HashSet<Integer>();
-    for (Iterator<Integer> iit = map.keyIterator(); iit.hasNext(); ) {
-      keySet.add(iit.next());
-    }
-    assertEquals(length, keySet.size());
-    for (int i = 0; i < length; ++i) {
-      assertTrue(keySet.contains(i * 64));
-    }
-    
-    HashSet<Integer> valueSet = new HashSet<Integer>();
-    for (IntIterator iit = map.iterator(); iit.hasNext(); ) {
-      valueSet.add(iit.next());
-    }
-    assertEquals(length, valueSet.size());
-    int[] array = map.toArray();
-    assertEquals(length, array.length);
-    for (int value: array) {
-      assertTrue(valueSet.contains(value));
-    }
-    
-    int[] array2 = new int[80];
-    array2 = map.toArray(array2);
-    assertEquals(80, array2.length);
-    for (int value: array2) {
-      assertTrue(valueSet.contains(value));
-    }
-    
-    int[] array3 = new int[120];
-    array3 = map.toArray(array3);
-    for (int i = 0 ;i < length; ++i) {
-      assertTrue(valueSet.contains(array3[i]));
-    }
-    assertIntegerMaxValue(array3[length]);
-    
-    for (int i = 0; i < length; ++i) {
-      assertTrue(map.containsValue(i + 100));
-      assertTrue(map.containsKey(i*64));
-    }
-    
-    for (Iterator<Integer> iit = map.keyIterator(); iit.hasNext(); ) {
-      iit.next();
-      iit.remove();
-    }
-    assertTrue(map.isEmpty());
-    assertEquals(0, map.size());
-    
-  }
-
-  // now with random data.. and lots of it
-  @Test
-  public void test4() {
-    ObjectToIntMap<Integer> map = new ObjectToIntMap<Integer>();
-    int length = ArrayHashMapTest.RANDOM_TEST_NUM_ITERATIONS;
-    
-    // for a repeatable random sequence
-    long seed = random().nextLong();
-    Random random = new Random(seed);
-    
-    for (int i = 0; i < length; ++i) {
-      int value = random.nextInt(Integer.MAX_VALUE);
-      map.put(i*128, value);
-    }
-
-    assertEquals(length, map.size());
-    
-    // now repeat
-    random.setSeed(seed);
-
-    for (int i = 0; i < length; ++i) {
-      int value = random.nextInt(Integer.MAX_VALUE);
-      assertTrue(map.containsValue(value));
-      assertTrue(map.containsKey(i*128));
-      assertEquals(value, map.remove(i*128));
-    }
-    assertEquals(0, map.size());
-    assertTrue(map.isEmpty());
-  }
-  
-  @Test
-  public void testEquals() {
-    ObjectToIntMap<Float> map1 = new ObjectToIntMap<Float>(100);
-    ObjectToIntMap<Float> map2 = new ObjectToIntMap<Float>(100);
-    assertEquals("Empty maps should be equal", map1, map2);
-    assertEquals("hashCode() for empty maps should be equal", 
-        map1.hashCode(), map2.hashCode());
-    
-    for (int i = 0; i < 100; ++i) {
-      map1.put(i * 1.1f, 100 + i);
-      map2.put(i * 1.1f, 100 + i);
-    }
-    assertEquals("Identical maps should be equal", map1, map2);
-    assertEquals("hashCode() for identical maps should be equal", 
-        map1.hashCode(), map2.hashCode());
-
-    for (int i = 10; i < 20; i++) {
-      map1.remove(i * 1.1f);
-    }
-    assertFalse("Different maps should not be equal", map1.equals(map2));
-    
-    for (int i = 19; i >=10; --i) {
-      map2.remove(i * 1.1f);
-    }
-    assertEquals("Identical maps should be equal", map1, map2);
-    assertEquals("hashCode() for identical maps should be equal", 
-        map1.hashCode(), map2.hashCode());
-    
-    map1.put(-1.1f,-1);
-    map2.put(-1.1f,-2);
-    assertFalse("Different maps should not be equal", map1.equals(map2));
-    
-    map2.put(-1.1f,-1);
-    assertEquals("Identical maps should be equal", map1, map2);
-    assertEquals("hashCode() for identical maps should be equal", 
-        map1.hashCode(), map2.hashCode());
-  }
-}
diff --git a/lucene/facet/src/test/org/apache/lucene/util/collections/TestLRUHashMap.java b/lucene/facet/src/test/org/apache/lucene/util/collections/TestLRUHashMap.java
deleted file mode 100644
index ddf3301..0000000
--- a/lucene/facet/src/test/org/apache/lucene/util/collections/TestLRUHashMap.java
+++ /dev/null
@@ -1,59 +0,0 @@
-package org.apache.lucene.util.collections;
-
-import org.apache.lucene.facet.FacetTestCase;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class TestLRUHashMap extends FacetTestCase {
-  // testLRU() tests that the specified size limit is indeed honored, and
-  // the remaining objects in the map are indeed those that have been most
-  // recently used
-  @Test
-  public void testLRU() throws Exception {
-    LRUHashMap<String, String> lru = new LRUHashMap<String, String>(3);
-    assertEquals(0, lru.size());
-    lru.put("one", "Hello world");
-    assertEquals(1, lru.size());
-    lru.put("two", "Hi man");
-    assertEquals(2, lru.size());
-    lru.put("three", "Bonjour");
-    assertEquals(3, lru.size());
-    lru.put("four", "Shalom");
-    assertEquals(3, lru.size());
-    assertNotNull(lru.get("three"));
-    assertNotNull(lru.get("two"));
-    assertNotNull(lru.get("four"));
-    assertNull(lru.get("one"));
-    lru.put("five", "Yo!");
-    assertEquals(3, lru.size());
-    assertNull(lru.get("three")); // three was last used, so it got removed
-    assertNotNull(lru.get("five"));
-    lru.get("four");
-    lru.put("six", "hi");
-    lru.put("seven", "hey dude");
-    assertEquals(3, lru.size());
-    assertNull(lru.get("one"));
-    assertNull(lru.get("two"));
-    assertNull(lru.get("three"));
-    assertNotNull(lru.get("four"));
-    assertNull(lru.get("five"));
-    assertNotNull(lru.get("six"));
-    assertNotNull(lru.get("seven"));
-  }
-}
diff --git a/lucene/facet/src/test/org/apache/lucene/util/encoding/EncodingSpeed.java b/lucene/facet/src/test/org/apache/lucene/util/encoding/EncodingSpeed.java
deleted file mode 100644
index 9b34bda..0000000
--- a/lucene/facet/src/test/org/apache/lucene/util/encoding/EncodingSpeed.java
+++ /dev/null
@@ -1,639 +0,0 @@
-package org.apache.lucene.util.encoding;
-
-import java.io.IOException;
-import java.text.NumberFormat;
-import java.util.Arrays;
-import java.util.Locale;
-
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IntsRef;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class EncodingSpeed {
-
-  private static int[] data3630 = null;
-  private static int[] data9910 = null;
-  private static int[] data501871 = null;
-  private static int[] data10k = null;
-  private static String resultsFormat = "%-60s %10s %20d %26s %20d %26s";
-  private static String headerFormat = "%-60s %10s %20s %26s %20s %26s";
-  private static int integers = 100000000;
-
-  private static NumberFormat nf;
-
-  public static void main(String[] args) throws IOException {
-    testFacetIDs(data3630, 3630);
-    testFacetIDs(data9910, 9910);
-    testFacetIDs(data10k, 10000);
-    testFacetIDs(data501871, 501871);
-  }
-
-  private static IntsRef newIntsRef(int[] data) {
-    IntsRef res = new IntsRef(data.length);
-    System.arraycopy(data, 0, res.ints, 0, data.length);
-    res.length = data.length;
-    return res;
-  }
-  
-  private static void testFacetIDs(int[] facetIDs, int docID) throws IOException {
-    int loopFactor = integers / facetIDs.length;
-    System.out
-        .println("\nEstimating ~"
-            + integers
-            + " Integers compression time by\nEncoding/decoding facets' ID payload of docID = "
-            + docID + " (unsorted, length of: " + facetIDs.length
-            + ") " + loopFactor + " times.");
-
-    System.out.println();
-    String header = String.format(Locale.ROOT, headerFormat, "Encoder", "Bits/Int",
-        "Encode Time", "Encode Time", "Decode Time", "Decode Time");
-
-    System.out.println(header);
-    String header2 = String.format(Locale.ROOT, headerFormat, "", "", "[milliseconds]",
-        "[microsecond / int]", "[milliseconds]", "[microsecond / int]");
-
-    System.out.println(header2);
-
-    char[] separator = header.toCharArray();
-    Arrays.fill(separator, '-');
-    System.out.println(separator);
-
-    encoderTest(new VInt8IntEncoder(), facetIDs, loopFactor);
-    encoderTest(new SortingIntEncoder(new UniqueValuesIntEncoder(new VInt8IntEncoder())), facetIDs, loopFactor);
-    encoderTest(new SortingIntEncoder(new UniqueValuesIntEncoder(new DGapIntEncoder(new VInt8IntEncoder()))), facetIDs, loopFactor);
-    encoderTest(new SortingIntEncoder(new UniqueValuesIntEncoder(new DGapVInt8IntEncoder())), facetIDs, loopFactor);
-    encoderTest(new SortingIntEncoder(new UniqueValuesIntEncoder(new DGapIntEncoder(new EightFlagsIntEncoder()))), facetIDs, loopFactor);
-    encoderTest(new SortingIntEncoder(new UniqueValuesIntEncoder(new DGapIntEncoder(new FourFlagsIntEncoder()))), facetIDs, loopFactor);
-    encoderTest(new SortingIntEncoder(new UniqueValuesIntEncoder(new DGapIntEncoder(new NOnesIntEncoder(3)))), facetIDs, loopFactor);
-    encoderTest(new SortingIntEncoder(new UniqueValuesIntEncoder(new DGapIntEncoder(new NOnesIntEncoder(4)))), facetIDs, loopFactor);
-
-    System.out.println();
-  }
-
-  private static void encoderTest(IntEncoder encoder, int[] values, int loopFactor) throws IOException {
-
-    BytesRef bytes = new BytesRef(values.length); // at least one byte per value
-
-    // -- Looping 100 times as a warm up --------------------------
-    for (int i = 100; i != 0; --i) {
-      IntsRef data = newIntsRef(values);
-      encoder.encode(data, bytes);
-    }
-    // -----------------------------------------------------------
-
-    long encodeTime = 0;
-    for (int factor = loopFactor; factor > 0; --factor) {
-      IntsRef data = newIntsRef(values);
-      long start = System.currentTimeMillis();
-      encoder.encode(data, bytes);
-      encodeTime += System.currentTimeMillis() - start;
-    }
-
-    IntsRef decoded = new IntsRef(values.length);
-    int encodedSize = bytes.length;
-    IntDecoder decoder = encoder.createMatchingDecoder();
-    
-    // -- Looping 100 times as a warm up --------------------------
-    for (int i = 100; i != 0; --i) {
-      decoder.decode(bytes, decoded);
-    }
-    // -----------------------------------------------------------
-
-    long decodeTime = 0;
-    for (int i = loopFactor; i > 0; --i) {
-      long start = System.currentTimeMillis();
-      decoder.decode(bytes, decoded);
-      decodeTime += System.currentTimeMillis() - start;
-    }
-    
-    if (decoded.length != values.length) {
-      throw new RuntimeException("wrong num values. expected=" + values.length + " actual=" + decoded.length + 
-          " decoder=" + decoder);
-    }
-
-    System.out.println(String.format(Locale.ROOT, resultsFormat, encoder, 
-        nf.format(encodedSize * 8.0 / values.length), 
-        encodeTime, 
-        nf.format(encodeTime * 1000000.0 / (loopFactor * values.length)), 
-        decodeTime, 
-        nf.format(decodeTime * 1000000.0 / (loopFactor * values.length))));
-  }
-
-  static {
-    nf = NumberFormat.getInstance(Locale.ROOT);
-    nf.setMaximumFractionDigits(4);
-    nf.setMinimumFractionDigits(4);
-
-    data9910 = new int[] { 2, 4, 149085, 11, 12292, 69060, 69061, 149309,
-        99090, 568, 5395, 149310, 3911, 149311, 149312, 148752, 1408,
-        1410, 1411, 1412, 4807, 1413, 1414, 1417, 1415, 1416, 1418,
-        1420, 470, 4808, 1422, 1423, 1424, 4809, 4810, 1427, 1429,
-        1430, 4811, 1432, 1433, 3752, 1435, 3753, 1437, 1439, 1440,
-        4812, 1442, 1443, 4813, 1445, 1446, 1447, 4814, 4815, 1450,
-        4816, 353, 1452, 89004, 1624, 1625, 2052, 1626, 1627, 63991,
-        725, 726, 727, 728, 35543, 729, 730, 731, 1633, 733, 734, 735,
-        37954, 737, 738, 76315, 23068, 76316, 1634, 740, 741, 742, 744,
-        745, 76317, 15645, 748, 17488, 2904, 89005, 752, 753, 89006,
-        754, 755, 756, 757, 41, 261, 758, 89007, 760, 762, 763, 89008,
-        764, 765, 766, 85930, 165, 768, 149313, 33593, 149314, 149315,
-        81589, 39456, 15467, 1296, 149316, 39457, 2235, 144, 2236,
-        2309, 3050, 2237, 2311, 89692, 2240, 2241, 2243, 2244, 2245,
-        2246, 2314, 12856, 2248, 2250, 2251, 2253, 2254, 12857, 7677,
-        12858, 39149, 2257, 23147, 3303, 2258, 7422, 2322, 2262, 2317,
-        2263, 7423, 24264, 2232, 89693, 12862, 89694, 12863, 12864,
-        23201, 2329, 33019, 2255, 12865, 3517, 2492, 2277, 2280, 2267,
-        2260, 25368, 12866, 2281, 2282, 2283, 12867, 2284, 9055, 2287,
-        125133, 2337, 2286, 2288, 2338, 125134, 2290, 125135, 12869,
-        965, 966, 1298, 17945, 1300, 970, 971, 972, 973, 974, 296,
-        17946, 1303, 1391, 902, 1304, 1395, 1308, 1309, 1310, 1312,
-        967, 9414, 1315, 1317, 1318, 9415, 1321, 23592, 1322, 22433,
-        1323, 1324, 1326, 109241, 31225, 1330, 1331, 2540, 27196, 1332,
-        1334, 1335, 11999, 414, 340, 3651, 44040, 31995, 1344, 1343,
-        4618, 116770, 116771, 1474, 1349, 42122, 14199, 149317, 451,
-        149318, 29, 14200, 14198, 14201, 1979, 1980, 1981, 3132, 3147,
-        34090, 1987, 12770, 1329, 80818, 80819, 1988, 23522, 1986,
-        15880, 1985, 32975, 1992, 1993, 7165, 3141, 3143, 86346, 1982,
-        1984, 3145, 86347, 78064, 23456, 29578, 3136, 17752, 4710,
-        4711, 4712, 149319, 424, 4713, 95735, 4715, 149320, 4717, 4718,
-        149321, 192, 149322, 108126, 29976, 5404, 38059, 5406, 2030,
-        289, 1804, 1557, 1558, 94080, 29651, 94317, 1561, 1562, 1563,
-        1565, 24632, 1927, 1928, 1566, 1570, 1571, 1572, 1573, 1574,
-        1575, 94318, 1576, 2674, 9351, 94319, 94320, 2677, 2678, 29654,
-        2946, 2945, 2682, 2683, 2947, 3102, 3402, 3104, 4780, 3106,
-        3107, 3108, 3109, 3110, 3111, 3112, 3113, 3114, 3116, 3117,
-        3118, 19610, 44805, 3119, 3407, 3121, 3122, 3124, 3126, 3127,
-        41745, 41746, 3130, 459, 460, 461, 462, 463, 464, 466, 467,
-        40306, 468, 471, 472, 40307, 4467, 475, 476, 477, 478, 479,
-        40308, 481, 482, 20129, 483, 484, 485, 486, 4473, 488, 489,
-        458, 491, 40309, 494, 495, 496, 497, 499, 500, 501, 502, 355,
-        356, 1549, 358, 359, 360, 37971, 362, 2579, 2581, 24578, 2583,
-        24579, 2586, 2587, 2588, 2590, 2591, 24580, 24581, 3666, 24582,
-        2594, 24583, 2596, 2597, 24584, 2599, 18013, 24585, 2601,
-        49361, 280, 3969, 11651, 11652, 3926, 5103, 11653, 11654,
-        11655, 6896, 417, 168, 149323, 11268, 11657, 38089, 59517,
-        149324, 38092, 149325, 5110, 38094, 59520, 38096, 38097, 28916,
-        59703, 4992, 149326, 32383, 2478, 3985, 2479, 2480, 2481, 2482,
-        2483, 2484, 2485, 2486, 24146, 22184, 2488, 2489, 2490, 2494,
-        2493, 18043, 2495, 2542, 2497, 5062, 2499, 2501, 24147, 24148,
-        2504, 2505, 2506, 2507, 2508, 394, 2660, 2509, 2511, 24149,
-        2512, 2513, 2514, 3988, 4410, 3989, 2518, 2522, 2521, 24150,
-        12082, 2524, 3990, 24151, 387, 24152, 2529, 2530, 2528, 3991,
-        24153, 2534, 24154, 2536, 24155, 2538, 22510, 6332, 3554, 5309,
-        7700, 6333, 6334, 6335, 6336, 6337, 5693, 117020, 6339, 149327,
-        149328, 149329, 6340, 6343, 117022, 4324, 283, 284, 285, 286,
-        2688, 287, 2689, 288, 8880, 290, 291, 2690, 292, 295, 294,
-        24543, 13899, 297, 298, 299, 300, 303, 301, 59178, 302, 8881,
-        34403, 13900, 17811, 305, 307, 306, 308, 2727, 368, 364,
-        110416, 1587, 366, 367, 2692, 26624, 7233, 9082, 35684, 7250,
-        13902, 304, 13903, 991, 110417, 273, 274, 275, 276, 277, 278,
-        41095, 281, 282, 4419, 2768, 229, 230, 231, 232, 233, 234, 235,
-        236, 237, 1065, 239, 2745, 2746, 240, 9250, 241, 242, 244, 245,
-        9251, 246, 247, 248, 249, 250, 251, 253, 254, 255, 9252, 257,
-        258, 259, 9253, 9254, 2751, 265, 266, 267, 268, 9255, 9256,
-        270, 271, 9257, 238, 1024, 829, 1025, 1026, 1028, 1029, 1030,
-        9258, 1032, 1033, 1034, 1027, 1035, 1036, 9259, 1037, 1038,
-        1039, 4594, 4429, 1041, 1042, 1043, 70332, 1045, 1046, 1047,
-        1048, 21128, 1050, 122787, 72433, 1052, 2762, 1054, 1055, 1056,
-        9548, 1057, 71311, 1058, 1059, 1060, 61413, 2765, 4436, 1064,
-        1066, 11610, 3485, 22357, 104580, 149330, 149331, 15471, 5679,
-        5680, 687, 5683, 5684, 953, 8849, 102120, 149332, 5688, 5689,
-        149333, 6920, 60202, 33855, 33856, 33857, 19163, 33858, 3491,
-        149334, 914, 2202, 916, 917, 919, 920, 921, 922, 3568, 924,
-        925, 926, 927, 928, 929, 8752, 931, 932, 933, 934, 3570, 1876,
-        9138, 1877, 1878, 2210, 1880, 1881, 3571, 1883, 1884, 2212,
-        1886, 2214, 1888, 1889, 1890, 8753, 1891, 1892, 1893, 1894,
-        1895, 1896, 1898, 2217, 3572, 1901, 1902, 688, 2219, 107, 1904,
-        1905, 3573, 1907, 3323, 1909, 1910, 1911, 8754, 1912, 55911,
-        1913, 1914, 3574, 1741, 3575, 1916, 2226, 3576, 1919, 2227,
-        1920, 3577, 3578, 2229, 1923, 85396, 174, 175, 114875, 178,
-        180, 181, 182, 1477, 185, 186, 172, 187, 188, 85397, 85398,
-        190, 191, 891, 893, 19778, 18068, 895, 897, 896, 25985, 894,
-        900, 361, 1206, 193, 194, 195, 196, 197, 198, 199, 200, 55009,
-        201, 33266, 29064, 204, 205, 40129, 206, 207, 208, 2842, 209,
-        210, 211, 212, 149335, 870, 871, 18005, 872, 18006, 874, 875,
-        876, 1479, 1480, 1481, 879, 881, 57212, 2779, 57213, 886, 887,
-        57214, 57215, 889, 890, 806, 69305, 808, 809, 86327, 812, 813,
-        814, 815, 26724, 816, 69307, 43484, 818, 819, 63904, 820, 821,
-        822, 86328, 13498, 824, 825, 12218, 149336, 49042, 4464, 4466,
-        35536, 73245, 73246, 474, 73247, 480, 46247, 29624, 21086,
-        73248, 490, 493, 73249, 73250, 401, 403, 405, 2860, 15483,
-        74826, 408, 409, 74827, 410, 411, 413, 74828, 415, 2863, 68707,
-        33284, 2865, 2866, 2867, 2868, 2869, 2870, 17976, 3032, 38498,
-        7350, 2876, 2874, 24506, 918, 923, 64562, 64563, 32648, 930,
-        1875, 32649, 1879, 32650, 1882, 1887, 32651, 64564, 32652,
-        1897, 32653, 18170, 1900, 32654, 1906, 1915, 64565, 1921, 1922,
-        90662, 2234, 37449, 8886, 37450, 7418, 37451, 37452, 37453,
-        37454, 1609, 1610, 1611, 1612, 113456, 1212, 1616, 1617,
-        113457, 1615, 1619, 113458, 1620, 8747, 113459, 8748, 42233,
-        78065, 42235, 2149, 42236, 78066, 42237, 42238, 4335, 42239,
-        78067, 42241, 78068, 42243, 78069, 42244, 78070, 54587, 12993,
-        2040, 1130, 1131, 51172, 1133, 1134, 1135, 1136, 1137, 1138,
-        1139, 1140, 1141, 149337, 1115, 5178, 149338, 452, 7784, 21522,
-        1361, 103718, 149339, 15990, 79432, 149340, 4232, 149341,
-        15998, 53917, 15996, 53918, 149342, 149343, 97544, 53920,
-        97546, 841, 1954, 842, 41926, 844, 2589, 845, 846, 27370, 848,
-        849, 41927, 25165, 852, 1956, 854, 856, 1957, 855, 1959, 35170,
-        23055, 75673, 116783, 857, 116784, 851, 116785, 858, 859, 860,
-        861, 57422, 1964, 864, 866, 867, 1965, 1966, 1968, 1969, 2989,
-        116786, 1972, 1973, 116787, 1975, 1976, 1977, 2580, 39540,
-        2585, 39541, 21755, 39542, 2592, 34859, 2593, 39543, 38540,
-        2595, 39544, 149344, 35433, 81849, 35434, 40257, 873, 877,
-        2778, 32040, 882, 883, 884, 885, 888, 3358, 1559, 1560, 1438,
-        25387, 1569, 38135, 66925, 2673, 3095, 2679, 59053, 25443,
-        34369, 1983, 17749, 9343, 1989, 13565, 31525, 61690, 18165,
-        17751, 78234, 26506, 9348, 20307, 18154, 3133, 2572, 3134,
-        12131, 19770, 48724, 25759, 13549, 65465, 19936, 13545, 25645,
-        4786, 15756, 19547, 1581, 92226, 1362, 21524, 13059, 23717,
-        149345, 20198, 27123, 149346, 149347, 26030, 27126, 27652,
-        10538, 1667, 40282, 14134, 40284, 16368, 149348, 40287, 8870,
-        40288, 149349, 40289, 149350, 149351, 40295, 10424, 7012,
-        13178, 45608, 10423, 13181, 4201, 672, 13182, 10174, 10607,
-        13183, 580, 149352, 149353, 96298, 53691, 3721, 66048, 21584,
-        149354, 48206, 48207, 149355, 1405, 1406, 1407, 11162, 577,
-        149356, 6941, 6942, 16583, 1284, 10511, 16584, 16585, 422, 423,
-        1249, 1244, 1245, 1247, 2544, 1248, 1250, 2545, 1252, 2547,
-        1253, 2549, 1259, 1257, 1258, 1260, 1261, 2551, 1262, 1263,
-        1264, 1265, 2553, 1266, 17795, 2554, 17796, 1270, 1271, 1273,
-        17797, 2556, 1275, 1276, 2557, 1277, 1278, 1279, 1280, 1282,
-        68, 69, 5080, 5256, 6869, 10148, 6960, 10150, 149357, 10152,
-        14413, 149358, 14414, 56037, 651, 56038, 131797, 555, 14415,
-        14416, 149359, 149360, 56042, 14418, 149361, 149, 56043, 97512,
-        34512, 797, 7396, 9395, 9396, 9397, 63417, 805, 23984, 13665,
-        10452, 55147, 5656, 53, 4348, 4349, 4350, 148488, 13669, 6527,
-        149362, 11374, 11376, 11377, 8092, 11378, 11380, 152, 5013,
-        8093, 561, 11381, 5623, 4176, 26840, 3564, 3565, 3708, 3567,
-        18783, 18784, 4039, 10540, 18786, 30100, 30101, 1528, 149363,
-        19561, 19562, 19563, 19564, 1110, 134146, 10600, 149364, 10602,
-        149365, 149366, 10603, 10604, 4981, 57075, 37508, 149367,
-        34589, 1209, 149368, 19592, 19593, 7620, 9674, 3481, 10240,
-        81835, 8001, 33872, 8907, 55155, 1585, 31731, 49694, 25760,
-        31733, 903, 904, 2539, 49695, 1194, 1195, 1196, 31734, 1197,
-        1198, 1199, 1593, 899, 1200, 1201, 9276, 1202, 40181, 40482,
-        55718, 80833, 24596, 3669, 15699, 55720, 55721, 40481, 3672,
-        39826, 80363, 2602, 2603, 2604, 62126, 2605, 2606, 2607, 8714,
-        2608, 2609, 2610, 2612, 149369, 2894, 15241, 15242, 15262,
-        5384, 20290, 20291, 7792, 20295, 64413, 39236, 18011, 71494,
-        898, 51015, 19782, 105107, 149370, 7634, 149371, 149372,
-        115458, 22821, 19894, 2213, 66926 };
-
-    data3630 = new int[] { 2, 4, 86133, 11, 16505, 86134, 86135, 86136,
-        1290, 86137, 86138, 32473, 19346, 32474, 4922, 32475, 86139,
-        16914, 86140, 86141, 86142, 86143, 32478, 86144, 86145, 32480,
-        4884, 4887, 32481, 86146, 16572, 86147, 16295, 165, 86148,
-        3183, 21920, 21921, 21922, 555, 4006, 32484, 21925, 21926,
-        13775, 86149, 13777, 85833, 85834, 13779, 13773, 13780, 75266,
-        17674, 13784, 13785, 13786, 13787, 13788, 6258, 86150, 13790,
-        75267, 13793, 13794, 13795, 312, 4914, 4915, 6222, 86151, 4845,
-        4883, 4918, 4894, 4919, 86152, 4921, 6223, 6224, 6225, 6226,
-        67909, 6229, 18170, 6230, 5198, 25625, 6231, 6232, 6233, 1808,
-        6234, 6235, 6236, 41376, 6238, 6239, 67911, 6240, 86153, 6243,
-        6244, 83549, 6246, 6247, 6248, 6249, 782, 444, 6251, 6250,
-        19863, 28963, 310, 2234, 144, 2236, 2309, 69437, 2311, 2325,
-        2241, 69438, 69439, 2244, 2245, 2246, 23504, 2314, 69440,
-        36603, 2250, 2268, 2271, 2251, 2254, 2255, 2257, 2240, 36604,
-        84726, 36605, 84727, 2262, 2263, 18431, 38853, 2317, 2149,
-        2326, 2327, 2329, 3980, 2275, 2277, 2258, 84728, 2260, 84729,
-        84730, 13766, 36607, 2282, 2283, 84731, 2284, 2286, 2287, 2337,
-        7424, 2288, 2338, 3522, 2290, 84733, 32902, 371, 37708, 2096,
-        3065, 3066, 375, 377, 374, 378, 2100, 86154, 381, 382, 58795,
-        379, 383, 384, 385, 4449, 387, 388, 389, 390, 9052, 391, 18358,
-        2107, 394, 2111, 2108, 393, 2109, 395, 86155, 86156, 397, 2113,
-        398, 399, 400, 273, 274, 275, 40980, 276, 277, 31716, 279, 280,
-        31717, 281, 282, 1628, 1623, 1624, 1625, 2052, 1626, 725, 727,
-        728, 729, 730, 731, 1633, 733, 734, 735, 86157, 737, 738, 739,
-        1634, 3563, 3564, 3565, 1667, 12461, 76276, 3567, 5413, 77622,
-        5415, 5416, 5417, 5418, 107, 86158, 7784, 15363, 153, 3723,
-        2713, 7786, 3835, 7787, 86159, 7789, 7791, 7792, 7794, 86160,
-        7796, 86161, 6708, 7798, 7799, 7800, 7801, 7802, 7803, 1665,
-        43150, 15365, 1581, 5656, 43152, 80258, 7450, 39922, 86162,
-        51587, 9059, 4606, 396, 86163, 86164, 7250, 401, 403, 2860,
-        33281, 2964, 408, 9119, 409, 86165, 7669, 2861, 410, 413,
-        86166, 414, 415, 33282, 405, 33283, 7498, 2865, 7230, 33284,
-        2866, 86167, 2867, 47518, 2868, 86168, 2869, 2870, 4712, 7096,
-        28484, 6913, 6914, 6915, 6916, 37169, 37170, 7103, 28269, 6919,
-        86169, 45431, 6922, 7104, 6923, 7108, 6924, 6925, 6926, 6927,
-        6928, 86170, 86171, 86172, 6930, 6931, 6932, 6934, 6935, 6936,
-        451, 6937, 6938, 4756, 3554, 5309, 8145, 3586, 16417, 9767,
-        14126, 25854, 6580, 10174, 86173, 5519, 21309, 8561, 20938,
-        10386, 86174, 781, 2030, 16419, 30323, 16420, 16421, 16424,
-        86175, 86176, 86177, 28871, 86178, 28872, 63980, 6329, 49561,
-        4271, 38778, 86179, 86180, 20126, 16245, 193, 195, 196, 197,
-        56973, 199, 200, 201, 202, 203, 204, 56974, 56975, 205, 206,
-        4662, 207, 208, 209, 210, 211, 212, 47901, 641, 642, 643, 1380,
-        1079, 47902, 1381, 1081, 1082, 1083, 47903, 1382, 47904, 1087,
-        47905, 965, 966, 1298, 968, 1387, 1300, 50288, 971, 972, 973,
-        974, 23974, 22183, 1390, 23313, 1389, 1391, 902, 23029, 296,
-        1304, 1395, 1303, 1309, 1308, 50289, 1312, 50290, 50291, 1315,
-        1317, 9270, 19796, 3605, 1320, 1321, 44946, 1322, 1323, 50292,
-        967, 1587, 1326, 1331, 17482, 633, 29115, 53858, 29118, 29119,
-        62624, 44494, 6965, 6966, 6959, 6967, 71562, 6969, 23459,
-        23460, 17464, 4225, 23461, 23462, 23463, 5893, 23464, 17467,
-        17468, 23465, 12562, 1405, 1406, 1407, 960, 961, 962, 687, 963,
-        86181, 86182, 5997, 10812, 11976, 11977, 1850, 577, 13393,
-        10810, 13394, 65040, 86183, 3935, 3936, 3937, 710, 86184, 5785,
-        5786, 29949, 5787, 5788, 283, 284, 2687, 285, 286, 287, 2689,
-        288, 289, 8880, 290, 2690, 13899, 991, 292, 295, 42007, 35616,
-        63103, 298, 299, 3520, 297, 9024, 303, 301, 302, 300, 31345,
-        3719, 304, 305, 306, 307, 308, 368, 364, 85002, 9026, 63105,
-        367, 39596, 25835, 19746, 293, 294, 26505, 85003, 18377, 56785,
-        10122, 10123, 10124, 86185, 39863, 86186, 10125, 39865, 4066,
-        4067, 24257, 4068, 4070, 86187, 4073, 4074, 86188, 4076, 7538,
-        4077, 86189, 4078, 4079, 7540, 7541, 4084, 4085, 7542, 86190,
-        4086, 86191, 4087, 4088, 86192, 7545, 44874, 7821, 44875,
-        86193, 4286, 86194, 51470, 17609, 1408, 47486, 1411, 1412,
-        47487, 1413, 1414, 1417, 1415, 47488, 1416, 1418, 1420, 470,
-        1422, 1423, 1424, 5001, 5002, 47489, 1427, 1429, 1430, 31811,
-        1432, 1433, 47490, 1435, 3753, 1437, 1439, 1440, 47491, 1443,
-        47492, 1446, 5004, 5005, 1450, 47493, 353, 1452, 42145, 3103,
-        3402, 3104, 3105, 4780, 3106, 3107, 3108, 12157, 3111, 42146,
-        42147, 3114, 4782, 42148, 3116, 3117, 42149, 42150, 3407, 3121,
-        3122, 18154, 3126, 3127, 3128, 3410, 3130, 3411, 3412, 3415,
-        24241, 3417, 3418, 3449, 42151, 3421, 3422, 7587, 42152, 3424,
-        3427, 3428, 3448, 3430, 3432, 42153, 42154, 41648, 1991, 407,
-        57234, 411, 2862, 57235, 2863, 18368, 57236, 2874, 7350, 4115,
-        2876, 2877, 17975, 86195, 4116, 2881, 2882, 2883, 2886, 463,
-        870, 872, 873, 874, 875, 8783, 8784, 877, 1480, 1481, 459,
-        2778, 881, 8785, 2779, 8786, 8787, 8788, 886, 887, 8789, 889,
-        8790, 86196, 6920, 86197, 5080, 5081, 7395, 7396, 9395, 9396,
-        1528, 42737, 805, 86198, 1209, 13595, 4126, 9680, 34368, 9682,
-        86199, 86200, 174, 175, 176, 177, 178, 179, 180, 182, 183,
-        1477, 31138, 186, 172, 187, 188, 189, 190, 191, 458, 871,
-        31294, 31295, 27604, 31296, 31297, 882, 883, 884, 31298, 890,
-        1089, 1488, 1489, 1092, 1093, 1094, 1095, 1096, 1097, 1490,
-        1098, 1495, 1502, 1099, 1100, 1101, 1493, 2997, 12223, 1103,
-        2654, 1498, 1499, 1500, 80615, 80616, 80617, 33359, 86201,
-        9294, 1501, 86202, 1506, 1507, 23454, 38802, 38803, 1014,
-        86203, 5583, 5584, 651, 74717, 5586, 5587, 5588, 5589, 74720,
-        5590, 38808, 33527, 78330, 10930, 5119, 10931, 1000, 10928,
-        10932, 10933, 10934, 10935, 5863, 10936, 86204, 10938, 10939,
-        86205, 192, 194, 38754, 38755, 198, 38756, 38757, 38758, 2842,
-        640, 22780, 22781, 1080, 86206, 86207, 1084, 1086, 1088, 63916,
-        9412, 970, 9413, 9414, 9415, 9416, 9417, 1310, 7168, 7169,
-        1318, 9418, 1324, 39159, 1804, 1557, 24850, 41499, 1560, 41500,
-        1562, 1563, 1565, 1927, 1928, 1566, 1569, 1570, 1571, 1572,
-        1573, 1574, 1575, 1576, 2674, 2677, 2678, 2679, 2946, 2682,
-        2676, 2683, 2947, 1156, 1157, 1158, 1467, 1160, 1468, 1469,
-        1161, 1162, 1163, 4369, 1165, 1166, 1167, 12923, 2917, 1169,
-        1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 18153, 8359,
-        1178, 1164, 1191, 1180, 12924, 86208, 86209, 54817, 66962,
-        2476, 86210, 86211, 41820, 41821, 41822, 41824, 1130, 1131,
-        1132, 32692, 1134, 34848, 1136, 1133, 1137, 1138, 1139, 1140,
-        1141, 1143, 1144, 1145, 34849, 2639, 34850, 1146, 1147, 1148,
-        34851, 1150, 1151, 1152, 1153, 1154, 1155, 1678, 1679, 1680,
-        1681, 40870, 2059, 1685, 1686, 32686, 14970, 1688, 1689, 86212,
-        1692, 1682, 1693, 1695, 1696, 1698, 12955, 8909, 41690, 1700,
-        41691, 86213, 30949, 41692, 1703, 1704, 1705, 41693, 14976,
-        1708, 2071, 1709, 1710, 1711, 1712, 1727, 86214, 86215, 86216,
-        1715, 86217, 1714, 1717, 1690, 41697, 86218, 1720, 86219, 2073,
-        41699, 1724, 2075, 1726, 1729, 1730, 1732, 2078, 2223, 1735,
-        1713, 41700, 1737, 14977, 1739, 1740, 1741, 2080, 1743, 1744,
-        1745, 1746, 1747, 1748, 1749, 1750, 1751, 41701, 1752, 1753,
-        1909, 86220, 2085, 1754, 19548, 86221, 19551, 5733, 3856, 5190,
-        4581, 25145, 86222, 86223, 4846, 86224, 4861, 86225, 86226,
-        86227, 25150, 86228, 86229, 13820, 2027, 4898, 4899, 4901,
-        2135, 4902, 4868, 4904, 86230, 4905, 25155, 4907, 86231, 4909,
-        4910, 4911, 4912, 86232, 6220, 81357, 86233, 2589, 73877,
-        29706, 6227, 6228, 86234, 6237, 86235, 6241, 6242, 1812, 13808,
-        13809, 70908, 2293, 2294, 86236, 2295, 2296, 2297, 22947,
-        16511, 2299, 2300, 2301, 13097, 73079, 86237, 13099, 50121,
-        86238, 86239, 13101, 86240, 2424, 4725, 4726, 4727, 4728, 4729,
-        4730, 86241, 26881, 10944, 4734, 4735, 4736, 26239, 26240,
-        71408, 86242, 57401, 71410, 26244, 5344, 26245, 86243, 4102,
-        71414, 11091, 6736, 86244, 6737, 6738, 38152, 6740, 6741, 6742,
-        6298, 6743, 6745, 6746, 20867, 6749, 20616, 86245, 9801, 65297,
-        20617, 65298, 20619, 5629, 65299, 20621, 20622, 8385, 20623,
-        20624, 5191, 20625, 20626, 442, 443, 445, 27837, 77681, 86246,
-        27839, 86247, 86248, 41435, 66511, 2478, 2479, 2480, 2481,
-        2482, 2483, 2484, 2485, 2486, 2487, 2488, 2489, 2490, 2494,
-        2493, 33025, 12084, 2542, 2497, 2499, 2501, 2503, 2504, 2505,
-        33026, 2506, 2507, 2508, 2509, 2511, 1787, 12080, 2513, 2514,
-        3988, 3176, 3989, 2518, 2521, 9285, 2522, 2524, 2525, 3990,
-        2527, 2528, 27499, 2529, 2530, 3991, 2532, 2534, 2535, 18038,
-        2536, 2538, 2495, 46077, 61493, 61494, 1006, 713, 4971, 4972,
-        4973, 4975, 4976, 650, 170, 7549, 7550, 7551, 7552, 7553,
-        86249, 7936, 956, 11169, 11170, 1249, 1244, 1245, 1247, 2544,
-        1250, 2545, 1252, 2547, 1253, 1254, 2549, 39636, 1259, 1257,
-        1258, 39637, 1260, 1261, 2551, 1262, 1263, 848, 86250, 86251,
-        854, 74596, 856, 1957, 86252, 855, 1959, 1961, 857, 86253, 851,
-        859, 860, 862, 1964, 864, 865, 866, 867, 1965, 1966, 1967,
-        1968, 1969, 86254, 1971, 1972, 1973, 1974, 1975, 1976, 1977,
-        841, 1954, 842, 2978, 846, 847, 849, 850, 852, 1956, 17452,
-        71941, 86255, 86256, 73665, 1471, 13690, 185, 503, 504, 2342,
-        505, 506, 4378, 508, 4379, 17313, 510, 511, 512, 520, 513,
-        4384, 17314, 514, 515, 46158, 17317, 518, 34269, 519, 4386,
-        523, 524, 525, 46159, 528, 529, 17319, 531, 532, 533, 534, 535,
-        7482, 537, 538, 5267, 536, 539, 541, 540, 19858, 17320, 17321,
-        906, 907, 908, 17322, 910, 17323, 912, 15850, 913, 4398, 17324,
-        86257, 278, 2948, 2949, 2950, 3007, 2951, 2952, 2953, 2954,
-        2955, 3013, 35352, 3014, 3015, 2962, 3016, 33505, 39118, 3017,
-        3018, 20492, 4000, 3021, 3022, 35353, 39293, 3024, 18443, 3029,
-        9467, 20529, 39119, 8380, 2965, 3030, 3043, 22714, 39120, 2956,
-        3035, 39121, 3037, 3038, 2688, 86258, 36675, 30894, 24505,
-        8888, 13541, 49728, 27660, 9082, 27661, 365, 366, 2232, 76098,
-        7233, 1494, 17391, 606, 607, 611, 610, 612, 614, 615, 613, 616,
-        9117, 617, 618, 21155, 1789, 619, 620, 7636, 12019, 621, 622,
-        1793, 623, 625, 624, 631, 626, 627, 21578, 21103, 628, 21579,
-        629, 9122, 9123, 12189, 9289, 3168, 3169, 630, 632, 634, 21580,
-        9121, 635, 636, 637, 21581, 12781, 1801, 638, 639, 1559, 24343,
-        9419, 9420, 795, 796, 1611, 86259, 1612, 21551, 21552, 3741,
-        1617, 3742, 1615, 1619, 1620, 6301, 3744, 1622, 67685, 8521,
-        55937, 9025, 27663, 8881, 13581, 86260, 11592, 44720, 86261,
-        63231, 50873, 42925, 52332, 86262, 72706, 17705, 17707, 17708,
-        3401, 40217, 1248, 40218, 86263, 7098, 86264, 86265, 1264,
-        86266, 1266, 1267, 1268, 1269, 86267, 1271, 1272, 1273, 1274,
-        2556, 1275, 1276, 1277, 1278, 1279, 1280, 1282, 1283, 22680,
-        11889, 86268, 45662, 7038, 86269, 19315, 45663, 45664, 86270,
-        5855, 34002, 49245, 10447, 5663, 86271, 15429, 53877, 49249,
-        86272, 86273, 86274, 60128, 60453, 60129, 5552, 31923, 43407,
-        4287, 17980, 64977, 86275, 86276, 8234, 86277, 3649, 8240,
-        1330, 11999, 1332, 27618, 1334, 1335, 340, 3651, 25640, 18165,
-        1343, 4618, 1474, 3653, 75921, 1349, 53519, 1779, 45454, 22778,
-        40153, 67677, 63826, 45455, 15128, 67678, 67679, 1792, 67680,
-        3171, 47816, 45457, 9288, 59891, 67681, 25703, 35731, 35732,
-        369, 35713, 35714, 35715, 34652, 35716, 31681, 35717, 12779,
-        35718, 35719, 11992, 806, 807, 808, 43499, 43500, 810, 776,
-        812, 813, 814, 241, 43501, 43502, 816, 755, 43503, 818, 819,
-        820, 43504, 821, 822, 823, 824, 825, 826, 43505, 43506, 43507,
-        828, 829, 20083, 43508, 43509, 832, 833, 834, 835, 86278,
-        19984, 19985, 86279, 24125, 19986, 86280, 19988, 86281, 5414,
-        86282, 85808, 5479, 5420, 5421, 5422, 5423, 63800, 86283,
-        86284, 30965, 86285, 416, 1510, 5740, 5741, 81991, 86286,
-        28938, 50149, 1003, 55512, 14306, 6960, 688, 86287, 14307,
-        5399, 5400, 17783, 24118, 720, 86288, 44913, 24557, 667, 24876,
-        6529, 24877, 24878, 24879, 24880, 31847, 20671, 4011, 171, 580,
-        86289, 3863, 914, 2202, 916, 917, 918, 919, 921, 922, 923,
-        7585, 925, 7586, 926, 927, 928, 7588, 929, 930, 931, 932, 933,
-        934, 1875, 1876, 7589, 7590, 1878, 1879, 7591, 7592, 1882,
-        1883, 1884, 2212, 7593, 1887, 1888, 1889, 1890, 1891, 1892,
-        1893, 1894, 1895, 1896, 1897, 1898, 2217, 1900, 7594, 1902,
-        2219, 7595, 1905, 1906, 1907, 3323, 7596, 1911, 1912, 7597,
-        1914, 1915, 1916, 2226, 1919, 7598, 2227, 1920, 1921, 7599,
-        7600, 4708, 1923, 355, 356, 1549, 358, 32077, 360, 32078,
-        21117, 362, 19043, 71677, 5716, 86290, 49790, 86291, 86292,
-        86293, 49792, 86294, 86295, 49794, 86296, 86297, 86298, 86299,
-        11882, 86300, 49798, 86301, 49800, 49801, 49802, 49803, 453,
-        49804, 8591, 6794, 49806, 18989, 49807, 49808, 16308, 49809,
-        86302, 86303, 10105, 86304, 5285, 10106, 10107, 6557, 86305,
-        23571, 10109, 38883, 10110, 5401, 86306, 67557, 16430, 67558,
-        40171, 16433, 25878, 86307, 21762, 23, 86308, 86309, 21766,
-        86310, 86311, 5149, 3926, 21768, 21769, 47826, 942, 46985,
-        6588, 58867, 6589, 6590, 86312, 6592, 6006, 53855, 9565, 359,
-        86313, 2845, 876, 879, 27556, 27557, 885, 27558, 888, 2847,
-        27559, 2115, 2116, 2117, 53962, 57839, 315, 316, 317, 318, 319,
-        86314, 321, 322, 2122, 323, 2123, 324, 325, 328, 326, 327,
-        40542, 329, 330, 18079, 18080, 331, 1790, 7382, 332, 7380,
-        7236, 23413, 23414, 18924, 18925, 333, 335, 336, 39750, 337,
-        86315, 339, 341, 342, 343, 16264, 16265, 6615, 86316, 86317,
-        86318, 86319, 16269, 10538, 33226, 86320, 16272, 5824, 16273,
-        16274, 16276, 16277, 16278, 16279, 16280, 14517, 1547, 6463,
-        3394, 49677, 659, 10380, 30013, 10382, 10378, 10379, 10383,
-        10384, 10385, 86321, 4139, 13370, 13371, 86322, 86323, 11878,
-        64509, 15141, 15142, 15143, 32737, 14183, 15144, 39101, 42768,
-        5645, 32738, 801, 803, 804, 86324, 14707, 86325, 6601, 12402,
-        712, 12403, 2936, 1447, 15477, 1410, 44872, 1550, 8614, 15478,
-        15479, 15480, 15481, 4811, 3752, 1442, 15482, 8818, 1445, 5006,
-        16304, 32277, 16305, 16306, 86326, 16307, 53691, 69305, 809,
-        86327, 815, 26724, 69307, 43484, 63904, 86328, 13498, 827,
-        86329, 831, 2857, 836, 86330, 86331, 837, 838, 839, 840, 228,
-        229, 43722, 230, 231, 43723, 234, 235, 236, 237, 238, 239,
-        2745, 2746, 240, 242, 243, 244, 43724, 19788, 246, 247, 21134,
-        248, 250, 251, 252, 253, 254, 255, 256, 257, 258, 43725, 43726,
-        41, 43727, 262, 43728, 2751, 264, 265, 266, 267, 268, 269, 270,
-        271, 272, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032,
-        1033, 1034, 43729, 1035, 43730, 1037, 21821, 2926, 14388,
-        10432, 14389, 14390, 14391, 14392, 86332, 14394, 14395, 2035,
-        2169, 86333, 14397, 14398, 14399, 14400, 52, 14401, 14402,
-        7077, 21822, 14405, 14406, 14396, 86334, 17356, 17357, 84679,
-        84680, 76383, 17360, 17361, 86335, 38801, 2060, 30850, 12963,
-        1684, 1687, 2061, 14978, 1694, 43387, 1697, 1699, 2067, 1701,
-        1702, 1706, 43388, 43389, 76325, 1716, 1718, 26832, 1719, 1723,
-        2081, 2063, 1728, 39059, 76326, 1731, 86336, 1736, 76327, 1738,
-        19657, 6579, 6581, 6582, 6583, 6584, 6585, 29979, 1818, 28239,
-        68, 69, 3391, 86337, 10266, 63528, 86338, 10269, 10270, 10271,
-        10272, 86339, 86340, 63530, 63531, 63532, 63533, 10273, 63534,
-        86341, 10681, 10682, 86342, 9673, 86343, 10683, 460, 461, 462,
-        467, 4464, 4466, 3729, 471, 472, 468, 81634, 474, 81635, 475,
-        476, 477, 479, 480, 81636, 81637, 482, 17442, 81638, 81639,
-        484, 485, 486, 4473, 488, 489, 490, 493, 466, 494, 495, 496,
-        497, 499, 500, 501, 502, 34376, 86344, 63836, 56281, 1707,
-        20416, 61452, 56282, 1755, 56283, 56284, 18508, 53650, 63444,
-        86345, 3579, 63445, 3677, 1979, 1980, 1981, 3132, 3147, 34090,
-        1987, 12770, 1329, 80818, 80819, 1988, 23522, 1986, 15880,
-        1985, 32975, 1992, 1993, 7165, 3141, 3143, 86346, 1982, 1984,
-        3145, 86347, 78064, 55453, 2656, 2657, 35634, 35635, 2167,
-        43479 };
-
-    data10k = new int[] { 2, 4, 149900, 11, 70236, 149901, 149902, 6721,
-        149929, 29212, 34600, 149930, 149931, 149932, 141696, 149908,
-        149909, 149910 };
-
-    data501871 = new int[] { 1368366, 1368367, 1817408, 11, 2513, 1817409,
-        1817410, 1817411, 1382349, 126700, 1817412, 5539, 21862, 21863,
-        21864, 1233, 1127, 121, 15254, 15255, 357, 449, 15256, 8817,
-        15257, 15258, 1406, 1096, 281, 4826, 4827, 223, 166, 2372, 168,
-        169, 2219, 170, 171, 1176, 172, 173, 2222, 3035, 177, 178, 179,
-        180, 181, 183, 3036, 2378, 1157, 1158, 2380, 1160, 1161, 1162,
-        2384, 1164, 1165, 1166, 1167, 1168, 2385, 3037, 1171, 1172,
-        1173, 2238, 1175, 1177, 1178, 1179, 1180, 1181, 2243, 3038,
-        1182, 2244, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190,
-        59766, 471, 7349, 3599, 2847, 59767, 59768, 59769, 59770,
-        59771, 59772, 59773, 59774, 59775, 2625, 852, 853, 2632, 854,
-        855, 856, 2284, 857, 862, 1031, 859, 860, 861, 866, 1033, 867,
-        1035, 868, 870, 2294, 871, 2295, 873, 874, 875, 876, 877, 878,
-        879, 66632, 66633, 66634, 66635, 14823, 66636, 66637, 3763,
-        77345, 1370, 3764, 3765, 3766, 5666, 3768, 3770, 16892, 3771,
-        3772, 3773, 3244, 3246, 3247, 1504, 266, 29250, 24764, 29251,
-        689, 12844, 8068, 29252, 38918, 750, 751, 770, 3704, 753, 754,
-        765, 755, 3708, 757, 758, 759, 760, 3710, 761, 762, 763, 3712,
-        766, 767, 768, 769, 771, 3719, 4380, 3722, 3723, 3725, 4381,
-        3727, 3728, 3731, 3732, 764, 4382, 2316, 334, 1637, 4383, 4384,
-        4385, 4386, 4387, 184, 185, 1134, 186, 1135, 187, 188, 1138,
-        197, 191, 3517, 193, 194, 195, 196, 208, 3519, 198, 9210, 937,
-        9211, 9212, 916, 917, 117, 118, 919, 122, 921, 123, 124, 125,
-        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 924, 137,
-        138, 139, 140, 141, 588, 928, 142, 143, 144, 929, 146, 147,
-        148, 149, 150, 151, 3775, 3776, 3777, 3778, 3780, 3781, 3783,
-        3784, 3785, 3796, 4169, 3788, 4170, 3790, 3791, 3793, 3803,
-        3794, 3797, 4171, 3799, 3800, 3801, 3802, 3804, 4172, 3806,
-        4173, 4174, 3811, 4175, 3813, 3829, 3815, 3816, 3817, 4176,
-        4177, 3820, 3821, 3822, 2168, 3039, 2460, 2170, 2459, 2174,
-        2175, 2176, 2461, 2462, 2463, 3040, 2466, 2467, 2469, 2468,
-        2470, 3041, 2472, 3042, 3043, 3044, 3045, 231, 881, 882, 1219,
-        884, 2038, 886, 887, 888, 891, 892, 1221, 894, 895, 1222, 2039,
-        899, 1225, 900, 901, 902, 2492, 2494, 2495, 2496, 4052, 2498,
-        2502, 2500, 2501, 2503, 2504, 4653, 5514, 18671, 10350, 1122,
-        44317, 44318, 44319, 44320, 44321, 44322, 44323, 44324, 7923,
-        1422, 10284, 10285, 6146, 9803, 10286, 466, 5998, 696, 3257,
-        6043, 6195, 6196, 6197, 6198, 6199, 6200, 6201, 7029, 4405,
-        4864, 450, 349, 11214, 3548, 1092, 5728, 7395, 6533, 1123,
-        5736, 1115, 6535, 6536, 2739, 2832, 2833, 2834, 2835, 2836,
-        23972, 2837, 23973, 2839, 2840, 2691, 1339, 20116, 3219, 8210,
-        3170, 3171, 3172, 3173, 2094, 2095, 2096, 2097, 2099, 2100,
-        2102, 3174, 2104, 1372, 2105, 2107, 2108, 2109, 2110, 2113,
-        2114, 2115, 2117, 2118, 3221, 3222, 2122, 2123, 2124, 4611,
-        2125, 2126, 2127, 2128, 2129, 2130, 2131, 575, 576, 2132, 4612,
-        2134, 2135, 2136, 4368, 5931, 5932, 5933, 5934, 5935, 5936,
-        5937, 5938, 5939, 2902, 4057, 4058, 4059, 4060, 4062, 4063,
-        4064, 4654, 4655, 4067, 4068, 4069, 4656, 4657, 4073, 4658,
-        4074, 4075, 4659, 4660, 4661, 4076, 4662, 4663, 4664, 4078,
-        4079, 4080, 4665, 4082, 4083, 4084, 4666, 4086, 4087, 4088,
-        544, 545, 546, 547, 548, 549, 550, 559, 1227, 552, 553, 5035,
-        555, 554, 1228, 556, 1229, 557, 558, 560, 561, 562, 563, 564,
-        565, 1230, 566, 567, 568, 569, 570, 572, 573, 222, 7461, 2059,
-        2060, 2061, 5664, 2062, 7463, 16997, 2065, 2066, 2067, 2068,
-        2069, 2070, 2072, 2073, 2074, 2075, 2076, 2077, 2078, 7464,
-        2079, 2080, 2081, 7465, 2082, 2083, 2084, 2085, 2086, 2087,
-        199, 206, 200, 203, 205, 211, 1140, 3699, 209, 214, 215, 216,
-        217, 218, 777, 778, 779, 780, 2298, 781, 782, 783, 784, 785,
-        787, 788, 384, 789, 790, 791, 2677, 793, 794, 795, 796, 797,
-        2307, 798, 799, 801, 802, 3645, 803, 4337, 805, 3648, 3649,
-        807, 808, 3651, 810, 812, 813, 814, 815, 816, 3654, 818, 819,
-        13780, 930, 932, 4221, 935, 936, 938, 2197, 939, 940, 941,
-        2200, 943, 1591, 1952, 2630, 1592, 2631, 1602, 1607, 1595,
-        1596, 1597, 1598, 1599, 1955, 1601, 1603, 1956, 1605, 1606,
-        1608, 1610, 1638, 20608, 968, 969, 970, 971, 972, 973, 974,
-        975, 2729, 2730, 977, 2731, 979, 980, 981, 982, 983, 984, 3506,
-        987, 989, 990, 991, 2732, 2733, 6051, 6053, 6055, 910, 6056,
-        4339, 4340, 577, 4341, 579, 580, 581, 616, 584, 585, 586, 4342,
-        4343, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 5046,
-        599, 600, 5047, 601, 602, 603, 604, 605, 5053, 608, 609, 610,
-        5055, 612, 613, 5056, 615, 617, 618, 619, 620, 621, 622, 623,
-        624, 6882, 627, 628, 629, 630, 631, 5330, 633, 634, 635, 636,
-        637, 639, 640, 7870, 632, 34480, 13118, 903, 904, 905, 907,
-        2616, 2617, 2618, 2619, 2620, 2621, 2622, 2623, 2624, 2643,
-        1685, 1686, 1687, 1688, 1690, 1691, 2644, 2645, 1695, 2646,
-        1699, 2647, 2648, 1702, 2649, 2650, 1706, 22082, 5516, 4307,
-        2203, 1995, 1996, 1998, 1999, 2206, 2002, 2003, 4407, 2005,
-        4408, 2007, 2008, 2009, 2010, 2011, 4409, 2013, 2014, 2015,
-        2017, 3227, 3149, 6025, 22913, 22914, 3228, 7925, 10123, 10124,
-        10125, 10127, 16978, 14094, 1593, 4869, 4870, 3477, 3844, 3845,
-        9923, 3846, 3847, 39767, 39768, 39769, 3541, 39770, 39771,
-        14179, 39772, 39773, 39774, 42558, 1043, 4203, 42559, 42560,
-        42561, 42562, 42563, 42564, 11018, 42565, 42566, 4589, 4590,
-        4591, 4312, 18283, 4317, 4318, 4319, 12659, 11706, 11707,
-        53395, 53396, 29410, 8040, 8041, 915, 20105, 22952, 22953,
-        20596, 4161, 3047, 3048, 3049, 3050, 3051, 3052, 3053, 3054,
-        3055, 1474, 3056, 3057, 3058, 3059, 3060, 3061, 2549, 2551,
-        3062, 3063, 3064, 3065, 3066, 3067, 3068, 3069, 515, 3070,
-        3071, 3072, 3073, 3074, 3075, 3076, 3077, 3078, 3079, 3080,
-        3081, 3082, 506, 3083, 3084, 3085, 3086, 3087, 3088, 3089,
-        3090, 3091, 527, 528, 2995, 530, 531, 533, 534, 535, 537, 538 };
-  }
-
-}
diff --git a/lucene/facet/src/test/org/apache/lucene/util/encoding/EncodingTest.java b/lucene/facet/src/test/org/apache/lucene/util/encoding/EncodingTest.java
deleted file mode 100644
index 6383bea..0000000
--- a/lucene/facet/src/test/org/apache/lucene/util/encoding/EncodingTest.java
+++ /dev/null
@@ -1,159 +0,0 @@
-package org.apache.lucene.util.encoding;
-
-import java.io.IOException;
-import java.util.Arrays;
-
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IntsRef;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class EncodingTest extends FacetTestCase {
-
-  private static IntsRef uniqueSortedData, data;
-  
-  @BeforeClass
-  public static void beforeClassEncodingTest() throws Exception {
-    int capacity = atLeast(10000);
-    data = new IntsRef(capacity);
-    for (int i = 0; i < 10; i++) {
-      data.ints[i] = i + 1; // small values
-    }
-    for (int i = 10; i < data.ints.length; i++) {
-      data.ints[i] = random().nextInt(Integer.MAX_VALUE - 1) + 1; // some encoders don't allow 0
-    }
-    data.length = data.ints.length;
-    
-    uniqueSortedData = IntsRef.deepCopyOf(data);
-    Arrays.sort(uniqueSortedData.ints);
-    uniqueSortedData.length = 0;
-    int prev = -1;
-    for (int i = 0; i < uniqueSortedData.ints.length; i++) {
-      if (uniqueSortedData.ints[i] != prev) {
-        uniqueSortedData.ints[uniqueSortedData.length++] = uniqueSortedData.ints[i];
-        prev = uniqueSortedData.ints[i];
-      }
-    }
-  }
-  
-  private static void encoderTest(IntEncoder encoder, IntsRef data, IntsRef expected) throws IOException {
-    // ensure toString is implemented
-    String toString = encoder.toString();
-    assertFalse(toString.startsWith(encoder.getClass().getName() + "@"));
-    IntDecoder decoder = encoder.createMatchingDecoder();
-    toString = decoder.toString();
-    assertFalse(toString.startsWith(decoder.getClass().getName() + "@"));
-    
-    BytesRef bytes = new BytesRef(100); // some initial capacity - encoders should grow the byte[]
-    IntsRef values = new IntsRef(100); // some initial capacity - decoders should grow the int[]
-    for (int i = 0; i < 2; i++) {
-      // run 2 iterations to catch encoders/decoders which don't reset properly
-      encoding(encoder, data, bytes);
-      decoding(bytes, values, encoder.createMatchingDecoder());
-      assertTrue(expected.intsEquals(values));
-    }
-  }
-
-  private static void encoding(IntEncoder encoder, IntsRef data, BytesRef bytes) throws IOException {
-    final IntsRef values;
-    if (random().nextBoolean()) { // randomly set the offset
-      values = new IntsRef(data.length + 1);
-      System.arraycopy(data.ints, 0, values.ints, 1, data.length);
-      values.offset = 1; // ints start at index 1
-      values.length = data.length;
-    } else {
-      // need to copy the array because it may be modified by encoders (e.g. sorting)
-      values = IntsRef.deepCopyOf(data);
-    }
-    encoder.encode(values, bytes);
-  }
-
-  private static void decoding(BytesRef bytes, IntsRef values, IntDecoder decoder) throws IOException {
-    int offset = 0;
-    if (random().nextBoolean()) { // randomly set the offset and length to other than 0,0
-      bytes.grow(bytes.length + 1); // ensure that we have enough capacity to shift values by 1
-      bytes.offset = 1; // bytes start at index 1 (must do that after grow)
-      System.arraycopy(bytes.bytes, 0, bytes.bytes, 1, bytes.length);
-      offset = 1;
-    }
-    decoder.decode(bytes, values);
-    assertEquals(offset, bytes.offset); // decoders should not mess with offsets
-  }
-
-  @Test
-  public void testVInt8() throws Exception {
-    encoderTest(new VInt8IntEncoder(), data, data);
-    
-    // cover negative numbers;
-    BytesRef bytes = new BytesRef(5);
-    IntEncoder enc = new VInt8IntEncoder();
-    IntsRef values = new IntsRef(1);
-    values.ints[values.length++] = -1;
-    enc.encode(values, bytes);
-    
-    IntDecoder dec = enc.createMatchingDecoder();
-    values.length = 0;
-    dec.decode(bytes, values);
-    assertEquals(1, values.length);
-    assertEquals(-1, values.ints[0]);
-  }
-  
-  @Test
-  public void testSimpleInt() throws Exception {
-    encoderTest(new SimpleIntEncoder(), data, data);
-  }
-  
-  @Test
-  public void testSortingUniqueValues() throws Exception {
-    encoderTest(new SortingIntEncoder(new UniqueValuesIntEncoder(new VInt8IntEncoder())), data, uniqueSortedData);
-  }
-
-  @Test
-  public void testSortingUniqueDGap() throws Exception {
-    encoderTest(new SortingIntEncoder(new UniqueValuesIntEncoder(new DGapIntEncoder(new VInt8IntEncoder()))), data, uniqueSortedData);
-  }
-
-  @Test
-  public void testSortingUniqueDGapEightFlags() throws Exception {
-    encoderTest(new SortingIntEncoder(new UniqueValuesIntEncoder(new DGapIntEncoder(new EightFlagsIntEncoder()))), data, uniqueSortedData);
-  }
-
-  @Test
-  public void testSortingUniqueDGapFourFlags() throws Exception {
-    encoderTest(new SortingIntEncoder(new UniqueValuesIntEncoder(new DGapIntEncoder(new FourFlagsIntEncoder()))), data, uniqueSortedData);
-  }
-
-  @Test
-  public void testSortingUniqueDGapNOnes4() throws Exception {
-    encoderTest(new SortingIntEncoder(new UniqueValuesIntEncoder(new DGapIntEncoder(new NOnesIntEncoder(4)))), data, uniqueSortedData);
-  }
-  
-  @Test
-  public void testSortingUniqueDGapNOnes3() throws Exception {
-    encoderTest(new SortingIntEncoder(new UniqueValuesIntEncoder(new DGapIntEncoder(new NOnesIntEncoder(3)))), data, uniqueSortedData);
-  }
-  
-  @Test
-  public void testSortingUniqueDGapVInt() throws Exception {
-    encoderTest(new SortingIntEncoder(new UniqueValuesIntEncoder(new DGapVInt8IntEncoder())), data, uniqueSortedData);
-  }
-
-}

