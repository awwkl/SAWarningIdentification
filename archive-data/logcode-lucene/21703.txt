GitDiffStart: 8af3598b74d56c69c911270f8486dc964c30c4d3 | Wed Mar 26 13:39:25 2008 +0000
diff --git a/CHANGES.txt b/CHANGES.txt
index d6d3ed7..bc91b25 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -61,6 +61,13 @@ API Changes
 
  7. LUCENE-1234: Make BoostingSpanScorer protected.  (Andi Vajda via Grant Ingersoll)
 
+ 8. LUCENE-510: The index now stores strings as true UTF-8 bytes
+    (previously it was Java's modified UTF-8).  If any text, either
+    stored fields or a token, has illegal UTF-16 surrogate characters,
+    these characters are now silently replaced with the Unicode
+    replacement character U+FFFD.  This is a change to the index file
+    format.  (Marvin Humphrey via Mike McCandless)
+
 Bug fixes
     
  1. LUCENE-1134: Fixed BooleanQuery.rewrite to only optimize a single 
diff --git a/LICENSE.txt b/LICENSE.txt
index d645695..59dbf93 100644
--- a/LICENSE.txt
+++ b/LICENSE.txt
@@ -200,3 +200,32 @@
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.
+
+
+
+Some code in src/java/org/apache/lucene/util/UnicodeUtil.java was
+derived from unicode conversion examples available at
+http://www.unicode.org/Public/PROGRAMS/CVTUTF.  Here is the copyright
+from those sources:
+
+/*
+ * Copyright 2001-2004 Unicode, Inc.
+ * 
+ * Disclaimer
+ * 
+ * This source code is provided as is by Unicode, Inc. No claims are
+ * made as to fitness for any particular purpose. No warranties of any
+ * kind are expressed or implied. The recipient agrees to determine
+ * applicability of information provided. If this file has been
+ * purchased on magnetic or optical media from Unicode, Inc., the
+ * sole remedy for any claim will be exchange of defective media
+ * within 90 days of receipt.
+ * 
+ * Limitations on Rights to Redistribute This Code
+ * 
+ * Unicode, Inc. hereby grants the right to freely use the information
+ * supplied in this file in the creation of products supporting the
+ * Unicode Standard, and to make copies of this file in any form
+ * for internal or external distribution as long as this notice
+ * remains attached.
+ */
diff --git a/NOTICE.txt b/NOTICE.txt
index 3331b44..92fd344 100644
--- a/NOTICE.txt
+++ b/NOTICE.txt
@@ -9,4 +9,3 @@ The snowball stemmers in
 were developed by Martin Porter and Richard Boulton.
 The full snowball package is available from
   http://snowball.tartarus.org/
-
diff --git a/docs/fileformats.html b/docs/fileformats.html
index 40152af..5f860df 100644
--- a/docs/fileformats.html
+++ b/docs/fileformats.html
@@ -1237,16 +1237,14 @@ document.write("Last Published: " + document.lastModified);
 <h3 class="boxed">Chars</h3>
 <p>
                     Lucene writes unicode
-                    character sequences using Java's
-                    <a href="http://en.wikipedia.org/wiki/UTF-8#Modified_UTF-8">"modified
-                        UTF-8 encoding"</a>
-                    .
+                    character sequences as UTF-8 encoded bytes.
                 </p>
-<a name="N10433"></a><a name="String"></a>
+<a name="N1042F"></a><a name="String"></a>
 <h3 class="boxed">String</h3>
 <p>
-                    Lucene writes strings as a VInt representing the length, followed by
-                    the character data.
+		    Lucene writes strings as UTF-8 encoded bytes.
+                    First the length, in bytes, is written as a VInt,
+                    followed by the bytes.
                 </p>
 <p>
                     String --&gt; VInt, Chars
@@ -1254,13 +1252,13 @@ document.write("Last Published: " + document.lastModified);
 </div>
 
         
-<a name="N10440"></a><a name="Per-Index Files"></a>
+<a name="N1043C"></a><a name="Per-Index Files"></a>
 <h2 class="boxed">Per-Index Files</h2>
 <div class="section">
 <p>
                 The files in this section exist one-per-index.
             </p>
-<a name="N10448"></a><a name="Segments File"></a>
+<a name="N10444"></a><a name="Segments File"></a>
 <h3 class="boxed">Segments File</h3>
 <p>
                     The active segments in the index are stored in the
@@ -1421,7 +1419,7 @@ document.write("Last Published: " + document.lastModified);
 		    This is used to verify integrity of the file on
 		    opening the index.
 		</p>
-<a name="N104DC"></a><a name="Lock File"></a>
+<a name="N104D8"></a><a name="Lock File"></a>
 <h3 class="boxed">Lock File</h3>
 <p>
                     The write lock, which is stored in the index
@@ -1439,7 +1437,7 @@ document.write("Last Published: " + document.lastModified);
                     Note that prior to version 2.1, Lucene also used a
                     commit lock. This was removed in 2.1.
                 </p>
-<a name="N104E8"></a><a name="Deletable File"></a>
+<a name="N104E4"></a><a name="Deletable File"></a>
 <h3 class="boxed">Deletable File</h3>
 <p>
                     Prior to Lucene 2.1 there was a file "deletable"
@@ -1448,7 +1446,7 @@ document.write("Last Published: " + document.lastModified);
                     the files that are deletable, instead, so no file
                     is written.
                 </p>
-<a name="N104F1"></a><a name="Compound Files"></a>
+<a name="N104ED"></a><a name="Compound Files"></a>
 <h3 class="boxed">Compound Files</h3>
 <p>Starting with Lucene 1.4 the compound file format became default. This
                     is simply a container for all files described in the next section
@@ -1475,14 +1473,14 @@ document.write("Last Published: " + document.lastModified);
 </div>
 
         
-<a name="N10519"></a><a name="Per-Segment Files"></a>
+<a name="N10515"></a><a name="Per-Segment Files"></a>
 <h2 class="boxed">Per-Segment Files</h2>
 <div class="section">
 <p>
                 The remaining files are all per-segment, and are
                 thus defined by suffix.
             </p>
-<a name="N10521"></a><a name="Fields"></a>
+<a name="N1051D"></a><a name="Fields"></a>
 <h3 class="boxed">Fields</h3>
 <p>
                     
@@ -1701,7 +1699,7 @@ document.write("Last Published: " + document.lastModified);
 </li>
                 
 </ol>
-<a name="N105DC"></a><a name="Term Dictionary"></a>
+<a name="N105D8"></a><a name="Term Dictionary"></a>
 <h3 class="boxed">Term Dictionary</h3>
 <p>
                     The term dictionary is represented as two files:
@@ -1764,10 +1762,12 @@ document.write("Last Published: " + document.lastModified);
                             --&gt; VInt
                         </p>
                         
-<p>This
-                            file is sorted by Term. Terms are ordered first lexicographically
-                            by the term's field name, and within that lexicographically by the
-                            term's text.
+<p>
+			    This file is sorted by Term. Terms are
+                            ordered first lexicographically (by UTF16
+                            character code) by the term's field name,
+                            and within that lexicographically (by
+                            UTF16 character code) by the term's text.
                         </p>
                         
 <p>TIVersion names the version of the format
@@ -1887,7 +1887,7 @@ document.write("Last Published: " + document.lastModified);
 </li>
                 
 </ol>
-<a name="N1065C"></a><a name="Frequencies"></a>
+<a name="N10658"></a><a name="Frequencies"></a>
 <h3 class="boxed">Frequencies</h3>
 <p>
                     The .frq file contains the lists of documents
@@ -2005,7 +2005,7 @@ document.write("Last Published: " + document.lastModified);
                    entry in level-1. In the example has entry 15 on level 1 a pointer to entry 15 on level 0 and entry 31 on level 1 a pointer
                    to entry 31 on level 0.                   
                 </p>
-<a name="N106DE"></a><a name="Positions"></a>
+<a name="N106DA"></a><a name="Positions"></a>
 <h3 class="boxed">Positions</h3>
 <p>
                     The .prx file contains the lists of positions that
@@ -2071,7 +2071,7 @@ document.write("Last Published: " + document.lastModified);
                     Payload. If PayloadLength is not stored, then this Payload has the same
                     length as the Payload at the previous position.
                 </p>
-<a name="N1071A"></a><a name="Normalization Factors"></a>
+<a name="N10716"></a><a name="Normalization Factors"></a>
 <h3 class="boxed">Normalization Factors</h3>
 <p>
                     
@@ -2175,7 +2175,7 @@ document.write("Last Published: " + document.lastModified);
 <b>2.1 and above:</b>
                     Separate norm files are created (when adequate) for both compound and non compound segments.
                 </p>
-<a name="N10783"></a><a name="Term Vectors"></a>
+<a name="N1077F"></a><a name="Term Vectors"></a>
 <h3 class="boxed">Term Vectors</h3>
 <p>
 		  Term Vector support is an optional on a field by
@@ -2308,7 +2308,7 @@ document.write("Last Published: " + document.lastModified);
 </li>
                 
 </ol>
-<a name="N10819"></a><a name="Deleted Documents"></a>
+<a name="N10815"></a><a name="Deleted Documents"></a>
 <h3 class="boxed">Deleted Documents</h3>
 <p>The .del file is
                     optional, and only exists when a segment contains deletions.
@@ -2380,7 +2380,7 @@ document.write("Last Published: " + document.lastModified);
 </div>
 
         
-<a name="N1085C"></a><a name="Limitations"></a>
+<a name="N10858"></a><a name="Limitations"></a>
 <h2 class="boxed">Limitations</h2>
 <div class="section">
 <p>There
diff --git a/docs/fileformats.pdf b/docs/fileformats.pdf
index 2dd3e7f..63f066c 100644
--- a/docs/fileformats.pdf
+++ b/docs/fileformats.pdf
@@ -25,7 +25,7 @@ Table of contents
     6.3 Deletable File...............................................................................................................10
     6.4 Compound Files...........................................................................................................10
    7 Per-Segment Files............................................................................................................ 10
-    7.1 Fields........................................................................................................................... 10
+    7.1 Fields........................................................................................................................... 11
 
                    Copyright © 2006 The Apache Software Foundation. All rights reserved.
                                                                                                             Apache Lucene - Index File Formats
@@ -238,7 +238,7 @@ VInt Encoding Example
 Copyright © 2006 The Apache Software Foundation. All rights reserved.
 Apache Lucene - Index File Formats
 
-Value   First byte                  Second byte                   Third byte
+Value   First byte                  Second byte                    Third byte
 
 0 00000000
 
@@ -260,26 +260,29 @@ Value   First byte                  Second byte                   Third byte
 
 16,383  11111111                    01111111
 
-16,384  10000000                    10000000                      00000001
+16,384  10000000                    10000000                       00000001
 
-16,385  10000001                    10000000                      00000001
+16,385  10000001                    10000000                       00000001
 
  ...
 
 This provides compression while still being efficient to decode.
 
 5.5. Chars
-Lucene writes unicode character sequences using Java's "modified UTF-8 encoding" .
+Lucene writes unicode character sequences as UTF-8 encoded bytes.
 
 5.6. String
-Lucene writes strings as a VInt representing the length, followed by the character data.
-String --> VInt, Chars
+
+Lucene writes strings as UTF-8 encoded bytes. First the length, in bytes, is written as a VInt,
+followed by the bytes.
 
 Page 7
 
         Copyright © 2006 The Apache Software Foundation. All rights reserved.
 Apache Lucene - Index File Formats
 
+String --> VInt, Chars
+
 6. Per-Index Files
 
 The files in this section exist one-per-index.
@@ -322,13 +325,13 @@ Version, DelGen, NormGen, Checksum --> Int64
 
 SegName, DocStoreSegment --> String
 
-IsCompoundFile, HasSingleNormFile, DocStoreIsCompoundFile --> Int8
-
                                                                        Page 8
 
 Copyright © 2006 The Apache Software Foundation. All rights reserved.
 Apache Lucene - Index File Formats
 
+IsCompoundFile, HasSingleNormFile, DocStoreIsCompoundFile --> Int8
+
 Format is -1 as of Lucene 1.4, -3 (SegmentInfos.FORMAT_SINGLE_NORM_FILE) as of
 Lucene 2.1 and 2.2, -4 (SegmentInfos.FORMAT_SHARED_DOC_STORE) as of Lucene 2.3
 and -5 (SegmentInfos.FORMAT_CHECKSUM) as of Lucene 2.4.
@@ -372,13 +375,14 @@ store files where this segment's documents begin. In this case, this segment doe
 own doc store files but instead shares a single set of these files with other segments.
 
 Checksum contains the CRC32 checksum of all bytes in the segments_N file up until the
-checksum. This is used to verify integrity of the file on opening the index.
 
 Page 9
 
         Copyright © 2006 The Apache Software Foundation. All rights reserved.
 Apache Lucene - Index File Formats
 
+checksum. This is used to verify integrity of the file on opening the index.
+
 6.2. Lock File
 
 The write lock, which is stored in the index directory by default, is named "write.lock". If the
@@ -502,8 +506,9 @@ Copyright © 2006 The Apache Software Foundation. All rights reserved.
     PrefixLength, DocFreq, FreqDelta, ProxDelta, SkipDelta
     --> VInt
 
-    This file is sorted by Term. Terms are ordered first lexicographically by the term's field
-    name, and within that lexicographically by the term's text.
+    This file is sorted by Term. Terms are ordered first lexicographically (by UTF16
+    character code) by the term's field name, and within that lexicographically (by UTF16
+    character code) by the term's text.
 
     TIVersion names the version of the format of this file and is -2 in Lucene 1.4.
 
diff --git a/src/java/org/apache/lucene/document/Document.java b/src/java/org/apache/lucene/document/Document.java
index 85bb1ea..7291cc2 100644
--- a/src/java/org/apache/lucene/document/Document.java
+++ b/src/java/org/apache/lucene/document/Document.java
@@ -18,9 +18,9 @@ package org.apache.lucene.document;
  */
 
 import java.util.*;             // for javadoc
-import org.apache.lucene.search.Hits; // for javadoc
-import org.apache.lucene.search.Searcher; // for javadoc
-import org.apache.lucene.index.IndexReader; // for javadoc
+import org.apache.lucene.search.Hits;  // for javadoc
+import org.apache.lucene.search.Searcher;  // for javadoc
+import org.apache.lucene.index.IndexReader;  // for javadoc
 
 /** Documents are the unit of indexing and search.
  *
diff --git a/src/java/org/apache/lucene/index/DocumentsWriter.java b/src/java/org/apache/lucene/index/DocumentsWriter.java
index a6896d0..2232f48 100644
--- a/src/java/org/apache/lucene/index/DocumentsWriter.java
+++ b/src/java/org/apache/lucene/index/DocumentsWriter.java
@@ -28,6 +28,7 @@ import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IndexOutput;
 import org.apache.lucene.store.IndexInput;
 import org.apache.lucene.store.AlreadyClosedException;
+import org.apache.lucene.util.UnicodeUtil;
 
 import java.io.IOException;
 import java.io.PrintStream;
@@ -291,7 +292,7 @@ final class DocumentsWriter {
         assert docStoreSegment != null;
         fieldsWriter.close();
         fieldsWriter = null;
-        assert numDocsInStore*8 == directory.fileLength(docStoreSegment + "." + IndexFileNames.FIELDS_INDEX_EXTENSION):
+        assert 4+numDocsInStore*8 == directory.fileLength(docStoreSegment + "." + IndexFileNames.FIELDS_INDEX_EXTENSION):
           "after flush: fdx size mismatch: " + numDocsInStore + " docs vs " + directory.fileLength(docStoreSegment + "." + IndexFileNames.FIELDS_INDEX_EXTENSION) + " length in bytes of " + docStoreSegment + "." + IndexFileNames.FIELDS_INDEX_EXTENSION;
       }
 
@@ -754,27 +755,26 @@ final class DocumentsWriter {
     return segment + "." + extension;
   }
 
-  static int compareText(final char[] text1, int pos1, final char[] text2, int pos2) {
+  private static int compareText(final char[] text1, int pos1, final char[] text2, int pos2) {
     while(true) {
       final char c1 = text1[pos1++];
       final char c2 = text2[pos2++];
-      if (c1 < c2)
+      if (c1 != c2) {
         if (0xffff == c2)
           return 1;
-        else
-          return -1;
-      else if (c2 < c1)
-        if (0xffff == c1)
+        else if (0xffff == c1)
           return -1;
         else
-          return 1;
-      else if (0xffff == c1)
+          return c1-c2;
+      } else if (0xffff == c1)
         return 0;
     }
   }
 
   private final TermInfo termInfo = new TermInfo(); // minimize consing
 
+  final UnicodeUtil.UTF8Result termsUTF8 = new UnicodeUtil.UTF8Result();
+
   /* Walk through all unique text tokens (Posting
    * instances) found in this field and serialize them
    * into a single RAM segment. */
@@ -831,9 +831,6 @@ final class DocumentsWriter {
 
       final char[] text = termStates[0].text;
       final int start = termStates[0].textOffset;
-      int pos = start;
-      while(text[pos] != 0xffff)
-        pos++;
 
       long freqPointer = freqOut.getFilePointer();
       long proxPointer = proxOut.getFilePointer();
@@ -932,7 +929,17 @@ final class DocumentsWriter {
 
       // Write term
       termInfo.set(df, freqPointer, proxPointer, (int) (skipPointer - freqPointer));
-      termsOut.add(fieldNumber, text, start, pos-start, termInfo);
+
+      // TODO: we could do this incrementally
+      UnicodeUtil.UTF16toUTF8(text, start, termsUTF8);
+
+      // TODO: we could save O(n) re-scan of the term by
+      // computing the shared prefix with the last term
+      // while during the UTF8 encoding
+      termsOut.add(fieldNumber,
+                   termsUTF8.result,
+                   termsUTF8.length,
+                   termInfo);
     }
   }
 
@@ -1048,7 +1055,12 @@ final class DocumentsWriter {
           // This call is not synchronized and does all the work
           state.processDocument(analyzer);
         } finally {
-          // This call is synchronized but fast
+          // Note that we must call finishDocument even on
+          // exception, because for a non-aborting
+          // exception, a portion of the document has been
+          // indexed (and its ID is marked for deletion), so
+          // all index files must be updated to record this
+          // document.  This call is synchronized but fast.
           finishDocument(state);
         }
         success = true;
diff --git a/src/java/org/apache/lucene/index/DocumentsWriterFieldData.java b/src/java/org/apache/lucene/index/DocumentsWriterFieldData.java
index f691271..3f14cc6 100644
--- a/src/java/org/apache/lucene/index/DocumentsWriterFieldData.java
+++ b/src/java/org/apache/lucene/index/DocumentsWriterFieldData.java
@@ -22,6 +22,7 @@ import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.Token;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.UnicodeUtil;
 import java.io.IOException;
 import java.io.Reader;
 import java.util.Arrays;
@@ -337,12 +338,36 @@ final class DocumentsWriterFieldData implements Comparable {
 
     int code = 0;
 
-    // Compute hashcode
+    // Compute hashcode & replace any invalid UTF16 sequences
     int downto = tokenTextLen;
-    while (downto > 0)
-      code = (code*31) + tokenText[--downto];
+    while (downto > 0) {
+      char ch = tokenText[--downto];
+
+      if (ch >= UnicodeUtil.UNI_SUR_LOW_START && ch <= UnicodeUtil.UNI_SUR_LOW_END) {
+        if (0 == downto) {
+          // Unpaired
+          ch = tokenText[downto] = UnicodeUtil.UNI_REPLACEMENT_CHAR;
+        } else {
+          final char ch2 = tokenText[downto-1];
+          if (ch2 >= UnicodeUtil.UNI_SUR_HIGH_START && ch2 <= UnicodeUtil.UNI_SUR_HIGH_END) {
+            // OK: high followed by low.  This is a valid
+            // surrogate pair.
+            code = ((code*31) + ch)*31+ch2;
+            downto--;
+            continue;
+          } else {
+            // Unpaired
+            ch = tokenText[downto] = UnicodeUtil.UNI_REPLACEMENT_CHAR;
+          }            
+        }
+      } else if (ch >= UnicodeUtil.UNI_SUR_HIGH_START && ch <= UnicodeUtil.UNI_SUR_HIGH_END)
+        // Unpaired
+        ch = tokenText[downto] = UnicodeUtil.UNI_REPLACEMENT_CHAR;
+
+      code = (code*31) + ch;
+    }
 
-    // System.out.println("  addPosition: buffer=" + new String(tokenText, 0, tokenTextLen) + " pos=" + position + " offsetStart=" + (offset+token.startOffset()) + " offsetEnd=" + (offset + token.endOffset()) + " docID=" + docID + " doPos=" + doVectorPositions + " doOffset=" + doVectorOffsets);
+    // System.out.println("  addPosition: field=" + fieldInfo.name + " buffer=" + new String(tokenText, 0, tokenTextLen) + " pos=" + position + " offsetStart=" + (offset+token.startOffset()) + " offsetEnd=" + (offset + token.endOffset()) + " docID=" + docID + " doPos=" + doVectorPositions + " doOffset=" + doVectorOffsets);
 
     int hashPos = code & postingsHashMask;
 
@@ -713,7 +738,8 @@ final class DocumentsWriterFieldData implements Comparable {
 
     threadState.doVectorSort(postingsVectors, numPostingsVectors);
 
-    Posting lastPosting = null;
+    int encoderUpto = 0;
+    int lastTermBytesCount = 0;
 
     final ByteSliceReader reader = vectorSliceReader;
     final char[][] charBuffers = threadState.charPool.buffers;
@@ -723,40 +749,37 @@ final class DocumentsWriterFieldData implements Comparable {
       Posting posting = vector.p;
       final int freq = posting.docFreq;
           
-      final int prefix;
       final char[] text2 = charBuffers[posting.textStart >> DocumentsWriter.CHAR_BLOCK_SHIFT];
       final int start2 = posting.textStart & DocumentsWriter.CHAR_BLOCK_MASK;
-      int pos2 = start2;
 
+      // We swap between two encoders to save copying
+      // last Term's byte array
+      final UnicodeUtil.UTF8Result utf8Result = threadState.utf8Results[encoderUpto];
+
+      // TODO: we could do this incrementally
+      UnicodeUtil.UTF16toUTF8(text2, start2, utf8Result);
+      final int termBytesCount = utf8Result.length;
+
+      // TODO: UTF16toUTF8 could tell us this prefix
       // Compute common prefix between last term and
       // this term
-      if (lastPosting == null)
-        prefix = 0;
-      else {
-        final char[] text1 = charBuffers[lastPosting.textStart >> DocumentsWriter.CHAR_BLOCK_SHIFT];
-        final int start1 = lastPosting.textStart & DocumentsWriter.CHAR_BLOCK_MASK;
-        int pos1 = start1;
-        while(true) {
-          final char c1 = text1[pos1];
-          final char c2 = text2[pos2];
-          if (c1 != c2 || c1 == 0xffff) {
-            prefix = pos1-start1;
+      int prefix = 0;
+      if (j > 0) {
+        final byte[] lastTermBytes = threadState.utf8Results[1-encoderUpto].result;
+        final byte[] termBytes = threadState.utf8Results[encoderUpto].result;
+        while(prefix < lastTermBytesCount && prefix < termBytesCount) {
+          if (lastTermBytes[prefix] != termBytes[prefix])
             break;
-          }
-          pos1++;
-          pos2++;
+          prefix++;
         }
       }
-      lastPosting = posting;
-
-      // Compute length
-      while(text2[pos2] != 0xffff)
-        pos2++;
+      encoderUpto = 1-encoderUpto;
+      lastTermBytesCount = termBytesCount;
 
-      final int suffix = pos2 - start2 - prefix;
+      final int suffix = termBytesCount - prefix;
       tvfLocal.writeVInt(prefix);
       tvfLocal.writeVInt(suffix);
-      tvfLocal.writeChars(text2, start2 + prefix, suffix);
+      tvfLocal.writeBytes(utf8Result.result, prefix, suffix);
       tvfLocal.writeVInt(freq);
 
       if (doVectorPositions) {
diff --git a/src/java/org/apache/lucene/index/DocumentsWriterThreadState.java b/src/java/org/apache/lucene/index/DocumentsWriterThreadState.java
index 0d97297..78a5013 100644
--- a/src/java/org/apache/lucene/index/DocumentsWriterThreadState.java
+++ b/src/java/org/apache/lucene/index/DocumentsWriterThreadState.java
@@ -24,6 +24,7 @@ import org.apache.lucene.store.IndexOutput;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Fieldable;
 import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.util.UnicodeUtil;
 
 /** Used by DocumentsWriter to maintain per-thread state.
  *  We keep a separate Posting hash and other state for each
@@ -311,6 +312,7 @@ final class DocumentsWriterThreadState {
       if (docWriter.fieldsWriter == null) {
         assert docWriter.docStoreSegment == null;
         assert docWriter.segment != null;
+        docWriter.files = null;
         docWriter.docStoreSegment = docWriter.segment;
         // If we hit an exception while init'ing the
         // fieldsWriter, we must abort this segment
@@ -321,7 +323,6 @@ final class DocumentsWriterThreadState {
         } catch (Throwable t) {
           throw new AbortException(t, docWriter);
         }
-        docWriter.files = null;
       }
       localFieldsWriter = new FieldsWriter(null, fdtLocal, docWriter.fieldInfos);
     }
@@ -331,17 +332,18 @@ final class DocumentsWriterThreadState {
     if (docHasVectors) {
       if (docWriter.tvx == null) {
         assert docWriter.docStoreSegment != null;
+        docWriter.files = null;
         // If we hit an exception while init'ing the term
         // vector output files, we must abort this segment
         // because those files will be in an unknown
         // state:
         try {
           docWriter.tvx = docWriter.directory.createOutput(docWriter.docStoreSegment + "." + IndexFileNames.VECTORS_INDEX_EXTENSION);
-          docWriter.tvx.writeInt(TermVectorsReader.FORMAT_VERSION2);
+          docWriter.tvx.writeInt(TermVectorsReader.FORMAT_CURRENT);
           docWriter.tvd = docWriter.directory.createOutput(docWriter.docStoreSegment +  "." + IndexFileNames.VECTORS_DOCUMENTS_EXTENSION);
-          docWriter.tvd.writeInt(TermVectorsReader.FORMAT_VERSION2);
+          docWriter.tvd.writeInt(TermVectorsReader.FORMAT_CURRENT);
           docWriter.tvf = docWriter.directory.createOutput(docWriter.docStoreSegment +  "." + IndexFileNames.VECTORS_FIELDS_EXTENSION);
-          docWriter.tvf.writeInt(TermVectorsReader.FORMAT_VERSION2);
+          docWriter.tvf.writeInt(TermVectorsReader.FORMAT_CURRENT);
 
           // We must "catch up" for all docs before us
           // that had no vectors:
@@ -353,7 +355,6 @@ final class DocumentsWriterThreadState {
         } catch (Throwable t) {
           throw new AbortException(t, docWriter);
         }
-        docWriter.files = null;
       }
       numVectorFields = 0;
     }
@@ -672,21 +673,23 @@ final class DocumentsWriterThreadState {
     int pos1 = p1.textStart & DocumentsWriter.CHAR_BLOCK_MASK;
     final char[] text2 = charPool.buffers[p2.textStart >> DocumentsWriter.CHAR_BLOCK_SHIFT];
     int pos2 = p2.textStart & DocumentsWriter.CHAR_BLOCK_MASK;
+
+    assert text1 != text2 || pos1 != pos2;
+
     while(true) {
       final char c1 = text1[pos1++];
       final char c2 = text2[pos2++];
-      if (c1 < c2)
+      if (c1 != c2) {
         if (0xffff == c2)
           return 1;
-        else
-          return -1;
-      else if (c2 < c1)
-        if (0xffff == c1)
+        else if (0xffff == c1)
           return -1;
         else
-          return 1;
-      else if (0xffff == c1)
-        return 0;
+          return c1-c2;
+      } else
+        // This method should never compare equal postings
+        // unless p1==p2
+        assert c1 != 0xffff;
     }
   }
 
@@ -715,5 +718,8 @@ final class DocumentsWriterThreadState {
 
   // Used to read a string value for a field
   ReusableStringReader stringReader = new ReusableStringReader();
+
+  final UnicodeUtil.UTF8Result utf8Results[] = {new UnicodeUtil.UTF8Result(),
+                                                new UnicodeUtil.UTF8Result()};
 }
 
diff --git a/src/java/org/apache/lucene/index/FieldsReader.java b/src/java/org/apache/lucene/index/FieldsReader.java
index 13a8777..45e06f2 100644
--- a/src/java/org/apache/lucene/index/FieldsReader.java
+++ b/src/java/org/apache/lucene/index/FieldsReader.java
@@ -51,6 +51,8 @@ final class FieldsReader {
   private int numTotalDocs;
   private int size;
   private boolean closed;
+  private final int format;
+  private final int formatSize;
 
   // The docID offset where our docs begin in the index
   // file.  This will be 0 if we have our own private file.
@@ -72,9 +74,33 @@ final class FieldsReader {
     try {
       fieldInfos = fn;
 
-      cloneableFieldsStream = d.openInput(segment + ".fdt", readBufferSize);
+      cloneableFieldsStream = d.openInput(segment + "." + IndexFileNames.FIELDS_EXTENSION, readBufferSize);
+      indexStream = d.openInput(segment + "." + IndexFileNames.FIELDS_INDEX_EXTENSION, readBufferSize);
+
+      // First version of fdx did not include a format
+      // header, but, the first int will always be 0 in that
+      // case
+      int firstInt = indexStream.readInt();
+      if (firstInt == 0)
+        format = 0;
+      else
+        format = firstInt;
+
+      if (format > FieldsWriter.FORMAT_CURRENT)
+        throw new CorruptIndexException("Incompatible format version: " + format + " expected " 
+                                        + FieldsWriter.FORMAT_CURRENT + " or lower");
+
+      if (format > FieldsWriter.FORMAT)
+        formatSize = 4;
+      else
+        formatSize = 0;
+
+      if (format < FieldsWriter.FORMAT_VERSION_UTF8_LENGTH_IN_BYTES)
+        cloneableFieldsStream.setModifiedUTF8StringsMode();
+
       fieldsStream = (IndexInput) cloneableFieldsStream.clone();
-      indexStream = d.openInput(segment + ".fdx", readBufferSize);
+
+      final long indexSize = indexStream.length()-formatSize;
 
       if (docStoreOffset != -1) {
         // We read only a slice out of this shared fields file
@@ -83,13 +109,13 @@ final class FieldsReader {
 
         // Verify the file is long enough to hold all of our
         // docs
-        assert ((int) (indexStream.length() / 8)) >= size + this.docStoreOffset;
+        assert ((int) (indexSize / 8)) >= size + this.docStoreOffset;
       } else {
         this.docStoreOffset = 0;
-        this.size = (int) (indexStream.length() >> 3);
+        this.size = (int) (indexSize >> 3);
       }
 
-      numTotalDocs = (int) (indexStream.length() >> 3);
+      numTotalDocs = (int) (indexSize >> 3);
       success = true;
     } finally {
       // With lock-less commits, it's entirely possible (and
@@ -142,8 +168,12 @@ final class FieldsReader {
     return size;
   }
 
+  private final void seekIndex(int docID) throws IOException {
+    indexStream.seek(formatSize + (docID + docStoreOffset) * 8L);
+  }
+
   final Document doc(int n, FieldSelector fieldSelector) throws CorruptIndexException, IOException {
-    indexStream.seek((n + docStoreOffset) * 8L);
+    seekIndex(n);
     long position = indexStream.readLong();
     fieldsStream.seek(position);
 
@@ -195,7 +225,7 @@ final class FieldsReader {
    *  startDocID.  Returns the IndexInput (the fieldStream),
    *  already seeked to the starting point for startDocID.*/
   final IndexInput rawDocs(int[] lengths, int startDocID, int numDocs) throws IOException {
-    indexStream.seek((docStoreOffset+startDocID) * 8L);
+    seekIndex(startDocID);
     long startOffset = indexStream.readLong();
     long lastOffset = startOffset;
     int count = 0;
@@ -225,13 +255,12 @@ final class FieldsReader {
   }
   
   private void skipField(boolean binary, boolean compressed, int toRead) throws IOException {
-    if (binary || compressed) {
-      long pointer = fieldsStream.getFilePointer();
-      fieldsStream.seek(pointer + toRead);
-    } else {
-      //We need to skip chars.  This will slow us down, but still better
-      fieldsStream.skipChars(toRead);
-    }
+   if (format >= FieldsWriter.FORMAT_VERSION_UTF8_LENGTH_IN_BYTES || binary || compressed) {
+     fieldsStream.seek(fieldsStream.getFilePointer() + toRead);
+   } else {
+     // We need to skip chars.  This will slow us down, but still better
+     fieldsStream.skipChars(toRead);
+   }
   }
 
   private void addFieldLazy(Document doc, FieldInfo fi, boolean binary, boolean compressed, boolean tokenize) throws IOException {
@@ -265,7 +294,10 @@ final class FieldsReader {
         int length = fieldsStream.readVInt();
         long pointer = fieldsStream.getFilePointer();
         //Skip ahead of where we are by the length of what is stored
-        fieldsStream.skipChars(length);
+        if (format >= FieldsWriter.FORMAT_VERSION_UTF8_LENGTH_IN_BYTES)
+          fieldsStream.seek(pointer+length);
+        else
+          fieldsStream.skipChars(length);
         f = new LazyField(fi.name, store, index, termVector, length, pointer, binary);
         f.setOmitNorms(fi.omitNorms);
       }
@@ -471,10 +503,16 @@ final class FieldsReader {
               localFieldsStream.readBytes(b, 0, b.length);
               fieldsData = new String(uncompress(b), "UTF-8");
             } else {
-              //read in chars b/c we already know the length we need to read
-              char[] chars = new char[toRead];
-              localFieldsStream.readChars(chars, 0, toRead);
-              fieldsData = new String(chars);
+              if (format >= FieldsWriter.FORMAT_VERSION_UTF8_LENGTH_IN_BYTES) {
+                byte[] bytes = new byte[toRead];
+                localFieldsStream.readBytes(bytes, 0, toRead);
+                fieldsData = new String(bytes, "UTF-8");
+              } else {
+                //read in chars b/c we already know the length we need to read
+                char[] chars = new char[toRead];
+                localFieldsStream.readChars(chars, 0, toRead);
+                fieldsData = new String(chars);
+              }
             }
           } catch (IOException e) {
             throw new FieldReaderException(e);
diff --git a/src/java/org/apache/lucene/index/FieldsWriter.java b/src/java/org/apache/lucene/index/FieldsWriter.java
index ac733e3..5dfb5b6 100644
--- a/src/java/org/apache/lucene/index/FieldsWriter.java
+++ b/src/java/org/apache/lucene/index/FieldsWriter.java
@@ -33,6 +33,17 @@ final class FieldsWriter
   static final byte FIELD_IS_TOKENIZED = 0x1;
   static final byte FIELD_IS_BINARY = 0x2;
   static final byte FIELD_IS_COMPRESSED = 0x4;
+
+  // Original format
+  static final int FORMAT = 0;
+
+  // Changed strings to UTF8
+  static final int FORMAT_VERSION_UTF8_LENGTH_IN_BYTES = 1;
+
+  // NOTE: if you introduce a new format, make it 1 higher
+  // than the current one, and always change this if you
+  // switch to a new format!
+  static final int FORMAT_CURRENT = FORMAT_VERSION_UTF8_LENGTH_IN_BYTES;
   
     private FieldInfos fieldInfos;
 
@@ -44,8 +55,34 @@ final class FieldsWriter
 
     FieldsWriter(Directory d, String segment, FieldInfos fn) throws IOException {
         fieldInfos = fn;
-        fieldsStream = d.createOutput(segment + ".fdt");
-        indexStream = d.createOutput(segment + ".fdx");
+        
+        boolean success = false;
+        final String fieldsName = segment + "." + IndexFileNames.FIELDS_EXTENSION;
+        try {
+          fieldsStream = d.createOutput(fieldsName);
+          fieldsStream.writeInt(FORMAT_CURRENT);
+          success = true;
+        } finally {
+          if (!success) {
+            close();
+            d.deleteFile(fieldsName);
+          }
+        }
+
+        success = false;
+        final String indexName = segment + "." + IndexFileNames.FIELDS_INDEX_EXTENSION;
+        try {
+          indexStream = d.createOutput(indexName);
+          indexStream.writeInt(FORMAT_CURRENT);
+          success = true;
+        } finally {
+          if (!success) {
+            close();
+            d.deleteFile(fieldsName);
+            d.deleteFile(indexName);
+          }
+        }
+
         doClose = true;
     }
 
@@ -73,8 +110,10 @@ final class FieldsWriter
 
     final void close() throws IOException {
       if (doClose) {
-        fieldsStream.close();
-        indexStream.close();
+        if (fieldsStream != null)
+          fieldsStream.close();
+        if (indexStream != null)
+          indexStream.close();
       }
     }
 
diff --git a/src/java/org/apache/lucene/index/IndexWriter.java b/src/java/org/apache/lucene/index/IndexWriter.java
index d18d3e1..a09d8ef 100644
--- a/src/java/org/apache/lucene/index/IndexWriter.java
+++ b/src/java/org/apache/lucene/index/IndexWriter.java
@@ -67,6 +67,7 @@ import java.util.Iterator;
   (which just deletes and then adds the entire document).
   When finished adding, deleting and updating documents, <a href="#close()"><b>close</b></a> should be called.</p>
 
+  <a name="flush"></a>
   <p>These changes are buffered in memory and periodically
   flushed to the {@link Directory} (during the above method
   calls).  A flush is triggered when there are enough
@@ -1843,26 +1844,30 @@ public class IndexWriter {
    * partially succeeded).</p>
    *
    * <p> This method periodically flushes pending documents
-   * to the Directory (every {@link #setMaxBufferedDocs}),
-   * and also periodically merges segments in the index
-   * (every {@link #setMergeFactor} flushes).  When this
-   * occurs, the method will take more time to run (possibly
-   * a long time if the index is large), and will require
-   * free temporary space in the Directory to do the
-   * merging.</p>
+   * to the Directory (see <a href="#flush">above</a>), and
+   * also periodically triggers segment merges in the index
+   * according to the {@link MergePolicy} in use.</p>
    *
-   * <p>The amount of free space required when a merge is triggered is
-   * up to 1X the size of all segments being merged, when no
-   * readers/searchers are open against the index, and up to 2X the
-   * size of all segments being merged when readers/searchers are open
-   * against the index (see {@link #optimize()} for details). The
-   * sequence of primitive merge operations performed is governed by
-   * the merge policy.
+   * <p>Merges temporarily consume space in the
+   * directory. The amount of space required is up to 1X the
+   * size of all segments being merged, when no
+   * readers/searchers are open against the index, and up to
+   * 2X the size of all segments being merged when
+   * readers/searchers are open against the index (see
+   * {@link #optimize()} for details). The sequence of
+   * primitive merge operations performed is governed by the
+   * merge policy.
    *
    * <p>Note that each term in the document can be no longer
    * than 16383 characters, otherwise an
    * IllegalArgumentException will be thrown.</p>
    *
+   * <p>Note that it's possible to create an invalid Unicode
+   * string in java if a UTF16 surrogate pair is malformed.
+   * In this case, the invalid characters are silently
+   * replaced with the Unicode replacement character
+   * U+FFFD.</p>
+   *
    * @throws CorruptIndexException if the index is corrupt
    * @throws IOException if there is a low-level IO error
    */
diff --git a/src/java/org/apache/lucene/index/SegmentMerger.java b/src/java/org/apache/lucene/index/SegmentMerger.java
index 60a8328..f4f299a 100644
--- a/src/java/org/apache/lucene/index/SegmentMerger.java
+++ b/src/java/org/apache/lucene/index/SegmentMerger.java
@@ -349,7 +349,7 @@ final class SegmentMerger {
         fieldsWriter.close();
       }
 
-      assert docCount*8 == directory.fileLength(segment + "." + IndexFileNames.FIELDS_INDEX_EXTENSION) :
+      assert 4+docCount*8 == directory.fileLength(segment + "." + IndexFileNames.FIELDS_INDEX_EXTENSION) :
         "after mergeFields: fdx size mismatch: " + docCount + " docs vs " + directory.fileLength(segment + "." + IndexFileNames.FIELDS_INDEX_EXTENSION) + " length in bytes of " + segment + "." + IndexFileNames.FIELDS_INDEX_EXTENSION;
 
     } else
diff --git a/src/java/org/apache/lucene/index/SegmentTermEnum.java b/src/java/org/apache/lucene/index/SegmentTermEnum.java
index 46e1a79..03c18e1 100644
--- a/src/java/org/apache/lucene/index/SegmentTermEnum.java
+++ b/src/java/org/apache/lucene/index/SegmentTermEnum.java
@@ -61,8 +61,8 @@ final class SegmentTermEnum extends TermEnum implements Cloneable {
       format = firstInt;
 
       // check that it is a format we can understand
-      if (format < TermInfosWriter.FORMAT)
-        throw new CorruptIndexException("Unknown format version:" + format);
+      if (format < TermInfosWriter.FORMAT_CURRENT)
+        throw new CorruptIndexException("Unknown format version:" + format + " expected " + TermInfosWriter.FORMAT_CURRENT + " or higher");
 
       size = input.readLong();                    // read the size
       
@@ -77,13 +77,17 @@ final class SegmentTermEnum extends TermEnum implements Cloneable {
       } else {
         indexInterval = input.readInt();
         skipInterval = input.readInt();
-        if (format == -3) {
+        if (format <= TermInfosWriter.FORMAT) {
           // this new format introduces multi-level skipping
           maxSkipLevels = input.readInt();
         }
       }
     }
-
+    if (format > TermInfosWriter.FORMAT_VERSION_UTF8_LENGTH_IN_BYTES) {
+      termBuffer.setPreUTF8Strings();
+      scanBuffer.setPreUTF8Strings();
+      prevBuffer.setPreUTF8Strings();
+    }
   }
 
   protected Object clone() {
diff --git a/src/java/org/apache/lucene/index/TermBuffer.java b/src/java/org/apache/lucene/index/TermBuffer.java
index 72054bb..4b180ce 100644
--- a/src/java/org/apache/lucene/index/TermBuffer.java
+++ b/src/java/org/apache/lucene/index/TermBuffer.java
@@ -19,28 +19,31 @@ package org.apache.lucene.index;
 
 import java.io.IOException;
 import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.UnicodeUtil;
 
 final class TermBuffer implements Cloneable {
-  private static final char[] NO_CHARS = new char[0];
 
   private String field;
-  private char[] text = NO_CHARS;
-  private int textLength;
   private Term term;                            // cached
+  private boolean preUTF8Strings;                // true if strings are stored in modified UTF8 encoding (LUCENE-510)
+  private boolean dirty;                          // true if text was set externally (ie not read via UTF8 bytes)
+
+  private UnicodeUtil.UTF16Result text = new UnicodeUtil.UTF16Result();
+  private UnicodeUtil.UTF8Result bytes = new UnicodeUtil.UTF8Result();
 
   public final int compareTo(TermBuffer other) {
-    if (field == other.field)			  // fields are interned
-      return compareChars(text, textLength, other.text, other.textLength);
+    if (field == other.field) 	  // fields are interned
+      return compareChars(text.result, text.length, other.text.result, other.text.length);
     else
       return field.compareTo(other.field);
   }
 
-  private static final int compareChars(char[] v1, int len1,
-                                        char[] v2, int len2) {
-    int end = Math.min(len1, len2);
+  private static final int compareChars(char[] chars1, int len1,
+                                        char[] chars2, int len2) {
+    final int end = len1 < len2 ? len1:len2;
     for (int k = 0; k < end; k++) {
-      char c1 = v1[k];
-      char c2 = v2[k];
+      char c1 = chars1[k];
+      char c2 = chars2[k];
       if (c1 != c2) {
         return c1 - c2;
       }
@@ -48,13 +51,11 @@ final class TermBuffer implements Cloneable {
     return len1 - len2;
   }
 
-  private final void setTextLength(int newLength) {
-    if (text.length < newLength) {
-      char[] newText = new char[newLength];
-      System.arraycopy(text, 0, newText, 0, textLength);
-      text = newText;
-    }
-    textLength = newLength;
+  /** Call this if the IndexInput passed to {@link #read}
+   *  stores terms in the "modified UTF8" (pre LUCENE-510)
+   *  format. */
+  void setPreUTF8Strings() {
+    preUTF8Strings = true;
   }
 
   public final void read(IndexInput input, FieldInfos fieldInfos)
@@ -63,8 +64,25 @@ final class TermBuffer implements Cloneable {
     int start = input.readVInt();
     int length = input.readVInt();
     int totalLength = start + length;
-    setTextLength(totalLength);
-    input.readChars(this.text, start, length);
+    if (preUTF8Strings) {
+      text.setLength(totalLength);
+      input.readChars(text.result, start, length);
+    } else {
+
+      if (dirty) {
+        // Fully convert all bytes since bytes is dirty
+        UnicodeUtil.UTF16toUTF8(text.result, 0, text.length, bytes);
+        bytes.setLength(totalLength);
+        input.readBytes(bytes.result, start, length);
+        UnicodeUtil.UTF8toUTF16(bytes.result, 0, totalLength, text);
+        dirty = false;
+      } else {
+        // Incrementally convert only the UTF8 bytes that are new:
+        bytes.setLength(totalLength);
+        input.readBytes(bytes.result, start, length);
+        UnicodeUtil.UTF8toUTF16(bytes.result, start, length, text);
+      }
+    }
     this.field = fieldInfos.fieldName(input.readVInt());
   }
 
@@ -73,27 +91,27 @@ final class TermBuffer implements Cloneable {
       reset();
       return;
     }
-
-    // copy text into the buffer
-    setTextLength(term.text().length());
-    term.text().getChars(0, term.text().length(), text, 0);
-
-    this.field = term.field();
+    final String termText = term.text();
+    final int termLen = termText.length();
+    text.setLength(termLen);
+    termText.getChars(0, termLen, text.result, 0);
+    dirty = true;
+    field = term.field();
     this.term = term;
   }
 
   public final void set(TermBuffer other) {
-    setTextLength(other.textLength);
-    System.arraycopy(other.text, 0, text, 0, textLength);
-
-    this.field = other.field;
-    this.term = other.term;
+    text.copyText(other.text);
+    dirty = true;
+    field = other.field;
+    term = other.term;
   }
 
   public void reset() {
-    this.field = null;
-    this.textLength = 0;
-    this.term = null;
+    field = null;
+    text.setLength(0);
+    term = null;
+    dirty = true;
   }
 
   public Term toTerm() {
@@ -101,7 +119,7 @@ final class TermBuffer implements Cloneable {
       return null;
 
     if (term == null)
-      term = new Term(field, new String(text, 0, textLength), false);
+      term = new Term(field, new String(text.result, 0, text.length), false);
 
     return term;
   }
@@ -112,9 +130,10 @@ final class TermBuffer implements Cloneable {
       clone = (TermBuffer)super.clone();
     } catch (CloneNotSupportedException e) {}
 
-    clone.text = new char[text.length];
-    System.arraycopy(text, 0, clone.text, 0, textLength);
-
+    clone.dirty = true;
+    clone.bytes = new UnicodeUtil.UTF8Result();
+    clone.text = new UnicodeUtil.UTF16Result();
+    clone.text.copyText(text);
     return clone;
   }
 }
diff --git a/src/java/org/apache/lucene/index/TermInfosReader.java b/src/java/org/apache/lucene/index/TermInfosReader.java
index 4d627f6..e08eb42 100644
--- a/src/java/org/apache/lucene/index/TermInfosReader.java
+++ b/src/java/org/apache/lucene/index/TermInfosReader.java
@@ -58,12 +58,12 @@ final class TermInfosReader {
       segment = seg;
       fieldInfos = fis;
 
-      origEnum = new SegmentTermEnum(directory.openInput(segment + ".tis",
+      origEnum = new SegmentTermEnum(directory.openInput(segment + "." + IndexFileNames.TERMS_EXTENSION,
           readBufferSize), fieldInfos, false);
       size = origEnum.size;
       totalIndexInterval = origEnum.indexInterval;
 
-      indexEnum = new SegmentTermEnum(directory.openInput(segment + ".tii",
+      indexEnum = new SegmentTermEnum(directory.openInput(segment + "." + IndexFileNames.TERMS_INDEX_EXTENSION,
           readBufferSize), fieldInfos, true);
 
       success = true;
diff --git a/src/java/org/apache/lucene/index/TermInfosWriter.java b/src/java/org/apache/lucene/index/TermInfosWriter.java
index c04fd92..a15309c 100644
--- a/src/java/org/apache/lucene/index/TermInfosWriter.java
+++ b/src/java/org/apache/lucene/index/TermInfosWriter.java
@@ -21,6 +21,7 @@ package org.apache.lucene.index;
 import java.io.IOException;
 import org.apache.lucene.store.IndexOutput;
 import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.UnicodeUtil;
 
 /** This stores a monotonically increasing set of <Term, TermInfo> pairs in a
   Directory.  A TermInfos can be written once, in order.  */
@@ -29,6 +30,13 @@ final class TermInfosWriter {
   /** The file format version, a negative number. */
   public static final int FORMAT = -3;
 
+  // Changed strings to true utf8 with length-in-bytes not
+  // length-in-chars
+  public static final int FORMAT_VERSION_UTF8_LENGTH_IN_BYTES = -4;
+
+  // NOTE: always change this if you switch to a new format!
+  public static final int FORMAT_CURRENT = FORMAT_VERSION_UTF8_LENGTH_IN_BYTES;
+
   private FieldInfos fieldInfos;
   private IndexOutput output;
   private TermInfo lastTi = new TermInfo();
@@ -62,13 +70,12 @@ final class TermInfosWriter {
 
   private long lastIndexPointer;
   private boolean isIndex;
-  private char[] lastTermText = new char[10];
-  private int lastTermTextLength;
+  private byte[] lastTermBytes = new byte[10];
+  private int lastTermBytesLength = 0;
   private int lastFieldNumber = -1;
 
-  private char[] termTextBuffer = new char[10];
-
   private TermInfosWriter other;
+  private UnicodeUtil.UTF8Result utf8Result = new UnicodeUtil.UTF8Result();
 
   TermInfosWriter(Directory directory, String segment, FieldInfos fis,
                   int interval)
@@ -89,27 +96,32 @@ final class TermInfosWriter {
     fieldInfos = fis;
     isIndex = isi;
     output = directory.createOutput(segment + (isIndex ? ".tii" : ".tis"));
-    output.writeInt(FORMAT);                      // write format
+    output.writeInt(FORMAT_CURRENT);              // write format
     output.writeLong(0);                          // leave space for size
-    output.writeInt(indexInterval);             // write indexInterval
-    output.writeInt(skipInterval);              // write skipInterval
-    output.writeInt(maxSkipLevels);              // write maxSkipLevels
+    output.writeInt(indexInterval);               // write indexInterval
+    output.writeInt(skipInterval);                // write skipInterval
+    output.writeInt(maxSkipLevels);               // write maxSkipLevels
+    assert initUTF16Results();
   }
 
   void add(Term term, TermInfo ti) throws IOException {
+    UnicodeUtil.UTF16toUTF8(term.text, 0, term.text.length(), utf8Result);
+    add(fieldInfos.fieldNumber(term.field), utf8Result.result, utf8Result.length, ti);
+  }
 
-    final int length = term.text.length();
-    if (termTextBuffer.length < length)
-      termTextBuffer = new char[(int) (length*1.25)];
-
-    term.text.getChars(0, length, termTextBuffer, 0);
+  // Currently used only by assert statements
+  UnicodeUtil.UTF16Result utf16Result1;
+  UnicodeUtil.UTF16Result utf16Result2;
 
-    add(fieldInfos.fieldNumber(term.field), termTextBuffer, 0, length, ti);
+  // Currently used only by assert statements
+  private boolean initUTF16Results() {
+    utf16Result1 = new UnicodeUtil.UTF16Result();
+    utf16Result2 = new UnicodeUtil.UTF16Result();
+    return true;
   }
 
   // Currently used only by assert statement
-  private int compareToLastTerm(int fieldNumber, char[] termText, int start, int length) {
-    int pos = 0;
+  private int compareToLastTerm(int fieldNumber, byte[] termBytes, int termBytesLength) {
 
     if (lastFieldNumber != fieldNumber) {
       final int cmp = fieldInfos.fieldName(lastFieldNumber).compareTo(fieldInfos.fieldName(fieldNumber));
@@ -121,45 +133,42 @@ final class TermInfosWriter {
         return cmp;
     }
 
-    while(pos < length && pos < lastTermTextLength) {
-      final char c1 = lastTermText[pos];
-      final char c2 = termText[pos + start];
-      if (c1 < c2)
-        return -1;
-      else if (c1 > c2)
-        return 1;
-      pos++;
-    }
-
-    if (pos < lastTermTextLength)
-      // Last term was longer
-      return 1;
-    else if (pos < length)
-      // Last term was shorter
-      return -1;
+    UnicodeUtil.UTF8toUTF16(lastTermBytes, 0, lastTermBytesLength, utf16Result1);
+    UnicodeUtil.UTF8toUTF16(termBytes, 0, termBytesLength, utf16Result2);
+    final int len;
+    if (utf16Result1.length < utf16Result2.length)
+      len = utf16Result1.length;
     else
-      return 0;
+      len = utf16Result2.length;
+
+    for(int i=0;i<len;i++) {
+      final char ch1 = utf16Result1.result[i];
+      final char ch2 = utf16Result2.result[i];
+      if (ch1 != ch2)
+        return ch1-ch2;
+    }
+    return utf16Result1.length - utf16Result2.length;
   }
 
-  /** Adds a new <<fieldNumber, termText>, TermInfo> pair to the set.
+  /** Adds a new <<fieldNumber, termBytes>, TermInfo> pair to the set.
     Term must be lexicographically greater than all previous Terms added.
     TermInfo pointers must be positive and greater than all previous.*/
-  void add(int fieldNumber, char[] termText, int termTextStart, int termTextLength, TermInfo ti)
+  void add(int fieldNumber, byte[] termBytes, int termBytesLength, TermInfo ti)
     throws IOException {
 
-    assert compareToLastTerm(fieldNumber, termText, termTextStart, termTextLength) < 0 ||
-      (isIndex && termTextLength == 0 && lastTermTextLength == 0) :
+    assert compareToLastTerm(fieldNumber, termBytes, termBytesLength) < 0 ||
+      (isIndex && termBytesLength == 0 && lastTermBytesLength == 0) :
       "Terms are out of order: field=" + fieldInfos.fieldName(fieldNumber) + " (number " + fieldNumber + ")" +
-      " lastField=" + fieldInfos.fieldName(lastFieldNumber) + " (number " + lastFieldNumber + ")" +
-      " text=" + new String(termText, termTextStart, termTextLength) + " lastText=" + new String(lastTermText, 0, lastTermTextLength);
+        " lastField=" + fieldInfos.fieldName(lastFieldNumber) + " (number " + lastFieldNumber + ")" +
+        " text=" + new String(termBytes, 0, termBytesLength, "UTF-8") + " lastText=" + new String(lastTermBytes, 0, lastTermBytesLength, "UTF-8");
 
     assert ti.freqPointer >= lastTi.freqPointer: "freqPointer out of order (" + ti.freqPointer + " < " + lastTi.freqPointer + ")";
     assert ti.proxPointer >= lastTi.proxPointer: "proxPointer out of order (" + ti.proxPointer + " < " + lastTi.proxPointer + ")";
 
     if (!isIndex && size % indexInterval == 0)
-      other.add(lastFieldNumber, lastTermText, 0, lastTermTextLength, lastTi);                      // add an index term
+      other.add(lastFieldNumber, lastTermBytes, lastTermBytesLength, lastTi);                      // add an index term
 
-    writeTerm(fieldNumber, termText, termTextStart, termTextLength);                        // write term
+    writeTerm(fieldNumber, termBytes, termBytesLength);                        // write term
 
     output.writeVInt(ti.docFreq);                       // write doc freq
     output.writeVLong(ti.freqPointer - lastTi.freqPointer); // write pointers
@@ -174,34 +183,36 @@ final class TermInfosWriter {
       lastIndexPointer = other.output.getFilePointer(); // write pointer
     }
 
-    if (lastTermText.length < termTextLength)
-      lastTermText = new char[(int) (termTextLength*1.25)];
-    System.arraycopy(termText, termTextStart, lastTermText, 0, termTextLength);
-    lastTermTextLength = termTextLength;
     lastFieldNumber = fieldNumber;
-
     lastTi.set(ti);
     size++;
   }
 
-  private void writeTerm(int fieldNumber, char[] termText, int termTextStart, int termTextLength)
+  private void writeTerm(int fieldNumber, byte[] termBytes, int termBytesLength)
        throws IOException {
 
+    // TODO: UTF16toUTF8 could tell us this prefix
     // Compute prefix in common with last term:
     int start = 0;
-    final int limit = termTextLength < lastTermTextLength ? termTextLength : lastTermTextLength;
+    final int limit = termBytesLength < lastTermBytesLength ? termBytesLength : lastTermBytesLength;
     while(start < limit) {
-      if (termText[termTextStart+start] != lastTermText[start])
+      if (termBytes[start] != lastTermBytes[start])
         break;
       start++;
     }
 
-    int length = termTextLength - start;
-
+    final int length = termBytesLength - start;
     output.writeVInt(start);                     // write shared prefix length
     output.writeVInt(length);                  // write delta length
-    output.writeChars(termText, start+termTextStart, length);  // write delta chars
+    output.writeBytes(termBytes, start, length);  // write delta bytes
     output.writeVInt(fieldNumber); // write field num
+    if (lastTermBytes.length < termBytesLength) {
+      byte[] newArray = new byte[(int) (termBytesLength*1.5)];
+      System.arraycopy(lastTermBytes, 0, newArray, 0, start);
+      lastTermBytes = newArray;
+    }
+    System.arraycopy(termBytes, start, lastTermBytes, start, length);
+    lastTermBytesLength = termBytesLength;
   }
 
   /** Called to complete TermInfos creation. */
diff --git a/src/java/org/apache/lucene/index/TermVectorsReader.java b/src/java/org/apache/lucene/index/TermVectorsReader.java
index 533a716..c90d1c2 100644
--- a/src/java/org/apache/lucene/index/TermVectorsReader.java
+++ b/src/java/org/apache/lucene/index/TermVectorsReader.java
@@ -32,8 +32,16 @@ class TermVectorsReader implements Cloneable {
   // NOTE: if you make a new format, it must be larger than
   // the current format
   static final int FORMAT_VERSION = 2;
+
+  // Changes to speed up bulk merging of term vectors:
   static final int FORMAT_VERSION2 = 3;
 
+  // Changed strings to UTF8 with length-in-bytes not length-in-chars
+  static final int FORMAT_UTF8_LENGTH_IN_BYTES = 4;
+
+  // NOTE: always change this if you switch to a new format!
+  static final int FORMAT_CURRENT = FORMAT_UTF8_LENGTH_IN_BYTES;
+
   //The size in bytes that the FORMAT_VERSION will take up at the beginning of each file 
   static final int FORMAT_SIZE = 4;
 
@@ -134,7 +142,7 @@ class TermVectorsReader implements Cloneable {
   }
 
   boolean canReadRawDocs() {
-    return format >= FORMAT_VERSION2;
+    return format >= FORMAT_UTF8_LENGTH_IN_BYTES;
   }
 
   /** Retrieve the length (in bytes) of the tvd and tvf
@@ -190,9 +198,9 @@ class TermVectorsReader implements Cloneable {
   private int checkValidFormat(IndexInput in) throws CorruptIndexException, IOException
   {
     int format = in.readInt();
-    if (format > FORMAT_VERSION2) {
+    if (format > FORMAT_CURRENT) {
       throw new CorruptIndexException("Incompatible format version: " + format + " expected " 
-                                      + FORMAT_VERSION2 + " or less");
+                                      + FORMAT_CURRENT + " or less");
     }
     return format;
   }
@@ -434,24 +442,45 @@ class TermVectorsReader implements Cloneable {
     int start = 0;
     int deltaLength = 0;
     int totalLength = 0;
-    char [] buffer = new char[10];    // init the buffer with a length of 10 character
-    char[] previousBuffer = {};
-    
+    byte[] byteBuffer;
+    char[] charBuffer;
+    final boolean preUTF8 = format < FORMAT_UTF8_LENGTH_IN_BYTES;
+
+    // init the buffers
+    if (preUTF8) {
+      charBuffer = new char[10];
+      byteBuffer = null;
+    } else {
+      charBuffer = null;
+      byteBuffer = new byte[20];
+    }
+
     for (int i = 0; i < numTerms; i++) {
       start = tvf.readVInt();
       deltaLength = tvf.readVInt();
       totalLength = start + deltaLength;
-      if (buffer.length < totalLength) {  // increase buffer
-        buffer = null;    // give a hint to garbage collector
-        buffer = new char[totalLength];
-        
-        if (start > 0)  // just copy if necessary
-          System.arraycopy(previousBuffer, 0, buffer, 0, start);
-      }
+
+      final String term;
       
-      tvf.readChars(buffer, start, deltaLength);
-      String term = new String(buffer, 0, totalLength);
-      previousBuffer = buffer;
+      if (preUTF8) {
+        // Term stored as java chars
+        if (charBuffer.length < totalLength) {
+          char[] newCharBuffer = new char[(int) (1.5*totalLength)];
+          System.arraycopy(charBuffer, 0, newCharBuffer, 0, start);
+          charBuffer = newCharBuffer;
+        }
+        tvf.readChars(charBuffer, start, deltaLength);
+        term = new String(charBuffer, 0, totalLength);
+      } else {
+        // Term stored as utf8 bytes
+        if (byteBuffer.length < totalLength) {
+          byte[] newByteBuffer = new byte[(int) (1.5*totalLength)];
+          System.arraycopy(byteBuffer, 0, newByteBuffer, 0, start);
+          byteBuffer = newByteBuffer;
+        }
+        tvf.readBytes(byteBuffer, start, deltaLength);
+        term = new String(byteBuffer, 0, totalLength, "UTF-8");
+      }
       int freq = tvf.readVInt();
       int [] positions = null;
       if (storePositions) { //read in the positions
diff --git a/src/java/org/apache/lucene/index/TermVectorsWriter.java b/src/java/org/apache/lucene/index/TermVectorsWriter.java
index 9ac2104..0d5e4fc 100644
--- a/src/java/org/apache/lucene/index/TermVectorsWriter.java
+++ b/src/java/org/apache/lucene/index/TermVectorsWriter.java
@@ -20,6 +20,7 @@ package org.apache.lucene.index;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IndexOutput;
 import org.apache.lucene.util.StringHelper;
+import org.apache.lucene.util.UnicodeUtil;
 
 import java.io.IOException;
 
@@ -27,17 +28,19 @@ final class TermVectorsWriter {
   
   private IndexOutput tvx = null, tvd = null, tvf = null;
   private FieldInfos fieldInfos;
+  final UnicodeUtil.UTF8Result[] utf8Results = new UnicodeUtil.UTF8Result[] {new UnicodeUtil.UTF8Result(),
+                                                                             new UnicodeUtil.UTF8Result()};
 
   public TermVectorsWriter(Directory directory, String segment,
                            FieldInfos fieldInfos)
     throws IOException {
     // Open files for TermVector storage
     tvx = directory.createOutput(segment + "." + IndexFileNames.VECTORS_INDEX_EXTENSION);
-    tvx.writeInt(TermVectorsReader.FORMAT_VERSION2);
+    tvx.writeInt(TermVectorsReader.FORMAT_CURRENT);
     tvd = directory.createOutput(segment + "." + IndexFileNames.VECTORS_DOCUMENTS_EXTENSION);
-    tvd.writeInt(TermVectorsReader.FORMAT_VERSION2);
+    tvd.writeInt(TermVectorsReader.FORMAT_CURRENT);
     tvf = directory.createOutput(segment + "." + IndexFileNames.VECTORS_FIELDS_EXTENSION);
-    tvf.writeInt(TermVectorsReader.FORMAT_VERSION2);
+    tvf.writeInt(TermVectorsReader.FORMAT_CURRENT);
 
     this.fieldInfos = fieldInfos;
   }
@@ -97,15 +100,22 @@ final class TermVectorsWriter {
         final String[] terms = vectors[i].getTerms();
         final int[] freqs = vectors[i].getTermFrequencies();
 
-        String lastTermText = "";
+        int utf8Upto = 0;
+        utf8Results[1].length = 0;
+
         for (int j=0; j<numTerms; j++) {
-          final String termText = terms[j];
-          int start = StringHelper.stringDifference(lastTermText, termText);
-          int length = termText.length() - start;
+
+          UnicodeUtil.UTF16toUTF8(terms[j], 0, terms[j].length(), utf8Results[utf8Upto]);
+          
+          int start = StringHelper.bytesDifference(utf8Results[1-utf8Upto].result,
+                                                   utf8Results[1-utf8Upto].length,
+                                                   utf8Results[utf8Upto].result,
+                                                   utf8Results[utf8Upto].length);
+          int length = utf8Results[utf8Upto].length - start;
           tvf.writeVInt(start);       // write shared prefix length
           tvf.writeVInt(length);        // write delta length
-          tvf.writeChars(termText, start, length);  // write delta chars
-          lastTermText = termText;
+          tvf.writeBytes(utf8Results[utf8Upto].result, start, length);  // write delta bytes
+          utf8Upto = 1-utf8Upto;
 
           final int termFreq = freqs[j];
 
diff --git a/src/java/org/apache/lucene/store/IndexInput.java b/src/java/org/apache/lucene/store/IndexInput.java
index a8aa50a..d414ed8 100644
--- a/src/java/org/apache/lucene/store/IndexInput.java
+++ b/src/java/org/apache/lucene/store/IndexInput.java
@@ -24,7 +24,9 @@ import java.io.IOException;
  * @see Directory
  */
 public abstract class IndexInput implements Cloneable {
-  private char[] chars;                           // used by readString()
+  private byte[] bytes;                           // used by readString()
+  private char[] chars;                           // used by readModifiedUTF8String()
+  private boolean preUTF8Strings;                 // true if we are reading old (modified UTF8) string format
 
   /** Reads and returns a single byte.
    * @see IndexOutput#writeByte(byte)
@@ -102,10 +104,28 @@ public abstract class IndexInput implements Cloneable {
     return i;
   }
 
+  /** Call this if readString should read characters stored
+   *  in the old modified UTF8 format (length in java chars
+   *  and java's modified UTF8 encoding).  This is used for
+   *  indices written pre-2.4 See LUCENE-510 for details. */
+  public void setModifiedUTF8StringsMode() {
+    preUTF8Strings = true;
+  }
+
   /** Reads a string.
    * @see IndexOutput#writeString(String)
    */
   public String readString() throws IOException {
+    if (preUTF8Strings)
+      return readModifiedUTF8String();
+    int length = readVInt();
+    if (bytes == null || length > bytes.length)
+      bytes = new byte[(int) (length*1.25)];
+    readBytes(bytes, 0, length);
+    return new String(bytes, 0, length, "UTF-8");
+  }
+
+  private String readModifiedUTF8String() throws IOException {
     int length = readVInt();
     if (chars == null || length > chars.length)
       chars = new char[length];
@@ -113,11 +133,15 @@ public abstract class IndexInput implements Cloneable {
     return new String(chars, 0, length);
   }
 
-  /** Reads UTF-8 encoded characters into an array.
+  /** Reads Lucene's old "modified UTF-8" encoded
+   *  characters into an array.
    * @param buffer the array to read characters into
    * @param start the offset in the array to start storing characters
    * @param length the number of characters to read
    * @see IndexOutput#writeChars(String,int,int)
+   * @deprecated -- please use readString or readBytes
+   *                instead, and construct the string
+   *                from those utf8 bytes
    */
   public void readChars(char[] buffer, int start, int length)
        throws IOException {
@@ -144,6 +168,8 @@ public abstract class IndexInput implements Cloneable {
    * and it does not have to do any of the bitwise operations, since we don't actually care what is in the byte except to determine
    * how many more bytes to read
    * @param length The number of chars to read
+   * @deprecated this method operates on old "modified utf8" encoded
+   *             strings
    */
   public void skipChars(int length) throws IOException{
     for (int i = 0; i < length; i++) {
@@ -194,6 +220,7 @@ public abstract class IndexInput implements Cloneable {
       clone = (IndexInput)super.clone();
     } catch (CloneNotSupportedException e) {}
 
+    clone.bytes = null;
     clone.chars = null;
 
     return clone;
diff --git a/src/java/org/apache/lucene/store/IndexOutput.java b/src/java/org/apache/lucene/store/IndexOutput.java
index 648355d..acd4dcf 100644
--- a/src/java/org/apache/lucene/store/IndexOutput.java
+++ b/src/java/org/apache/lucene/store/IndexOutput.java
@@ -18,6 +18,7 @@ package org.apache.lucene.store;
  */
 
 import java.io.IOException;
+import org.apache.lucene.util.UnicodeUtil;
 
 /** Abstract base class for output to a file in a Directory.  A random-access
  * output stream.  Used for all Lucene index output operations.
@@ -26,6 +27,8 @@ import java.io.IOException;
  */
 public abstract class IndexOutput {
 
+  private UnicodeUtil.UTF8Result utf8Result = new UnicodeUtil.UTF8Result();
+
   /** Writes a single byte.
    * @see IndexInput#readByte()
    */
@@ -96,16 +99,18 @@ public abstract class IndexOutput {
    * @see IndexInput#readString()
    */
   public void writeString(String s) throws IOException {
-    int length = s.length();
-    writeVInt(length);
-    writeChars(s, 0, length);
+    UnicodeUtil.UTF16toUTF8(s, 0, s.length(), utf8Result);
+    writeVInt(utf8Result.length);
+    writeBytes(utf8Result.result, 0, utf8Result.length);
   }
 
-  /** Writes a sequence of UTF-8 encoded characters from a string.
+  /** Writes a sub sequence of characters from s as the old
+   *  format (modified UTF-8 encoded bytes).
    * @param s the source of the characters
    * @param start the first character in the sequence
    * @param length the number of characters in the sequence
-   * @see IndexInput#readChars(char[],int,int)
+   * @deprecated -- please pre-convert to utf8 bytes
+   * instead or use {@link #writeString}
    */
   public void writeChars(String s, int start, int length)
        throws IOException {
@@ -125,11 +130,12 @@ public abstract class IndexOutput {
     }
   }
 
-  /** Writes a sequence of UTF-8 encoded characters from a char[].
+  /** Writes a sub sequence of characters from char[] as
+   *  the old format (modified UTF-8 encoded bytes).
    * @param s the source of the characters
    * @param start the first character in the sequence
    * @param length the number of characters in the sequence
-   * @see IndexInput#readChars(char[],int,int)
+   * @deprecated -- please pre-convert to utf8 bytes instead or use {@link #writeString}
    */
   public void writeChars(char[] s, int start, int length)
     throws IOException {
diff --git a/src/java/org/apache/lucene/util/StringHelper.java b/src/java/org/apache/lucene/util/StringHelper.java
index 7d422b3..7ffdd92 100644
--- a/src/java/org/apache/lucene/util/StringHelper.java
+++ b/src/java/org/apache/lucene/util/StringHelper.java
@@ -26,6 +26,22 @@ package org.apache.lucene.util;
 public abstract class StringHelper {
 
   /**
+   * Compares two byte[] arrays, element by element, and returns the
+   * number of elements common to both arrays.
+   *
+   * @param bytes1 The first byte[] to compare
+   * @param bytes2 The second byte[] to compare
+   * @return The number of common elements.
+   */
+  public static final int bytesDifference(byte[] bytes1, int len1, byte[] bytes2, int len2) {
+    int len = len1 < len2 ? len1 : len2;
+    for (int i = 0; i < len; i++)
+      if (bytes1[i] != bytes2[i])
+        return i;
+    return len;
+  }
+
+  /**
    * Compares two strings, character by character, and returns the
    * first position where the two strings differ from one another.
    *
@@ -45,7 +61,6 @@ public abstract class StringHelper {
     return len;
   }
 
-
   private StringHelper() {
   }
 }
diff --git a/src/java/org/apache/lucene/util/UnicodeUtil.java b/src/java/org/apache/lucene/util/UnicodeUtil.java
new file mode 100644
index 0000000..020bc69
--- /dev/null
+++ b/src/java/org/apache/lucene/util/UnicodeUtil.java
@@ -0,0 +1,447 @@
+package org.apache.lucene.util;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+/*
+ * Some of this code came from the excellent Unicode
+ * conversion examples from:
+ *
+ *   http://www.unicode.org/Public/PROGRAMS/CVTUTF
+ *
+ * Full Copyright for that code follows:
+*/
+
+/*
+ * Copyright 2001-2004 Unicode, Inc.
+ * 
+ * Disclaimer
+ * 
+ * This source code is provided as is by Unicode, Inc. No claims are
+ * made as to fitness for any particular purpose. No warranties of any
+ * kind are expressed or implied. The recipient agrees to determine
+ * applicability of information provided. If this file has been
+ * purchased on magnetic or optical media from Unicode, Inc., the
+ * sole remedy for any claim will be exchange of defective media
+ * within 90 days of receipt.
+ * 
+ * Limitations on Rights to Redistribute This Code
+ * 
+ * Unicode, Inc. hereby grants the right to freely use the information
+ * supplied in this file in the creation of products supporting the
+ * Unicode Standard, and to make copies of this file in any form
+ * for internal or external distribution as long as this notice
+ * remains attached.
+ */
+
+/**
+ * Class to encode java's UTF16 char[] into UTF8 byte[]
+ * without always allocating a new byte[] as
+ * String.getBytes("UTF-8") does.
+ *
+ * <p><b>WARNING</b>: This API is a new and experimental and
+ * may suddenly change. </p>
+ */
+
+final public class UnicodeUtil {
+
+  public static final int UNI_SUR_HIGH_START = 0xD800;
+  public static final int UNI_SUR_HIGH_END = 0xDBFF;
+  public static final int UNI_SUR_LOW_START = 0xDC00;
+  public static final int UNI_SUR_LOW_END = 0xDFFF;
+  public static final int UNI_REPLACEMENT_CHAR = 0xFFFD;
+
+  private static final long UNI_MAX_BMP = 0x0000FFFF;
+
+  private static final int HALF_BASE = 0x0010000;
+  private static final long HALF_SHIFT = 10;
+  private static final long HALF_MASK = 0x3FFL;
+
+  public static final class UTF8Result {
+    public byte[] result = new byte[10];
+    public int length;
+
+    public void setLength(int newLength) {
+      if (result.length < newLength) {
+        byte[] newArray = new byte[(int) (1.5*newLength)];
+        System.arraycopy(result, 0, newArray, 0, length);
+        result = newArray;
+      }
+      length = newLength;
+    }
+  }
+
+  public static final class UTF16Result {
+    public char[] result = new char[10];
+    public int[] offsets = new int[10];
+    public int length;
+
+    public void setLength(int newLength) {
+      if (result.length < newLength) {
+        char[] newArray = new char[(int) (1.5*newLength)];
+        System.arraycopy(result, 0, newArray, 0, length);
+        result = newArray;
+      }
+      length = newLength;
+    }
+
+    public void copyText(UTF16Result other) {
+      setLength(other.length);
+      System.arraycopy(other.result, 0, result, 0, length);
+    }
+  }
+
+  /** Encode characters from a char[] source, starting at
+   *  offset and stopping when the character 0xffff is seen.
+   *  Returns the number of bytes written to bytesOut. */
+  public static void UTF16toUTF8(final char[] source, final int offset, UTF8Result result) {
+
+    int upto = 0;
+    int i = offset;
+    byte[] out = result.result;
+
+    while(true) {
+      
+      final int code = (int) source[i++];
+
+      if (upto+4 > out.length) {
+        byte[] newOut = new byte[2*out.length];
+        assert newOut.length >= upto+4;
+        System.arraycopy(out, 0, newOut, 0, upto);
+        result.result = out = newOut;
+      }
+      if (code < 0x80)
+        out[upto++] = (byte) code;
+      else if (code < 0x800) {
+        out[upto++] = (byte) (0xC0 | (code >> 6));
+        out[upto++] = (byte)(0x80 | (code & 0x3F));
+      } else if (code < 0xD800 || code > 0xDFFF) {
+        if (code == 0xffff)
+          // END
+          break;
+        out[upto++] = (byte)(0xE0 | (code >> 12));
+        out[upto++] = (byte)(0x80 | ((code >> 6) & 0x3F));
+        out[upto++] = (byte)(0x80 | (code & 0x3F));
+      } else {
+        // surrogate pair
+        // confirm valid high surrogate
+        if (code < 0xDC00 && source[i] != 0xffff) {
+          int utf32 = (int) source[i];
+          // confirm valid low surrogate and write pair
+          if (utf32 >= 0xDC00 && utf32 <= 0xDFFF) { 
+            utf32 = ((code - 0xD7C0) << 10) + (utf32 & 0x3FF);
+            i++;
+            out[upto++] = (byte)(0xF0 | (utf32 >> 18));
+            out[upto++] = (byte)(0x80 | ((utf32 >> 12) & 0x3F));
+            out[upto++] = (byte)(0x80 | ((utf32 >> 6) & 0x3F));
+            out[upto++] = (byte)(0x80 | (utf32 & 0x3F));
+            continue;
+          }
+        }
+        // replace unpaired surrogate or out-of-order low surrogate
+        // with substitution character
+        out[upto++] = (byte) 0xEF;
+        out[upto++] = (byte) 0xBF;
+        out[upto++] = (byte) 0xBD;
+      }
+    }
+    //assert matches(source, offset, i-offset-1, out, upto);
+    result.length = upto;
+  }
+
+  /** Encode characters from a char[] source, starting at
+   *  offset for length chars.  Returns the number of bytes
+   *  written to bytesOut. */
+  public static void UTF16toUTF8(final char[] source, final int offset, final int length, UTF8Result result) {
+
+    int upto = 0;
+    int i = offset;
+    final int end = offset + length;
+    byte[] out = result.result;
+
+    while(i < end) {
+      
+      final int code = (int) source[i++];
+
+      if (upto+4 > out.length) {
+        byte[] newOut = new byte[2*out.length];
+        assert newOut.length >= upto+4;
+        System.arraycopy(out, 0, newOut, 0, upto);
+        result.result = out = newOut;
+      }
+      if (code < 0x80)
+        out[upto++] = (byte) code;
+      else if (code < 0x800) {
+        out[upto++] = (byte) (0xC0 | (code >> 6));
+        out[upto++] = (byte)(0x80 | (code & 0x3F));
+      } else if (code < 0xD800 || code > 0xDFFF) {
+        out[upto++] = (byte)(0xE0 | (code >> 12));
+        out[upto++] = (byte)(0x80 | ((code >> 6) & 0x3F));
+        out[upto++] = (byte)(0x80 | (code & 0x3F));
+      } else {
+        // surrogate pair
+        // confirm valid high surrogate
+        if (code < 0xDC00 && i < end && source[i] != 0xffff) {
+          int utf32 = (int) source[i];
+          // confirm valid low surrogate and write pair
+          if (utf32 >= 0xDC00 && utf32 <= 0xDFFF) { 
+            utf32 = ((code - 0xD7C0) << 10) + (utf32 & 0x3FF);
+            i++;
+            out[upto++] = (byte)(0xF0 | (utf32 >> 18));
+            out[upto++] = (byte)(0x80 | ((utf32 >> 12) & 0x3F));
+            out[upto++] = (byte)(0x80 | ((utf32 >> 6) & 0x3F));
+            out[upto++] = (byte)(0x80 | (utf32 & 0x3F));
+            continue;
+          }
+        }
+        // replace unpaired surrogate or out-of-order low surrogate
+        // with substitution character
+        out[upto++] = (byte) 0xEF;
+        out[upto++] = (byte) 0xBF;
+        out[upto++] = (byte) 0xBD;
+      }
+    }
+    //assert matches(source, offset, length, out, upto);
+    result.length = upto;
+  }
+
+  /** Encode characters from this String, starting at offset
+   *  for length characters.  Returns the number of bytes
+   *  written to bytesOut. */
+  public static void UTF16toUTF8(final String s, final int offset, final int length, UTF8Result result) {
+    final int end = offset + length;
+
+    byte[] out = result.result;
+
+    int upto = 0;
+    for(int i=offset;i<end;i++) {
+      final int code = (int) s.charAt(i);
+
+      if (upto+4 > out.length) {
+        byte[] newOut = new byte[2*out.length];
+        assert newOut.length >= upto+4;
+        System.arraycopy(out, 0, newOut, 0, upto);
+        result.result = out = newOut;
+      }
+      if (code < 0x80)
+        out[upto++] = (byte) code;
+      else if (code < 0x800) {
+        out[upto++] = (byte) (0xC0 | (code >> 6));
+        out[upto++] = (byte)(0x80 | (code & 0x3F));
+      } else if (code < 0xD800 || code > 0xDFFF) {
+        out[upto++] = (byte)(0xE0 | (code >> 12));
+        out[upto++] = (byte)(0x80 | ((code >> 6) & 0x3F));
+        out[upto++] = (byte)(0x80 | (code & 0x3F));
+      } else {
+        // surrogate pair
+        // confirm valid high surrogate
+        if (code < 0xDC00 && (i < end-1)) {
+          int utf32 = (int) s.charAt(i+1);
+          // confirm valid low surrogate and write pair
+          if (utf32 >= 0xDC00 && utf32 <= 0xDFFF) { 
+            utf32 = ((code - 0xD7C0) << 10) + (utf32 & 0x3FF);
+            i++;
+            out[upto++] = (byte)(0xF0 | (utf32 >> 18));
+            out[upto++] = (byte)(0x80 | ((utf32 >> 12) & 0x3F));
+            out[upto++] = (byte)(0x80 | ((utf32 >> 6) & 0x3F));
+            out[upto++] = (byte)(0x80 | (utf32 & 0x3F));
+            continue;
+          }
+        }
+        // replace unpaired surrogate or out-of-order low surrogate
+        // with substitution character
+        out[upto++] = (byte) 0xEF;
+        out[upto++] = (byte) 0xBF;
+        out[upto++] = (byte) 0xBD;
+      }
+    }
+    //assert matches(s, offset, length, out, upto);
+    result.length = upto;
+  }
+
+  /** Convert UTF8 bytes into UTF16 characters.  If offset
+   *  is non-zero, conversion starts at that starting point
+   *  in utf8, re-using the results from the previous call
+   *  up until offset. */
+  public static void UTF8toUTF16(final byte[] utf8, final int offset, final int length, final UTF16Result result) {
+
+    final int end = offset + length;
+    char[] out = result.result;
+    if (result.offsets.length <= end) {
+      int[] newOffsets = new int[2*end];
+      System.arraycopy(result.offsets, 0, newOffsets, 0, result.offsets.length);
+      result.offsets  = newOffsets;
+    }
+    final int[] offsets = result.offsets;
+
+    // If incremental decoding fell in the middle of a
+    // single unicode character, rollback to its start:
+    int upto = offset;
+    while(offsets[upto] == -1)
+      upto--;
+
+    int outUpto = offsets[upto];
+
+    // Pre-allocate for worst case 1-for-1
+    if (outUpto+length >= out.length) {
+      char[] newOut = new char[2*(outUpto+length)];
+      System.arraycopy(out, 0, newOut, 0, outUpto);
+      result.result = out = newOut;
+    }
+
+    while (upto < end) {
+
+      final int b = utf8[upto]&0xff;
+      final int ch;
+
+      offsets[upto++] = outUpto;
+
+      if (b < 0xc0) {
+        assert b < 0x80;
+        ch = b;
+      } else if (b < 0xe0) {
+        ch = ((b&0x1f)<<6) + (utf8[upto]&0x3f);
+        offsets[upto++] = -1;
+      } else if (b < 0xf0) {
+        ch = ((b&0xf)<<12) + ((utf8[upto]&0x3f)<<6) + (utf8[upto+1]&0x3f);
+        offsets[upto++] = -1;
+        offsets[upto++] = -1;
+      } else {
+        assert b < 0xf8;
+        ch = ((b&0x7)<<18) + ((utf8[upto]&0x3f)<<12) + ((utf8[upto+1]&0x3f)<<6) + (utf8[upto+2]&0x3f);
+        offsets[upto++] = -1;
+        offsets[upto++] = -1;
+        offsets[upto++] = -1;
+      }
+
+      if (ch <= UNI_MAX_BMP) {
+        // target is a character <= 0xFFFF
+        out[outUpto++] = (char) ch;
+      } else {
+        // target is a character in range 0xFFFF - 0x10FFFF
+        final int chHalf = ch - HALF_BASE;
+        out[outUpto++] = (char) ((chHalf >> HALF_SHIFT) + UNI_SUR_HIGH_START);
+        out[outUpto++] = (char) ((chHalf & HALF_MASK) + UNI_SUR_LOW_START);
+      }
+    }
+
+    offsets[upto] = outUpto;
+    result.length = outUpto;
+  }
+
+  // Only called from assert
+  /*
+  private static boolean matches(char[] source, int offset, int length, byte[] result, int upto) {
+    try {
+      String s1 = new String(source, offset, length);
+      String s2 = new String(result, 0, upto, "UTF-8");
+      if (!s1.equals(s2)) {
+        //System.out.println("DIFF: s1 len=" + s1.length());
+        //for(int i=0;i<s1.length();i++)
+        //  System.out.println("    " + i + ": " + (int) s1.charAt(i));
+        //System.out.println("s2 len=" + s2.length());
+        //for(int i=0;i<s2.length();i++)
+        //  System.out.println("    " + i + ": " + (int) s2.charAt(i));
+
+        // If the input string was invalid, then the
+        // difference is OK
+        if (!validUTF16String(s1))
+          return true;
+
+        return false;
+      }
+      return s1.equals(s2);
+    } catch (UnsupportedEncodingException uee) {
+      return false;
+    }
+  }
+
+  // Only called from assert
+  private static boolean matches(String source, int offset, int length, byte[] result, int upto) {
+    try {
+      String s1 = source.substring(offset, offset+length);
+      String s2 = new String(result, 0, upto, "UTF-8");
+      if (!s1.equals(s2)) {
+        // Allow a difference if s1 is not valid UTF-16
+
+        //System.out.println("DIFF: s1 len=" + s1.length());
+        //for(int i=0;i<s1.length();i++)
+        //  System.out.println("    " + i + ": " + (int) s1.charAt(i));
+        //System.out.println("  s2 len=" + s2.length());
+        //for(int i=0;i<s2.length();i++)
+        //  System.out.println("    " + i + ": " + (int) s2.charAt(i));
+
+        // If the input string was invalid, then the
+        // difference is OK
+        if (!validUTF16String(s1))
+          return true;
+
+        return false;
+      }
+      return s1.equals(s2);
+    } catch (UnsupportedEncodingException uee) {
+      return false;
+    }
+  }
+
+  public static final boolean validUTF16String(String s) {
+    final int size = s.length();
+    for(int i=0;i<size;i++) {
+      char ch = s.charAt(i);
+      if (ch >= UNI_SUR_HIGH_START && ch <= UNI_SUR_HIGH_END) {
+        if (i < size-1) {
+          i++;
+          char nextCH = s.charAt(i);
+          if (nextCH >= UNI_SUR_LOW_START && nextCH <= UNI_SUR_LOW_END) {
+            // Valid surrogate pair
+          } else
+            // Unmatched hight surrogate
+            return false;
+        } else
+          // Unmatched hight surrogate
+          return false;
+      } else if (ch >= UNI_SUR_LOW_START && ch <= UNI_SUR_LOW_END)
+        // Unmatched low surrogate
+        return false;
+    }
+
+    return true;
+  }
+
+  public static final boolean validUTF16String(char[] s, int size) {
+    for(int i=0;i<size;i++) {
+      char ch = s[i];
+      if (ch >= UNI_SUR_HIGH_START && ch <= UNI_SUR_HIGH_END) {
+        if (i < size-1) {
+          i++;
+          char nextCH = s[i];
+          if (nextCH >= UNI_SUR_LOW_START && nextCH <= UNI_SUR_LOW_END) {
+            // Valid surrogate pair
+          } else
+            return false;
+        } else
+          return false;
+      } else if (ch >= UNI_SUR_LOW_START && ch <= UNI_SUR_LOW_END)
+        // Unmatched low surrogate
+        return false;
+    }
+
+    return true;
+  }
+  */
+}
diff --git a/src/site/src/documentation/content/xdocs/fileformats.xml b/src/site/src/documentation/content/xdocs/fileformats.xml
index a776abf..a1d7f96 100644
--- a/src/site/src/documentation/content/xdocs/fileformats.xml
+++ b/src/site/src/documentation/content/xdocs/fileformats.xml
@@ -736,10 +736,7 @@
 
                 <p>
                     Lucene writes unicode
-                    character sequences using Java's
-                    <a href="http://en.wikipedia.org/wiki/UTF-8#Modified_UTF-8">"modified
-                        UTF-8 encoding"</a>
-                    .
+                    character sequences as UTF-8 encoded bytes.
                 </p>
 
 
@@ -748,8 +745,9 @@
             <section id="String"><title>String</title>
 
                 <p>
-                    Lucene writes strings as a VInt representing the length, followed by
-                    the character data.
+		    Lucene writes strings as UTF-8 encoded bytes.
+                    First the length, in bytes, is written as a VInt,
+                    followed by the bytes.
                 </p>
 
                 <p>
@@ -1233,10 +1231,12 @@
                             <br/>
                             --&gt; VInt
                         </p>
-                        <p>This
-                            file is sorted by Term. Terms are ordered first lexicographically
-                            by the term's field name, and within that lexicographically by the
-                            term's text.
+                        <p>
+			    This file is sorted by Term. Terms are
+                            ordered first lexicographically (by UTF16
+                            character code) by the term's field name,
+                            and within that lexicographically (by
+                            UTF16 character code) by the term's text.
                         </p>
                         <p>TIVersion names the version of the format
                             of this file and is -2 in Lucene 1.4.
diff --git a/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java b/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java
index ad4309f..6add9cd 100644
--- a/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java
+++ b/src/test/org/apache/lucene/index/TestBackwardsCompatibility.java
@@ -20,6 +20,7 @@ package org.apache.lucene.index;
 import org.apache.lucene.util.LuceneTestCase;
 
 import java.util.Arrays;
+import java.util.List;
 import java.util.Enumeration;
 import java.util.zip.ZipFile;
 import java.util.zip.ZipEntry;
@@ -39,6 +40,7 @@ import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.FSDirectory;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Field;
+import org.apache.lucene.util._TestUtil;
 
 /*
   Verify we can read the pre-2.1 file format, do searches
@@ -131,7 +133,7 @@ public class TestBackwardsCompatibility extends LuceneTestCase
     for(int i=0;i<oldNames.length;i++) {
       String dirName = "src/test/org/apache/lucene/index/index." + oldNames[i];
       unzip(dirName, oldNames[i]);
-      searchIndex(oldNames[i]);
+      searchIndex(oldNames[i], oldNames[i]);
       rmDir(oldNames[i]);
     }
   }
@@ -171,7 +173,7 @@ public class TestBackwardsCompatibility extends LuceneTestCase
     }
   }
 
-  public void searchIndex(String dirName) throws IOException {
+  public void searchIndex(String dirName, String oldName) throws IOException {
     //QueryParser parser = new QueryParser("contents", new WhitespaceAnalyzer());
     //Query query = parser.parse("handle:1");
 
@@ -179,6 +181,29 @@ public class TestBackwardsCompatibility extends LuceneTestCase
 
     Directory dir = FSDirectory.getDirectory(dirName);
     IndexSearcher searcher = new IndexSearcher(dir);
+    IndexReader reader = searcher.getIndexReader();
+
+    _TestUtil.checkIndex(dir);
+
+    for(int i=0;i<35;i++) {
+      if (!reader.isDeleted(i)) {
+        Document d = reader.document(i);
+        List fields = d.getFields();
+        if (oldName.startsWith("23.")) {
+          assertEquals(3, fields.size());
+          Field f = (Field) d.getField("id");
+          assertEquals(""+i, f.stringValue());
+
+          f = (Field) d.getField("utf8");
+          assertEquals("Lu\uD834\uDD1Ece\uD834\uDD60ne \u0000 \u2620 ab\ud917\udc17cd", f.stringValue());
+        
+          f = (Field) d.getField("content2");
+          assertEquals("here is more content with aaa aaa aaa", f.stringValue());
+        }        
+      } else
+        // Only ID 7 is deleted
+        assertEquals(7, i);
+    }
     
     Hits hits = searcher.search(new TermQuery(new Term("content", "aaa")));
 
@@ -189,6 +214,15 @@ public class TestBackwardsCompatibility extends LuceneTestCase
 
     testHits(hits, 34, searcher.getIndexReader());
 
+    if (oldName.startsWith("23.")) {
+      hits = searcher.search(new TermQuery(new Term("utf8", "\u0000")));
+      assertEquals(34, hits.length());
+      hits = searcher.search(new TermQuery(new Term("utf8", "Lu\uD834\uDD1Ece\uD834\uDD60ne")));
+      assertEquals(34, hits.length());
+      hits = searcher.search(new TermQuery(new Term("utf8", "ab\ud917\udc17cd")));
+      assertEquals(34, hits.length());
+    }
+
     searcher.close();
     dir.close();
   }
@@ -421,6 +455,7 @@ public class TestBackwardsCompatibility extends LuceneTestCase
     Document doc = new Document();
     doc.add(new Field("content", "aaa", Field.Store.NO, Field.Index.TOKENIZED));
     doc.add(new Field("id", Integer.toString(id), Field.Store.YES, Field.Index.UN_TOKENIZED));
+    doc.add(new Field("utf8", "Lu\uD834\uDD1Ece\uD834\uDD60ne \u0000 \u2620 ab\ud917\udc17cd", Field.Store.YES, Field.Index.TOKENIZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
     doc.add(new Field("content2", "here is more content with aaa aaa aaa", Field.Store.YES, Field.Index.TOKENIZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
     writer.addDocument(doc);
   }
diff --git a/src/test/org/apache/lucene/index/TestIndexInput.java b/src/test/org/apache/lucene/index/TestIndexInput.java
index d3c727d..6abf4c7 100644
--- a/src/test/org/apache/lucene/index/TestIndexInput.java
+++ b/src/test/org/apache/lucene/index/TestIndexInput.java
@@ -24,16 +24,70 @@ import java.io.IOException;
 
 public class TestIndexInput extends LuceneTestCase {
   public void testRead() throws IOException {
-    IndexInput is = new MockIndexInput(new byte[]{(byte) 0x80, 0x01,
-            (byte) 0xFF, 0x7F,
-            (byte) 0x80, (byte) 0x80, 0x01,
-            (byte) 0x81, (byte) 0x80, 0x01,
-            0x06, 'L', 'u', 'c', 'e', 'n', 'e'});
-    assertEquals(128, is.readVInt());
-    assertEquals(16383, is.readVInt());
-    assertEquals(16384, is.readVInt());
-    assertEquals(16385, is.readVInt());
-    assertEquals("Lucene", is.readString());
+    IndexInput is = new MockIndexInput(new byte[] { 
+      (byte) 0x80, 0x01,
+      (byte) 0xFF, 0x7F,
+      (byte) 0x80, (byte) 0x80, 0x01,
+      (byte) 0x81, (byte) 0x80, 0x01,
+      0x06, 'L', 'u', 'c', 'e', 'n', 'e',
+
+      // 2-byte UTF-8 (U+00BF "INVERTED QUESTION MARK") 
+      0x02, (byte) 0xC2, (byte) 0xBF,
+      0x0A, 'L', 'u', (byte) 0xC2, (byte) 0xBF, 
+            'c', 'e', (byte) 0xC2, (byte) 0xBF, 
+            'n', 'e',
+
+      // 3-byte UTF-8 (U+2620 "SKULL AND CROSSBONES") 
+      0x03, (byte) 0xE2, (byte) 0x98, (byte) 0xA0,
+      0x0C, 'L', 'u', (byte) 0xE2, (byte) 0x98, (byte) 0xA0,
+            'c', 'e', (byte) 0xE2, (byte) 0x98, (byte) 0xA0,
+            'n', 'e',
+
+      // surrogate pairs
+      // (U+1D11E "MUSICAL SYMBOL G CLEF")
+      // (U+1D160 "MUSICAL SYMBOL EIGHTH NOTE")
+      0x04, (byte) 0xF0, (byte) 0x9D, (byte) 0x84, (byte) 0x9E,
+      0x08, (byte) 0xF0, (byte) 0x9D, (byte) 0x84, (byte) 0x9E, 
+            (byte) 0xF0, (byte) 0x9D, (byte) 0x85, (byte) 0xA0, 
+      0x0E, 'L', 'u',
+            (byte) 0xF0, (byte) 0x9D, (byte) 0x84, (byte) 0x9E,
+            'c', 'e', 
+            (byte) 0xF0, (byte) 0x9D, (byte) 0x85, (byte) 0xA0, 
+            'n', 'e',  
+
+      // null bytes
+      0x01, 0x00,
+      0x08, 'L', 'u', 0x00, 'c', 'e', 0x00, 'n', 'e',
+      
+      // Modified UTF-8 null bytes
+      0x02, (byte) 0xC0, (byte) 0x80,
+      0x0A, 'L', 'u', (byte) 0xC0, (byte) 0x80, 
+            'c', 'e', (byte) 0xC0, (byte) 0x80, 
+            'n', 'e',
+
+    });
+        
+    assertEquals(128,is.readVInt());
+    assertEquals(16383,is.readVInt());
+    assertEquals(16384,is.readVInt());
+    assertEquals(16385,is.readVInt());
+    assertEquals("Lucene",is.readString());
+
+    assertEquals("\u00BF",is.readString());
+    assertEquals("Lu\u00BFce\u00BFne",is.readString());
+
+    assertEquals("\u2620",is.readString());
+    assertEquals("Lu\u2620ce\u2620ne",is.readString());
+
+    assertEquals("\uD834\uDD1E",is.readString());
+    assertEquals("\uD834\uDD1E\uD834\uDD60",is.readString());
+    assertEquals("Lu\uD834\uDD1Ece\uD834\uDD60ne",is.readString());
+    
+    assertEquals("\u0000",is.readString());
+    assertEquals("Lu\u0000ce\u0000ne",is.readString());
+
+    assertEquals("\u0000",is.readString());
+    assertEquals("Lu\u0000ce\u0000ne",is.readString());
   }
 
   /**
diff --git a/src/test/org/apache/lucene/index/TestIndexWriter.java b/src/test/org/apache/lucene/index/TestIndexWriter.java
index d6e3249..79e4183 100644
--- a/src/test/org/apache/lucene/index/TestIndexWriter.java
+++ b/src/test/org/apache/lucene/index/TestIndexWriter.java
@@ -25,6 +25,7 @@ import java.util.ArrayList;
 import java.util.Random;
 
 import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.UnicodeUtil;
 
 import org.apache.lucene.analysis.WhitespaceAnalyzer;
 import org.apache.lucene.analysis.WhitespaceTokenizer;
@@ -3329,4 +3330,223 @@ public class TestIndexWriter extends LuceneTestCase
     w.abort();
     dir.close();
   }
+  
+  final String[] utf8Data = new String[] {
+    // unpaired low surrogate
+    "ab\udc17cd", "ab\ufffdcd",
+    "\udc17abcd", "\ufffdabcd",
+    "\udc17", "\ufffd",
+    "ab\udc17\udc17cd", "ab\ufffd\ufffdcd",
+    "\udc17\udc17abcd", "\ufffd\ufffdabcd",
+    "\udc17\udc17", "\ufffd\ufffd",
+
+    // unpaired high surrogate
+    "ab\ud917cd", "ab\ufffdcd",
+    "\ud917abcd", "\ufffdabcd",
+    "\ud917", "\ufffd",
+    "ab\ud917\ud917cd", "ab\ufffd\ufffdcd",
+    "\ud917\ud917abcd", "\ufffd\ufffdabcd",
+    "\ud917\ud917", "\ufffd\ufffd",
+
+    // backwards surrogates
+    "ab\udc17\ud917cd", "ab\ufffd\ufffdcd",
+    "\udc17\ud917abcd", "\ufffd\ufffdabcd",
+    "\udc17\ud917", "\ufffd\ufffd",
+    "ab\udc17\ud917\udc17\ud917cd", "ab\ufffd\ud917\udc17\ufffdcd",
+    "\udc17\ud917\udc17\ud917abcd", "\ufffd\ud917\udc17\ufffdabcd",
+    "\udc17\ud917\udc17\ud917", "\ufffd\ud917\udc17\ufffd"
+  };
+
+  // LUCENE-510
+  public void testInvalidUTF16() throws Throwable {
+    MockRAMDirectory dir = new MockRAMDirectory();
+    IndexWriter w = new IndexWriter(dir, false, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
+    Document doc = new Document();
+
+    final int count = utf8Data.length/2;
+    for(int i=0;i<count;i++)
+      doc.add(new Field("f" + i, utf8Data[2*i], Field.Store.YES, Field.Index.TOKENIZED));
+    w.addDocument(doc);
+    w.close();
+
+    IndexReader ir = IndexReader.open(dir);
+    Document doc2 = ir.document(0);
+    for(int i=0;i<count;i++) {
+      assertEquals("field " + i + " was not indexed correctly", 1, ir.docFreq(new Term("f"+i, utf8Data[2*i+1])));
+      assertEquals("field " + i + " is incorrect", utf8Data[2*i+1], doc2.getField("f"+i).stringValue());
+    }
+    ir.close();
+    dir.close();
+  }
+
+  // LUCENE-510
+  public void testAllUnicodeChars() throws Throwable {
+
+    UnicodeUtil.UTF8Result utf8 = new UnicodeUtil.UTF8Result();
+    UnicodeUtil.UTF16Result utf16 = new UnicodeUtil.UTF16Result();
+    char[] chars = new char[2];
+    for(int ch=0;ch<0x0010FFFF;ch++) {
+
+      if (ch == 0xd800)
+        // Skip invalid code points
+        ch = 0xe000;
+
+      int len = 0;
+      if (ch <= 0xffff) {
+        chars[len++] = (char) ch;
+      } else {
+        chars[len++] = (char) (((ch-0x0010000) >> 10) + UnicodeUtil.UNI_SUR_HIGH_START);
+        chars[len++] = (char) (((ch-0x0010000) & 0x3FFL) + UnicodeUtil.UNI_SUR_LOW_START);
+      }
+
+      UnicodeUtil.UTF16toUTF8(chars, 0, len, utf8);
+      
+      String s1 = new String(chars, 0, len);
+      String s2 = new String(utf8.result, 0, utf8.length, "UTF-8");
+      assertEquals("codepoint " + ch, s1, s2);
+
+      UnicodeUtil.UTF8toUTF16(utf8.result, 0, utf8.length, utf16);
+      assertEquals("codepoint " + ch, s1, new String(utf16.result, 0, utf16.length));
+
+      byte[] b = s1.getBytes("UTF-8");
+      assertEquals(utf8.length, b.length);
+      for(int j=0;j<utf8.length;j++)
+        assertEquals(utf8.result[j], b[j]);
+    }
+  }
+
+  Random r = new Random();
+
+  private int nextInt(int lim) {
+    return r.nextInt(lim);
+  }
+
+  private int nextInt(int start, int end) {
+    return start + nextInt(end-start);
+  }
+
+  private boolean fillUnicode(char[] buffer, char[] expected, int offset, int count) {
+    final int len = offset + count;
+    boolean hasIllegal = false;
+
+    if (offset > 0 && buffer[offset] >= 0xdc00 && buffer[offset] < 0xe000)
+      // Don't start in the middle of a valid surrogate pair
+      offset--;
+
+    for(int i=offset;i<len;i++) {
+      int t = nextInt(6);
+      if (0 == t && i < len-1) {
+        // Make a surrogate pair
+        // High surrogate
+        expected[i] = buffer[i++] = (char) nextInt(0xd800, 0xdc00);
+        // Low surrogate
+        expected[i] = buffer[i] = (char) nextInt(0xdc00, 0xe000);
+      } else if (t <= 1)
+        expected[i] = buffer[i] = (char) nextInt(0x80);
+      else if (2 == t)
+        expected[i] = buffer[i] = (char) nextInt(0x80, 0x800);
+      else if (3 == t)
+        expected[i] = buffer[i] = (char) nextInt(0x800, 0xd800);
+      else if (4 == t)
+        expected[i] = buffer[i] = (char) nextInt(0xe000, 0xffff);
+      else if (5 == t && i < len-1) {
+        // Illegal unpaired surrogate
+        if (nextInt(10) == 7) {
+          if (r.nextBoolean())
+            buffer[i] = (char) nextInt(0xd800, 0xdc00);
+          else
+            buffer[i] = (char) nextInt(0xdc00, 0xe000);
+          expected[i++] = 0xfffd;
+          expected[i] = buffer[i] = (char) nextInt(0x800, 0xd800);
+          hasIllegal = true;
+        } else 
+          expected[i] = buffer[i] = (char) nextInt(0x800, 0xd800);
+      } else {
+        expected[i] = buffer[i] = ' ';
+      }
+    }
+
+    return hasIllegal;
+  }
+
+  // LUCENE-510
+  public void testRandomUnicodeStrings() throws Throwable {
+
+    char[] buffer = new char[20];
+    char[] expected = new char[20];
+
+    UnicodeUtil.UTF8Result utf8 = new UnicodeUtil.UTF8Result();
+    UnicodeUtil.UTF16Result utf16 = new UnicodeUtil.UTF16Result();
+
+    for(int iter=0;iter<100000;iter++) {
+      boolean hasIllegal = fillUnicode(buffer, expected, 0, 20);
+
+      UnicodeUtil.UTF16toUTF8(buffer, 0, 20, utf8);
+      if (!hasIllegal) {
+        byte[] b = new String(buffer, 0, 20).getBytes("UTF-8");
+        assertEquals(b.length, utf8.length);
+        for(int i=0;i<b.length;i++)
+          assertEquals(b[i], utf8.result[i]);
+      }
+
+      UnicodeUtil.UTF8toUTF16(utf8.result, 0, utf8.length, utf16);
+      assertEquals(utf16.length, 20);
+      for(int i=0;i<20;i++)
+        assertEquals(expected[i], utf16.result[i]);
+    }
+  }
+
+  // LUCENE-510
+  public void testIncrementalUnicodeStrings() throws Throwable {
+    char[] buffer = new char[20];
+    char[] expected = new char[20];
+
+    UnicodeUtil.UTF8Result utf8 = new UnicodeUtil.UTF8Result();
+    UnicodeUtil.UTF16Result utf16 = new UnicodeUtil.UTF16Result();
+    UnicodeUtil.UTF16Result utf16a = new UnicodeUtil.UTF16Result();
+
+    boolean hasIllegal = false;
+    byte[] last = new byte[60];
+
+    for(int iter=0;iter<100000;iter++) {
+
+      final int prefix;
+
+      if (iter == 0 || hasIllegal)
+        prefix = 0;
+      else
+        prefix = nextInt(20);
+
+      hasIllegal = fillUnicode(buffer, expected, prefix, 20-prefix);
+
+      UnicodeUtil.UTF16toUTF8(buffer, 0, 20, utf8);
+      if (!hasIllegal) {
+        byte[] b = new String(buffer, 0, 20).getBytes("UTF-8");
+        assertEquals(b.length, utf8.length);
+        for(int i=0;i<b.length;i++)
+          assertEquals(b[i], utf8.result[i]);
+      }
+
+      int bytePrefix = 20;
+      if (iter == 0 || hasIllegal)
+        bytePrefix = 0;
+      else
+        for(int i=0;i<20;i++)
+          if (last[i] != utf8.result[i]) {
+            bytePrefix = i;
+            break;
+          }
+      System.arraycopy(utf8.result, 0, last, 0, utf8.length);
+
+      UnicodeUtil.UTF8toUTF16(utf8.result, bytePrefix, utf8.length-bytePrefix, utf16);
+      assertEquals(20, utf16.length);
+      for(int i=0;i<20;i++)
+        assertEquals(expected[i], utf16.result[i]);
+
+      UnicodeUtil.UTF8toUTF16(utf8.result, 0, utf8.length, utf16a);
+      assertEquals(20, utf16a.length);
+      for(int i=0;i<20;i++)
+        assertEquals(expected[i], utf16a.result[i]);
+    }
+  }
 }
diff --git a/src/test/org/apache/lucene/index/TestStressIndexing2.java b/src/test/org/apache/lucene/index/TestStressIndexing2.java
index 69bb27f..8eb1a28 100644
--- a/src/test/org/apache/lucene/index/TestStressIndexing2.java
+++ b/src/test/org/apache/lucene/index/TestStressIndexing2.java
@@ -415,8 +415,56 @@ public class TestStressIndexing2 extends LuceneTestCase {
       return r.nextInt(lim);
     }
 
+    // start is inclusive and end is exclusive
+    public int nextInt(int start, int end) {
+      return start + r.nextInt(end-start);
+    }
+
+    char[] buffer = new char[100];
+
+    private int addUTF8Token(int start) {
+      final int end = start + nextInt(20);
+      if (buffer.length < 1+end) {
+        char[] newBuffer = new char[(int) ((1+end)*1.25)];
+        System.arraycopy(buffer, 0, newBuffer, 0, buffer.length);
+        buffer = newBuffer;
+      }
+
+      for(int i=start;i<end;i++) {
+        int t = nextInt(6);
+        if (0 == t && i < end-1) {
+          // Make a surrogate pair
+          // High surrogate
+          buffer[i++] = (char) nextInt(0xd800, 0xdc00);
+          // Low surrogate
+          buffer[i] = (char) nextInt(0xdc00, 0xe000);
+        } else if (t <= 1)
+          buffer[i] = (char) nextInt(0x80);
+        else if (2 == t)
+          buffer[i] = (char) nextInt(0x80, 0x800);
+        else if (3 == t)
+          buffer[i] = (char) nextInt(0x800, 0xd800);
+        else if (4 == t)
+          buffer[i] = (char) nextInt(0xe000, 0xffff);
+        else if (5 == t) {
+          // Illegal unpaired surrogate
+          if (r.nextBoolean())
+            buffer[i] = (char) nextInt(0xd800, 0xdc00);
+          else
+            buffer[i] = (char) nextInt(0xdc00, 0xe000);
+        }
+      }
+      buffer[end] = ' ';
+      return 1+end;
+    }
+
     public String getString(int nTokens) {
       nTokens = nTokens!=0 ? nTokens : r.nextInt(4)+1;
+
+      // Half the time make a random UTF8 string
+      if (r.nextBoolean())
+        return getUTF8String(nTokens);
+
       // avoid StringBuffer because it adds extra synchronization.
       char[] arr = new char[nTokens*2];
       for (int i=0; i<nTokens; i++) {
@@ -425,6 +473,14 @@ public class TestStressIndexing2 extends LuceneTestCase {
       }
       return new String(arr);
     }
+    
+    public String getUTF8String(int nTokens) {
+      int upto = 0;
+      Arrays.fill(buffer, (char) 0);
+      for(int i=0;i<nTokens;i++)
+        upto = addUTF8Token(upto);
+      return new String(buffer, 0, upto);
+    }
 
     public String getIdString() {
       return Integer.toString(base + nextInt(range));
diff --git a/src/test/org/apache/lucene/index/index.23.cfs.zip b/src/test/org/apache/lucene/index/index.23.cfs.zip
index 5854655..213dfb3 100644
Binary files a/src/test/org/apache/lucene/index/index.23.cfs.zip and b/src/test/org/apache/lucene/index/index.23.cfs.zip differ
diff --git a/src/test/org/apache/lucene/index/index.23.nocfs.zip b/src/test/org/apache/lucene/index/index.23.nocfs.zip
index 0876921..c357f7e 100644
Binary files a/src/test/org/apache/lucene/index/index.23.nocfs.zip and b/src/test/org/apache/lucene/index/index.23.nocfs.zip differ

