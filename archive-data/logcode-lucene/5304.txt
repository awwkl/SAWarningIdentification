GitDiffStart: 83cbdfda3def4123e577ba544ed08bcd031a3650 | Wed Sep 24 08:47:38 2014 +0000
diff --git a/lucene/core/src/java/org/apache/lucene/index/CheckIndex.java b/lucene/core/src/java/org/apache/lucene/index/CheckIndex.java
index 6dfc107..db4a5a3 100644
--- a/lucene/core/src/java/org/apache/lucene/index/CheckIndex.java
+++ b/lucene/core/src/java/org/apache/lucene/index/CheckIndex.java
@@ -170,7 +170,7 @@ public class CheckIndex {
       /** Current deletions generation. */
       public long deletionsGen;
 
-      /** True if we were able to open an AtomicReader on this
+      /** True if we were able to open an LeafReader on this
        *  segment. */
       public boolean openReaderPassed;
 
diff --git a/lucene/core/src/java/org/apache/lucene/index/CompositeReader.java b/lucene/core/src/java/org/apache/lucene/index/CompositeReader.java
index 5516a3e..def95de 100644
--- a/lucene/core/src/java/org/apache/lucene/index/CompositeReader.java
+++ b/lucene/core/src/java/org/apache/lucene/index/CompositeReader.java
@@ -23,7 +23,7 @@ import org.apache.lucene.store.*;
 
 /**
  Instances of this reader type can only
- be used to get stored fields from the underlying AtomicReaders,
+ be used to get stored fields from the underlying LeafReaders,
  but it is not possible to directly retrieve postings. To do that, get
  the {@link LeafReaderContext} for all sub-readers via {@link #leaves()}.
  Alternatively, you can mimic an {@link LeafReader} (with a serious slowdown),
diff --git a/lucene/core/src/java/org/apache/lucene/index/DocValues.java b/lucene/core/src/java/org/apache/lucene/index/DocValues.java
index fa7e87a..03f062b 100644
--- a/lucene/core/src/java/org/apache/lucene/index/DocValues.java
+++ b/lucene/core/src/java/org/apache/lucene/index/DocValues.java
@@ -197,7 +197,7 @@ public final class DocValues {
   }
   
   // some helpers, for transition from fieldcache apis.
-  // as opposed to the AtomicReader apis (which must be strict for consistency), these are lenient
+  // as opposed to the LeafReader apis (which must be strict for consistency), these are lenient
   
   /**
    * Returns NumericDocValues for the reader, or {@link #emptyNumeric()} if it has none. 
diff --git a/lucene/core/src/java/org/apache/lucene/index/FilterDirectoryReader.java b/lucene/core/src/java/org/apache/lucene/index/FilterDirectoryReader.java
index 091bc21..db30d76 100644
--- a/lucene/core/src/java/org/apache/lucene/index/FilterDirectoryReader.java
+++ b/lucene/core/src/java/org/apache/lucene/index/FilterDirectoryReader.java
@@ -37,7 +37,7 @@ public abstract class FilterDirectoryReader extends DirectoryReader {
    * Factory class passed to FilterDirectoryReader constructor that allows
    * subclasses to wrap the filtered DirectoryReader's subreaders.  You
    * can use this to, e.g., wrap the subreaders with specialised
-   * FilterAtomicReader implementations.
+   * FilterLeafReader implementations.
    */
   public static abstract class SubReaderWrapper {
 
@@ -55,7 +55,7 @@ public abstract class FilterDirectoryReader extends DirectoryReader {
     /**
      * Wrap one of the parent DirectoryReader's subreaders
      * @param reader the subreader to wrap
-     * @return a wrapped/filtered AtomicReader
+     * @return a wrapped/filtered LeafReader
      */
     public abstract LeafReader wrap(LeafReader reader);
 
diff --git a/lucene/core/src/java/org/apache/lucene/index/FilterLeafReader.java b/lucene/core/src/java/org/apache/lucene/index/FilterLeafReader.java
index 822bda2..2025bcb 100644
--- a/lucene/core/src/java/org/apache/lucene/index/FilterLeafReader.java
+++ b/lucene/core/src/java/org/apache/lucene/index/FilterLeafReader.java
@@ -25,12 +25,12 @@ import org.apache.lucene.util.AttributeSource;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.BytesRef;
 
-/**  A <code>FilterAtomicReader</code> contains another AtomicReader, which it
+/**  A <code>FilterLeafReader</code> contains another LeafReader, which it
  * uses as its basic source of data, possibly transforming the data along the
  * way or providing additional functionality. The class
- * <code>FilterAtomicReader</code> itself simply implements all abstract methods
+ * <code>FilterLeafReader</code> itself simply implements all abstract methods
  * of <code>IndexReader</code> with versions that pass all requests to the
- * contained index reader. Subclasses of <code>FilterAtomicReader</code> may
+ * contained index reader. Subclasses of <code>FilterLeafReader</code> may
  * further override some of these methods and may also provide additional
  * methods and fields.
  * <p><b>NOTE</b>: If you override {@link #getLiveDocs()}, you will likely need
@@ -317,12 +317,12 @@ public class FilterLeafReader extends LeafReader {
     }
   }
 
-  /** The underlying AtomicReader. */
+  /** The underlying LeafReader. */
   protected final LeafReader in;
 
   /**
-   * <p>Construct a FilterAtomicReader based on the specified base reader.
-   * <p>Note that base reader is closed if this FilterAtomicReader is closed.</p>
+   * <p>Construct a FilterLeafReader based on the specified base reader.
+   * <p>Note that base reader is closed if this FilterLeafReader is closed.</p>
    * @param in specified base reader.
    */
   public FilterLeafReader(LeafReader in) {
@@ -390,7 +390,7 @@ public class FilterLeafReader extends LeafReader {
 
   @Override
   public String toString() {
-    final StringBuilder buffer = new StringBuilder("FilterAtomicReader(");
+    final StringBuilder buffer = new StringBuilder("FilterLeafReader(");
     buffer.append(in);
     buffer.append(')');
     return buffer.toString();
diff --git a/lucene/core/src/java/org/apache/lucene/index/IndexReader.java b/lucene/core/src/java/org/apache/lucene/index/IndexReader.java
index 6378974..0c52ed3 100644
--- a/lucene/core/src/java/org/apache/lucene/index/IndexReader.java
+++ b/lucene/core/src/java/org/apache/lucene/index/IndexReader.java
@@ -53,7 +53,7 @@ import java.util.concurrent.atomic.AtomicInteger;
   and postings.
   <li>{@link CompositeReader}: Instances (like {@link DirectoryReader})
   of this reader can only
-  be used to get stored fields from the underlying AtomicReaders,
+  be used to get stored fields from the underlying LeafReaders,
   but it is not possible to directly retrieve postings. To do that, get
   the sub-readers via {@link CompositeReader#getSequentialSubReaders}.
   Alternatively, you can mimic an {@link LeafReader} (with a serious slowdown),
@@ -88,7 +88,7 @@ public abstract class IndexReader implements Closeable {
 
   IndexReader() {
     if (!(this instanceof CompositeReader || this instanceof LeafReader))
-      throw new Error("IndexReader should never be directly extended, subclass AtomicReader or CompositeReader instead.");
+      throw new Error("IndexReader should never be directly extended, subclass LeafReader or CompositeReader instead.");
   }
   
   /**
diff --git a/lucene/core/src/java/org/apache/lucene/index/LeafReader.java b/lucene/core/src/java/org/apache/lucene/index/LeafReader.java
index ac421c7..c8bb0a8 100644
--- a/lucene/core/src/java/org/apache/lucene/index/LeafReader.java
+++ b/lucene/core/src/java/org/apache/lucene/index/LeafReader.java
@@ -22,7 +22,7 @@ import java.io.IOException;
 import org.apache.lucene.index.IndexReader.ReaderClosedListener;
 import org.apache.lucene.util.Bits;
 
-/** {@code AtomicReader} is an abstract class, providing an interface for accessing an
+/** {@code LeafReader} is an abstract class, providing an interface for accessing an
  index.  Search of an index is done entirely through this abstract interface,
  so that any subclass which implements it is searchable. IndexReaders implemented
  by this subclass do not consist of several sub-readers,
diff --git a/lucene/core/src/java/org/apache/lucene/index/MultiDocValues.java b/lucene/core/src/java/org/apache/lucene/index/MultiDocValues.java
index 4532105..23b3bb3 100644
--- a/lucene/core/src/java/org/apache/lucene/index/MultiDocValues.java
+++ b/lucene/core/src/java/org/apache/lucene/index/MultiDocValues.java
@@ -41,7 +41,7 @@ import org.apache.lucene.util.packed.PackedLongValues;
  * <p><b>NOTE</b>: for multi readers, you'll get better
  * performance by gathering the sub readers using
  * {@link IndexReader#getContext()} to get the
- * atomic leaves and then operate per-AtomicReader,
+ * atomic leaves and then operate per-LeafReader,
  * instead of using this class.
  * 
  * <p><b>NOTE</b>: This is very costly.
diff --git a/lucene/core/src/java/org/apache/lucene/index/MultiFields.java b/lucene/core/src/java/org/apache/lucene/index/MultiFields.java
index 48fe362..4950a11 100644
--- a/lucene/core/src/java/org/apache/lucene/index/MultiFields.java
+++ b/lucene/core/src/java/org/apache/lucene/index/MultiFields.java
@@ -40,7 +40,7 @@ import org.apache.lucene.util.MergedIterator;
  * <p><b>NOTE</b>: for composite readers, you'll get better
  * performance by gathering the sub readers using
  * {@link IndexReader#getContext()} to get the
- * atomic leaves and then operate per-AtomicReader,
+ * atomic leaves and then operate per-LeafReader,
  * instead of using this class.
  *
  * @lucene.experimental
diff --git a/lucene/core/src/java/org/apache/lucene/index/ParallelLeafReader.java b/lucene/core/src/java/org/apache/lucene/index/ParallelLeafReader.java
index 6d1e952..0b4495f 100644
--- a/lucene/core/src/java/org/apache/lucene/index/ParallelLeafReader.java
+++ b/lucene/core/src/java/org/apache/lucene/index/ParallelLeafReader.java
@@ -59,19 +59,19 @@ public class ParallelLeafReader extends LeafReader {
   private final SortedMap<String,LeafReader> fieldToReader = new TreeMap<>();
   private final SortedMap<String,LeafReader> tvFieldToReader = new TreeMap<>();
   
-  /** Create a ParallelAtomicReader based on the provided
+  /** Create a ParallelLeafReader based on the provided
    *  readers; auto-closes the given readers on {@link #close()}. */
   public ParallelLeafReader(LeafReader... readers) throws IOException {
     this(true, readers);
   }
 
-  /** Create a ParallelAtomicReader based on the provided
+  /** Create a ParallelLeafReader based on the provided
    *  readers. */
   public ParallelLeafReader(boolean closeSubReaders, LeafReader... readers) throws IOException {
     this(closeSubReaders, readers, readers);
   }
 
-  /** Expert: create a ParallelAtomicReader based on the provided
+  /** Expert: create a ParallelLeafReader based on the provided
    *  readers and storedFieldReaders; when a document is
    *  loaded, only storedFieldsReaders will be used. */
   public ParallelLeafReader(boolean closeSubReaders, LeafReader[] readers, LeafReader[] storedFieldsReaders) throws IOException {
@@ -141,7 +141,7 @@ public class ParallelLeafReader extends LeafReader {
 
   @Override
   public String toString() {
-    final StringBuilder buffer = new StringBuilder("ParallelAtomicReader(");
+    final StringBuilder buffer = new StringBuilder("ParallelLeafReader(");
     for (final Iterator<LeafReader> iter = completeReaderSet.iterator(); iter.hasNext();) {
       buffer.append(iter.next());
       if (iter.hasNext()) buffer.append(", ");
diff --git a/lucene/core/src/java/org/apache/lucene/index/SlowCompositeReaderWrapper.java b/lucene/core/src/java/org/apache/lucene/index/SlowCompositeReaderWrapper.java
index 4dff639..bf24714 100644
--- a/lucene/core/src/java/org/apache/lucene/index/SlowCompositeReaderWrapper.java
+++ b/lucene/core/src/java/org/apache/lucene/index/SlowCompositeReaderWrapper.java
@@ -40,7 +40,7 @@ import org.apache.lucene.index.MultiDocValues.OrdinalMap;
  * performance hit.  If this is important to your use case,
  * you'll get better performance by gathering the sub readers using
  * {@link IndexReader#getContext()} to get the
- * atomic leaves and then operate per-AtomicReader,
+ * atomic leaves and then operate per-LeafReader,
  * instead of using this class.
  */
 public final class SlowCompositeReaderWrapper extends LeafReader {
diff --git a/lucene/core/src/java/org/apache/lucene/search/LeafCollector.java b/lucene/core/src/java/org/apache/lucene/search/LeafCollector.java
index 5fd20d2..38a05aa 100644
--- a/lucene/core/src/java/org/apache/lucene/search/LeafCollector.java
+++ b/lucene/core/src/java/org/apache/lucene/search/LeafCollector.java
@@ -43,7 +43,7 @@ import java.io.IOException;
  * final BitSet bits = new BitSet(indexReader.maxDoc());
  * searcher.search(query, new Collector() {
  *
- *   public LeafCollector getLeafCollector(AtomicReaderContext context)
+ *   public LeafCollector getLeafCollector(LeafReaderContext context)
  *       throws IOException {
  *     final int docBase = context.docBase;
  *     return new LeafCollector() {
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestFilterAtomicReader.java b/lucene/core/src/test/org/apache/lucene/index/TestFilterAtomicReader.java
deleted file mode 100644
index ad35569..0000000
--- a/lucene/core/src/test/org/apache/lucene/index/TestFilterAtomicReader.java
+++ /dev/null
@@ -1,195 +0,0 @@
-package org.apache.lucene.index;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-import java.io.IOException;
-import java.lang.reflect.Method;
-import java.lang.reflect.Modifier;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.search.DocIdSetIterator;
-import org.apache.lucene.store.BaseDirectoryWrapper;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.LuceneTestCase;
-
-public class TestFilterAtomicReader extends LuceneTestCase {
-
-  private static class TestReader extends FilterLeafReader {
-
-    /** Filter that only permits terms containing 'e'.*/
-    private static class TestFields extends FilterFields {
-      TestFields(Fields in) {
-        super(in);
-      }
-
-      @Override
-      public Terms terms(String field) throws IOException {
-        return new TestTerms(super.terms(field));
-      }
-    }
-
-    private static class TestTerms extends FilterTerms {
-      TestTerms(Terms in) {
-        super(in);
-      }
-
-      @Override
-      public TermsEnum iterator(TermsEnum reuse) throws IOException {
-        return new TestTermsEnum(super.iterator(reuse));
-      }
-    }
-
-    private static class TestTermsEnum extends FilterTermsEnum {
-      public TestTermsEnum(TermsEnum in) {
-        super(in);
-      }
-
-      /** Scan for terms containing the letter 'e'.*/
-      @Override
-      public BytesRef next() throws IOException {
-        BytesRef text;
-        while ((text = in.next()) != null) {
-          if (text.utf8ToString().indexOf('e') != -1)
-            return text;
-        }
-        return null;
-      }
-
-      @Override
-      public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum reuse, int flags) throws IOException {
-        return new TestPositions(super.docsAndPositions(liveDocs, reuse == null ? null : ((FilterDocsAndPositionsEnum) reuse).in, flags));
-      }
-    }
-
-    /** Filter that only returns odd numbered documents. */
-    private static class TestPositions extends FilterDocsAndPositionsEnum {
-      public TestPositions(DocsAndPositionsEnum in) {
-        super(in);
-      }
-
-      /** Scan for odd numbered documents. */
-      @Override
-      public int nextDoc() throws IOException {
-        int doc;
-        while ((doc = in.nextDoc()) != NO_MORE_DOCS) {
-          if ((doc % 2) == 1)
-            return doc;
-        }
-        return NO_MORE_DOCS;
-      }
-    }
-    
-    public TestReader(IndexReader reader) throws IOException {
-      super(SlowCompositeReaderWrapper.wrap(reader));
-    }
-
-    @Override
-    public Fields fields() throws IOException {
-      return new TestFields(super.fields());
-    }
-  }
-    
-  /**
-   * Tests the IndexReader.getFieldNames implementation
-   * @throws Exception on error
-   */
-  public void testFilterIndexReader() throws Exception {
-    Directory directory = newDirectory();
-
-    IndexWriter writer = new IndexWriter(directory, newIndexWriterConfig(new MockAnalyzer(random())));
-
-    Document d1 = new Document();
-    d1.add(newTextField("default", "one two", Field.Store.YES));
-    writer.addDocument(d1);
-
-    Document d2 = new Document();
-    d2.add(newTextField("default", "one three", Field.Store.YES));
-    writer.addDocument(d2);
-
-    Document d3 = new Document();
-    d3.add(newTextField("default", "two four", Field.Store.YES));
-    writer.addDocument(d3);
-
-    writer.close();
-
-    Directory target = newDirectory();
-
-    // We mess with the postings so this can fail:
-    ((BaseDirectoryWrapper) target).setCrossCheckTermVectorsOnClose(false);
-
-    writer = new IndexWriter(target, newIndexWriterConfig(new MockAnalyzer(random())));
-    IndexReader reader = new TestReader(DirectoryReader.open(directory));
-    writer.addIndexes(reader);
-    writer.close();
-    reader.close();
-    reader = DirectoryReader.open(target);
-    
-    TermsEnum terms = MultiFields.getTerms(reader, "default").iterator(null);
-    while (terms.next() != null) {
-      assertTrue(terms.term().utf8ToString().indexOf('e') != -1);
-    }
-    
-    assertEquals(TermsEnum.SeekStatus.FOUND, terms.seekCeil(new BytesRef("one")));
-    
-    DocsAndPositionsEnum positions = terms.docsAndPositions(MultiFields.getLiveDocs(reader), null);
-    while (positions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {
-      assertTrue((positions.docID() % 2) == 1);
-    }
-
-    reader.close();
-    directory.close();
-    target.close();
-  }
-
-  private static void checkOverrideMethods(Class<?> clazz) throws NoSuchMethodException, SecurityException {
-    final Class<?> superClazz = clazz.getSuperclass();
-    for (Method m : superClazz.getMethods()) {
-      final int mods = m.getModifiers();
-      if (Modifier.isStatic(mods) || Modifier.isAbstract(mods) || Modifier.isFinal(mods) || m.isSynthetic()
-          || m.getName().equals("attributes")) {
-        continue;
-      }
-      // The point of these checks is to ensure that methods that have a default
-      // impl through other methods are not overridden. This makes the number of
-      // methods to override to have a working impl minimal and prevents from some
-      // traps: for example, think about having getCoreCacheKey delegate to the
-      // filtered impl by default
-      final Method subM = clazz.getMethod(m.getName(), m.getParameterTypes());
-      if (subM.getDeclaringClass() == clazz
-          && m.getDeclaringClass() != Object.class
-          && m.getDeclaringClass() != subM.getDeclaringClass()) {
-        fail(clazz + " overrides " + m + " although it has a default impl");
-      }
-    }
-  }
-
-  public void testOverrideMethods() throws Exception {
-    checkOverrideMethods(FilterLeafReader.class);
-    checkOverrideMethods(FilterLeafReader.FilterFields.class);
-    checkOverrideMethods(FilterLeafReader.FilterTerms.class);
-    checkOverrideMethods(FilterLeafReader.FilterTermsEnum.class);
-    checkOverrideMethods(FilterLeafReader.FilterDocsEnum.class);
-    checkOverrideMethods(FilterLeafReader.FilterDocsAndPositionsEnum.class);
-  }
-
-}
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestFilterLeafReader.java b/lucene/core/src/test/org/apache/lucene/index/TestFilterLeafReader.java
new file mode 100644
index 0000000..cbd9c1c
--- /dev/null
+++ b/lucene/core/src/test/org/apache/lucene/index/TestFilterLeafReader.java
@@ -0,0 +1,195 @@
+package org.apache.lucene.index;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+import java.io.IOException;
+import java.lang.reflect.Method;
+import java.lang.reflect.Modifier;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.search.DocIdSetIterator;
+import org.apache.lucene.store.BaseDirectoryWrapper;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.LuceneTestCase;
+
+public class TestFilterLeafReader extends LuceneTestCase {
+
+  private static class TestReader extends FilterLeafReader {
+
+    /** Filter that only permits terms containing 'e'.*/
+    private static class TestFields extends FilterFields {
+      TestFields(Fields in) {
+        super(in);
+      }
+
+      @Override
+      public Terms terms(String field) throws IOException {
+        return new TestTerms(super.terms(field));
+      }
+    }
+
+    private static class TestTerms extends FilterTerms {
+      TestTerms(Terms in) {
+        super(in);
+      }
+
+      @Override
+      public TermsEnum iterator(TermsEnum reuse) throws IOException {
+        return new TestTermsEnum(super.iterator(reuse));
+      }
+    }
+
+    private static class TestTermsEnum extends FilterTermsEnum {
+      public TestTermsEnum(TermsEnum in) {
+        super(in);
+      }
+
+      /** Scan for terms containing the letter 'e'.*/
+      @Override
+      public BytesRef next() throws IOException {
+        BytesRef text;
+        while ((text = in.next()) != null) {
+          if (text.utf8ToString().indexOf('e') != -1)
+            return text;
+        }
+        return null;
+      }
+
+      @Override
+      public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum reuse, int flags) throws IOException {
+        return new TestPositions(super.docsAndPositions(liveDocs, reuse == null ? null : ((FilterDocsAndPositionsEnum) reuse).in, flags));
+      }
+    }
+
+    /** Filter that only returns odd numbered documents. */
+    private static class TestPositions extends FilterDocsAndPositionsEnum {
+      public TestPositions(DocsAndPositionsEnum in) {
+        super(in);
+      }
+
+      /** Scan for odd numbered documents. */
+      @Override
+      public int nextDoc() throws IOException {
+        int doc;
+        while ((doc = in.nextDoc()) != NO_MORE_DOCS) {
+          if ((doc % 2) == 1)
+            return doc;
+        }
+        return NO_MORE_DOCS;
+      }
+    }
+    
+    public TestReader(IndexReader reader) throws IOException {
+      super(SlowCompositeReaderWrapper.wrap(reader));
+    }
+
+    @Override
+    public Fields fields() throws IOException {
+      return new TestFields(super.fields());
+    }
+  }
+    
+  /**
+   * Tests the IndexReader.getFieldNames implementation
+   * @throws Exception on error
+   */
+  public void testFilterIndexReader() throws Exception {
+    Directory directory = newDirectory();
+
+    IndexWriter writer = new IndexWriter(directory, newIndexWriterConfig(new MockAnalyzer(random())));
+
+    Document d1 = new Document();
+    d1.add(newTextField("default", "one two", Field.Store.YES));
+    writer.addDocument(d1);
+
+    Document d2 = new Document();
+    d2.add(newTextField("default", "one three", Field.Store.YES));
+    writer.addDocument(d2);
+
+    Document d3 = new Document();
+    d3.add(newTextField("default", "two four", Field.Store.YES));
+    writer.addDocument(d3);
+
+    writer.close();
+
+    Directory target = newDirectory();
+
+    // We mess with the postings so this can fail:
+    ((BaseDirectoryWrapper) target).setCrossCheckTermVectorsOnClose(false);
+
+    writer = new IndexWriter(target, newIndexWriterConfig(new MockAnalyzer(random())));
+    IndexReader reader = new TestReader(DirectoryReader.open(directory));
+    writer.addIndexes(reader);
+    writer.close();
+    reader.close();
+    reader = DirectoryReader.open(target);
+    
+    TermsEnum terms = MultiFields.getTerms(reader, "default").iterator(null);
+    while (terms.next() != null) {
+      assertTrue(terms.term().utf8ToString().indexOf('e') != -1);
+    }
+    
+    assertEquals(TermsEnum.SeekStatus.FOUND, terms.seekCeil(new BytesRef("one")));
+    
+    DocsAndPositionsEnum positions = terms.docsAndPositions(MultiFields.getLiveDocs(reader), null);
+    while (positions.nextDoc() != DocIdSetIterator.NO_MORE_DOCS) {
+      assertTrue((positions.docID() % 2) == 1);
+    }
+
+    reader.close();
+    directory.close();
+    target.close();
+  }
+
+  private static void checkOverrideMethods(Class<?> clazz) throws NoSuchMethodException, SecurityException {
+    final Class<?> superClazz = clazz.getSuperclass();
+    for (Method m : superClazz.getMethods()) {
+      final int mods = m.getModifiers();
+      if (Modifier.isStatic(mods) || Modifier.isAbstract(mods) || Modifier.isFinal(mods) || m.isSynthetic()
+          || m.getName().equals("attributes")) {
+        continue;
+      }
+      // The point of these checks is to ensure that methods that have a default
+      // impl through other methods are not overridden. This makes the number of
+      // methods to override to have a working impl minimal and prevents from some
+      // traps: for example, think about having getCoreCacheKey delegate to the
+      // filtered impl by default
+      final Method subM = clazz.getMethod(m.getName(), m.getParameterTypes());
+      if (subM.getDeclaringClass() == clazz
+          && m.getDeclaringClass() != Object.class
+          && m.getDeclaringClass() != subM.getDeclaringClass()) {
+        fail(clazz + " overrides " + m + " although it has a default impl");
+      }
+    }
+  }
+
+  public void testOverrideMethods() throws Exception {
+    checkOverrideMethods(FilterLeafReader.class);
+    checkOverrideMethods(FilterLeafReader.FilterFields.class);
+    checkOverrideMethods(FilterLeafReader.FilterTerms.class);
+    checkOverrideMethods(FilterLeafReader.FilterTermsEnum.class);
+    checkOverrideMethods(FilterLeafReader.FilterDocsEnum.class);
+    checkOverrideMethods(FilterLeafReader.FilterDocsAndPositionsEnum.class);
+  }
+
+}
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestParallelAtomicReader.java b/lucene/core/src/test/org/apache/lucene/index/TestParallelAtomicReader.java
deleted file mode 100644
index d406444..0000000
--- a/lucene/core/src/test/org/apache/lucene/index/TestParallelAtomicReader.java
+++ /dev/null
@@ -1,324 +0,0 @@
-package org.apache.lucene.index;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Random;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.search.BooleanClause.Occur;
-import org.apache.lucene.search.*;
-import org.apache.lucene.store.AlreadyClosedException;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util.TestUtil;
-
-public class TestParallelAtomicReader extends LuceneTestCase {
-
-  private IndexSearcher parallel, single;
-  private Directory dir, dir1, dir2;
-
-  public void testQueries() throws Exception {
-    single = single(random());
-    parallel = parallel(random());
-    
-    queryTest(new TermQuery(new Term("f1", "v1")));
-    queryTest(new TermQuery(new Term("f1", "v2")));
-    queryTest(new TermQuery(new Term("f2", "v1")));
-    queryTest(new TermQuery(new Term("f2", "v2")));
-    queryTest(new TermQuery(new Term("f3", "v1")));
-    queryTest(new TermQuery(new Term("f3", "v2")));
-    queryTest(new TermQuery(new Term("f4", "v1")));
-    queryTest(new TermQuery(new Term("f4", "v2")));
-
-    BooleanQuery bq1 = new BooleanQuery();
-    bq1.add(new TermQuery(new Term("f1", "v1")), Occur.MUST);
-    bq1.add(new TermQuery(new Term("f4", "v1")), Occur.MUST);
-    queryTest(bq1);
-    
-    single.getIndexReader().close(); single = null;
-    parallel.getIndexReader().close(); parallel = null;
-    dir.close(); dir = null;
-    dir1.close(); dir1 = null;
-    dir2.close(); dir2 = null;
-  }
-
-  public void testFieldNames() throws Exception {
-    Directory dir1 = getDir1(random());
-    Directory dir2 = getDir2(random());
-    ParallelLeafReader pr = new ParallelLeafReader(SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir1)),
-                                                       SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir2)));
-    FieldInfos fieldInfos = pr.getFieldInfos();
-    assertEquals(4, fieldInfos.size());
-    assertNotNull(fieldInfos.fieldInfo("f1"));
-    assertNotNull(fieldInfos.fieldInfo("f2"));
-    assertNotNull(fieldInfos.fieldInfo("f3"));
-    assertNotNull(fieldInfos.fieldInfo("f4"));
-    pr.close();
-    dir1.close();
-    dir2.close();
-  }
-  
-  public void testRefCounts1() throws IOException {
-    Directory dir1 = getDir1(random());
-    Directory dir2 = getDir2(random());
-    LeafReader ir1, ir2;
-    // close subreaders, ParallelReader will not change refCounts, but close on its own close
-    ParallelLeafReader pr = new ParallelLeafReader(ir1 = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir1)),
-                                                       ir2 = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir2)));
-                                                       
-    // check RefCounts
-    assertEquals(1, ir1.getRefCount());
-    assertEquals(1, ir2.getRefCount());
-    pr.close();
-    assertEquals(0, ir1.getRefCount());
-    assertEquals(0, ir2.getRefCount());
-    dir1.close();
-    dir2.close();    
-  }
-  
-  public void testRefCounts2() throws IOException {
-    Directory dir1 = getDir1(random());
-    Directory dir2 = getDir2(random());
-    LeafReader ir1 = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir1));
-    LeafReader ir2 = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir2));
-    // don't close subreaders, so ParallelReader will increment refcounts
-    ParallelLeafReader pr = new ParallelLeafReader(false, ir1, ir2);
-    // check RefCounts
-    assertEquals(2, ir1.getRefCount());
-    assertEquals(2, ir2.getRefCount());
-    pr.close();
-    assertEquals(1, ir1.getRefCount());
-    assertEquals(1, ir2.getRefCount());
-    ir1.close();
-    ir2.close();
-    assertEquals(0, ir1.getRefCount());
-    assertEquals(0, ir2.getRefCount());
-    dir1.close();
-    dir2.close();    
-  }
-  
-  public void testCloseInnerReader() throws Exception {
-    Directory dir1 = getDir1(random());
-    LeafReader ir1 = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir1));
-    
-    // with overlapping
-    ParallelLeafReader pr = new ParallelLeafReader(true,
-     new LeafReader[] {ir1},
-     new LeafReader[] {ir1});
-
-    ir1.close();
-    
-    try {
-      pr.document(0);
-      fail("ParallelAtomicReader should be already closed because inner reader was closed!");
-    } catch (AlreadyClosedException e) {
-      // pass
-    }
-    
-    // noop:
-    pr.close();
-    dir1.close();
-  }
-
-  public void testIncompatibleIndexes() throws IOException {
-    // two documents:
-    Directory dir1 = getDir1(random());
-
-    // one document only:
-    Directory dir2 = newDirectory();
-    IndexWriter w2 = new IndexWriter(dir2, newIndexWriterConfig(new MockAnalyzer(random())));
-    Document d3 = new Document();
-
-    d3.add(newTextField("f3", "v1", Field.Store.YES));
-    w2.addDocument(d3);
-    w2.close();
-    
-    LeafReader ir1 = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir1));
-    LeafReader ir2 = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir2));
-
-    try {
-      new ParallelLeafReader(ir1, ir2);
-      fail("didn't get exptected exception: indexes don't have same number of documents");
-    } catch (IllegalArgumentException e) {
-      // expected exception
-    }
-
-    try {
-      new ParallelLeafReader(random().nextBoolean(),
-                               new LeafReader[] {ir1, ir2},
-                               new LeafReader[] {ir1, ir2});
-      fail("didn't get expected exception: indexes don't have same number of documents");
-    } catch (IllegalArgumentException e) {
-      // expected exception
-    }
-    // check RefCounts
-    assertEquals(1, ir1.getRefCount());
-    assertEquals(1, ir2.getRefCount());
-    ir1.close();
-    ir2.close();
-    dir1.close();
-    dir2.close();
-  }
-
-  public void testIgnoreStoredFields() throws IOException {
-    Directory dir1 = getDir1(random());
-    Directory dir2 = getDir2(random());
-    LeafReader ir1 = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir1));
-    LeafReader ir2 = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir2));
-    
-    // with overlapping
-    ParallelLeafReader pr = new ParallelLeafReader(false,
-        new LeafReader[] {ir1, ir2},
-        new LeafReader[] {ir1});
-    assertEquals("v1", pr.document(0).get("f1"));
-    assertEquals("v1", pr.document(0).get("f2"));
-    assertNull(pr.document(0).get("f3"));
-    assertNull(pr.document(0).get("f4"));
-    // check that fields are there
-    assertNotNull(pr.terms("f1"));
-    assertNotNull(pr.terms("f2"));
-    assertNotNull(pr.terms("f3"));
-    assertNotNull(pr.terms("f4"));
-    pr.close();
-    
-    // no stored fields at all
-    pr = new ParallelLeafReader(false,
-        new LeafReader[] {ir2},
-        new LeafReader[0]);
-    assertNull(pr.document(0).get("f1"));
-    assertNull(pr.document(0).get("f2"));
-    assertNull(pr.document(0).get("f3"));
-    assertNull(pr.document(0).get("f4"));
-    // check that fields are there
-    assertNull(pr.terms("f1"));
-    assertNull(pr.terms("f2"));
-    assertNotNull(pr.terms("f3"));
-    assertNotNull(pr.terms("f4"));
-    pr.close();
-    
-    // without overlapping
-    pr = new ParallelLeafReader(true,
-        new LeafReader[] {ir2},
-        new LeafReader[] {ir1});
-    assertEquals("v1", pr.document(0).get("f1"));
-    assertEquals("v1", pr.document(0).get("f2"));
-    assertNull(pr.document(0).get("f3"));
-    assertNull(pr.document(0).get("f4"));
-    // check that fields are there
-    assertNull(pr.terms("f1"));
-    assertNull(pr.terms("f2"));
-    assertNotNull(pr.terms("f3"));
-    assertNotNull(pr.terms("f4"));
-    pr.close();
-    
-    // no main readers
-    try {
-      new ParallelLeafReader(true,
-        new LeafReader[0],
-        new LeafReader[] {ir1});
-      fail("didn't get expected exception: need a non-empty main-reader array");
-    } catch (IllegalArgumentException iae) {
-      // pass
-    }
-    
-    dir1.close();
-    dir2.close();
-  }
-
-  private void queryTest(Query query) throws IOException {
-    ScoreDoc[] parallelHits = parallel.search(query, null, 1000).scoreDocs;
-    ScoreDoc[] singleHits = single.search(query, null, 1000).scoreDocs;
-    assertEquals(parallelHits.length, singleHits.length);
-    for(int i = 0; i < parallelHits.length; i++) {
-      assertEquals(parallelHits[i].score, singleHits[i].score, 0.001f);
-      StoredDocument docParallel = parallel.doc(parallelHits[i].doc);
-      StoredDocument docSingle = single.doc(singleHits[i].doc);
-      assertEquals(docParallel.get("f1"), docSingle.get("f1"));
-      assertEquals(docParallel.get("f2"), docSingle.get("f2"));
-      assertEquals(docParallel.get("f3"), docSingle.get("f3"));
-      assertEquals(docParallel.get("f4"), docSingle.get("f4"));
-    }
-  }
-
-  // Fields 1-4 indexed together:
-  private IndexSearcher single(Random random) throws IOException {
-    dir = newDirectory();
-    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random)));
-    Document d1 = new Document();
-    d1.add(newTextField("f1", "v1", Field.Store.YES));
-    d1.add(newTextField("f2", "v1", Field.Store.YES));
-    d1.add(newTextField("f3", "v1", Field.Store.YES));
-    d1.add(newTextField("f4", "v1", Field.Store.YES));
-    w.addDocument(d1);
-    Document d2 = new Document();
-    d2.add(newTextField("f1", "v2", Field.Store.YES));
-    d2.add(newTextField("f2", "v2", Field.Store.YES));
-    d2.add(newTextField("f3", "v2", Field.Store.YES));
-    d2.add(newTextField("f4", "v2", Field.Store.YES));
-    w.addDocument(d2);
-    w.close();
-
-    DirectoryReader ir = DirectoryReader.open(dir);
-    return newSearcher(ir);
-  }
-
-  // Fields 1 & 2 in one index, 3 & 4 in other, with ParallelReader:
-  private IndexSearcher parallel(Random random) throws IOException {
-    dir1 = getDir1(random);
-    dir2 = getDir2(random);
-    ParallelLeafReader pr = new ParallelLeafReader(
-        SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir1)),
-        SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir2)));
-    TestUtil.checkReader(pr);
-    return newSearcher(pr);
-  }
-
-  private Directory getDir1(Random random) throws IOException {
-    Directory dir1 = newDirectory();
-    IndexWriter w1 = new IndexWriter(dir1, newIndexWriterConfig(new MockAnalyzer(random)));
-    Document d1 = new Document();
-    d1.add(newTextField("f1", "v1", Field.Store.YES));
-    d1.add(newTextField("f2", "v1", Field.Store.YES));
-    w1.addDocument(d1);
-    Document d2 = new Document();
-    d2.add(newTextField("f1", "v2", Field.Store.YES));
-    d2.add(newTextField("f2", "v2", Field.Store.YES));
-    w1.addDocument(d2);
-    w1.close();
-    return dir1;
-  }
-
-  private Directory getDir2(Random random) throws IOException {
-    Directory dir2 = newDirectory();
-    IndexWriter w2 = new IndexWriter(dir2, newIndexWriterConfig(new MockAnalyzer(random)));
-    Document d3 = new Document();
-    d3.add(newTextField("f3", "v1", Field.Store.YES));
-    d3.add(newTextField("f4", "v1", Field.Store.YES));
-    w2.addDocument(d3);
-    Document d4 = new Document();
-    d4.add(newTextField("f3", "v2", Field.Store.YES));
-    d4.add(newTextField("f4", "v2", Field.Store.YES));
-    w2.addDocument(d4);
-    w2.close();
-    return dir2;
-  }
-
-}
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestParallelCompositeReader.java b/lucene/core/src/test/org/apache/lucene/index/TestParallelCompositeReader.java
index 64ccb79..ae6c372 100644
--- a/lucene/core/src/test/org/apache/lucene/index/TestParallelCompositeReader.java
+++ b/lucene/core/src/test/org/apache/lucene/index/TestParallelCompositeReader.java
@@ -383,7 +383,7 @@ public class TestParallelCompositeReader extends LuceneTestCase {
     ParallelCompositeReader pr = new ParallelCompositeReader(new CompositeReader[] {ir1});
     
     final String s = pr.toString();
-    assertTrue("toString incorrect: " + s, s.startsWith("ParallelCompositeReader(ParallelAtomicReader("));
+    assertTrue("toString incorrect: " + s, s.startsWith("ParallelCompositeReader(ParallelLeafReader("));
 
     pr.close();
     dir1.close();
@@ -395,7 +395,7 @@ public class TestParallelCompositeReader extends LuceneTestCase {
     ParallelCompositeReader pr = new ParallelCompositeReader(new CompositeReader[] {new MultiReader(ir1)});
     
     final String s = pr.toString();
-    assertTrue("toString incorrect: " + s, s.startsWith("ParallelCompositeReader(ParallelCompositeReader(ParallelAtomicReader("));
+    assertTrue("toString incorrect: " + s, s.startsWith("ParallelCompositeReader(ParallelCompositeReader(ParallelLeafReader("));
 
     pr.close();
     dir1.close();
diff --git a/lucene/core/src/test/org/apache/lucene/index/TestParallelLeafReader.java b/lucene/core/src/test/org/apache/lucene/index/TestParallelLeafReader.java
new file mode 100644
index 0000000..6aedca7
--- /dev/null
+++ b/lucene/core/src/test/org/apache/lucene/index/TestParallelLeafReader.java
@@ -0,0 +1,324 @@
+package org.apache.lucene.index;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Random;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.search.BooleanClause.Occur;
+import org.apache.lucene.search.*;
+import org.apache.lucene.store.AlreadyClosedException;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.TestUtil;
+
+public class TestParallelLeafReader extends LuceneTestCase {
+
+  private IndexSearcher parallel, single;
+  private Directory dir, dir1, dir2;
+
+  public void testQueries() throws Exception {
+    single = single(random());
+    parallel = parallel(random());
+    
+    queryTest(new TermQuery(new Term("f1", "v1")));
+    queryTest(new TermQuery(new Term("f1", "v2")));
+    queryTest(new TermQuery(new Term("f2", "v1")));
+    queryTest(new TermQuery(new Term("f2", "v2")));
+    queryTest(new TermQuery(new Term("f3", "v1")));
+    queryTest(new TermQuery(new Term("f3", "v2")));
+    queryTest(new TermQuery(new Term("f4", "v1")));
+    queryTest(new TermQuery(new Term("f4", "v2")));
+
+    BooleanQuery bq1 = new BooleanQuery();
+    bq1.add(new TermQuery(new Term("f1", "v1")), Occur.MUST);
+    bq1.add(new TermQuery(new Term("f4", "v1")), Occur.MUST);
+    queryTest(bq1);
+    
+    single.getIndexReader().close(); single = null;
+    parallel.getIndexReader().close(); parallel = null;
+    dir.close(); dir = null;
+    dir1.close(); dir1 = null;
+    dir2.close(); dir2 = null;
+  }
+
+  public void testFieldNames() throws Exception {
+    Directory dir1 = getDir1(random());
+    Directory dir2 = getDir2(random());
+    ParallelLeafReader pr = new ParallelLeafReader(SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir1)),
+                                                       SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir2)));
+    FieldInfos fieldInfos = pr.getFieldInfos();
+    assertEquals(4, fieldInfos.size());
+    assertNotNull(fieldInfos.fieldInfo("f1"));
+    assertNotNull(fieldInfos.fieldInfo("f2"));
+    assertNotNull(fieldInfos.fieldInfo("f3"));
+    assertNotNull(fieldInfos.fieldInfo("f4"));
+    pr.close();
+    dir1.close();
+    dir2.close();
+  }
+  
+  public void testRefCounts1() throws IOException {
+    Directory dir1 = getDir1(random());
+    Directory dir2 = getDir2(random());
+    LeafReader ir1, ir2;
+    // close subreaders, ParallelReader will not change refCounts, but close on its own close
+    ParallelLeafReader pr = new ParallelLeafReader(ir1 = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir1)),
+                                                       ir2 = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir2)));
+                                                       
+    // check RefCounts
+    assertEquals(1, ir1.getRefCount());
+    assertEquals(1, ir2.getRefCount());
+    pr.close();
+    assertEquals(0, ir1.getRefCount());
+    assertEquals(0, ir2.getRefCount());
+    dir1.close();
+    dir2.close();    
+  }
+  
+  public void testRefCounts2() throws IOException {
+    Directory dir1 = getDir1(random());
+    Directory dir2 = getDir2(random());
+    LeafReader ir1 = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir1));
+    LeafReader ir2 = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir2));
+    // don't close subreaders, so ParallelReader will increment refcounts
+    ParallelLeafReader pr = new ParallelLeafReader(false, ir1, ir2);
+    // check RefCounts
+    assertEquals(2, ir1.getRefCount());
+    assertEquals(2, ir2.getRefCount());
+    pr.close();
+    assertEquals(1, ir1.getRefCount());
+    assertEquals(1, ir2.getRefCount());
+    ir1.close();
+    ir2.close();
+    assertEquals(0, ir1.getRefCount());
+    assertEquals(0, ir2.getRefCount());
+    dir1.close();
+    dir2.close();    
+  }
+  
+  public void testCloseInnerReader() throws Exception {
+    Directory dir1 = getDir1(random());
+    LeafReader ir1 = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir1));
+    
+    // with overlapping
+    ParallelLeafReader pr = new ParallelLeafReader(true,
+     new LeafReader[] {ir1},
+     new LeafReader[] {ir1});
+
+    ir1.close();
+    
+    try {
+      pr.document(0);
+      fail("ParallelLeafReader should be already closed because inner reader was closed!");
+    } catch (AlreadyClosedException e) {
+      // pass
+    }
+    
+    // noop:
+    pr.close();
+    dir1.close();
+  }
+
+  public void testIncompatibleIndexes() throws IOException {
+    // two documents:
+    Directory dir1 = getDir1(random());
+
+    // one document only:
+    Directory dir2 = newDirectory();
+    IndexWriter w2 = new IndexWriter(dir2, newIndexWriterConfig(new MockAnalyzer(random())));
+    Document d3 = new Document();
+
+    d3.add(newTextField("f3", "v1", Field.Store.YES));
+    w2.addDocument(d3);
+    w2.close();
+    
+    LeafReader ir1 = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir1));
+    LeafReader ir2 = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir2));
+
+    try {
+      new ParallelLeafReader(ir1, ir2);
+      fail("didn't get exptected exception: indexes don't have same number of documents");
+    } catch (IllegalArgumentException e) {
+      // expected exception
+    }
+
+    try {
+      new ParallelLeafReader(random().nextBoolean(),
+                               new LeafReader[] {ir1, ir2},
+                               new LeafReader[] {ir1, ir2});
+      fail("didn't get expected exception: indexes don't have same number of documents");
+    } catch (IllegalArgumentException e) {
+      // expected exception
+    }
+    // check RefCounts
+    assertEquals(1, ir1.getRefCount());
+    assertEquals(1, ir2.getRefCount());
+    ir1.close();
+    ir2.close();
+    dir1.close();
+    dir2.close();
+  }
+
+  public void testIgnoreStoredFields() throws IOException {
+    Directory dir1 = getDir1(random());
+    Directory dir2 = getDir2(random());
+    LeafReader ir1 = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir1));
+    LeafReader ir2 = SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir2));
+    
+    // with overlapping
+    ParallelLeafReader pr = new ParallelLeafReader(false,
+        new LeafReader[] {ir1, ir2},
+        new LeafReader[] {ir1});
+    assertEquals("v1", pr.document(0).get("f1"));
+    assertEquals("v1", pr.document(0).get("f2"));
+    assertNull(pr.document(0).get("f3"));
+    assertNull(pr.document(0).get("f4"));
+    // check that fields are there
+    assertNotNull(pr.terms("f1"));
+    assertNotNull(pr.terms("f2"));
+    assertNotNull(pr.terms("f3"));
+    assertNotNull(pr.terms("f4"));
+    pr.close();
+    
+    // no stored fields at all
+    pr = new ParallelLeafReader(false,
+        new LeafReader[] {ir2},
+        new LeafReader[0]);
+    assertNull(pr.document(0).get("f1"));
+    assertNull(pr.document(0).get("f2"));
+    assertNull(pr.document(0).get("f3"));
+    assertNull(pr.document(0).get("f4"));
+    // check that fields are there
+    assertNull(pr.terms("f1"));
+    assertNull(pr.terms("f2"));
+    assertNotNull(pr.terms("f3"));
+    assertNotNull(pr.terms("f4"));
+    pr.close();
+    
+    // without overlapping
+    pr = new ParallelLeafReader(true,
+        new LeafReader[] {ir2},
+        new LeafReader[] {ir1});
+    assertEquals("v1", pr.document(0).get("f1"));
+    assertEquals("v1", pr.document(0).get("f2"));
+    assertNull(pr.document(0).get("f3"));
+    assertNull(pr.document(0).get("f4"));
+    // check that fields are there
+    assertNull(pr.terms("f1"));
+    assertNull(pr.terms("f2"));
+    assertNotNull(pr.terms("f3"));
+    assertNotNull(pr.terms("f4"));
+    pr.close();
+    
+    // no main readers
+    try {
+      new ParallelLeafReader(true,
+        new LeafReader[0],
+        new LeafReader[] {ir1});
+      fail("didn't get expected exception: need a non-empty main-reader array");
+    } catch (IllegalArgumentException iae) {
+      // pass
+    }
+    
+    dir1.close();
+    dir2.close();
+  }
+
+  private void queryTest(Query query) throws IOException {
+    ScoreDoc[] parallelHits = parallel.search(query, null, 1000).scoreDocs;
+    ScoreDoc[] singleHits = single.search(query, null, 1000).scoreDocs;
+    assertEquals(parallelHits.length, singleHits.length);
+    for(int i = 0; i < parallelHits.length; i++) {
+      assertEquals(parallelHits[i].score, singleHits[i].score, 0.001f);
+      StoredDocument docParallel = parallel.doc(parallelHits[i].doc);
+      StoredDocument docSingle = single.doc(singleHits[i].doc);
+      assertEquals(docParallel.get("f1"), docSingle.get("f1"));
+      assertEquals(docParallel.get("f2"), docSingle.get("f2"));
+      assertEquals(docParallel.get("f3"), docSingle.get("f3"));
+      assertEquals(docParallel.get("f4"), docSingle.get("f4"));
+    }
+  }
+
+  // Fields 1-4 indexed together:
+  private IndexSearcher single(Random random) throws IOException {
+    dir = newDirectory();
+    IndexWriter w = new IndexWriter(dir, newIndexWriterConfig(new MockAnalyzer(random)));
+    Document d1 = new Document();
+    d1.add(newTextField("f1", "v1", Field.Store.YES));
+    d1.add(newTextField("f2", "v1", Field.Store.YES));
+    d1.add(newTextField("f3", "v1", Field.Store.YES));
+    d1.add(newTextField("f4", "v1", Field.Store.YES));
+    w.addDocument(d1);
+    Document d2 = new Document();
+    d2.add(newTextField("f1", "v2", Field.Store.YES));
+    d2.add(newTextField("f2", "v2", Field.Store.YES));
+    d2.add(newTextField("f3", "v2", Field.Store.YES));
+    d2.add(newTextField("f4", "v2", Field.Store.YES));
+    w.addDocument(d2);
+    w.close();
+
+    DirectoryReader ir = DirectoryReader.open(dir);
+    return newSearcher(ir);
+  }
+
+  // Fields 1 & 2 in one index, 3 & 4 in other, with ParallelReader:
+  private IndexSearcher parallel(Random random) throws IOException {
+    dir1 = getDir1(random);
+    dir2 = getDir2(random);
+    ParallelLeafReader pr = new ParallelLeafReader(
+        SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir1)),
+        SlowCompositeReaderWrapper.wrap(DirectoryReader.open(dir2)));
+    TestUtil.checkReader(pr);
+    return newSearcher(pr);
+  }
+
+  private Directory getDir1(Random random) throws IOException {
+    Directory dir1 = newDirectory();
+    IndexWriter w1 = new IndexWriter(dir1, newIndexWriterConfig(new MockAnalyzer(random)));
+    Document d1 = new Document();
+    d1.add(newTextField("f1", "v1", Field.Store.YES));
+    d1.add(newTextField("f2", "v1", Field.Store.YES));
+    w1.addDocument(d1);
+    Document d2 = new Document();
+    d2.add(newTextField("f1", "v2", Field.Store.YES));
+    d2.add(newTextField("f2", "v2", Field.Store.YES));
+    w1.addDocument(d2);
+    w1.close();
+    return dir1;
+  }
+
+  private Directory getDir2(Random random) throws IOException {
+    Directory dir2 = newDirectory();
+    IndexWriter w2 = new IndexWriter(dir2, newIndexWriterConfig(new MockAnalyzer(random)));
+    Document d3 = new Document();
+    d3.add(newTextField("f3", "v1", Field.Store.YES));
+    d3.add(newTextField("f4", "v1", Field.Store.YES));
+    w2.addDocument(d3);
+    Document d4 = new Document();
+    d4.add(newTextField("f3", "v2", Field.Store.YES));
+    d4.add(newTextField("f4", "v2", Field.Store.YES));
+    w2.addDocument(d4);
+    w2.close();
+    return dir2;
+  }
+
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/OrdinalMappingLeafReader.java b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/OrdinalMappingLeafReader.java
index 672e7cb..38589a7 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/OrdinalMappingLeafReader.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/OrdinalMappingLeafReader.java
@@ -51,10 +51,10 @@ import org.apache.lucene.util.IntsRef;
  * DirectoryReader reader = DirectoryReader.open(oldDir);
  * IndexWriterConfig conf = new IndexWriterConfig(VER, ANALYZER);
  * IndexWriter writer = new IndexWriter(newDir, conf);
- * List&lt;AtomicReaderContext&gt; leaves = reader.leaves();
- * AtomicReader wrappedLeaves[] = new AtomicReader[leaves.size()];
+ * List&lt;LeafReaderContext&gt; leaves = reader.leaves();
+ * LeafReader wrappedLeaves[] = new LeafReader[leaves.size()];
  * for (int i = 0; i < leaves.size(); i++) {
- *   wrappedLeaves[i] = new OrdinalMappingAtomicReader(leaves.get(i).reader(), ordmap);
+ *   wrappedLeaves[i] = new OrdinalMappingLeafReader(leaves.get(i).reader(), ordmap);
  * }
  * writer.addIndexes(new MultiReader(wrappedLeaves));
  * writer.commit();
@@ -113,7 +113,7 @@ public class OrdinalMappingLeafReader extends FilterLeafReader {
   private final Set<String> facetFields;
   
   /**
-   * Wraps an AtomicReader, mapping ordinals according to the ordinalMap, using
+   * Wraps an LeafReader, mapping ordinals according to the ordinalMap, using
    * the provided {@link FacetsConfig} which was used to build the wrapped
    * reader.
    */
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestOrdinalMappingAtomicReader.java b/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestOrdinalMappingAtomicReader.java
deleted file mode 100644
index 1682d3c..0000000
--- a/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestOrdinalMappingAtomicReader.java
+++ /dev/null
@@ -1,140 +0,0 @@
-package org.apache.lucene.facet.taxonomy;
-
-import java.io.IOException;
-
-import org.apache.lucene.document.BinaryDocValuesField;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.facet.FacetField;
-import org.apache.lucene.facet.FacetResult;
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.Facets;
-import org.apache.lucene.facet.FacetsCollector;
-import org.apache.lucene.facet.FacetsConfig;
-import org.apache.lucene.facet.LabelAndValue;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
-import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter.MemoryOrdinalMap;
-import org.apache.lucene.index.BinaryDocValues;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.MultiDocValues;
-import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.MatchAllDocsQuery;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-import org.junit.Before;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class TestOrdinalMappingAtomicReader extends FacetTestCase {
-  
-  private static final int NUM_DOCS = 100;
-  private final FacetsConfig facetConfig = new FacetsConfig();
-  
-  @Before
-  @Override
-  public void setUp() throws Exception {
-    super.setUp();
-    facetConfig.setMultiValued("tag", true);
-    facetConfig.setIndexFieldName("tag", "$tags"); // add custom index field name
-  }
-
-  @Test
-  public void testTaxonomyMergeUtils() throws Exception {
-    Directory srcIndexDir = newDirectory();
-    Directory srcTaxoDir = newDirectory();
-    buildIndexWithFacets(srcIndexDir, srcTaxoDir, true);
-    
-    Directory targetIndexDir = newDirectory();
-    Directory targetTaxoDir = newDirectory();
-    buildIndexWithFacets(targetIndexDir, targetTaxoDir, false);
-    
-    IndexWriter destIndexWriter = new IndexWriter(targetIndexDir, newIndexWriterConfig(null));
-    DirectoryTaxonomyWriter destTaxoWriter = new DirectoryTaxonomyWriter(targetTaxoDir);
-    try {
-      TaxonomyMergeUtils.merge(srcIndexDir, srcTaxoDir, new MemoryOrdinalMap(), destIndexWriter, destTaxoWriter, facetConfig);
-    } finally {
-      IOUtils.close(destIndexWriter, destTaxoWriter);
-    }
-    verifyResults(targetIndexDir, targetTaxoDir);
-    
-    IOUtils.close(targetIndexDir, targetTaxoDir, srcIndexDir, srcTaxoDir);
-  }
-  
-  private void verifyResults(Directory indexDir, Directory taxoDir) throws IOException {
-    DirectoryReader indexReader = DirectoryReader.open(indexDir);
-    DirectoryTaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoDir);
-    IndexSearcher searcher = newSearcher(indexReader);
-    
-    FacetsCollector collector = new FacetsCollector();
-    FacetsCollector.search(searcher, new MatchAllDocsQuery(), 10, collector);
-
-    // tag facets
-    Facets tagFacets = new FastTaxonomyFacetCounts("$tags", taxoReader, facetConfig, collector);
-    FacetResult result = tagFacets.getTopChildren(10, "tag");
-    for (LabelAndValue lv: result.labelValues) {
-      if (VERBOSE) {
-        System.out.println(lv);
-      }
-      assertEquals(NUM_DOCS, lv.value.intValue());
-    }
-    
-    // id facets
-    Facets idFacets = new FastTaxonomyFacetCounts(taxoReader, facetConfig, collector);
-    FacetResult idResult = idFacets.getTopChildren(10, "id");
-    assertEquals(NUM_DOCS, idResult.childCount);
-    assertEquals(NUM_DOCS * 2, idResult.value); // each "id" appears twice
-    
-    BinaryDocValues bdv = MultiDocValues.getBinaryValues(indexReader, "bdv");
-    BinaryDocValues cbdv = MultiDocValues.getBinaryValues(indexReader, "cbdv");
-    for (int i = 0; i < indexReader.maxDoc(); i++) {
-      assertEquals(Integer.parseInt(cbdv.get(i).utf8ToString()), Integer.parseInt(bdv.get(i).utf8ToString())*2);
-    }
-    IOUtils.close(indexReader, taxoReader);
-  }
-  
-  private void buildIndexWithFacets(Directory indexDir, Directory taxoDir, boolean asc) throws IOException {
-    IndexWriterConfig config = newIndexWriterConfig(null);
-    RandomIndexWriter writer = new RandomIndexWriter(random(), indexDir, config);
-    
-    DirectoryTaxonomyWriter taxonomyWriter = new DirectoryTaxonomyWriter(taxoDir);
-    for (int i = 1; i <= NUM_DOCS; i++) {
-      Document doc = new Document();
-      for (int j = i; j <= NUM_DOCS; j++) {
-        int facetValue = asc ? j: NUM_DOCS - j;
-        doc.add(new FacetField("tag", Integer.toString(facetValue)));
-      }
-      // add a facet under default dim config
-      doc.add(new FacetField("id", Integer.toString(i)));
-      
-      // make sure OrdinalMappingAtomicReader ignores non-facet BinaryDocValues fields
-      doc.add(new BinaryDocValuesField("bdv", new BytesRef(Integer.toString(i))));
-      doc.add(new BinaryDocValuesField("cbdv", new BytesRef(Integer.toString(i*2))));
-      writer.addDocument(facetConfig.build(taxonomyWriter, doc));
-    }
-    taxonomyWriter.commit();
-    taxonomyWriter.close();
-    writer.commit();
-    writer.close();
-  }
-
-}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestOrdinalMappingLeafReader.java b/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestOrdinalMappingLeafReader.java
new file mode 100644
index 0000000..0ee930f
--- /dev/null
+++ b/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/TestOrdinalMappingLeafReader.java
@@ -0,0 +1,140 @@
+package org.apache.lucene.facet.taxonomy;
+
+import java.io.IOException;
+
+import org.apache.lucene.document.BinaryDocValuesField;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.facet.FacetField;
+import org.apache.lucene.facet.FacetResult;
+import org.apache.lucene.facet.FacetTestCase;
+import org.apache.lucene.facet.Facets;
+import org.apache.lucene.facet.FacetsCollector;
+import org.apache.lucene.facet.FacetsConfig;
+import org.apache.lucene.facet.LabelAndValue;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter.MemoryOrdinalMap;
+import org.apache.lucene.index.BinaryDocValues;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.MultiDocValues;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+import org.junit.Before;
+import org.junit.Test;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+public class TestOrdinalMappingLeafReader extends FacetTestCase {
+  
+  private static final int NUM_DOCS = 100;
+  private final FacetsConfig facetConfig = new FacetsConfig();
+  
+  @Before
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    facetConfig.setMultiValued("tag", true);
+    facetConfig.setIndexFieldName("tag", "$tags"); // add custom index field name
+  }
+
+  @Test
+  public void testTaxonomyMergeUtils() throws Exception {
+    Directory srcIndexDir = newDirectory();
+    Directory srcTaxoDir = newDirectory();
+    buildIndexWithFacets(srcIndexDir, srcTaxoDir, true);
+    
+    Directory targetIndexDir = newDirectory();
+    Directory targetTaxoDir = newDirectory();
+    buildIndexWithFacets(targetIndexDir, targetTaxoDir, false);
+    
+    IndexWriter destIndexWriter = new IndexWriter(targetIndexDir, newIndexWriterConfig(null));
+    DirectoryTaxonomyWriter destTaxoWriter = new DirectoryTaxonomyWriter(targetTaxoDir);
+    try {
+      TaxonomyMergeUtils.merge(srcIndexDir, srcTaxoDir, new MemoryOrdinalMap(), destIndexWriter, destTaxoWriter, facetConfig);
+    } finally {
+      IOUtils.close(destIndexWriter, destTaxoWriter);
+    }
+    verifyResults(targetIndexDir, targetTaxoDir);
+    
+    IOUtils.close(targetIndexDir, targetTaxoDir, srcIndexDir, srcTaxoDir);
+  }
+  
+  private void verifyResults(Directory indexDir, Directory taxoDir) throws IOException {
+    DirectoryReader indexReader = DirectoryReader.open(indexDir);
+    DirectoryTaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoDir);
+    IndexSearcher searcher = newSearcher(indexReader);
+    
+    FacetsCollector collector = new FacetsCollector();
+    FacetsCollector.search(searcher, new MatchAllDocsQuery(), 10, collector);
+
+    // tag facets
+    Facets tagFacets = new FastTaxonomyFacetCounts("$tags", taxoReader, facetConfig, collector);
+    FacetResult result = tagFacets.getTopChildren(10, "tag");
+    for (LabelAndValue lv: result.labelValues) {
+      if (VERBOSE) {
+        System.out.println(lv);
+      }
+      assertEquals(NUM_DOCS, lv.value.intValue());
+    }
+    
+    // id facets
+    Facets idFacets = new FastTaxonomyFacetCounts(taxoReader, facetConfig, collector);
+    FacetResult idResult = idFacets.getTopChildren(10, "id");
+    assertEquals(NUM_DOCS, idResult.childCount);
+    assertEquals(NUM_DOCS * 2, idResult.value); // each "id" appears twice
+    
+    BinaryDocValues bdv = MultiDocValues.getBinaryValues(indexReader, "bdv");
+    BinaryDocValues cbdv = MultiDocValues.getBinaryValues(indexReader, "cbdv");
+    for (int i = 0; i < indexReader.maxDoc(); i++) {
+      assertEquals(Integer.parseInt(cbdv.get(i).utf8ToString()), Integer.parseInt(bdv.get(i).utf8ToString())*2);
+    }
+    IOUtils.close(indexReader, taxoReader);
+  }
+  
+  private void buildIndexWithFacets(Directory indexDir, Directory taxoDir, boolean asc) throws IOException {
+    IndexWriterConfig config = newIndexWriterConfig(null);
+    RandomIndexWriter writer = new RandomIndexWriter(random(), indexDir, config);
+    
+    DirectoryTaxonomyWriter taxonomyWriter = new DirectoryTaxonomyWriter(taxoDir);
+    for (int i = 1; i <= NUM_DOCS; i++) {
+      Document doc = new Document();
+      for (int j = i; j <= NUM_DOCS; j++) {
+        int facetValue = asc ? j: NUM_DOCS - j;
+        doc.add(new FacetField("tag", Integer.toString(facetValue)));
+      }
+      // add a facet under default dim config
+      doc.add(new FacetField("id", Integer.toString(i)));
+      
+      // make sure OrdinalMappingLeafReader ignores non-facet BinaryDocValues fields
+      doc.add(new BinaryDocValuesField("bdv", new BytesRef(Integer.toString(i))));
+      doc.add(new BinaryDocValuesField("cbdv", new BytesRef(Integer.toString(i*2))));
+      writer.addDocument(facetConfig.build(taxonomyWriter, doc));
+    }
+    taxonomyWriter.commit();
+    taxonomyWriter.close();
+    writer.commit();
+    writer.close();
+  }
+
+}
diff --git a/lucene/highlighter/src/java/org/apache/lucene/search/highlight/WeightedSpanTermExtractor.java b/lucene/highlighter/src/java/org/apache/lucene/search/highlight/WeightedSpanTermExtractor.java
index e4b0004..bfd4465 100644
--- a/lucene/highlighter/src/java/org/apache/lucene/search/highlight/WeightedSpanTermExtractor.java
+++ b/lucene/highlighter/src/java/org/apache/lucene/search/highlight/WeightedSpanTermExtractor.java
@@ -368,7 +368,7 @@ public class WeightedSpanTermExtractor {
   
   /*
    * This reader will just delegate every call to a single field in the wrapped
-   * AtomicReader. This way we only need to build this field once rather than
+   * LeafReader. This way we only need to build this field once rather than
    * N-Times
    */
   static final class DelegatingLeafReader extends FilterLeafReader {
diff --git a/lucene/misc/src/java/org/apache/lucene/index/sorter/BlockJoinComparatorSource.java b/lucene/misc/src/java/org/apache/lucene/index/sorter/BlockJoinComparatorSource.java
index ec5eeb3..cb5847c 100644
--- a/lucene/misc/src/java/org/apache/lucene/index/sorter/BlockJoinComparatorSource.java
+++ b/lucene/misc/src/java/org/apache/lucene/index/sorter/BlockJoinComparatorSource.java
@@ -146,7 +146,7 @@ public class BlockJoinComparatorSource extends FieldComparatorSource {
       public FieldComparator<Integer> setNextReader(LeafReaderContext context) throws IOException {
         final DocIdSet parents = parentsFilter.getDocIdSet(context, null);
         if (parents == null) {
-          throw new IllegalStateException("AtomicReader " + context.reader() + " contains no parents!");
+          throw new IllegalStateException("LeafReader " + context.reader() + " contains no parents!");
         }
         if (!(parents instanceof FixedBitSet)) {
           throw new IllegalStateException("parentFilter must return FixedBitSet; got " + parents);
diff --git a/lucene/misc/src/java/org/apache/lucene/index/sorter/SortingLeafReader.java b/lucene/misc/src/java/org/apache/lucene/index/sorter/SortingLeafReader.java
index f96bff7..1f9bb89 100644
--- a/lucene/misc/src/java/org/apache/lucene/index/sorter/SortingLeafReader.java
+++ b/lucene/misc/src/java/org/apache/lucene/index/sorter/SortingLeafReader.java
@@ -57,7 +57,7 @@ import org.apache.lucene.util.automaton.CompiledAutomaton;
  * IndexWriter writer; // writer to which the sorted index will be added
  * DirectoryReader reader; // reader on the input index
  * Sort sort; // determines how the documents are sorted
- * AtomicReader sortingReader = SortingAtomicReader.wrap(SlowCompositeReaderWrapper.wrap(reader), sort);
+ * LeafReader sortingReader = SortingLeafReader.wrap(SlowCompositeReaderWrapper.wrap(reader), sort);
  * writer.addIndexes(reader);
  * writer.close();
  * reader.close();
diff --git a/lucene/misc/src/java/org/apache/lucene/uninverting/UninvertingReader.java b/lucene/misc/src/java/org/apache/lucene/uninverting/UninvertingReader.java
index 1cbb678..50a60af 100644
--- a/lucene/misc/src/java/org/apache/lucene/uninverting/UninvertingReader.java
+++ b/lucene/misc/src/java/org/apache/lucene/uninverting/UninvertingReader.java
@@ -52,7 +52,7 @@ import org.apache.lucene.util.Bits;
  * The uninversion process happens lazily: upon the first request for the 
  * field's docvalues (e.g. via {@link org.apache.lucene.index.LeafReader#getNumericDocValues(String)} 
  * or similar), it will create the docvalues on-the-fly if needed and cache it,
- * based on the core cache key of the wrapped AtomicReader.
+ * based on the core cache key of the wrapped LeafReader.
  */
 public class UninvertingReader extends FilterLeafReader {
   
diff --git a/lucene/misc/src/test/org/apache/lucene/index/sorter/SortingLeafReaderTest.java b/lucene/misc/src/test/org/apache/lucene/index/sorter/SortingLeafReaderTest.java
index b055a07..37d230b 100644
--- a/lucene/misc/src/test/org/apache/lucene/index/sorter/SortingLeafReaderTest.java
+++ b/lucene/misc/src/test/org/apache/lucene/index/sorter/SortingLeafReaderTest.java
@@ -29,7 +29,7 @@ import org.junit.BeforeClass;
 public class SortingLeafReaderTest extends SorterTestBase {
   
   @BeforeClass
-  public static void beforeClassSortingAtomicReaderTest() throws Exception {
+  public static void beforeClassSortingLeafReaderTest() throws Exception {
     
     // sort the index by id (as integer, in NUMERIC_DV_FIELD)
     Sort sort = new Sort(new SortField(NUMERIC_DV_FIELD, SortField.Type.INT));
diff --git a/lucene/queries/src/java/org/apache/lucene/queries/ChainedFilter.java b/lucene/queries/src/java/org/apache/lucene/queries/ChainedFilter.java
index 034af01..4f242b9 100644
--- a/lucene/queries/src/java/org/apache/lucene/queries/ChainedFilter.java
+++ b/lucene/queries/src/java/org/apache/lucene/queries/ChainedFilter.java
@@ -144,7 +144,7 @@ public class ChainedFilter extends Filter {
   /**
    * Delegates to each filter in the chain.
    *
-   * @param context AtomicReaderContext
+   * @param context LeafReaderContext
    * @param logic Logical operation
    * @return DocIdSet
    */
@@ -161,7 +161,7 @@ public class ChainedFilter extends Filter {
   /**
    * Delegates to each filter in the chain.
    *
-   * @param context AtomicReaderContext
+   * @param context LeafReaderContext
    * @param logic Logical operation
    * @return DocIdSet
    */
diff --git a/lucene/queries/src/java/org/apache/lucene/queries/function/FunctionValues.java b/lucene/queries/src/java/org/apache/lucene/queries/function/FunctionValues.java
index 85054c4..4b6dead 100644
--- a/lucene/queries/src/java/org/apache/lucene/queries/function/FunctionValues.java
+++ b/lucene/queries/src/java/org/apache/lucene/queries/function/FunctionValues.java
@@ -143,7 +143,7 @@ public abstract class FunctionValues {
   // A RangeValueSource can't easily be a ValueSource that takes another ValueSource
   // because it needs different behavior depending on the type of fields.  There is also
   // a setup cost - parsing and normalizing params, and doing a binary search on the StringIndex.
-  // TODO: change "reader" to AtomicReaderContext
+  // TODO: change "reader" to LeafReaderContext
   public ValueSourceScorer getRangeScorer(IndexReader reader, String lowerVal, String upperVal, boolean includeLower, boolean includeUpper) {
     float lower;
     float upper;
diff --git a/lucene/spatial/src/java/org/apache/lucene/spatial/prefix/AbstractVisitingPrefixTreeFilter.java b/lucene/spatial/src/java/org/apache/lucene/spatial/prefix/AbstractVisitingPrefixTreeFilter.java
index 9bc3f97..56e282a 100644
--- a/lucene/spatial/src/java/org/apache/lucene/spatial/prefix/AbstractVisitingPrefixTreeFilter.java
+++ b/lucene/spatial/src/java/org/apache/lucene/spatial/prefix/AbstractVisitingPrefixTreeFilter.java
@@ -79,7 +79,7 @@ public abstract class AbstractVisitingPrefixTreeFilter extends AbstractPrefixTre
   /**
    * An abstract class designed to make it easy to implement predicates or
    * other operations on a {@link SpatialPrefixTree} indexed field. An instance
-   * of this class is not designed to be re-used across AtomicReaderContext
+   * of this class is not designed to be re-used across LeafReaderContext
    * instances so simply create a new one for each call to, say a {@link
    * org.apache.lucene.search.Filter#getDocIdSet(org.apache.lucene.index.LeafReaderContext, org.apache.lucene.util.Bits)}.
    * The {@link #getDocIdSet()} method here starts the work. It first checks
diff --git a/lucene/test-framework/src/java/org/apache/lucene/index/FieldFilterLeafReader.java b/lucene/test-framework/src/java/org/apache/lucene/index/FieldFilterLeafReader.java
index 14e65f7..74f2936 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/index/FieldFilterLeafReader.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/index/FieldFilterLeafReader.java
@@ -154,7 +154,7 @@ public final class FieldFilterLeafReader extends FilterLeafReader {
 
   @Override
   public String toString() {
-    final StringBuilder sb = new StringBuilder("FieldFilterAtomicReader(reader=");
+    final StringBuilder sb = new StringBuilder("FieldFilterLeafReader(reader=");
     sb.append(in).append(", fields=");
     if (negate) sb.append('!');
     return sb.append(fields).append(')').toString();
diff --git a/lucene/test-framework/src/java/org/apache/lucene/util/LuceneTestCase.java b/lucene/test-framework/src/java/org/apache/lucene/util/LuceneTestCase.java
index eb179e3..a7d3c8b 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/util/LuceneTestCase.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/util/LuceneTestCase.java
@@ -1431,7 +1431,7 @@ public abstract class LuceneTestCase extends Assert {
             r = SlowCompositeReaderWrapper.wrap(r);
             break;
           case 1:
-            // will create no FC insanity in atomic case, as ParallelAtomicReader has own cache key:
+            // will create no FC insanity in atomic case, as ParallelLeafReader has own cache key:
             r = (r instanceof LeafReader) ?
               new ParallelLeafReader((LeafReader) r) :
               new ParallelCompositeReader((CompositeReader) r);
@@ -1451,7 +1451,7 @@ public abstract class LuceneTestCase extends Assert {
             Collections.shuffle(allFields, random);
             final int end = allFields.isEmpty() ? 0 : random.nextInt(allFields.size());
             final Set<String> fields = new HashSet<>(allFields.subList(0, end));
-            // will create no FC insanity as ParallelAtomicReader has own cache key:
+            // will create no FC insanity as ParallelLeafReader has own cache key:
             r = new ParallelLeafReader(
               new FieldFilterLeafReader(ar, fields, false),
               new FieldFilterLeafReader(ar, fields, true)
diff --git a/lucene/tools/junit4/cached-timehints.txt b/lucene/tools/junit4/cached-timehints.txt
index b892b2d..102028c 100644
--- a/lucene/tools/junit4/cached-timehints.txt
+++ b/lucene/tools/junit4/cached-timehints.txt
@@ -429,7 +429,7 @@ org.apache.lucene.index.TestDocumentsWriterStallControl=1910,1896,1537,543,2200,
 org.apache.lucene.index.TestDuelingCodecs=11794,3845,2763,3152,6520,2614,1843
 org.apache.lucene.index.TestFieldInfos=10,16,19,47,25,8,14
 org.apache.lucene.index.TestFieldsReader=942,3392,480,688,404,1603,388
-org.apache.lucene.index.TestFilterAtomicReader=30,41,268,248,20,28,134
+org.apache.lucene.index.TestFilterLeafReader=30,41,268,248,20,28,134
 org.apache.lucene.index.TestFlex=145,529,161,594,878,665,65
 org.apache.lucene.index.TestFlushByRamOrCountsPolicy=5259,3813,4027,3368,5559,1244,4965
 org.apache.lucene.index.TestForTooMuchCloning=201,234,378,440,92,327,119
@@ -476,7 +476,7 @@ org.apache.lucene.index.TestOmitNorms=379,1609,1493,414,651,1578,1018
 org.apache.lucene.index.TestOmitPositions=1178,558,273,362,619,373,83
 org.apache.lucene.index.TestOmitTf=503,300,1123,210,376,208,295
 org.apache.lucene.index.TestPKIndexSplitter=1876,1437,1779,1962,1522,1627,1592
-org.apache.lucene.index.TestParallelAtomicReader=373,981,97,186,567,351,258
+org.apache.lucene.index.TestParallelLeafReader=373,981,97,186,567,351,258
 org.apache.lucene.index.TestParallelCompositeReader=1119,934,1773,975,678,822,784
 org.apache.lucene.index.TestParallelReaderEmptyIndex=75,76,150,142,130,342,59
 org.apache.lucene.index.TestParallelTermEnum=84,12,12,66,67,106,120
diff --git a/solr/core/src/java/org/apache/solr/search/EarlyTerminatingCollectorException.java b/solr/core/src/java/org/apache/solr/search/EarlyTerminatingCollectorException.java
index 27d4d40..ed662ff 100644
--- a/solr/core/src/java/org/apache/solr/search/EarlyTerminatingCollectorException.java
+++ b/solr/core/src/java/org/apache/solr/search/EarlyTerminatingCollectorException.java
@@ -42,10 +42,10 @@ public class EarlyTerminatingCollectorException extends RuntimeException {
    * This number represents the sum of:
    * </p>
    * <ul>
-   *  <li>The total number of documents in all AtomicReaders
+   *  <li>The total number of documents in all LeafReaders
    *      that were fully exhausted during collection
    *  </li>
-   *  <li>The id of the last doc collected in the last AtomicReader
+   *  <li>The id of the last doc collected in the last LeafReader
    *      consulted during collection.
    *  </li>
    * </ul>
diff --git a/solr/core/src/test/org/apache/solr/core/TestMergePolicyConfig.java b/solr/core/src/test/org/apache/solr/core/TestMergePolicyConfig.java
index 068e26c..7e1ebec 100644
--- a/solr/core/src/test/org/apache/solr/core/TestMergePolicyConfig.java
+++ b/solr/core/src/test/org/apache/solr/core/TestMergePolicyConfig.java
@@ -199,7 +199,7 @@ public class TestMergePolicyConfig extends SolrTestCaseJ4 {
 
   /**
    * Given an IndexReader, asserts that there is at least one AtomcReader leaf,
-   * and that all AtomicReader leaves are SegmentReader's that have a compound 
+   * and that all LeafReader leaves are SegmentReader's that have a compound 
    * file status that matches the expected input.
    */
   private static void assertCompoundSegments(IndexReader reader, 

