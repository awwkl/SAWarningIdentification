GitDiffStart: 46e67d8f04be8d095e2897132f76d1ac84205249 | Sun Nov 17 12:28:18 2013 +0000
diff --git a/TODO b/TODO
index cc5b8af..f63cf13 100644
--- a/TODO
+++ b/TODO
@@ -2,6 +2,7 @@ nocommit this!
 
 TODO
   - associations
+  - simplify ddq api
   - SSDVValueSourceFacets?
   - we could put more stuff into the "schema", e.g. this field is
     sorted-set-DV and that one is taxo?
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/search/DrillSideways.java b/lucene/facet/src/java/org/apache/lucene/facet/search/DrillSideways.java
index b2654ce..661164f 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/search/DrillSideways.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/search/DrillSideways.java
@@ -180,7 +180,7 @@ public class DrillSideways {
       // Just do ordinary search when there are no drill-downs:
       FacetsCollector c = FacetsCollector.create(getDrillDownAccumulator(fsp));
       searcher.search(query, MultiCollector.wrap(hitCollector, c));
-      return new DrillSidewaysResult(c.getFacetResults(), null);      
+      return new DrillSidewaysResult(c.getFacetResults(), null);
     }
 
     List<FacetRequest> ddRequests = new ArrayList<FacetRequest>();
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/simple/MultiFacets.java b/lucene/facet/src/java/org/apache/lucene/facet/simple/MultiFacets.java
new file mode 100644
index 0000000..ca73edb
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/simple/MultiFacets.java
@@ -0,0 +1,56 @@
+package org.apache.lucene.facet.simple;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.List;
+import java.util.Map;
+
+/** Maps specified dims to provided Facets impls; else, uses
+ *  the default Facets impl. */
+public class MultiFacets extends Facets {
+  private final Map<String,Facets> dimToFacets;
+  private final Facets defaultFacets;
+
+  public MultiFacets(Map<String,Facets> dimToFacets, Facets defaultFacets) {
+    this.dimToFacets = dimToFacets;
+    this.defaultFacets = defaultFacets;
+  }
+
+  public SimpleFacetResult getTopChildren(int topN, String dim, String... path) throws IOException {
+    Facets facets = dimToFacets.get(dim);
+    if (facets == null) {
+      facets = defaultFacets;
+    }
+    return facets.getTopChildren(topN, dim, path);
+  }
+
+  public Number getSpecificValue(String dim, String... path) throws IOException {
+    Facets facets = dimToFacets.get(dim);
+    if (facets == null) {
+      facets = defaultFacets;
+    }
+    return facets.getSpecificValue(dim, path);
+  }
+
+  public List<SimpleFacetResult> getAllDims(int topN) throws IOException {
+    // nocommit can/should we impl this?  ie, sparse
+    // faceting after drill sideways
+    throw new UnsupportedOperationException();
+  }
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/simple/SimpleDrillDownQuery.java b/lucene/facet/src/java/org/apache/lucene/facet/simple/SimpleDrillDownQuery.java
index eb98893..cc0681c 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/simple/SimpleDrillDownQuery.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/simple/SimpleDrillDownQuery.java
@@ -118,17 +118,17 @@ public final class SimpleDrillDownQuery extends Query {
    * Adds one dimension of drill downs; if you pass multiple values they are
    * OR'd, and then the entire dimension is AND'd against the base query.
    */
-  // nocommit can we remove CatPath here?
+  // nocommit can we remove FacetLabel here?
   public void add(FacetLabel... paths) {
     add(FacetsConfig.DEFAULT_INDEXED_FIELD_NAME, Constants.DEFAULT_DELIM_CHAR, paths);
   }
 
-  // nocommit can we remove CatPath here?
+  // nocommit can we remove FacetLabel here?
   public void add(String field, FacetLabel... paths) {
     add(field, Constants.DEFAULT_DELIM_CHAR, paths);
   }
 
-  // nocommit can we remove CatPath here?
+  // nocommit can we remove FacetLabel here?
   public void add(String field, char delimChar, FacetLabel... paths) {
     Query q;
     if (paths[0].length == 0) {
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/simple/SimpleDrillSideways.java b/lucene/facet/src/java/org/apache/lucene/facet/simple/SimpleDrillSideways.java
new file mode 100644
index 0000000..edca6ca
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/simple/SimpleDrillSideways.java
@@ -0,0 +1,429 @@
+package org.apache.lucene.facet.simple;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.LinkedHashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.lucene.facet.index.FacetFields;
+import org.apache.lucene.facet.params.FacetSearchParams;
+import org.apache.lucene.facet.search.DrillDownQuery;
+import org.apache.lucene.facet.sortedset.SortedSetDocValuesFacetFields;
+import org.apache.lucene.facet.sortedset.SortedSetDocValuesReaderState;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.BooleanClause;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.Collector;
+import org.apache.lucene.search.ConstantScoreQuery;
+import org.apache.lucene.search.FieldDoc;
+import org.apache.lucene.search.Filter;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.search.MultiCollector;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.ScoreDoc;
+import org.apache.lucene.search.Sort;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.TopDocs;
+import org.apache.lucene.search.TopFieldCollector;
+import org.apache.lucene.search.TopScoreDocCollector;
+import org.apache.lucene.search.Weight;
+
+/**     
+ * Computes drill down and sideways counts for the provided
+ * {@link DrillDownQuery}.  Drill sideways counts include
+ * alternative values/aggregates for the drill-down
+ * dimensions so that a dimension does not disappear after
+ * the user drills down into it.
+ *
+ * <p> Use one of the static search
+ * methods to do the search, and then get the hits and facet
+ * results from the returned {@link DrillSidewaysResult}.
+ *
+ * <p><b>NOTE</b>: this allocates one {@link
+ * FacetsCollector} for each drill-down, plus one.  If your
+ * index has high number of facet labels then this will
+ * multiply your memory usage.
+ *
+ * @lucene.experimental
+ */
+
+public class SimpleDrillSideways {
+
+  protected final IndexSearcher searcher;
+  protected final TaxonomyReader taxoReader;
+  protected final SortedSetDocValuesReaderState state;
+  protected final FacetsConfig facetsConfig;
+
+  /**
+   * Create a new {@code DrillSideways} instance, assuming the categories were
+   * indexed with {@link FacetFields}.
+   */
+  public SimpleDrillSideways(IndexSearcher searcher, FacetsConfig facetsConfig, TaxonomyReader taxoReader) {
+    this(searcher, facetsConfig, taxoReader, null);
+  }
+    
+  /**
+   * Create a new {@code DrillSideways} instance, assuming the categories were
+   * indexed with {@link SortedSetDocValuesFacetFields}.
+   */
+  public SimpleDrillSideways(IndexSearcher searcher, FacetsConfig facetsConfig, SortedSetDocValuesReaderState state) {
+    this(searcher, facetsConfig, null, state);
+  }
+
+  /**
+   * Create a new {@code DrillSideways} instance, where some
+   * dimensions are sorted set facets and others are
+   * taxononmy facets.
+   */
+  public SimpleDrillSideways(IndexSearcher searcher, FacetsConfig facetsConfig, TaxonomyReader taxoReader, SortedSetDocValuesReaderState state) {
+    this.searcher = searcher;
+    this.facetsConfig = facetsConfig;
+    this.taxoReader = taxoReader;
+    this.state = state;
+  }
+
+  /** Subclass can override to customize per-dim Facets
+   *  impl. */
+  protected Facets buildFacetsResult(SimpleFacetsCollector drillDowns, SimpleFacetsCollector[] drillSideways, String[] drillSidewaysDims) throws IOException {
+
+    Facets drillDownFacets = new TaxonomyFacetCounts(taxoReader, facetsConfig, drillDowns);
+
+    if (drillSideways == null) {
+      return drillDownFacets;
+    } else {
+      Map<String,Facets> drillSidewaysFacets = new HashMap<String,Facets>();
+      for(int i=0;i<drillSideways.length;i++) {
+        drillSidewaysFacets.put(drillSidewaysDims[i],
+                                new TaxonomyFacetCounts(taxoReader, facetsConfig, drillSideways[i]));
+      }
+      return new MultiFacets(drillSidewaysFacets, drillDownFacets);
+    }
+  }
+
+  /**
+   * Search, collecting hits with a {@link Collector}, and
+   * computing drill down and sideways counts.
+   */
+  @SuppressWarnings({"rawtypes","unchecked"})
+  public SimpleDrillSidewaysResult search(SimpleDrillDownQuery query, Collector hitCollector) throws IOException {
+
+    Map<String,Integer> drillDownDims = query.getDims();
+
+    SimpleFacetsCollector drillDownCollector = new SimpleFacetsCollector();
+    
+    if (drillDownDims.isEmpty()) {
+      // There are no drill-down dims, so there is no
+      // drill-sideways to compute:
+      searcher.search(query, MultiCollector.wrap(hitCollector, drillDownCollector));
+      return new SimpleDrillSidewaysResult(buildFacetsResult(drillDownCollector, null, null), null);
+    }
+
+    BooleanQuery ddq = query.getBooleanQuery();
+    BooleanClause[] clauses = ddq.getClauses();
+
+    Query baseQuery;
+    int startClause;
+    if (clauses.length == drillDownDims.size()) {
+      // TODO: we could optimize this pure-browse case by
+      // making a custom scorer instead:
+      baseQuery = new MatchAllDocsQuery();
+      startClause = 0;
+    } else {
+      assert clauses.length == 1+drillDownDims.size();
+      baseQuery = clauses[0].getQuery();
+      startClause = 1;
+    }
+
+    SimpleFacetsCollector[] drillSidewaysCollectors = new SimpleFacetsCollector[drillDownDims.size()];
+
+    int idx = 0;
+    for(String dim : drillDownDims.keySet()) {
+      drillSidewaysCollectors[idx++] = new SimpleFacetsCollector();
+    }
+
+    boolean useCollectorMethod = scoreSubDocsAtOnce();
+
+    Term[][] drillDownTerms = null;
+
+    if (!useCollectorMethod) {
+      // Optimistic: assume subQueries of the DDQ are either
+      // TermQuery or BQ OR of TermQuery; if this is wrong
+      // then we detect it and fallback to the mome general
+      // but slower DrillSidewaysCollector:
+      drillDownTerms = new Term[clauses.length-startClause][];
+      for(int i=startClause;i<clauses.length;i++) {
+        Query q = clauses[i].getQuery();
+
+        // DrillDownQuery always wraps each subQuery in
+        // ConstantScoreQuery:
+        assert q instanceof ConstantScoreQuery;
+
+        q = ((ConstantScoreQuery) q).getQuery();
+
+        if (q instanceof TermQuery) {
+          drillDownTerms[i-startClause] = new Term[] {((TermQuery) q).getTerm()};
+        } else if (q instanceof BooleanQuery) {
+          BooleanQuery q2 = (BooleanQuery) q;
+          BooleanClause[] clauses2 = q2.getClauses();
+          drillDownTerms[i-startClause] = new Term[clauses2.length];
+          for(int j=0;j<clauses2.length;j++) {
+            if (clauses2[j].getQuery() instanceof TermQuery) {
+              drillDownTerms[i-startClause][j] = ((TermQuery) clauses2[j].getQuery()).getTerm();
+            } else {
+              useCollectorMethod = true;
+              break;
+            }
+          }
+        } else {
+          useCollectorMethod = true;
+        }
+      }
+    }
+
+    if (useCollectorMethod) {
+      // TODO: maybe we could push the "collector method"
+      // down into the optimized scorer to have a tighter
+      // integration ... and so TermQuery clauses could
+      // continue to run "optimized"
+      collectorMethod(query, baseQuery, startClause, hitCollector, drillDownCollector, drillSidewaysCollectors);
+    } else {
+      SimpleDrillSidewaysQuery dsq = new SimpleDrillSidewaysQuery(baseQuery, drillDownCollector, drillSidewaysCollectors, drillDownTerms);
+      searcher.search(dsq, hitCollector);
+    }
+
+    return new SimpleDrillSidewaysResult(buildFacetsResult(drillDownCollector, drillSidewaysCollectors, drillDownDims.keySet().toArray(new String[drillDownDims.size()])), null);
+  }
+
+  /** Uses the more general but slower method of sideways
+   *  counting. This method allows an arbitrary subQuery to
+   *  implement the drill down for a given dimension. */
+  private void collectorMethod(SimpleDrillDownQuery ddq, Query baseQuery, int startClause, Collector hitCollector, Collector drillDownCollector, Collector[] drillSidewaysCollectors) throws IOException {
+
+    BooleanClause[] clauses = ddq.getBooleanQuery().getClauses();
+
+    Map<String,Integer> drillDownDims = ddq.getDims();
+
+    BooleanQuery topQuery = new BooleanQuery(true);
+    final SimpleDrillSidewaysCollector collector = new SimpleDrillSidewaysCollector(hitCollector, drillDownCollector, drillSidewaysCollectors,
+                                                                                    drillDownDims);
+
+    // TODO: if query is already a BQ we could copy that and
+    // add clauses to it, instead of doing BQ inside BQ
+    // (should be more efficient)?  Problem is this can
+    // affect scoring (coord) ... too bad we can't disable
+    // coord on a clause by clause basis:
+    topQuery.add(baseQuery, BooleanClause.Occur.MUST);
+
+    // NOTE: in theory we could just make a single BQ, with
+    // +query a b c minShouldMatch=2, but in this case,
+    // annoyingly, BS2 wraps a sub-scorer that always
+    // returns 2 as the .freq(), not how many of the
+    // SHOULD clauses matched:
+    BooleanQuery subQuery = new BooleanQuery(true);
+
+    Query wrappedSubQuery = new QueryWrapper(subQuery,
+                                             new SetWeight() {
+                                               @Override
+                                               public void set(Weight w) {
+                                                 collector.setWeight(w, -1);
+                                               }
+                                             });
+    Query constantScoreSubQuery = new ConstantScoreQuery(wrappedSubQuery);
+
+    // Don't impact score of original query:
+    constantScoreSubQuery.setBoost(0.0f);
+
+    topQuery.add(constantScoreSubQuery, BooleanClause.Occur.MUST);
+
+    // Unfortunately this sub-BooleanQuery
+    // will never get BS1 because today BS1 only works
+    // if topScorer=true... and actually we cannot use BS1
+    // anyways because we need subDocsScoredAtOnce:
+    int dimIndex = 0;
+    for(int i=startClause;i<clauses.length;i++) {
+      Query q = clauses[i].getQuery();
+      // DrillDownQuery always wraps each subQuery in
+      // ConstantScoreQuery:
+      assert q instanceof ConstantScoreQuery;
+      q = ((ConstantScoreQuery) q).getQuery();
+
+      final int finalDimIndex = dimIndex;
+      subQuery.add(new QueryWrapper(q,
+                                    new SetWeight() {
+                                      @Override
+                                      public void set(Weight w) {
+                                        collector.setWeight(w, finalDimIndex);
+                                      }
+                                    }),
+                   BooleanClause.Occur.SHOULD);
+      dimIndex++;
+    }
+
+    // TODO: we could better optimize the "just one drill
+    // down" case w/ a separate [specialized]
+    // collector...
+    int minShouldMatch = drillDownDims.size()-1;
+    if (minShouldMatch == 0) {
+      // Must add another "fake" clause so BQ doesn't erase
+      // itself by rewriting to the single clause:
+      Query end = new MatchAllDocsQuery();
+      end.setBoost(0.0f);
+      subQuery.add(end, BooleanClause.Occur.SHOULD);
+      minShouldMatch++;
+    }
+
+    subQuery.setMinimumNumberShouldMatch(minShouldMatch);
+
+    // System.out.println("EXE " + topQuery);
+
+    // Collects against the passed-in
+    // drillDown/SidewaysCollectors as a side effect:
+    searcher.search(topQuery, collector);
+  }
+
+  /**
+   * Search, sorting by {@link Sort}, and computing
+   * drill down and sideways counts.
+   */
+  public SimpleDrillSidewaysResult search(SimpleDrillDownQuery query,
+                                          Filter filter, FieldDoc after, int topN, Sort sort, boolean doDocScores,
+                                          boolean doMaxScore) throws IOException {
+    if (filter != null) {
+      query = new SimpleDrillDownQuery(filter, query);
+    }
+    if (sort != null) {
+      int limit = searcher.getIndexReader().maxDoc();
+      if (limit == 0) {
+        limit = 1; // the collector does not alow numHits = 0
+      }
+      topN = Math.min(topN, limit);
+      final TopFieldCollector hitCollector = TopFieldCollector.create(sort,
+                                                                      topN,
+                                                                      after,
+                                                                      true,
+                                                                      doDocScores,
+                                                                      doMaxScore,
+                                                                      true);
+      SimpleDrillSidewaysResult r = search(query, hitCollector);
+      return new SimpleDrillSidewaysResult(r.facets, hitCollector.topDocs());
+    } else {
+      return search(after, query, topN);
+    }
+  }
+
+  /**
+   * Search, sorting by score, and computing
+   * drill down and sideways counts.
+   */
+  public SimpleDrillSidewaysResult search(ScoreDoc after,
+                                          SimpleDrillDownQuery query, int topN) throws IOException {
+    int limit = searcher.getIndexReader().maxDoc();
+    if (limit == 0) {
+      limit = 1; // the collector does not alow numHits = 0
+    }
+    topN = Math.min(topN, limit);
+    TopScoreDocCollector hitCollector = TopScoreDocCollector.create(topN, after, true);
+    SimpleDrillSidewaysResult r = search(query, hitCollector);
+    return new SimpleDrillSidewaysResult(r.facets, hitCollector.topDocs());
+  }
+
+  /** Override this and return true if your collector
+   *  (e.g., ToParentBlockJoinCollector) expects all
+   *  sub-scorers to be positioned on the document being
+   *  collected.  This will cause some performance loss;
+   *  default is false.  Note that if you return true from
+   *  this method (in a subclass) be sure your collector
+   *  also returns false from {@link
+   *  Collector#acceptsDocsOutOfOrder}: this will trick
+   *  BooleanQuery into also scoring all subDocs at once. */
+  protected boolean scoreSubDocsAtOnce() {
+    return false;
+  }
+
+  public static class SimpleDrillSidewaysResult {
+    /** Combined drill down & sideways results. */
+    public final Facets facets;
+
+    /** Hits. */
+    public final TopDocs hits;
+
+    public SimpleDrillSidewaysResult(Facets facets, TopDocs hits) {
+      this.facets = facets;
+      this.hits = hits;
+    }
+  }
+  private interface SetWeight {
+    public void set(Weight w);
+  }
+
+  /** Just records which Weight was given out for the
+   *  (possibly rewritten) Query. */
+  private static class QueryWrapper extends Query {
+    private final Query originalQuery;
+    private final SetWeight setter;
+
+    public QueryWrapper(Query originalQuery, SetWeight setter) {
+      this.originalQuery = originalQuery;
+      this.setter = setter;
+    }
+
+    @Override
+    public Weight createWeight(final IndexSearcher searcher) throws IOException {
+      Weight w = originalQuery.createWeight(searcher);
+      setter.set(w);
+      return w;
+    }
+
+    @Override
+    public Query rewrite(IndexReader reader) throws IOException {
+      Query rewritten = originalQuery.rewrite(reader);
+      if (rewritten != originalQuery) {
+        return new QueryWrapper(rewritten, setter);
+      } else {
+        return this;
+      }
+    }
+
+    @Override
+    public String toString(String s) {
+      return originalQuery.toString(s);
+    }
+
+    @Override
+    public boolean equals(Object o) {
+      if (!(o instanceof QueryWrapper)) return false;
+      final QueryWrapper other = (QueryWrapper) o;
+      return super.equals(o) && originalQuery.equals(other.originalQuery);
+    }
+
+    @Override
+    public int hashCode() {
+      return super.hashCode() * 31 + originalQuery.hashCode();
+    }
+  }
+}
+
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/simple/SimpleDrillSidewaysCollector.java b/lucene/facet/src/java/org/apache/lucene/facet/simple/SimpleDrillSidewaysCollector.java
new file mode 100644
index 0000000..c8421b4
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/simple/SimpleDrillSidewaysCollector.java
@@ -0,0 +1,188 @@
+package org.apache.lucene.facet.simple;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Arrays;
+import java.util.IdentityHashMap;
+import java.util.Map;
+
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.search.Collector;
+import org.apache.lucene.search.Scorer.ChildScorer;
+import org.apache.lucene.search.Scorer;
+import org.apache.lucene.search.Weight;
+
+/** Collector that scrutinizes each hit to determine if it
+ *  passed all constraints (a true hit) or if it missed
+ *  exactly one dimension (a near-miss, to count for
+ *  drill-sideways counts on that dimension). */
+class SimpleDrillSidewaysCollector extends Collector {
+
+  private final Collector hitCollector;
+  private final Collector drillDownCollector;
+  private final Collector[] drillSidewaysCollectors;
+  private final Scorer[] subScorers;
+  private final int exactCount;
+
+  // Maps Weight to either -1 (mainQuery) or to integer
+  // index of the dims drillDown.  We needs this when
+  // visiting the child scorers to correlate back to the
+  // right scorers:
+  private final Map<Weight,Integer> weightToIndex = new IdentityHashMap<Weight,Integer>();
+
+  private Scorer mainScorer;
+
+  public SimpleDrillSidewaysCollector(Collector hitCollector, Collector drillDownCollector, Collector[] drillSidewaysCollectors,
+                                      Map<String,Integer> dims) {
+    this.hitCollector = hitCollector;
+    this.drillDownCollector = drillDownCollector;
+    this.drillSidewaysCollectors = drillSidewaysCollectors;
+    subScorers = new Scorer[dims.size()];
+
+    if (dims.size() == 1) {
+      // When we have only one dim, we insert the
+      // MatchAllDocsQuery, bringing the clause count to
+      // 2:
+      exactCount = 2;
+    } else {
+      exactCount = dims.size();
+    }
+  }
+
+  @Override
+  public void collect(int doc) throws IOException {
+    //System.out.println("collect doc=" + doc + " main.freq=" + mainScorer.freq() + " main.doc=" + mainScorer.docID() + " exactCount=" + exactCount);
+      
+    if (mainScorer == null) {
+      // This segment did not have any docs with any
+      // drill-down field & value:
+      return;
+    }
+
+    if (mainScorer.freq() == exactCount) {
+      // All sub-clauses from the drill-down filters
+      // matched, so this is a "real" hit, so we first
+      // collect in both the hitCollector and the
+      // drillDown collector:
+      //System.out.println("  hit " + drillDownCollector);
+      hitCollector.collect(doc);
+      if (drillDownCollector != null) {
+        drillDownCollector.collect(doc);
+      }
+
+      // Also collect across all drill-sideways counts so
+      // we "merge in" drill-down counts for this
+      // dimension.
+      for(int i=0;i<subScorers.length;i++) {
+        // This cannot be null, because it was a hit,
+        // meaning all drill-down dims matched, so all
+        // dims must have non-null scorers:
+        assert subScorers[i] != null;
+        int subDoc = subScorers[i].docID();
+        assert subDoc == doc;
+        drillSidewaysCollectors[i].collect(doc);
+      }
+
+    } else {
+      boolean found = false;
+      for(int i=0;i<subScorers.length;i++) {
+        if (subScorers[i] == null) {
+          // This segment did not have any docs with this
+          // drill-down field & value:
+          drillSidewaysCollectors[i].collect(doc);
+          assert allMatchesFrom(i+1, doc);
+          found = true;
+          break;
+        }
+        int subDoc = subScorers[i].docID();
+        //System.out.println("  i=" + i + " sub: " + subDoc);
+        if (subDoc != doc) {
+          //System.out.println("  +ds[" + i + "]");
+          assert subDoc > doc: "subDoc=" + subDoc + " doc=" + doc;
+          drillSidewaysCollectors[i].collect(doc);
+          assert allMatchesFrom(i+1, doc);
+          found = true;
+          break;
+        }
+      }
+      assert found;
+    }
+  }
+
+  // Only used by assert:
+  private boolean allMatchesFrom(int startFrom, int doc) {
+    for(int i=startFrom;i<subScorers.length;i++) {
+      assert subScorers[i].docID() == doc;
+    }
+    return true;
+  }
+
+  @Override
+  public boolean acceptsDocsOutOfOrder() {
+    // We actually could accept docs out of order, but, we
+    // need to force BooleanScorer2 so that the
+    // sub-scorers are "on" each docID we are collecting:
+    return false;
+  }
+
+  @Override
+  public void setNextReader(AtomicReaderContext leaf) throws IOException {
+    //System.out.println("DS.setNextReader reader=" + leaf.reader());
+    hitCollector.setNextReader(leaf);
+    if (drillDownCollector != null) {
+      drillDownCollector.setNextReader(leaf);
+    }
+    for(Collector dsc : drillSidewaysCollectors) {
+      dsc.setNextReader(leaf);
+    }
+  }
+
+  void setWeight(Weight weight, int index) {
+    assert !weightToIndex.containsKey(weight);
+    weightToIndex.put(weight, index);
+  }
+
+  private void findScorers(Scorer scorer) {
+    Integer index = weightToIndex.get(scorer.getWeight());
+    if (index != null) {
+      if (index.intValue() == -1) {
+        mainScorer = scorer;
+      } else {
+        subScorers[index] = scorer;
+      }
+    }
+    for(ChildScorer child : scorer.getChildren()) {
+      findScorers(child.child);
+    }
+  }
+
+  @Override
+  public void setScorer(Scorer scorer) throws IOException {
+    mainScorer = null;
+    Arrays.fill(subScorers, null);
+    findScorers(scorer);
+    hitCollector.setScorer(scorer);
+    if (drillDownCollector != null) {
+      drillDownCollector.setScorer(scorer);
+    }
+    for(Collector dsc : drillSidewaysCollectors) {
+      dsc.setScorer(scorer);
+    }
+  }
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/simple/SimpleDrillSidewaysQuery.java b/lucene/facet/src/java/org/apache/lucene/facet/simple/SimpleDrillSidewaysQuery.java
new file mode 100644
index 0000000..34a6813
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/simple/SimpleDrillSidewaysQuery.java
@@ -0,0 +1,198 @@
+package org.apache.lucene.facet.simple;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+import java.io.IOException;
+import java.util.Arrays;
+
+import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.index.Terms;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.search.Collector;
+import org.apache.lucene.search.Explanation;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.Scorer;
+import org.apache.lucene.search.Weight;
+import org.apache.lucene.util.Bits;
+
+/** Only purpose is to punch through and return a
+ *  SimpleDrillSidewaysScorer */ 
+
+class SimpleDrillSidewaysQuery extends Query {
+  final Query baseQuery;
+  final Collector drillDownCollector;
+  final Collector[] drillSidewaysCollectors;
+  final Term[][] drillDownTerms;
+
+  SimpleDrillSidewaysQuery(Query baseQuery, Collector drillDownCollector, Collector[] drillSidewaysCollectors, Term[][] drillDownTerms) {
+    this.baseQuery = baseQuery;
+    this.drillDownCollector = drillDownCollector;
+    this.drillSidewaysCollectors = drillSidewaysCollectors;
+    this.drillDownTerms = drillDownTerms;
+  }
+
+  @Override
+  public String toString(String field) {
+    return "DrillSidewaysQuery";
+  }
+
+  @Override
+  public Query rewrite(IndexReader reader) throws IOException {
+    Query newQuery = baseQuery;
+    while(true) {
+      Query rewrittenQuery = newQuery.rewrite(reader);
+      if (rewrittenQuery == newQuery) {
+        break;
+      }
+      newQuery = rewrittenQuery;
+    }
+    if (newQuery == baseQuery) {
+      return this;
+    } else {
+      return new SimpleDrillSidewaysQuery(newQuery, drillDownCollector, drillSidewaysCollectors, drillDownTerms);
+    }
+  }
+  
+  @Override
+  public Weight createWeight(IndexSearcher searcher) throws IOException {
+    final Weight baseWeight = baseQuery.createWeight(searcher);
+
+    return new Weight() {
+      @Override
+      public Explanation explain(AtomicReaderContext context, int doc) throws IOException {
+        return baseWeight.explain(context, doc);
+      }
+
+      @Override
+      public Query getQuery() {
+        return baseQuery;
+      }
+
+      @Override
+      public float getValueForNormalization() throws IOException {
+        return baseWeight.getValueForNormalization();
+      }
+
+      @Override
+      public void normalize(float norm, float topLevelBoost) {
+        baseWeight.normalize(norm, topLevelBoost);
+      }
+
+      @Override
+      public boolean scoresDocsOutOfOrder() {
+        // TODO: would be nice if AssertingIndexSearcher
+        // confirmed this for us
+        return false;
+      }
+
+      @Override
+      public Scorer scorer(AtomicReaderContext context, boolean scoreDocsInOrder,
+                           boolean topScorer, Bits acceptDocs) throws IOException {
+
+        SimpleDrillSidewaysScorer.DocsEnumsAndFreq[] dims = new SimpleDrillSidewaysScorer.DocsEnumsAndFreq[drillDownTerms.length];
+        TermsEnum termsEnum = null;
+        String lastField = null;
+        int nullCount = 0;
+        for(int dim=0;dim<dims.length;dim++) {
+          dims[dim] = new SimpleDrillSidewaysScorer.DocsEnumsAndFreq();
+          dims[dim].sidewaysCollector = drillSidewaysCollectors[dim];
+          String field = drillDownTerms[dim][0].field();
+          dims[dim].dim = drillDownTerms[dim][0].text();
+          if (lastField == null || !lastField.equals(field)) {
+            AtomicReader reader = context.reader();
+            Terms terms = reader.terms(field);
+            if (terms != null) {
+              termsEnum = terms.iterator(null);
+            } else {
+              termsEnum = null;
+            }
+            lastField = field;
+          }
+          dims[dim].docsEnums = new DocsEnum[drillDownTerms[dim].length];
+          if (termsEnum == null) {
+            nullCount++;
+            continue;
+          }
+          for(int i=0;i<drillDownTerms[dim].length;i++) {
+            if (termsEnum.seekExact(drillDownTerms[dim][i].bytes())) {
+              DocsEnum docsEnum = termsEnum.docs(null, null, 0);
+              if (docsEnum != null) {
+                dims[dim].docsEnums[i] = docsEnum;
+                dims[dim].maxCost = Math.max(dims[dim].maxCost, docsEnum.cost());
+              }
+            }
+          }
+        }
+
+        if (nullCount > 1 || (nullCount == 1 && dims.length == 1)) {
+          return null;
+        }
+
+        // Sort drill-downs by most restrictive first:
+        Arrays.sort(dims);
+
+        // TODO: it could be better if we take acceptDocs
+        // into account instead of baseScorer?
+        Scorer baseScorer = baseWeight.scorer(context, scoreDocsInOrder, false, acceptDocs);
+
+        if (baseScorer == null) {
+          return null;
+        }
+
+        return new SimpleDrillSidewaysScorer(this, context,
+                                             baseScorer,
+                                             drillDownCollector, dims);
+      }
+    };
+  }
+
+  // TODO: these should do "deeper" equals/hash on the 2-D drillDownTerms array
+
+  @Override
+  public int hashCode() {
+    final int prime = 31;
+    int result = super.hashCode();
+    result = prime * result + ((baseQuery == null) ? 0 : baseQuery.hashCode());
+    result = prime * result
+        + ((drillDownCollector == null) ? 0 : drillDownCollector.hashCode());
+    result = prime * result + Arrays.hashCode(drillDownTerms);
+    result = prime * result + Arrays.hashCode(drillSidewaysCollectors);
+    return result;
+  }
+
+  @Override
+  public boolean equals(Object obj) {
+    if (this == obj) return true;
+    if (!super.equals(obj)) return false;
+    if (getClass() != obj.getClass()) return false;
+    SimpleDrillSidewaysQuery other = (SimpleDrillSidewaysQuery) obj;
+    if (baseQuery == null) {
+      if (other.baseQuery != null) return false;
+    } else if (!baseQuery.equals(other.baseQuery)) return false;
+    if (drillDownCollector == null) {
+      if (other.drillDownCollector != null) return false;
+    } else if (!drillDownCollector.equals(other.drillDownCollector)) return false;
+    if (!Arrays.equals(drillDownTerms, other.drillDownTerms)) return false;
+    if (!Arrays.equals(drillSidewaysCollectors, other.drillSidewaysCollectors)) return false;
+    return true;
+  }
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/simple/SimpleDrillSidewaysScorer.java b/lucene/facet/src/java/org/apache/lucene/facet/simple/SimpleDrillSidewaysScorer.java
new file mode 100644
index 0000000..6be41af
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/simple/SimpleDrillSidewaysScorer.java
@@ -0,0 +1,654 @@
+package org.apache.lucene.facet.simple;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Collection;
+import java.util.Collections;
+
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.search.Collector;
+import org.apache.lucene.search.Scorer;
+import org.apache.lucene.search.Weight;
+import org.apache.lucene.util.FixedBitSet;
+
+class SimpleDrillSidewaysScorer extends Scorer {
+
+  //private static boolean DEBUG = false;
+
+  private final Collector drillDownCollector;
+
+  private final DocsEnumsAndFreq[] dims;
+
+  // DrillDown DocsEnums:
+  private final Scorer baseScorer;
+
+  private final AtomicReaderContext context;
+
+  private static final int CHUNK = 2048;
+  private static final int MASK = CHUNK-1;
+
+  private int collectDocID = -1;
+  private float collectScore;
+
+  SimpleDrillSidewaysScorer(Weight w, AtomicReaderContext context, Scorer baseScorer, Collector drillDownCollector,
+                            DocsEnumsAndFreq[] dims) {
+    super(w);
+    this.dims = dims;
+    this.context = context;
+    this.baseScorer = baseScorer;
+    this.drillDownCollector = drillDownCollector;
+  }
+
+  @Override
+  public void score(Collector collector) throws IOException {
+    //if (DEBUG) {
+    //  System.out.println("\nscore: reader=" + context.reader());
+    //}
+    //System.out.println("score r=" + context.reader());
+    collector.setScorer(this);
+    if (drillDownCollector != null) {
+      drillDownCollector.setScorer(this);
+      drillDownCollector.setNextReader(context);
+    }
+    for(DocsEnumsAndFreq dim : dims) {
+      dim.sidewaysCollector.setScorer(this);
+      dim.sidewaysCollector.setNextReader(context);
+    }
+
+    // TODO: if we ever allow null baseScorer ... it will
+    // mean we DO score docs out of order ... hmm, or if we
+    // change up the order of the conjuntions below
+    assert baseScorer != null;
+
+    // Position all scorers to their first matching doc:
+    baseScorer.nextDoc();
+    for(DocsEnumsAndFreq dim : dims) {
+      for (DocsEnum docsEnum : dim.docsEnums) {
+        if (docsEnum != null) {
+          docsEnum.nextDoc();
+        }
+      }
+    }
+
+    final int numDims = dims.length;
+
+    DocsEnum[][] docsEnums = new DocsEnum[numDims][];
+    Collector[] sidewaysCollectors = new Collector[numDims];
+    long drillDownCost = 0;
+    for(int dim=0;dim<numDims;dim++) {
+      docsEnums[dim] = dims[dim].docsEnums;
+      sidewaysCollectors[dim] = dims[dim].sidewaysCollector;
+      for (DocsEnum de : dims[dim].docsEnums) {
+        if (de != null) {
+          drillDownCost += de.cost();
+        }
+      }
+    }
+
+    long baseQueryCost = baseScorer.cost();
+
+    /*
+    System.out.println("\nbaseDocID=" + baseScorer.docID() + " est=" + estBaseHitCount);
+    System.out.println("  maxDoc=" + context.reader().maxDoc());
+    System.out.println("  maxCost=" + maxCost);
+    System.out.println("  dims[0].freq=" + dims[0].freq);
+    if (numDims > 1) {
+      System.out.println("  dims[1].freq=" + dims[1].freq);
+    }
+    */
+
+    if (baseQueryCost < drillDownCost/10) {
+      //System.out.println("baseAdvance");
+      doBaseAdvanceScoring(collector, docsEnums, sidewaysCollectors);
+    } else if (numDims > 1 && (dims[1].maxCost < baseQueryCost/10)) {
+      //System.out.println("drillDownAdvance");
+      doDrillDownAdvanceScoring(collector, docsEnums, sidewaysCollectors);
+    } else {
+      //System.out.println("union");
+      doUnionScoring(collector, docsEnums, sidewaysCollectors);
+    }
+  }
+
+  /** Used when drill downs are highly constraining vs
+   *  baseQuery. */
+  private void doDrillDownAdvanceScoring(Collector collector, DocsEnum[][] docsEnums, Collector[] sidewaysCollectors) throws IOException {
+    final int maxDoc = context.reader().maxDoc();
+    final int numDims = dims.length;
+
+    //if (DEBUG) {
+    //  System.out.println("  doDrillDownAdvanceScoring");
+    //}
+
+    // TODO: maybe a class like BS, instead of parallel arrays
+    int[] filledSlots = new int[CHUNK];
+    int[] docIDs = new int[CHUNK];
+    float[] scores = new float[CHUNK];
+    int[] missingDims = new int[CHUNK];
+    int[] counts = new int[CHUNK];
+
+    docIDs[0] = -1;
+    int nextChunkStart = CHUNK;
+
+    final FixedBitSet seen = new FixedBitSet(CHUNK);
+
+    while (true) {
+      //if (DEBUG) {
+      //  System.out.println("\ncycle nextChunkStart=" + nextChunkStart + " docIds[0]=" + docIDs[0]);
+      //}
+
+      // First dim:
+      //if (DEBUG) {
+      //  System.out.println("  dim0");
+      //}
+      for(DocsEnum docsEnum : docsEnums[0]) {
+        if (docsEnum == null) {
+          continue;
+        }
+        int docID = docsEnum.docID();
+        while (docID < nextChunkStart) {
+          int slot = docID & MASK;
+
+          if (docIDs[slot] != docID) {
+            seen.set(slot);
+            // Mark slot as valid:
+            //if (DEBUG) {
+            //  System.out.println("    set docID=" + docID + " id=" + context.reader().document(docID).get("id"));
+            //}
+            docIDs[slot] = docID;
+            missingDims[slot] = 1;
+            counts[slot] = 1;
+          }
+
+          docID = docsEnum.nextDoc();
+        }
+      }
+
+      // Second dim:
+      //if (DEBUG) {
+      //  System.out.println("  dim1");
+      //}
+      for(DocsEnum docsEnum : docsEnums[1]) {
+        if (docsEnum == null) {
+          continue;
+        }
+        int docID = docsEnum.docID();
+        while (docID < nextChunkStart) {
+          int slot = docID & MASK;
+
+          if (docIDs[slot] != docID) {
+            // Mark slot as valid:
+            seen.set(slot);
+            //if (DEBUG) {
+            //  System.out.println("    set docID=" + docID + " missingDim=0 id=" + context.reader().document(docID).get("id"));
+            //}
+            docIDs[slot] = docID;
+            missingDims[slot] = 0;
+            counts[slot] = 1;
+          } else {
+            // TODO: single-valued dims will always be true
+            // below; we could somehow specialize
+            if (missingDims[slot] >= 1) {
+              missingDims[slot] = 2;
+              counts[slot] = 2;
+              //if (DEBUG) {
+              //  System.out.println("    set docID=" + docID + " missingDim=2 id=" + context.reader().document(docID).get("id"));
+              //}
+            } else {
+              counts[slot] = 1;
+              //if (DEBUG) {
+              //  System.out.println("    set docID=" + docID + " missingDim=" + missingDims[slot] + " id=" + context.reader().document(docID).get("id"));
+              //}
+            }
+          }
+
+          docID = docsEnum.nextDoc();
+        }
+      }
+
+      // After this we can "upgrade" to conjunction, because
+      // any doc not seen by either dim 0 or dim 1 cannot be
+      // a hit or a near miss:
+
+      //if (DEBUG) {
+      //  System.out.println("  baseScorer");
+      //}
+
+      // Fold in baseScorer, using advance:
+      int filledCount = 0;
+      int slot0 = 0;
+      while (slot0 < CHUNK && (slot0 = seen.nextSetBit(slot0)) != -1) {
+        int ddDocID = docIDs[slot0];
+        assert ddDocID != -1;
+
+        int baseDocID = baseScorer.docID();
+        if (baseDocID < ddDocID) {
+          baseDocID = baseScorer.advance(ddDocID);
+        }
+        if (baseDocID == ddDocID) {
+          //if (DEBUG) {
+          //  System.out.println("    keep docID=" + ddDocID + " id=" + context.reader().document(ddDocID).get("id"));
+          //}
+          scores[slot0] = baseScorer.score();
+          filledSlots[filledCount++] = slot0;
+          counts[slot0]++;
+        } else {
+          //if (DEBUG) {
+          //  System.out.println("    no docID=" + ddDocID + " id=" + context.reader().document(ddDocID).get("id"));
+          //}
+          docIDs[slot0] = -1;
+
+          // TODO: we could jump slot0 forward to the
+          // baseDocID ... but we'd need to set docIDs for
+          // intervening slots to -1
+        }
+        slot0++;
+      }
+      seen.clear(0, CHUNK);
+
+      if (filledCount == 0) {
+        if (nextChunkStart >= maxDoc) {
+          break;
+        }
+        nextChunkStart += CHUNK;
+        continue;
+      }
+      
+      // TODO: factor this out & share w/ union scorer,
+      // except we start from dim=2 instead:
+      for(int dim=2;dim<numDims;dim++) {
+        //if (DEBUG) {
+        //  System.out.println("  dim=" + dim + " [" + dims[dim].dim + "]");
+        //}
+        for(DocsEnum docsEnum : docsEnums[dim]) {
+          if (docsEnum == null) {
+            continue;
+          }
+          int docID = docsEnum.docID();
+          while (docID < nextChunkStart) {
+            int slot = docID & MASK;
+            if (docIDs[slot] == docID && counts[slot] >= dim) {
+              // TODO: single-valued dims will always be true
+              // below; we could somehow specialize
+              if (missingDims[slot] >= dim) {
+                //if (DEBUG) {
+                //  System.out.println("    set docID=" + docID + " count=" + (dim+2));
+                //}
+                missingDims[slot] = dim+1;
+                counts[slot] = dim+2;
+              } else {
+                //if (DEBUG) {
+                //  System.out.println("    set docID=" + docID + " missing count=" + (dim+1));
+                //}
+                counts[slot] = dim+1;
+              }
+            }
+            // TODO: sometimes use advance?
+            docID = docsEnum.nextDoc();
+          }
+        }
+      }
+
+      // Collect:
+      //if (DEBUG) {
+      //  System.out.println("  now collect: " + filledCount + " hits");
+      //}
+      for(int i=0;i<filledCount;i++) {
+        int slot = filledSlots[i];
+        collectDocID = docIDs[slot];
+        collectScore = scores[slot];
+        //if (DEBUG) {
+        //  System.out.println("    docID=" + docIDs[slot] + " count=" + counts[slot]);
+        //}
+        if (counts[slot] == 1+numDims) {
+          collectHit(collector, sidewaysCollectors);
+        } else if (counts[slot] == numDims) {
+          collectNearMiss(sidewaysCollectors, missingDims[slot]);
+        }
+      }
+
+      if (nextChunkStart >= maxDoc) {
+        break;
+      }
+
+      nextChunkStart += CHUNK;
+    }
+  }
+
+  /** Used when base query is highly constraining vs the
+   *  drilldowns; in this case we just .next() on base and
+   *  .advance() on the dims. */
+  private void doBaseAdvanceScoring(Collector collector, DocsEnum[][] docsEnums, Collector[] sidewaysCollectors) throws IOException {
+    //if (DEBUG) {
+    //  System.out.println("  doBaseAdvanceScoring");
+    //}
+    int docID = baseScorer.docID();
+
+    final int numDims = dims.length;
+
+    nextDoc: while (docID != NO_MORE_DOCS) {
+      int failedDim = -1;
+      for(int dim=0;dim<numDims;dim++) {
+        // TODO: should we sort this 2nd dimension of
+        // docsEnums from most frequent to least?
+        boolean found = false;
+        for(DocsEnum docsEnum : docsEnums[dim]) {
+          if (docsEnum == null) {
+            continue;
+          }
+          if (docsEnum.docID() < docID) {
+            docsEnum.advance(docID);
+          }
+          if (docsEnum.docID() == docID) {
+            found = true;
+            break;
+          }
+        }
+        if (!found) {
+          if (failedDim != -1) {
+            // More than one dim fails on this document, so
+            // it's neither a hit nor a near-miss; move to
+            // next doc:
+            docID = baseScorer.nextDoc();
+            continue nextDoc;
+          } else {
+            failedDim = dim;
+          }
+        }
+      }
+
+      collectDocID = docID;
+
+      // TODO: we could score on demand instead since we are
+      // daat here:
+      collectScore = baseScorer.score();
+
+      if (failedDim == -1) {
+        collectHit(collector, sidewaysCollectors);
+      } else {
+        collectNearMiss(sidewaysCollectors, failedDim);
+      }
+
+      docID = baseScorer.nextDoc();
+    }
+  }
+
+  private void collectHit(Collector collector, Collector[] sidewaysCollectors) throws IOException {
+    //if (DEBUG) {
+    //  System.out.println("      hit");
+    //}
+
+    collector.collect(collectDocID);
+    if (drillDownCollector != null) {
+      drillDownCollector.collect(collectDocID);
+    }
+
+    // TODO: we could "fix" faceting of the sideways counts
+    // to do this "union" (of the drill down hits) in the
+    // end instead:
+
+    // Tally sideways counts:
+    for(int dim=0;dim<sidewaysCollectors.length;dim++) {
+      sidewaysCollectors[dim].collect(collectDocID);
+    }
+  }
+
+  private void collectNearMiss(Collector[] sidewaysCollectors, int dim) throws IOException {
+    //if (DEBUG) {
+    //  System.out.println("      missingDim=" + dim);
+    //}
+    sidewaysCollectors[dim].collect(collectDocID);
+  }
+
+  private void doUnionScoring(Collector collector, DocsEnum[][] docsEnums, Collector[] sidewaysCollectors) throws IOException {
+    //if (DEBUG) {
+    //  System.out.println("  doUnionScoring");
+    //}
+
+    final int maxDoc = context.reader().maxDoc();
+    final int numDims = dims.length;
+
+    // TODO: maybe a class like BS, instead of parallel arrays
+    int[] filledSlots = new int[CHUNK];
+    int[] docIDs = new int[CHUNK];
+    float[] scores = new float[CHUNK];
+    int[] missingDims = new int[CHUNK];
+    int[] counts = new int[CHUNK];
+
+    docIDs[0] = -1;
+
+    // NOTE: this is basically a specialized version of
+    // BooleanScorer, to the minShouldMatch=N-1 case, but
+    // carefully tracking which dimension failed to match
+
+    int nextChunkStart = CHUNK;
+
+    while (true) {
+      //if (DEBUG) {
+      //  System.out.println("\ncycle nextChunkStart=" + nextChunkStart + " docIds[0]=" + docIDs[0]);
+      //}
+      int filledCount = 0;
+      int docID = baseScorer.docID();
+      //if (DEBUG) {
+      //  System.out.println("  base docID=" + docID);
+      //}
+      while (docID < nextChunkStart) {
+        int slot = docID & MASK;
+        //if (DEBUG) {
+        //  System.out.println("    docIDs[slot=" + slot + "]=" + docID + " id=" + context.reader().document(docID).get("id"));
+        //}
+
+        // Mark slot as valid:
+        assert docIDs[slot] != docID: "slot=" + slot + " docID=" + docID;
+        docIDs[slot] = docID;
+        scores[slot] = baseScorer.score();
+        filledSlots[filledCount++] = slot;
+        missingDims[slot] = 0;
+        counts[slot] = 1;
+
+        docID = baseScorer.nextDoc();
+      }
+
+      if (filledCount == 0) {
+        if (nextChunkStart >= maxDoc) {
+          break;
+        }
+        nextChunkStart += CHUNK;
+        continue;
+      }
+
+      // First drill-down dim, basically adds SHOULD onto
+      // the baseQuery:
+      //if (DEBUG) {
+      //  System.out.println("  dim=0 [" + dims[0].dim + "]");
+      //}
+      for(DocsEnum docsEnum : docsEnums[0]) {
+        if (docsEnum == null) {
+          continue;
+        }
+        docID = docsEnum.docID();
+        //if (DEBUG) {
+        //  System.out.println("    start docID=" + docID);
+        //}
+        while (docID < nextChunkStart) {
+          int slot = docID & MASK;
+          if (docIDs[slot] == docID) {
+            //if (DEBUG) {
+            //  System.out.println("      set docID=" + docID + " count=2");
+            //}
+            missingDims[slot] = 1;
+            counts[slot] = 2;
+          }
+          docID = docsEnum.nextDoc();
+        }
+      }
+
+      for(int dim=1;dim<numDims;dim++) {
+        //if (DEBUG) {
+        //  System.out.println("  dim=" + dim + " [" + dims[dim].dim + "]");
+        //}
+        for(DocsEnum docsEnum : docsEnums[dim]) {
+          if (docsEnum == null) {
+            continue;
+          }
+          docID = docsEnum.docID();
+          //if (DEBUG) {
+          //  System.out.println("    start docID=" + docID);
+          //}
+          while (docID < nextChunkStart) {
+            int slot = docID & MASK;
+            if (docIDs[slot] == docID && counts[slot] >= dim) {
+              // This doc is still in the running...
+              // TODO: single-valued dims will always be true
+              // below; we could somehow specialize
+              if (missingDims[slot] >= dim) {
+                //if (DEBUG) {
+                //  System.out.println("      set docID=" + docID + " count=" + (dim+2));
+                //}
+                missingDims[slot] = dim+1;
+                counts[slot] = dim+2;
+              } else {
+                //if (DEBUG) {
+                //  System.out.println("      set docID=" + docID + " missing count=" + (dim+1));
+                //}
+                counts[slot] = dim+1;
+              }
+            }
+            docID = docsEnum.nextDoc();
+          }
+
+          // TODO: sometimes use advance?
+
+          /*
+            int docBase = nextChunkStart - CHUNK;
+            for(int i=0;i<filledCount;i++) {
+              int slot = filledSlots[i];
+              docID = docBase + filledSlots[i];
+              if (docIDs[slot] == docID && counts[slot] >= dim) {
+                // This doc is still in the running...
+                int ddDocID = docsEnum.docID();
+                if (ddDocID < docID) {
+                  ddDocID = docsEnum.advance(docID);
+                }
+                if (ddDocID == docID) {
+                  if (missingDims[slot] >= dim && counts[slot] == allMatchCount) {
+                  //if (DEBUG) {
+                  //    System.out.println("    set docID=" + docID + " count=" + (dim+2));
+                   // }
+                    missingDims[slot] = dim+1;
+                    counts[slot] = dim+2;
+                  } else {
+                  //if (DEBUG) {
+                  //    System.out.println("    set docID=" + docID + " missing count=" + (dim+1));
+                   // }
+                    counts[slot] = dim+1;
+                  }
+                }
+              }
+            }            
+          */
+        }
+      }
+
+      // Collect:
+      //if (DEBUG) {
+      //  System.out.println("  now collect: " + filledCount + " hits");
+      //}
+      for(int i=0;i<filledCount;i++) {
+        // NOTE: This is actually in-order collection,
+        // because we only accept docs originally returned by
+        // the baseScorer (ie that Scorer is AND'd)
+        int slot = filledSlots[i];
+        collectDocID = docIDs[slot];
+        collectScore = scores[slot];
+        //if (DEBUG) {
+        //  System.out.println("    docID=" + docIDs[slot] + " count=" + counts[slot]);
+        //}
+        //System.out.println("  collect doc=" + collectDocID + " main.freq=" + (counts[slot]-1) + " main.doc=" + collectDocID + " exactCount=" + numDims);
+        if (counts[slot] == 1+numDims) {
+          //System.out.println("    hit");
+          collectHit(collector, sidewaysCollectors);
+        } else if (counts[slot] == numDims) {
+          //System.out.println("    sw");
+          collectNearMiss(sidewaysCollectors, missingDims[slot]);
+        }
+      }
+
+      if (nextChunkStart >= maxDoc) {
+        break;
+      }
+
+      nextChunkStart += CHUNK;
+    }
+  }
+
+  @Override
+  public int docID() {
+    return collectDocID;
+  }
+
+  @Override
+  public float score() {
+    return collectScore;
+  }
+
+  @Override
+  public int freq() {
+    return 1+dims.length;
+  }
+
+  @Override
+  public int nextDoc() {
+    throw new UnsupportedOperationException();
+  }
+
+  @Override
+  public int advance(int target) {
+    throw new UnsupportedOperationException();
+  }
+
+  @Override
+  public long cost() {
+    return baseScorer.cost();
+  }
+
+  @Override
+  public Collection<ChildScorer> getChildren() {
+    return Collections.singletonList(new ChildScorer(baseScorer, "MUST"));
+  }
+
+  static class DocsEnumsAndFreq implements Comparable<DocsEnumsAndFreq> {
+    DocsEnum[] docsEnums;
+    // Max cost for all docsEnums for this dim:
+    long maxCost;
+    Collector sidewaysCollector;
+    String dim;
+
+    @Override
+    public int compareTo(DocsEnumsAndFreq other) {
+      if (maxCost < other.maxCost) {
+        return -1;
+      } else if (maxCost > other.maxCost) {
+        return 1;
+      } else {
+        return 0;
+      }
+    }
+  }
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/simple/SimpleFacetResult.java b/lucene/facet/src/java/org/apache/lucene/facet/simple/SimpleFacetResult.java
index 28cedba..4be86b7 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/simple/SimpleFacetResult.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/simple/SimpleFacetResult.java
@@ -31,6 +31,8 @@ public final class SimpleFacetResult {
 
   /** Child counts. */
   public final LabelAndValue[] labelValues;
+
+  // nocommit also return number of children?
   
   public SimpleFacetResult(FacetLabel path, Number value, LabelAndValue[] labelValues) {
     this.path = path;
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/simple/TestSimpleDrillSideways.java b/lucene/facet/src/test/org/apache/lucene/facet/simple/TestSimpleDrillSideways.java
new file mode 100644
index 0000000..c803329
--- /dev/null
+++ b/lucene/facet/src/test/org/apache/lucene/facet/simple/TestSimpleDrillSideways.java
@@ -0,0 +1,1152 @@
+package org.apache.lucene.facet.simple;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.StringField;
+import org.apache.lucene.facet.FacetTestCase;
+import org.apache.lucene.facet.FacetTestUtils;
+import org.apache.lucene.facet.index.FacetFields;
+import org.apache.lucene.facet.params.FacetIndexingParams;
+import org.apache.lucene.facet.params.FacetSearchParams;
+import org.apache.lucene.facet.simple.SimpleDrillSideways.SimpleDrillSidewaysResult;
+import org.apache.lucene.facet.sortedset.SortedSetDocValuesFacetFields;
+import org.apache.lucene.facet.sortedset.SortedSetDocValuesReaderState;
+import org.apache.lucene.facet.taxonomy.FacetLabel;
+import org.apache.lucene.facet.taxonomy.TaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;
+import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.Collector;
+import org.apache.lucene.search.DocIdSet;
+import org.apache.lucene.search.Filter;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.MatchAllDocsQuery;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.ScoreDoc;
+import org.apache.lucene.search.Scorer;
+import org.apache.lucene.search.Sort;
+import org.apache.lucene.search.SortField.Type;
+import org.apache.lucene.search.SortField;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.TopDocs;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.FixedBitSet;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.InPlaceMergeSorter;
+import org.apache.lucene.util.InfoStream;
+import org.apache.lucene.util._TestUtil;
+import org.junit.Test;
+
+public class TestSimpleDrillSideways extends FacetTestCase {
+
+  private DirectoryTaxonomyWriter taxoWriter;
+  private RandomIndexWriter writer;
+  private FacetFields facetFields;
+
+  public void testBasic() throws Exception {
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+
+    // Writes facet ords to a separate directory from the
+    // main index:
+    taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+
+    FacetsConfig config = new FacetsConfig();
+    config.setHierarchical("Publish Date");
+
+    IndexWriter writer = new FacetIndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())), taxoWriter, config);
+
+    Document doc = new Document();
+    doc.add(new FacetField("Author", "Bob"));
+    doc.add(new FacetField("Publish Date", "2010", "10", "15"));
+    writer.addDocument(doc);
+
+    doc = new Document();
+    doc.add(new FacetField("Author", "Lisa"));
+    doc.add(new FacetField("Publish Date", "2010", "10", "20"));
+    writer.addDocument(doc);
+
+    doc = new Document();
+    doc.add(new FacetField("Author", "Lisa"));
+    doc.add(new FacetField("Publish Date", "2012", "1", "1"));
+    writer.addDocument(doc);
+
+    doc = new Document();
+    doc.add(new FacetField("Author", "Susan"));
+    doc.add(new FacetField("Publish Date", "2012", "1", "7"));
+    writer.addDocument(doc);
+
+    doc = new Document();
+    doc.add(new FacetField("Author", "Frank"));
+    doc.add(new FacetField("Publish Date", "1999", "5", "5"));
+    writer.addDocument(doc);
+
+    // NRT open
+    IndexSearcher searcher = newSearcher(DirectoryReader.open(writer, true));
+    writer.close();
+
+    //System.out.println("searcher=" + searcher);
+
+    // NRT open
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+    taxoWriter.close();
+
+    SimpleDrillSideways ds = new SimpleDrillSideways(searcher, config, taxoReader);
+
+    // Simple case: drill-down on a single field; in this
+    // case the drill-sideways + drill-down counts ==
+    // drill-down of just the query: 
+    SimpleDrillDownQuery ddq = new SimpleDrillDownQuery();
+    ddq.add(new FacetLabel("Author", "Lisa"));
+    SimpleDrillSidewaysResult r = ds.search(null, ddq, 10);
+    assertEquals(2, r.hits.totalHits);
+    // Publish Date is only drill-down, and Lisa published
+    // one in 2012 and one in 2010:
+    assertEquals("Publish Date (2)\n  2010 (1)\n  2012 (1)\n", r.facets.getTopChildren(10, "Publish Date").toString());
+
+    // Author is drill-sideways + drill-down: Lisa
+    // (drill-down) published twice, and Frank/Susan/Bob
+    // published once:
+    assertEquals("Author (5)\n  Lisa (2)\n  Bob (1)\n  Susan (1)\n  Frank (1)\n", r.facets.getTopChildren(10, "Author").toString());
+
+    // Same simple case, but no baseQuery (pure browse):
+    // drill-down on a single field; in this case the
+    // drill-sideways + drill-down counts == drill-down of
+    // just the query:
+    ddq = new SimpleDrillDownQuery();
+    ddq.add(new FacetLabel("Author", "Lisa"));
+    r = ds.search(null, ddq, 10);
+
+    assertEquals(2, r.hits.totalHits);
+    // Publish Date is only drill-down, and Lisa published
+    // one in 2012 and one in 2010:
+    assertEquals("Publish Date (2)\n  2010 (1)\n  2012 (1)\n", r.facets.getTopChildren(10, "Publish Date").toString());
+
+    // Author is drill-sideways + drill-down: Lisa
+    // (drill-down) published twice, and Frank/Susan/Bob
+    // published once:
+    assertEquals("Author (5)\n  Lisa (2)\n  Bob (1)\n  Susan (1)\n  Frank (1)\n", r.facets.getTopChildren(10, "Author").toString());
+
+    // Another simple case: drill-down on on single fields
+    // but OR of two values
+    ddq = new SimpleDrillDownQuery();
+    ddq.add(new FacetLabel("Author", "Lisa"), new FacetLabel("Author", "Bob"));
+    r = ds.search(null, ddq, 10);
+    assertEquals(3, r.hits.totalHits);
+    // Publish Date is only drill-down: Lisa and Bob
+    // (drill-down) published twice in 2010 and once in 2012:
+    assertEquals("Publish Date (3)\n  2010 (2)\n  2012 (1)\n", r.facets.getTopChildren(10, "Publish Date").toString());
+    // Author is drill-sideways + drill-down: Lisa
+    // (drill-down) published twice, and Frank/Susan/Bob
+    // published once:
+    assertEquals("Author (5)\n  Lisa (2)\n  Bob (1)\n  Susan (1)\n  Frank (1)\n", r.facets.getTopChildren(10, "Author").toString());
+
+    // More interesting case: drill-down on two fields
+    ddq = new SimpleDrillDownQuery();
+    ddq.add(new FacetLabel("Author", "Lisa"));
+    ddq.add(new FacetLabel("Publish Date", "2010"));
+    r = ds.search(null, ddq, 10);
+    assertEquals(1, r.hits.totalHits);
+    // Publish Date is drill-sideways + drill-down: Lisa
+    // (drill-down) published once in 2010 and once in 2012:
+    assertEquals("Publish Date (2)\n  2010 (1)\n  2012 (1)\n", r.facets.getTopChildren(10, "Publish Date").toString());
+    // Author is drill-sideways + drill-down:
+    // only Lisa & Bob published (once each) in 2010:
+    assertEquals("Author (2)\n  Bob (1)\n  Lisa (1)\n", r.facets.getTopChildren(10, "Author").toString());
+
+    // Even more interesting case: drill down on two fields,
+    // but one of them is OR
+    ddq = new SimpleDrillDownQuery();
+
+    // Drill down on Lisa or Bob:
+    ddq.add(new FacetLabel("Author", "Lisa"),
+            new FacetLabel("Author", "Bob"));
+    ddq.add(new FacetLabel("Publish Date", "2010"));
+    r = ds.search(null, ddq, 10);
+    assertEquals(2, r.hits.totalHits);
+    // Publish Date is both drill-sideways + drill-down:
+    // Lisa or Bob published twice in 2010 and once in 2012:
+    assertEquals("Publish Date (3)\n  2010 (2)\n  2012 (1)\n", r.facets.getTopChildren(10, "Publish Date").toString());
+    // Author is drill-sideways + drill-down:
+    // only Lisa & Bob published (once each) in 2010:
+    assertEquals("Author (2)\n  Bob (1)\n  Lisa (1)\n", r.facets.getTopChildren(10, "Author").toString());
+
+    // Test drilling down on invalid field:
+    ddq = new SimpleDrillDownQuery();
+    ddq.add(new FacetLabel("Foobar", "Baz"));
+    r = ds.search(null, ddq, 10);
+    assertEquals(0, r.hits.totalHits);
+    assertNull(r.facets.getTopChildren(10, "Publish Date"));
+    assertNull(r.facets.getTopChildren(10, "Foobar"));
+
+    // Test drilling down on valid term or'd with invalid term:
+    ddq = new SimpleDrillDownQuery();
+    ddq.add(new FacetLabel("Author", "Lisa"),
+            new FacetLabel("Author", "Tom"));
+    r = ds.search(null, ddq, 10);
+    assertEquals(2, r.hits.totalHits);
+    // Publish Date is only drill-down, and Lisa published
+    // one in 2012 and one in 2010:
+    assertEquals("Publish Date (2)\n  2010 (1)\n  2012 (1)\n", r.facets.getTopChildren(10, "Publish Date").toString());
+    // Author is drill-sideways + drill-down: Lisa
+    // (drill-down) published twice, and Frank/Susan/Bob
+    // published once:
+    assertEquals("Author (5)\n  Lisa (2)\n  Bob (1)\n  Susan (1)\n  Frank (1)\n", r.facets.getTopChildren(10, "Author").toString());
+
+    // LUCENE-4915: test drilling down on a dimension but
+    // NOT facet counting it:
+    ddq = new SimpleDrillDownQuery();
+    ddq.add(new FacetLabel("Author", "Lisa"),
+            new FacetLabel("Author", "Tom"));
+    r = ds.search(null, ddq, 10);
+    assertEquals(2, r.hits.totalHits);
+    // Publish Date is only drill-down, and Lisa published
+    // one in 2012 and one in 2010:
+    assertEquals("Publish Date (2)\n  2010 (1)\n  2012 (1)\n", r.facets.getTopChildren(10, "Publish Date").toString());
+
+    // Test main query gets null scorer:
+    ddq = new SimpleDrillDownQuery(new TermQuery(new Term("foobar", "baz")));
+    ddq.add(new FacetLabel("Author", "Lisa"));
+    r = ds.search(null, ddq, 10);
+
+    assertEquals(0, r.hits.totalHits);
+    assertNull(r.facets.getTopChildren(10, "Publish Date"));
+    assertNull(r.facets.getTopChildren(10, "Author"));
+    searcher.getIndexReader().close();
+    taxoReader.close();
+    dir.close();
+    taxoDir.close();
+  }
+
+  /*
+  public void testSometimesInvalidDrillDown() throws Exception {
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+    writer = new RandomIndexWriter(random(), dir);
+
+    // Writes facet ords to a separate directory from the
+    // main index:
+    taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+
+    // Reused across documents, to add the necessary facet
+    // fields:
+    facetFields = new FacetFields(taxoWriter);
+
+    add("Author/Bob", "Publish Date/2010/10/15");
+    add("Author/Lisa", "Publish Date/2010/10/20");
+    writer.commit();
+    // 2nd segment has no Author:
+    add("Foobar/Lisa", "Publish Date/2012/1/1");
+
+    // NRT open
+    IndexSearcher searcher = newSearcher(writer.getReader());
+    writer.close();
+
+    //System.out.println("searcher=" + searcher);
+
+    // NRT open
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+    taxoWriter.close();
+
+    // Count both "Publish Date" and "Author" dimensions, in
+    // drill-down:
+    FacetSearchParams fsp = new FacetSearchParams(
+        new CountFacetRequest(new FacetLabel("Publish Date"), 10), 
+        new CountFacetRequest(new FacetLabel("Author"), 10));
+
+    DrillDownQuery ddq = new DrillDownQuery(fsp.indexingParams, new MatchAllDocsQuery());
+    ddq.add(new FacetLabel("Author", "Lisa"));
+    DrillSidewaysResult r = new DrillSideways(searcher, taxoReader).search(null, ddq, 10, fsp);
+
+    assertEquals(1, r.hits.totalHits);
+    assertEquals(2, r.facetResults.size());
+    // Publish Date is only drill-down, and Lisa published
+    // one in 2012 and one in 2010:
+    assertEquals("Publish Date: 2010=1", toString(r.facetResults.get(0)));
+    // Author is drill-sideways + drill-down: Lisa
+    // (drill-down) published once, and Bob
+    // published once:
+    assertEquals("Author: Lisa=1 Bob=1", toString(r.facetResults.get(1)));
+
+    searcher.getIndexReader().close();
+    taxoReader.close();
+    dir.close();
+    taxoDir.close();
+  }
+
+  private static class Doc implements Comparable<Doc> {
+    String id;
+    String contentToken;
+
+    public Doc() {}
+    
+    // -1 if the doc is missing this dim, else the index
+    // -into the values for this dim:
+    int[] dims;
+
+    // 2nd value per dim for the doc (so we test
+    // multi-valued fields):
+    int[] dims2;
+    boolean deleted;
+
+    @Override
+    public int compareTo(Doc other) {
+      return id.compareTo(other.id);
+    }
+  }
+
+  private double aChance, bChance, cChance;
+
+  private String randomContentToken(boolean isQuery) {
+    double d = random().nextDouble();
+    if (isQuery) {
+      if (d < 0.33) {
+        return "a";
+      } else if (d < 0.66) {
+        return "b";
+      } else {
+        return "c";
+      }
+    } else {
+      if (d <= aChance) {
+        return "a";
+      } else if (d < aChance + bChance) {
+        return "b";
+      } else {
+        return "c";
+      }
+    }
+  }
+
+  public void testMultipleRequestsPerDim() throws Exception {
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+    writer = new RandomIndexWriter(random(), dir);
+
+    // Writes facet ords to a separate directory from the
+    // main index:
+    taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+
+    // Reused across documents, to add the necessary facet
+    // fields:
+    facetFields = new FacetFields(taxoWriter);
+
+    add("dim/a/x");
+    add("dim/a/y");
+    add("dim/a/z");
+    add("dim/b");
+    add("dim/c");
+    add("dim/d");
+
+    // NRT open
+    IndexSearcher searcher = newSearcher(writer.getReader());
+    writer.close();
+
+    //System.out.println("searcher=" + searcher);
+
+    // NRT open
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+    taxoWriter.close();
+
+    // Two requests against the same dim:
+    FacetSearchParams fsp = new FacetSearchParams(
+        new CountFacetRequest(new FacetLabel("dim"), 10), 
+        new CountFacetRequest(new FacetLabel("dim", "a"), 10));
+
+    DrillDownQuery ddq = new DrillDownQuery(fsp.indexingParams, new MatchAllDocsQuery());
+    ddq.add(new FacetLabel("dim", "a"));
+    DrillSidewaysResult r = new DrillSideways(searcher, taxoReader).search(null, ddq, 10, fsp);
+
+    assertEquals(3, r.hits.totalHits);
+    assertEquals(2, r.facetResults.size());
+    // Publish Date is only drill-down, and Lisa published
+    // one in 2012 and one in 2010:
+    assertEquals("dim: a=3 d=1 c=1 b=1", toString(r.facetResults.get(0)));
+    // Author is drill-sideways + drill-down: Lisa
+    // (drill-down) published twice, and Frank/Susan/Bob
+    // published once:
+    assertEquals("a (3)\n  z (1)\n  y (1)\n  x (1)\n", FacetTestUtils.toSimpleString(r.facetResults.get(1)));
+
+    searcher.getIndexReader().close();
+    taxoReader.close();
+    dir.close();
+    taxoDir.close();
+  }
+
+  public void testRandom() throws Exception {
+
+    boolean canUseDV = defaultCodecSupportsSortedSet();
+
+    while (aChance == 0.0) {
+      aChance = random().nextDouble();
+    }
+    while (bChance == 0.0) {
+      bChance = random().nextDouble();
+    }
+    while (cChance == 0.0) {
+      cChance = random().nextDouble();
+    }
+    //aChance = .01;
+    //bChance = 0.5;
+    //cChance = 1.0;
+    double sum = aChance + bChance + cChance;
+    aChance /= sum;
+    bChance /= sum;
+    cChance /= sum;
+
+    int numDims = _TestUtil.nextInt(random(), 2, 5);
+    //int numDims = 3;
+    int numDocs = atLeast(3000);
+    //int numDocs = 20;
+    if (VERBOSE) {
+      System.out.println("numDims=" + numDims + " numDocs=" + numDocs + " aChance=" + aChance + " bChance=" + bChance + " cChance=" + cChance);
+    }
+    String[][] dimValues = new String[numDims][];
+    int valueCount = 2;
+
+    for(int dim=0;dim<numDims;dim++) {
+      Set<String> values = new HashSet<String>();
+      while (values.size() < valueCount) {
+        String s;
+        while (true) {
+          s = _TestUtil.randomRealisticUnicodeString(random());
+          //s = _TestUtil.randomSimpleString(random());
+          // We cannot include this character else we hit
+          // IllegalArgExc: 
+          if (s.indexOf(FacetIndexingParams.DEFAULT_FACET_DELIM_CHAR) == -1 &&
+              (!canUseDV || s.indexOf('/') == -1)) {
+            break;
+          }
+        }
+        if (s.length() > 0) {
+          values.add(s);
+        }
+      } 
+      dimValues[dim] = values.toArray(new String[values.size()]);
+      valueCount *= 2;
+    }
+
+    List<Doc> docs = new ArrayList<Doc>();
+    for(int i=0;i<numDocs;i++) {
+      Doc doc = new Doc();
+      doc.id = ""+i;
+      doc.contentToken = randomContentToken(false);
+      doc.dims = new int[numDims];
+      doc.dims2 = new int[numDims];
+      for(int dim=0;dim<numDims;dim++) {
+        if (random().nextInt(5) == 3) {
+          // This doc is missing this dim:
+          doc.dims[dim] = -1;
+        } else if (dimValues[dim].length <= 4) {
+          int dimUpto = 0;
+          doc.dims[dim] = dimValues[dim].length-1;
+          while (dimUpto < dimValues[dim].length) {
+            if (random().nextBoolean()) {
+              doc.dims[dim] = dimUpto;
+              break;
+            }
+            dimUpto++;
+          }
+        } else {
+          doc.dims[dim] = random().nextInt(dimValues[dim].length);
+        }
+
+        if (random().nextInt(5) == 3) {
+          // 2nd value:
+          doc.dims2[dim] = random().nextInt(dimValues[dim].length);
+        } else {
+          doc.dims2[dim] = -1;
+        }
+      }
+      docs.add(doc);
+    }
+
+    Directory d = newDirectory();
+    Directory td = newDirectory();
+
+    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
+    iwc.setInfoStream(InfoStream.NO_OUTPUT);
+    RandomIndexWriter w = new RandomIndexWriter(random(), d, iwc);
+    DirectoryTaxonomyWriter tw = new DirectoryTaxonomyWriter(td, IndexWriterConfig.OpenMode.CREATE);
+    facetFields = new FacetFields(tw);
+    SortedSetDocValuesFacetFields dvFacetFields = new SortedSetDocValuesFacetFields();
+
+    boolean doUseDV = canUseDV && random().nextBoolean();
+
+    for(Doc rawDoc : docs) {
+      Document doc = new Document();
+      doc.add(newStringField("id", rawDoc.id, Field.Store.YES));
+      doc.add(newStringField("content", rawDoc.contentToken, Field.Store.NO));
+      List<FacetLabel> paths = new ArrayList<FacetLabel>();
+
+      if (VERBOSE) {
+        System.out.println("  doc id=" + rawDoc.id + " token=" + rawDoc.contentToken);
+      }
+      for(int dim=0;dim<numDims;dim++) {
+        int dimValue = rawDoc.dims[dim];
+        if (dimValue != -1) {
+          FacetLabel cp = new FacetLabel("dim" + dim, dimValues[dim][dimValue]);
+          paths.add(cp);
+          doc.add(new StringField("dim" + dim, dimValues[dim][dimValue], Field.Store.YES));
+          if (VERBOSE) {
+            System.out.println("    dim" + dim + "=" + new BytesRef(dimValues[dim][dimValue]));
+          }
+        }
+        int dimValue2 = rawDoc.dims2[dim];
+        if (dimValue2 != -1) {
+          FacetLabel cp = new FacetLabel("dim" + dim, dimValues[dim][dimValue2]);
+          paths.add(cp);
+          doc.add(new StringField("dim" + dim, dimValues[dim][dimValue2], Field.Store.YES));
+          if (VERBOSE) {
+            System.out.println("      dim" + dim + "=" + new BytesRef(dimValues[dim][dimValue2]));
+          }
+        }
+      }
+      if (!paths.isEmpty()) {
+        if (doUseDV) {
+          dvFacetFields.addFields(doc, paths);
+        } else {
+          facetFields.addFields(doc, paths);
+        }
+      }
+
+      w.addDocument(doc);
+    }
+
+    if (random().nextBoolean()) {
+      // Randomly delete a few docs:
+      int numDel = _TestUtil.nextInt(random(), 1, (int) (numDocs*0.05));
+      if (VERBOSE) {
+        System.out.println("delete " + numDel);
+      }
+      int delCount = 0;
+      while (delCount < numDel) {
+        Doc doc = docs.get(random().nextInt(docs.size()));
+        if (!doc.deleted) {
+          if (VERBOSE) {
+            System.out.println("  delete id=" + doc.id);
+          }
+          doc.deleted = true;
+          w.deleteDocuments(new Term("id", doc.id));
+          delCount++;
+        }
+      }
+    }
+
+    if (random().nextBoolean()) {
+      if (VERBOSE) {
+        System.out.println("TEST: forceMerge(1)...");
+      }
+      w.forceMerge(1);
+    }
+    IndexReader r = w.getReader();
+    w.close();
+
+    final SortedSetDocValuesReaderState sortedSetDVState;
+    IndexSearcher s = newSearcher(r);
+    if (doUseDV) {
+      sortedSetDVState = new SortedSetDocValuesReaderState(s.getIndexReader());
+    } else {
+      sortedSetDVState = null;
+    }
+
+    if (VERBOSE) {
+      System.out.println("r.numDocs() = " + r.numDocs());
+    }
+
+    // NRT open
+    TaxonomyReader tr = new DirectoryTaxonomyReader(tw);
+    tw.close();
+
+    int numIters = atLeast(10);
+
+    for(int iter=0;iter<numIters;iter++) {
+
+      String contentToken = random().nextInt(30) == 17 ? null : randomContentToken(true);
+      int numDrillDown = _TestUtil.nextInt(random(), 1, Math.min(4, numDims));
+      if (VERBOSE) {
+        System.out.println("\nTEST: iter=" + iter + " baseQuery=" + contentToken + " numDrillDown=" + numDrillDown + " useSortedSetDV=" + doUseDV);
+      }
+
+      List<FacetRequest> requests = new ArrayList<FacetRequest>();
+      while(true) {
+        for(int i=0;i<numDims;i++) {
+          // LUCENE-4915: sometimes don't request facet
+          // counts on the dim(s) we drill down on
+          if (random().nextDouble() <= 0.9) {
+            if (VERBOSE) {
+              System.out.println("  do facet request on dim=" + i);
+            }
+            requests.add(new CountFacetRequest(new FacetLabel("dim" + i), dimValues[numDims-1].length));
+          } else {
+            if (VERBOSE) {
+              System.out.println("  skip facet request on dim=" + i);
+            }
+          }
+        }
+        if (!requests.isEmpty()) {
+          break;
+        }
+      }
+      FacetSearchParams fsp = new FacetSearchParams(requests);
+      String[][] drillDowns = new String[numDims][];
+
+      int count = 0;
+      boolean anyMultiValuedDrillDowns = false;
+      while (count < numDrillDown) {
+        int dim = random().nextInt(numDims);
+        if (drillDowns[dim] == null) {
+          if (random().nextBoolean()) {
+            // Drill down on one value:
+            drillDowns[dim] = new String[] {dimValues[dim][random().nextInt(dimValues[dim].length)]};
+          } else {
+            int orCount = _TestUtil.nextInt(random(), 1, Math.min(5, dimValues[dim].length));
+            drillDowns[dim] = new String[orCount];
+            anyMultiValuedDrillDowns |= orCount > 1;
+            for(int i=0;i<orCount;i++) {
+              while (true) {
+                String value = dimValues[dim][random().nextInt(dimValues[dim].length)];
+                for(int j=0;j<i;j++) {
+                  if (value.equals(drillDowns[dim][j])) {
+                    value = null;
+                    break;
+                  }
+                }
+                if (value != null) {
+                  drillDowns[dim][i] = value;
+                  break;
+                }
+              }
+            }
+          }
+          if (VERBOSE) {
+            BytesRef[] values = new BytesRef[drillDowns[dim].length];
+            for(int i=0;i<values.length;i++) {
+              values[i] = new BytesRef(drillDowns[dim][i]);
+            }
+            System.out.println("  dim" + dim + "=" + Arrays.toString(values));
+          }
+          count++;
+        }
+      }
+
+      Query baseQuery;
+      if (contentToken == null) {
+        baseQuery = new MatchAllDocsQuery();
+      } else {
+        baseQuery = new TermQuery(new Term("content", contentToken));
+      }
+
+      DrillDownQuery ddq = new DrillDownQuery(fsp.indexingParams, baseQuery);
+
+      for(int dim=0;dim<numDims;dim++) {
+        if (drillDowns[dim] != null) {
+          FacetLabel[] paths = new FacetLabel[drillDowns[dim].length];
+          int upto = 0;
+          for(String value : drillDowns[dim]) {
+            paths[upto++] = new FacetLabel("dim" + dim, value);
+          }
+          ddq.add(paths);
+        }
+      }
+
+      Filter filter;
+      if (random().nextInt(7) == 6) {
+        if (VERBOSE) {
+          System.out.println("  only-even filter");
+        }
+        filter = new Filter() {
+            @Override
+            public DocIdSet getDocIdSet(AtomicReaderContext context, Bits acceptDocs) throws IOException {
+              int maxDoc = context.reader().maxDoc();
+              final FixedBitSet bits = new FixedBitSet(maxDoc);
+              for(int docID=0;docID < maxDoc;docID++) {
+                // Keeps only the even ids:
+                if ((acceptDocs == null || acceptDocs.get(docID)) && ((Integer.parseInt(context.reader().document(docID).get("id")) & 1) == 0)) {
+                  bits.set(docID);
+                }
+              }
+              return bits;
+            }
+          };
+      } else {
+        filter = null;
+      }
+
+      // Verify docs are always collected in order.  If we
+      // had an AssertingScorer it could catch it when
+      // Weight.scoresDocsOutOfOrder lies!:
+      new DrillSideways(s, tr).search(ddq,
+                           new Collector() {
+                             int lastDocID;
+
+                             @Override
+                             public void setScorer(Scorer s) {
+                             }
+
+                             @Override
+                             public void collect(int doc) {
+                               assert doc > lastDocID;
+                               lastDocID = doc;
+                             }
+
+                             @Override
+                             public void setNextReader(AtomicReaderContext context) {
+                               lastDocID = -1;
+                             }
+
+                             @Override
+                             public boolean acceptsDocsOutOfOrder() {
+                               return false;
+                             }
+                           }, fsp);
+
+      // Also separately verify that DS respects the
+      // scoreSubDocsAtOnce method, to ensure that all
+      // subScorers are on the same docID:
+      if (!anyMultiValuedDrillDowns) {
+        // Can only do this test when there are no OR'd
+        // drill-down values, beacuse in that case it's
+        // easily possible for one of the DD terms to be on
+        // a future docID:
+        new DrillSideways(s, tr) {
+          @Override
+          protected boolean scoreSubDocsAtOnce() {
+            return true;
+          }
+        }.search(ddq, new AssertingSubDocsAtOnceCollector(), fsp);
+      }
+
+      SimpleFacetResult expected = slowDrillSidewaysSearch(s, requests, docs, contentToken, drillDowns, dimValues, filter);
+
+      Sort sort = new Sort(new SortField("id", SortField.Type.STRING));
+      DrillSideways ds;
+      if (doUseDV) {
+        ds = new DrillSideways(s, sortedSetDVState);
+      } else {
+        ds = new DrillSideways(s, tr);
+      }
+
+      // Retrieve all facets:
+      DrillSidewaysResult actual = ds.search(ddq, filter, null, numDocs, sort, true, true, fsp);
+
+      TopDocs hits = s.search(baseQuery, numDocs);
+      Map<String,Float> scores = new HashMap<String,Float>();
+      for(ScoreDoc sd : hits.scoreDocs) {
+        scores.put(s.doc(sd.doc).get("id"), sd.score);
+      }
+      if (VERBOSE) {
+        System.out.println("  verify all facets");
+      }
+      verifyEquals(requests, dimValues, s, expected, actual, scores, -1, doUseDV);
+
+      // Retrieve topN facets:
+      int topN = _TestUtil.nextInt(random(), 1, 20);
+
+      List<FacetRequest> newRequests = new ArrayList<FacetRequest>();
+      for(FacetRequest oldRequest : requests) {
+        newRequests.add(new CountFacetRequest(oldRequest.categoryPath, topN));
+      }
+      fsp = new FacetSearchParams(newRequests);
+      actual = ds.search(ddq, filter, null, numDocs, sort, true, true, fsp);
+      if (VERBOSE) {
+        System.out.println("  verify topN=" + topN);
+      }
+      verifyEquals(newRequests, dimValues, s, expected, actual, scores, topN, doUseDV);
+
+      // Make sure drill down doesn't change score:
+      TopDocs ddqHits = s.search(ddq, filter, numDocs);
+      assertEquals(expected.hits.size(), ddqHits.totalHits);
+      for(int i=0;i<expected.hits.size();i++) {
+        // Score should be IDENTICAL:
+        assertEquals(scores.get(expected.hits.get(i).id), ddqHits.scoreDocs[i].score, 0.0f);
+      }
+    }
+
+    tr.close();
+    r.close();
+    td.close();
+    d.close();
+  }
+
+  private static class Counters {
+    int[][] counts;
+
+    public Counters(String[][] dimValues) {
+      counts = new int[dimValues.length][];
+      for(int dim=0;dim<dimValues.length;dim++) {
+        counts[dim] = new int[dimValues[dim].length];
+      }
+    }
+
+    public void inc(int[] dims, int[] dims2) {
+      inc(dims, dims2, -1);
+    }
+
+    public void inc(int[] dims, int[] dims2, int onlyDim) {
+      assert dims.length == counts.length;
+      assert dims2.length == counts.length;
+      for(int dim=0;dim<dims.length;dim++) {
+        if (onlyDim == -1 || dim == onlyDim) {
+          if (dims[dim] != -1) {
+            counts[dim][dims[dim]]++;
+          }
+          if (dims2[dim] != -1 && dims2[dim] != dims[dim]) {
+            counts[dim][dims2[dim]]++;
+          }
+        }
+      }
+    }
+  }
+
+  private static class SimpleFacetResult {
+    List<Doc> hits;
+    int[][] counts;
+    int[] uniqueCounts;
+    public SimpleFacetResult() {}
+  }
+  
+  private int[] getTopNOrds(final int[] counts, final String[] values, int topN) {
+    final int[] ids = new int[counts.length];
+    for(int i=0;i<ids.length;i++) {
+      ids[i] = i;
+    }
+
+    // Naive (on purpose, to reduce bug in tester/gold):
+    // sort all ids, then return top N slice:
+    new InPlaceMergeSorter() {
+
+      @Override
+      protected void swap(int i, int j) {
+        int id = ids[i];
+        ids[i] = ids[j];
+        ids[j] = id;
+      }
+
+      @Override
+      protected int compare(int i, int j) {
+        int counti = counts[ids[i]];
+        int countj = counts[ids[j]];
+        // Sort by count descending...
+        if (counti > countj) {
+          return -1;
+        } else if (counti < countj) {
+          return 1;
+        } else {
+          // ... then by label ascending:
+          return new BytesRef(values[ids[i]]).compareTo(new BytesRef(values[ids[j]]));
+        }
+      }
+
+    }.sort(0, ids.length);
+
+    if (topN > ids.length) {
+      topN = ids.length;
+    }
+
+    int numSet = topN;
+    for(int i=0;i<topN;i++) {
+      if (counts[ids[i]] == 0) {
+        numSet = i;
+        break;
+      }
+    }
+
+    int[] topNIDs = new int[numSet];
+    System.arraycopy(ids, 0, topNIDs, 0, topNIDs.length);
+    return topNIDs;
+  }
+
+  private SimpleFacetResult slowDrillSidewaysSearch(IndexSearcher s, List<FacetRequest> requests, List<Doc> docs,
+                                                    String contentToken, String[][] drillDowns,
+                                                    String[][] dimValues, Filter onlyEven) throws Exception {
+    int numDims = dimValues.length;
+
+    List<Doc> hits = new ArrayList<Doc>();
+    Counters drillDownCounts = new Counters(dimValues);
+    Counters[] drillSidewaysCounts = new Counters[dimValues.length];
+    for(int dim=0;dim<numDims;dim++) {
+      drillSidewaysCounts[dim] = new Counters(dimValues);
+    }
+
+    if (VERBOSE) {
+      System.out.println("  compute expected");
+    }
+
+    nextDoc: for(Doc doc : docs) {
+      if (doc.deleted) {
+        continue;
+      }
+      if (onlyEven != null & (Integer.parseInt(doc.id) & 1) != 0) {
+        continue;
+      }
+      if (contentToken == null || doc.contentToken.equals(contentToken)) {
+        int failDim = -1;
+        for(int dim=0;dim<numDims;dim++) {
+          if (drillDowns[dim] != null) {
+            String docValue = doc.dims[dim] == -1 ? null : dimValues[dim][doc.dims[dim]];
+            String docValue2 = doc.dims2[dim] == -1 ? null : dimValues[dim][doc.dims2[dim]];
+            boolean matches = false;
+            for(String value : drillDowns[dim]) {
+              if (value.equals(docValue) || value.equals(docValue2)) {
+                matches = true;
+                break;
+              }
+            }
+            if (!matches) {
+              if (failDim == -1) {
+                // Doc could be a near-miss, if no other dim fails
+                failDim = dim;
+              } else {
+                // Doc isn't a hit nor a near-miss
+                continue nextDoc;
+              }
+            }
+          }
+        }
+
+        if (failDim == -1) {
+          if (VERBOSE) {
+            System.out.println("    exp: id=" + doc.id + " is a hit");
+          }
+          // Hit:
+          hits.add(doc);
+          drillDownCounts.inc(doc.dims, doc.dims2);
+          for(int dim=0;dim<dimValues.length;dim++) {
+            drillSidewaysCounts[dim].inc(doc.dims, doc.dims2);
+          }
+        } else {
+          if (VERBOSE) {
+            System.out.println("    exp: id=" + doc.id + " is a near-miss on dim=" + failDim);
+          }
+          drillSidewaysCounts[failDim].inc(doc.dims, doc.dims2, failDim);
+        }
+      }
+    }
+
+    Map<String,Integer> idToDocID = new HashMap<String,Integer>();
+    for(int i=0;i<s.getIndexReader().maxDoc();i++) {
+      idToDocID.put(s.doc(i).get("id"), i);
+    }
+
+    Collections.sort(hits);
+
+    SimpleFacetResult res = new SimpleFacetResult();
+    res.hits = hits;
+    res.counts = new int[numDims][];
+    res.uniqueCounts = new int[numDims];
+    for (int i = 0; i < requests.size(); i++) {
+      int dim = Integer.parseInt(requests.get(i).categoryPath.components[0].substring(3));
+      if (drillDowns[dim] != null) {
+        res.counts[dim] = drillSidewaysCounts[dim].counts[dim];
+      } else {
+        res.counts[dim] = drillDownCounts.counts[dim];
+      }
+      int uniqueCount = 0;
+      for (int j = 0; j < res.counts[dim].length; j++) {
+        if (res.counts[dim][j] != 0) {
+          uniqueCount++;
+        }
+      }
+      res.uniqueCounts[dim] = uniqueCount;
+    }
+
+    return res;
+  }
+
+  void verifyEquals(List<FacetRequest> requests, String[][] dimValues, IndexSearcher s, SimpleFacetResult expected,
+                    DrillSidewaysResult actual, Map<String,Float> scores, int topN, boolean isSortedSetDV) throws Exception {
+    if (VERBOSE) {
+      System.out.println("  verify totHits=" + expected.hits.size());
+    }
+    assertEquals(expected.hits.size(), actual.hits.totalHits);
+    assertEquals(expected.hits.size(), actual.hits.scoreDocs.length);
+    for(int i=0;i<expected.hits.size();i++) {
+      if (VERBOSE) {
+        System.out.println("    hit " + i + " expected=" + expected.hits.get(i).id);
+      }
+      assertEquals(expected.hits.get(i).id,
+                   s.doc(actual.hits.scoreDocs[i].doc).get("id"));
+      // Score should be IDENTICAL:
+      assertEquals(scores.get(expected.hits.get(i).id), actual.hits.scoreDocs[i].score, 0.0f);
+    }
+
+    int numExpected = 0;
+    for(int dim=0;dim<expected.counts.length;dim++) {
+      if (expected.counts[dim] != null) {
+        numExpected++;
+      }
+    }
+
+    assertEquals(numExpected, actual.facetResults.size());
+
+    for(int dim=0;dim<expected.counts.length;dim++) {
+      if (expected.counts[dim] == null) {
+        continue;
+      }
+      int idx = -1;
+      for(int i=0;i<requests.size();i++) {
+        if (Integer.parseInt(requests.get(i).categoryPath.components[0].substring(3)) == dim) {
+          idx = i;
+          break;
+        }
+      }
+      assert idx != -1;
+      FacetResult fr = actual.facetResults.get(idx);
+      List<FacetResultNode> subResults = fr.getFacetResultNode().subResults;
+      if (VERBOSE) {
+        System.out.println("    dim" + dim);
+        System.out.println("      actual");
+      }
+
+      Map<String,Integer> actualValues = new HashMap<String,Integer>();
+      idx = 0;
+      for(FacetResultNode childNode : subResults) {
+        actualValues.put(childNode.label.components[1], (int) childNode.value);
+        if (VERBOSE) {
+          System.out.println("        " + idx + ": " + new BytesRef(childNode.label.components[1]) + ": " + (int) childNode.value);
+          idx++;
+        }
+      }
+
+      if (topN != -1) {
+        int[] topNIDs = getTopNOrds(expected.counts[dim], dimValues[dim], topN);
+        if (VERBOSE) {
+          idx = 0;
+          System.out.println("      expected (sorted)");
+          for(int i=0;i<topNIDs.length;i++) {
+            int expectedOrd = topNIDs[i];
+            String value = dimValues[dim][expectedOrd];
+            System.out.println("        " + idx + ": " + new BytesRef(value) + ": " + expected.counts[dim][expectedOrd]);
+            idx++;
+          }
+        }
+        if (VERBOSE) {
+          System.out.println("      topN=" + topN + " expectedTopN=" + topNIDs.length);
+        }
+
+        assertEquals(topNIDs.length, subResults.size());
+        for(int i=0;i<topNIDs.length;i++) {
+          FacetResultNode node = subResults.get(i);
+          int expectedOrd = topNIDs[i];
+          assertEquals(expected.counts[dim][expectedOrd], (int) node.value);
+          assertEquals(2, node.label.length);
+          if (isSortedSetDV) {
+            // Tie-break facet labels are only in unicode
+            // order with SortedSetDVFacets:
+            assertEquals("value @ idx=" + i, dimValues[dim][expectedOrd], node.label.components[1]);
+          }
+        }
+      } else {
+
+        if (VERBOSE) {
+          idx = 0;
+          System.out.println("      expected (unsorted)");
+          for(int i=0;i<dimValues[dim].length;i++) {
+            String value = dimValues[dim][i];
+            if (expected.counts[dim][i] != 0) {
+              System.out.println("        " + idx + ": " + new BytesRef(value) + ": " + expected.counts[dim][i]);
+              idx++;
+            } 
+          }
+        }
+
+        int setCount = 0;
+        for(int i=0;i<dimValues[dim].length;i++) {
+          String value = dimValues[dim][i];
+          if (expected.counts[dim][i] != 0) {
+            assertTrue(actualValues.containsKey(value));
+            assertEquals(expected.counts[dim][i], actualValues.get(value).intValue());
+            setCount++;
+          } else {
+            assertFalse(actualValues.containsKey(value));
+          }
+        }
+        assertEquals(setCount, actualValues.size());
+      }
+
+      assertEquals("dim=" + dim, expected.uniqueCounts[dim], fr.getNumValidDescendants());
+    }
+  }
+
+  / ** Just gathers counts of values under the dim. * /
+  private String toString(FacetResult fr) {
+    StringBuilder b = new StringBuilder();
+    FacetResultNode node = fr.getFacetResultNode();
+    b.append(node.label);
+    b.append(":");
+    for(FacetResultNode childNode : node.subResults) {
+      b.append(' ');
+      b.append(childNode.label.components[1]);
+      b.append('=');
+      b.append((int) childNode.value);
+    }
+    return b.toString();
+  }
+  
+  @Test
+  public void testEmptyIndex() throws Exception {
+    // LUCENE-5045: make sure DrillSideways works with an empty index
+    Directory dir = newDirectory();
+    Directory taxoDir = newDirectory();
+    writer = new RandomIndexWriter(random(), dir);
+    taxoWriter = new DirectoryTaxonomyWriter(taxoDir, IndexWriterConfig.OpenMode.CREATE);
+    IndexSearcher searcher = newSearcher(writer.getReader());
+    writer.close();
+    TaxonomyReader taxoReader = new DirectoryTaxonomyReader(taxoWriter);
+    taxoWriter.close();
+
+    // Count "Author"
+    FacetSearchParams fsp = new FacetSearchParams(new CountFacetRequest(new FacetLabel("Author"), 10));
+
+    DrillSideways ds = new DrillSideways(searcher, taxoReader);
+    DrillDownQuery ddq = new DrillDownQuery(fsp.indexingParams, new MatchAllDocsQuery());
+    ddq.add(new FacetLabel("Author", "Lisa"));
+    
+    DrillSidewaysResult r = ds.search(null, ddq, 10, fsp); // this used to fail on IllegalArgEx
+    assertEquals(0, r.hits.totalHits);
+
+    r = ds.search(ddq, null, null, 10, new Sort(new SortField("foo", Type.INT)), false, false, fsp); // this used to fail on IllegalArgEx
+    assertEquals(0, r.hits.totalHits);
+    
+    IOUtils.close(searcher.getIndexReader(), taxoReader, dir, taxoDir);
+  }
+  */
+}
+
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/simple/TestTaxonomyFacets.java b/lucene/facet/src/test/org/apache/lucene/facet/simple/TestTaxonomyFacets.java
index b27dacd..e7e28cd 100644
--- a/lucene/facet/src/test/org/apache/lucene/facet/simple/TestTaxonomyFacets.java
+++ b/lucene/facet/src/test/org/apache/lucene/facet/simple/TestTaxonomyFacets.java
@@ -65,8 +65,6 @@ public class TestTaxonomyFacets extends FacetTestCase {
 
     IndexWriter writer = new FacetIndexWriter(dir, newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())), taxoWriter, fts);
 
-    // Reused across documents, to add the necessary facet
-    // fields:
     Document doc = new Document();
     doc.add(new FacetField("Author", "Bob"));
     doc.add(new FacetField("Publish Date", "2010", "10", "15"));

