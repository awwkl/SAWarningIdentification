GitDiffStart: cda911959851d6e8fd6c5066e670c401220746cd | Fri Apr 9 18:40:55 2010 +0000
diff --git a/lucene/build.xml b/lucene/build.xml
index 6f7fe92..1d4d937 100644
--- a/lucene/build.xml
+++ b/lucene/build.xml
@@ -348,7 +348,6 @@ The source distribution does not contain sources of the previous Lucene Java ver
           <packageset dir="contrib/spellchecker/src/java"/>
           <packageset dir="contrib/surround/src/java"/>
           <packageset dir="contrib/swing/src/java"/>
-          <packageset dir="contrib/wikipedia/src/java"/>
           <packageset dir="contrib/wordnet/src/java"/>
           <packageset dir="contrib/xml-query-parser/src/java"/>
           <packageset dir="contrib/queryparser/src/java"/>
@@ -378,7 +377,6 @@ The source distribution does not contain sources of the previous Lucene Java ver
           <group title="contrib: SpellChecker" packages="org.apache.lucene.search.spell*"/>
           <group title="contrib: Surround Parser" packages="org.apache.lucene.queryParser.surround*"/>
           <group title="contrib: Swing" packages="org.apache.lucene.swing*"/>
-          <group title="contrib: Wikipedia" packages="org.apache.lucene.wikipedia*"/>
           <group title="contrib: WordNet" packages="org.apache.lucene.wordnet*"/>
           <group title="contrib: XML Query Parser" packages="org.apache.lucene.xmlparser*"/>
           
diff --git a/lucene/contrib/CHANGES.txt b/lucene/contrib/CHANGES.txt
index 53fb2a1..e02efbf 100644
--- a/lucene/contrib/CHANGES.txt
+++ b/lucene/contrib/CHANGES.txt
@@ -19,6 +19,10 @@ Changes in backwards compatibility policy
  * LUCENE-2226: Moved contrib/snowball functionality into contrib/analyzers.
    Be sure to remove any old obselete lucene-snowball jar files from your
    classpath!  (Robert Muir)
+   
+ * LUCENE-2323: Moved contrib/wikipedia functionality into contrib/analyzers.
+   Additionally the package was changed from org.apache.lucene.wikipedia.analysis
+   to org.apache.lucene.analysis.wikipedia.  (Robert Muir)
     
 Changes in runtime behavior
 
diff --git a/lucene/contrib/analyzers/common/build.xml b/lucene/contrib/analyzers/common/build.xml
index a59f32d..62b9a0a 100644
--- a/lucene/contrib/analyzers/common/build.xml
+++ b/lucene/contrib/analyzers/common/build.xml
@@ -35,4 +35,23 @@
     <path refid="junit-path"/>
     <pathelement location="${build.dir}/classes/java"/>
   </path>	
+
+  <target name="jflex" depends="clean-jflex,jflex-wiki-tokenizer"/>
+
+  <target name="jflex-wiki-tokenizer" depends="init,jflex-check" if="jflex.present">
+    <taskdef classname="JFlex.anttask.JFlexTask" name="jflex">
+      <classpath location="${jflex.home}/lib/JFlex.jar"/>
+    </taskdef>
+    <jflex file="src/java/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerImpl.jflex"
+           outdir="src/java/org/apache/lucene/analysis/wikipedia"
+           nobak="on"/>
+  </target>
+
+  <target name="clean-jflex">
+    <delete>
+      <fileset dir="src/java/org/apache/lucene/analysis/wikipedia" includes="*.java">
+        <containsregexp expression="generated.*by.*JFlex"/>
+      </fileset>
+    </delete>
+  </target>
 </project>
diff --git a/lucene/contrib/analyzers/common/src/java/org/apache/lucene/analysis/wikipedia/WikipediaTokenizer.java b/lucene/contrib/analyzers/common/src/java/org/apache/lucene/analysis/wikipedia/WikipediaTokenizer.java
new file mode 100644
index 0000000..4ff201d
--- /dev/null
+++ b/lucene/contrib/analyzers/common/src/java/org/apache/lucene/analysis/wikipedia/WikipediaTokenizer.java
@@ -0,0 +1,327 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.lucene.analysis.wikipedia;
+
+import org.apache.lucene.analysis.Tokenizer;
+import org.apache.lucene.analysis.tokenattributes.FlagsAttribute;
+import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
+import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
+import org.apache.lucene.analysis.tokenattributes.TermAttribute;
+import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
+import org.apache.lucene.util.AttributeSource;
+
+import java.io.IOException;
+import java.io.Reader;
+import java.util.*;
+
+
+/**
+ * Extension of StandardTokenizer that is aware of Wikipedia syntax.  It is based off of the
+ * Wikipedia tutorial available at http://en.wikipedia.org/wiki/Wikipedia:Tutorial, but it may not be complete.
+ * <p/>
+ * <p/>
+ * @lucene.experimental
+ */
+public final class WikipediaTokenizer extends Tokenizer {
+  public static final String INTERNAL_LINK = "il";
+  public static final String EXTERNAL_LINK = "el";
+  //The URL part of the link, i.e. the first token
+  public static final String EXTERNAL_LINK_URL = "elu";
+  public static final String CITATION = "ci";
+  public static final String CATEGORY = "c";
+  public static final String BOLD = "b";
+  public static final String ITALICS = "i";
+  public static final String BOLD_ITALICS = "bi";
+  public static final String HEADING = "h";
+  public static final String SUB_HEADING = "sh";
+
+  public static final int ALPHANUM_ID          = 0;
+  public static final int APOSTROPHE_ID        = 1;
+  public static final int ACRONYM_ID           = 2;
+  public static final int COMPANY_ID           = 3;
+  public static final int EMAIL_ID             = 4;
+  public static final int HOST_ID              = 5;
+  public static final int NUM_ID               = 6;
+  public static final int CJ_ID                = 7;
+  public static final int INTERNAL_LINK_ID     = 8;
+  public static final int EXTERNAL_LINK_ID     = 9;
+  public static final int CITATION_ID          = 10;
+  public static final int CATEGORY_ID          = 11;
+  public static final int BOLD_ID              = 12;
+  public static final int ITALICS_ID           = 13;
+  public static final int BOLD_ITALICS_ID      = 14;
+  public static final int HEADING_ID           = 15;
+  public static final int SUB_HEADING_ID       = 16;
+  public static final int EXTERNAL_LINK_URL_ID = 17;
+
+  /** String token types that correspond to token type int constants */
+  public static final String [] TOKEN_TYPES = new String [] {
+    "<ALPHANUM>",
+    "<APOSTROPHE>",
+    "<ACRONYM>",
+    "<COMPANY>",
+    "<EMAIL>",
+    "<HOST>",
+    "<NUM>",
+    "<CJ>",
+    INTERNAL_LINK,
+    EXTERNAL_LINK,
+    CITATION,
+    CATEGORY,
+    BOLD,
+    ITALICS,
+    BOLD_ITALICS,
+    HEADING,
+    SUB_HEADING,
+    EXTERNAL_LINK_URL
+  };
+
+  /**
+   * Only output tokens
+   */
+  public static final int TOKENS_ONLY = 0;
+  /**
+   * Only output untokenized tokens, which are tokens that would normally be split into several tokens
+   */
+  public static final int UNTOKENIZED_ONLY = 1;
+  /**
+   * Output the both the untokenized token and the splits
+   */
+  public static final int BOTH = 2;
+  /**
+   * This flag is used to indicate that the produced "Token" would, if {@link #TOKENS_ONLY} was used, produce multiple tokens.
+   */
+  public static final int UNTOKENIZED_TOKEN_FLAG = 1;
+  /**
+   * A private instance of the JFlex-constructed scanner
+   */
+  private final WikipediaTokenizerImpl scanner;
+
+  private int tokenOutput = TOKENS_ONLY;
+  private Set<String> untokenizedTypes = Collections.emptySet();
+  private Iterator<AttributeSource.State> tokens = null;
+  
+  private OffsetAttribute offsetAtt;
+  private TypeAttribute typeAtt;
+  private PositionIncrementAttribute posIncrAtt;
+  private TermAttribute termAtt;
+  private FlagsAttribute flagsAtt;
+
+  /**
+   * Creates a new instance of the {@link WikipediaTokenizer}. Attaches the
+   * <code>input</code> to a newly created JFlex scanner.
+   *
+   * @param input The Input Reader
+   */
+  public WikipediaTokenizer(Reader input) {
+    this(input, TOKENS_ONLY, Collections.<String>emptySet());
+  }
+
+  /**
+   * Creates a new instance of the {@link org.apache.lucene.analysis.wikipedia.WikipediaTokenizer}.  Attaches the
+   * <code>input</code> to a the newly created JFlex scanner.
+   *
+   * @param input The input
+   * @param tokenOutput One of {@link #TOKENS_ONLY}, {@link #UNTOKENIZED_ONLY}, {@link #BOTH}
+   * @param untokenizedTypes
+   */
+  public WikipediaTokenizer(Reader input, int tokenOutput, Set<String> untokenizedTypes) {
+    super(input);
+    this.scanner = new WikipediaTokenizerImpl(input);
+    init(tokenOutput, untokenizedTypes);
+  }
+
+  /**
+   * Creates a new instance of the {@link org.apache.lucene.analysis.wikipedia.WikipediaTokenizer}.  Attaches the
+   * <code>input</code> to a the newly created JFlex scanner. Uses the given {@link org.apache.lucene.util.AttributeSource.AttributeFactory}.
+   *
+   * @param input The input
+   * @param tokenOutput One of {@link #TOKENS_ONLY}, {@link #UNTOKENIZED_ONLY}, {@link #BOTH}
+   * @param untokenizedTypes
+   */
+  public WikipediaTokenizer(AttributeFactory factory, Reader input, int tokenOutput, Set<String> untokenizedTypes) {
+    super(factory, input);
+    this.scanner = new WikipediaTokenizerImpl(input);
+    init(tokenOutput, untokenizedTypes);
+  }
+
+  /**
+   * Creates a new instance of the {@link org.apache.lucene.analysis.wikipedia.WikipediaTokenizer}.  Attaches the
+   * <code>input</code> to a the newly created JFlex scanner. Uses the given {@link AttributeSource}.
+   *
+   * @param input The input
+   * @param tokenOutput One of {@link #TOKENS_ONLY}, {@link #UNTOKENIZED_ONLY}, {@link #BOTH}
+   * @param untokenizedTypes
+   */
+  public WikipediaTokenizer(AttributeSource source, Reader input, int tokenOutput, Set<String> untokenizedTypes) {
+    super(source, input);
+    this.scanner = new WikipediaTokenizerImpl(input);
+    init(tokenOutput, untokenizedTypes);
+  }
+  
+  private void init(int tokenOutput, Set<String> untokenizedTypes) {
+    this.tokenOutput = tokenOutput;
+    this.untokenizedTypes = untokenizedTypes;
+    this.offsetAtt = addAttribute(OffsetAttribute.class);
+    this.typeAtt = addAttribute(TypeAttribute.class);
+    this.posIncrAtt = addAttribute(PositionIncrementAttribute.class);
+    this.termAtt = addAttribute(TermAttribute.class);
+    this.flagsAtt = addAttribute(FlagsAttribute.class);    
+  }
+  
+  /*
+  * (non-Javadoc)
+  *
+  * @see org.apache.lucene.analysis.TokenStream#next()
+  */
+  @Override
+  public final boolean incrementToken() throws IOException {
+    if (tokens != null && tokens.hasNext()){
+      AttributeSource.State state = tokens.next();
+      restoreState(state);
+      return true;
+    }
+    clearAttributes();
+    int tokenType = scanner.getNextToken();
+
+    if (tokenType == WikipediaTokenizerImpl.YYEOF) {
+      return false;
+    }
+    String type = WikipediaTokenizerImpl.TOKEN_TYPES[tokenType];
+    if (tokenOutput == TOKENS_ONLY || untokenizedTypes.contains(type) == false){
+      setupToken();
+    } else if (tokenOutput == UNTOKENIZED_ONLY && untokenizedTypes.contains(type) == true){
+      collapseTokens(tokenType);
+
+    }
+    else if (tokenOutput == BOTH){
+      //collapse into a single token, add it to tokens AND output the individual tokens
+      //output the untokenized Token first
+      collapseAndSaveTokens(tokenType, type);
+    }
+    posIncrAtt.setPositionIncrement(scanner.getPositionIncrement());
+    typeAtt.setType(type);
+    return true;
+  }
+
+  private void collapseAndSaveTokens(int tokenType, String type) throws IOException {
+    //collapse
+    StringBuilder buffer = new StringBuilder(32);
+    int numAdded = scanner.setText(buffer);
+    //TODO: how to know how much whitespace to add
+    int theStart = scanner.yychar();
+    int lastPos = theStart + numAdded;
+    int tmpTokType;
+    int numSeen = 0;
+    List<AttributeSource.State> tmp = new ArrayList<AttributeSource.State>();
+    setupSavedToken(0, type);
+    tmp.add(captureState());
+    //while we can get a token and that token is the same type and we have not transitioned to a new wiki-item of the same type
+    while ((tmpTokType = scanner.getNextToken()) != WikipediaTokenizerImpl.YYEOF && tmpTokType == tokenType && scanner.getNumWikiTokensSeen() > numSeen){
+      int currPos = scanner.yychar();
+      //append whitespace
+      for (int i = 0; i < (currPos - lastPos); i++){
+        buffer.append(' ');
+      }
+      numAdded = scanner.setText(buffer);
+      setupSavedToken(scanner.getPositionIncrement(), type);
+      tmp.add(captureState());
+      numSeen++;
+      lastPos = currPos + numAdded;
+    }
+    //trim the buffer
+    String s = buffer.toString().trim();
+    termAtt.setTermBuffer(s.toCharArray(), 0, s.length());
+    offsetAtt.setOffset(correctOffset(theStart), correctOffset(theStart + s.length()));
+    flagsAtt.setFlags(UNTOKENIZED_TOKEN_FLAG);
+    //The way the loop is written, we will have proceeded to the next token.  We need to pushback the scanner to lastPos
+    if (tmpTokType != WikipediaTokenizerImpl.YYEOF){
+      scanner.yypushback(scanner.yylength());
+    }
+    tokens = tmp.iterator();
+  }
+
+  private void setupSavedToken(int positionInc, String type){
+    setupToken();
+    posIncrAtt.setPositionIncrement(positionInc);
+    typeAtt.setType(type);
+  }
+
+  private void collapseTokens(int tokenType) throws IOException {
+    //collapse
+    StringBuilder buffer = new StringBuilder(32);
+    int numAdded = scanner.setText(buffer);
+    //TODO: how to know how much whitespace to add
+    int theStart = scanner.yychar();
+    int lastPos = theStart + numAdded;
+    int tmpTokType;
+    int numSeen = 0;
+    //while we can get a token and that token is the same type and we have not transitioned to a new wiki-item of the same type
+    while ((tmpTokType = scanner.getNextToken()) != WikipediaTokenizerImpl.YYEOF && tmpTokType == tokenType && scanner.getNumWikiTokensSeen() > numSeen){
+      int currPos = scanner.yychar();
+      //append whitespace
+      for (int i = 0; i < (currPos - lastPos); i++){
+        buffer.append(' ');
+      }
+      numAdded = scanner.setText(buffer);
+      numSeen++;
+      lastPos = currPos + numAdded;
+    }
+    //trim the buffer
+    String s = buffer.toString().trim();
+    termAtt.setTermBuffer(s.toCharArray(), 0, s.length());
+    offsetAtt.setOffset(correctOffset(theStart), correctOffset(theStart + s.length()));
+    flagsAtt.setFlags(UNTOKENIZED_TOKEN_FLAG);
+    //The way the loop is written, we will have proceeded to the next token.  We need to pushback the scanner to lastPos
+    if (tmpTokType != WikipediaTokenizerImpl.YYEOF){
+      scanner.yypushback(scanner.yylength());
+    } else {
+      tokens = null;
+    }
+  }
+
+  private void setupToken() {
+    scanner.getText(termAtt);
+    final int start = scanner.yychar();
+    offsetAtt.setOffset(correctOffset(start), correctOffset(start + termAtt.termLength()));
+  }
+
+  /*
+  * (non-Javadoc)
+  *
+  * @see org.apache.lucene.analysis.TokenStream#reset()
+  */
+  @Override
+  public void reset() throws IOException {
+    super.reset();
+    scanner.yyreset(input);
+  }
+
+  @Override
+  public void reset(Reader reader) throws IOException {
+    super.reset(reader);
+    reset();
+  }
+
+  @Override
+  public void end() throws IOException {
+    // set final offset
+    final int finalOffset = correctOffset(scanner.yychar() + scanner.yylength());
+    this.offsetAtt.setOffset(finalOffset, finalOffset);
+  }
+}
\ No newline at end of file
diff --git a/lucene/contrib/analyzers/common/src/java/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerImpl.java b/lucene/contrib/analyzers/common/src/java/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerImpl.java
new file mode 100644
index 0000000..9ffe21a
--- /dev/null
+++ b/lucene/contrib/analyzers/common/src/java/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerImpl.java
@@ -0,0 +1,974 @@
+/* The following code was generated by JFlex 1.4.1 on 4/15/08 4:31 AM */
+
+package org.apache.lucene.analysis.wikipedia;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.analysis.tokenattributes.TermAttribute;
+
+
+/**
+ * This class is a scanner generated by 
+ * <a href="http://www.jflex.de/">JFlex</a> 1.4.1
+ * on 4/15/08 4:31 AM from the specification file
+ * <tt>/mnt2/mike/src/lucene.clean/contrib/wikipedia/src/java/org/apache/lucene/wikipedia/analysis/WikipediaTokenizerImpl.jflex</tt>
+ */
+class WikipediaTokenizerImpl {
+
+  /** This character denotes the end of file */
+  public static final int YYEOF = -1;
+
+  /** initial size of the lookahead buffer */
+  private static final int ZZ_BUFFERSIZE = 16384;
+
+  /** lexical states */
+  public static final int DOUBLE_BRACE_STATE = 8;
+  public static final int INTERNAL_LINK_STATE = 2;
+  public static final int TWO_SINGLE_QUOTES_STATE = 4;
+  public static final int CATEGORY_STATE = 1;
+  public static final int FIVE_SINGLE_QUOTES_STATE = 6;
+  public static final int STRING = 9;
+  public static final int YYINITIAL = 0;
+  public static final int DOUBLE_EQUALS_STATE = 7;
+  public static final int THREE_SINGLE_QUOTES_STATE = 5;
+  public static final int EXTERNAL_LINK_STATE = 3;
+
+  /** 
+   * Translates characters to character classes
+   */
+  private static final String ZZ_CMAP_PACKED = 
+    "\11\0\1\24\1\23\1\0\1\24\1\22\22\0\1\24\1\0\1\12"+
+    "\1\53\2\0\1\3\1\1\4\0\1\14\1\5\1\2\1\10\12\16"+
+    "\1\27\1\0\1\7\1\11\1\13\1\53\1\4\2\15\1\30\5\15"+
+    "\1\41\21\15\1\25\1\0\1\26\1\0\1\6\1\0\1\31\1\43"+
+    "\2\15\1\33\1\40\1\34\1\50\1\41\4\15\1\42\1\35\1\51"+
+    "\1\15\1\36\1\52\1\32\3\15\1\44\1\37\1\15\1\45\1\47"+
+    "\1\46\102\0\27\15\1\0\37\15\1\0\u0568\15\12\17\206\15\12\17"+
+    "\u026c\15\12\17\166\15\12\17\166\15\12\17\166\15\12\17\166\15\12\17"+
+    "\167\15\11\17\166\15\12\17\166\15\12\17\166\15\12\17\340\15\12\17"+
+    "\166\15\12\17\u0166\15\12\17\266\15\u0100\15\u0e00\15\u1040\0\u0150\21\140\0"+
+    "\20\21\u0100\0\200\21\200\0\u19c0\21\100\0\u5200\21\u0c00\0\u2bb0\20\u2150\0"+
+    "\u0200\21\u0465\0\73\21\75\15\43\0";
+
+  /** 
+   * Translates characters to character classes
+   */
+  private static final char [] ZZ_CMAP = zzUnpackCMap(ZZ_CMAP_PACKED);
+
+  /** 
+   * Translates DFA states to action switch labels.
+   */
+  private static final int [] ZZ_ACTION = zzUnpackAction();
+
+  private static final String ZZ_ACTION_PACKED_0 =
+    "\12\0\4\1\4\2\1\3\1\1\1\4\1\1\2\5"+
+    "\1\6\2\5\1\7\1\5\2\10\1\11\1\12\1\11"+
+    "\1\13\1\14\1\10\1\15\1\16\1\15\1\17\1\20"+
+    "\1\10\1\21\1\10\4\22\1\23\1\22\1\24\1\25"+
+    "\1\26\3\0\1\27\14\0\1\30\1\31\1\32\1\33"+
+    "\1\11\1\0\1\34\1\35\1\0\1\36\1\0\1\37"+
+    "\3\0\1\40\1\41\2\42\1\41\2\43\2\0\1\42"+
+    "\1\0\14\42\1\41\3\0\1\11\1\44\3\0\1\45"+
+    "\1\46\5\0\1\47\4\0\1\47\2\0\2\47\2\0"+
+    "\1\11\5\0\1\31\1\41\1\42\1\50\3\0\1\11"+
+    "\2\0\1\51\30\0\1\52\2\0\1\53\1\54\1\55";
+
+  private static int [] zzUnpackAction() {
+    int [] result = new int[183];
+    int offset = 0;
+    offset = zzUnpackAction(ZZ_ACTION_PACKED_0, offset, result);
+    return result;
+  }
+
+  private static int zzUnpackAction(String packed, int offset, int [] result) {
+    int i = 0;       /* index in packed string  */
+    int j = offset;  /* index in unpacked array */
+    int l = packed.length();
+    while (i < l) {
+      int count = packed.charAt(i++);
+      int value = packed.charAt(i++);
+      do result[j++] = value; while (--count > 0);
+    }
+    return j;
+  }
+
+
+  /** 
+   * Translates a state to a row index in the transition table
+   */
+  private static final int [] ZZ_ROWMAP = zzUnpackRowMap();
+
+  private static final String ZZ_ROWMAP_PACKED_0 =
+    "\0\0\0\54\0\130\0\204\0\260\0\334\0\u0108\0\u0134"+
+    "\0\u0160\0\u018c\0\u01b8\0\u01e4\0\u0210\0\u023c\0\u0268\0\u0294"+
+    "\0\u02c0\0\u02ec\0\u01b8\0\u0318\0\u0344\0\u0370\0\u01b8\0\u039c"+
+    "\0\u03c8\0\u03f4\0\u0420\0\u044c\0\u0478\0\u01b8\0\u039c\0\u04a4"+
+    "\0\u01b8\0\u04d0\0\u04fc\0\u0528\0\u0554\0\u0580\0\u05ac\0\u05d8"+
+    "\0\u0604\0\u0630\0\u065c\0\u0688\0\u06b4\0\u01b8\0\u06e0\0\u039c"+
+    "\0\u070c\0\u0738\0\u0764\0\u0790\0\u01b8\0\u01b8\0\u07bc\0\u07e8"+
+    "\0\u0814\0\u01b8\0\u0840\0\u086c\0\u0898\0\u08c4\0\u08f0\0\u091c"+
+    "\0\u0948\0\u0974\0\u09a0\0\u09cc\0\u09f8\0\u0a24\0\u0a50\0\u0a7c"+
+    "\0\u01b8\0\u01b8\0\u0aa8\0\u0ad4\0\u0b00\0\u0b00\0\u0b2c\0\u0b58"+
+    "\0\u0b84\0\u0bb0\0\u0bdc\0\u0c08\0\u0c34\0\u0c60\0\u0c8c\0\u0cb8"+
+    "\0\u0ce4\0\u0d10\0\u0898\0\u0d3c\0\u0d68\0\u0d94\0\u0dc0\0\u0dec"+
+    "\0\u0e18\0\u0e44\0\u0e70\0\u0e9c\0\u0ec8\0\u0ef4\0\u0f20\0\u0f4c"+
+    "\0\u0f78\0\u0fa4\0\u0fd0\0\u0ffc\0\u1028\0\u1054\0\u1080\0\u10ac"+
+    "\0\u10d8\0\u01b8\0\u1104\0\u1130\0\u115c\0\u1188\0\u01b8\0\u11b4"+
+    "\0\u11e0\0\u120c\0\u1238\0\u1264\0\u1290\0\u12bc\0\u12e8\0\u1314"+
+    "\0\u1340\0\u136c\0\u1398\0\u13c4\0\u086c\0\u09f8\0\u13f0\0\u141c"+
+    "\0\u1448\0\u1474\0\u14a0\0\u14cc\0\u14f8\0\u1524\0\u01b8\0\u1550"+
+    "\0\u157c\0\u15a8\0\u15d4\0\u1600\0\u162c\0\u1658\0\u1684\0\u16b0"+
+    "\0\u01b8\0\u16dc\0\u1708\0\u1734\0\u1760\0\u178c\0\u17b8\0\u17e4"+
+    "\0\u1810\0\u183c\0\u1868\0\u1894\0\u18c0\0\u18ec\0\u1918\0\u1944"+
+    "\0\u1970\0\u199c\0\u19c8\0\u19f4\0\u1a20\0\u1a4c\0\u1a78\0\u1aa4"+
+    "\0\u1ad0\0\u1afc\0\u1b28\0\u1b54\0\u01b8\0\u01b8\0\u01b8";
+
+  private static int [] zzUnpackRowMap() {
+    int [] result = new int[183];
+    int offset = 0;
+    offset = zzUnpackRowMap(ZZ_ROWMAP_PACKED_0, offset, result);
+    return result;
+  }
+
+  private static int zzUnpackRowMap(String packed, int offset, int [] result) {
+    int i = 0;  /* index in packed string  */
+    int j = offset;  /* index in unpacked array */
+    int l = packed.length();
+    while (i < l) {
+      int high = packed.charAt(i++) << 16;
+      result[j++] = high | packed.charAt(i++);
+    }
+    return j;
+  }
+
+  /** 
+   * The transition table of the DFA
+   */
+  private static final int [] ZZ_TRANS = zzUnpackTrans();
+
+  private static final String ZZ_TRANS_PACKED_0 =
+    "\1\13\1\14\5\13\1\15\1\13\1\16\3\13\1\17"+
+    "\1\20\1\21\1\22\1\23\1\24\2\13\1\25\2\13"+
+    "\15\17\1\26\2\13\3\17\1\13\7\27\1\30\5\27"+
+    "\4\31\1\27\1\32\3\27\1\33\1\27\15\31\3\27"+
+    "\3\31\10\27\1\30\5\27\4\34\1\27\1\32\3\27"+
+    "\1\35\1\27\15\34\3\27\3\34\1\27\7\36\1\37"+
+    "\5\36\4\40\1\36\1\32\2\27\1\36\1\41\1\36"+
+    "\15\40\3\36\1\42\2\40\2\36\1\43\5\36\1\37"+
+    "\5\36\4\44\1\36\1\45\2\36\1\46\2\36\15\44"+
+    "\3\36\3\44\10\36\1\37\5\36\4\47\1\36\1\45"+
+    "\2\36\1\46\2\36\15\47\3\36\3\47\10\36\1\37"+
+    "\5\36\4\47\1\36\1\45\2\36\1\50\2\36\15\47"+
+    "\3\36\3\47\10\36\1\37\1\36\1\51\3\36\4\52"+
+    "\1\36\1\45\5\36\15\52\3\36\3\52\10\36\1\53"+
+    "\5\36\4\54\1\36\1\45\5\36\15\54\1\36\1\55"+
+    "\1\36\3\54\1\36\1\56\1\57\5\56\1\60\1\56"+
+    "\1\61\3\56\4\62\1\56\1\63\2\56\1\64\2\56"+
+    "\15\62\2\56\1\65\3\62\1\56\55\0\1\66\62\0"+
+    "\1\67\4\0\4\70\7\0\6\70\1\71\6\70\3\0"+
+    "\3\70\12\0\1\72\43\0\1\73\1\74\1\75\1\76"+
+    "\2\77\1\0\1\100\3\0\1\100\1\17\1\20\1\21"+
+    "\1\22\7\0\15\17\3\0\3\17\3\0\1\101\1\0"+
+    "\1\102\2\103\1\0\1\104\3\0\1\104\3\20\1\22"+
+    "\7\0\15\20\3\0\3\20\2\0\1\73\1\105\1\75"+
+    "\1\76\2\103\1\0\1\104\3\0\1\104\1\21\1\20"+
+    "\1\21\1\22\7\0\15\21\3\0\3\21\3\0\1\106"+
+    "\1\0\1\102\2\77\1\0\1\100\3\0\1\100\4\22"+
+    "\7\0\15\22\3\0\3\22\24\0\1\13\55\0\1\107"+
+    "\73\0\1\110\16\0\1\67\4\0\4\70\7\0\15\70"+
+    "\3\0\3\70\16\0\4\31\7\0\15\31\3\0\3\31"+
+    "\24\0\1\27\56\0\1\111\42\0\4\34\7\0\15\34"+
+    "\3\0\3\34\27\0\1\112\42\0\4\40\7\0\15\40"+
+    "\3\0\3\40\16\0\4\40\7\0\2\40\1\113\12\40"+
+    "\3\0\3\40\2\0\1\114\67\0\4\44\7\0\15\44"+
+    "\3\0\3\44\24\0\1\36\55\0\1\115\43\0\4\47"+
+    "\7\0\15\47\3\0\3\47\26\0\1\116\37\0\1\111"+
+    "\57\0\4\52\7\0\15\52\3\0\3\52\11\0\1\117"+
+    "\4\0\4\70\7\0\15\70\3\0\3\70\16\0\4\54"+
+    "\7\0\15\54\3\0\3\54\47\0\1\111\6\0\1\120"+
+    "\63\0\1\121\57\0\4\62\7\0\15\62\3\0\3\62"+
+    "\24\0\1\56\55\0\1\122\43\0\4\70\7\0\15\70"+
+    "\3\0\3\70\14\0\1\36\1\0\4\123\1\0\3\124"+
+    "\3\0\15\123\3\0\3\123\14\0\1\36\1\0\4\123"+
+    "\1\0\3\124\3\0\3\123\1\125\11\123\3\0\3\123"+
+    "\16\0\1\126\1\0\1\126\10\0\15\126\3\0\3\126"+
+    "\16\0\1\127\1\130\1\131\1\132\7\0\15\127\3\0"+
+    "\3\127\16\0\1\133\1\0\1\133\10\0\15\133\3\0"+
+    "\3\133\16\0\1\134\1\135\1\134\1\135\7\0\15\134"+
+    "\3\0\3\134\16\0\1\136\2\137\1\140\7\0\15\136"+
+    "\3\0\3\136\16\0\1\100\2\141\10\0\15\100\3\0"+
+    "\3\100\16\0\1\142\2\143\1\144\7\0\15\142\3\0"+
+    "\3\142\16\0\4\135\7\0\15\135\3\0\3\135\16\0"+
+    "\1\145\2\146\1\147\7\0\15\145\3\0\3\145\16\0"+
+    "\1\150\2\151\1\152\7\0\15\150\3\0\3\150\16\0"+
+    "\1\153\1\143\1\154\1\144\7\0\15\153\3\0\3\153"+
+    "\16\0\1\155\2\130\1\132\7\0\15\155\3\0\3\155"+
+    "\30\0\1\156\1\157\64\0\1\160\27\0\4\40\7\0"+
+    "\2\40\1\161\12\40\3\0\3\40\2\0\1\162\101\0"+
+    "\1\163\1\164\40\0\4\70\7\0\6\70\1\165\6\70"+
+    "\3\0\3\70\2\0\1\166\63\0\1\167\71\0\1\170"+
+    "\1\171\34\0\1\172\1\0\1\36\1\0\4\123\1\0"+
+    "\3\124\3\0\15\123\3\0\3\123\16\0\4\173\1\0"+
+    "\3\124\3\0\15\173\3\0\3\173\12\0\1\172\1\0"+
+    "\1\36\1\0\4\123\1\0\3\124\3\0\10\123\1\174"+
+    "\4\123\3\0\3\123\2\0\1\73\13\0\1\126\1\0"+
+    "\1\126\10\0\15\126\3\0\3\126\3\0\1\175\1\0"+
+    "\1\102\2\176\6\0\1\127\1\130\1\131\1\132\7\0"+
+    "\15\127\3\0\3\127\3\0\1\177\1\0\1\102\2\200"+
+    "\1\0\1\201\3\0\1\201\3\130\1\132\7\0\15\130"+
+    "\3\0\3\130\3\0\1\202\1\0\1\102\2\200\1\0"+
+    "\1\201\3\0\1\201\1\131\1\130\1\131\1\132\7\0"+
+    "\15\131\3\0\3\131\3\0\1\203\1\0\1\102\2\176"+
+    "\6\0\4\132\7\0\15\132\3\0\3\132\3\0\1\204"+
+    "\2\0\1\204\7\0\1\134\1\135\1\134\1\135\7\0"+
+    "\15\134\3\0\3\134\3\0\1\204\2\0\1\204\7\0"+
+    "\4\135\7\0\15\135\3\0\3\135\3\0\1\176\1\0"+
+    "\1\102\2\176\6\0\1\136\2\137\1\140\7\0\15\136"+
+    "\3\0\3\136\3\0\1\200\1\0\1\102\2\200\1\0"+
+    "\1\201\3\0\1\201\3\137\1\140\7\0\15\137\3\0"+
+    "\3\137\3\0\1\176\1\0\1\102\2\176\6\0\4\140"+
+    "\7\0\15\140\3\0\3\140\3\0\1\201\2\0\2\201"+
+    "\1\0\1\201\3\0\1\201\3\141\10\0\15\141\3\0"+
+    "\3\141\3\0\1\106\1\0\1\102\2\77\1\0\1\100"+
+    "\3\0\1\100\1\142\2\143\1\144\7\0\15\142\3\0"+
+    "\3\142\3\0\1\101\1\0\1\102\2\103\1\0\1\104"+
+    "\3\0\1\104\3\143\1\144\7\0\15\143\3\0\3\143"+
+    "\3\0\1\106\1\0\1\102\2\77\1\0\1\100\3\0"+
+    "\1\100\4\144\7\0\15\144\3\0\3\144\3\0\1\77"+
+    "\1\0\1\102\2\77\1\0\1\100\3\0\1\100\1\145"+
+    "\2\146\1\147\7\0\15\145\3\0\3\145\3\0\1\103"+
+    "\1\0\1\102\2\103\1\0\1\104\3\0\1\104\3\146"+
+    "\1\147\7\0\15\146\3\0\3\146\3\0\1\77\1\0"+
+    "\1\102\2\77\1\0\1\100\3\0\1\100\4\147\7\0"+
+    "\15\147\3\0\3\147\3\0\1\100\2\0\2\100\1\0"+
+    "\1\100\3\0\1\100\1\150\2\151\1\152\7\0\15\150"+
+    "\3\0\3\150\3\0\1\104\2\0\2\104\1\0\1\104"+
+    "\3\0\1\104\3\151\1\152\7\0\15\151\3\0\3\151"+
+    "\3\0\1\100\2\0\2\100\1\0\1\100\3\0\1\100"+
+    "\4\152\7\0\15\152\3\0\3\152\3\0\1\205\1\0"+
+    "\1\102\2\77\1\0\1\100\3\0\1\100\1\153\1\143"+
+    "\1\154\1\144\7\0\15\153\3\0\3\153\3\0\1\206"+
+    "\1\0\1\102\2\103\1\0\1\104\3\0\1\104\1\154"+
+    "\1\143\1\154\1\144\7\0\15\154\3\0\3\154\3\0"+
+    "\1\203\1\0\1\102\2\176\6\0\1\155\2\130\1\132"+
+    "\7\0\15\155\3\0\3\155\31\0\1\157\54\0\1\207"+
+    "\64\0\1\210\26\0\4\40\7\0\15\40\3\0\1\40"+
+    "\1\211\1\40\31\0\1\164\54\0\1\212\35\0\1\36"+
+    "\1\0\4\123\1\0\3\124\3\0\3\123\1\213\11\123"+
+    "\3\0\3\123\2\0\1\214\102\0\1\171\54\0\1\215"+
+    "\34\0\1\216\52\0\1\172\3\0\4\173\7\0\15\173"+
+    "\3\0\3\173\12\0\1\172\1\0\1\217\1\0\4\123"+
+    "\1\0\3\124\3\0\15\123\3\0\3\123\16\0\1\220"+
+    "\1\132\1\220\1\132\7\0\15\220\3\0\3\220\16\0"+
+    "\4\140\7\0\15\140\3\0\3\140\16\0\4\144\7\0"+
+    "\15\144\3\0\3\144\16\0\4\147\7\0\15\147\3\0"+
+    "\3\147\16\0\4\152\7\0\15\152\3\0\3\152\16\0"+
+    "\1\221\1\144\1\221\1\144\7\0\15\221\3\0\3\221"+
+    "\16\0\4\132\7\0\15\132\3\0\3\132\16\0\4\222"+
+    "\7\0\15\222\3\0\3\222\33\0\1\223\61\0\1\224"+
+    "\30\0\4\40\6\0\1\225\15\40\3\0\2\40\1\226"+
+    "\33\0\1\227\32\0\1\172\1\0\1\36\1\0\4\123"+
+    "\1\0\3\124\3\0\10\123\1\230\4\123\3\0\3\123"+
+    "\2\0\1\231\104\0\1\232\36\0\4\233\7\0\15\233"+
+    "\3\0\3\233\3\0\1\175\1\0\1\102\2\176\6\0"+
+    "\1\220\1\132\1\220\1\132\7\0\15\220\3\0\3\220"+
+    "\3\0\1\205\1\0\1\102\2\77\1\0\1\100\3\0"+
+    "\1\100\1\221\1\144\1\221\1\144\7\0\15\221\3\0"+
+    "\3\221\3\0\1\204\2\0\1\204\7\0\4\222\7\0"+
+    "\15\222\3\0\3\222\34\0\1\234\55\0\1\235\26\0"+
+    "\1\236\60\0\4\40\6\0\1\225\15\40\3\0\3\40"+
+    "\34\0\1\237\31\0\1\172\1\0\1\111\1\0\4\123"+
+    "\1\0\3\124\3\0\15\123\3\0\3\123\34\0\1\240"+
+    "\32\0\1\241\2\0\4\233\7\0\15\233\3\0\3\233"+
+    "\35\0\1\242\62\0\1\243\20\0\1\244\77\0\1\245"+
+    "\53\0\1\246\32\0\1\36\1\0\4\173\1\0\3\124"+
+    "\3\0\15\173\3\0\3\173\36\0\1\247\53\0\1\250"+
+    "\33\0\4\251\7\0\15\251\3\0\3\251\36\0\1\252"+
+    "\53\0\1\253\54\0\1\254\61\0\1\255\11\0\1\256"+
+    "\12\0\4\251\7\0\15\251\3\0\3\251\37\0\1\257"+
+    "\53\0\1\260\54\0\1\261\22\0\1\13\62\0\4\262"+
+    "\7\0\15\262\3\0\3\262\40\0\1\263\53\0\1\264"+
+    "\43\0\1\265\26\0\2\262\1\0\2\262\1\0\2\262"+
+    "\2\0\5\262\7\0\15\262\3\0\4\262\27\0\1\266"+
+    "\53\0\1\267\24\0";
+
+  private static int [] zzUnpackTrans() {
+    int [] result = new int[7040];
+    int offset = 0;
+    offset = zzUnpackTrans(ZZ_TRANS_PACKED_0, offset, result);
+    return result;
+  }
+
+  private static int zzUnpackTrans(String packed, int offset, int [] result) {
+    int i = 0;       /* index in packed string  */
+    int j = offset;  /* index in unpacked array */
+    int l = packed.length();
+    while (i < l) {
+      int count = packed.charAt(i++);
+      int value = packed.charAt(i++);
+      value--;
+      do result[j++] = value; while (--count > 0);
+    }
+    return j;
+  }
+
+
+  /* error codes */
+  private static final int ZZ_UNKNOWN_ERROR = 0;
+  private static final int ZZ_NO_MATCH = 1;
+  private static final int ZZ_PUSHBACK_2BIG = 2;
+
+  /* error messages for the codes above */
+  private static final String ZZ_ERROR_MSG[] = {
+    "Unkown internal scanner error",
+    "Error: could not match input",
+    "Error: pushback value was too large"
+  };
+
+  /**
+   * ZZ_ATTRIBUTE[aState] contains the attributes of state <code>aState</code>
+   */
+  private static final int [] ZZ_ATTRIBUTE = zzUnpackAttribute();
+
+  private static final String ZZ_ATTRIBUTE_PACKED_0 =
+    "\12\0\1\11\7\1\1\11\3\1\1\11\6\1\1\11"+
+    "\2\1\1\11\14\1\1\11\6\1\2\11\3\0\1\11"+
+    "\14\0\2\1\2\11\1\1\1\0\2\1\1\0\1\1"+
+    "\1\0\1\1\3\0\7\1\2\0\1\1\1\0\15\1"+
+    "\3\0\1\1\1\11\3\0\1\1\1\11\5\0\1\1"+
+    "\4\0\1\1\2\0\2\1\2\0\1\1\5\0\1\11"+
+    "\3\1\3\0\1\1\2\0\1\11\30\0\1\1\2\0"+
+    "\3\11";
+
+  private static int [] zzUnpackAttribute() {
+    int [] result = new int[183];
+    int offset = 0;
+    offset = zzUnpackAttribute(ZZ_ATTRIBUTE_PACKED_0, offset, result);
+    return result;
+  }
+
+  private static int zzUnpackAttribute(String packed, int offset, int [] result) {
+    int i = 0;       /* index in packed string  */
+    int j = offset;  /* index in unpacked array */
+    int l = packed.length();
+    while (i < l) {
+      int count = packed.charAt(i++);
+      int value = packed.charAt(i++);
+      do result[j++] = value; while (--count > 0);
+    }
+    return j;
+  }
+
+  /** the input device */
+  private java.io.Reader zzReader;
+
+  /** the current state of the DFA */
+  private int zzState;
+
+  /** the current lexical state */
+  private int zzLexicalState = YYINITIAL;
+
+  /** this buffer contains the current text to be matched and is
+      the source of the yytext() string */
+  private char zzBuffer[] = new char[ZZ_BUFFERSIZE];
+
+  /** the textposition at the last accepting state */
+  private int zzMarkedPos;
+
+  /** the textposition at the last state to be included in yytext */
+  private int zzPushbackPos;
+
+  /** the current text position in the buffer */
+  private int zzCurrentPos;
+
+  /** startRead marks the beginning of the yytext() string in the buffer */
+  private int zzStartRead;
+
+  /** endRead marks the last character in the buffer, that has been read
+      from input */
+  private int zzEndRead;
+
+  /** number of newlines encountered up to the start of the matched text */
+  private int yyline;
+
+  /** the number of characters up to the start of the matched text */
+  private int yychar;
+
+  /**
+   * the number of characters from the last newline up to the start of the 
+   * matched text
+   */
+  private int yycolumn;
+
+  /** 
+   * zzAtBOL == true <=> the scanner is currently at the beginning of a line
+   */
+  private boolean zzAtBOL = true;
+
+  /** zzAtEOF == true <=> the scanner is at the EOF */
+  private boolean zzAtEOF;
+
+  /* user code: */
+
+public static final int ALPHANUM          = WikipediaTokenizer.ALPHANUM_ID;
+public static final int APOSTROPHE        = WikipediaTokenizer.APOSTROPHE_ID;
+public static final int ACRONYM           = WikipediaTokenizer.ACRONYM_ID;
+public static final int COMPANY           = WikipediaTokenizer.COMPANY_ID;
+public static final int EMAIL             = WikipediaTokenizer.EMAIL_ID;
+public static final int HOST              = WikipediaTokenizer.HOST_ID;
+public static final int NUM               = WikipediaTokenizer.NUM_ID;
+public static final int CJ                = WikipediaTokenizer.CJ_ID;
+public static final int INTERNAL_LINK     = WikipediaTokenizer.INTERNAL_LINK_ID;
+public static final int EXTERNAL_LINK     = WikipediaTokenizer.EXTERNAL_LINK_ID;
+public static final int CITATION          = WikipediaTokenizer.CITATION_ID;
+public static final int CATEGORY          = WikipediaTokenizer.CATEGORY_ID;
+public static final int BOLD              = WikipediaTokenizer.BOLD_ID;
+public static final int ITALICS           = WikipediaTokenizer.ITALICS_ID;
+public static final int BOLD_ITALICS      = WikipediaTokenizer.BOLD_ITALICS_ID;
+public static final int HEADING           = WikipediaTokenizer.HEADING_ID;
+public static final int SUB_HEADING       = WikipediaTokenizer.SUB_HEADING_ID;
+public static final int EXTERNAL_LINK_URL = WikipediaTokenizer.EXTERNAL_LINK_URL_ID;
+
+
+private int currentTokType;
+private int numBalanced = 0;
+private int positionInc = 1;
+private int numLinkToks = 0;
+//Anytime we start a new on a Wiki reserved token (category, link, etc.) this value will be 0, otherwise it will be the number of tokens seen
+//this can be useful for detecting when a new reserved token is encountered
+//see https://issues.apache.org/jira/browse/LUCENE-1133
+private int numWikiTokensSeen = 0;
+
+public static final String [] TOKEN_TYPES = WikipediaTokenizer.TOKEN_TYPES;
+
+/**
+Returns the number of tokens seen inside a category or link, etc.
+@return the number of tokens seen inside the context of wiki syntax.
+**/
+public final int getNumWikiTokensSeen(){
+  return numWikiTokensSeen;
+}
+
+public final int yychar()
+{
+    return yychar;
+}
+
+public final int getPositionIncrement(){
+  return positionInc;
+}
+
+/**
+ * Fills Lucene token with the current token text.
+ */
+final void getText(TermAttribute t) {
+  t.setTermBuffer(zzBuffer, zzStartRead, zzMarkedPos-zzStartRead);
+}
+
+final int setText(StringBuilder buffer){
+  int length = zzMarkedPos - zzStartRead;
+  buffer.append(zzBuffer, zzStartRead, length);
+  return length;
+}
+
+
+
+
+  /**
+   * Creates a new scanner
+   * There is also a java.io.InputStream version of this constructor.
+   *
+   * @param   in  the java.io.Reader to read input from.
+   */
+  WikipediaTokenizerImpl(java.io.Reader in) {
+    this.zzReader = in;
+  }
+
+  /**
+   * Creates a new scanner.
+   * There is also java.io.Reader version of this constructor.
+   *
+   * @param   in  the java.io.Inputstream to read input from.
+   */
+  WikipediaTokenizerImpl(java.io.InputStream in) {
+    this(new java.io.InputStreamReader(in));
+  }
+
+  /** 
+   * Unpacks the compressed character translation table.
+   *
+   * @param packed   the packed character translation table
+   * @return         the unpacked character translation table
+   */
+  private static char [] zzUnpackCMap(String packed) {
+    char [] map = new char[0x10000];
+    int i = 0;  /* index in packed string  */
+    int j = 0;  /* index in unpacked array */
+    while (i < 230) {
+      int  count = packed.charAt(i++);
+      char value = packed.charAt(i++);
+      do map[j++] = value; while (--count > 0);
+    }
+    return map;
+  }
+
+
+  /**
+   * Refills the input buffer.
+   *
+   * @return      <code>false</code>, iff there was new input.
+   * 
+   * @exception   java.io.IOException  if any I/O-Error occurs
+   */
+  private boolean zzRefill() throws java.io.IOException {
+
+    /* first: make room (if you can) */
+    if (zzStartRead > 0) {
+      System.arraycopy(zzBuffer, zzStartRead,
+                       zzBuffer, 0,
+                       zzEndRead-zzStartRead);
+
+      /* translate stored positions */
+      zzEndRead-= zzStartRead;
+      zzCurrentPos-= zzStartRead;
+      zzMarkedPos-= zzStartRead;
+      zzPushbackPos-= zzStartRead;
+      zzStartRead = 0;
+    }
+
+    /* is the buffer big enough? */
+    if (zzCurrentPos >= zzBuffer.length) {
+      /* if not: blow it up */
+      char newBuffer[] = new char[zzCurrentPos*2];
+      System.arraycopy(zzBuffer, 0, newBuffer, 0, zzBuffer.length);
+      zzBuffer = newBuffer;
+    }
+
+    /* finally: fill the buffer with new input */
+    int numRead = zzReader.read(zzBuffer, zzEndRead,
+                                            zzBuffer.length-zzEndRead);
+
+    if (numRead < 0) {
+      return true;
+    }
+    else {
+      zzEndRead+= numRead;
+      return false;
+    }
+  }
+
+    
+  /**
+   * Closes the input stream.
+   */
+  public final void yyclose() throws java.io.IOException {
+    zzAtEOF = true;            /* indicate end of file */
+    zzEndRead = zzStartRead;  /* invalidate buffer    */
+
+    if (zzReader != null)
+      zzReader.close();
+  }
+
+
+  /**
+   * Resets the scanner to read from a new input stream.
+   * Does not close the old reader.
+   *
+   * All internal variables are reset, the old input stream 
+   * <b>cannot</b> be reused (internal buffer is discarded and lost).
+   * Lexical state is set to <tt>ZZ_INITIAL</tt>.
+   *
+   * @param reader   the new input stream 
+   */
+  public final void yyreset(java.io.Reader reader) {
+    zzReader = reader;
+    zzAtBOL  = true;
+    zzAtEOF  = false;
+    zzEndRead = zzStartRead = 0;
+    zzCurrentPos = zzMarkedPos = zzPushbackPos = 0;
+    yyline = yychar = yycolumn = 0;
+    zzLexicalState = YYINITIAL;
+  }
+
+
+  /**
+   * Returns the current lexical state.
+   */
+  public final int yystate() {
+    return zzLexicalState;
+  }
+
+
+  /**
+   * Enters a new lexical state
+   *
+   * @param newState the new lexical state
+   */
+  public final void yybegin(int newState) {
+    zzLexicalState = newState;
+  }
+
+
+  /**
+   * Returns the text matched by the current regular expression.
+   */
+  public final String yytext() {
+    return new String( zzBuffer, zzStartRead, zzMarkedPos-zzStartRead );
+  }
+
+
+  /**
+   * Returns the character at position <tt>pos</tt> from the 
+   * matched text. 
+   * 
+   * It is equivalent to yytext().charAt(pos), but faster
+   *
+   * @param pos the position of the character to fetch. 
+   *            A value from 0 to yylength()-1.
+   *
+   * @return the character at position pos
+   */
+  public final char yycharat(int pos) {
+    return zzBuffer[zzStartRead+pos];
+  }
+
+
+  /**
+   * Returns the length of the matched text region.
+   */
+  public final int yylength() {
+    return zzMarkedPos-zzStartRead;
+  }
+
+
+  /**
+   * Reports an error that occured while scanning.
+   *
+   * In a wellformed scanner (no or only correct usage of 
+   * yypushback(int) and a match-all fallback rule) this method 
+   * will only be called with things that "Can't Possibly Happen".
+   * If this method is called, something is seriously wrong
+   * (e.g. a JFlex bug producing a faulty scanner etc.).
+   *
+   * Usual syntax/scanner level error handling should be done
+   * in error fallback rules.
+   *
+   * @param   errorCode  the code of the errormessage to display
+   */
+  private void zzScanError(int errorCode) {
+    String message;
+    try {
+      message = ZZ_ERROR_MSG[errorCode];
+    }
+    catch (ArrayIndexOutOfBoundsException e) {
+      message = ZZ_ERROR_MSG[ZZ_UNKNOWN_ERROR];
+    }
+
+    throw new Error(message);
+  } 
+
+
+  /**
+   * Pushes the specified amount of characters back into the input stream.
+   *
+   * They will be read again by then next call of the scanning method
+   *
+   * @param number  the number of characters to be read again.
+   *                This number must not be greater than yylength()!
+   */
+  public void yypushback(int number)  {
+    if ( number > yylength() )
+      zzScanError(ZZ_PUSHBACK_2BIG);
+
+    zzMarkedPos -= number;
+  }
+
+
+  /**
+   * Resumes scanning until the next regular expression is matched,
+   * the end of input is encountered or an I/O-Error occurs.
+   *
+   * @return      the next token
+   * @exception   java.io.IOException  if any I/O-Error occurs
+   */
+  public int getNextToken() throws java.io.IOException {
+    int zzInput;
+    int zzAction;
+
+    // cached fields:
+    int zzCurrentPosL;
+    int zzMarkedPosL;
+    int zzEndReadL = zzEndRead;
+    char [] zzBufferL = zzBuffer;
+    char [] zzCMapL = ZZ_CMAP;
+
+    int [] zzTransL = ZZ_TRANS;
+    int [] zzRowMapL = ZZ_ROWMAP;
+    int [] zzAttrL = ZZ_ATTRIBUTE;
+
+    while (true) {
+      zzMarkedPosL = zzMarkedPos;
+
+      yychar+= zzMarkedPosL-zzStartRead;
+
+      zzAction = -1;
+
+      zzCurrentPosL = zzCurrentPos = zzStartRead = zzMarkedPosL;
+  
+      zzState = zzLexicalState;
+
+
+      zzForAction: {
+        while (true) {
+    
+          if (zzCurrentPosL < zzEndReadL)
+            zzInput = zzBufferL[zzCurrentPosL++];
+          else if (zzAtEOF) {
+            zzInput = YYEOF;
+            break zzForAction;
+          }
+          else {
+            // store back cached positions
+            zzCurrentPos  = zzCurrentPosL;
+            zzMarkedPos   = zzMarkedPosL;
+            boolean eof = zzRefill();
+            // get translated positions and possibly new buffer
+            zzCurrentPosL  = zzCurrentPos;
+            zzMarkedPosL   = zzMarkedPos;
+            zzBufferL      = zzBuffer;
+            zzEndReadL     = zzEndRead;
+            if (eof) {
+              zzInput = YYEOF;
+              break zzForAction;
+            }
+            else {
+              zzInput = zzBufferL[zzCurrentPosL++];
+            }
+          }
+          int zzNext = zzTransL[ zzRowMapL[zzState] + zzCMapL[zzInput] ];
+          if (zzNext == -1) break zzForAction;
+          zzState = zzNext;
+
+          int zzAttributes = zzAttrL[zzState];
+          if ( (zzAttributes & 1) == 1 ) {
+            zzAction = zzState;
+            zzMarkedPosL = zzCurrentPosL;
+            if ( (zzAttributes & 8) == 8 ) break zzForAction;
+          }
+
+        }
+      }
+
+      // store back cached position
+      zzMarkedPos = zzMarkedPosL;
+
+      switch (zzAction < 0 ? zzAction : ZZ_ACTION[zzAction]) {
+        case 8: 
+          { /* ignore */
+          }
+        case 46: break;
+        case 28: 
+          { currentTokType = INTERNAL_LINK; numWikiTokensSeen = 0; yybegin(INTERNAL_LINK_STATE);
+          }
+        case 47: break;
+        case 3: 
+          { positionInc = 1; return CJ;
+          }
+        case 48: break;
+        case 30: 
+          { numBalanced = 0;currentTokType = ALPHANUM; yybegin(YYINITIAL);/*end italics*/
+          }
+        case 49: break;
+        case 10: 
+          { numLinkToks = 0; positionInc = 0; yybegin(YYINITIAL);
+          }
+        case 50: break;
+        case 41: 
+          { numBalanced = 0;currentTokType = ALPHANUM; yybegin(YYINITIAL);/*end bold italics*/
+          }
+        case 51: break;
+        case 7: 
+          { yybegin(INTERNAL_LINK_STATE); numWikiTokensSeen++; return currentTokType;
+          }
+        case 52: break;
+        case 23: 
+          { numWikiTokensSeen = 0; positionInc = 1; yybegin(DOUBLE_EQUALS_STATE);
+          }
+        case 53: break;
+        case 38: 
+          { numBalanced = 0;currentTokType = ALPHANUM; yybegin(YYINITIAL);/*end sub header*/
+          }
+        case 54: break;
+        case 17: 
+          { yybegin(DOUBLE_BRACE_STATE); numWikiTokensSeen = 0; return currentTokType;
+          }
+        case 55: break;
+        case 24: 
+          { numWikiTokensSeen = 0; positionInc = 1; currentTokType = INTERNAL_LINK; yybegin(INTERNAL_LINK_STATE);
+          }
+        case 56: break;
+        case 14: 
+          { yybegin(STRING); numWikiTokensSeen++; return currentTokType;
+          }
+        case 57: break;
+        case 5: 
+          { positionInc = 1;
+          }
+        case 58: break;
+        case 43: 
+          { numWikiTokensSeen = 0; positionInc = 1; currentTokType = CATEGORY; yybegin(CATEGORY_STATE);
+          }
+        case 59: break;
+        case 26: 
+          { yybegin(YYINITIAL);
+          }
+        case 60: break;
+        case 20: 
+          { numBalanced = 0; numWikiTokensSeen = 0; currentTokType = EXTERNAL_LINK;yybegin(EXTERNAL_LINK_STATE);
+          }
+        case 61: break;
+        case 1: 
+          { numWikiTokensSeen = 0;  positionInc = 1;
+          }
+        case 62: break;
+        case 40: 
+          { positionInc = 1; return EMAIL;
+          }
+        case 63: break;
+        case 25: 
+          { numWikiTokensSeen = 0; positionInc = 1; currentTokType = CITATION; yybegin(DOUBLE_BRACE_STATE);
+          }
+        case 64: break;
+        case 39: 
+          { positionInc = 1; return ACRONYM;
+          }
+        case 65: break;
+        case 9: 
+          { if (numLinkToks == 0){positionInc = 0;} else{positionInc = 1;} numWikiTokensSeen++; currentTokType = EXTERNAL_LINK; yybegin(EXTERNAL_LINK_STATE); numLinkToks++; return currentTokType;
+          }
+        case 66: break;
+        case 22: 
+          { numWikiTokensSeen = 0; positionInc = 1; if (numBalanced == 0){numBalanced++;yybegin(TWO_SINGLE_QUOTES_STATE);} else{numBalanced = 0;}
+          }
+        case 67: break;
+        case 31: 
+          { numBalanced = 0; numWikiTokensSeen = 0; currentTokType = INTERNAL_LINK;yybegin(INTERNAL_LINK_STATE);
+          }
+        case 68: break;
+        case 15: 
+          { currentTokType = SUB_HEADING; numWikiTokensSeen = 0; yybegin(STRING);
+          }
+        case 69: break;
+        case 18: 
+          { /* ignore STRING */
+          }
+        case 70: break;
+        case 42: 
+          { positionInc = 1; numWikiTokensSeen++; yybegin(EXTERNAL_LINK_STATE); return currentTokType;
+          }
+        case 71: break;
+        case 21: 
+          { yybegin(STRING); return currentTokType;/*pipe*/
+          }
+        case 72: break;
+        case 37: 
+          { numBalanced = 0;currentTokType = ALPHANUM;yybegin(YYINITIAL);/*end bold*/
+          }
+        case 73: break;
+        case 33: 
+          { positionInc = 1; return HOST;
+          }
+        case 74: break;
+        case 45: 
+          { numBalanced = 0; numWikiTokensSeen = 0; currentTokType = CATEGORY;yybegin(CATEGORY_STATE);
+          }
+        case 75: break;
+        case 36: 
+          { currentTokType = BOLD_ITALICS;  yybegin(FIVE_SINGLE_QUOTES_STATE);
+          }
+        case 76: break;
+        case 13: 
+          { currentTokType = EXTERNAL_LINK; numWikiTokensSeen = 0; yybegin(EXTERNAL_LINK_STATE);
+          }
+        case 77: break;
+        case 16: 
+          { currentTokType = HEADING; yybegin(DOUBLE_EQUALS_STATE); numWikiTokensSeen++; return currentTokType;
+          }
+        case 78: break;
+        case 12: 
+          { currentTokType = ITALICS; numWikiTokensSeen++;  yybegin(STRING); return currentTokType;/*italics*/
+          }
+        case 79: break;
+        case 6: 
+          { yybegin(CATEGORY_STATE); numWikiTokensSeen++; return currentTokType;
+          }
+        case 80: break;
+        case 32: 
+          { positionInc = 1; return APOSTROPHE;
+          }
+        case 81: break;
+        case 19: 
+          { yybegin(STRING); numWikiTokensSeen++; return currentTokType;/* STRING ALPHANUM*/
+          }
+        case 82: break;
+        case 34: 
+          { positionInc = 1; return NUM;
+          }
+        case 83: break;
+        case 44: 
+          { currentTokType = CATEGORY; numWikiTokensSeen = 0; yybegin(CATEGORY_STATE);
+          }
+        case 84: break;
+        case 2: 
+          { positionInc = 1; return ALPHANUM;
+          }
+        case 85: break;
+        case 35: 
+          { positionInc = 1; return COMPANY;
+          }
+        case 86: break;
+        case 11: 
+          { currentTokType = BOLD;  yybegin(THREE_SINGLE_QUOTES_STATE);
+          }
+        case 87: break;
+        case 29: 
+          { currentTokType = INTERNAL_LINK; numWikiTokensSeen = 0;  yybegin(INTERNAL_LINK_STATE);
+          }
+        case 88: break;
+        case 4: 
+          { numWikiTokensSeen = 0; positionInc = 1; currentTokType = EXTERNAL_LINK_URL; yybegin(EXTERNAL_LINK_STATE);
+          }
+        case 89: break;
+        case 27: 
+          { numLinkToks = 0; yybegin(YYINITIAL);
+          }
+        case 90: break;
+        default: 
+          if (zzInput == YYEOF && zzStartRead == zzCurrentPos) {
+            zzAtEOF = true;
+            return YYEOF;
+          } 
+          else {
+            zzScanError(ZZ_NO_MATCH);
+          }
+      }
+    }
+  }
+
+
+}
diff --git a/lucene/contrib/analyzers/common/src/java/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerImpl.jflex b/lucene/contrib/analyzers/common/src/java/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerImpl.jflex
new file mode 100644
index 0000000..9b2c0af
--- /dev/null
+++ b/lucene/contrib/analyzers/common/src/java/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerImpl.jflex
@@ -0,0 +1,330 @@
+package org.apache.lucene.analysis.wikipedia;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.analysis.Token;
+
+%%
+
+%class WikipediaTokenizerImpl
+%unicode
+%integer
+%function getNextToken
+%pack
+%char
+
+%{
+
+public static final int ALPHANUM          = WikipediaTokenizer.ALPHANUM_ID;
+public static final int APOSTROPHE        = WikipediaTokenizer.APOSTROPHE_ID;
+public static final int ACRONYM           = WikipediaTokenizer.ACRONYM_ID;
+public static final int COMPANY           = WikipediaTokenizer.COMPANY_ID;
+public static final int EMAIL             = WikipediaTokenizer.EMAIL_ID;
+public static final int HOST              = WikipediaTokenizer.HOST_ID;
+public static final int NUM               = WikipediaTokenizer.NUM_ID;
+public static final int CJ                = WikipediaTokenizer.CJ_ID;
+public static final int INTERNAL_LINK     = WikipediaTokenizer.INTERNAL_LINK_ID;
+public static final int EXTERNAL_LINK     = WikipediaTokenizer.EXTERNAL_LINK_ID;
+public static final int CITATION          = WikipediaTokenizer.CITATION_ID;
+public static final int CATEGORY          = WikipediaTokenizer.CATEGORY_ID;
+public static final int BOLD              = WikipediaTokenizer.BOLD_ID;
+public static final int ITALICS           = WikipediaTokenizer.ITALICS_ID;
+public static final int BOLD_ITALICS      = WikipediaTokenizer.BOLD_ITALICS_ID;
+public static final int HEADING           = WikipediaTokenizer.HEADING_ID;
+public static final int SUB_HEADING       = WikipediaTokenizer.SUB_HEADING_ID;
+public static final int EXTERNAL_LINK_URL = WikipediaTokenizer.EXTERNAL_LINK_URL_ID;
+
+
+private int currentTokType;
+private int numBalanced = 0;
+private int positionInc = 1;
+private int numLinkToks = 0;
+//Anytime we start a new on a Wiki reserved token (category, link, etc.) this value will be 0, otherwise it will be the number of tokens seen
+//this can be useful for detecting when a new reserved token is encountered
+//see https://issues.apache.org/jira/browse/LUCENE-1133
+private int numWikiTokensSeen = 0;
+
+public static final String [] TOKEN_TYPES = WikipediaTokenizer.TOKEN_TYPES;
+
+/**
+Returns the number of tokens seen inside a category or link, etc.
+@return the number of tokens seen inside the context of wiki syntax.
+**/
+public final int getNumWikiTokensSeen(){
+  return numWikiTokensSeen;
+}
+
+public final int yychar()
+{
+    return yychar;
+}
+
+public final int getPositionIncrement(){
+  return positionInc;
+}
+
+/**
+ * Fills Lucene token with the current token text.
+ */
+final void getText(Token t) {
+  t.setTermBuffer(zzBuffer, zzStartRead, zzMarkedPos-zzStartRead);
+}
+
+final int setText(StringBuilder buffer){
+  int length = zzMarkedPos - zzStartRead;
+  buffer.append(zzBuffer, zzStartRead, length);
+  return length;
+}
+
+
+%}
+
+// basic word: a sequence of digits & letters
+ALPHANUM   = ({LETTER}|{DIGIT}|{KOREAN})+
+
+// internal apostrophes: O'Reilly, you're, O'Reilly's
+// use a post-filter to remove possesives
+APOSTROPHE =  {ALPHA} ("'" {ALPHA})+
+
+// acronyms: U.S.A., I.B.M., etc.
+// use a post-filter to remove dots
+ACRONYM    =  {ALPHA} "." ({ALPHA} ".")+
+
+// company names like AT&T and Excite@Home.
+COMPANY    =  {ALPHA} ("&"|"@") {ALPHA}
+
+// email addresses
+EMAIL      =  {ALPHANUM} (("."|"-"|"_") {ALPHANUM})* "@" {ALPHANUM} (("."|"-") {ALPHANUM})+
+
+// hostname
+HOST       =  {ALPHANUM} ((".") {ALPHANUM})+
+
+// floating point, serial, model numbers, ip addresses, etc.
+// every other segment must have at least one digit
+NUM        = ({ALPHANUM} {P} {HAS_DIGIT}
+           | {DIGIT}+ {P} {DIGIT}+
+           | {HAS_DIGIT} {P} {ALPHANUM}
+           | {ALPHANUM} ({P} {HAS_DIGIT} {P} {ALPHANUM})+
+           | {HAS_DIGIT} ({P} {ALPHANUM} {P} {HAS_DIGIT})+
+           | {ALPHANUM} {P} {HAS_DIGIT} ({P} {ALPHANUM} {P} {HAS_DIGIT})+
+           | {HAS_DIGIT} {P} {ALPHANUM} ({P} {HAS_DIGIT} {P} {ALPHANUM})+)
+
+TAGS = "<"\/?{ALPHANUM}({WHITESPACE}*{ALPHANUM}=\"{ALPHANUM}\")*">"
+
+// punctuation
+P	         = ("_"|"-"|"/"|"."|",")
+
+// at least one digit
+HAS_DIGIT  =
+    ({LETTER}|{DIGIT})*
+    {DIGIT}
+    ({LETTER}|{DIGIT})*
+
+ALPHA      = ({LETTER})+
+
+
+LETTER     = [\u0041-\u005a\u0061-\u007a\u00c0-\u00d6\u00d8-\u00f6\u00f8-\u00ff\u0100-\u1fff\uffa0-\uffdc]
+
+DIGIT      = [\u0030-\u0039\u0660-\u0669\u06f0-\u06f9\u0966-\u096f\u09e6-\u09ef\u0a66-\u0a6f\u0ae6-\u0aef\u0b66-\u0b6f\u0be7-\u0bef\u0c66-\u0c6f\u0ce6-\u0cef\u0d66-\u0d6f\u0e50-\u0e59\u0ed0-\u0ed9\u1040-\u1049]
+
+KOREAN     = [\uac00-\ud7af\u1100-\u11ff]
+
+// Chinese, Japanese
+CJ         = [\u3040-\u318f\u3100-\u312f\u3040-\u309F\u30A0-\u30FF\u31F0-\u31FF\u3300-\u337f\u3400-\u4dbf\u4e00-\u9fff\uf900-\ufaff\uff65-\uff9f]
+
+WHITESPACE = \r\n | [ \r\n\t\f]
+
+//Wikipedia
+DOUBLE_BRACKET = "["{2}
+DOUBLE_BRACKET_CLOSE = "]"{2}
+DOUBLE_BRACKET_CAT = "["{2}":"?"Category:"
+EXTERNAL_LINK = "["
+TWO_SINGLE_QUOTES = "'"{2}
+CITATION = "<ref>"
+CITATION_CLOSE = "</ref>"
+INFOBOX = {DOUBLE_BRACE}("I"|"i")nfobox_
+
+DOUBLE_BRACE = "{"{2}
+DOUBLE_BRACE_CLOSE = "}"{2}
+PIPE = "|"
+DOUBLE_EQUALS = "="{2}
+
+
+%state CATEGORY_STATE
+%state INTERNAL_LINK_STATE
+%state EXTERNAL_LINK_STATE
+
+%state TWO_SINGLE_QUOTES_STATE
+%state THREE_SINGLE_QUOTES_STATE
+%state FIVE_SINGLE_QUOTES_STATE
+%state DOUBLE_EQUALS_STATE
+%state DOUBLE_BRACE_STATE
+%state STRING
+
+%%
+
+<YYINITIAL>{ALPHANUM}                                                     {positionInc = 1; return ALPHANUM; }
+<YYINITIAL>{APOSTROPHE}                                                   {positionInc = 1; return APOSTROPHE; }
+<YYINITIAL>{ACRONYM}                                                      {positionInc = 1; return ACRONYM; }
+<YYINITIAL>{COMPANY}                                                      {positionInc = 1; return COMPANY; }
+<YYINITIAL>{EMAIL}                                                        {positionInc = 1; return EMAIL; }
+<YYINITIAL>{NUM}                                                          {positionInc = 1; return NUM; }
+<YYINITIAL>{HOST}                                                         {positionInc = 1; return HOST; }
+<YYINITIAL>{CJ}                                                           {positionInc = 1; return CJ; }
+
+//wikipedia
+<YYINITIAL>{
+  //First {ALPHANUM} is always the link, set positioninc to 1 for double bracket, but then inside the internal link state
+  //set it to 0 for the next token, such that the link and the first token are in the same position, but then subsequent
+  //tokens within the link are incremented
+  {DOUBLE_BRACKET} {numWikiTokensSeen = 0; positionInc = 1; currentTokType = INTERNAL_LINK; yybegin(INTERNAL_LINK_STATE);}
+  {DOUBLE_BRACKET_CAT} {numWikiTokensSeen = 0; positionInc = 1; currentTokType = CATEGORY; yybegin(CATEGORY_STATE);}
+  {EXTERNAL_LINK} {numWikiTokensSeen = 0; positionInc = 1; currentTokType = EXTERNAL_LINK_URL; yybegin(EXTERNAL_LINK_STATE);}
+  {TWO_SINGLE_QUOTES} {numWikiTokensSeen = 0; positionInc = 1; if (numBalanced == 0){numBalanced++;yybegin(TWO_SINGLE_QUOTES_STATE);} else{numBalanced = 0;}}
+  {DOUBLE_EQUALS} {numWikiTokensSeen = 0; positionInc = 1; yybegin(DOUBLE_EQUALS_STATE);}
+  {DOUBLE_BRACE} {numWikiTokensSeen = 0; positionInc = 1; currentTokType = CITATION; yybegin(DOUBLE_BRACE_STATE);}
+  {CITATION} {numWikiTokensSeen = 0; positionInc = 1; currentTokType = CITATION; yybegin(DOUBLE_BRACE_STATE);}
+//ignore
+  . | {WHITESPACE} |{INFOBOX}                                               {numWikiTokensSeen = 0;  positionInc = 1; }
+}
+
+<INTERNAL_LINK_STATE>{
+//First {ALPHANUM} is always the link, set position to 0 for these
+//This is slightly different from EXTERNAL_LINK_STATE because that one has an explicit grammar for capturing the URL
+  {ALPHANUM} {yybegin(INTERNAL_LINK_STATE); numWikiTokensSeen++; return currentTokType;}
+  {DOUBLE_BRACKET_CLOSE} {numLinkToks = 0; yybegin(YYINITIAL);}
+  //ignore
+  . | {WHITESPACE}                                               { positionInc = 1; }
+}
+
+<EXTERNAL_LINK_STATE>{
+//increment the link token, but then don't increment the tokens after that which are still in the link
+  ("http://"|"https://"){HOST}("/"?({ALPHANUM}|{P}|\?|"&"|"="|"#")*)* {positionInc = 1; numWikiTokensSeen++; yybegin(EXTERNAL_LINK_STATE); return currentTokType;}
+  {ALPHANUM} {if (numLinkToks == 0){positionInc = 0;} else{positionInc = 1;} numWikiTokensSeen++; currentTokType = EXTERNAL_LINK; yybegin(EXTERNAL_LINK_STATE); numLinkToks++; return currentTokType;}
+  "]" {numLinkToks = 0; positionInc = 0; yybegin(YYINITIAL);}
+  {WHITESPACE}                                               { positionInc = 1; }
+}
+
+<CATEGORY_STATE>{
+  {ALPHANUM} {yybegin(CATEGORY_STATE); numWikiTokensSeen++; return currentTokType;}
+  {DOUBLE_BRACKET_CLOSE} {yybegin(YYINITIAL);}
+  //ignore
+  . | {WHITESPACE}                                               { positionInc = 1; }
+}
+//italics
+<TWO_SINGLE_QUOTES_STATE>{
+  "'" {currentTokType = BOLD;  yybegin(THREE_SINGLE_QUOTES_STATE);}
+   "'''" {currentTokType = BOLD_ITALICS;  yybegin(FIVE_SINGLE_QUOTES_STATE);}
+   {ALPHANUM} {currentTokType = ITALICS; numWikiTokensSeen++;  yybegin(STRING); return currentTokType;/*italics*/}
+   //we can have links inside, let those override
+   {DOUBLE_BRACKET} {currentTokType = INTERNAL_LINK; numWikiTokensSeen = 0; yybegin(INTERNAL_LINK_STATE);}
+   {DOUBLE_BRACKET_CAT} {currentTokType = CATEGORY; numWikiTokensSeen = 0; yybegin(CATEGORY_STATE);}
+   {EXTERNAL_LINK} {currentTokType = EXTERNAL_LINK; numWikiTokensSeen = 0; yybegin(EXTERNAL_LINK_STATE);}
+
+   //ignore
+  . | {WHITESPACE}                                               { /* ignore */ }
+}
+//bold
+<THREE_SINGLE_QUOTES_STATE>{
+  {ALPHANUM} {yybegin(STRING); numWikiTokensSeen++; return currentTokType;}
+  //we can have links inside, let those override
+   {DOUBLE_BRACKET} {currentTokType = INTERNAL_LINK; numWikiTokensSeen = 0; yybegin(INTERNAL_LINK_STATE);}
+   {DOUBLE_BRACKET_CAT} {currentTokType = CATEGORY; numWikiTokensSeen = 0; yybegin(CATEGORY_STATE);}
+   {EXTERNAL_LINK} {currentTokType = EXTERNAL_LINK; numWikiTokensSeen = 0; yybegin(EXTERNAL_LINK_STATE);}
+
+   //ignore
+  . | {WHITESPACE}                                               { /* ignore */ }
+
+}
+//bold italics
+<FIVE_SINGLE_QUOTES_STATE>{
+  {ALPHANUM} {yybegin(STRING); numWikiTokensSeen++; return currentTokType;}
+  //we can have links inside, let those override
+   {DOUBLE_BRACKET} {currentTokType = INTERNAL_LINK; numWikiTokensSeen = 0;  yybegin(INTERNAL_LINK_STATE);}
+   {DOUBLE_BRACKET_CAT} {currentTokType = CATEGORY; numWikiTokensSeen = 0; yybegin(CATEGORY_STATE);}
+   {EXTERNAL_LINK} {currentTokType = EXTERNAL_LINK; numWikiTokensSeen = 0; yybegin(EXTERNAL_LINK_STATE);}
+
+   //ignore
+  . | {WHITESPACE}                                               { /* ignore */ }
+}
+
+<DOUBLE_EQUALS_STATE>{
+ "=" {currentTokType = SUB_HEADING; numWikiTokensSeen = 0; yybegin(STRING);}
+ {ALPHANUM} {currentTokType = HEADING; yybegin(DOUBLE_EQUALS_STATE); numWikiTokensSeen++; return currentTokType;}
+ {DOUBLE_EQUALS} {yybegin(YYINITIAL);}
+  //ignore
+  . | {WHITESPACE}                                               { /* ignore */ }
+}
+
+<DOUBLE_BRACE_STATE>{
+  {ALPHANUM} {yybegin(DOUBLE_BRACE_STATE); numWikiTokensSeen = 0; return currentTokType;}
+  {DOUBLE_BRACE_CLOSE} {yybegin(YYINITIAL);}
+  {CITATION_CLOSE} {yybegin(YYINITIAL);}
+   //ignore
+  . | {WHITESPACE}                                               { /* ignore */ }
+}
+
+<STRING> {
+  "'''''" {numBalanced = 0;currentTokType = ALPHANUM; yybegin(YYINITIAL);/*end bold italics*/}
+  "'''" {numBalanced = 0;currentTokType = ALPHANUM;yybegin(YYINITIAL);/*end bold*/}
+  "''" {numBalanced = 0;currentTokType = ALPHANUM; yybegin(YYINITIAL);/*end italics*/}
+  "===" {numBalanced = 0;currentTokType = ALPHANUM; yybegin(YYINITIAL);/*end sub header*/}
+  {ALPHANUM} {yybegin(STRING); numWikiTokensSeen++; return currentTokType;/* STRING ALPHANUM*/}
+  //we can have links inside, let those override
+   {DOUBLE_BRACKET} {numBalanced = 0; numWikiTokensSeen = 0; currentTokType = INTERNAL_LINK;yybegin(INTERNAL_LINK_STATE);}
+   {DOUBLE_BRACKET_CAT} {numBalanced = 0; numWikiTokensSeen = 0; currentTokType = CATEGORY;yybegin(CATEGORY_STATE);}
+   {EXTERNAL_LINK} {numBalanced = 0; numWikiTokensSeen = 0; currentTokType = EXTERNAL_LINK;yybegin(EXTERNAL_LINK_STATE);}
+
+
+  {PIPE} {yybegin(STRING); return currentTokType;/*pipe*/}
+
+  .|{WHITESPACE}                                              { /* ignore STRING */ }
+}
+
+
+
+
+/*
+{INTERNAL_LINK}                                                { return curentTokType; }
+
+{CITATION}                                                { return currentTokType; }
+{CATEGORY}                                                { return currentTokType; }
+
+{BOLD}                                                { return currentTokType; }
+{ITALICS}                                                { return currentTokType; }
+{BOLD_ITALICS}                                                { return currentTokType; }
+{HEADING}                                                { return currentTokType; }
+{SUB_HEADING}                                                { return currentTokType; }
+
+*/
+//end wikipedia
+
+/** Ignore the rest */
+. | {WHITESPACE}|{TAGS}                                                { /* ignore */ }
+
+
+//INTERNAL_LINK = "["{2}({ALPHANUM}+{WHITESPACE}*)+"]"{2}
+//EXTERNAL_LINK = "["http://"{HOST}.*?"]"
+//CITATION = "{"{2}({ALPHANUM}+{WHITESPACE}*)+"}"{2}
+//CATEGORY = "["{2}"Category:"({ALPHANUM}+{WHITESPACE}*)+"]"{2}
+//CATEGORY_COLON = "["{2}":Category:"({ALPHANUM}+{WHITESPACE}*)+"]"{2}
+//BOLD = '''({ALPHANUM}+{WHITESPACE}*)+'''
+//ITALICS = ''({ALPHANUM}+{WHITESPACE}*)+''
+//BOLD_ITALICS = '''''({ALPHANUM}+{WHITESPACE}*)+'''''
+//HEADING = "="{2}({ALPHANUM}+{WHITESPACE}*)+"="{2}
+//SUB_HEADING ="="{3}({ALPHANUM}+{WHITESPACE}*)+"="{3}
diff --git a/lucene/contrib/analyzers/common/src/java/org/apache/lucene/analysis/wikipedia/package.html b/lucene/contrib/analyzers/common/src/java/org/apache/lucene/analysis/wikipedia/package.html
new file mode 100644
index 0000000..7b23869
--- /dev/null
+++ b/lucene/contrib/analyzers/common/src/java/org/apache/lucene/analysis/wikipedia/package.html
@@ -0,0 +1,22 @@
+<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+<html>
+<body>
+Tokenizer that is aware of Wikipedia syntax.
+</body>
+</html>
diff --git a/lucene/contrib/analyzers/common/src/test/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerTest.java b/lucene/contrib/analyzers/common/src/test/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerTest.java
new file mode 100644
index 0000000..300595b
--- /dev/null
+++ b/lucene/contrib/analyzers/common/src/test/org/apache/lucene/analysis/wikipedia/WikipediaTokenizerTest.java
@@ -0,0 +1,558 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+package org.apache.lucene.analysis.wikipedia;
+
+import java.io.StringReader;
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.Set;
+import java.util.HashSet;
+
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
+import org.apache.lucene.analysis.tokenattributes.FlagsAttribute;
+import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
+import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
+import org.apache.lucene.analysis.tokenattributes.TermAttribute;
+import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
+
+
+/**
+ *
+ *
+ **/
+public class WikipediaTokenizerTest extends BaseTokenStreamTestCase {
+  protected static final String LINK_PHRASES = "click [[link here again]] click [http://lucene.apache.org here again] [[Category:a b c d]]";
+
+  public WikipediaTokenizerTest(String s) {
+    super(s);
+  }
+
+  public void testSimple() throws Exception {
+    String text = "This is a [[Category:foo]]";
+    WikipediaTokenizer tf = new WikipediaTokenizer(new StringReader(text));
+    assertTokenStreamContents(tf,
+        new String[] { "This", "is", "a", "foo" },
+        new int[] { 0, 5, 8, 21 },
+        new int[] { 4, 7, 9, 24 },
+        new String[] { "<ALPHANUM>", "<ALPHANUM>", "<ALPHANUM>", WikipediaTokenizer.CATEGORY },
+        new int[] { 1, 1, 1, 1, },
+        text.length());
+  }
+  
+  public void testHandwritten() throws Exception {
+    //make sure all tokens are in only one type
+    String test = "[[link]] This is a [[Category:foo]] Category  This is a linked [[:Category:bar none withstanding]] " +
+            "Category This is (parens) This is a [[link]]  This is an external URL [http://lucene.apache.org] " +
+            "Here is ''italics'' and ''more italics'', '''bold''' and '''''five quotes''''' " +
+            " This is a [[link|display info]]  This is a period.  Here is $3.25 and here is 3.50.  Here's Johnny.  " +
+            "==heading== ===sub head=== followed by some text  [[Category:blah| ]] " +
+            "''[[Category:ital_cat]]''  here is some that is ''italics [[Category:foo]] but is never closed." +
+            "'''same [[Category:foo]] goes for this '''''and2 [[Category:foo]] and this" +
+            " [http://foo.boo.com/test/test/ Test Test] [http://foo.boo.com/test/test/test.html Test Test]" +
+            " [http://foo.boo.com/test/test/test.html?g=b&c=d Test Test] <ref>Citation</ref> <sup>martian</sup> <span class=\"glue\">code</span>";
+    Map<String,String> tcm = new HashMap<String,String>();//map tokens to types
+    tcm.put("link", WikipediaTokenizer.INTERNAL_LINK);
+    tcm.put("display", WikipediaTokenizer.INTERNAL_LINK);
+    tcm.put("info", WikipediaTokenizer.INTERNAL_LINK);
+
+    tcm.put("http://lucene.apache.org", WikipediaTokenizer.EXTERNAL_LINK_URL);
+    tcm.put("http://foo.boo.com/test/test/", WikipediaTokenizer.EXTERNAL_LINK_URL);
+    tcm.put("http://foo.boo.com/test/test/test.html", WikipediaTokenizer.EXTERNAL_LINK_URL);
+    tcm.put("http://foo.boo.com/test/test/test.html?g=b&c=d", WikipediaTokenizer.EXTERNAL_LINK_URL);
+    tcm.put("Test", WikipediaTokenizer.EXTERNAL_LINK);
+    
+    //alphanums
+    tcm.put("This", "<ALPHANUM>");
+    tcm.put("is", "<ALPHANUM>");
+    tcm.put("a", "<ALPHANUM>");
+    tcm.put("Category", "<ALPHANUM>");
+    tcm.put("linked", "<ALPHANUM>");
+    tcm.put("parens", "<ALPHANUM>");
+    tcm.put("external", "<ALPHANUM>");
+    tcm.put("URL", "<ALPHANUM>");
+    tcm.put("and", "<ALPHANUM>");
+    tcm.put("period", "<ALPHANUM>");
+    tcm.put("Here", "<ALPHANUM>");
+    tcm.put("Here's", "<APOSTROPHE>");
+    tcm.put("here", "<ALPHANUM>");
+    tcm.put("Johnny", "<ALPHANUM>");
+    tcm.put("followed", "<ALPHANUM>");
+    tcm.put("by", "<ALPHANUM>");
+    tcm.put("text", "<ALPHANUM>");
+    tcm.put("that", "<ALPHANUM>");
+    tcm.put("but", "<ALPHANUM>");
+    tcm.put("never", "<ALPHANUM>");
+    tcm.put("closed", "<ALPHANUM>");
+    tcm.put("goes", "<ALPHANUM>");
+    tcm.put("for", "<ALPHANUM>");
+    tcm.put("this", "<ALPHANUM>");
+    tcm.put("an", "<ALPHANUM>");
+    tcm.put("some", "<ALPHANUM>");
+    tcm.put("martian", "<ALPHANUM>");
+    tcm.put("code", "<ALPHANUM>");
+
+    tcm.put("foo", WikipediaTokenizer.CATEGORY);
+    tcm.put("bar", WikipediaTokenizer.CATEGORY);
+    tcm.put("none", WikipediaTokenizer.CATEGORY);
+    tcm.put("withstanding", WikipediaTokenizer.CATEGORY);
+    tcm.put("blah", WikipediaTokenizer.CATEGORY);
+    tcm.put("ital", WikipediaTokenizer.CATEGORY);
+    tcm.put("cat", WikipediaTokenizer.CATEGORY);
+
+    tcm.put("italics", WikipediaTokenizer.ITALICS);
+    tcm.put("more", WikipediaTokenizer.ITALICS);
+    tcm.put("bold", WikipediaTokenizer.BOLD);
+    tcm.put("same", WikipediaTokenizer.BOLD);
+    tcm.put("five", WikipediaTokenizer.BOLD_ITALICS);
+    tcm.put("and2", WikipediaTokenizer.BOLD_ITALICS);
+    tcm.put("quotes", WikipediaTokenizer.BOLD_ITALICS);
+
+    tcm.put("heading", WikipediaTokenizer.HEADING);
+    tcm.put("sub", WikipediaTokenizer.SUB_HEADING);
+    tcm.put("head", WikipediaTokenizer.SUB_HEADING);
+    
+    tcm.put("Citation", WikipediaTokenizer.CITATION);
+
+    tcm.put("3.25", "<NUM>");
+    tcm.put("3.50", "<NUM>");
+    WikipediaTokenizer tf = new WikipediaTokenizer(new StringReader(test));
+    int count = 0;
+    int numItalics = 0;
+    int numBoldItalics = 0;
+    int numCategory = 0;
+    int numCitation = 0;
+    TermAttribute termAtt = tf.addAttribute(TermAttribute.class);
+    TypeAttribute typeAtt = tf.addAttribute(TypeAttribute.class);
+    
+    while (tf.incrementToken()) {
+      String tokText = termAtt.term();
+      //System.out.println("Text: " + tokText + " Type: " + token.type());
+      String expectedType = tcm.get(tokText);
+      assertTrue("expectedType is null and it shouldn't be for: " + tf.toString(), expectedType != null);
+      assertTrue(typeAtt.type() + " is not equal to " + expectedType + " for " + tf.toString(), typeAtt.type().equals(expectedType) == true);
+      count++;
+      if (typeAtt.type().equals(WikipediaTokenizer.ITALICS)  == true){
+        numItalics++;
+      } else if (typeAtt.type().equals(WikipediaTokenizer.BOLD_ITALICS)  == true){
+        numBoldItalics++;
+      } else if (typeAtt.type().equals(WikipediaTokenizer.CATEGORY)  == true){
+        numCategory++;
+      }
+      else if (typeAtt.type().equals(WikipediaTokenizer.CITATION)  == true){
+        numCitation++;
+      }
+    }
+    assertTrue("We have not seen enough tokens: " + count + " is not >= " + tcm.size(), count >= tcm.size());
+    assertTrue(numItalics + " does not equal: " + 4 + " for numItalics", numItalics == 4);
+    assertTrue(numBoldItalics + " does not equal: " + 3 + " for numBoldItalics", numBoldItalics == 3);
+    assertTrue(numCategory + " does not equal: " + 10 + " for numCategory", numCategory == 10);
+    assertTrue(numCitation + " does not equal: " + 1 + " for numCitation", numCitation == 1);
+  }
+
+  public void testLinkPhrases() throws Exception {
+
+    WikipediaTokenizer tf = new WikipediaTokenizer(new StringReader(LINK_PHRASES));
+    checkLinkPhrases(tf);
+    
+  }
+
+  private void checkLinkPhrases(WikipediaTokenizer tf) throws IOException {
+    TermAttribute termAtt = tf.addAttribute(TermAttribute.class);
+    PositionIncrementAttribute posIncrAtt = tf.addAttribute(PositionIncrementAttribute.class);
+    
+    assertTrue(tf.incrementToken());
+    assertTrue(termAtt.term() + " is not equal to " + "click", termAtt.term().equals("click") == true);
+    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 1, posIncrAtt.getPositionIncrement() == 1);
+    assertTrue(tf.incrementToken());
+    assertTrue(termAtt.term() + " is not equal to " + "link", termAtt.term().equals("link") == true);
+    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 1, posIncrAtt.getPositionIncrement() == 1);
+    assertTrue(tf.incrementToken());
+    assertTrue(termAtt.term() + " is not equal to " + "here",
+        termAtt.term().equals("here") == true);
+    //The link, and here should be at the same position for phrases to work
+    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 1, posIncrAtt.getPositionIncrement() == 1);
+    assertTrue(tf.incrementToken());
+    assertTrue(termAtt.term() + " is not equal to " + "again",
+        termAtt.term().equals("again") == true);
+    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 1, posIncrAtt.getPositionIncrement() == 1);
+
+    assertTrue(tf.incrementToken());
+    assertTrue(termAtt.term() + " is not equal to " + "click",
+        termAtt.term().equals("click") == true);
+    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 1, posIncrAtt.getPositionIncrement() == 1);
+
+    assertTrue(tf.incrementToken());
+    assertTrue(termAtt.term() + " is not equal to " + "http://lucene.apache.org",
+        termAtt.term().equals("http://lucene.apache.org") == true);
+    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 1, posIncrAtt.getPositionIncrement() == 1);
+
+    assertTrue(tf.incrementToken());
+    assertTrue(termAtt.term() + " is not equal to " + "here",
+        termAtt.term().equals("here") == true);
+    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 0, posIncrAtt.getPositionIncrement() == 0);
+
+    assertTrue(tf.incrementToken());
+    assertTrue(termAtt.term() + " is not equal to " + "again",
+        termAtt.term().equals("again") == true);
+    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 1, posIncrAtt.getPositionIncrement() == 1);
+
+    assertTrue(tf.incrementToken());
+    assertTrue(termAtt.term() + " is not equal to " + "a",
+        termAtt.term().equals("a") == true);
+    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 1, posIncrAtt.getPositionIncrement() == 1);
+
+    assertTrue(tf.incrementToken());
+    assertTrue(termAtt.term() + " is not equal to " + "b",
+        termAtt.term().equals("b") == true);
+    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 1, posIncrAtt.getPositionIncrement() == 1);
+
+    assertTrue(tf.incrementToken());
+    assertTrue(termAtt.term() + " is not equal to " + "c",
+        termAtt.term().equals("c") == true);
+    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 1, posIncrAtt.getPositionIncrement() == 1);
+
+    assertTrue(tf.incrementToken());
+    assertTrue(termAtt.term() + " is not equal to " + "d",
+        termAtt.term().equals("d") == true);
+    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 1, posIncrAtt.getPositionIncrement() == 1);
+
+    assertFalse(tf.incrementToken());  
+  }
+
+  public void testLinks() throws Exception {
+    String test = "[http://lucene.apache.org/java/docs/index.html#news here] [http://lucene.apache.org/java/docs/index.html?b=c here] [https://lucene.apache.org/java/docs/index.html?b=c here]";
+    WikipediaTokenizer tf = new WikipediaTokenizer(new StringReader(test));
+    TermAttribute termAtt = tf.addAttribute(TermAttribute.class);
+    TypeAttribute typeAtt = tf.addAttribute(TypeAttribute.class);
+    
+    assertTrue(tf.incrementToken());
+    assertTrue(termAtt.term() + " is not equal to " + "http://lucene.apache.org/java/docs/index.html#news",
+        termAtt.term().equals("http://lucene.apache.org/java/docs/index.html#news") == true);
+    assertTrue(typeAtt.type() + " is not equal to " + WikipediaTokenizer.EXTERNAL_LINK_URL, typeAtt.type().equals(WikipediaTokenizer.EXTERNAL_LINK_URL) == true);
+    tf.incrementToken();//skip here
+    
+    assertTrue(tf.incrementToken());
+    assertTrue(termAtt.term() + " is not equal to " + "http://lucene.apache.org/java/docs/index.html?b=c",
+        termAtt.term().equals("http://lucene.apache.org/java/docs/index.html?b=c") == true);
+    assertTrue(typeAtt.type() + " is not equal to " + WikipediaTokenizer.EXTERNAL_LINK_URL, typeAtt.type().equals(WikipediaTokenizer.EXTERNAL_LINK_URL) == true);
+    tf.incrementToken();//skip here
+    
+    assertTrue(tf.incrementToken());
+    assertTrue(termAtt.term() + " is not equal to " + "https://lucene.apache.org/java/docs/index.html?b=c",
+        termAtt.term().equals("https://lucene.apache.org/java/docs/index.html?b=c") == true);
+    assertTrue(typeAtt.type() + " is not equal to " + WikipediaTokenizer.EXTERNAL_LINK_URL, typeAtt.type().equals(WikipediaTokenizer.EXTERNAL_LINK_URL) == true);
+    
+    assertTrue(tf.incrementToken());
+    assertFalse(tf.incrementToken());
+  }
+
+  public void testLucene1133() throws Exception {
+    Set<String> untoks = new HashSet<String>();
+    untoks.add(WikipediaTokenizer.CATEGORY);
+    untoks.add(WikipediaTokenizer.ITALICS);
+    //should be exactly the same, regardless of untoks
+    WikipediaTokenizer tf = new WikipediaTokenizer(new StringReader(LINK_PHRASES), WikipediaTokenizer.TOKENS_ONLY, untoks);
+    checkLinkPhrases(tf);
+    String test = "[[Category:a b c d]] [[Category:e f g]] [[link here]] [[link there]] ''italics here'' something ''more italics'' [[Category:h   i   j]]";
+    tf = new WikipediaTokenizer(new StringReader(test), WikipediaTokenizer.UNTOKENIZED_ONLY, untoks);
+    TermAttribute termAtt = tf.addAttribute(TermAttribute.class);
+    PositionIncrementAttribute posIncrAtt = tf.addAttribute(PositionIncrementAttribute.class);
+    OffsetAttribute offsetAtt = tf.addAttribute(OffsetAttribute.class);
+    
+    assertTrue(tf.incrementToken());
+    assertTrue(termAtt.term() + " is not equal to " + "a b c d",
+        termAtt.term().equals("a b c d") == true);
+    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 1, posIncrAtt.getPositionIncrement() == 1);
+    assertTrue(offsetAtt.startOffset() + " does not equal: " + 11, offsetAtt.startOffset() == 11);
+    assertTrue(offsetAtt.endOffset() + " does not equal: " + 18, offsetAtt.endOffset() == 18);
+    
+    assertTrue(tf.incrementToken());
+    assertTrue(termAtt.term() + " is not equal to " + "e f g",
+        termAtt.term().equals("e f g") == true);
+    assertTrue(offsetAtt.startOffset() + " does not equal: " + 32, offsetAtt.startOffset() == 32);
+    assertTrue(offsetAtt.endOffset() + " does not equal: " + 37, offsetAtt.endOffset() == 37);
+    
+    assertTrue(tf.incrementToken());
+    assertTrue(termAtt.term() + " is not equal to " + "link",
+        termAtt.term().equals("link") == true);
+    assertTrue(offsetAtt.startOffset() + " does not equal: " + 42, offsetAtt.startOffset() == 42);
+    assertTrue(offsetAtt.endOffset() + " does not equal: " + 46, offsetAtt.endOffset() == 46);
+    
+    assertTrue(tf.incrementToken());
+    assertTrue(termAtt.term() + " is not equal to " + "here",
+        termAtt.term().equals("here") == true);
+    assertTrue(offsetAtt.startOffset() + " does not equal: " + 47, offsetAtt.startOffset() == 47);
+    assertTrue(offsetAtt.endOffset() + " does not equal: " + 51, offsetAtt.endOffset() == 51);
+
+    assertTrue(tf.incrementToken());
+    assertTrue(termAtt.term() + " is not equal to " + "link",
+        termAtt.term().equals("link") == true);
+    assertTrue(offsetAtt.startOffset() + " does not equal: " + 56, offsetAtt.startOffset() == 56);
+    assertTrue(offsetAtt.endOffset() + " does not equal: " + 60, offsetAtt.endOffset() == 60);
+    
+    assertTrue(tf.incrementToken());
+    assertTrue(termAtt.term() + " is not equal to " + "there",
+        termAtt.term().equals("there") == true);
+
+    assertTrue(offsetAtt.startOffset() + " does not equal: " + 61, offsetAtt.startOffset() == 61);
+    assertTrue(offsetAtt.endOffset() + " does not equal: " + 66, offsetAtt.endOffset() == 66);
+    
+    assertTrue(tf.incrementToken());
+    assertTrue(termAtt.term() + " is not equal to " + "italics here",
+        termAtt.term().equals("italics here") == true);
+    assertTrue(offsetAtt.startOffset() + " does not equal: " + 71, offsetAtt.startOffset() == 71);
+    assertTrue(offsetAtt.endOffset() + " does not equal: " + 83, offsetAtt.endOffset() == 83);
+    
+    assertTrue(tf.incrementToken());
+    assertTrue(termAtt.term() + " is not equal to " + "something",
+        termAtt.term().equals("something") == true);
+    assertTrue(offsetAtt.startOffset() + " does not equal: " + 86, offsetAtt.startOffset() == 86);
+    assertTrue(offsetAtt.endOffset() + " does not equal: " + 95, offsetAtt.endOffset() == 95);
+    
+    assertTrue(tf.incrementToken());
+    assertTrue(termAtt.term() + " is not equal to " + "more italics",
+        termAtt.term().equals("more italics") == true);
+    assertTrue(offsetAtt.startOffset() + " does not equal: " + 98, offsetAtt.startOffset() == 98);
+    assertTrue(offsetAtt.endOffset() + " does not equal: " + 110, offsetAtt.endOffset() == 110);
+
+    assertTrue(tf.incrementToken());
+    assertTrue(termAtt.term() + " is not equal to " + "h   i   j",
+        termAtt.term().equals("h   i   j") == true);
+    assertTrue(offsetAtt.startOffset() + " does not equal: " + 124, offsetAtt.startOffset() == 124);
+    assertTrue(offsetAtt.endOffset() + " does not equal: " + 133, offsetAtt.endOffset() == 133);
+
+    assertFalse(tf.incrementToken());
+  }
+
+  public void testBoth() throws Exception {
+    Set<String> untoks = new HashSet<String>();
+    untoks.add(WikipediaTokenizer.CATEGORY);
+    untoks.add(WikipediaTokenizer.ITALICS);
+    String test = "[[Category:a b c d]] [[Category:e f g]] [[link here]] [[link there]] ''italics here'' something ''more italics'' [[Category:h   i   j]]";
+    //should output all the indivual tokens plus the untokenized tokens as well.  Untokenized tokens
+    WikipediaTokenizer tf = new WikipediaTokenizer(new StringReader(test), WikipediaTokenizer.BOTH, untoks);
+    TermAttribute termAtt = tf.addAttribute(TermAttribute.class);
+    TypeAttribute typeAtt = tf.addAttribute(TypeAttribute.class);
+    PositionIncrementAttribute posIncrAtt = tf.addAttribute(PositionIncrementAttribute.class);
+    OffsetAttribute offsetAtt = tf.addAttribute(OffsetAttribute.class);
+    FlagsAttribute flagsAtt = tf.addAttribute(FlagsAttribute.class);
+    
+    assertTrue(tf.incrementToken());
+    assertTrue(termAtt.term() + " is not equal to " + "a b c d",
+            termAtt.term().equals("a b c d") == true);
+    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 1, posIncrAtt.getPositionIncrement() == 1);
+    assertTrue(typeAtt.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);
+    assertTrue(flagsAtt.getFlags() + " does not equal: " + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, flagsAtt.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);
+    assertTrue(offsetAtt.startOffset() + " does not equal: " + 11, offsetAtt.startOffset() == 11);
+    assertTrue(offsetAtt.endOffset() + " does not equal: " + 18, offsetAtt.endOffset() == 18);
+    
+    assertTrue(tf.incrementToken());
+    assertTrue(termAtt.term() + " is not equal to " + "a",
+            termAtt.term().equals("a") == true);
+    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 0, posIncrAtt.getPositionIncrement() == 0);
+    assertTrue(typeAtt.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);
+    assertTrue(flagsAtt.getFlags() + " equals: " + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG + " and it shouldn't", flagsAtt.getFlags() != WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);
+    assertTrue(offsetAtt.startOffset() + " does not equal: " + 11, offsetAtt.startOffset() == 11);
+    assertTrue(offsetAtt.endOffset() + " does not equal: " + 12, offsetAtt.endOffset() == 12);
+
+    assertTrue(tf.incrementToken());
+    assertTrue(termAtt.term() + " is not equal to " + "b",
+            termAtt.term().equals("b") == true);
+    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 1, posIncrAtt.getPositionIncrement() == 1);
+    assertTrue(typeAtt.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);
+    assertTrue(offsetAtt.startOffset() + " does not equal: " + 13, offsetAtt.startOffset() == 13);
+    assertTrue(offsetAtt.endOffset() + " does not equal: " + 14, offsetAtt.endOffset() == 14);
+
+    assertTrue(tf.incrementToken());
+    assertTrue(termAtt.term() + " is not equal to " + "c",
+            termAtt.term().equals("c") == true);
+    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 1, posIncrAtt.getPositionIncrement() == 1);
+    assertTrue(typeAtt.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);
+    assertTrue(offsetAtt.startOffset() + " does not equal: " + 15, offsetAtt.startOffset() == 15);
+    assertTrue(offsetAtt.endOffset() + " does not equal: " + 16, offsetAtt.endOffset() == 16);
+
+    assertTrue(tf.incrementToken());
+    assertTrue(termAtt.term() + " is not equal to " + "d",
+            termAtt.term().equals("d") == true);
+    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 1, posIncrAtt.getPositionIncrement() == 1);
+    assertTrue(typeAtt.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);
+    assertTrue(offsetAtt.startOffset() + " does not equal: " + 17, offsetAtt.startOffset() == 17);
+    assertTrue(offsetAtt.endOffset() + " does not equal: " + 18, offsetAtt.endOffset() == 18);
+
+
+
+    assertTrue(tf.incrementToken());
+    assertTrue(termAtt.term() + " is not equal to " + "e f g",
+            termAtt.term().equals("e f g") == true);
+    assertTrue(typeAtt.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);
+    assertTrue(flagsAtt.getFlags() + " does not equal: " + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, flagsAtt.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);
+    assertTrue(offsetAtt.startOffset() + " does not equal: " + 32, offsetAtt.startOffset() == 32);
+    assertTrue(offsetAtt.endOffset() + " does not equal: " + 37, offsetAtt.endOffset() == 37);
+
+    assertTrue(tf.incrementToken());
+    assertTrue(termAtt.term() + " is not equal to " + "e",
+            termAtt.term().equals("e") == true);
+    assertTrue(typeAtt.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);
+    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 0, posIncrAtt.getPositionIncrement() == 0);
+    assertTrue(offsetAtt.startOffset() + " does not equal: " + 32, offsetAtt.startOffset() == 32);
+    assertTrue(offsetAtt.endOffset() + " does not equal: " + 33, offsetAtt.endOffset() == 33);
+
+    assertTrue(tf.incrementToken());
+    assertTrue(termAtt.term() + " is not equal to " + "f",
+            termAtt.term().equals("f") == true);
+    assertTrue(typeAtt.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);
+    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 1, posIncrAtt.getPositionIncrement() == 1);
+    assertTrue(offsetAtt.startOffset() + " does not equal: " + 34, offsetAtt.startOffset() == 34);
+    assertTrue(offsetAtt.endOffset() + " does not equal: " + 35, offsetAtt.endOffset() == 35);
+
+    assertTrue(tf.incrementToken());
+    assertTrue(termAtt.term() + " is not equal to " + "g",
+            termAtt.term().equals("g") == true);
+    assertTrue(typeAtt.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);
+    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 1, posIncrAtt.getPositionIncrement() == 1);
+    assertTrue(offsetAtt.startOffset() + " does not equal: " + 36, offsetAtt.startOffset() == 36);
+    assertTrue(offsetAtt.endOffset() + " does not equal: " + 37, offsetAtt.endOffset() == 37);
+
+    assertTrue(tf.incrementToken());
+    assertTrue(termAtt.term() + " is not equal to " + "link",
+            termAtt.term().equals("link") == true);
+    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 1, posIncrAtt.getPositionIncrement() == 1);
+    assertTrue(typeAtt.type() + " is not equal to " + WikipediaTokenizer.INTERNAL_LINK, typeAtt.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);
+    assertTrue(offsetAtt.startOffset() + " does not equal: " + 42, offsetAtt.startOffset() == 42);
+    assertTrue(offsetAtt.endOffset() + " does not equal: " + 46, offsetAtt.endOffset() == 46);
+    
+    assertTrue(tf.incrementToken());
+    assertTrue(termAtt.term() + " is not equal to " + "here",
+            termAtt.term().equals("here") == true);
+    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 1, posIncrAtt.getPositionIncrement() == 1);
+    assertTrue(typeAtt.type() + " is not equal to " + WikipediaTokenizer.INTERNAL_LINK, typeAtt.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);
+    assertTrue(offsetAtt.startOffset() + " does not equal: " + 47, offsetAtt.startOffset() == 47);
+    assertTrue(offsetAtt.endOffset() + " does not equal: " + 51, offsetAtt.endOffset() == 51);
+    
+    assertTrue(tf.incrementToken());
+    assertTrue(termAtt.term() + " is not equal to " + "link",
+            termAtt.term().equals("link") == true);
+    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 1, posIncrAtt.getPositionIncrement() == 1);
+    assertTrue(offsetAtt.startOffset() + " does not equal: " + 56, offsetAtt.startOffset() == 56);
+    assertTrue(typeAtt.type() + " is not equal to " + WikipediaTokenizer.INTERNAL_LINK, typeAtt.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);
+    assertTrue(offsetAtt.endOffset() + " does not equal: " + 60, offsetAtt.endOffset() == 60);
+    
+    assertTrue(tf.incrementToken());
+    assertTrue(termAtt.term() + " is not equal to " + "there",
+            termAtt.term().equals("there") == true);
+    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 1, posIncrAtt.getPositionIncrement() == 1);
+    assertTrue(typeAtt.type() + " is not equal to " + WikipediaTokenizer.INTERNAL_LINK, typeAtt.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);
+    assertTrue(offsetAtt.startOffset() + " does not equal: " + 61, offsetAtt.startOffset() == 61);
+    assertTrue(offsetAtt.endOffset() + " does not equal: " + 66, offsetAtt.endOffset() == 66);
+    
+    assertTrue(tf.incrementToken());
+    assertTrue(termAtt.term() + " is not equal to " + "italics here",
+            termAtt.term().equals("italics here") == true);
+    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 1, posIncrAtt.getPositionIncrement() == 1);
+    assertTrue(typeAtt.type() + " is not equal to " + WikipediaTokenizer.ITALICS, typeAtt.type().equals(WikipediaTokenizer.ITALICS) == true);
+    assertTrue(flagsAtt.getFlags() + " does not equal: " + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, flagsAtt.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);
+    assertTrue(offsetAtt.startOffset() + " does not equal: " + 71, offsetAtt.startOffset() == 71);
+    assertTrue(offsetAtt.endOffset() + " does not equal: " + 83, offsetAtt.endOffset() == 83);
+
+    assertTrue(tf.incrementToken());
+    assertTrue(termAtt.term() + " is not equal to " + "italics",
+            termAtt.term().equals("italics") == true);
+    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 0, posIncrAtt.getPositionIncrement() == 0);
+    assertTrue(typeAtt.type() + " is not equal to " + WikipediaTokenizer.ITALICS, typeAtt.type().equals(WikipediaTokenizer.ITALICS) == true);
+    assertTrue(offsetAtt.startOffset() + " does not equal: " + 71, offsetAtt.startOffset() == 71);
+    assertTrue(offsetAtt.endOffset() + " does not equal: " + 78, offsetAtt.endOffset() == 78);
+
+    assertTrue(tf.incrementToken());
+    assertTrue(termAtt.term() + " is not equal to " + "here",
+            termAtt.term().equals("here") == true);
+    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 1, posIncrAtt.getPositionIncrement() == 1);
+    assertTrue(typeAtt.type() + " is not equal to " + WikipediaTokenizer.ITALICS, typeAtt.type().equals(WikipediaTokenizer.ITALICS) == true);
+    assertTrue(offsetAtt.startOffset() + " does not equal: " + 79, offsetAtt.startOffset() == 79);
+    assertTrue(offsetAtt.endOffset() + " does not equal: " + 83, offsetAtt.endOffset() == 83);
+
+    assertTrue(tf.incrementToken());
+    assertTrue(termAtt.term() + " is not equal to " + "something",
+            termAtt.term().equals("something") == true);
+    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 1, posIncrAtt.getPositionIncrement() == 1);
+    assertTrue(offsetAtt.startOffset() + " does not equal: " + 86, offsetAtt.startOffset() == 86);
+    assertTrue(offsetAtt.endOffset() + " does not equal: " + 95, offsetAtt.endOffset() == 95);
+    
+    assertTrue(tf.incrementToken());
+    assertTrue(termAtt.term() + " is not equal to " + "more italics",
+            termAtt.term().equals("more italics") == true);
+    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 1, posIncrAtt.getPositionIncrement() == 1);
+    assertTrue(typeAtt.type() + " is not equal to " + WikipediaTokenizer.ITALICS, typeAtt.type().equals(WikipediaTokenizer.ITALICS) == true);
+    assertTrue(flagsAtt.getFlags() + " does not equal: " + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, flagsAtt.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);
+    assertTrue(offsetAtt.startOffset() + " does not equal: " + 98, offsetAtt.startOffset() == 98);
+    assertTrue(offsetAtt.endOffset() + " does not equal: " + 110, offsetAtt.endOffset() == 110);
+
+    assertTrue(tf.incrementToken());
+    assertTrue(termAtt.term() + " is not equal to " + "more",
+            termAtt.term().equals("more") == true);
+    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 0, posIncrAtt.getPositionIncrement() == 0);
+    assertTrue(typeAtt.type() + " is not equal to " + WikipediaTokenizer.ITALICS, typeAtt.type().equals(WikipediaTokenizer.ITALICS) == true);
+    assertTrue(offsetAtt.startOffset() + " does not equal: " + 98, offsetAtt.startOffset() == 98);
+    assertTrue(offsetAtt.endOffset() + " does not equal: " + 102, offsetAtt.endOffset() == 102);
+
+    assertTrue(tf.incrementToken());
+    assertTrue(termAtt.term() + " is not equal to " + "italics",
+            termAtt.term().equals("italics") == true);
+    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 1, posIncrAtt.getPositionIncrement() == 1);
+        assertTrue(typeAtt.type() + " is not equal to " + WikipediaTokenizer.ITALICS, typeAtt.type().equals(WikipediaTokenizer.ITALICS) == true);
+
+    assertTrue(offsetAtt.startOffset() + " does not equal: " + 103, offsetAtt.startOffset() == 103);
+    assertTrue(offsetAtt.endOffset() + " does not equal: " + 110, offsetAtt.endOffset() == 110);
+
+    assertTrue(tf.incrementToken());
+    assertTrue(termAtt.term() + " is not equal to " + "h   i   j",
+            termAtt.term().equals("h   i   j") == true);
+    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 1, posIncrAtt.getPositionIncrement() == 1);
+    assertTrue(typeAtt.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);
+    assertTrue(flagsAtt.getFlags() + " does not equal: " + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, flagsAtt.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);
+    assertTrue(offsetAtt.startOffset() + " does not equal: " + 124, offsetAtt.startOffset() == 124);
+    assertTrue(offsetAtt.endOffset() + " does not equal: " + 133, offsetAtt.endOffset() == 133);
+
+    assertTrue(tf.incrementToken());
+    assertTrue(termAtt.term() + " is not equal to " + "h",
+            termAtt.term().equals("h") == true);
+    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 0, posIncrAtt.getPositionIncrement() == 0);
+    assertTrue(typeAtt.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);
+    assertTrue(offsetAtt.startOffset() + " does not equal: " + 124, offsetAtt.startOffset() == 124);
+    assertTrue(offsetAtt.endOffset() + " does not equal: " + 125, offsetAtt.endOffset() == 125);
+
+    assertTrue(tf.incrementToken());
+    assertTrue(termAtt.term() + " is not equal to " + "i",
+            termAtt.term().equals("i") == true);
+    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 1, posIncrAtt.getPositionIncrement() == 1);
+    assertTrue(typeAtt.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);
+    assertTrue(offsetAtt.startOffset() + " does not equal: " + 128, offsetAtt.startOffset() == 128);
+    assertTrue(offsetAtt.endOffset() + " does not equal: " + 129, offsetAtt.endOffset() == 129);
+    
+    assertTrue(tf.incrementToken());
+    assertTrue(termAtt.term() + " is not equal to " + "j",
+            termAtt.term().equals("j") == true);
+    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 1, posIncrAtt.getPositionIncrement() == 1);
+    assertTrue(typeAtt.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);
+    assertTrue(offsetAtt.startOffset() + " does not equal: " + 132, offsetAtt.startOffset() == 132);
+    assertTrue(offsetAtt.endOffset() + " does not equal: " + 133, offsetAtt.endOffset() == 133);
+
+    assertFalse(tf.incrementToken());
+  }
+}
diff --git a/lucene/contrib/wikipedia/build.xml b/lucene/contrib/wikipedia/build.xml
deleted file mode 100644
index 6fe1683..0000000
--- a/lucene/contrib/wikipedia/build.xml
+++ /dev/null
@@ -1,49 +0,0 @@
-<?xml version="1.0"?>
-
-<!--
-    Licensed to the Apache Software Foundation (ASF) under one or more
-    contributor license agreements.  See the NOTICE file distributed with
-    this work for additional information regarding copyright ownership.
-    The ASF licenses this file to You under the Apache License, Version 2.0
-    the "License"); you may not use this file except in compliance with
-    the License.  You may obtain a copy of the License at
- 
-        http://www.apache.org/licenses/LICENSE-2.0
- 
-    Unless required by applicable law or agreed to in writing, software
-    distributed under the License is distributed on an "AS IS" BASIS,
-    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-    See the License for the specific language governing permissions and
-    limitations under the License.
- -->
-
-<project name="wikipedia" default="default">
-
-  <description>
-    Tools for working with Wikipedia
-  </description>
-
-
-  <import file="../contrib-build.xml"/>
-
-
-  <target name="jflex" depends="clean-jflex,jflex-wiki-tokenizer"/>
-
-  <target name="jflex-wiki-tokenizer" depends="init,jflex-check" if="jflex.present">
-    <taskdef classname="JFlex.anttask.JFlexTask" name="jflex">
-      <classpath location="${jflex.home}/lib/JFlex.jar"/>
-    </taskdef>
-
-    <jflex file="src/java/org/apache/lucene/wikipedia/analysis/WikipediaTokenizerImpl.jflex"
-           outdir="src/java/org/apache/lucene/wikipedia/analysis"
-           nobak="on"/>
-  </target>
-
-  <target name="clean-jflex">
-    <delete>
-      <fileset dir="src/java/org/apache/lucene/wikipedia" includes="*.java">
-        <containsregexp expression="generated.*by.*JFlex"/>
-      </fileset>
-    </delete>
-  </target>
-</project>
diff --git a/lucene/contrib/wikipedia/pom.xml.template b/lucene/contrib/wikipedia/pom.xml.template
deleted file mode 100644
index 21c214f..0000000
--- a/lucene/contrib/wikipedia/pom.xml.template
+++ /dev/null
@@ -1,43 +0,0 @@
-
-<project xmlns="http://maven.apache.org/POM/4.0.0"
-  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
-  xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">
-
-  <!--
-    Licensed to the Apache Software Foundation (ASF) under one
-    or more contributor license agreements.  See the NOTICE file
-    distributed with this work for additional information
-    regarding copyright ownership.  The ASF licenses this file
-    to you under the Apache License, Version 2.0 (the
-    "License"); you may not use this file except in compliance
-    with the License.  You may obtain a copy of the License at
-    
-    http://www.apache.org/licenses/LICENSE-2.0
-    
-    Unless required by applicable law or agreed to in writing,
-    software distributed under the License is distributed on an
-    "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-    KIND, either express or implied.  See the License for the
-    specific language governing permissions and limitations
-    under the License.
-  -->
-  <modelVersion>4.0.0</modelVersion>
-  <parent>
-    <groupId>org.apache.lucene</groupId>
-    <artifactId>lucene-contrib</artifactId>
-    <version>@version@</version>
-  </parent>
-  <groupId>org.apache.lucene</groupId>
-  <artifactId>lucene-wikipedia</artifactId>
-  <name>Lucene Wikipedia Tools</name>
-  <version>@version@</version>
-  <description>Lucene Wikipedia Contributions</description>
-  <packaging>jar</packaging>
-  <dependencies>
-    <dependency>
-      <groupId>org.apache.lucene</groupId>
-      <artifactId>lucene-benchmark</artifactId>
-      <version>@version@</version>
-    </dependency>
-  </dependencies>
-</project>
diff --git a/lucene/contrib/wikipedia/src/java/org/apache/lucene/wikipedia/analysis/WikipediaTokenizer.java b/lucene/contrib/wikipedia/src/java/org/apache/lucene/wikipedia/analysis/WikipediaTokenizer.java
deleted file mode 100644
index 9ac9050..0000000
--- a/lucene/contrib/wikipedia/src/java/org/apache/lucene/wikipedia/analysis/WikipediaTokenizer.java
+++ /dev/null
@@ -1,327 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.lucene.wikipedia.analysis;
-
-import org.apache.lucene.analysis.Tokenizer;
-import org.apache.lucene.analysis.tokenattributes.FlagsAttribute;
-import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
-import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
-import org.apache.lucene.analysis.tokenattributes.TermAttribute;
-import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
-import org.apache.lucene.util.AttributeSource;
-
-import java.io.IOException;
-import java.io.Reader;
-import java.util.*;
-
-
-/**
- * Extension of StandardTokenizer that is aware of Wikipedia syntax.  It is based off of the
- * Wikipedia tutorial available at http://en.wikipedia.org/wiki/Wikipedia:Tutorial, but it may not be complete.
- * <p/>
- * <p/>
- * @lucene.experimental
- */
-public final class WikipediaTokenizer extends Tokenizer {
-  public static final String INTERNAL_LINK = "il";
-  public static final String EXTERNAL_LINK = "el";
-  //The URL part of the link, i.e. the first token
-  public static final String EXTERNAL_LINK_URL = "elu";
-  public static final String CITATION = "ci";
-  public static final String CATEGORY = "c";
-  public static final String BOLD = "b";
-  public static final String ITALICS = "i";
-  public static final String BOLD_ITALICS = "bi";
-  public static final String HEADING = "h";
-  public static final String SUB_HEADING = "sh";
-
-  public static final int ALPHANUM_ID          = 0;
-  public static final int APOSTROPHE_ID        = 1;
-  public static final int ACRONYM_ID           = 2;
-  public static final int COMPANY_ID           = 3;
-  public static final int EMAIL_ID             = 4;
-  public static final int HOST_ID              = 5;
-  public static final int NUM_ID               = 6;
-  public static final int CJ_ID                = 7;
-  public static final int INTERNAL_LINK_ID     = 8;
-  public static final int EXTERNAL_LINK_ID     = 9;
-  public static final int CITATION_ID          = 10;
-  public static final int CATEGORY_ID          = 11;
-  public static final int BOLD_ID              = 12;
-  public static final int ITALICS_ID           = 13;
-  public static final int BOLD_ITALICS_ID      = 14;
-  public static final int HEADING_ID           = 15;
-  public static final int SUB_HEADING_ID       = 16;
-  public static final int EXTERNAL_LINK_URL_ID = 17;
-
-  /** String token types that correspond to token type int constants */
-  public static final String [] TOKEN_TYPES = new String [] {
-    "<ALPHANUM>",
-    "<APOSTROPHE>",
-    "<ACRONYM>",
-    "<COMPANY>",
-    "<EMAIL>",
-    "<HOST>",
-    "<NUM>",
-    "<CJ>",
-    INTERNAL_LINK,
-    EXTERNAL_LINK,
-    CITATION,
-    CATEGORY,
-    BOLD,
-    ITALICS,
-    BOLD_ITALICS,
-    HEADING,
-    SUB_HEADING,
-    EXTERNAL_LINK_URL
-  };
-
-  /**
-   * Only output tokens
-   */
-  public static final int TOKENS_ONLY = 0;
-  /**
-   * Only output untokenized tokens, which are tokens that would normally be split into several tokens
-   */
-  public static final int UNTOKENIZED_ONLY = 1;
-  /**
-   * Output the both the untokenized token and the splits
-   */
-  public static final int BOTH = 2;
-  /**
-   * This flag is used to indicate that the produced "Token" would, if {@link #TOKENS_ONLY} was used, produce multiple tokens.
-   */
-  public static final int UNTOKENIZED_TOKEN_FLAG = 1;
-  /**
-   * A private instance of the JFlex-constructed scanner
-   */
-  private final WikipediaTokenizerImpl scanner;
-
-  private int tokenOutput = TOKENS_ONLY;
-  private Set<String> untokenizedTypes = Collections.emptySet();
-  private Iterator<AttributeSource.State> tokens = null;
-  
-  private OffsetAttribute offsetAtt;
-  private TypeAttribute typeAtt;
-  private PositionIncrementAttribute posIncrAtt;
-  private TermAttribute termAtt;
-  private FlagsAttribute flagsAtt;
-
-  /**
-   * Creates a new instance of the {@link WikipediaTokenizer}. Attaches the
-   * <code>input</code> to a newly created JFlex scanner.
-   *
-   * @param input The Input Reader
-   */
-  public WikipediaTokenizer(Reader input) {
-    this(input, TOKENS_ONLY, Collections.<String>emptySet());
-  }
-
-  /**
-   * Creates a new instance of the {@link org.apache.lucene.wikipedia.analysis.WikipediaTokenizer}.  Attaches the
-   * <code>input</code> to a the newly created JFlex scanner.
-   *
-   * @param input The input
-   * @param tokenOutput One of {@link #TOKENS_ONLY}, {@link #UNTOKENIZED_ONLY}, {@link #BOTH}
-   * @param untokenizedTypes
-   */
-  public WikipediaTokenizer(Reader input, int tokenOutput, Set<String> untokenizedTypes) {
-    super(input);
-    this.scanner = new WikipediaTokenizerImpl(input);
-    init(tokenOutput, untokenizedTypes);
-  }
-
-  /**
-   * Creates a new instance of the {@link org.apache.lucene.wikipedia.analysis.WikipediaTokenizer}.  Attaches the
-   * <code>input</code> to a the newly created JFlex scanner. Uses the given {@link org.apache.lucene.util.AttributeSource.AttributeFactory}.
-   *
-   * @param input The input
-   * @param tokenOutput One of {@link #TOKENS_ONLY}, {@link #UNTOKENIZED_ONLY}, {@link #BOTH}
-   * @param untokenizedTypes
-   */
-  public WikipediaTokenizer(AttributeFactory factory, Reader input, int tokenOutput, Set<String> untokenizedTypes) {
-    super(factory, input);
-    this.scanner = new WikipediaTokenizerImpl(input);
-    init(tokenOutput, untokenizedTypes);
-  }
-
-  /**
-   * Creates a new instance of the {@link org.apache.lucene.wikipedia.analysis.WikipediaTokenizer}.  Attaches the
-   * <code>input</code> to a the newly created JFlex scanner. Uses the given {@link AttributeSource}.
-   *
-   * @param input The input
-   * @param tokenOutput One of {@link #TOKENS_ONLY}, {@link #UNTOKENIZED_ONLY}, {@link #BOTH}
-   * @param untokenizedTypes
-   */
-  public WikipediaTokenizer(AttributeSource source, Reader input, int tokenOutput, Set<String> untokenizedTypes) {
-    super(source, input);
-    this.scanner = new WikipediaTokenizerImpl(input);
-    init(tokenOutput, untokenizedTypes);
-  }
-  
-  private void init(int tokenOutput, Set<String> untokenizedTypes) {
-    this.tokenOutput = tokenOutput;
-    this.untokenizedTypes = untokenizedTypes;
-    this.offsetAtt = addAttribute(OffsetAttribute.class);
-    this.typeAtt = addAttribute(TypeAttribute.class);
-    this.posIncrAtt = addAttribute(PositionIncrementAttribute.class);
-    this.termAtt = addAttribute(TermAttribute.class);
-    this.flagsAtt = addAttribute(FlagsAttribute.class);    
-  }
-  
-  /*
-  * (non-Javadoc)
-  *
-  * @see org.apache.lucene.analysis.TokenStream#next()
-  */
-  @Override
-  public final boolean incrementToken() throws IOException {
-    if (tokens != null && tokens.hasNext()){
-      AttributeSource.State state = tokens.next();
-      restoreState(state);
-      return true;
-    }
-    clearAttributes();
-    int tokenType = scanner.getNextToken();
-
-    if (tokenType == WikipediaTokenizerImpl.YYEOF) {
-      return false;
-    }
-    String type = WikipediaTokenizerImpl.TOKEN_TYPES[tokenType];
-    if (tokenOutput == TOKENS_ONLY || untokenizedTypes.contains(type) == false){
-      setupToken();
-    } else if (tokenOutput == UNTOKENIZED_ONLY && untokenizedTypes.contains(type) == true){
-      collapseTokens(tokenType);
-
-    }
-    else if (tokenOutput == BOTH){
-      //collapse into a single token, add it to tokens AND output the individual tokens
-      //output the untokenized Token first
-      collapseAndSaveTokens(tokenType, type);
-    }
-    posIncrAtt.setPositionIncrement(scanner.getPositionIncrement());
-    typeAtt.setType(type);
-    return true;
-  }
-
-  private void collapseAndSaveTokens(int tokenType, String type) throws IOException {
-    //collapse
-    StringBuilder buffer = new StringBuilder(32);
-    int numAdded = scanner.setText(buffer);
-    //TODO: how to know how much whitespace to add
-    int theStart = scanner.yychar();
-    int lastPos = theStart + numAdded;
-    int tmpTokType;
-    int numSeen = 0;
-    List<AttributeSource.State> tmp = new ArrayList<AttributeSource.State>();
-    setupSavedToken(0, type);
-    tmp.add(captureState());
-    //while we can get a token and that token is the same type and we have not transitioned to a new wiki-item of the same type
-    while ((tmpTokType = scanner.getNextToken()) != WikipediaTokenizerImpl.YYEOF && tmpTokType == tokenType && scanner.getNumWikiTokensSeen() > numSeen){
-      int currPos = scanner.yychar();
-      //append whitespace
-      for (int i = 0; i < (currPos - lastPos); i++){
-        buffer.append(' ');
-      }
-      numAdded = scanner.setText(buffer);
-      setupSavedToken(scanner.getPositionIncrement(), type);
-      tmp.add(captureState());
-      numSeen++;
-      lastPos = currPos + numAdded;
-    }
-    //trim the buffer
-    String s = buffer.toString().trim();
-    termAtt.setTermBuffer(s.toCharArray(), 0, s.length());
-    offsetAtt.setOffset(correctOffset(theStart), correctOffset(theStart + s.length()));
-    flagsAtt.setFlags(UNTOKENIZED_TOKEN_FLAG);
-    //The way the loop is written, we will have proceeded to the next token.  We need to pushback the scanner to lastPos
-    if (tmpTokType != WikipediaTokenizerImpl.YYEOF){
-      scanner.yypushback(scanner.yylength());
-    }
-    tokens = tmp.iterator();
-  }
-
-  private void setupSavedToken(int positionInc, String type){
-    setupToken();
-    posIncrAtt.setPositionIncrement(positionInc);
-    typeAtt.setType(type);
-  }
-
-  private void collapseTokens(int tokenType) throws IOException {
-    //collapse
-    StringBuilder buffer = new StringBuilder(32);
-    int numAdded = scanner.setText(buffer);
-    //TODO: how to know how much whitespace to add
-    int theStart = scanner.yychar();
-    int lastPos = theStart + numAdded;
-    int tmpTokType;
-    int numSeen = 0;
-    //while we can get a token and that token is the same type and we have not transitioned to a new wiki-item of the same type
-    while ((tmpTokType = scanner.getNextToken()) != WikipediaTokenizerImpl.YYEOF && tmpTokType == tokenType && scanner.getNumWikiTokensSeen() > numSeen){
-      int currPos = scanner.yychar();
-      //append whitespace
-      for (int i = 0; i < (currPos - lastPos); i++){
-        buffer.append(' ');
-      }
-      numAdded = scanner.setText(buffer);
-      numSeen++;
-      lastPos = currPos + numAdded;
-    }
-    //trim the buffer
-    String s = buffer.toString().trim();
-    termAtt.setTermBuffer(s.toCharArray(), 0, s.length());
-    offsetAtt.setOffset(correctOffset(theStart), correctOffset(theStart + s.length()));
-    flagsAtt.setFlags(UNTOKENIZED_TOKEN_FLAG);
-    //The way the loop is written, we will have proceeded to the next token.  We need to pushback the scanner to lastPos
-    if (tmpTokType != WikipediaTokenizerImpl.YYEOF){
-      scanner.yypushback(scanner.yylength());
-    } else {
-      tokens = null;
-    }
-  }
-
-  private void setupToken() {
-    scanner.getText(termAtt);
-    final int start = scanner.yychar();
-    offsetAtt.setOffset(correctOffset(start), correctOffset(start + termAtt.termLength()));
-  }
-
-  /*
-  * (non-Javadoc)
-  *
-  * @see org.apache.lucene.analysis.TokenStream#reset()
-  */
-  @Override
-  public void reset() throws IOException {
-    super.reset();
-    scanner.yyreset(input);
-  }
-
-  @Override
-  public void reset(Reader reader) throws IOException {
-    super.reset(reader);
-    reset();
-  }
-
-  @Override
-  public void end() throws IOException {
-    // set final offset
-    final int finalOffset = correctOffset(scanner.yychar() + scanner.yylength());
-    this.offsetAtt.setOffset(finalOffset, finalOffset);
-  }
-}
\ No newline at end of file
diff --git a/lucene/contrib/wikipedia/src/java/org/apache/lucene/wikipedia/analysis/WikipediaTokenizerImpl.java b/lucene/contrib/wikipedia/src/java/org/apache/lucene/wikipedia/analysis/WikipediaTokenizerImpl.java
deleted file mode 100644
index 34f8052..0000000
--- a/lucene/contrib/wikipedia/src/java/org/apache/lucene/wikipedia/analysis/WikipediaTokenizerImpl.java
+++ /dev/null
@@ -1,974 +0,0 @@
-/* The following code was generated by JFlex 1.4.1 on 4/15/08 4:31 AM */
-
-package org.apache.lucene.wikipedia.analysis;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.analysis.tokenattributes.TermAttribute;
-
-
-/**
- * This class is a scanner generated by 
- * <a href="http://www.jflex.de/">JFlex</a> 1.4.1
- * on 4/15/08 4:31 AM from the specification file
- * <tt>/mnt2/mike/src/lucene.clean/contrib/wikipedia/src/java/org/apache/lucene/wikipedia/analysis/WikipediaTokenizerImpl.jflex</tt>
- */
-class WikipediaTokenizerImpl {
-
-  /** This character denotes the end of file */
-  public static final int YYEOF = -1;
-
-  /** initial size of the lookahead buffer */
-  private static final int ZZ_BUFFERSIZE = 16384;
-
-  /** lexical states */
-  public static final int DOUBLE_BRACE_STATE = 8;
-  public static final int INTERNAL_LINK_STATE = 2;
-  public static final int TWO_SINGLE_QUOTES_STATE = 4;
-  public static final int CATEGORY_STATE = 1;
-  public static final int FIVE_SINGLE_QUOTES_STATE = 6;
-  public static final int STRING = 9;
-  public static final int YYINITIAL = 0;
-  public static final int DOUBLE_EQUALS_STATE = 7;
-  public static final int THREE_SINGLE_QUOTES_STATE = 5;
-  public static final int EXTERNAL_LINK_STATE = 3;
-
-  /** 
-   * Translates characters to character classes
-   */
-  private static final String ZZ_CMAP_PACKED = 
-    "\11\0\1\24\1\23\1\0\1\24\1\22\22\0\1\24\1\0\1\12"+
-    "\1\53\2\0\1\3\1\1\4\0\1\14\1\5\1\2\1\10\12\16"+
-    "\1\27\1\0\1\7\1\11\1\13\1\53\1\4\2\15\1\30\5\15"+
-    "\1\41\21\15\1\25\1\0\1\26\1\0\1\6\1\0\1\31\1\43"+
-    "\2\15\1\33\1\40\1\34\1\50\1\41\4\15\1\42\1\35\1\51"+
-    "\1\15\1\36\1\52\1\32\3\15\1\44\1\37\1\15\1\45\1\47"+
-    "\1\46\102\0\27\15\1\0\37\15\1\0\u0568\15\12\17\206\15\12\17"+
-    "\u026c\15\12\17\166\15\12\17\166\15\12\17\166\15\12\17\166\15\12\17"+
-    "\167\15\11\17\166\15\12\17\166\15\12\17\166\15\12\17\340\15\12\17"+
-    "\166\15\12\17\u0166\15\12\17\266\15\u0100\15\u0e00\15\u1040\0\u0150\21\140\0"+
-    "\20\21\u0100\0\200\21\200\0\u19c0\21\100\0\u5200\21\u0c00\0\u2bb0\20\u2150\0"+
-    "\u0200\21\u0465\0\73\21\75\15\43\0";
-
-  /** 
-   * Translates characters to character classes
-   */
-  private static final char [] ZZ_CMAP = zzUnpackCMap(ZZ_CMAP_PACKED);
-
-  /** 
-   * Translates DFA states to action switch labels.
-   */
-  private static final int [] ZZ_ACTION = zzUnpackAction();
-
-  private static final String ZZ_ACTION_PACKED_0 =
-    "\12\0\4\1\4\2\1\3\1\1\1\4\1\1\2\5"+
-    "\1\6\2\5\1\7\1\5\2\10\1\11\1\12\1\11"+
-    "\1\13\1\14\1\10\1\15\1\16\1\15\1\17\1\20"+
-    "\1\10\1\21\1\10\4\22\1\23\1\22\1\24\1\25"+
-    "\1\26\3\0\1\27\14\0\1\30\1\31\1\32\1\33"+
-    "\1\11\1\0\1\34\1\35\1\0\1\36\1\0\1\37"+
-    "\3\0\1\40\1\41\2\42\1\41\2\43\2\0\1\42"+
-    "\1\0\14\42\1\41\3\0\1\11\1\44\3\0\1\45"+
-    "\1\46\5\0\1\47\4\0\1\47\2\0\2\47\2\0"+
-    "\1\11\5\0\1\31\1\41\1\42\1\50\3\0\1\11"+
-    "\2\0\1\51\30\0\1\52\2\0\1\53\1\54\1\55";
-
-  private static int [] zzUnpackAction() {
-    int [] result = new int[183];
-    int offset = 0;
-    offset = zzUnpackAction(ZZ_ACTION_PACKED_0, offset, result);
-    return result;
-  }
-
-  private static int zzUnpackAction(String packed, int offset, int [] result) {
-    int i = 0;       /* index in packed string  */
-    int j = offset;  /* index in unpacked array */
-    int l = packed.length();
-    while (i < l) {
-      int count = packed.charAt(i++);
-      int value = packed.charAt(i++);
-      do result[j++] = value; while (--count > 0);
-    }
-    return j;
-  }
-
-
-  /** 
-   * Translates a state to a row index in the transition table
-   */
-  private static final int [] ZZ_ROWMAP = zzUnpackRowMap();
-
-  private static final String ZZ_ROWMAP_PACKED_0 =
-    "\0\0\0\54\0\130\0\204\0\260\0\334\0\u0108\0\u0134"+
-    "\0\u0160\0\u018c\0\u01b8\0\u01e4\0\u0210\0\u023c\0\u0268\0\u0294"+
-    "\0\u02c0\0\u02ec\0\u01b8\0\u0318\0\u0344\0\u0370\0\u01b8\0\u039c"+
-    "\0\u03c8\0\u03f4\0\u0420\0\u044c\0\u0478\0\u01b8\0\u039c\0\u04a4"+
-    "\0\u01b8\0\u04d0\0\u04fc\0\u0528\0\u0554\0\u0580\0\u05ac\0\u05d8"+
-    "\0\u0604\0\u0630\0\u065c\0\u0688\0\u06b4\0\u01b8\0\u06e0\0\u039c"+
-    "\0\u070c\0\u0738\0\u0764\0\u0790\0\u01b8\0\u01b8\0\u07bc\0\u07e8"+
-    "\0\u0814\0\u01b8\0\u0840\0\u086c\0\u0898\0\u08c4\0\u08f0\0\u091c"+
-    "\0\u0948\0\u0974\0\u09a0\0\u09cc\0\u09f8\0\u0a24\0\u0a50\0\u0a7c"+
-    "\0\u01b8\0\u01b8\0\u0aa8\0\u0ad4\0\u0b00\0\u0b00\0\u0b2c\0\u0b58"+
-    "\0\u0b84\0\u0bb0\0\u0bdc\0\u0c08\0\u0c34\0\u0c60\0\u0c8c\0\u0cb8"+
-    "\0\u0ce4\0\u0d10\0\u0898\0\u0d3c\0\u0d68\0\u0d94\0\u0dc0\0\u0dec"+
-    "\0\u0e18\0\u0e44\0\u0e70\0\u0e9c\0\u0ec8\0\u0ef4\0\u0f20\0\u0f4c"+
-    "\0\u0f78\0\u0fa4\0\u0fd0\0\u0ffc\0\u1028\0\u1054\0\u1080\0\u10ac"+
-    "\0\u10d8\0\u01b8\0\u1104\0\u1130\0\u115c\0\u1188\0\u01b8\0\u11b4"+
-    "\0\u11e0\0\u120c\0\u1238\0\u1264\0\u1290\0\u12bc\0\u12e8\0\u1314"+
-    "\0\u1340\0\u136c\0\u1398\0\u13c4\0\u086c\0\u09f8\0\u13f0\0\u141c"+
-    "\0\u1448\0\u1474\0\u14a0\0\u14cc\0\u14f8\0\u1524\0\u01b8\0\u1550"+
-    "\0\u157c\0\u15a8\0\u15d4\0\u1600\0\u162c\0\u1658\0\u1684\0\u16b0"+
-    "\0\u01b8\0\u16dc\0\u1708\0\u1734\0\u1760\0\u178c\0\u17b8\0\u17e4"+
-    "\0\u1810\0\u183c\0\u1868\0\u1894\0\u18c0\0\u18ec\0\u1918\0\u1944"+
-    "\0\u1970\0\u199c\0\u19c8\0\u19f4\0\u1a20\0\u1a4c\0\u1a78\0\u1aa4"+
-    "\0\u1ad0\0\u1afc\0\u1b28\0\u1b54\0\u01b8\0\u01b8\0\u01b8";
-
-  private static int [] zzUnpackRowMap() {
-    int [] result = new int[183];
-    int offset = 0;
-    offset = zzUnpackRowMap(ZZ_ROWMAP_PACKED_0, offset, result);
-    return result;
-  }
-
-  private static int zzUnpackRowMap(String packed, int offset, int [] result) {
-    int i = 0;  /* index in packed string  */
-    int j = offset;  /* index in unpacked array */
-    int l = packed.length();
-    while (i < l) {
-      int high = packed.charAt(i++) << 16;
-      result[j++] = high | packed.charAt(i++);
-    }
-    return j;
-  }
-
-  /** 
-   * The transition table of the DFA
-   */
-  private static final int [] ZZ_TRANS = zzUnpackTrans();
-
-  private static final String ZZ_TRANS_PACKED_0 =
-    "\1\13\1\14\5\13\1\15\1\13\1\16\3\13\1\17"+
-    "\1\20\1\21\1\22\1\23\1\24\2\13\1\25\2\13"+
-    "\15\17\1\26\2\13\3\17\1\13\7\27\1\30\5\27"+
-    "\4\31\1\27\1\32\3\27\1\33\1\27\15\31\3\27"+
-    "\3\31\10\27\1\30\5\27\4\34\1\27\1\32\3\27"+
-    "\1\35\1\27\15\34\3\27\3\34\1\27\7\36\1\37"+
-    "\5\36\4\40\1\36\1\32\2\27\1\36\1\41\1\36"+
-    "\15\40\3\36\1\42\2\40\2\36\1\43\5\36\1\37"+
-    "\5\36\4\44\1\36\1\45\2\36\1\46\2\36\15\44"+
-    "\3\36\3\44\10\36\1\37\5\36\4\47\1\36\1\45"+
-    "\2\36\1\46\2\36\15\47\3\36\3\47\10\36\1\37"+
-    "\5\36\4\47\1\36\1\45\2\36\1\50\2\36\15\47"+
-    "\3\36\3\47\10\36\1\37\1\36\1\51\3\36\4\52"+
-    "\1\36\1\45\5\36\15\52\3\36\3\52\10\36\1\53"+
-    "\5\36\4\54\1\36\1\45\5\36\15\54\1\36\1\55"+
-    "\1\36\3\54\1\36\1\56\1\57\5\56\1\60\1\56"+
-    "\1\61\3\56\4\62\1\56\1\63\2\56\1\64\2\56"+
-    "\15\62\2\56\1\65\3\62\1\56\55\0\1\66\62\0"+
-    "\1\67\4\0\4\70\7\0\6\70\1\71\6\70\3\0"+
-    "\3\70\12\0\1\72\43\0\1\73\1\74\1\75\1\76"+
-    "\2\77\1\0\1\100\3\0\1\100\1\17\1\20\1\21"+
-    "\1\22\7\0\15\17\3\0\3\17\3\0\1\101\1\0"+
-    "\1\102\2\103\1\0\1\104\3\0\1\104\3\20\1\22"+
-    "\7\0\15\20\3\0\3\20\2\0\1\73\1\105\1\75"+
-    "\1\76\2\103\1\0\1\104\3\0\1\104\1\21\1\20"+
-    "\1\21\1\22\7\0\15\21\3\0\3\21\3\0\1\106"+
-    "\1\0\1\102\2\77\1\0\1\100\3\0\1\100\4\22"+
-    "\7\0\15\22\3\0\3\22\24\0\1\13\55\0\1\107"+
-    "\73\0\1\110\16\0\1\67\4\0\4\70\7\0\15\70"+
-    "\3\0\3\70\16\0\4\31\7\0\15\31\3\0\3\31"+
-    "\24\0\1\27\56\0\1\111\42\0\4\34\7\0\15\34"+
-    "\3\0\3\34\27\0\1\112\42\0\4\40\7\0\15\40"+
-    "\3\0\3\40\16\0\4\40\7\0\2\40\1\113\12\40"+
-    "\3\0\3\40\2\0\1\114\67\0\4\44\7\0\15\44"+
-    "\3\0\3\44\24\0\1\36\55\0\1\115\43\0\4\47"+
-    "\7\0\15\47\3\0\3\47\26\0\1\116\37\0\1\111"+
-    "\57\0\4\52\7\0\15\52\3\0\3\52\11\0\1\117"+
-    "\4\0\4\70\7\0\15\70\3\0\3\70\16\0\4\54"+
-    "\7\0\15\54\3\0\3\54\47\0\1\111\6\0\1\120"+
-    "\63\0\1\121\57\0\4\62\7\0\15\62\3\0\3\62"+
-    "\24\0\1\56\55\0\1\122\43\0\4\70\7\0\15\70"+
-    "\3\0\3\70\14\0\1\36\1\0\4\123\1\0\3\124"+
-    "\3\0\15\123\3\0\3\123\14\0\1\36\1\0\4\123"+
-    "\1\0\3\124\3\0\3\123\1\125\11\123\3\0\3\123"+
-    "\16\0\1\126\1\0\1\126\10\0\15\126\3\0\3\126"+
-    "\16\0\1\127\1\130\1\131\1\132\7\0\15\127\3\0"+
-    "\3\127\16\0\1\133\1\0\1\133\10\0\15\133\3\0"+
-    "\3\133\16\0\1\134\1\135\1\134\1\135\7\0\15\134"+
-    "\3\0\3\134\16\0\1\136\2\137\1\140\7\0\15\136"+
-    "\3\0\3\136\16\0\1\100\2\141\10\0\15\100\3\0"+
-    "\3\100\16\0\1\142\2\143\1\144\7\0\15\142\3\0"+
-    "\3\142\16\0\4\135\7\0\15\135\3\0\3\135\16\0"+
-    "\1\145\2\146\1\147\7\0\15\145\3\0\3\145\16\0"+
-    "\1\150\2\151\1\152\7\0\15\150\3\0\3\150\16\0"+
-    "\1\153\1\143\1\154\1\144\7\0\15\153\3\0\3\153"+
-    "\16\0\1\155\2\130\1\132\7\0\15\155\3\0\3\155"+
-    "\30\0\1\156\1\157\64\0\1\160\27\0\4\40\7\0"+
-    "\2\40\1\161\12\40\3\0\3\40\2\0\1\162\101\0"+
-    "\1\163\1\164\40\0\4\70\7\0\6\70\1\165\6\70"+
-    "\3\0\3\70\2\0\1\166\63\0\1\167\71\0\1\170"+
-    "\1\171\34\0\1\172\1\0\1\36\1\0\4\123\1\0"+
-    "\3\124\3\0\15\123\3\0\3\123\16\0\4\173\1\0"+
-    "\3\124\3\0\15\173\3\0\3\173\12\0\1\172\1\0"+
-    "\1\36\1\0\4\123\1\0\3\124\3\0\10\123\1\174"+
-    "\4\123\3\0\3\123\2\0\1\73\13\0\1\126\1\0"+
-    "\1\126\10\0\15\126\3\0\3\126\3\0\1\175\1\0"+
-    "\1\102\2\176\6\0\1\127\1\130\1\131\1\132\7\0"+
-    "\15\127\3\0\3\127\3\0\1\177\1\0\1\102\2\200"+
-    "\1\0\1\201\3\0\1\201\3\130\1\132\7\0\15\130"+
-    "\3\0\3\130\3\0\1\202\1\0\1\102\2\200\1\0"+
-    "\1\201\3\0\1\201\1\131\1\130\1\131\1\132\7\0"+
-    "\15\131\3\0\3\131\3\0\1\203\1\0\1\102\2\176"+
-    "\6\0\4\132\7\0\15\132\3\0\3\132\3\0\1\204"+
-    "\2\0\1\204\7\0\1\134\1\135\1\134\1\135\7\0"+
-    "\15\134\3\0\3\134\3\0\1\204\2\0\1\204\7\0"+
-    "\4\135\7\0\15\135\3\0\3\135\3\0\1\176\1\0"+
-    "\1\102\2\176\6\0\1\136\2\137\1\140\7\0\15\136"+
-    "\3\0\3\136\3\0\1\200\1\0\1\102\2\200\1\0"+
-    "\1\201\3\0\1\201\3\137\1\140\7\0\15\137\3\0"+
-    "\3\137\3\0\1\176\1\0\1\102\2\176\6\0\4\140"+
-    "\7\0\15\140\3\0\3\140\3\0\1\201\2\0\2\201"+
-    "\1\0\1\201\3\0\1\201\3\141\10\0\15\141\3\0"+
-    "\3\141\3\0\1\106\1\0\1\102\2\77\1\0\1\100"+
-    "\3\0\1\100\1\142\2\143\1\144\7\0\15\142\3\0"+
-    "\3\142\3\0\1\101\1\0\1\102\2\103\1\0\1\104"+
-    "\3\0\1\104\3\143\1\144\7\0\15\143\3\0\3\143"+
-    "\3\0\1\106\1\0\1\102\2\77\1\0\1\100\3\0"+
-    "\1\100\4\144\7\0\15\144\3\0\3\144\3\0\1\77"+
-    "\1\0\1\102\2\77\1\0\1\100\3\0\1\100\1\145"+
-    "\2\146\1\147\7\0\15\145\3\0\3\145\3\0\1\103"+
-    "\1\0\1\102\2\103\1\0\1\104\3\0\1\104\3\146"+
-    "\1\147\7\0\15\146\3\0\3\146\3\0\1\77\1\0"+
-    "\1\102\2\77\1\0\1\100\3\0\1\100\4\147\7\0"+
-    "\15\147\3\0\3\147\3\0\1\100\2\0\2\100\1\0"+
-    "\1\100\3\0\1\100\1\150\2\151\1\152\7\0\15\150"+
-    "\3\0\3\150\3\0\1\104\2\0\2\104\1\0\1\104"+
-    "\3\0\1\104\3\151\1\152\7\0\15\151\3\0\3\151"+
-    "\3\0\1\100\2\0\2\100\1\0\1\100\3\0\1\100"+
-    "\4\152\7\0\15\152\3\0\3\152\3\0\1\205\1\0"+
-    "\1\102\2\77\1\0\1\100\3\0\1\100\1\153\1\143"+
-    "\1\154\1\144\7\0\15\153\3\0\3\153\3\0\1\206"+
-    "\1\0\1\102\2\103\1\0\1\104\3\0\1\104\1\154"+
-    "\1\143\1\154\1\144\7\0\15\154\3\0\3\154\3\0"+
-    "\1\203\1\0\1\102\2\176\6\0\1\155\2\130\1\132"+
-    "\7\0\15\155\3\0\3\155\31\0\1\157\54\0\1\207"+
-    "\64\0\1\210\26\0\4\40\7\0\15\40\3\0\1\40"+
-    "\1\211\1\40\31\0\1\164\54\0\1\212\35\0\1\36"+
-    "\1\0\4\123\1\0\3\124\3\0\3\123\1\213\11\123"+
-    "\3\0\3\123\2\0\1\214\102\0\1\171\54\0\1\215"+
-    "\34\0\1\216\52\0\1\172\3\0\4\173\7\0\15\173"+
-    "\3\0\3\173\12\0\1\172\1\0\1\217\1\0\4\123"+
-    "\1\0\3\124\3\0\15\123\3\0\3\123\16\0\1\220"+
-    "\1\132\1\220\1\132\7\0\15\220\3\0\3\220\16\0"+
-    "\4\140\7\0\15\140\3\0\3\140\16\0\4\144\7\0"+
-    "\15\144\3\0\3\144\16\0\4\147\7\0\15\147\3\0"+
-    "\3\147\16\0\4\152\7\0\15\152\3\0\3\152\16\0"+
-    "\1\221\1\144\1\221\1\144\7\0\15\221\3\0\3\221"+
-    "\16\0\4\132\7\0\15\132\3\0\3\132\16\0\4\222"+
-    "\7\0\15\222\3\0\3\222\33\0\1\223\61\0\1\224"+
-    "\30\0\4\40\6\0\1\225\15\40\3\0\2\40\1\226"+
-    "\33\0\1\227\32\0\1\172\1\0\1\36\1\0\4\123"+
-    "\1\0\3\124\3\0\10\123\1\230\4\123\3\0\3\123"+
-    "\2\0\1\231\104\0\1\232\36\0\4\233\7\0\15\233"+
-    "\3\0\3\233\3\0\1\175\1\0\1\102\2\176\6\0"+
-    "\1\220\1\132\1\220\1\132\7\0\15\220\3\0\3\220"+
-    "\3\0\1\205\1\0\1\102\2\77\1\0\1\100\3\0"+
-    "\1\100\1\221\1\144\1\221\1\144\7\0\15\221\3\0"+
-    "\3\221\3\0\1\204\2\0\1\204\7\0\4\222\7\0"+
-    "\15\222\3\0\3\222\34\0\1\234\55\0\1\235\26\0"+
-    "\1\236\60\0\4\40\6\0\1\225\15\40\3\0\3\40"+
-    "\34\0\1\237\31\0\1\172\1\0\1\111\1\0\4\123"+
-    "\1\0\3\124\3\0\15\123\3\0\3\123\34\0\1\240"+
-    "\32\0\1\241\2\0\4\233\7\0\15\233\3\0\3\233"+
-    "\35\0\1\242\62\0\1\243\20\0\1\244\77\0\1\245"+
-    "\53\0\1\246\32\0\1\36\1\0\4\173\1\0\3\124"+
-    "\3\0\15\173\3\0\3\173\36\0\1\247\53\0\1\250"+
-    "\33\0\4\251\7\0\15\251\3\0\3\251\36\0\1\252"+
-    "\53\0\1\253\54\0\1\254\61\0\1\255\11\0\1\256"+
-    "\12\0\4\251\7\0\15\251\3\0\3\251\37\0\1\257"+
-    "\53\0\1\260\54\0\1\261\22\0\1\13\62\0\4\262"+
-    "\7\0\15\262\3\0\3\262\40\0\1\263\53\0\1\264"+
-    "\43\0\1\265\26\0\2\262\1\0\2\262\1\0\2\262"+
-    "\2\0\5\262\7\0\15\262\3\0\4\262\27\0\1\266"+
-    "\53\0\1\267\24\0";
-
-  private static int [] zzUnpackTrans() {
-    int [] result = new int[7040];
-    int offset = 0;
-    offset = zzUnpackTrans(ZZ_TRANS_PACKED_0, offset, result);
-    return result;
-  }
-
-  private static int zzUnpackTrans(String packed, int offset, int [] result) {
-    int i = 0;       /* index in packed string  */
-    int j = offset;  /* index in unpacked array */
-    int l = packed.length();
-    while (i < l) {
-      int count = packed.charAt(i++);
-      int value = packed.charAt(i++);
-      value--;
-      do result[j++] = value; while (--count > 0);
-    }
-    return j;
-  }
-
-
-  /* error codes */
-  private static final int ZZ_UNKNOWN_ERROR = 0;
-  private static final int ZZ_NO_MATCH = 1;
-  private static final int ZZ_PUSHBACK_2BIG = 2;
-
-  /* error messages for the codes above */
-  private static final String ZZ_ERROR_MSG[] = {
-    "Unkown internal scanner error",
-    "Error: could not match input",
-    "Error: pushback value was too large"
-  };
-
-  /**
-   * ZZ_ATTRIBUTE[aState] contains the attributes of state <code>aState</code>
-   */
-  private static final int [] ZZ_ATTRIBUTE = zzUnpackAttribute();
-
-  private static final String ZZ_ATTRIBUTE_PACKED_0 =
-    "\12\0\1\11\7\1\1\11\3\1\1\11\6\1\1\11"+
-    "\2\1\1\11\14\1\1\11\6\1\2\11\3\0\1\11"+
-    "\14\0\2\1\2\11\1\1\1\0\2\1\1\0\1\1"+
-    "\1\0\1\1\3\0\7\1\2\0\1\1\1\0\15\1"+
-    "\3\0\1\1\1\11\3\0\1\1\1\11\5\0\1\1"+
-    "\4\0\1\1\2\0\2\1\2\0\1\1\5\0\1\11"+
-    "\3\1\3\0\1\1\2\0\1\11\30\0\1\1\2\0"+
-    "\3\11";
-
-  private static int [] zzUnpackAttribute() {
-    int [] result = new int[183];
-    int offset = 0;
-    offset = zzUnpackAttribute(ZZ_ATTRIBUTE_PACKED_0, offset, result);
-    return result;
-  }
-
-  private static int zzUnpackAttribute(String packed, int offset, int [] result) {
-    int i = 0;       /* index in packed string  */
-    int j = offset;  /* index in unpacked array */
-    int l = packed.length();
-    while (i < l) {
-      int count = packed.charAt(i++);
-      int value = packed.charAt(i++);
-      do result[j++] = value; while (--count > 0);
-    }
-    return j;
-  }
-
-  /** the input device */
-  private java.io.Reader zzReader;
-
-  /** the current state of the DFA */
-  private int zzState;
-
-  /** the current lexical state */
-  private int zzLexicalState = YYINITIAL;
-
-  /** this buffer contains the current text to be matched and is
-      the source of the yytext() string */
-  private char zzBuffer[] = new char[ZZ_BUFFERSIZE];
-
-  /** the textposition at the last accepting state */
-  private int zzMarkedPos;
-
-  /** the textposition at the last state to be included in yytext */
-  private int zzPushbackPos;
-
-  /** the current text position in the buffer */
-  private int zzCurrentPos;
-
-  /** startRead marks the beginning of the yytext() string in the buffer */
-  private int zzStartRead;
-
-  /** endRead marks the last character in the buffer, that has been read
-      from input */
-  private int zzEndRead;
-
-  /** number of newlines encountered up to the start of the matched text */
-  private int yyline;
-
-  /** the number of characters up to the start of the matched text */
-  private int yychar;
-
-  /**
-   * the number of characters from the last newline up to the start of the 
-   * matched text
-   */
-  private int yycolumn;
-
-  /** 
-   * zzAtBOL == true <=> the scanner is currently at the beginning of a line
-   */
-  private boolean zzAtBOL = true;
-
-  /** zzAtEOF == true <=> the scanner is at the EOF */
-  private boolean zzAtEOF;
-
-  /* user code: */
-
-public static final int ALPHANUM          = WikipediaTokenizer.ALPHANUM_ID;
-public static final int APOSTROPHE        = WikipediaTokenizer.APOSTROPHE_ID;
-public static final int ACRONYM           = WikipediaTokenizer.ACRONYM_ID;
-public static final int COMPANY           = WikipediaTokenizer.COMPANY_ID;
-public static final int EMAIL             = WikipediaTokenizer.EMAIL_ID;
-public static final int HOST              = WikipediaTokenizer.HOST_ID;
-public static final int NUM               = WikipediaTokenizer.NUM_ID;
-public static final int CJ                = WikipediaTokenizer.CJ_ID;
-public static final int INTERNAL_LINK     = WikipediaTokenizer.INTERNAL_LINK_ID;
-public static final int EXTERNAL_LINK     = WikipediaTokenizer.EXTERNAL_LINK_ID;
-public static final int CITATION          = WikipediaTokenizer.CITATION_ID;
-public static final int CATEGORY          = WikipediaTokenizer.CATEGORY_ID;
-public static final int BOLD              = WikipediaTokenizer.BOLD_ID;
-public static final int ITALICS           = WikipediaTokenizer.ITALICS_ID;
-public static final int BOLD_ITALICS      = WikipediaTokenizer.BOLD_ITALICS_ID;
-public static final int HEADING           = WikipediaTokenizer.HEADING_ID;
-public static final int SUB_HEADING       = WikipediaTokenizer.SUB_HEADING_ID;
-public static final int EXTERNAL_LINK_URL = WikipediaTokenizer.EXTERNAL_LINK_URL_ID;
-
-
-private int currentTokType;
-private int numBalanced = 0;
-private int positionInc = 1;
-private int numLinkToks = 0;
-//Anytime we start a new on a Wiki reserved token (category, link, etc.) this value will be 0, otherwise it will be the number of tokens seen
-//this can be useful for detecting when a new reserved token is encountered
-//see https://issues.apache.org/jira/browse/LUCENE-1133
-private int numWikiTokensSeen = 0;
-
-public static final String [] TOKEN_TYPES = WikipediaTokenizer.TOKEN_TYPES;
-
-/**
-Returns the number of tokens seen inside a category or link, etc.
-@return the number of tokens seen inside the context of wiki syntax.
-**/
-public final int getNumWikiTokensSeen(){
-  return numWikiTokensSeen;
-}
-
-public final int yychar()
-{
-    return yychar;
-}
-
-public final int getPositionIncrement(){
-  return positionInc;
-}
-
-/**
- * Fills Lucene token with the current token text.
- */
-final void getText(TermAttribute t) {
-  t.setTermBuffer(zzBuffer, zzStartRead, zzMarkedPos-zzStartRead);
-}
-
-final int setText(StringBuilder buffer){
-  int length = zzMarkedPos - zzStartRead;
-  buffer.append(zzBuffer, zzStartRead, length);
-  return length;
-}
-
-
-
-
-  /**
-   * Creates a new scanner
-   * There is also a java.io.InputStream version of this constructor.
-   *
-   * @param   in  the java.io.Reader to read input from.
-   */
-  WikipediaTokenizerImpl(java.io.Reader in) {
-    this.zzReader = in;
-  }
-
-  /**
-   * Creates a new scanner.
-   * There is also java.io.Reader version of this constructor.
-   *
-   * @param   in  the java.io.Inputstream to read input from.
-   */
-  WikipediaTokenizerImpl(java.io.InputStream in) {
-    this(new java.io.InputStreamReader(in));
-  }
-
-  /** 
-   * Unpacks the compressed character translation table.
-   *
-   * @param packed   the packed character translation table
-   * @return         the unpacked character translation table
-   */
-  private static char [] zzUnpackCMap(String packed) {
-    char [] map = new char[0x10000];
-    int i = 0;  /* index in packed string  */
-    int j = 0;  /* index in unpacked array */
-    while (i < 230) {
-      int  count = packed.charAt(i++);
-      char value = packed.charAt(i++);
-      do map[j++] = value; while (--count > 0);
-    }
-    return map;
-  }
-
-
-  /**
-   * Refills the input buffer.
-   *
-   * @return      <code>false</code>, iff there was new input.
-   * 
-   * @exception   java.io.IOException  if any I/O-Error occurs
-   */
-  private boolean zzRefill() throws java.io.IOException {
-
-    /* first: make room (if you can) */
-    if (zzStartRead > 0) {
-      System.arraycopy(zzBuffer, zzStartRead,
-                       zzBuffer, 0,
-                       zzEndRead-zzStartRead);
-
-      /* translate stored positions */
-      zzEndRead-= zzStartRead;
-      zzCurrentPos-= zzStartRead;
-      zzMarkedPos-= zzStartRead;
-      zzPushbackPos-= zzStartRead;
-      zzStartRead = 0;
-    }
-
-    /* is the buffer big enough? */
-    if (zzCurrentPos >= zzBuffer.length) {
-      /* if not: blow it up */
-      char newBuffer[] = new char[zzCurrentPos*2];
-      System.arraycopy(zzBuffer, 0, newBuffer, 0, zzBuffer.length);
-      zzBuffer = newBuffer;
-    }
-
-    /* finally: fill the buffer with new input */
-    int numRead = zzReader.read(zzBuffer, zzEndRead,
-                                            zzBuffer.length-zzEndRead);
-
-    if (numRead < 0) {
-      return true;
-    }
-    else {
-      zzEndRead+= numRead;
-      return false;
-    }
-  }
-
-    
-  /**
-   * Closes the input stream.
-   */
-  public final void yyclose() throws java.io.IOException {
-    zzAtEOF = true;            /* indicate end of file */
-    zzEndRead = zzStartRead;  /* invalidate buffer    */
-
-    if (zzReader != null)
-      zzReader.close();
-  }
-
-
-  /**
-   * Resets the scanner to read from a new input stream.
-   * Does not close the old reader.
-   *
-   * All internal variables are reset, the old input stream 
-   * <b>cannot</b> be reused (internal buffer is discarded and lost).
-   * Lexical state is set to <tt>ZZ_INITIAL</tt>.
-   *
-   * @param reader   the new input stream 
-   */
-  public final void yyreset(java.io.Reader reader) {
-    zzReader = reader;
-    zzAtBOL  = true;
-    zzAtEOF  = false;
-    zzEndRead = zzStartRead = 0;
-    zzCurrentPos = zzMarkedPos = zzPushbackPos = 0;
-    yyline = yychar = yycolumn = 0;
-    zzLexicalState = YYINITIAL;
-  }
-
-
-  /**
-   * Returns the current lexical state.
-   */
-  public final int yystate() {
-    return zzLexicalState;
-  }
-
-
-  /**
-   * Enters a new lexical state
-   *
-   * @param newState the new lexical state
-   */
-  public final void yybegin(int newState) {
-    zzLexicalState = newState;
-  }
-
-
-  /**
-   * Returns the text matched by the current regular expression.
-   */
-  public final String yytext() {
-    return new String( zzBuffer, zzStartRead, zzMarkedPos-zzStartRead );
-  }
-
-
-  /**
-   * Returns the character at position <tt>pos</tt> from the 
-   * matched text. 
-   * 
-   * It is equivalent to yytext().charAt(pos), but faster
-   *
-   * @param pos the position of the character to fetch. 
-   *            A value from 0 to yylength()-1.
-   *
-   * @return the character at position pos
-   */
-  public final char yycharat(int pos) {
-    return zzBuffer[zzStartRead+pos];
-  }
-
-
-  /**
-   * Returns the length of the matched text region.
-   */
-  public final int yylength() {
-    return zzMarkedPos-zzStartRead;
-  }
-
-
-  /**
-   * Reports an error that occured while scanning.
-   *
-   * In a wellformed scanner (no or only correct usage of 
-   * yypushback(int) and a match-all fallback rule) this method 
-   * will only be called with things that "Can't Possibly Happen".
-   * If this method is called, something is seriously wrong
-   * (e.g. a JFlex bug producing a faulty scanner etc.).
-   *
-   * Usual syntax/scanner level error handling should be done
-   * in error fallback rules.
-   *
-   * @param   errorCode  the code of the errormessage to display
-   */
-  private void zzScanError(int errorCode) {
-    String message;
-    try {
-      message = ZZ_ERROR_MSG[errorCode];
-    }
-    catch (ArrayIndexOutOfBoundsException e) {
-      message = ZZ_ERROR_MSG[ZZ_UNKNOWN_ERROR];
-    }
-
-    throw new Error(message);
-  } 
-
-
-  /**
-   * Pushes the specified amount of characters back into the input stream.
-   *
-   * They will be read again by then next call of the scanning method
-   *
-   * @param number  the number of characters to be read again.
-   *                This number must not be greater than yylength()!
-   */
-  public void yypushback(int number)  {
-    if ( number > yylength() )
-      zzScanError(ZZ_PUSHBACK_2BIG);
-
-    zzMarkedPos -= number;
-  }
-
-
-  /**
-   * Resumes scanning until the next regular expression is matched,
-   * the end of input is encountered or an I/O-Error occurs.
-   *
-   * @return      the next token
-   * @exception   java.io.IOException  if any I/O-Error occurs
-   */
-  public int getNextToken() throws java.io.IOException {
-    int zzInput;
-    int zzAction;
-
-    // cached fields:
-    int zzCurrentPosL;
-    int zzMarkedPosL;
-    int zzEndReadL = zzEndRead;
-    char [] zzBufferL = zzBuffer;
-    char [] zzCMapL = ZZ_CMAP;
-
-    int [] zzTransL = ZZ_TRANS;
-    int [] zzRowMapL = ZZ_ROWMAP;
-    int [] zzAttrL = ZZ_ATTRIBUTE;
-
-    while (true) {
-      zzMarkedPosL = zzMarkedPos;
-
-      yychar+= zzMarkedPosL-zzStartRead;
-
-      zzAction = -1;
-
-      zzCurrentPosL = zzCurrentPos = zzStartRead = zzMarkedPosL;
-  
-      zzState = zzLexicalState;
-
-
-      zzForAction: {
-        while (true) {
-    
-          if (zzCurrentPosL < zzEndReadL)
-            zzInput = zzBufferL[zzCurrentPosL++];
-          else if (zzAtEOF) {
-            zzInput = YYEOF;
-            break zzForAction;
-          }
-          else {
-            // store back cached positions
-            zzCurrentPos  = zzCurrentPosL;
-            zzMarkedPos   = zzMarkedPosL;
-            boolean eof = zzRefill();
-            // get translated positions and possibly new buffer
-            zzCurrentPosL  = zzCurrentPos;
-            zzMarkedPosL   = zzMarkedPos;
-            zzBufferL      = zzBuffer;
-            zzEndReadL     = zzEndRead;
-            if (eof) {
-              zzInput = YYEOF;
-              break zzForAction;
-            }
-            else {
-              zzInput = zzBufferL[zzCurrentPosL++];
-            }
-          }
-          int zzNext = zzTransL[ zzRowMapL[zzState] + zzCMapL[zzInput] ];
-          if (zzNext == -1) break zzForAction;
-          zzState = zzNext;
-
-          int zzAttributes = zzAttrL[zzState];
-          if ( (zzAttributes & 1) == 1 ) {
-            zzAction = zzState;
-            zzMarkedPosL = zzCurrentPosL;
-            if ( (zzAttributes & 8) == 8 ) break zzForAction;
-          }
-
-        }
-      }
-
-      // store back cached position
-      zzMarkedPos = zzMarkedPosL;
-
-      switch (zzAction < 0 ? zzAction : ZZ_ACTION[zzAction]) {
-        case 8: 
-          { /* ignore */
-          }
-        case 46: break;
-        case 28: 
-          { currentTokType = INTERNAL_LINK; numWikiTokensSeen = 0; yybegin(INTERNAL_LINK_STATE);
-          }
-        case 47: break;
-        case 3: 
-          { positionInc = 1; return CJ;
-          }
-        case 48: break;
-        case 30: 
-          { numBalanced = 0;currentTokType = ALPHANUM; yybegin(YYINITIAL);/*end italics*/
-          }
-        case 49: break;
-        case 10: 
-          { numLinkToks = 0; positionInc = 0; yybegin(YYINITIAL);
-          }
-        case 50: break;
-        case 41: 
-          { numBalanced = 0;currentTokType = ALPHANUM; yybegin(YYINITIAL);/*end bold italics*/
-          }
-        case 51: break;
-        case 7: 
-          { yybegin(INTERNAL_LINK_STATE); numWikiTokensSeen++; return currentTokType;
-          }
-        case 52: break;
-        case 23: 
-          { numWikiTokensSeen = 0; positionInc = 1; yybegin(DOUBLE_EQUALS_STATE);
-          }
-        case 53: break;
-        case 38: 
-          { numBalanced = 0;currentTokType = ALPHANUM; yybegin(YYINITIAL);/*end sub header*/
-          }
-        case 54: break;
-        case 17: 
-          { yybegin(DOUBLE_BRACE_STATE); numWikiTokensSeen = 0; return currentTokType;
-          }
-        case 55: break;
-        case 24: 
-          { numWikiTokensSeen = 0; positionInc = 1; currentTokType = INTERNAL_LINK; yybegin(INTERNAL_LINK_STATE);
-          }
-        case 56: break;
-        case 14: 
-          { yybegin(STRING); numWikiTokensSeen++; return currentTokType;
-          }
-        case 57: break;
-        case 5: 
-          { positionInc = 1;
-          }
-        case 58: break;
-        case 43: 
-          { numWikiTokensSeen = 0; positionInc = 1; currentTokType = CATEGORY; yybegin(CATEGORY_STATE);
-          }
-        case 59: break;
-        case 26: 
-          { yybegin(YYINITIAL);
-          }
-        case 60: break;
-        case 20: 
-          { numBalanced = 0; numWikiTokensSeen = 0; currentTokType = EXTERNAL_LINK;yybegin(EXTERNAL_LINK_STATE);
-          }
-        case 61: break;
-        case 1: 
-          { numWikiTokensSeen = 0;  positionInc = 1;
-          }
-        case 62: break;
-        case 40: 
-          { positionInc = 1; return EMAIL;
-          }
-        case 63: break;
-        case 25: 
-          { numWikiTokensSeen = 0; positionInc = 1; currentTokType = CITATION; yybegin(DOUBLE_BRACE_STATE);
-          }
-        case 64: break;
-        case 39: 
-          { positionInc = 1; return ACRONYM;
-          }
-        case 65: break;
-        case 9: 
-          { if (numLinkToks == 0){positionInc = 0;} else{positionInc = 1;} numWikiTokensSeen++; currentTokType = EXTERNAL_LINK; yybegin(EXTERNAL_LINK_STATE); numLinkToks++; return currentTokType;
-          }
-        case 66: break;
-        case 22: 
-          { numWikiTokensSeen = 0; positionInc = 1; if (numBalanced == 0){numBalanced++;yybegin(TWO_SINGLE_QUOTES_STATE);} else{numBalanced = 0;}
-          }
-        case 67: break;
-        case 31: 
-          { numBalanced = 0; numWikiTokensSeen = 0; currentTokType = INTERNAL_LINK;yybegin(INTERNAL_LINK_STATE);
-          }
-        case 68: break;
-        case 15: 
-          { currentTokType = SUB_HEADING; numWikiTokensSeen = 0; yybegin(STRING);
-          }
-        case 69: break;
-        case 18: 
-          { /* ignore STRING */
-          }
-        case 70: break;
-        case 42: 
-          { positionInc = 1; numWikiTokensSeen++; yybegin(EXTERNAL_LINK_STATE); return currentTokType;
-          }
-        case 71: break;
-        case 21: 
-          { yybegin(STRING); return currentTokType;/*pipe*/
-          }
-        case 72: break;
-        case 37: 
-          { numBalanced = 0;currentTokType = ALPHANUM;yybegin(YYINITIAL);/*end bold*/
-          }
-        case 73: break;
-        case 33: 
-          { positionInc = 1; return HOST;
-          }
-        case 74: break;
-        case 45: 
-          { numBalanced = 0; numWikiTokensSeen = 0; currentTokType = CATEGORY;yybegin(CATEGORY_STATE);
-          }
-        case 75: break;
-        case 36: 
-          { currentTokType = BOLD_ITALICS;  yybegin(FIVE_SINGLE_QUOTES_STATE);
-          }
-        case 76: break;
-        case 13: 
-          { currentTokType = EXTERNAL_LINK; numWikiTokensSeen = 0; yybegin(EXTERNAL_LINK_STATE);
-          }
-        case 77: break;
-        case 16: 
-          { currentTokType = HEADING; yybegin(DOUBLE_EQUALS_STATE); numWikiTokensSeen++; return currentTokType;
-          }
-        case 78: break;
-        case 12: 
-          { currentTokType = ITALICS; numWikiTokensSeen++;  yybegin(STRING); return currentTokType;/*italics*/
-          }
-        case 79: break;
-        case 6: 
-          { yybegin(CATEGORY_STATE); numWikiTokensSeen++; return currentTokType;
-          }
-        case 80: break;
-        case 32: 
-          { positionInc = 1; return APOSTROPHE;
-          }
-        case 81: break;
-        case 19: 
-          { yybegin(STRING); numWikiTokensSeen++; return currentTokType;/* STRING ALPHANUM*/
-          }
-        case 82: break;
-        case 34: 
-          { positionInc = 1; return NUM;
-          }
-        case 83: break;
-        case 44: 
-          { currentTokType = CATEGORY; numWikiTokensSeen = 0; yybegin(CATEGORY_STATE);
-          }
-        case 84: break;
-        case 2: 
-          { positionInc = 1; return ALPHANUM;
-          }
-        case 85: break;
-        case 35: 
-          { positionInc = 1; return COMPANY;
-          }
-        case 86: break;
-        case 11: 
-          { currentTokType = BOLD;  yybegin(THREE_SINGLE_QUOTES_STATE);
-          }
-        case 87: break;
-        case 29: 
-          { currentTokType = INTERNAL_LINK; numWikiTokensSeen = 0;  yybegin(INTERNAL_LINK_STATE);
-          }
-        case 88: break;
-        case 4: 
-          { numWikiTokensSeen = 0; positionInc = 1; currentTokType = EXTERNAL_LINK_URL; yybegin(EXTERNAL_LINK_STATE);
-          }
-        case 89: break;
-        case 27: 
-          { numLinkToks = 0; yybegin(YYINITIAL);
-          }
-        case 90: break;
-        default: 
-          if (zzInput == YYEOF && zzStartRead == zzCurrentPos) {
-            zzAtEOF = true;
-            return YYEOF;
-          } 
-          else {
-            zzScanError(ZZ_NO_MATCH);
-          }
-      }
-    }
-  }
-
-
-}
diff --git a/lucene/contrib/wikipedia/src/java/org/apache/lucene/wikipedia/analysis/WikipediaTokenizerImpl.jflex b/lucene/contrib/wikipedia/src/java/org/apache/lucene/wikipedia/analysis/WikipediaTokenizerImpl.jflex
deleted file mode 100644
index e74b21a..0000000
--- a/lucene/contrib/wikipedia/src/java/org/apache/lucene/wikipedia/analysis/WikipediaTokenizerImpl.jflex
+++ /dev/null
@@ -1,330 +0,0 @@
-package org.apache.lucene.wikipedia.analysis;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.analysis.Token;
-
-%%
-
-%class WikipediaTokenizerImpl
-%unicode
-%integer
-%function getNextToken
-%pack
-%char
-
-%{
-
-public static final int ALPHANUM          = WikipediaTokenizer.ALPHANUM_ID;
-public static final int APOSTROPHE        = WikipediaTokenizer.APOSTROPHE_ID;
-public static final int ACRONYM           = WikipediaTokenizer.ACRONYM_ID;
-public static final int COMPANY           = WikipediaTokenizer.COMPANY_ID;
-public static final int EMAIL             = WikipediaTokenizer.EMAIL_ID;
-public static final int HOST              = WikipediaTokenizer.HOST_ID;
-public static final int NUM               = WikipediaTokenizer.NUM_ID;
-public static final int CJ                = WikipediaTokenizer.CJ_ID;
-public static final int INTERNAL_LINK     = WikipediaTokenizer.INTERNAL_LINK_ID;
-public static final int EXTERNAL_LINK     = WikipediaTokenizer.EXTERNAL_LINK_ID;
-public static final int CITATION          = WikipediaTokenizer.CITATION_ID;
-public static final int CATEGORY          = WikipediaTokenizer.CATEGORY_ID;
-public static final int BOLD              = WikipediaTokenizer.BOLD_ID;
-public static final int ITALICS           = WikipediaTokenizer.ITALICS_ID;
-public static final int BOLD_ITALICS      = WikipediaTokenizer.BOLD_ITALICS_ID;
-public static final int HEADING           = WikipediaTokenizer.HEADING_ID;
-public static final int SUB_HEADING       = WikipediaTokenizer.SUB_HEADING_ID;
-public static final int EXTERNAL_LINK_URL = WikipediaTokenizer.EXTERNAL_LINK_URL_ID;
-
-
-private int currentTokType;
-private int numBalanced = 0;
-private int positionInc = 1;
-private int numLinkToks = 0;
-//Anytime we start a new on a Wiki reserved token (category, link, etc.) this value will be 0, otherwise it will be the number of tokens seen
-//this can be useful for detecting when a new reserved token is encountered
-//see https://issues.apache.org/jira/browse/LUCENE-1133
-private int numWikiTokensSeen = 0;
-
-public static final String [] TOKEN_TYPES = WikipediaTokenizer.TOKEN_TYPES;
-
-/**
-Returns the number of tokens seen inside a category or link, etc.
-@return the number of tokens seen inside the context of wiki syntax.
-**/
-public final int getNumWikiTokensSeen(){
-  return numWikiTokensSeen;
-}
-
-public final int yychar()
-{
-    return yychar;
-}
-
-public final int getPositionIncrement(){
-  return positionInc;
-}
-
-/**
- * Fills Lucene token with the current token text.
- */
-final void getText(Token t) {
-  t.setTermBuffer(zzBuffer, zzStartRead, zzMarkedPos-zzStartRead);
-}
-
-final int setText(StringBuilder buffer){
-  int length = zzMarkedPos - zzStartRead;
-  buffer.append(zzBuffer, zzStartRead, length);
-  return length;
-}
-
-
-%}
-
-// basic word: a sequence of digits & letters
-ALPHANUM   = ({LETTER}|{DIGIT}|{KOREAN})+
-
-// internal apostrophes: O'Reilly, you're, O'Reilly's
-// use a post-filter to remove possesives
-APOSTROPHE =  {ALPHA} ("'" {ALPHA})+
-
-// acronyms: U.S.A., I.B.M., etc.
-// use a post-filter to remove dots
-ACRONYM    =  {ALPHA} "." ({ALPHA} ".")+
-
-// company names like AT&T and Excite@Home.
-COMPANY    =  {ALPHA} ("&"|"@") {ALPHA}
-
-// email addresses
-EMAIL      =  {ALPHANUM} (("."|"-"|"_") {ALPHANUM})* "@" {ALPHANUM} (("."|"-") {ALPHANUM})+
-
-// hostname
-HOST       =  {ALPHANUM} ((".") {ALPHANUM})+
-
-// floating point, serial, model numbers, ip addresses, etc.
-// every other segment must have at least one digit
-NUM        = ({ALPHANUM} {P} {HAS_DIGIT}
-           | {DIGIT}+ {P} {DIGIT}+
-           | {HAS_DIGIT} {P} {ALPHANUM}
-           | {ALPHANUM} ({P} {HAS_DIGIT} {P} {ALPHANUM})+
-           | {HAS_DIGIT} ({P} {ALPHANUM} {P} {HAS_DIGIT})+
-           | {ALPHANUM} {P} {HAS_DIGIT} ({P} {ALPHANUM} {P} {HAS_DIGIT})+
-           | {HAS_DIGIT} {P} {ALPHANUM} ({P} {HAS_DIGIT} {P} {ALPHANUM})+)
-
-TAGS = "<"\/?{ALPHANUM}({WHITESPACE}*{ALPHANUM}=\"{ALPHANUM}\")*">"
-
-// punctuation
-P	         = ("_"|"-"|"/"|"."|",")
-
-// at least one digit
-HAS_DIGIT  =
-    ({LETTER}|{DIGIT})*
-    {DIGIT}
-    ({LETTER}|{DIGIT})*
-
-ALPHA      = ({LETTER})+
-
-
-LETTER     = [\u0041-\u005a\u0061-\u007a\u00c0-\u00d6\u00d8-\u00f6\u00f8-\u00ff\u0100-\u1fff\uffa0-\uffdc]
-
-DIGIT      = [\u0030-\u0039\u0660-\u0669\u06f0-\u06f9\u0966-\u096f\u09e6-\u09ef\u0a66-\u0a6f\u0ae6-\u0aef\u0b66-\u0b6f\u0be7-\u0bef\u0c66-\u0c6f\u0ce6-\u0cef\u0d66-\u0d6f\u0e50-\u0e59\u0ed0-\u0ed9\u1040-\u1049]
-
-KOREAN     = [\uac00-\ud7af\u1100-\u11ff]
-
-// Chinese, Japanese
-CJ         = [\u3040-\u318f\u3100-\u312f\u3040-\u309F\u30A0-\u30FF\u31F0-\u31FF\u3300-\u337f\u3400-\u4dbf\u4e00-\u9fff\uf900-\ufaff\uff65-\uff9f]
-
-WHITESPACE = \r\n | [ \r\n\t\f]
-
-//Wikipedia
-DOUBLE_BRACKET = "["{2}
-DOUBLE_BRACKET_CLOSE = "]"{2}
-DOUBLE_BRACKET_CAT = "["{2}":"?"Category:"
-EXTERNAL_LINK = "["
-TWO_SINGLE_QUOTES = "'"{2}
-CITATION = "<ref>"
-CITATION_CLOSE = "</ref>"
-INFOBOX = {DOUBLE_BRACE}("I"|"i")nfobox_
-
-DOUBLE_BRACE = "{"{2}
-DOUBLE_BRACE_CLOSE = "}"{2}
-PIPE = "|"
-DOUBLE_EQUALS = "="{2}
-
-
-%state CATEGORY_STATE
-%state INTERNAL_LINK_STATE
-%state EXTERNAL_LINK_STATE
-
-%state TWO_SINGLE_QUOTES_STATE
-%state THREE_SINGLE_QUOTES_STATE
-%state FIVE_SINGLE_QUOTES_STATE
-%state DOUBLE_EQUALS_STATE
-%state DOUBLE_BRACE_STATE
-%state STRING
-
-%%
-
-<YYINITIAL>{ALPHANUM}                                                     {positionInc = 1; return ALPHANUM; }
-<YYINITIAL>{APOSTROPHE}                                                   {positionInc = 1; return APOSTROPHE; }
-<YYINITIAL>{ACRONYM}                                                      {positionInc = 1; return ACRONYM; }
-<YYINITIAL>{COMPANY}                                                      {positionInc = 1; return COMPANY; }
-<YYINITIAL>{EMAIL}                                                        {positionInc = 1; return EMAIL; }
-<YYINITIAL>{NUM}                                                          {positionInc = 1; return NUM; }
-<YYINITIAL>{HOST}                                                         {positionInc = 1; return HOST; }
-<YYINITIAL>{CJ}                                                           {positionInc = 1; return CJ; }
-
-//wikipedia
-<YYINITIAL>{
-  //First {ALPHANUM} is always the link, set positioninc to 1 for double bracket, but then inside the internal link state
-  //set it to 0 for the next token, such that the link and the first token are in the same position, but then subsequent
-  //tokens within the link are incremented
-  {DOUBLE_BRACKET} {numWikiTokensSeen = 0; positionInc = 1; currentTokType = INTERNAL_LINK; yybegin(INTERNAL_LINK_STATE);}
-  {DOUBLE_BRACKET_CAT} {numWikiTokensSeen = 0; positionInc = 1; currentTokType = CATEGORY; yybegin(CATEGORY_STATE);}
-  {EXTERNAL_LINK} {numWikiTokensSeen = 0; positionInc = 1; currentTokType = EXTERNAL_LINK_URL; yybegin(EXTERNAL_LINK_STATE);}
-  {TWO_SINGLE_QUOTES} {numWikiTokensSeen = 0; positionInc = 1; if (numBalanced == 0){numBalanced++;yybegin(TWO_SINGLE_QUOTES_STATE);} else{numBalanced = 0;}}
-  {DOUBLE_EQUALS} {numWikiTokensSeen = 0; positionInc = 1; yybegin(DOUBLE_EQUALS_STATE);}
-  {DOUBLE_BRACE} {numWikiTokensSeen = 0; positionInc = 1; currentTokType = CITATION; yybegin(DOUBLE_BRACE_STATE);}
-  {CITATION} {numWikiTokensSeen = 0; positionInc = 1; currentTokType = CITATION; yybegin(DOUBLE_BRACE_STATE);}
-//ignore
-  . | {WHITESPACE} |{INFOBOX}                                               {numWikiTokensSeen = 0;  positionInc = 1; }
-}
-
-<INTERNAL_LINK_STATE>{
-//First {ALPHANUM} is always the link, set position to 0 for these
-//This is slightly different from EXTERNAL_LINK_STATE because that one has an explicit grammar for capturing the URL
-  {ALPHANUM} {yybegin(INTERNAL_LINK_STATE); numWikiTokensSeen++; return currentTokType;}
-  {DOUBLE_BRACKET_CLOSE} {numLinkToks = 0; yybegin(YYINITIAL);}
-  //ignore
-  . | {WHITESPACE}                                               { positionInc = 1; }
-}
-
-<EXTERNAL_LINK_STATE>{
-//increment the link token, but then don't increment the tokens after that which are still in the link
-  ("http://"|"https://"){HOST}("/"?({ALPHANUM}|{P}|\?|"&"|"="|"#")*)* {positionInc = 1; numWikiTokensSeen++; yybegin(EXTERNAL_LINK_STATE); return currentTokType;}
-  {ALPHANUM} {if (numLinkToks == 0){positionInc = 0;} else{positionInc = 1;} numWikiTokensSeen++; currentTokType = EXTERNAL_LINK; yybegin(EXTERNAL_LINK_STATE); numLinkToks++; return currentTokType;}
-  "]" {numLinkToks = 0; positionInc = 0; yybegin(YYINITIAL);}
-  {WHITESPACE}                                               { positionInc = 1; }
-}
-
-<CATEGORY_STATE>{
-  {ALPHANUM} {yybegin(CATEGORY_STATE); numWikiTokensSeen++; return currentTokType;}
-  {DOUBLE_BRACKET_CLOSE} {yybegin(YYINITIAL);}
-  //ignore
-  . | {WHITESPACE}                                               { positionInc = 1; }
-}
-//italics
-<TWO_SINGLE_QUOTES_STATE>{
-  "'" {currentTokType = BOLD;  yybegin(THREE_SINGLE_QUOTES_STATE);}
-   "'''" {currentTokType = BOLD_ITALICS;  yybegin(FIVE_SINGLE_QUOTES_STATE);}
-   {ALPHANUM} {currentTokType = ITALICS; numWikiTokensSeen++;  yybegin(STRING); return currentTokType;/*italics*/}
-   //we can have links inside, let those override
-   {DOUBLE_BRACKET} {currentTokType = INTERNAL_LINK; numWikiTokensSeen = 0; yybegin(INTERNAL_LINK_STATE);}
-   {DOUBLE_BRACKET_CAT} {currentTokType = CATEGORY; numWikiTokensSeen = 0; yybegin(CATEGORY_STATE);}
-   {EXTERNAL_LINK} {currentTokType = EXTERNAL_LINK; numWikiTokensSeen = 0; yybegin(EXTERNAL_LINK_STATE);}
-
-   //ignore
-  . | {WHITESPACE}                                               { /* ignore */ }
-}
-//bold
-<THREE_SINGLE_QUOTES_STATE>{
-  {ALPHANUM} {yybegin(STRING); numWikiTokensSeen++; return currentTokType;}
-  //we can have links inside, let those override
-   {DOUBLE_BRACKET} {currentTokType = INTERNAL_LINK; numWikiTokensSeen = 0; yybegin(INTERNAL_LINK_STATE);}
-   {DOUBLE_BRACKET_CAT} {currentTokType = CATEGORY; numWikiTokensSeen = 0; yybegin(CATEGORY_STATE);}
-   {EXTERNAL_LINK} {currentTokType = EXTERNAL_LINK; numWikiTokensSeen = 0; yybegin(EXTERNAL_LINK_STATE);}
-
-   //ignore
-  . | {WHITESPACE}                                               { /* ignore */ }
-
-}
-//bold italics
-<FIVE_SINGLE_QUOTES_STATE>{
-  {ALPHANUM} {yybegin(STRING); numWikiTokensSeen++; return currentTokType;}
-  //we can have links inside, let those override
-   {DOUBLE_BRACKET} {currentTokType = INTERNAL_LINK; numWikiTokensSeen = 0;  yybegin(INTERNAL_LINK_STATE);}
-   {DOUBLE_BRACKET_CAT} {currentTokType = CATEGORY; numWikiTokensSeen = 0; yybegin(CATEGORY_STATE);}
-   {EXTERNAL_LINK} {currentTokType = EXTERNAL_LINK; numWikiTokensSeen = 0; yybegin(EXTERNAL_LINK_STATE);}
-
-   //ignore
-  . | {WHITESPACE}                                               { /* ignore */ }
-}
-
-<DOUBLE_EQUALS_STATE>{
- "=" {currentTokType = SUB_HEADING; numWikiTokensSeen = 0; yybegin(STRING);}
- {ALPHANUM} {currentTokType = HEADING; yybegin(DOUBLE_EQUALS_STATE); numWikiTokensSeen++; return currentTokType;}
- {DOUBLE_EQUALS} {yybegin(YYINITIAL);}
-  //ignore
-  . | {WHITESPACE}                                               { /* ignore */ }
-}
-
-<DOUBLE_BRACE_STATE>{
-  {ALPHANUM} {yybegin(DOUBLE_BRACE_STATE); numWikiTokensSeen = 0; return currentTokType;}
-  {DOUBLE_BRACE_CLOSE} {yybegin(YYINITIAL);}
-  {CITATION_CLOSE} {yybegin(YYINITIAL);}
-   //ignore
-  . | {WHITESPACE}                                               { /* ignore */ }
-}
-
-<STRING> {
-  "'''''" {numBalanced = 0;currentTokType = ALPHANUM; yybegin(YYINITIAL);/*end bold italics*/}
-  "'''" {numBalanced = 0;currentTokType = ALPHANUM;yybegin(YYINITIAL);/*end bold*/}
-  "''" {numBalanced = 0;currentTokType = ALPHANUM; yybegin(YYINITIAL);/*end italics*/}
-  "===" {numBalanced = 0;currentTokType = ALPHANUM; yybegin(YYINITIAL);/*end sub header*/}
-  {ALPHANUM} {yybegin(STRING); numWikiTokensSeen++; return currentTokType;/* STRING ALPHANUM*/}
-  //we can have links inside, let those override
-   {DOUBLE_BRACKET} {numBalanced = 0; numWikiTokensSeen = 0; currentTokType = INTERNAL_LINK;yybegin(INTERNAL_LINK_STATE);}
-   {DOUBLE_BRACKET_CAT} {numBalanced = 0; numWikiTokensSeen = 0; currentTokType = CATEGORY;yybegin(CATEGORY_STATE);}
-   {EXTERNAL_LINK} {numBalanced = 0; numWikiTokensSeen = 0; currentTokType = EXTERNAL_LINK;yybegin(EXTERNAL_LINK_STATE);}
-
-
-  {PIPE} {yybegin(STRING); return currentTokType;/*pipe*/}
-
-  .|{WHITESPACE}                                              { /* ignore STRING */ }
-}
-
-
-
-
-/*
-{INTERNAL_LINK}                                                { return curentTokType; }
-
-{CITATION}                                                { return currentTokType; }
-{CATEGORY}                                                { return currentTokType; }
-
-{BOLD}                                                { return currentTokType; }
-{ITALICS}                                                { return currentTokType; }
-{BOLD_ITALICS}                                                { return currentTokType; }
-{HEADING}                                                { return currentTokType; }
-{SUB_HEADING}                                                { return currentTokType; }
-
-*/
-//end wikipedia
-
-/** Ignore the rest */
-. | {WHITESPACE}|{TAGS}                                                { /* ignore */ }
-
-
-//INTERNAL_LINK = "["{2}({ALPHANUM}+{WHITESPACE}*)+"]"{2}
-//EXTERNAL_LINK = "["http://"{HOST}.*?"]"
-//CITATION = "{"{2}({ALPHANUM}+{WHITESPACE}*)+"}"{2}
-//CATEGORY = "["{2}"Category:"({ALPHANUM}+{WHITESPACE}*)+"]"{2}
-//CATEGORY_COLON = "["{2}":Category:"({ALPHANUM}+{WHITESPACE}*)+"]"{2}
-//BOLD = '''({ALPHANUM}+{WHITESPACE}*)+'''
-//ITALICS = ''({ALPHANUM}+{WHITESPACE}*)+''
-//BOLD_ITALICS = '''''({ALPHANUM}+{WHITESPACE}*)+'''''
-//HEADING = "="{2}({ALPHANUM}+{WHITESPACE}*)+"="{2}
-//SUB_HEADING ="="{3}({ALPHANUM}+{WHITESPACE}*)+"="{3}
diff --git a/lucene/contrib/wikipedia/src/java/org/apache/lucene/wikipedia/analysis/package.html b/lucene/contrib/wikipedia/src/java/org/apache/lucene/wikipedia/analysis/package.html
deleted file mode 100644
index 7b23869..0000000
--- a/lucene/contrib/wikipedia/src/java/org/apache/lucene/wikipedia/analysis/package.html
+++ /dev/null
@@ -1,22 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<body>
-Tokenizer that is aware of Wikipedia syntax.
-</body>
-</html>
diff --git a/lucene/contrib/wikipedia/src/java/org/apache/lucene/wikipedia/package.html b/lucene/contrib/wikipedia/src/java/org/apache/lucene/wikipedia/package.html
deleted file mode 100644
index e0f30a6..0000000
--- a/lucene/contrib/wikipedia/src/java/org/apache/lucene/wikipedia/package.html
+++ /dev/null
@@ -1,35 +0,0 @@
-<!--
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-  -->
-
-<HTML>
- <!--
- * 
- --><HEAD>
-    <TITLE>org.apache.lucene.wikipedia</TITLE>
-</HEAD>
-<BODY>
-<DIV>Tools for working with <a href="http://www.wikipedia.org">Wikipedia</a> content.
-</DIV>
-<DIV>&nbsp;</DIV>
-<DIV align="center">
-Copyright &copy; 2007 <A HREF="http://www.apache.org">Apache Software Foundation</A>
-</DIV>
-</BODY>
-</HTML>
\ No newline at end of file
diff --git a/lucene/contrib/wikipedia/src/java/overview.html b/lucene/contrib/wikipedia/src/java/overview.html
deleted file mode 100644
index 118283f..0000000
--- a/lucene/contrib/wikipedia/src/java/overview.html
+++ /dev/null
@@ -1,26 +0,0 @@
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-  <head>
-    <title>
-    wikipedia
-    </title>
-  </head>
-  <body>
-  wikipedia
-  </body>
-</html>
\ No newline at end of file
diff --git a/lucene/contrib/wikipedia/src/test/org/apache/lucene/wikipedia/analysis/WikipediaTokenizerTest.java b/lucene/contrib/wikipedia/src/test/org/apache/lucene/wikipedia/analysis/WikipediaTokenizerTest.java
deleted file mode 100644
index 77f7fd4..0000000
--- a/lucene/contrib/wikipedia/src/test/org/apache/lucene/wikipedia/analysis/WikipediaTokenizerTest.java
+++ /dev/null
@@ -1,558 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-package org.apache.lucene.wikipedia.analysis;
-
-import java.io.StringReader;
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-import java.util.Set;
-import java.util.HashSet;
-
-import org.apache.lucene.analysis.BaseTokenStreamTestCase;
-import org.apache.lucene.analysis.tokenattributes.FlagsAttribute;
-import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
-import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
-import org.apache.lucene.analysis.tokenattributes.TermAttribute;
-import org.apache.lucene.analysis.tokenattributes.TypeAttribute;
-
-
-/**
- *
- *
- **/
-public class WikipediaTokenizerTest extends BaseTokenStreamTestCase {
-  protected static final String LINK_PHRASES = "click [[link here again]] click [http://lucene.apache.org here again] [[Category:a b c d]]";
-
-  public WikipediaTokenizerTest(String s) {
-    super(s);
-  }
-
-  public void testSimple() throws Exception {
-    String text = "This is a [[Category:foo]]";
-    WikipediaTokenizer tf = new WikipediaTokenizer(new StringReader(text));
-    assertTokenStreamContents(tf,
-        new String[] { "This", "is", "a", "foo" },
-        new int[] { 0, 5, 8, 21 },
-        new int[] { 4, 7, 9, 24 },
-        new String[] { "<ALPHANUM>", "<ALPHANUM>", "<ALPHANUM>", WikipediaTokenizer.CATEGORY },
-        new int[] { 1, 1, 1, 1, },
-        text.length());
-  }
-  
-  public void testHandwritten() throws Exception {
-    //make sure all tokens are in only one type
-    String test = "[[link]] This is a [[Category:foo]] Category  This is a linked [[:Category:bar none withstanding]] " +
-            "Category This is (parens) This is a [[link]]  This is an external URL [http://lucene.apache.org] " +
-            "Here is ''italics'' and ''more italics'', '''bold''' and '''''five quotes''''' " +
-            " This is a [[link|display info]]  This is a period.  Here is $3.25 and here is 3.50.  Here's Johnny.  " +
-            "==heading== ===sub head=== followed by some text  [[Category:blah| ]] " +
-            "''[[Category:ital_cat]]''  here is some that is ''italics [[Category:foo]] but is never closed." +
-            "'''same [[Category:foo]] goes for this '''''and2 [[Category:foo]] and this" +
-            " [http://foo.boo.com/test/test/ Test Test] [http://foo.boo.com/test/test/test.html Test Test]" +
-            " [http://foo.boo.com/test/test/test.html?g=b&c=d Test Test] <ref>Citation</ref> <sup>martian</sup> <span class=\"glue\">code</span>";
-    Map<String,String> tcm = new HashMap<String,String>();//map tokens to types
-    tcm.put("link", WikipediaTokenizer.INTERNAL_LINK);
-    tcm.put("display", WikipediaTokenizer.INTERNAL_LINK);
-    tcm.put("info", WikipediaTokenizer.INTERNAL_LINK);
-
-    tcm.put("http://lucene.apache.org", WikipediaTokenizer.EXTERNAL_LINK_URL);
-    tcm.put("http://foo.boo.com/test/test/", WikipediaTokenizer.EXTERNAL_LINK_URL);
-    tcm.put("http://foo.boo.com/test/test/test.html", WikipediaTokenizer.EXTERNAL_LINK_URL);
-    tcm.put("http://foo.boo.com/test/test/test.html?g=b&c=d", WikipediaTokenizer.EXTERNAL_LINK_URL);
-    tcm.put("Test", WikipediaTokenizer.EXTERNAL_LINK);
-    
-    //alphanums
-    tcm.put("This", "<ALPHANUM>");
-    tcm.put("is", "<ALPHANUM>");
-    tcm.put("a", "<ALPHANUM>");
-    tcm.put("Category", "<ALPHANUM>");
-    tcm.put("linked", "<ALPHANUM>");
-    tcm.put("parens", "<ALPHANUM>");
-    tcm.put("external", "<ALPHANUM>");
-    tcm.put("URL", "<ALPHANUM>");
-    tcm.put("and", "<ALPHANUM>");
-    tcm.put("period", "<ALPHANUM>");
-    tcm.put("Here", "<ALPHANUM>");
-    tcm.put("Here's", "<APOSTROPHE>");
-    tcm.put("here", "<ALPHANUM>");
-    tcm.put("Johnny", "<ALPHANUM>");
-    tcm.put("followed", "<ALPHANUM>");
-    tcm.put("by", "<ALPHANUM>");
-    tcm.put("text", "<ALPHANUM>");
-    tcm.put("that", "<ALPHANUM>");
-    tcm.put("but", "<ALPHANUM>");
-    tcm.put("never", "<ALPHANUM>");
-    tcm.put("closed", "<ALPHANUM>");
-    tcm.put("goes", "<ALPHANUM>");
-    tcm.put("for", "<ALPHANUM>");
-    tcm.put("this", "<ALPHANUM>");
-    tcm.put("an", "<ALPHANUM>");
-    tcm.put("some", "<ALPHANUM>");
-    tcm.put("martian", "<ALPHANUM>");
-    tcm.put("code", "<ALPHANUM>");
-
-    tcm.put("foo", WikipediaTokenizer.CATEGORY);
-    tcm.put("bar", WikipediaTokenizer.CATEGORY);
-    tcm.put("none", WikipediaTokenizer.CATEGORY);
-    tcm.put("withstanding", WikipediaTokenizer.CATEGORY);
-    tcm.put("blah", WikipediaTokenizer.CATEGORY);
-    tcm.put("ital", WikipediaTokenizer.CATEGORY);
-    tcm.put("cat", WikipediaTokenizer.CATEGORY);
-
-    tcm.put("italics", WikipediaTokenizer.ITALICS);
-    tcm.put("more", WikipediaTokenizer.ITALICS);
-    tcm.put("bold", WikipediaTokenizer.BOLD);
-    tcm.put("same", WikipediaTokenizer.BOLD);
-    tcm.put("five", WikipediaTokenizer.BOLD_ITALICS);
-    tcm.put("and2", WikipediaTokenizer.BOLD_ITALICS);
-    tcm.put("quotes", WikipediaTokenizer.BOLD_ITALICS);
-
-    tcm.put("heading", WikipediaTokenizer.HEADING);
-    tcm.put("sub", WikipediaTokenizer.SUB_HEADING);
-    tcm.put("head", WikipediaTokenizer.SUB_HEADING);
-    
-    tcm.put("Citation", WikipediaTokenizer.CITATION);
-
-    tcm.put("3.25", "<NUM>");
-    tcm.put("3.50", "<NUM>");
-    WikipediaTokenizer tf = new WikipediaTokenizer(new StringReader(test));
-    int count = 0;
-    int numItalics = 0;
-    int numBoldItalics = 0;
-    int numCategory = 0;
-    int numCitation = 0;
-    TermAttribute termAtt = tf.addAttribute(TermAttribute.class);
-    TypeAttribute typeAtt = tf.addAttribute(TypeAttribute.class);
-    
-    while (tf.incrementToken()) {
-      String tokText = termAtt.term();
-      //System.out.println("Text: " + tokText + " Type: " + token.type());
-      String expectedType = tcm.get(tokText);
-      assertTrue("expectedType is null and it shouldn't be for: " + tf.toString(), expectedType != null);
-      assertTrue(typeAtt.type() + " is not equal to " + expectedType + " for " + tf.toString(), typeAtt.type().equals(expectedType) == true);
-      count++;
-      if (typeAtt.type().equals(WikipediaTokenizer.ITALICS)  == true){
-        numItalics++;
-      } else if (typeAtt.type().equals(WikipediaTokenizer.BOLD_ITALICS)  == true){
-        numBoldItalics++;
-      } else if (typeAtt.type().equals(WikipediaTokenizer.CATEGORY)  == true){
-        numCategory++;
-      }
-      else if (typeAtt.type().equals(WikipediaTokenizer.CITATION)  == true){
-        numCitation++;
-      }
-    }
-    assertTrue("We have not seen enough tokens: " + count + " is not >= " + tcm.size(), count >= tcm.size());
-    assertTrue(numItalics + " does not equal: " + 4 + " for numItalics", numItalics == 4);
-    assertTrue(numBoldItalics + " does not equal: " + 3 + " for numBoldItalics", numBoldItalics == 3);
-    assertTrue(numCategory + " does not equal: " + 10 + " for numCategory", numCategory == 10);
-    assertTrue(numCitation + " does not equal: " + 1 + " for numCitation", numCitation == 1);
-  }
-
-  public void testLinkPhrases() throws Exception {
-
-    WikipediaTokenizer tf = new WikipediaTokenizer(new StringReader(LINK_PHRASES));
-    checkLinkPhrases(tf);
-    
-  }
-
-  private void checkLinkPhrases(WikipediaTokenizer tf) throws IOException {
-    TermAttribute termAtt = tf.addAttribute(TermAttribute.class);
-    PositionIncrementAttribute posIncrAtt = tf.addAttribute(PositionIncrementAttribute.class);
-    
-    assertTrue(tf.incrementToken());
-    assertTrue(termAtt.term() + " is not equal to " + "click", termAtt.term().equals("click") == true);
-    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 1, posIncrAtt.getPositionIncrement() == 1);
-    assertTrue(tf.incrementToken());
-    assertTrue(termAtt.term() + " is not equal to " + "link", termAtt.term().equals("link") == true);
-    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 1, posIncrAtt.getPositionIncrement() == 1);
-    assertTrue(tf.incrementToken());
-    assertTrue(termAtt.term() + " is not equal to " + "here",
-        termAtt.term().equals("here") == true);
-    //The link, and here should be at the same position for phrases to work
-    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 1, posIncrAtt.getPositionIncrement() == 1);
-    assertTrue(tf.incrementToken());
-    assertTrue(termAtt.term() + " is not equal to " + "again",
-        termAtt.term().equals("again") == true);
-    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 1, posIncrAtt.getPositionIncrement() == 1);
-
-    assertTrue(tf.incrementToken());
-    assertTrue(termAtt.term() + " is not equal to " + "click",
-        termAtt.term().equals("click") == true);
-    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 1, posIncrAtt.getPositionIncrement() == 1);
-
-    assertTrue(tf.incrementToken());
-    assertTrue(termAtt.term() + " is not equal to " + "http://lucene.apache.org",
-        termAtt.term().equals("http://lucene.apache.org") == true);
-    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 1, posIncrAtt.getPositionIncrement() == 1);
-
-    assertTrue(tf.incrementToken());
-    assertTrue(termAtt.term() + " is not equal to " + "here",
-        termAtt.term().equals("here") == true);
-    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 0, posIncrAtt.getPositionIncrement() == 0);
-
-    assertTrue(tf.incrementToken());
-    assertTrue(termAtt.term() + " is not equal to " + "again",
-        termAtt.term().equals("again") == true);
-    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 1, posIncrAtt.getPositionIncrement() == 1);
-
-    assertTrue(tf.incrementToken());
-    assertTrue(termAtt.term() + " is not equal to " + "a",
-        termAtt.term().equals("a") == true);
-    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 1, posIncrAtt.getPositionIncrement() == 1);
-
-    assertTrue(tf.incrementToken());
-    assertTrue(termAtt.term() + " is not equal to " + "b",
-        termAtt.term().equals("b") == true);
-    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 1, posIncrAtt.getPositionIncrement() == 1);
-
-    assertTrue(tf.incrementToken());
-    assertTrue(termAtt.term() + " is not equal to " + "c",
-        termAtt.term().equals("c") == true);
-    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 1, posIncrAtt.getPositionIncrement() == 1);
-
-    assertTrue(tf.incrementToken());
-    assertTrue(termAtt.term() + " is not equal to " + "d",
-        termAtt.term().equals("d") == true);
-    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 1, posIncrAtt.getPositionIncrement() == 1);
-
-    assertFalse(tf.incrementToken());  
-  }
-
-  public void testLinks() throws Exception {
-    String test = "[http://lucene.apache.org/java/docs/index.html#news here] [http://lucene.apache.org/java/docs/index.html?b=c here] [https://lucene.apache.org/java/docs/index.html?b=c here]";
-    WikipediaTokenizer tf = new WikipediaTokenizer(new StringReader(test));
-    TermAttribute termAtt = tf.addAttribute(TermAttribute.class);
-    TypeAttribute typeAtt = tf.addAttribute(TypeAttribute.class);
-    
-    assertTrue(tf.incrementToken());
-    assertTrue(termAtt.term() + " is not equal to " + "http://lucene.apache.org/java/docs/index.html#news",
-        termAtt.term().equals("http://lucene.apache.org/java/docs/index.html#news") == true);
-    assertTrue(typeAtt.type() + " is not equal to " + WikipediaTokenizer.EXTERNAL_LINK_URL, typeAtt.type().equals(WikipediaTokenizer.EXTERNAL_LINK_URL) == true);
-    tf.incrementToken();//skip here
-    
-    assertTrue(tf.incrementToken());
-    assertTrue(termAtt.term() + " is not equal to " + "http://lucene.apache.org/java/docs/index.html?b=c",
-        termAtt.term().equals("http://lucene.apache.org/java/docs/index.html?b=c") == true);
-    assertTrue(typeAtt.type() + " is not equal to " + WikipediaTokenizer.EXTERNAL_LINK_URL, typeAtt.type().equals(WikipediaTokenizer.EXTERNAL_LINK_URL) == true);
-    tf.incrementToken();//skip here
-    
-    assertTrue(tf.incrementToken());
-    assertTrue(termAtt.term() + " is not equal to " + "https://lucene.apache.org/java/docs/index.html?b=c",
-        termAtt.term().equals("https://lucene.apache.org/java/docs/index.html?b=c") == true);
-    assertTrue(typeAtt.type() + " is not equal to " + WikipediaTokenizer.EXTERNAL_LINK_URL, typeAtt.type().equals(WikipediaTokenizer.EXTERNAL_LINK_URL) == true);
-    
-    assertTrue(tf.incrementToken());
-    assertFalse(tf.incrementToken());
-  }
-
-  public void testLucene1133() throws Exception {
-    Set<String> untoks = new HashSet<String>();
-    untoks.add(WikipediaTokenizer.CATEGORY);
-    untoks.add(WikipediaTokenizer.ITALICS);
-    //should be exactly the same, regardless of untoks
-    WikipediaTokenizer tf = new WikipediaTokenizer(new StringReader(LINK_PHRASES), WikipediaTokenizer.TOKENS_ONLY, untoks);
-    checkLinkPhrases(tf);
-    String test = "[[Category:a b c d]] [[Category:e f g]] [[link here]] [[link there]] ''italics here'' something ''more italics'' [[Category:h   i   j]]";
-    tf = new WikipediaTokenizer(new StringReader(test), WikipediaTokenizer.UNTOKENIZED_ONLY, untoks);
-    TermAttribute termAtt = tf.addAttribute(TermAttribute.class);
-    PositionIncrementAttribute posIncrAtt = tf.addAttribute(PositionIncrementAttribute.class);
-    OffsetAttribute offsetAtt = tf.addAttribute(OffsetAttribute.class);
-    
-    assertTrue(tf.incrementToken());
-    assertTrue(termAtt.term() + " is not equal to " + "a b c d",
-        termAtt.term().equals("a b c d") == true);
-    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 1, posIncrAtt.getPositionIncrement() == 1);
-    assertTrue(offsetAtt.startOffset() + " does not equal: " + 11, offsetAtt.startOffset() == 11);
-    assertTrue(offsetAtt.endOffset() + " does not equal: " + 18, offsetAtt.endOffset() == 18);
-    
-    assertTrue(tf.incrementToken());
-    assertTrue(termAtt.term() + " is not equal to " + "e f g",
-        termAtt.term().equals("e f g") == true);
-    assertTrue(offsetAtt.startOffset() + " does not equal: " + 32, offsetAtt.startOffset() == 32);
-    assertTrue(offsetAtt.endOffset() + " does not equal: " + 37, offsetAtt.endOffset() == 37);
-    
-    assertTrue(tf.incrementToken());
-    assertTrue(termAtt.term() + " is not equal to " + "link",
-        termAtt.term().equals("link") == true);
-    assertTrue(offsetAtt.startOffset() + " does not equal: " + 42, offsetAtt.startOffset() == 42);
-    assertTrue(offsetAtt.endOffset() + " does not equal: " + 46, offsetAtt.endOffset() == 46);
-    
-    assertTrue(tf.incrementToken());
-    assertTrue(termAtt.term() + " is not equal to " + "here",
-        termAtt.term().equals("here") == true);
-    assertTrue(offsetAtt.startOffset() + " does not equal: " + 47, offsetAtt.startOffset() == 47);
-    assertTrue(offsetAtt.endOffset() + " does not equal: " + 51, offsetAtt.endOffset() == 51);
-
-    assertTrue(tf.incrementToken());
-    assertTrue(termAtt.term() + " is not equal to " + "link",
-        termAtt.term().equals("link") == true);
-    assertTrue(offsetAtt.startOffset() + " does not equal: " + 56, offsetAtt.startOffset() == 56);
-    assertTrue(offsetAtt.endOffset() + " does not equal: " + 60, offsetAtt.endOffset() == 60);
-    
-    assertTrue(tf.incrementToken());
-    assertTrue(termAtt.term() + " is not equal to " + "there",
-        termAtt.term().equals("there") == true);
-
-    assertTrue(offsetAtt.startOffset() + " does not equal: " + 61, offsetAtt.startOffset() == 61);
-    assertTrue(offsetAtt.endOffset() + " does not equal: " + 66, offsetAtt.endOffset() == 66);
-    
-    assertTrue(tf.incrementToken());
-    assertTrue(termAtt.term() + " is not equal to " + "italics here",
-        termAtt.term().equals("italics here") == true);
-    assertTrue(offsetAtt.startOffset() + " does not equal: " + 71, offsetAtt.startOffset() == 71);
-    assertTrue(offsetAtt.endOffset() + " does not equal: " + 83, offsetAtt.endOffset() == 83);
-    
-    assertTrue(tf.incrementToken());
-    assertTrue(termAtt.term() + " is not equal to " + "something",
-        termAtt.term().equals("something") == true);
-    assertTrue(offsetAtt.startOffset() + " does not equal: " + 86, offsetAtt.startOffset() == 86);
-    assertTrue(offsetAtt.endOffset() + " does not equal: " + 95, offsetAtt.endOffset() == 95);
-    
-    assertTrue(tf.incrementToken());
-    assertTrue(termAtt.term() + " is not equal to " + "more italics",
-        termAtt.term().equals("more italics") == true);
-    assertTrue(offsetAtt.startOffset() + " does not equal: " + 98, offsetAtt.startOffset() == 98);
-    assertTrue(offsetAtt.endOffset() + " does not equal: " + 110, offsetAtt.endOffset() == 110);
-
-    assertTrue(tf.incrementToken());
-    assertTrue(termAtt.term() + " is not equal to " + "h   i   j",
-        termAtt.term().equals("h   i   j") == true);
-    assertTrue(offsetAtt.startOffset() + " does not equal: " + 124, offsetAtt.startOffset() == 124);
-    assertTrue(offsetAtt.endOffset() + " does not equal: " + 133, offsetAtt.endOffset() == 133);
-
-    assertFalse(tf.incrementToken());
-  }
-
-  public void testBoth() throws Exception {
-    Set<String> untoks = new HashSet<String>();
-    untoks.add(WikipediaTokenizer.CATEGORY);
-    untoks.add(WikipediaTokenizer.ITALICS);
-    String test = "[[Category:a b c d]] [[Category:e f g]] [[link here]] [[link there]] ''italics here'' something ''more italics'' [[Category:h   i   j]]";
-    //should output all the indivual tokens plus the untokenized tokens as well.  Untokenized tokens
-    WikipediaTokenizer tf = new WikipediaTokenizer(new StringReader(test), WikipediaTokenizer.BOTH, untoks);
-    TermAttribute termAtt = tf.addAttribute(TermAttribute.class);
-    TypeAttribute typeAtt = tf.addAttribute(TypeAttribute.class);
-    PositionIncrementAttribute posIncrAtt = tf.addAttribute(PositionIncrementAttribute.class);
-    OffsetAttribute offsetAtt = tf.addAttribute(OffsetAttribute.class);
-    FlagsAttribute flagsAtt = tf.addAttribute(FlagsAttribute.class);
-    
-    assertTrue(tf.incrementToken());
-    assertTrue(termAtt.term() + " is not equal to " + "a b c d",
-            termAtt.term().equals("a b c d") == true);
-    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 1, posIncrAtt.getPositionIncrement() == 1);
-    assertTrue(typeAtt.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);
-    assertTrue(flagsAtt.getFlags() + " does not equal: " + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, flagsAtt.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);
-    assertTrue(offsetAtt.startOffset() + " does not equal: " + 11, offsetAtt.startOffset() == 11);
-    assertTrue(offsetAtt.endOffset() + " does not equal: " + 18, offsetAtt.endOffset() == 18);
-    
-    assertTrue(tf.incrementToken());
-    assertTrue(termAtt.term() + " is not equal to " + "a",
-            termAtt.term().equals("a") == true);
-    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 0, posIncrAtt.getPositionIncrement() == 0);
-    assertTrue(typeAtt.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);
-    assertTrue(flagsAtt.getFlags() + " equals: " + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG + " and it shouldn't", flagsAtt.getFlags() != WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);
-    assertTrue(offsetAtt.startOffset() + " does not equal: " + 11, offsetAtt.startOffset() == 11);
-    assertTrue(offsetAtt.endOffset() + " does not equal: " + 12, offsetAtt.endOffset() == 12);
-
-    assertTrue(tf.incrementToken());
-    assertTrue(termAtt.term() + " is not equal to " + "b",
-            termAtt.term().equals("b") == true);
-    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 1, posIncrAtt.getPositionIncrement() == 1);
-    assertTrue(typeAtt.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);
-    assertTrue(offsetAtt.startOffset() + " does not equal: " + 13, offsetAtt.startOffset() == 13);
-    assertTrue(offsetAtt.endOffset() + " does not equal: " + 14, offsetAtt.endOffset() == 14);
-
-    assertTrue(tf.incrementToken());
-    assertTrue(termAtt.term() + " is not equal to " + "c",
-            termAtt.term().equals("c") == true);
-    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 1, posIncrAtt.getPositionIncrement() == 1);
-    assertTrue(typeAtt.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);
-    assertTrue(offsetAtt.startOffset() + " does not equal: " + 15, offsetAtt.startOffset() == 15);
-    assertTrue(offsetAtt.endOffset() + " does not equal: " + 16, offsetAtt.endOffset() == 16);
-
-    assertTrue(tf.incrementToken());
-    assertTrue(termAtt.term() + " is not equal to " + "d",
-            termAtt.term().equals("d") == true);
-    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 1, posIncrAtt.getPositionIncrement() == 1);
-    assertTrue(typeAtt.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);
-    assertTrue(offsetAtt.startOffset() + " does not equal: " + 17, offsetAtt.startOffset() == 17);
-    assertTrue(offsetAtt.endOffset() + " does not equal: " + 18, offsetAtt.endOffset() == 18);
-
-
-
-    assertTrue(tf.incrementToken());
-    assertTrue(termAtt.term() + " is not equal to " + "e f g",
-            termAtt.term().equals("e f g") == true);
-    assertTrue(typeAtt.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);
-    assertTrue(flagsAtt.getFlags() + " does not equal: " + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, flagsAtt.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);
-    assertTrue(offsetAtt.startOffset() + " does not equal: " + 32, offsetAtt.startOffset() == 32);
-    assertTrue(offsetAtt.endOffset() + " does not equal: " + 37, offsetAtt.endOffset() == 37);
-
-    assertTrue(tf.incrementToken());
-    assertTrue(termAtt.term() + " is not equal to " + "e",
-            termAtt.term().equals("e") == true);
-    assertTrue(typeAtt.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);
-    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 0, posIncrAtt.getPositionIncrement() == 0);
-    assertTrue(offsetAtt.startOffset() + " does not equal: " + 32, offsetAtt.startOffset() == 32);
-    assertTrue(offsetAtt.endOffset() + " does not equal: " + 33, offsetAtt.endOffset() == 33);
-
-    assertTrue(tf.incrementToken());
-    assertTrue(termAtt.term() + " is not equal to " + "f",
-            termAtt.term().equals("f") == true);
-    assertTrue(typeAtt.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);
-    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 1, posIncrAtt.getPositionIncrement() == 1);
-    assertTrue(offsetAtt.startOffset() + " does not equal: " + 34, offsetAtt.startOffset() == 34);
-    assertTrue(offsetAtt.endOffset() + " does not equal: " + 35, offsetAtt.endOffset() == 35);
-
-    assertTrue(tf.incrementToken());
-    assertTrue(termAtt.term() + " is not equal to " + "g",
-            termAtt.term().equals("g") == true);
-    assertTrue(typeAtt.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);
-    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 1, posIncrAtt.getPositionIncrement() == 1);
-    assertTrue(offsetAtt.startOffset() + " does not equal: " + 36, offsetAtt.startOffset() == 36);
-    assertTrue(offsetAtt.endOffset() + " does not equal: " + 37, offsetAtt.endOffset() == 37);
-
-    assertTrue(tf.incrementToken());
-    assertTrue(termAtt.term() + " is not equal to " + "link",
-            termAtt.term().equals("link") == true);
-    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 1, posIncrAtt.getPositionIncrement() == 1);
-    assertTrue(typeAtt.type() + " is not equal to " + WikipediaTokenizer.INTERNAL_LINK, typeAtt.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);
-    assertTrue(offsetAtt.startOffset() + " does not equal: " + 42, offsetAtt.startOffset() == 42);
-    assertTrue(offsetAtt.endOffset() + " does not equal: " + 46, offsetAtt.endOffset() == 46);
-    
-    assertTrue(tf.incrementToken());
-    assertTrue(termAtt.term() + " is not equal to " + "here",
-            termAtt.term().equals("here") == true);
-    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 1, posIncrAtt.getPositionIncrement() == 1);
-    assertTrue(typeAtt.type() + " is not equal to " + WikipediaTokenizer.INTERNAL_LINK, typeAtt.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);
-    assertTrue(offsetAtt.startOffset() + " does not equal: " + 47, offsetAtt.startOffset() == 47);
-    assertTrue(offsetAtt.endOffset() + " does not equal: " + 51, offsetAtt.endOffset() == 51);
-    
-    assertTrue(tf.incrementToken());
-    assertTrue(termAtt.term() + " is not equal to " + "link",
-            termAtt.term().equals("link") == true);
-    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 1, posIncrAtt.getPositionIncrement() == 1);
-    assertTrue(offsetAtt.startOffset() + " does not equal: " + 56, offsetAtt.startOffset() == 56);
-    assertTrue(typeAtt.type() + " is not equal to " + WikipediaTokenizer.INTERNAL_LINK, typeAtt.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);
-    assertTrue(offsetAtt.endOffset() + " does not equal: " + 60, offsetAtt.endOffset() == 60);
-    
-    assertTrue(tf.incrementToken());
-    assertTrue(termAtt.term() + " is not equal to " + "there",
-            termAtt.term().equals("there") == true);
-    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 1, posIncrAtt.getPositionIncrement() == 1);
-    assertTrue(typeAtt.type() + " is not equal to " + WikipediaTokenizer.INTERNAL_LINK, typeAtt.type().equals(WikipediaTokenizer.INTERNAL_LINK) == true);
-    assertTrue(offsetAtt.startOffset() + " does not equal: " + 61, offsetAtt.startOffset() == 61);
-    assertTrue(offsetAtt.endOffset() + " does not equal: " + 66, offsetAtt.endOffset() == 66);
-    
-    assertTrue(tf.incrementToken());
-    assertTrue(termAtt.term() + " is not equal to " + "italics here",
-            termAtt.term().equals("italics here") == true);
-    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 1, posIncrAtt.getPositionIncrement() == 1);
-    assertTrue(typeAtt.type() + " is not equal to " + WikipediaTokenizer.ITALICS, typeAtt.type().equals(WikipediaTokenizer.ITALICS) == true);
-    assertTrue(flagsAtt.getFlags() + " does not equal: " + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, flagsAtt.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);
-    assertTrue(offsetAtt.startOffset() + " does not equal: " + 71, offsetAtt.startOffset() == 71);
-    assertTrue(offsetAtt.endOffset() + " does not equal: " + 83, offsetAtt.endOffset() == 83);
-
-    assertTrue(tf.incrementToken());
-    assertTrue(termAtt.term() + " is not equal to " + "italics",
-            termAtt.term().equals("italics") == true);
-    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 0, posIncrAtt.getPositionIncrement() == 0);
-    assertTrue(typeAtt.type() + " is not equal to " + WikipediaTokenizer.ITALICS, typeAtt.type().equals(WikipediaTokenizer.ITALICS) == true);
-    assertTrue(offsetAtt.startOffset() + " does not equal: " + 71, offsetAtt.startOffset() == 71);
-    assertTrue(offsetAtt.endOffset() + " does not equal: " + 78, offsetAtt.endOffset() == 78);
-
-    assertTrue(tf.incrementToken());
-    assertTrue(termAtt.term() + " is not equal to " + "here",
-            termAtt.term().equals("here") == true);
-    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 1, posIncrAtt.getPositionIncrement() == 1);
-    assertTrue(typeAtt.type() + " is not equal to " + WikipediaTokenizer.ITALICS, typeAtt.type().equals(WikipediaTokenizer.ITALICS) == true);
-    assertTrue(offsetAtt.startOffset() + " does not equal: " + 79, offsetAtt.startOffset() == 79);
-    assertTrue(offsetAtt.endOffset() + " does not equal: " + 83, offsetAtt.endOffset() == 83);
-
-    assertTrue(tf.incrementToken());
-    assertTrue(termAtt.term() + " is not equal to " + "something",
-            termAtt.term().equals("something") == true);
-    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 1, posIncrAtt.getPositionIncrement() == 1);
-    assertTrue(offsetAtt.startOffset() + " does not equal: " + 86, offsetAtt.startOffset() == 86);
-    assertTrue(offsetAtt.endOffset() + " does not equal: " + 95, offsetAtt.endOffset() == 95);
-    
-    assertTrue(tf.incrementToken());
-    assertTrue(termAtt.term() + " is not equal to " + "more italics",
-            termAtt.term().equals("more italics") == true);
-    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 1, posIncrAtt.getPositionIncrement() == 1);
-    assertTrue(typeAtt.type() + " is not equal to " + WikipediaTokenizer.ITALICS, typeAtt.type().equals(WikipediaTokenizer.ITALICS) == true);
-    assertTrue(flagsAtt.getFlags() + " does not equal: " + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, flagsAtt.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);
-    assertTrue(offsetAtt.startOffset() + " does not equal: " + 98, offsetAtt.startOffset() == 98);
-    assertTrue(offsetAtt.endOffset() + " does not equal: " + 110, offsetAtt.endOffset() == 110);
-
-    assertTrue(tf.incrementToken());
-    assertTrue(termAtt.term() + " is not equal to " + "more",
-            termAtt.term().equals("more") == true);
-    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 0, posIncrAtt.getPositionIncrement() == 0);
-    assertTrue(typeAtt.type() + " is not equal to " + WikipediaTokenizer.ITALICS, typeAtt.type().equals(WikipediaTokenizer.ITALICS) == true);
-    assertTrue(offsetAtt.startOffset() + " does not equal: " + 98, offsetAtt.startOffset() == 98);
-    assertTrue(offsetAtt.endOffset() + " does not equal: " + 102, offsetAtt.endOffset() == 102);
-
-    assertTrue(tf.incrementToken());
-    assertTrue(termAtt.term() + " is not equal to " + "italics",
-            termAtt.term().equals("italics") == true);
-    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 1, posIncrAtt.getPositionIncrement() == 1);
-        assertTrue(typeAtt.type() + " is not equal to " + WikipediaTokenizer.ITALICS, typeAtt.type().equals(WikipediaTokenizer.ITALICS) == true);
-
-    assertTrue(offsetAtt.startOffset() + " does not equal: " + 103, offsetAtt.startOffset() == 103);
-    assertTrue(offsetAtt.endOffset() + " does not equal: " + 110, offsetAtt.endOffset() == 110);
-
-    assertTrue(tf.incrementToken());
-    assertTrue(termAtt.term() + " is not equal to " + "h   i   j",
-            termAtt.term().equals("h   i   j") == true);
-    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 1, posIncrAtt.getPositionIncrement() == 1);
-    assertTrue(typeAtt.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);
-    assertTrue(flagsAtt.getFlags() + " does not equal: " + WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG, flagsAtt.getFlags() == WikipediaTokenizer.UNTOKENIZED_TOKEN_FLAG);
-    assertTrue(offsetAtt.startOffset() + " does not equal: " + 124, offsetAtt.startOffset() == 124);
-    assertTrue(offsetAtt.endOffset() + " does not equal: " + 133, offsetAtt.endOffset() == 133);
-
-    assertTrue(tf.incrementToken());
-    assertTrue(termAtt.term() + " is not equal to " + "h",
-            termAtt.term().equals("h") == true);
-    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 0, posIncrAtt.getPositionIncrement() == 0);
-    assertTrue(typeAtt.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);
-    assertTrue(offsetAtt.startOffset() + " does not equal: " + 124, offsetAtt.startOffset() == 124);
-    assertTrue(offsetAtt.endOffset() + " does not equal: " + 125, offsetAtt.endOffset() == 125);
-
-    assertTrue(tf.incrementToken());
-    assertTrue(termAtt.term() + " is not equal to " + "i",
-            termAtt.term().equals("i") == true);
-    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 1, posIncrAtt.getPositionIncrement() == 1);
-    assertTrue(typeAtt.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);
-    assertTrue(offsetAtt.startOffset() + " does not equal: " + 128, offsetAtt.startOffset() == 128);
-    assertTrue(offsetAtt.endOffset() + " does not equal: " + 129, offsetAtt.endOffset() == 129);
-    
-    assertTrue(tf.incrementToken());
-    assertTrue(termAtt.term() + " is not equal to " + "j",
-            termAtt.term().equals("j") == true);
-    assertTrue(posIncrAtt.getPositionIncrement() + " does not equal: " + 1, posIncrAtt.getPositionIncrement() == 1);
-    assertTrue(typeAtt.type() + " is not equal to " + WikipediaTokenizer.CATEGORY, typeAtt.type().equals(WikipediaTokenizer.CATEGORY) == true);
-    assertTrue(offsetAtt.startOffset() + " does not equal: " + 132, offsetAtt.startOffset() == 132);
-    assertTrue(offsetAtt.endOffset() + " does not equal: " + 133, offsetAtt.endOffset() == 133);
-
-    assertFalse(tf.incrementToken());
-  }
-}
diff --git a/lucene/docs/contributions.html b/lucene/docs/contributions.html
index ddbdb1d..1866ec3 100644
--- a/lucene/docs/contributions.html
+++ b/lucene/docs/contributions.html
@@ -192,9 +192,6 @@ document.write("Last Published: " + document.lastModified);
 <a href="api/contrib-swing/index.html">Swing</a>
 </div>
 <div class="menuitem">
-<a href="api/contrib-wikipedia/index.html">Wikipedia</a>
-</div>
-<div class="menuitem">
 <a href="api/contrib-wordnet/index.html">Wordnet</a>
 </div>
 <div class="menuitem">
diff --git a/lucene/docs/demo.html b/lucene/docs/demo.html
index db7c848..6e53394 100644
--- a/lucene/docs/demo.html
+++ b/lucene/docs/demo.html
@@ -192,9 +192,6 @@ document.write("Last Published: " + document.lastModified);
 <a href="api/contrib-swing/index.html">Swing</a>
 </div>
 <div class="menuitem">
-<a href="api/contrib-wikipedia/index.html">Wikipedia</a>
-</div>
-<div class="menuitem">
 <a href="api/contrib-wordnet/index.html">Wordnet</a>
 </div>
 <div class="menuitem">
diff --git a/lucene/docs/demo2.html b/lucene/docs/demo2.html
index 0386692..4973a5e 100644
--- a/lucene/docs/demo2.html
+++ b/lucene/docs/demo2.html
@@ -192,9 +192,6 @@ document.write("Last Published: " + document.lastModified);
 <a href="api/contrib-swing/index.html">Swing</a>
 </div>
 <div class="menuitem">
-<a href="api/contrib-wikipedia/index.html">Wikipedia</a>
-</div>
-<div class="menuitem">
 <a href="api/contrib-wordnet/index.html">Wordnet</a>
 </div>
 <div class="menuitem">
diff --git a/lucene/docs/demo3.html b/lucene/docs/demo3.html
index 258b043..b498e15 100644
--- a/lucene/docs/demo3.html
+++ b/lucene/docs/demo3.html
@@ -192,9 +192,6 @@ document.write("Last Published: " + document.lastModified);
 <a href="api/contrib-swing/index.html">Swing</a>
 </div>
 <div class="menuitem">
-<a href="api/contrib-wikipedia/index.html">Wikipedia</a>
-</div>
-<div class="menuitem">
 <a href="api/contrib-wordnet/index.html">Wordnet</a>
 </div>
 <div class="menuitem">
diff --git a/lucene/docs/demo4.html b/lucene/docs/demo4.html
index 46e9790..585385a 100644
--- a/lucene/docs/demo4.html
+++ b/lucene/docs/demo4.html
@@ -192,9 +192,6 @@ document.write("Last Published: " + document.lastModified);
 <a href="api/contrib-swing/index.html">Swing</a>
 </div>
 <div class="menuitem">
-<a href="api/contrib-wikipedia/index.html">Wikipedia</a>
-</div>
-<div class="menuitem">
 <a href="api/contrib-wordnet/index.html">Wordnet</a>
 </div>
 <div class="menuitem">
diff --git a/lucene/docs/fileformats.html b/lucene/docs/fileformats.html
index 1919ce1..f25bf4d 100644
--- a/lucene/docs/fileformats.html
+++ b/lucene/docs/fileformats.html
@@ -192,9 +192,6 @@ document.write("Last Published: " + document.lastModified);
 <a href="api/contrib-swing/index.html">Swing</a>
 </div>
 <div class="menuitem">
-<a href="api/contrib-wikipedia/index.html">Wikipedia</a>
-</div>
-<div class="menuitem">
 <a href="api/contrib-wordnet/index.html">Wordnet</a>
 </div>
 <div class="menuitem">
diff --git a/lucene/docs/gettingstarted.html b/lucene/docs/gettingstarted.html
index 5f68fad..350e109 100644
--- a/lucene/docs/gettingstarted.html
+++ b/lucene/docs/gettingstarted.html
@@ -192,9 +192,6 @@ document.write("Last Published: " + document.lastModified);
 <a href="api/contrib-swing/index.html">Swing</a>
 </div>
 <div class="menuitem">
-<a href="api/contrib-wikipedia/index.html">Wikipedia</a>
-</div>
-<div class="menuitem">
 <a href="api/contrib-wordnet/index.html">Wordnet</a>
 </div>
 <div class="menuitem">
diff --git a/lucene/docs/index.html b/lucene/docs/index.html
index 93b6d92..92a000d 100644
--- a/lucene/docs/index.html
+++ b/lucene/docs/index.html
@@ -190,9 +190,6 @@ document.write("Last Published: " + document.lastModified);
 <a href="api/contrib-swing/index.html">Swing</a>
 </div>
 <div class="menuitem">
-<a href="api/contrib-wikipedia/index.html">Wikipedia</a>
-</div>
-<div class="menuitem">
 <a href="api/contrib-wordnet/index.html">Wordnet</a>
 </div>
 <div class="menuitem">
diff --git a/lucene/docs/linkmap.html b/lucene/docs/linkmap.html
index f447575..5512a47 100644
--- a/lucene/docs/linkmap.html
+++ b/lucene/docs/linkmap.html
@@ -190,9 +190,6 @@ document.write("Last Published: " + document.lastModified);
 <a href="api/contrib-swing/index.html">Swing</a>
 </div>
 <div class="menuitem">
-<a href="api/contrib-wikipedia/index.html">Wikipedia</a>
-</div>
-<div class="menuitem">
 <a href="api/contrib-wordnet/index.html">Wordnet</a>
 </div>
 <div class="menuitem">
@@ -436,13 +433,7 @@ document.write("Last Published: " + document.lastModified);
 <li>
 <a href="api/contrib-swing/index.html">Swing</a>&nbsp;&nbsp;___________________&nbsp;&nbsp;<em>javadoc-contrib-swing</em>
 </li>
-</ul>
-			
-<ul>
-<li>
-<a href="api/contrib-wikipedia/index.html">Wikipedia</a>&nbsp;&nbsp;___________________&nbsp;&nbsp;<em>javadoc-contrib-wikipedia</em>
-</li>
-</ul>					    
+</ul>				    
 			
 <ul>
 <li>
diff --git a/lucene/docs/linkmap.pdf b/lucene/docs/linkmap.pdf
index 099d597..e0bfcff 100644
--- a/lucene/docs/linkmap.pdf
+++ b/lucene/docs/linkmap.pdf
@@ -39,7 +39,6 @@ This is a map of the complete site and its structure.
                   ?? Spellchecker ___________________ javadoc-contrib-spellchecker
                   ?? Surround ___________________ javadoc-contrib-surround
                   ?? Swing ___________________ javadoc-contrib-swing
-                  ?? Wikipedia ___________________ javadoc-contrib-wikipedia
                   ?? Wordnet ___________________ javadoc-contrib-wordnet
                   ?? XML Query
 
diff --git a/lucene/docs/lucene-contrib/index.html b/lucene/docs/lucene-contrib/index.html
index 2d6cbef..805458b 100644
--- a/lucene/docs/lucene-contrib/index.html
+++ b/lucene/docs/lucene-contrib/index.html
@@ -192,9 +192,6 @@ document.write("Last Published: " + document.lastModified);
 <a href="../api/contrib-swing/index.html">Swing</a>
 </div>
 <div class="menuitem">
-<a href="../api/contrib-wikipedia/index.html">Wikipedia</a>
-</div>
-<div class="menuitem">
 <a href="../api/contrib-wordnet/index.html">Wordnet</a>
 </div>
 <div class="menuitem">
@@ -312,9 +309,6 @@ document.write("Last Published: " + document.lastModified);
 <a href="#swing">swing</a>
 </li>
 <li>
-<a href="#wikipedia">wikipedia</a>
-</li>
-<li>
 <a href="#wordnet">wordnet</a>
 </li>
 <li>
@@ -457,17 +451,12 @@ document.write("Last Published: " + document.lastModified);
 <p>Swing components designed to integrate with Lucene.</p>
 <p>See <a href="../api/contrib-swing/index.html">swing javadoc</a>
 </p>
-<a name="N10134"></a><a name="wikipedia"></a>
-<h3 class="boxed">wikipedia</h3>
-<p>Tools for working with wikipedia content.</p>
-<p>See <a href="../api/contrib-wikipedia/index.html">wikipedia javadoc</a>
-</p>
-<a name="N10143"></a><a name="wordnet"></a>
+<a name="N10134"></a><a name="wordnet"></a>
 <h3 class="boxed">wordnet</h3>
 <p>Tools to help utilize wordnet synonyms with Lucene</p>
 <p>See <a href="../api/contrib-wordnet/index.html">wordnet javadoc</a>
 </p>
-<a name="N10152"></a><a name="xml-query-parser"></a>
+<a name="N10143"></a><a name="xml-query-parser"></a>
 <h3 class="boxed">xml-query-parser</h3>
 <p>A QueryParser that can read queries written in an XML format.</p>
 <p>See <a href="../api/contrib-wordnet/index.html">xml-query-parser javadoc</a>
diff --git a/lucene/docs/lucene-contrib/index.pdf b/lucene/docs/lucene-contrib/index.pdf
index 41eed6e..b8bd0da 100644
--- a/lucene/docs/lucene-contrib/index.pdf
+++ b/lucene/docs/lucene-contrib/index.pdf
@@ -20,9 +20,8 @@ Table of contents
     1.15 spellchecker................................................................................................................. 4
     1.16 surround....................................................................................................................... 4
     1.17 swing............................................................................................................................4
-    1.18 wikipedia......................................................................................................................5
-    1.19 wordnet........................................................................................................................ 5
-    1.20 xml-query-parser..........................................................................................................5
+    1.18 wordnet........................................................................................................................ 5
+    1.19 xml-query-parser..........................................................................................................5
 
                    Copyright © 2006 The Apache Software Foundation. All rights reserved.
 Apache Lucene - Lucene Contrib
@@ -148,13 +147,10 @@ Copyright © 2006 The Apache Software Foundation. All rights reserved.
 Apache Lucene - Lucene Contrib
 
 See swing javadoc
-1.18. wikipedia
-Tools for working with wikipedia content.
-See wikipedia javadoc
-1.19. wordnet
+1.18. wordnet
 Tools to help utilize wordnet synonyms with Lucene
 See wordnet javadoc
-1.20. xml-query-parser
+1.19. xml-query-parser
 A QueryParser that can read queries written in an XML format.
 See xml-query-parser javadoc
 
diff --git a/lucene/docs/queryparsersyntax.html b/lucene/docs/queryparsersyntax.html
index 17cf145..a8642d3 100644
--- a/lucene/docs/queryparsersyntax.html
+++ b/lucene/docs/queryparsersyntax.html
@@ -192,9 +192,6 @@ document.write("Last Published: " + document.lastModified);
 <a href="api/contrib-swing/index.html">Swing</a>
 </div>
 <div class="menuitem">
-<a href="api/contrib-wikipedia/index.html">Wikipedia</a>
-</div>
-<div class="menuitem">
 <a href="api/contrib-wordnet/index.html">Wordnet</a>
 </div>
 <div class="menuitem">
diff --git a/lucene/docs/scoring.html b/lucene/docs/scoring.html
index b11a7f9..b56e145 100644
--- a/lucene/docs/scoring.html
+++ b/lucene/docs/scoring.html
@@ -192,9 +192,6 @@ document.write("Last Published: " + document.lastModified);
 <a href="api/contrib-swing/index.html">Swing</a>
 </div>
 <div class="menuitem">
-<a href="api/contrib-wikipedia/index.html">Wikipedia</a>
-</div>
-<div class="menuitem">
 <a href="api/contrib-wordnet/index.html">Wordnet</a>
 </div>
 <div class="menuitem">
diff --git a/lucene/docs/systemrequirements.html b/lucene/docs/systemrequirements.html
index f8e85ce..973a805 100644
--- a/lucene/docs/systemrequirements.html
+++ b/lucene/docs/systemrequirements.html
@@ -190,9 +190,6 @@ document.write("Last Published: " + document.lastModified);
 <a href="api/contrib-swing/index.html">Swing</a>
 </div>
 <div class="menuitem">
-<a href="api/contrib-wikipedia/index.html">Wikipedia</a>
-</div>
-<div class="menuitem">
 <a href="api/contrib-wordnet/index.html">Wordnet</a>
 </div>
 <div class="menuitem">
diff --git a/lucene/src/site/src/documentation/content/xdocs/lucene-contrib/index.xml b/lucene/src/site/src/documentation/content/xdocs/lucene-contrib/index.xml
index 7bf93d4..db52629 100644
--- a/lucene/src/site/src/documentation/content/xdocs/lucene-contrib/index.xml
+++ b/lucene/src/site/src/documentation/content/xdocs/lucene-contrib/index.xml
@@ -135,11 +135,6 @@
                 <p>Swing components designed to integrate with Lucene.</p>
                 <p>See <a href="../api/contrib-swing/index.html">swing javadoc</a></p>
             </section>    
-           
-            <section id="wikipedia"><title>wikipedia</title>
-                <p>Tools for working with wikipedia content.</p>
-                <p>See <a href="../api/contrib-wikipedia/index.html">wikipedia javadoc</a></p>
-            </section>   
             
             <section id="wordnet"><title>wordnet</title>
                 <p>Tools to help utilize wordnet synonyms with Lucene</p>
diff --git a/lucene/src/site/src/documentation/content/xdocs/site.xml b/lucene/src/site/src/documentation/content/xdocs/site.xml
index 14c079b..91ffa41 100755
--- a/lucene/src/site/src/documentation/content/xdocs/site.xml
+++ b/lucene/src/site/src/documentation/content/xdocs/site.xml
@@ -73,8 +73,7 @@ See http://forrest.apache.org/docs/linking.html for more info
 		    <javadoc-contrib-spatial label="Spatial" href="ext:javadocs-contrib-spatial"/>
 		    <javadoc-contrib-spellchecker label="Spellchecker" href="ext:javadocs-contrib-spellchecker"/>
 		    <javadoc-contrib-surround label="Surround" href="ext:javadocs-contrib-surround"/>			
-		    <javadoc-contrib-swing label="Swing" href="ext:javadocs-contrib-swing"/>
-			<javadoc-contrib-wikipedia label="Wikipedia" href="ext:javadocs-contrib-wikipedia"/>					    
+		    <javadoc-contrib-swing label="Swing" href="ext:javadocs-contrib-swing"/>				    
 			<javadoc-contrib-wordnet label="Wordnet" href="ext:javadocs-contrib-wordnet"/>			
 		    <javadoc-contrib-xml-query-parser label="XML Query Parser" href="ext:javadocs-contrib-xml-query-parser"/>			
 		 </javadoc-contrib>
@@ -124,7 +123,6 @@ See http://forrest.apache.org/docs/linking.html for more info
 	<javadocs-contrib-spellchecker href="api/contrib-spellchecker/index.html"/>
 	<javadocs-contrib-surround href="api/contrib-surround/index.html"/>
 	<javadocs-contrib-swing href="api/contrib-swing/index.html"/>
-	<javadocs-contrib-wikipedia href="api/contrib-wikipedia/index.html"/>
 	<javadocs-contrib-wordnet href="api/contrib-wordnet/index.html"/>
 	<javadocs-contrib-xml-query-parser href="api/contrib-xml-query-parser/index.html"/>
 	
diff --git a/solr/CHANGES.txt b/solr/CHANGES.txt
index 4eb4f6d..e897122 100644
--- a/solr/CHANGES.txt
+++ b/solr/CHANGES.txt
@@ -128,7 +128,7 @@ New Features
 
 * SOLR-1857: Synced Solr analysis with Lucene 3.1. Added KeywordMarkerFilterFactory 
   and StemmerOverrideFilterFactory, which can be used to tune stemming algorithms. 
-  Added factories for Bulgarian, Czech, Hindi, and Turkish analysis. Improved the
+  Added factories for Bulgarian, Czech, Hindi, Turkish, and Wikipedia analysis. Improved the
   performance of SnowballPorterFilterFactory.  (rmuir)
 
 * SOLR-1657: Converted remaining TokenStreams to the Attributes-based API. All Solr 
diff --git a/solr/src/java/org/apache/solr/analysis/WikipediaTokenizerFactory.java b/solr/src/java/org/apache/solr/analysis/WikipediaTokenizerFactory.java
new file mode 100644
index 0000000..57b09e7
--- /dev/null
+++ b/solr/src/java/org/apache/solr/analysis/WikipediaTokenizerFactory.java
@@ -0,0 +1,31 @@
+package org.apache.solr.analysis;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.Reader;
+
+import org.apache.lucene.analysis.Tokenizer;
+import org.apache.lucene.analysis.wikipedia.WikipediaTokenizer;
+
+/** Factory for {@link WikipediaTokenizer}*/
+public class WikipediaTokenizerFactory extends BaseTokenizerFactory {
+  // TODO: add support for WikipediaTokenizer's advanced options.
+  public Tokenizer create(Reader input) {
+    return new WikipediaTokenizer(input);
+  }
+}
diff --git a/solr/src/test/org/apache/solr/analysis/TestWikipediaTokenizerFactory.java b/solr/src/test/org/apache/solr/analysis/TestWikipediaTokenizerFactory.java
new file mode 100644
index 0000000..57fa7b6
--- /dev/null
+++ b/solr/src/test/org/apache/solr/analysis/TestWikipediaTokenizerFactory.java
@@ -0,0 +1,42 @@
+package org.apache.solr.analysis;
+
+import java.io.IOException;
+import java.io.Reader;
+import java.io.StringReader;
+
+import org.apache.lucene.analysis.Tokenizer;
+import org.apache.lucene.analysis.wikipedia.WikipediaTokenizer;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Simple tests to ensure the wikipedia tokenizer is working.
+ */
+public class TestWikipediaTokenizerFactory extends BaseTokenTestCase {
+  public void testTokenizer() throws IOException {
+    Reader reader = new StringReader("This is a [[Category:foo]]");
+    WikipediaTokenizerFactory factory = new WikipediaTokenizerFactory();
+    Tokenizer tokenizer = factory.create(reader);
+    assertTokenStreamContents(tokenizer,
+        new String[] { "This", "is", "a", "foo" },
+        new int[] { 0, 5, 8, 21 },
+        new int[] { 4, 7, 9, 24 },
+        new String[] { "<ALPHANUM>", "<ALPHANUM>", "<ALPHANUM>", WikipediaTokenizer.CATEGORY },
+        new int[] { 1, 1, 1, 1, });
+  }
+}

