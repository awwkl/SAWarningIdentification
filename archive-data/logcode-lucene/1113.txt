GitDiffStart: 630a8c950d89064b7f2e8dbe865f964a21f9f501 | Wed May 4 20:07:54 2016 +0100
diff --git a/solr/CHANGES.txt b/solr/CHANGES.txt
index c5643e8..c84affa 100644
--- a/solr/CHANGES.txt
+++ b/solr/CHANGES.txt
@@ -245,6 +245,8 @@ Other Changes
 
 * SOLR-9066 Make CountMetric return long instead of double (Kevin Risden)
 
+* SOLR-9065: Migrate SolrJ distributed tests to SolrCloudTestCase. (Alan Woodward)
+
 ==================  6.0.0 ==================
 
 Consult the LUCENE_CHANGES.txt file for additional, low level, changes in this release
diff --git a/solr/solrj/src/java/org/apache/solr/client/solrj/io/stream/SolrStream.java b/solr/solrj/src/java/org/apache/solr/client/solrj/io/stream/SolrStream.java
index 007c644..f4aabec 100644
--- a/solr/solrj/src/java/org/apache/solr/client/solrj/io/stream/SolrStream.java
+++ b/solr/solrj/src/java/org/apache/solr/client/solrj/io/stream/SolrStream.java
@@ -215,7 +215,7 @@ public class SolrStream extends TupleStream {
       throw new IOException("--> "+this.baseUrl+":"+e.getMessage());
     } catch (Exception e) {
       //The Stream source did not provide an exception in a format that the SolrStream could propagate.
-      throw new IOException("--> "+this.baseUrl+": An exception has occurred on the server, refer to server log for details.");
+      throw new IOException("--> "+this.baseUrl+": An exception has occurred on the server, refer to server log for details.", e);
     }
   }
 
diff --git a/solr/solrj/src/java/org/apache/solr/client/solrj/request/CollectionAdminRequest.java b/solr/solrj/src/java/org/apache/solr/client/solrj/request/CollectionAdminRequest.java
index b7a9a60..452c7a1 100644
--- a/solr/solrj/src/java/org/apache/solr/client/solrj/request/CollectionAdminRequest.java
+++ b/solr/solrj/src/java/org/apache/solr/client/solrj/request/CollectionAdminRequest.java
@@ -1042,6 +1042,10 @@ public abstract class CollectionAdminRequest<T extends CollectionAdminResponse>
     return new RequestStatus(requestId);
   }
 
+  public static void waitForAsyncRequest(String requestId, SolrClient client, long timeout) throws SolrServerException, InterruptedException, IOException {
+    requestStatus(requestId).waitFor(client, timeout);
+  }
+
   // REQUESTSTATUS request
   public static class RequestStatus extends CollectionAdminRequest<RequestStatusResponse> {
 
diff --git a/solr/solrj/src/java/org/apache/solr/client/solrj/request/UpdateRequest.java b/solr/solrj/src/java/org/apache/solr/client/solrj/request/UpdateRequest.java
index d0f7759..f93a197 100644
--- a/solr/solrj/src/java/org/apache/solr/client/solrj/request/UpdateRequest.java
+++ b/solr/solrj/src/java/org/apache/solr/client/solrj/request/UpdateRequest.java
@@ -30,13 +30,17 @@ import java.util.Map.Entry;
 import java.util.Objects;
 import java.util.Set;
 
+import org.apache.solr.client.solrj.SolrClient;
+import org.apache.solr.client.solrj.SolrServerException;
 import org.apache.solr.client.solrj.impl.LBHttpSolrClient;
+import org.apache.solr.client.solrj.response.UpdateResponse;
 import org.apache.solr.client.solrj.util.ClientUtils;
 import org.apache.solr.common.SolrInputDocument;
 import org.apache.solr.common.cloud.DocCollection;
 import org.apache.solr.common.cloud.DocRouter;
 import org.apache.solr.common.cloud.Slice;
 import org.apache.solr.common.params.ModifiableSolrParams;
+import org.apache.solr.common.params.UpdateParams;
 import org.apache.solr.common.util.ContentStream;
 import org.apache.solr.common.util.XML;
 
@@ -103,6 +107,10 @@ public class UpdateRequest extends AbstractUpdateRequest {
     return this;
   }
 
+  public UpdateRequest add(String... fields) {
+    return add(new SolrInputDocument(fields));
+  }
+
   /**
    * Add a SolrInputDocument to this request
    * @param doc the document
@@ -209,6 +217,13 @@ public class UpdateRequest extends AbstractUpdateRequest {
     deleteQuery.add(q);
     return this;
   }
+
+  public UpdateResponse commit(SolrClient client, String collection) throws IOException, SolrServerException {
+    if (params == null)
+      params = new ModifiableSolrParams();
+    params.set(UpdateParams.COMMIT, "true");
+    return process(client, collection);
+  }
   
   /**
    * @param router to route updates with
@@ -383,7 +398,7 @@ public class UpdateRequest extends AbstractUpdateRequest {
   /**
    * @since solr 1.4
    */
-  public void writeXML(Writer writer) throws IOException {
+  public UpdateRequest writeXML(Writer writer) throws IOException {
     List<Map<SolrInputDocument,Map<String,Object>>> getDocLists = getDocLists(documents);
     
     for (Map<SolrInputDocument,Map<String,Object>> docs : getDocLists) {
@@ -457,6 +472,7 @@ public class UpdateRequest extends AbstractUpdateRequest {
       }
       writer.append("</delete>");
     }
+    return this;
   }
   
   // --------------------------------------------------------------------------
diff --git a/solr/solrj/src/java/org/apache/solr/common/SolrInputDocument.java b/solr/solrj/src/java/org/apache/solr/common/SolrInputDocument.java
index 6796300..3d3c060 100644
--- a/solr/solrj/src/java/org/apache/solr/common/SolrInputDocument.java
+++ b/solr/solrj/src/java/org/apache/solr/common/SolrInputDocument.java
@@ -38,8 +38,12 @@ public class SolrInputDocument extends SolrDocumentBase<SolrInputField, SolrInpu
   private float _documentBoost = 1.0f;
   private List<SolrInputDocument> _childDocuments;
   
-  public SolrInputDocument() {
+  public SolrInputDocument(String... fields) {
     _fields = new LinkedHashMap<>();
+    assert fields.length % 2 == 0;
+    for (int i = 0; i < fields.length; i += 2) {
+      addField(fields[i], fields[i + 1]);
+    }
   }
   
   public SolrInputDocument(Map<String,SolrInputField> fields) {
diff --git a/solr/solrj/src/test-files/solrj/solr/collection1/conf/schema-streaming.xml b/solr/solrj/src/test-files/solrj/solr/collection1/conf/schema-streaming.xml
deleted file mode 100644
index 575b622..0000000
--- a/solr/solrj/src/test-files/solrj/solr/collection1/conf/schema-streaming.xml
+++ /dev/null
@@ -1,605 +0,0 @@
-<?xml version="1.0" ?>
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-
-<!-- The Solr schema file. This file should be named "schema.xml" and
-     should be located where the classloader for the Solr webapp can find it.
-
-     This schema is used for testing, and as such has everything and the
-     kitchen sink thrown in. See example/solr/conf/schema.xml for a
-     more concise example.
-
-  -->
-
-<schema name="test" version="1.6">
-  <types>
-
-    <!-- field type definitions... note that the "name" attribute is
-         just a label to be used by field definitions.  The "class"
-         attribute and any other attributes determine the real type and
-         behavior of the fieldtype.
-      -->
-
-    <!-- numeric field types that store and index the text
-         value verbatim (and hence don't sort correctly or support range queries.)
-         These are provided more for backward compatability, allowing one
-         to create a schema that matches an existing lucene index.
-    -->
-
-
-    <fieldType name="int" docValues="true" class="solr.TrieIntField" precisionStep="0" omitNorms="true" positionIncrementGap="0"/>
-    <fieldType name="float" docValues="true" class="solr.TrieFloatField" precisionStep="0" omitNorms="true" positionIncrementGap="0"/>
-    <fieldType name="long" class="solr.TrieLongField" precisionStep="0" omitNorms="true" positionIncrementGap="0"/>
-    <fieldType name="double" class="solr.TrieDoubleField" precisionStep="0" omitNorms="true" positionIncrementGap="0"/>
-
-    <fieldType name="tint" class="solr.TrieIntField" precisionStep="8" omitNorms="true" positionIncrementGap="0"/>
-    <fieldType name="tfloat" class="solr.TrieFloatField" precisionStep="8" omitNorms="true" positionIncrementGap="0"/>
-    <fieldType name="tlong" class="solr.TrieLongField" precisionStep="8" omitNorms="true" positionIncrementGap="0"/>
-    <fieldType name="tdouble" class="solr.TrieDoubleField" precisionStep="8" omitNorms="true" positionIncrementGap="0"/>
-
-    <fieldType name="random" class="solr.RandomSortField" indexed="true" />
-
-    <!-- numeric field types that manipulate the value into
-       a string value that isn't human readable in it's internal form,
-       but sorts correctly and supports range queries.
-
-         If sortMissingLast="true" then a sort on this field will cause documents
-       without the field to come after documents with the field,
-       regardless of the requested sort order.
-         If sortMissingFirst="true" then a sort on this field will cause documents
-       without the field to come before documents with the field,
-       regardless of the requested sort order.
-         If sortMissingLast="false" and sortMissingFirst="false" (the default),
-       then default lucene sorting will be used which places docs without the field
-       first in an ascending sort and last in a descending sort.
-    -->
-
-
-
-    <!-- Field type demonstrating an Analyzer failure -->
-    <fieldtype name="failtype1" class="solr.TextField">
-      <analyzer type="index">
-        <tokenizer class="solr.MockTokenizerFactory"/>
-        <filter class="solr.WordDelimiterFilterFactory" generateWordParts="1" generateNumberParts="0" catenateWords="0" catenateNumbers="0" catenateAll="0"/>
-        <filter class="solr.LowerCaseFilterFactory"/>
-      </analyzer>
-    </fieldtype>
-
-    <!-- Demonstrating ignoreCaseChange -->
-    <fieldtype name="wdf_nocase" class="solr.TextField">
-      <analyzer>
-        <tokenizer class="solr.MockTokenizerFactory"/>
-        <filter class="solr.WordDelimiterFilterFactory" generateWordParts="1" generateNumberParts="0" catenateWords="0" catenateNumbers="0" catenateAll="0" splitOnCaseChange="0" preserveOriginal="0"/>
-        <filter class="solr.LowerCaseFilterFactory"/>
-      </analyzer>
-    </fieldtype>
-
-    <fieldtype name="wdf_preserve" class="solr.TextField">
-      <analyzer>
-        <tokenizer class="solr.MockTokenizerFactory"/>
-        <filter class="solr.WordDelimiterFilterFactory" generateWordParts="0" generateNumberParts="1" catenateWords="0" catenateNumbers="0" catenateAll="0" splitOnCaseChange="0" preserveOriginal="1"/>
-        <filter class="solr.LowerCaseFilterFactory"/>
-      </analyzer>
-    </fieldtype>
-
-
-    <fieldtype name="boolean" class="solr.BoolField" sortMissingLast="true"/>
-    <fieldtype name="string" class="solr.StrField" sortMissingLast="true" docValues="true"/>
-
-    <!-- format for date is 1995-12-31T23:59:59.999Z and only the fractional
-         seconds part (.999) is optional.
-      -->
-    <fieldtype name="date" class="solr.TrieDateField" precisionStep="0"/>
-    <fieldtype name="tdate" class="solr.TrieDateField" precisionStep="6"/>
-
-
-    <!-- solr.TextField allows the specification of custom
-         text analyzers specified as a tokenizer and a list
-         of token filters.
-      -->
-    <fieldtype name="text" class="solr.TextField">
-      <analyzer>
-        <tokenizer class="solr.StandardTokenizerFactory"/>
-        <filter class="solr.StandardFilterFactory"/>
-        <filter class="solr.LowerCaseFilterFactory"/>
-        <filter class="solr.StopFilterFactory"/>
-        <filter class="solr.PorterStemFilterFactory"/>
-      </analyzer>
-    </fieldtype>
-
-
-    <fieldtype name="nametext" class="solr.TextField">
-      <analyzer class="org.apache.lucene.analysis.core.WhitespaceAnalyzer"/>
-    </fieldtype>
-
-    <fieldtype name="teststop" class="solr.TextField">
-      <analyzer>
-        <tokenizer class="solr.LowerCaseTokenizerFactory"/>
-        <filter class="solr.StandardFilterFactory"/>
-      </analyzer>
-    </fieldtype>
-
-    <!-- fieldtypes in this section isolate tokenizers and tokenfilters for testing -->
-    <fieldtype name="lowertok" class="solr.TextField">
-      <analyzer><tokenizer class="solr.LowerCaseTokenizerFactory"/></analyzer>
-    </fieldtype>
-    <fieldtype name="keywordtok" class="solr.TextField">
-      <analyzer><tokenizer class="solr.MockTokenizerFactory" pattern="keyword"/></analyzer>
-    </fieldtype>
-    <fieldtype name="standardtok" class="solr.TextField">
-      <analyzer><tokenizer class="solr.StandardTokenizerFactory"/></analyzer>
-    </fieldtype>
-    <fieldtype name="lettertok" class="solr.TextField">
-      <analyzer><tokenizer class="solr.LetterTokenizerFactory"/></analyzer>
-    </fieldtype>
-    <fieldtype name="whitetok" class="solr.TextField">
-      <analyzer><tokenizer class="solr.MockTokenizerFactory"/></analyzer>
-    </fieldtype>
-    <fieldtype name="HTMLstandardtok" class="solr.TextField">
-      <analyzer>
-        <charFilter class="solr.HTMLStripCharFilterFactory"/>
-        <tokenizer class="solr.StandardTokenizerFactory"/>
-      </analyzer>
-    </fieldtype>
-    <fieldtype name="HTMLwhitetok" class="solr.TextField">
-      <analyzer>
-        <charFilter class="solr.HTMLStripCharFilterFactory"/>
-        <tokenizer class="solr.MockTokenizerFactory"/>
-      </analyzer>
-    </fieldtype>
-    <fieldtype name="standardtokfilt" class="solr.TextField">
-      <analyzer>
-        <tokenizer class="solr.StandardTokenizerFactory"/>
-        <filter class="solr.StandardFilterFactory"/>
-      </analyzer>
-    </fieldtype>
-    <fieldtype name="standardfilt" class="solr.TextField">
-      <analyzer>
-        <tokenizer class="solr.MockTokenizerFactory"/>
-        <filter class="solr.StandardFilterFactory"/>
-      </analyzer>
-    </fieldtype>
-    <fieldtype name="lowerfilt" class="solr.TextField">
-      <analyzer>
-        <tokenizer class="solr.MockTokenizerFactory"/>
-        <filter class="solr.LowerCaseFilterFactory"/>
-      </analyzer>
-    </fieldtype>
-    <fieldtype name="lowerpunctfilt" class="solr.TextField">
-      <analyzer>
-        <tokenizer class="solr.MockTokenizerFactory"/>
-        <filter class="solr.WordDelimiterFilterFactory" generateWordParts="1" generateNumberParts="1" catenateWords="1" catenateNumbers="1" catenateAll="1" splitOnCaseChange="1"/>
-        <filter class="solr.LowerCaseFilterFactory"/>
-      </analyzer>
-    </fieldtype>
-    <fieldtype name="patternreplacefilt" class="solr.TextField">
-      <analyzer type="index">
-        <tokenizer class="solr.MockTokenizerFactory" pattern="keyword"/>
-        <filter class="solr.PatternReplaceFilterFactory"
-                pattern="([^a-zA-Z])" replacement="_" replace="all"
-            />
-      </analyzer>
-      <analyzer type="query">
-        <tokenizer class="solr.MockTokenizerFactory" pattern="keyword"/>
-      </analyzer>
-    </fieldtype>
-    <fieldtype name="patterntok" class="solr.TextField">
-      <analyzer>
-        <tokenizer class="solr.PatternTokenizerFactory" pattern=","/>
-      </analyzer>
-    </fieldtype>
-    <fieldtype name="porterfilt" class="solr.TextField">
-      <analyzer>
-        <tokenizer class="solr.MockTokenizerFactory"/>
-        <filter class="solr.PorterStemFilterFactory"/>
-      </analyzer>
-    </fieldtype>
-    <!-- fieldtype name="snowballfilt" class="solr.TextField">
-      <analyzer>
-        <tokenizer class="solr.MockTokenizerFactory"/>
-        <filter class="solr.SnowballPorterFilterFactory"/>
-      </analyzer>
-    </fieldtype -->
-    <fieldtype name="engporterfilt" class="solr.TextField">
-      <analyzer>
-        <tokenizer class="solr.MockTokenizerFactory"/>
-        <filter class="solr.PorterStemFilterFactory"/>
-      </analyzer>
-    </fieldtype>
-    <fieldtype name="custengporterfilt" class="solr.TextField">
-      <analyzer>
-        <tokenizer class="solr.MockTokenizerFactory"/>
-        <filter class="solr.PorterStemFilterFactory"/>
-      </analyzer>
-    </fieldtype>
-    <fieldtype name="stopfilt" class="solr.TextField">
-      <analyzer>
-        <tokenizer class="solr.MockTokenizerFactory"/>
-        <filter class="solr.StopFilterFactory" ignoreCase="true"/>
-      </analyzer>
-    </fieldtype>
-    <fieldtype name="custstopfilt" class="solr.TextField">
-      <analyzer>
-        <tokenizer class="solr.MockTokenizerFactory"/>
-      </analyzer>
-    </fieldtype>
-    <fieldtype name="lengthfilt" class="solr.TextField">
-      <analyzer>
-        <tokenizer class="solr.MockTokenizerFactory"/>
-        <filter class="solr.LengthFilterFactory" min="2" max="5"/>
-      </analyzer>
-    </fieldtype>
-    <fieldType name="charfilthtmlmap" class="solr.TextField">
-      <analyzer>
-        <charFilter class="solr.HTMLStripCharFilterFactory"/>
-        <tokenizer class="solr.MockTokenizerFactory"/>
-      </analyzer>
-    </fieldType>
-
-    <fieldtype name="subword" class="solr.TextField" multiValued="true" positionIncrementGap="100">
-      <analyzer type="index">
-        <tokenizer class="solr.MockTokenizerFactory"/>
-        <filter class="solr.WordDelimiterFilterFactory" generateWordParts="1" generateNumberParts="1" catenateWords="1" catenateNumbers="1" catenateAll="0"/>
-        <filter class="solr.LowerCaseFilterFactory"/>
-        <filter class="solr.StopFilterFactory"/>
-        <filter class="solr.PorterStemFilterFactory"/>
-      </analyzer>
-      <analyzer type="query">
-        <tokenizer class="solr.MockTokenizerFactory"/>
-        <filter class="solr.WordDelimiterFilterFactory" generateWordParts="1" generateNumberParts="1" catenateWords="0" catenateNumbers="0" catenateAll="0"/>
-        <filter class="solr.LowerCaseFilterFactory"/>
-        <filter class="solr.StopFilterFactory"/>
-        <filter class="solr.PorterStemFilterFactory"/>
-      </analyzer>
-    </fieldtype>
-
-    <fieldtype name="numericsubword" class="solr.TextField" multiValued="true" positionIncrementGap="100">
-      <analyzer type="index">
-        <tokenizer class="solr.MockTokenizerFactory"/>
-        <filter class="solr.LowerCaseFilterFactory"/>
-        <filter class="solr.WordDelimiterFilterFactory" splitOnNumerics="0" splitOnCaseChange="0" generateWordParts="1" generateNumberParts="0" catenateWords="0" catenateNumbers="0" catenateAll="0"/>
-        <filter class="solr.StopFilterFactory"/>
-        <filter class="solr.PorterStemFilterFactory"/>
-      </analyzer>
-      <analyzer type="query">
-        <tokenizer class="solr.MockTokenizerFactory"/>
-        <filter class="solr.LowerCaseFilterFactory"/>
-        <filter class="solr.WordDelimiterFilterFactory" splitOnNumerics="0" splitOnCaseChange="0" generateWordParts="1" generateNumberParts="1" catenateWords="1" catenateNumbers="1" catenateAll="0"/>
-        <filter class="solr.StopFilterFactory"/>
-        <filter class="solr.PorterStemFilterFactory"/>
-      </analyzer>
-    </fieldtype>
-
-    <fieldtype name="protectedsubword" class="solr.TextField" multiValued="true" positionIncrementGap="100">
-      <analyzer type="index">
-        <tokenizer class="solr.MockTokenizerFactory"/>
-        <filter class="solr.LowerCaseFilterFactory"/>
-        <filter class="solr.WordDelimiterFilterFactory"  splitOnNumerics="0" splitOnCaseChange="0" generateWordParts="1" generateNumberParts="1" catenateWords="0" catenateNumbers="0" catenateAll="0"/>
-      </analyzer>
-      <analyzer type="query">
-        <tokenizer class="solr.MockTokenizerFactory"/>
-        <filter class="solr.LowerCaseFilterFactory"/>
-      </analyzer>
-    </fieldtype>
-
-
-    <!-- more flexible in matching skus, but more chance of a false match -->
-    <fieldtype name="skutype1" class="solr.TextField">
-      <analyzer type="index">
-        <tokenizer class="solr.MockTokenizerFactory"/>
-        <filter class="solr.WordDelimiterFilterFactory" generateWordParts="1" generateNumberParts="1" catenateWords="1" catenateNumbers="1" catenateAll="0"/>
-        <filter class="solr.LowerCaseFilterFactory"/>
-      </analyzer>
-      <analyzer type="query">
-        <tokenizer class="solr.MockTokenizerFactory"/>
-        <filter class="solr.WordDelimiterFilterFactory" generateWordParts="0" generateNumberParts="0" catenateWords="1" catenateNumbers="1" catenateAll="0"/>
-        <filter class="solr.LowerCaseFilterFactory"/>
-      </analyzer>
-    </fieldtype>
-
-    <!-- less flexible in matching skus, but less chance of a false match -->
-    <fieldtype name="skutype2" class="solr.TextField">
-      <analyzer type="index">
-        <tokenizer class="solr.MockTokenizerFactory"/>
-        <filter class="solr.WordDelimiterFilterFactory" generateWordParts="0" generateNumberParts="0" catenateWords="1" catenateNumbers="1" catenateAll="0"/>
-        <filter class="solr.LowerCaseFilterFactory"/>
-      </analyzer>
-      <analyzer type="query">
-        <tokenizer class="solr.MockTokenizerFactory"/>
-        <filter class="solr.WordDelimiterFilterFactory" generateWordParts="0" generateNumberParts="0" catenateWords="1" catenateNumbers="1" catenateAll="0"/>
-        <filter class="solr.LowerCaseFilterFactory"/>
-      </analyzer>
-    </fieldtype>
-
-    <!-- less flexible in matching skus, but less chance of a false match -->
-    <fieldtype name="syn" class="solr.TextField">
-      <analyzer>
-        <tokenizer class="solr.MockTokenizerFactory"/>
-      </analyzer>
-    </fieldtype>
-
-
-    <fieldtype  name="unstored" class="solr.StrField" indexed="true" stored="false"/>
-
-
-    <fieldtype name="textgap" class="solr.TextField" multiValued="true" positionIncrementGap="100">
-      <analyzer>
-        <tokenizer class="solr.MockTokenizerFactory"/>
-        <filter class="solr.LowerCaseFilterFactory"/>
-      </analyzer>
-    </fieldtype>
-
-    <fieldType name="uuid" class="solr.UUIDField" />
-
-    <!-- Try out some point types -->
-    <fieldType name="xy" class="solr.PointType" dimension="2" subFieldType="double"/>
-    <fieldType name="x" class="solr.PointType" dimension="1" subFieldType="double"/>
-    <fieldType name="tenD" class="solr.PointType" dimension="10" subFieldType="double"/>
-    <!-- Use the sub field suffix -->
-    <fieldType name="xyd" class="solr.PointType" dimension="2" subFieldSuffix="_d1"/>
-    <fieldtype name="geohash" class="solr.GeoHashField"/>
-
-
-    <fieldType name="latLon" class="solr.LatLonType" subFieldType="double"/>
-
-    <!--  some per-field similarity examples -->
-
-    <!--  specify a Similarity classname directly -->
-    <!--
-    <fieldType name="sim1" class="solr.TextField">
-      <analyzer>
-        <tokenizer class="solr.MockTokenizerFactory"/>
-      </analyzer>
-      <similarity class="org.apache.lucene.misc.SweetSpotSimilarity"/>
-    </fieldType>
-    -->
-    <!--  specify a Similarity factory -->
-    <!--
-    <fieldType name="sim2" class="solr.TextField">
-      <analyzer>
-        <tokenizer class="solr.MockTokenizerFactory"/>
-      </analyzer>
-      <similarity class="org.apache.solr.search.similarities.CustomSimilarityFactory">
-        <str name="echo">is there an echo?</str>
-      </similarity>
-    </fieldType>
-    -->
-    <!-- don't specify any sim at all: get the default  -->
-    <!--
-    <fieldType name="sim3" class="solr.TextField">
-      <analyzer>
-        <tokenizer class="solr.MockTokenizerFactory"/>
-      </analyzer>
-    </fieldType>
-    -->
-  </types>
-
-
-  <fields>
-    <field name="id" type="int" indexed="true" stored="true" multiValued="false" required="false"/>
-    <field name="signatureField" type="string" indexed="true" stored="false"/>
-
-    <field name="s_multi" type="string" indexed="true" stored="true" docValues="true" multiValued="true"/>
-    <field name="i_multi" type="int" indexed="true" stored="true" docValues="true" multiValued="true"/>
-    <field name="f_multi" type="float" indexed="true" stored="true" docValues="true" multiValued="true"/>
-    <field name="l_multi" type="long" indexed="true" stored="true" docValues="true" multiValued="true"/>
-    <field name="d_multi" type="double" indexed="true" stored="true" docValues="true" multiValued="true"/>
-
-    <field name="uuid" type="uuid" stored="true" />
-    <field name="name" type="nametext" indexed="true" stored="true"/>
-    <field name="text" type="text" indexed="true" stored="false"/>
-    <field name="subject" type="text" indexed="true" stored="true"/>
-    <field name="title" type="nametext" indexed="true" stored="true"/>
-    <field name="weight" type="float" indexed="true" stored="true" multiValued="false"/>
-    <field name="bday" type="date" indexed="true" stored="true" multiValued="false"/>
-
-    <field name="title_stemmed" type="text" indexed="true" stored="false"/>
-    <field name="title_lettertok" type="lettertok" indexed="true" stored="false"/>
-
-    <field name="syn" type="syn" indexed="true" stored="true"/>
-
-    <!-- to test property inheritance and overriding -->
-    <field name="shouldbeunstored" type="unstored" />
-    <field name="shouldbestored" type="unstored" stored="true"/>
-    <field name="shouldbeunindexed" type="unstored" indexed="false" stored="true"/>
-
-    <!-- Test points -->
-    <!-- Test points -->
-    <field name="home" type="xy" indexed="true" stored="true" multiValued="false"/>
-    <field name="x" type="x" indexed="true" stored="true" multiValued="false"/>
-    <field name="homed" type="xyd" indexed="true" stored="true" multiValued="false"/>
-    <field name="home_ns" type="xy" indexed="true" stored="false" multiValued="false"/>
-    <field name="work" type="xy" indexed="true" stored="true" multiValued="false"/>
-
-    <field name="home_ll" type="latLon" indexed="true" stored="true" multiValued="false"/>
-    <field name="home_gh" type="geohash" indexed="true" stored="true" multiValued="false"/>
-
-
-    <field name="point10" type="tenD" indexed="true" stored="true" multiValued="false"/>
-
-
-    <!-- test different combinations of indexed and stored -->
-    <field name="bind" type="boolean" indexed="true" stored="false"/>
-    <field name="bsto" type="boolean" indexed="false" stored="true"/>
-    <field name="bindsto" type="boolean" indexed="true" stored="true"/>
-    <field name="isto" type="int" indexed="false" stored="true"/>
-    <field name="iind" type="int" indexed="true" stored="false"/>
-    <field name="ssto" type="string" indexed="false" stored="true"/>
-    <field name="sind" type="string" indexed="true" stored="false"/>
-    <field name="sindsto" type="string" indexed="true" stored="true"/>
-
-    <!-- test combinations of term vector settings -->
-    <field name="test_basictv" type="text" termVectors="true"/>
-    <field name="test_notv" type="text" termVectors="false"/>
-    <field name="test_postv" type="text" termVectors="true" termPositions="true"/>
-    <field name="test_offtv" type="text" termVectors="true" termOffsets="true"/>
-    <field name="test_posofftv" type="text" termVectors="true"
-           termPositions="true" termOffsets="true"/>
-
-    <!-- fields to test individual tokenizers and tokenfilters -->
-    <field name="teststop" type="teststop" indexed="true" stored="true"/>
-    <field name="lowertok" type="lowertok" indexed="true" stored="true"/>
-    <field name="keywordtok" type="keywordtok" indexed="true" stored="true"/>
-    <field name="standardtok" type="standardtok" indexed="true" stored="true"/>
-    <field name="HTMLstandardtok" type="HTMLstandardtok" indexed="true" stored="true"/>
-    <field name="lettertok" type="lettertok" indexed="true" stored="true"/>
-    <field name="whitetok" type="whitetok" indexed="true" stored="true"/>
-    <field name="HTMLwhitetok" type="HTMLwhitetok" indexed="true" stored="true"/>
-    <field name="standardtokfilt" type="standardtokfilt" indexed="true" stored="true"/>
-    <field name="standardfilt" type="standardfilt" indexed="true" stored="true"/>
-    <field name="lowerfilt" type="lowerfilt" indexed="true" stored="true"/>
-    <field name="lowerfilt1" type="lowerfilt" indexed="true" stored="true"/>
-    <field name="lowerfilt1and2" type="lowerfilt" indexed="true" stored="true"/>
-    <field name="patterntok" type="patterntok" indexed="true" stored="true"/>
-    <field name="patternreplacefilt" type="patternreplacefilt" indexed="true" stored="true"/>
-    <field name="porterfilt" type="porterfilt" indexed="true" stored="true"/>
-    <field name="engporterfilt" type="engporterfilt" indexed="true" stored="true"/>
-    <field name="custengporterfilt" type="custengporterfilt" indexed="true" stored="true"/>
-    <field name="stopfilt" type="stopfilt" indexed="true" stored="true"/>
-    <field name="custstopfilt" type="custstopfilt" indexed="true" stored="true"/>
-    <field name="lengthfilt" type="lengthfilt" indexed="true" stored="true"/>
-    <field name="wdf_nocase" type="wdf_nocase" indexed="true" stored="true"/>
-    <field name="wdf_preserve" type="wdf_preserve" indexed="true" stored="true"/>
-
-    <field name="numberpartfail" type="failtype1" indexed="true" stored="true"/>
-
-    <field name="nullfirst" type="string" indexed="true" stored="true" sortMissingFirst="true" multiValued="false"/>
-
-    <field name="subword" type="subword" indexed="true" stored="true"/>
-    <field name="subword_offsets" type="subword" indexed="true" stored="true" termOffsets="true"/>
-    <field name="numericsubword" type="numericsubword" indexed="true" stored="true"/>
-    <field name="protectedsubword" type="protectedsubword" indexed="true" stored="true"/>
-
-    <field name="sku1" type="skutype1" indexed="true" stored="true"/>
-    <field name="sku2" type="skutype2" indexed="true" stored="true"/>
-
-    <field name="textgap" type="textgap" indexed="true" stored="true"/>
-
-    <field name="timestamp" type="date" indexed="true" stored="true" default="NOW" multiValued="false"/>
-    <field name="multiDefault" type="string" indexed="true" stored="true" default="muLti-Default" multiValued="true"/>
-    <field name="intDefault" type="int" indexed="true" stored="true" default="42" multiValued="false"/>
-
-    <!--
-    <field name="sim1text" type="sim1" indexed="true" stored="true"/>
-    <field name="sim2text" type="sim2" indexed="true" stored="true"/>
-    <field name="sim3text" type="sim3" indexed="true" stored="true"/>
-    -->
-
-    <field name="tlong" type="tlong" indexed="true" stored="true" />
-
-    <field name="_version_" type="long" indexed="true" stored="true"/>
-
-    <!-- Dynamic field definitions.  If a field name is not found, dynamicFields
-         will be used if the name matches any of the patterns.
-         RESTRICTION: the glob-like pattern in the name attribute must have
-         a "*" only at the start or the end.
-         EXAMPLE:  name="*_i" will match any field ending in _i (like myid_i, z_i)
-         Longer patterns will be matched first.  if equal size patterns
-         both match, the first appearing in the schema will be used.
-    -->
-    <dynamicField name="*_i"  type="int"    indexed="true"  stored="true"/>
-    <dynamicField name="*_i1"  type="int"    indexed="true" stored="true" multiValued="false"/>
-
-    <dynamicField name="*_s"  type="string"  indexed="true"  stored="true"/>
-    <dynamicField name="*_ss" type="string"  indexed="true"  stored="true" multiValued="true"/>
-    <dynamicField name="*_s1"  type="string"  indexed="true"  stored="true" multiValued="false"/>
-    <dynamicField name="*_l"  type="long"   indexed="true"  stored="true"/>
-    <dynamicField name="*_l1"  type="long"   indexed="true"  stored="true" multiValued="false"/>
-    <dynamicField name="*_t"  type="text"    indexed="true"  stored="true"/>
-    <dynamicField name="*_b"  type="boolean" indexed="true"  stored="true"/>
-    <dynamicField name="*_f"  type="float"  indexed="true"  stored="true"/>
-    <dynamicField name="*_f1"  type="float"  indexed="true"  stored="true" multiValued="false"/>
-    <dynamicField name="*_d"  type="double" indexed="true"  stored="true"/>
-    <dynamicField name="*_d1"  type="double" indexed="true"  stored="true" multiValued="false"/>
-    <dynamicField name="*_dt" type="date"    indexed="true"  stored="true"/>
-    <dynamicField name="*_dt1" type="date"    indexed="true"  stored="true" multiValued="false"/>
-
-    <!-- some trie-coded dynamic fields for faster range queries -->
-    <dynamicField name="*_ti" type="tint"    indexed="true"  stored="true"/>
-    <dynamicField name="*_ti1" type="tint"    indexed="true"  stored="true" multiValued="false"/>
-    <dynamicField name="*_tl" type="tlong"   indexed="true"  stored="true"/>
-    <dynamicField name="*_tl1" type="tlong"   indexed="true"  stored="true" multiValued="false"/>
-    <dynamicField name="*_tf" type="tfloat"  indexed="true"  stored="true"/>
-    <dynamicField name="*_tf1" type="tfloat"  indexed="true"  stored="true" multiValued="false"/>
-    <dynamicField name="*_td" type="tdouble" indexed="true"  stored="true"/>
-    <dynamicField name="*_td1" type="tdouble" indexed="true" stored="true" multiValued="false"/>
-    <dynamicField name="*_tds" type="tdouble" indexed="true" stored="true" multiValued="false"/>
-    <dynamicField name="*_tdt" type="tdate"  indexed="true"  stored="true"/>
-    <dynamicField name="*_tdt1" type="tdate"  indexed="true"  stored="true" multiValued="false"/>
-
-
-
-
-    <dynamicField name="*_sI" type="string"  indexed="true"  stored="false"/>
-    <dynamicField name="*_sS" type="string"  indexed="false" stored="true"/>
-    <dynamicField name="t_*"  type="text"    indexed="true"  stored="true"/>
-    <dynamicField name="tv_*"  type="text" indexed="true"  stored="true"
-                  termVectors="true" termPositions="true" termOffsets="true"/>
-    <dynamicField name="tv_mv_*"  type="text" indexed="true"  stored="true" multiValued="true"
-                  termVectors="true" termPositions="true" termOffsets="true"/>
-
-    <dynamicField name="*_p"  type="xyd" indexed="true"  stored="true" multiValued="false"/>
-
-    <!-- special fields for dynamic copyField test -->
-    <dynamicField name="dynamic_*" type="string" indexed="true" stored="true"/>
-    <dynamicField name="*_dynamic" type="string" indexed="true" stored="true"/>
-
-    <!-- for testing to ensure that longer patterns are matched first -->
-    <dynamicField name="*aa"  type="string"  indexed="true" stored="true"/>
-
-    <!-- ignored becuase not stored or indexed -->
-    <dynamicField name="*_ignored" type="text" indexed="false" stored="false"/>
-
-    <dynamicField name="*_mfacet" type="string" indexed="true" stored="false" multiValued="true" />
-
-    <dynamicField name="random_*" type="random" />
-
-
-    <!-- make sure custom sims work with dynamic fields -->
-    <!--
-    <dynamicField name="*_sim1" type="sim1" indexed="true" stored="true"/>
-    <dynamicField name="*_sim2" type="sim2" indexed="true" stored="true"/>
-    <dynamicField name="*_sim3" type="sim3" indexed="true" stored="true"/>
-    -->
-  </fields>
-
-  <defaultSearchField>text</defaultSearchField>
-  <uniqueKey>id</uniqueKey>
-
-  <!-- copyField commands copy one field to another at the time a document
-        is added to the index.  It's used either to index the same field different
-        ways, or to add multiple fields to the same field for easier/faster searching.
-   -->
-  <copyField source="title" dest="title_stemmed"/>
-  <copyField source="title" dest="title_lettertok"/>
-
-  <copyField source="title" dest="text"/>
-  <copyField source="subject" dest="text"/>
-
-  <copyField source="lowerfilt1" dest="lowerfilt1and2"/>
-  <copyField source="lowerfilt" dest="lowerfilt1and2"/>
-
-  <copyField source="*_t" dest="text"/>
-
-
-
-
-
-  <!-- dynamic destination -->
-  <copyField source="*_dynamic" dest="dynamic_*"/>
-
-</schema>
diff --git a/solr/solrj/src/test-files/solrj/solr/collection1/conf/solrconfig-streaming.xml b/solr/solrj/src/test-files/solrj/solr/collection1/conf/solrconfig-streaming.xml
deleted file mode 100644
index 6b10869..0000000
--- a/solr/solrj/src/test-files/solrj/solr/collection1/conf/solrconfig-streaming.xml
+++ /dev/null
@@ -1,51 +0,0 @@
-<?xml version="1.0" encoding="UTF-8" ?>
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-
-<!--
- This is a stripped down config file used for a simple example...
- It is *not* a good example to work from.
--->
-<config>
-  <luceneMatchVersion>${tests.luceneMatchVersion:LUCENE_CURRENT}</luceneMatchVersion>
-  <indexConfig>
-    <useCompoundFile>${useCompoundFile:false}</useCompoundFile>
-  </indexConfig>
-  <dataDir>${solr.data.dir:}</dataDir>
-  <directoryFactory name="DirectoryFactory" class="${solr.directoryFactory:solr.StandardDirectoryFactory}"/>
-  <schemaFactory class="ClassicIndexSchemaFactory"/>
-
-  <updateHandler class="solr.DirectUpdateHandler2">
-    <updateLog>
-      <str name="dir">${solr.data.dir:}</str>
-    </updateLog>
-  </updateHandler>
-
-
-  <requestDispatcher handleSelect="true" >
-    <requestParsers enableRemoteStreaming="false" multipartUploadLimitInKB="2048" />
-  </requestDispatcher>
-
-  <requestHandler name="standard" class="solr.StandardRequestHandler" default="true" />
-
-  <!-- config for the admin interface -->
-  <admin>
-    <defaultQuery>solr</defaultQuery>
-  </admin>
-
-</config>
-
diff --git a/solr/solrj/src/test-files/solrj/solr/configsets/streaming/conf/schema.xml b/solr/solrj/src/test-files/solrj/solr/configsets/streaming/conf/schema.xml
new file mode 100644
index 0000000..575b622
--- /dev/null
+++ b/solr/solrj/src/test-files/solrj/solr/configsets/streaming/conf/schema.xml
@@ -0,0 +1,605 @@
+<?xml version="1.0" ?>
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+
+<!-- The Solr schema file. This file should be named "schema.xml" and
+     should be located where the classloader for the Solr webapp can find it.
+
+     This schema is used for testing, and as such has everything and the
+     kitchen sink thrown in. See example/solr/conf/schema.xml for a
+     more concise example.
+
+  -->
+
+<schema name="test" version="1.6">
+  <types>
+
+    <!-- field type definitions... note that the "name" attribute is
+         just a label to be used by field definitions.  The "class"
+         attribute and any other attributes determine the real type and
+         behavior of the fieldtype.
+      -->
+
+    <!-- numeric field types that store and index the text
+         value verbatim (and hence don't sort correctly or support range queries.)
+         These are provided more for backward compatability, allowing one
+         to create a schema that matches an existing lucene index.
+    -->
+
+
+    <fieldType name="int" docValues="true" class="solr.TrieIntField" precisionStep="0" omitNorms="true" positionIncrementGap="0"/>
+    <fieldType name="float" docValues="true" class="solr.TrieFloatField" precisionStep="0" omitNorms="true" positionIncrementGap="0"/>
+    <fieldType name="long" class="solr.TrieLongField" precisionStep="0" omitNorms="true" positionIncrementGap="0"/>
+    <fieldType name="double" class="solr.TrieDoubleField" precisionStep="0" omitNorms="true" positionIncrementGap="0"/>
+
+    <fieldType name="tint" class="solr.TrieIntField" precisionStep="8" omitNorms="true" positionIncrementGap="0"/>
+    <fieldType name="tfloat" class="solr.TrieFloatField" precisionStep="8" omitNorms="true" positionIncrementGap="0"/>
+    <fieldType name="tlong" class="solr.TrieLongField" precisionStep="8" omitNorms="true" positionIncrementGap="0"/>
+    <fieldType name="tdouble" class="solr.TrieDoubleField" precisionStep="8" omitNorms="true" positionIncrementGap="0"/>
+
+    <fieldType name="random" class="solr.RandomSortField" indexed="true" />
+
+    <!-- numeric field types that manipulate the value into
+       a string value that isn't human readable in it's internal form,
+       but sorts correctly and supports range queries.
+
+         If sortMissingLast="true" then a sort on this field will cause documents
+       without the field to come after documents with the field,
+       regardless of the requested sort order.
+         If sortMissingFirst="true" then a sort on this field will cause documents
+       without the field to come before documents with the field,
+       regardless of the requested sort order.
+         If sortMissingLast="false" and sortMissingFirst="false" (the default),
+       then default lucene sorting will be used which places docs without the field
+       first in an ascending sort and last in a descending sort.
+    -->
+
+
+
+    <!-- Field type demonstrating an Analyzer failure -->
+    <fieldtype name="failtype1" class="solr.TextField">
+      <analyzer type="index">
+        <tokenizer class="solr.MockTokenizerFactory"/>
+        <filter class="solr.WordDelimiterFilterFactory" generateWordParts="1" generateNumberParts="0" catenateWords="0" catenateNumbers="0" catenateAll="0"/>
+        <filter class="solr.LowerCaseFilterFactory"/>
+      </analyzer>
+    </fieldtype>
+
+    <!-- Demonstrating ignoreCaseChange -->
+    <fieldtype name="wdf_nocase" class="solr.TextField">
+      <analyzer>
+        <tokenizer class="solr.MockTokenizerFactory"/>
+        <filter class="solr.WordDelimiterFilterFactory" generateWordParts="1" generateNumberParts="0" catenateWords="0" catenateNumbers="0" catenateAll="0" splitOnCaseChange="0" preserveOriginal="0"/>
+        <filter class="solr.LowerCaseFilterFactory"/>
+      </analyzer>
+    </fieldtype>
+
+    <fieldtype name="wdf_preserve" class="solr.TextField">
+      <analyzer>
+        <tokenizer class="solr.MockTokenizerFactory"/>
+        <filter class="solr.WordDelimiterFilterFactory" generateWordParts="0" generateNumberParts="1" catenateWords="0" catenateNumbers="0" catenateAll="0" splitOnCaseChange="0" preserveOriginal="1"/>
+        <filter class="solr.LowerCaseFilterFactory"/>
+      </analyzer>
+    </fieldtype>
+
+
+    <fieldtype name="boolean" class="solr.BoolField" sortMissingLast="true"/>
+    <fieldtype name="string" class="solr.StrField" sortMissingLast="true" docValues="true"/>
+
+    <!-- format for date is 1995-12-31T23:59:59.999Z and only the fractional
+         seconds part (.999) is optional.
+      -->
+    <fieldtype name="date" class="solr.TrieDateField" precisionStep="0"/>
+    <fieldtype name="tdate" class="solr.TrieDateField" precisionStep="6"/>
+
+
+    <!-- solr.TextField allows the specification of custom
+         text analyzers specified as a tokenizer and a list
+         of token filters.
+      -->
+    <fieldtype name="text" class="solr.TextField">
+      <analyzer>
+        <tokenizer class="solr.StandardTokenizerFactory"/>
+        <filter class="solr.StandardFilterFactory"/>
+        <filter class="solr.LowerCaseFilterFactory"/>
+        <filter class="solr.StopFilterFactory"/>
+        <filter class="solr.PorterStemFilterFactory"/>
+      </analyzer>
+    </fieldtype>
+
+
+    <fieldtype name="nametext" class="solr.TextField">
+      <analyzer class="org.apache.lucene.analysis.core.WhitespaceAnalyzer"/>
+    </fieldtype>
+
+    <fieldtype name="teststop" class="solr.TextField">
+      <analyzer>
+        <tokenizer class="solr.LowerCaseTokenizerFactory"/>
+        <filter class="solr.StandardFilterFactory"/>
+      </analyzer>
+    </fieldtype>
+
+    <!-- fieldtypes in this section isolate tokenizers and tokenfilters for testing -->
+    <fieldtype name="lowertok" class="solr.TextField">
+      <analyzer><tokenizer class="solr.LowerCaseTokenizerFactory"/></analyzer>
+    </fieldtype>
+    <fieldtype name="keywordtok" class="solr.TextField">
+      <analyzer><tokenizer class="solr.MockTokenizerFactory" pattern="keyword"/></analyzer>
+    </fieldtype>
+    <fieldtype name="standardtok" class="solr.TextField">
+      <analyzer><tokenizer class="solr.StandardTokenizerFactory"/></analyzer>
+    </fieldtype>
+    <fieldtype name="lettertok" class="solr.TextField">
+      <analyzer><tokenizer class="solr.LetterTokenizerFactory"/></analyzer>
+    </fieldtype>
+    <fieldtype name="whitetok" class="solr.TextField">
+      <analyzer><tokenizer class="solr.MockTokenizerFactory"/></analyzer>
+    </fieldtype>
+    <fieldtype name="HTMLstandardtok" class="solr.TextField">
+      <analyzer>
+        <charFilter class="solr.HTMLStripCharFilterFactory"/>
+        <tokenizer class="solr.StandardTokenizerFactory"/>
+      </analyzer>
+    </fieldtype>
+    <fieldtype name="HTMLwhitetok" class="solr.TextField">
+      <analyzer>
+        <charFilter class="solr.HTMLStripCharFilterFactory"/>
+        <tokenizer class="solr.MockTokenizerFactory"/>
+      </analyzer>
+    </fieldtype>
+    <fieldtype name="standardtokfilt" class="solr.TextField">
+      <analyzer>
+        <tokenizer class="solr.StandardTokenizerFactory"/>
+        <filter class="solr.StandardFilterFactory"/>
+      </analyzer>
+    </fieldtype>
+    <fieldtype name="standardfilt" class="solr.TextField">
+      <analyzer>
+        <tokenizer class="solr.MockTokenizerFactory"/>
+        <filter class="solr.StandardFilterFactory"/>
+      </analyzer>
+    </fieldtype>
+    <fieldtype name="lowerfilt" class="solr.TextField">
+      <analyzer>
+        <tokenizer class="solr.MockTokenizerFactory"/>
+        <filter class="solr.LowerCaseFilterFactory"/>
+      </analyzer>
+    </fieldtype>
+    <fieldtype name="lowerpunctfilt" class="solr.TextField">
+      <analyzer>
+        <tokenizer class="solr.MockTokenizerFactory"/>
+        <filter class="solr.WordDelimiterFilterFactory" generateWordParts="1" generateNumberParts="1" catenateWords="1" catenateNumbers="1" catenateAll="1" splitOnCaseChange="1"/>
+        <filter class="solr.LowerCaseFilterFactory"/>
+      </analyzer>
+    </fieldtype>
+    <fieldtype name="patternreplacefilt" class="solr.TextField">
+      <analyzer type="index">
+        <tokenizer class="solr.MockTokenizerFactory" pattern="keyword"/>
+        <filter class="solr.PatternReplaceFilterFactory"
+                pattern="([^a-zA-Z])" replacement="_" replace="all"
+            />
+      </analyzer>
+      <analyzer type="query">
+        <tokenizer class="solr.MockTokenizerFactory" pattern="keyword"/>
+      </analyzer>
+    </fieldtype>
+    <fieldtype name="patterntok" class="solr.TextField">
+      <analyzer>
+        <tokenizer class="solr.PatternTokenizerFactory" pattern=","/>
+      </analyzer>
+    </fieldtype>
+    <fieldtype name="porterfilt" class="solr.TextField">
+      <analyzer>
+        <tokenizer class="solr.MockTokenizerFactory"/>
+        <filter class="solr.PorterStemFilterFactory"/>
+      </analyzer>
+    </fieldtype>
+    <!-- fieldtype name="snowballfilt" class="solr.TextField">
+      <analyzer>
+        <tokenizer class="solr.MockTokenizerFactory"/>
+        <filter class="solr.SnowballPorterFilterFactory"/>
+      </analyzer>
+    </fieldtype -->
+    <fieldtype name="engporterfilt" class="solr.TextField">
+      <analyzer>
+        <tokenizer class="solr.MockTokenizerFactory"/>
+        <filter class="solr.PorterStemFilterFactory"/>
+      </analyzer>
+    </fieldtype>
+    <fieldtype name="custengporterfilt" class="solr.TextField">
+      <analyzer>
+        <tokenizer class="solr.MockTokenizerFactory"/>
+        <filter class="solr.PorterStemFilterFactory"/>
+      </analyzer>
+    </fieldtype>
+    <fieldtype name="stopfilt" class="solr.TextField">
+      <analyzer>
+        <tokenizer class="solr.MockTokenizerFactory"/>
+        <filter class="solr.StopFilterFactory" ignoreCase="true"/>
+      </analyzer>
+    </fieldtype>
+    <fieldtype name="custstopfilt" class="solr.TextField">
+      <analyzer>
+        <tokenizer class="solr.MockTokenizerFactory"/>
+      </analyzer>
+    </fieldtype>
+    <fieldtype name="lengthfilt" class="solr.TextField">
+      <analyzer>
+        <tokenizer class="solr.MockTokenizerFactory"/>
+        <filter class="solr.LengthFilterFactory" min="2" max="5"/>
+      </analyzer>
+    </fieldtype>
+    <fieldType name="charfilthtmlmap" class="solr.TextField">
+      <analyzer>
+        <charFilter class="solr.HTMLStripCharFilterFactory"/>
+        <tokenizer class="solr.MockTokenizerFactory"/>
+      </analyzer>
+    </fieldType>
+
+    <fieldtype name="subword" class="solr.TextField" multiValued="true" positionIncrementGap="100">
+      <analyzer type="index">
+        <tokenizer class="solr.MockTokenizerFactory"/>
+        <filter class="solr.WordDelimiterFilterFactory" generateWordParts="1" generateNumberParts="1" catenateWords="1" catenateNumbers="1" catenateAll="0"/>
+        <filter class="solr.LowerCaseFilterFactory"/>
+        <filter class="solr.StopFilterFactory"/>
+        <filter class="solr.PorterStemFilterFactory"/>
+      </analyzer>
+      <analyzer type="query">
+        <tokenizer class="solr.MockTokenizerFactory"/>
+        <filter class="solr.WordDelimiterFilterFactory" generateWordParts="1" generateNumberParts="1" catenateWords="0" catenateNumbers="0" catenateAll="0"/>
+        <filter class="solr.LowerCaseFilterFactory"/>
+        <filter class="solr.StopFilterFactory"/>
+        <filter class="solr.PorterStemFilterFactory"/>
+      </analyzer>
+    </fieldtype>
+
+    <fieldtype name="numericsubword" class="solr.TextField" multiValued="true" positionIncrementGap="100">
+      <analyzer type="index">
+        <tokenizer class="solr.MockTokenizerFactory"/>
+        <filter class="solr.LowerCaseFilterFactory"/>
+        <filter class="solr.WordDelimiterFilterFactory" splitOnNumerics="0" splitOnCaseChange="0" generateWordParts="1" generateNumberParts="0" catenateWords="0" catenateNumbers="0" catenateAll="0"/>
+        <filter class="solr.StopFilterFactory"/>
+        <filter class="solr.PorterStemFilterFactory"/>
+      </analyzer>
+      <analyzer type="query">
+        <tokenizer class="solr.MockTokenizerFactory"/>
+        <filter class="solr.LowerCaseFilterFactory"/>
+        <filter class="solr.WordDelimiterFilterFactory" splitOnNumerics="0" splitOnCaseChange="0" generateWordParts="1" generateNumberParts="1" catenateWords="1" catenateNumbers="1" catenateAll="0"/>
+        <filter class="solr.StopFilterFactory"/>
+        <filter class="solr.PorterStemFilterFactory"/>
+      </analyzer>
+    </fieldtype>
+
+    <fieldtype name="protectedsubword" class="solr.TextField" multiValued="true" positionIncrementGap="100">
+      <analyzer type="index">
+        <tokenizer class="solr.MockTokenizerFactory"/>
+        <filter class="solr.LowerCaseFilterFactory"/>
+        <filter class="solr.WordDelimiterFilterFactory"  splitOnNumerics="0" splitOnCaseChange="0" generateWordParts="1" generateNumberParts="1" catenateWords="0" catenateNumbers="0" catenateAll="0"/>
+      </analyzer>
+      <analyzer type="query">
+        <tokenizer class="solr.MockTokenizerFactory"/>
+        <filter class="solr.LowerCaseFilterFactory"/>
+      </analyzer>
+    </fieldtype>
+
+
+    <!-- more flexible in matching skus, but more chance of a false match -->
+    <fieldtype name="skutype1" class="solr.TextField">
+      <analyzer type="index">
+        <tokenizer class="solr.MockTokenizerFactory"/>
+        <filter class="solr.WordDelimiterFilterFactory" generateWordParts="1" generateNumberParts="1" catenateWords="1" catenateNumbers="1" catenateAll="0"/>
+        <filter class="solr.LowerCaseFilterFactory"/>
+      </analyzer>
+      <analyzer type="query">
+        <tokenizer class="solr.MockTokenizerFactory"/>
+        <filter class="solr.WordDelimiterFilterFactory" generateWordParts="0" generateNumberParts="0" catenateWords="1" catenateNumbers="1" catenateAll="0"/>
+        <filter class="solr.LowerCaseFilterFactory"/>
+      </analyzer>
+    </fieldtype>
+
+    <!-- less flexible in matching skus, but less chance of a false match -->
+    <fieldtype name="skutype2" class="solr.TextField">
+      <analyzer type="index">
+        <tokenizer class="solr.MockTokenizerFactory"/>
+        <filter class="solr.WordDelimiterFilterFactory" generateWordParts="0" generateNumberParts="0" catenateWords="1" catenateNumbers="1" catenateAll="0"/>
+        <filter class="solr.LowerCaseFilterFactory"/>
+      </analyzer>
+      <analyzer type="query">
+        <tokenizer class="solr.MockTokenizerFactory"/>
+        <filter class="solr.WordDelimiterFilterFactory" generateWordParts="0" generateNumberParts="0" catenateWords="1" catenateNumbers="1" catenateAll="0"/>
+        <filter class="solr.LowerCaseFilterFactory"/>
+      </analyzer>
+    </fieldtype>
+
+    <!-- less flexible in matching skus, but less chance of a false match -->
+    <fieldtype name="syn" class="solr.TextField">
+      <analyzer>
+        <tokenizer class="solr.MockTokenizerFactory"/>
+      </analyzer>
+    </fieldtype>
+
+
+    <fieldtype  name="unstored" class="solr.StrField" indexed="true" stored="false"/>
+
+
+    <fieldtype name="textgap" class="solr.TextField" multiValued="true" positionIncrementGap="100">
+      <analyzer>
+        <tokenizer class="solr.MockTokenizerFactory"/>
+        <filter class="solr.LowerCaseFilterFactory"/>
+      </analyzer>
+    </fieldtype>
+
+    <fieldType name="uuid" class="solr.UUIDField" />
+
+    <!-- Try out some point types -->
+    <fieldType name="xy" class="solr.PointType" dimension="2" subFieldType="double"/>
+    <fieldType name="x" class="solr.PointType" dimension="1" subFieldType="double"/>
+    <fieldType name="tenD" class="solr.PointType" dimension="10" subFieldType="double"/>
+    <!-- Use the sub field suffix -->
+    <fieldType name="xyd" class="solr.PointType" dimension="2" subFieldSuffix="_d1"/>
+    <fieldtype name="geohash" class="solr.GeoHashField"/>
+
+
+    <fieldType name="latLon" class="solr.LatLonType" subFieldType="double"/>
+
+    <!--  some per-field similarity examples -->
+
+    <!--  specify a Similarity classname directly -->
+    <!--
+    <fieldType name="sim1" class="solr.TextField">
+      <analyzer>
+        <tokenizer class="solr.MockTokenizerFactory"/>
+      </analyzer>
+      <similarity class="org.apache.lucene.misc.SweetSpotSimilarity"/>
+    </fieldType>
+    -->
+    <!--  specify a Similarity factory -->
+    <!--
+    <fieldType name="sim2" class="solr.TextField">
+      <analyzer>
+        <tokenizer class="solr.MockTokenizerFactory"/>
+      </analyzer>
+      <similarity class="org.apache.solr.search.similarities.CustomSimilarityFactory">
+        <str name="echo">is there an echo?</str>
+      </similarity>
+    </fieldType>
+    -->
+    <!-- don't specify any sim at all: get the default  -->
+    <!--
+    <fieldType name="sim3" class="solr.TextField">
+      <analyzer>
+        <tokenizer class="solr.MockTokenizerFactory"/>
+      </analyzer>
+    </fieldType>
+    -->
+  </types>
+
+
+  <fields>
+    <field name="id" type="int" indexed="true" stored="true" multiValued="false" required="false"/>
+    <field name="signatureField" type="string" indexed="true" stored="false"/>
+
+    <field name="s_multi" type="string" indexed="true" stored="true" docValues="true" multiValued="true"/>
+    <field name="i_multi" type="int" indexed="true" stored="true" docValues="true" multiValued="true"/>
+    <field name="f_multi" type="float" indexed="true" stored="true" docValues="true" multiValued="true"/>
+    <field name="l_multi" type="long" indexed="true" stored="true" docValues="true" multiValued="true"/>
+    <field name="d_multi" type="double" indexed="true" stored="true" docValues="true" multiValued="true"/>
+
+    <field name="uuid" type="uuid" stored="true" />
+    <field name="name" type="nametext" indexed="true" stored="true"/>
+    <field name="text" type="text" indexed="true" stored="false"/>
+    <field name="subject" type="text" indexed="true" stored="true"/>
+    <field name="title" type="nametext" indexed="true" stored="true"/>
+    <field name="weight" type="float" indexed="true" stored="true" multiValued="false"/>
+    <field name="bday" type="date" indexed="true" stored="true" multiValued="false"/>
+
+    <field name="title_stemmed" type="text" indexed="true" stored="false"/>
+    <field name="title_lettertok" type="lettertok" indexed="true" stored="false"/>
+
+    <field name="syn" type="syn" indexed="true" stored="true"/>
+
+    <!-- to test property inheritance and overriding -->
+    <field name="shouldbeunstored" type="unstored" />
+    <field name="shouldbestored" type="unstored" stored="true"/>
+    <field name="shouldbeunindexed" type="unstored" indexed="false" stored="true"/>
+
+    <!-- Test points -->
+    <!-- Test points -->
+    <field name="home" type="xy" indexed="true" stored="true" multiValued="false"/>
+    <field name="x" type="x" indexed="true" stored="true" multiValued="false"/>
+    <field name="homed" type="xyd" indexed="true" stored="true" multiValued="false"/>
+    <field name="home_ns" type="xy" indexed="true" stored="false" multiValued="false"/>
+    <field name="work" type="xy" indexed="true" stored="true" multiValued="false"/>
+
+    <field name="home_ll" type="latLon" indexed="true" stored="true" multiValued="false"/>
+    <field name="home_gh" type="geohash" indexed="true" stored="true" multiValued="false"/>
+
+
+    <field name="point10" type="tenD" indexed="true" stored="true" multiValued="false"/>
+
+
+    <!-- test different combinations of indexed and stored -->
+    <field name="bind" type="boolean" indexed="true" stored="false"/>
+    <field name="bsto" type="boolean" indexed="false" stored="true"/>
+    <field name="bindsto" type="boolean" indexed="true" stored="true"/>
+    <field name="isto" type="int" indexed="false" stored="true"/>
+    <field name="iind" type="int" indexed="true" stored="false"/>
+    <field name="ssto" type="string" indexed="false" stored="true"/>
+    <field name="sind" type="string" indexed="true" stored="false"/>
+    <field name="sindsto" type="string" indexed="true" stored="true"/>
+
+    <!-- test combinations of term vector settings -->
+    <field name="test_basictv" type="text" termVectors="true"/>
+    <field name="test_notv" type="text" termVectors="false"/>
+    <field name="test_postv" type="text" termVectors="true" termPositions="true"/>
+    <field name="test_offtv" type="text" termVectors="true" termOffsets="true"/>
+    <field name="test_posofftv" type="text" termVectors="true"
+           termPositions="true" termOffsets="true"/>
+
+    <!-- fields to test individual tokenizers and tokenfilters -->
+    <field name="teststop" type="teststop" indexed="true" stored="true"/>
+    <field name="lowertok" type="lowertok" indexed="true" stored="true"/>
+    <field name="keywordtok" type="keywordtok" indexed="true" stored="true"/>
+    <field name="standardtok" type="standardtok" indexed="true" stored="true"/>
+    <field name="HTMLstandardtok" type="HTMLstandardtok" indexed="true" stored="true"/>
+    <field name="lettertok" type="lettertok" indexed="true" stored="true"/>
+    <field name="whitetok" type="whitetok" indexed="true" stored="true"/>
+    <field name="HTMLwhitetok" type="HTMLwhitetok" indexed="true" stored="true"/>
+    <field name="standardtokfilt" type="standardtokfilt" indexed="true" stored="true"/>
+    <field name="standardfilt" type="standardfilt" indexed="true" stored="true"/>
+    <field name="lowerfilt" type="lowerfilt" indexed="true" stored="true"/>
+    <field name="lowerfilt1" type="lowerfilt" indexed="true" stored="true"/>
+    <field name="lowerfilt1and2" type="lowerfilt" indexed="true" stored="true"/>
+    <field name="patterntok" type="patterntok" indexed="true" stored="true"/>
+    <field name="patternreplacefilt" type="patternreplacefilt" indexed="true" stored="true"/>
+    <field name="porterfilt" type="porterfilt" indexed="true" stored="true"/>
+    <field name="engporterfilt" type="engporterfilt" indexed="true" stored="true"/>
+    <field name="custengporterfilt" type="custengporterfilt" indexed="true" stored="true"/>
+    <field name="stopfilt" type="stopfilt" indexed="true" stored="true"/>
+    <field name="custstopfilt" type="custstopfilt" indexed="true" stored="true"/>
+    <field name="lengthfilt" type="lengthfilt" indexed="true" stored="true"/>
+    <field name="wdf_nocase" type="wdf_nocase" indexed="true" stored="true"/>
+    <field name="wdf_preserve" type="wdf_preserve" indexed="true" stored="true"/>
+
+    <field name="numberpartfail" type="failtype1" indexed="true" stored="true"/>
+
+    <field name="nullfirst" type="string" indexed="true" stored="true" sortMissingFirst="true" multiValued="false"/>
+
+    <field name="subword" type="subword" indexed="true" stored="true"/>
+    <field name="subword_offsets" type="subword" indexed="true" stored="true" termOffsets="true"/>
+    <field name="numericsubword" type="numericsubword" indexed="true" stored="true"/>
+    <field name="protectedsubword" type="protectedsubword" indexed="true" stored="true"/>
+
+    <field name="sku1" type="skutype1" indexed="true" stored="true"/>
+    <field name="sku2" type="skutype2" indexed="true" stored="true"/>
+
+    <field name="textgap" type="textgap" indexed="true" stored="true"/>
+
+    <field name="timestamp" type="date" indexed="true" stored="true" default="NOW" multiValued="false"/>
+    <field name="multiDefault" type="string" indexed="true" stored="true" default="muLti-Default" multiValued="true"/>
+    <field name="intDefault" type="int" indexed="true" stored="true" default="42" multiValued="false"/>
+
+    <!--
+    <field name="sim1text" type="sim1" indexed="true" stored="true"/>
+    <field name="sim2text" type="sim2" indexed="true" stored="true"/>
+    <field name="sim3text" type="sim3" indexed="true" stored="true"/>
+    -->
+
+    <field name="tlong" type="tlong" indexed="true" stored="true" />
+
+    <field name="_version_" type="long" indexed="true" stored="true"/>
+
+    <!-- Dynamic field definitions.  If a field name is not found, dynamicFields
+         will be used if the name matches any of the patterns.
+         RESTRICTION: the glob-like pattern in the name attribute must have
+         a "*" only at the start or the end.
+         EXAMPLE:  name="*_i" will match any field ending in _i (like myid_i, z_i)
+         Longer patterns will be matched first.  if equal size patterns
+         both match, the first appearing in the schema will be used.
+    -->
+    <dynamicField name="*_i"  type="int"    indexed="true"  stored="true"/>
+    <dynamicField name="*_i1"  type="int"    indexed="true" stored="true" multiValued="false"/>
+
+    <dynamicField name="*_s"  type="string"  indexed="true"  stored="true"/>
+    <dynamicField name="*_ss" type="string"  indexed="true"  stored="true" multiValued="true"/>
+    <dynamicField name="*_s1"  type="string"  indexed="true"  stored="true" multiValued="false"/>
+    <dynamicField name="*_l"  type="long"   indexed="true"  stored="true"/>
+    <dynamicField name="*_l1"  type="long"   indexed="true"  stored="true" multiValued="false"/>
+    <dynamicField name="*_t"  type="text"    indexed="true"  stored="true"/>
+    <dynamicField name="*_b"  type="boolean" indexed="true"  stored="true"/>
+    <dynamicField name="*_f"  type="float"  indexed="true"  stored="true"/>
+    <dynamicField name="*_f1"  type="float"  indexed="true"  stored="true" multiValued="false"/>
+    <dynamicField name="*_d"  type="double" indexed="true"  stored="true"/>
+    <dynamicField name="*_d1"  type="double" indexed="true"  stored="true" multiValued="false"/>
+    <dynamicField name="*_dt" type="date"    indexed="true"  stored="true"/>
+    <dynamicField name="*_dt1" type="date"    indexed="true"  stored="true" multiValued="false"/>
+
+    <!-- some trie-coded dynamic fields for faster range queries -->
+    <dynamicField name="*_ti" type="tint"    indexed="true"  stored="true"/>
+    <dynamicField name="*_ti1" type="tint"    indexed="true"  stored="true" multiValued="false"/>
+    <dynamicField name="*_tl" type="tlong"   indexed="true"  stored="true"/>
+    <dynamicField name="*_tl1" type="tlong"   indexed="true"  stored="true" multiValued="false"/>
+    <dynamicField name="*_tf" type="tfloat"  indexed="true"  stored="true"/>
+    <dynamicField name="*_tf1" type="tfloat"  indexed="true"  stored="true" multiValued="false"/>
+    <dynamicField name="*_td" type="tdouble" indexed="true"  stored="true"/>
+    <dynamicField name="*_td1" type="tdouble" indexed="true" stored="true" multiValued="false"/>
+    <dynamicField name="*_tds" type="tdouble" indexed="true" stored="true" multiValued="false"/>
+    <dynamicField name="*_tdt" type="tdate"  indexed="true"  stored="true"/>
+    <dynamicField name="*_tdt1" type="tdate"  indexed="true"  stored="true" multiValued="false"/>
+
+
+
+
+    <dynamicField name="*_sI" type="string"  indexed="true"  stored="false"/>
+    <dynamicField name="*_sS" type="string"  indexed="false" stored="true"/>
+    <dynamicField name="t_*"  type="text"    indexed="true"  stored="true"/>
+    <dynamicField name="tv_*"  type="text" indexed="true"  stored="true"
+                  termVectors="true" termPositions="true" termOffsets="true"/>
+    <dynamicField name="tv_mv_*"  type="text" indexed="true"  stored="true" multiValued="true"
+                  termVectors="true" termPositions="true" termOffsets="true"/>
+
+    <dynamicField name="*_p"  type="xyd" indexed="true"  stored="true" multiValued="false"/>
+
+    <!-- special fields for dynamic copyField test -->
+    <dynamicField name="dynamic_*" type="string" indexed="true" stored="true"/>
+    <dynamicField name="*_dynamic" type="string" indexed="true" stored="true"/>
+
+    <!-- for testing to ensure that longer patterns are matched first -->
+    <dynamicField name="*aa"  type="string"  indexed="true" stored="true"/>
+
+    <!-- ignored becuase not stored or indexed -->
+    <dynamicField name="*_ignored" type="text" indexed="false" stored="false"/>
+
+    <dynamicField name="*_mfacet" type="string" indexed="true" stored="false" multiValued="true" />
+
+    <dynamicField name="random_*" type="random" />
+
+
+    <!-- make sure custom sims work with dynamic fields -->
+    <!--
+    <dynamicField name="*_sim1" type="sim1" indexed="true" stored="true"/>
+    <dynamicField name="*_sim2" type="sim2" indexed="true" stored="true"/>
+    <dynamicField name="*_sim3" type="sim3" indexed="true" stored="true"/>
+    -->
+  </fields>
+
+  <defaultSearchField>text</defaultSearchField>
+  <uniqueKey>id</uniqueKey>
+
+  <!-- copyField commands copy one field to another at the time a document
+        is added to the index.  It's used either to index the same field different
+        ways, or to add multiple fields to the same field for easier/faster searching.
+   -->
+  <copyField source="title" dest="title_stemmed"/>
+  <copyField source="title" dest="title_lettertok"/>
+
+  <copyField source="title" dest="text"/>
+  <copyField source="subject" dest="text"/>
+
+  <copyField source="lowerfilt1" dest="lowerfilt1and2"/>
+  <copyField source="lowerfilt" dest="lowerfilt1and2"/>
+
+  <copyField source="*_t" dest="text"/>
+
+
+
+
+
+  <!-- dynamic destination -->
+  <copyField source="*_dynamic" dest="dynamic_*"/>
+
+</schema>
diff --git a/solr/solrj/src/test-files/solrj/solr/configsets/streaming/conf/solrconfig.xml b/solr/solrj/src/test-files/solrj/solr/configsets/streaming/conf/solrconfig.xml
new file mode 100644
index 0000000..6b10869
--- /dev/null
+++ b/solr/solrj/src/test-files/solrj/solr/configsets/streaming/conf/solrconfig.xml
@@ -0,0 +1,51 @@
+<?xml version="1.0" encoding="UTF-8" ?>
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+
+<!--
+ This is a stripped down config file used for a simple example...
+ It is *not* a good example to work from.
+-->
+<config>
+  <luceneMatchVersion>${tests.luceneMatchVersion:LUCENE_CURRENT}</luceneMatchVersion>
+  <indexConfig>
+    <useCompoundFile>${useCompoundFile:false}</useCompoundFile>
+  </indexConfig>
+  <dataDir>${solr.data.dir:}</dataDir>
+  <directoryFactory name="DirectoryFactory" class="${solr.directoryFactory:solr.StandardDirectoryFactory}"/>
+  <schemaFactory class="ClassicIndexSchemaFactory"/>
+
+  <updateHandler class="solr.DirectUpdateHandler2">
+    <updateLog>
+      <str name="dir">${solr.data.dir:}</str>
+    </updateLog>
+  </updateHandler>
+
+
+  <requestDispatcher handleSelect="true" >
+    <requestParsers enableRemoteStreaming="false" multipartUploadLimitInKB="2048" />
+  </requestDispatcher>
+
+  <requestHandler name="standard" class="solr.StandardRequestHandler" default="true" />
+
+  <!-- config for the admin interface -->
+  <admin>
+    <defaultQuery>solr</defaultQuery>
+  </admin>
+
+</config>
+
diff --git a/solr/solrj/src/test/org/apache/solr/client/solrj/impl/CloudSolrClientTest.java b/solr/solrj/src/test/org/apache/solr/client/solrj/impl/CloudSolrClientTest.java
index 5864389..616ddc4 100644
--- a/solr/solrj/src/test/org/apache/solr/client/solrj/impl/CloudSolrClientTest.java
+++ b/solr/solrj/src/test/org/apache/solr/client/solrj/impl/CloudSolrClientTest.java
@@ -16,6 +16,19 @@
  */
 package org.apache.solr.client.solrj.impl;
 
+import java.io.IOException;
+import java.lang.invoke.MethodHandles;
+import java.net.URL;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collection;
+import java.util.HashSet;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.concurrent.TimeoutException;
+
 import com.google.common.collect.Lists;
 import com.google.common.collect.Maps;
 import com.google.common.collect.Sets;
@@ -26,12 +39,13 @@ import org.apache.solr.client.solrj.SolrClient;
 import org.apache.solr.client.solrj.SolrQuery;
 import org.apache.solr.client.solrj.SolrServerException;
 import org.apache.solr.client.solrj.request.AbstractUpdateRequest;
+import org.apache.solr.client.solrj.request.CollectionAdminRequest;
 import org.apache.solr.client.solrj.request.QueryRequest;
 import org.apache.solr.client.solrj.request.UpdateRequest;
 import org.apache.solr.client.solrj.response.QueryResponse;
 import org.apache.solr.client.solrj.response.UpdateResponse;
-import org.apache.solr.cloud.AbstractFullDistribZkTestBase;
-import org.apache.solr.cloud.AbstractZkTestCase;
+import org.apache.solr.cloud.AbstractDistribZkTestBase;
+import org.apache.solr.cloud.SolrCloudTestCase;
 import org.apache.solr.common.SolrDocumentList;
 import org.apache.solr.common.SolrException;
 import org.apache.solr.common.SolrInputDocument;
@@ -46,6 +60,7 @@ import org.apache.solr.common.params.ModifiableSolrParams;
 import org.apache.solr.common.params.ShardParams;
 import org.apache.solr.common.util.NamedList;
 import org.apache.solr.common.util.SimpleOrderedMap;
+import org.junit.Before;
 import org.junit.BeforeClass;
 import org.junit.Rule;
 import org.junit.Test;
@@ -53,163 +68,87 @@ import org.junit.rules.ExpectedException;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-import java.io.File;
-import java.io.IOException;
-import java.lang.invoke.MethodHandles;
-import java.net.URL;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Collection;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map;
-import java.util.Set;
-import java.util.concurrent.TimeoutException;
-
-import static org.apache.solr.cloud.OverseerCollectionMessageHandler.NUM_SLICES;
-import static org.apache.solr.common.util.Utils.makeMap;
-import static org.apache.solr.common.cloud.ZkStateReader.MAX_SHARDS_PER_NODE;
-import static org.apache.solr.common.cloud.ZkStateReader.REPLICATION_FACTOR;
-
 
 /**
  * This test would be faster if we simulated the zk state instead.
  */
 @Slow
-public class CloudSolrClientTest extends AbstractFullDistribZkTestBase {
-  private static final Logger log = LoggerFactory.getLogger(MethodHandles.lookup().lookupClass());
+public class CloudSolrClientTest extends SolrCloudTestCase {
+
+  private static final String COLLECTION = "collection1";
+
+  private static final String id = "id";
 
-  private static final String SOLR_HOME = getFile("solrj" + File.separator + "solr").getAbsolutePath();
+  private static final int TIMEOUT = 30;
 
   @BeforeClass
-  public static void beforeSuperClass() {
-    // this is necessary because AbstractZkTestCase.buildZooKeeper is used by AbstractDistribZkTestBase
-    // and the auto-detected SOLRHOME=TEST_HOME() does not exist for solrj tests
-    // todo fix this
-    AbstractZkTestCase.SOLRHOME = new File(SOLR_HOME());
+  public static void setupCluster() throws Exception {
+    configureCluster(3)
+        .addConfig("conf", getFile("solrj").toPath().resolve("solr").resolve("configsets").resolve("streaming").resolve("conf"))
+        .configure();
+
+    CollectionAdminRequest.createCollection(COLLECTION, "conf", 2, 1).process(cluster.getSolrClient());
+    AbstractDistribZkTestBase.waitForRecoveriesToFinish(COLLECTION, cluster.getSolrClient().getZkStateReader(),
+        false, true, TIMEOUT);
   }
 
-  protected String getCloudSolrConfig() {
-    return "solrconfig.xml";
-  }
-  
-  @Override
-  public String getSolrHome() {
-    return SOLR_HOME;
-  }
-  
-  public static String SOLR_HOME() {
-    return SOLR_HOME;
-  }
-  
-  public CloudSolrClientTest() {
-    super();
-    sliceCount = 2;
-    fixShardCount(3);
-  }
+  private static final Logger log = LoggerFactory.getLogger(MethodHandles.lookup().lookupClass());
 
-  @Test
-  public void test() throws Exception {
-    testParallelUpdateQTime();
-    checkCollectionParameters();
-    allTests();
-    stateVersionParamTest();
-    customHttpClientTest();
-    testOverwriteOption();
-    preferLocalShardsTest();
+  @Before
+  public void cleanIndex() throws Exception {
+    new UpdateRequest()
+        .deleteByQuery("*:*")
+        .commit(cluster.getSolrClient(), COLLECTION);
   }
 
-  private void testParallelUpdateQTime() throws Exception {
+  @Test
+  public void testParallelUpdateQTime() throws Exception {
     UpdateRequest req = new UpdateRequest();
     for (int i=0; i<10; i++)  {
       SolrInputDocument doc = new SolrInputDocument();
       doc.addField("id", String.valueOf(TestUtil.nextInt(random(), 1000, 1100)));
       req.add(doc);
     }
-    UpdateResponse response = req.process(cloudClient);
+    UpdateResponse response = req.process(cluster.getSolrClient(), COLLECTION);
     // See SOLR-6547, we just need to ensure that no exception is thrown here
     assertTrue(response.getQTime() >= 0);
   }
 
-  private void testOverwriteOption() throws Exception, SolrServerException,
-      IOException {
-    String collectionName = "overwriteCollection";
-    createCollection(collectionName, controlClientCloud, 1, 1);
-    waitForRecoveriesToFinish(collectionName, false);
-    try (CloudSolrClient cloudClient = createCloudClient(collectionName)) {
-      SolrInputDocument doc1 = new SolrInputDocument();
-      doc1.addField(id, "0");
-      doc1.addField("a_t", "hello1");
-      SolrInputDocument doc2 = new SolrInputDocument();
-      doc2.addField(id, "0");
-      doc2.addField("a_t", "hello2");
-      
-      UpdateRequest request = new UpdateRequest();
-      request.add(doc1);
-      request.add(doc2);
-      request.setAction(AbstractUpdateRequest.ACTION.COMMIT, false, false);
-      NamedList<Object> response = cloudClient.request(request);
-      QueryResponse resp = cloudClient.query(new SolrQuery("*:*"));
-      
-      assertEquals("There should be one document because overwrite=true", 1, resp.getResults().getNumFound());
-      
-      doc1 = new SolrInputDocument();
-      doc1.addField(id, "1");
-      doc1.addField("a_t", "hello1");
-      doc2 = new SolrInputDocument();
-      doc2.addField(id, "1");
-      doc2.addField("a_t", "hello2");
-      
-      request = new UpdateRequest();
-      // overwrite=false
-      request.add(doc1, false);
-      request.add(doc2, false);
-      request.setAction(AbstractUpdateRequest.ACTION.COMMIT, false, false);
-      response = cloudClient.request(request);
-      
-      resp = cloudClient.query(new SolrQuery("*:*"));
-
-      assertEquals("There should be 3 documents because there should be two id=1 docs due to overwrite=false", 3, resp.getResults().getNumFound());
-    }
-  }
-
-  private void allTests() throws Exception {
+  @Test
+  public void testOverwriteOption() throws Exception {
 
-    String collectionName = "clientTestExternColl";
-    createCollection(collectionName, controlClientCloud, 2, 2);
-    waitForRecoveriesToFinish(collectionName, false);
-    CloudSolrClient cloudClient = createCloudClient(collectionName);
+    CollectionAdminRequest.createCollection("overwrite", "conf", 1, 1)
+        .processAndWait(cluster.getSolrClient(), TIMEOUT);
+    AbstractDistribZkTestBase.waitForRecoveriesToFinish("overwrite", cluster.getSolrClient().getZkStateReader(), false, true, TIMEOUT);
 
-    assertNotNull(cloudClient);
-    
-    handle.clear();
-    handle.put("timestamp", SKIPVAL);
-    
-    waitForThingsToLevelOut(30);
+    new UpdateRequest()
+        .add("id", "0", "a_t", "hello1")
+        .add("id", "0", "a_t", "hello2")
+        .commit(cluster.getSolrClient(), "overwrite");
 
-    controlClient.deleteByQuery("*:*");
-    cloudClient.deleteByQuery("*:*");
+    QueryResponse resp = cluster.getSolrClient().query("overwrite", new SolrQuery("*:*"));
+    assertEquals("There should be one document because overwrite=true", 1, resp.getResults().getNumFound());
 
+    new UpdateRequest()
+        .add(new SolrInputDocument(id, "1", "a_t", "hello1"), /* overwrite = */ false)
+        .add(new SolrInputDocument(id, "1", "a_t", "hello2"), false)
+        .commit(cluster.getSolrClient(), "overwrite");
+      
+    resp = cluster.getSolrClient().query("overwrite", new SolrQuery("*:*"));
+    assertEquals("There should be 3 documents because there should be two id=1 docs due to overwrite=false", 3, resp.getResults().getNumFound());
 
-    controlClient.commit();
-    this.cloudClient.commit();
+  }
 
-    SolrInputDocument doc1 = new SolrInputDocument();
-    doc1.addField(id, "0");
-    doc1.addField("a_t", "hello1");
-    SolrInputDocument doc2 = new SolrInputDocument();
-    doc2.addField(id, "2");
-    doc2.addField("a_t", "hello2");
+  @Test
+  public void testRouting() throws Exception {
     
-    UpdateRequest request = new UpdateRequest();
-    request.add(doc1);
-    request.add(doc2);
-    request.setAction(AbstractUpdateRequest.ACTION.COMMIT, false, false);
+    AbstractUpdateRequest request = new UpdateRequest()
+        .add(id, "0", "a_t", "hello1")
+        .add(id, "2", "a_t", "hello2")
+        .setAction(AbstractUpdateRequest.ACTION.COMMIT, true, true);
     
     // Test single threaded routed updates for UpdateRequest
-    NamedList<Object> response = cloudClient.request(request);
+    NamedList<Object> response = cluster.getSolrClient().request(request, COLLECTION);
     CloudSolrClient.RouteResponse rr = (CloudSolrClient.RouteResponse) response;
     Map<String,LBHttpSolrClient.Req> routes = rr.getRoutes();
     Iterator<Map.Entry<String,LBHttpSolrClient.Req>> it = routes.entrySet()
@@ -234,22 +173,19 @@ public class CloudSolrClientTest extends AbstractFullDistribZkTestBase {
     
     // Test the deleteById routing for UpdateRequest
     
-    UpdateRequest delRequest = new UpdateRequest();
-    delRequest.deleteById("0");
-    delRequest.deleteById("2");
-    delRequest.setAction(AbstractUpdateRequest.ACTION.COMMIT, false, false);
-    cloudClient.request(delRequest);
-    ModifiableSolrParams qParams = new ModifiableSolrParams();
-    qParams.add("q", "*:*");
-    QueryRequest qRequest = new QueryRequest(qParams);
-    QueryResponse qResponse = qRequest.process(cloudClient);
+    new UpdateRequest()
+        .deleteById("0")
+        .deleteById("2")
+        .commit(cluster.getSolrClient(), COLLECTION);
+
+    QueryResponse qResponse = cluster.getSolrClient().query(COLLECTION, new SolrQuery("*:*"));
     SolrDocumentList docs = qResponse.getResults();
-    assertTrue(docs.getNumFound() == 0);
+    assertEquals(0, docs.getNumFound());
     
     // Test Multi-Threaded routed updates for UpdateRequest
-    try (CloudSolrClient threadedClient = getCloudSolrClient(zkServer.getZkAddress())) {
+    try (CloudSolrClient threadedClient = getCloudSolrClient(cluster.getZkServer().getZkAddress())) {
       threadedClient.setParallelUpdates(true);
-      threadedClient.setDefaultCollection(collectionName);
+      threadedClient.setDefaultCollection(COLLECTION);
       response = threadedClient.request(request);
       rr = (CloudSolrClient.RouteResponse) response;
       routes = rr.getRoutes();
@@ -277,13 +213,13 @@ public class CloudSolrClientTest extends AbstractFullDistribZkTestBase {
     // Test that queries with _route_ params are routed by the client
 
     // Track request counts on each node before query calls
-    ClusterState clusterState = cloudClient.getZkStateReader().getClusterState();
-    DocCollection col = clusterState.getCollection(collectionName);
+    ClusterState clusterState = cluster.getSolrClient().getZkStateReader().getClusterState();
+    DocCollection col = clusterState.getCollection(COLLECTION);
     Map<String, Long> requestCountsMap = Maps.newHashMap();
     for (Slice slice : col.getSlices()) {
       for (Replica replica : slice.getReplicas()) {
         String baseURL = (String) replica.get(ZkStateReader.BASE_URL_PROP);
-        requestCountsMap.put(baseURL, getNumRequests(baseURL,collectionName));
+        requestCountsMap.put(baseURL, getNumRequests(baseURL, COLLECTION));
       }
     }
 
@@ -328,7 +264,7 @@ public class CloudSolrClientTest extends AbstractFullDistribZkTestBase {
       ModifiableSolrParams solrParams = new ModifiableSolrParams();
       solrParams.set(CommonParams.Q, "*:*");
       solrParams.set(ShardParams._ROUTE_, sameShardRoutes.get(random().nextInt(sameShardRoutes.size())));
-      log.info("output  : {}" ,cloudClient.query(solrParams));
+      log.info("output: {}", cluster.getSolrClient().query(COLLECTION, solrParams));
     }
 
     // Request counts increase from expected nodes should aggregate to 1000, while there should be
@@ -341,7 +277,7 @@ public class CloudSolrClientTest extends AbstractFullDistribZkTestBase {
         String baseURL = (String) replica.get(ZkStateReader.BASE_URL_PROP);
 
         Long prevNumRequests = requestCountsMap.get(baseURL);
-        Long curNumRequests = getNumRequests(baseURL, collectionName);
+        Long curNumRequests = getNumRequests(baseURL, COLLECTION);
 
         long delta = curNumRequests - prevNumRequests;
         if (expectedBaseURLs.contains(baseURL)) {
@@ -357,74 +293,36 @@ public class CloudSolrClientTest extends AbstractFullDistribZkTestBase {
     assertEquals("Unexpected number of requests to unexpected URLs: " + numRequestsToUnexpectedUrls,
         0, increaseFromUnexpectedUrls);
 
-    controlClient.deleteByQuery("*:*");
-    cloudClient.deleteByQuery("*:*");
-
-    controlClient.commit();
-    cloudClient.commit();
-    cloudClient.close();
   }
 
   /**
    * Tests if the specification of 'preferLocalShards' in the query-params
    * limits the distributed query to locally hosted shards only
    */
-  private void preferLocalShardsTest() throws Exception {
+  @Test
+  public void preferLocalShardsTest() throws Exception {
 
     String collectionName = "localShardsTestColl";
 
-    int liveNodes = getCommonCloudSolrClient()
-        .getZkStateReader().getClusterState().getLiveNodes().size();
+    int liveNodes = cluster.getJettySolrRunners().size();
 
     // For preferLocalShards to succeed in a test, every shard should have
     // all its cores on the same node.
     // Hence the below configuration for our collection
-    Map<String, Object> props = makeMap(
-        REPLICATION_FACTOR, liveNodes,
-        MAX_SHARDS_PER_NODE, liveNodes,
-        NUM_SLICES, liveNodes);
-    Map<String,List<Integer>> collectionInfos = new HashMap<String,List<Integer>>();
-    createCollection(collectionInfos, collectionName, props, controlClientCloud);
-    waitForRecoveriesToFinish(collectionName, false);
-
-    CloudSolrClient cloudClient = createCloudClient(collectionName);
-    assertNotNull(cloudClient);
-    handle.clear();
-    handle.put("timestamp", SKIPVAL);
-    waitForThingsToLevelOut(30);
-
-    // Remove any documents from previous test (if any)
-    controlClient.deleteByQuery("*:*");
-    cloudClient.deleteByQuery("*:*");
-    controlClient.commit();
-    cloudClient.commit();
+    CollectionAdminRequest.createCollection(collectionName, "conf", liveNodes, liveNodes)
+        .setMaxShardsPerNode(liveNodes)
+        .processAndWait(cluster.getSolrClient(), TIMEOUT);
+    AbstractDistribZkTestBase.waitForRecoveriesToFinish(collectionName, cluster.getSolrClient().getZkStateReader(), false, true, TIMEOUT);
 
     // Add some new documents
-    SolrInputDocument doc1 = new SolrInputDocument();
-    doc1.addField(id, "0");
-    doc1.addField("a_t", "hello1");
-    SolrInputDocument doc2 = new SolrInputDocument();
-    doc2.addField(id, "2");
-    doc2.addField("a_t", "hello2");
-    SolrInputDocument doc3 = new SolrInputDocument();
-    doc3.addField(id, "3");
-    doc3.addField("a_t", "hello2");
-
-    UpdateRequest request = new UpdateRequest();
-    request.add(doc1);
-    request.add(doc2);
-    request.add(doc3);
-    request.setAction(AbstractUpdateRequest.ACTION.COMMIT, false, false);
+    new UpdateRequest()
+        .add(id, "0", "a_t", "hello1")
+        .add(id, "2", "a_t", "hello2")
+        .add(id, "3", "a_t", "hello2")
+        .commit(cluster.getSolrClient(), collectionName);
 
     // Run the actual test for 'preferLocalShards'
-    queryWithPreferLocalShards(cloudClient, true, collectionName);
-
-    // Cleanup
-    controlClient.deleteByQuery("*:*");
-    cloudClient.deleteByQuery("*:*");
-    controlClient.commit();
-    cloudClient.commit();
-    cloudClient.close();
+    queryWithPreferLocalShards(cluster.getSolrClient(), true, collectionName);
   }
 
   private void queryWithPreferLocalShards(CloudSolrClient cloudClient,
@@ -432,8 +330,7 @@ public class CloudSolrClientTest extends AbstractFullDistribZkTestBase {
                                           String collectionName)
       throws Exception
   {
-    SolrQuery qRequest = new SolrQuery();
-    qRequest.setQuery("*:*");
+    SolrQuery qRequest = new SolrQuery("*:*");
 
     ModifiableSolrParams qParams = new ModifiableSolrParams();
     qParams.add("preferLocalShards", Boolean.toString(preferLocalShards));
@@ -444,7 +341,7 @@ public class CloudSolrClientTest extends AbstractFullDistribZkTestBase {
     // And since all the nodes are hosting cores from all shards, the
     // distributed query formed by this node will select cores from the
     // local shards only
-    QueryResponse qResponse = cloudClient.query (qRequest);
+    QueryResponse qResponse = cloudClient.query(collectionName, qRequest);
 
     Object shardsInfo = qResponse.getResponse().get(ShardParams.SHARDS_INFO);
     assertNotNull("Unable to obtain "+ShardParams.SHARDS_INFO, shardsInfo);
@@ -495,21 +392,23 @@ public class CloudSolrClientTest extends AbstractFullDistribZkTestBase {
     return (Long) resp.findRecursive("solr-mbeans", "QUERYHANDLER",
         "standard", "stats", "requests");
   }
-  
-  @Override
-  protected void indexr(Object... fields) throws Exception {
-    SolrInputDocument doc = getDoc(fields);
-    indexDoc(doc);
-  }
 
-  private void checkCollectionParameters() throws Exception {
+  @Test
+  public void checkCollectionParameters() throws Exception {
+
+    try (CloudSolrClient client = getCloudSolrClient(cluster.getZkServer().getZkAddress())) {
 
-    try (CloudSolrClient client = createCloudClient("multicollection1")) {
+      String async1 = CollectionAdminRequest.createCollection("multicollection1", "conf", 2, 1)
+          .processAsync(client);
+      String async2 = CollectionAdminRequest.createCollection("multicollection2", "conf", 2, 1)
+          .processAsync(client);
 
-      createCollection("multicollection1", client, 2, 2);
-      createCollection("multicollection2", client, 2, 2);
-      waitForRecoveriesToFinish("multicollection1", false);
-      waitForRecoveriesToFinish("multicollection2", false);
+      CollectionAdminRequest.waitForAsyncRequest(async1, client, TIMEOUT);
+      CollectionAdminRequest.waitForAsyncRequest(async2, client, TIMEOUT);
+      AbstractDistribZkTestBase.waitForRecoveriesToFinish("multicollection1", client.getZkStateReader(), false, true, TIMEOUT);
+      AbstractDistribZkTestBase.waitForRecoveriesToFinish("multicollection2", client.getZkStateReader(), false, true, TIMEOUT);
+
+      client.setDefaultCollection("multicollection1");
 
       List<SolrInputDocument> docs = new ArrayList<>(3);
       for (int i = 0; i < 3; i++) {
@@ -540,73 +439,70 @@ public class CloudSolrClientTest extends AbstractFullDistribZkTestBase {
 
   }
 
-  private void stateVersionParamTest() throws Exception {
+  @Test
+  public void stateVersionParamTest() throws Exception {
 
-    try (CloudSolrClient client = createCloudClient(null)) {
-      String collectionName = "checkStateVerCol";
-      createCollection(collectionName, client, 1, 3);
-      waitForRecoveriesToFinish(collectionName, false);
-      DocCollection coll = client.getZkStateReader().getClusterState().getCollection(collectionName);
-      Replica r = coll.getSlices().iterator().next().getReplicas().iterator().next();
+    DocCollection coll = cluster.getSolrClient().getZkStateReader().getClusterState().getCollection(COLLECTION);
+    Replica r = coll.getSlices().iterator().next().getReplicas().iterator().next();
 
-      SolrQuery q = new SolrQuery().setQuery("*:*");
-      HttpSolrClient.RemoteSolrException sse = null;
+    SolrQuery q = new SolrQuery().setQuery("*:*");
+    HttpSolrClient.RemoteSolrException sse = null;
 
-      final String url = r.getStr(ZkStateReader.BASE_URL_PROP) + "/" +collectionName;
-      try (HttpSolrClient solrClient = getHttpSolrClient(url)) {
+    final String url = r.getStr(ZkStateReader.BASE_URL_PROP) + "/" + COLLECTION;
+    try (HttpSolrClient solrClient = getHttpSolrClient(url)) {
 
-        log.info("should work query, result {}", solrClient.query(q));
-        //no problem
-        q.setParam(CloudSolrClient.STATE_VERSION, collectionName + ":" + coll.getZNodeVersion());
-        log.info("2nd query , result {}", solrClient.query(q));
-        //no error yet good
+      log.info("should work query, result {}", solrClient.query(q));
+      //no problem
+      q.setParam(CloudSolrClient.STATE_VERSION, COLLECTION + ":" + coll.getZNodeVersion());
+      log.info("2nd query , result {}", solrClient.query(q));
+      //no error yet good
 
-        q.setParam(CloudSolrClient.STATE_VERSION, collectionName + ":" + (coll.getZNodeVersion() - 1)); //an older version expect error
+      q.setParam(CloudSolrClient.STATE_VERSION, COLLECTION + ":" + (coll.getZNodeVersion() - 1)); //an older version expect error
 
-        QueryResponse rsp = solrClient.query(q);
-        Map m = (Map) rsp.getResponse().get(CloudSolrClient.STATE_VERSION, rsp.getResponse().size()-1);
-        assertNotNull("Expected an extra information from server with the list of invalid collection states", m);
-        assertNotNull(m.get(collectionName));
-      }
+      QueryResponse rsp = solrClient.query(q);
+      Map m = (Map) rsp.getResponse().get(CloudSolrClient.STATE_VERSION, rsp.getResponse().size()-1);
+      assertNotNull("Expected an extra information from server with the list of invalid collection states", m);
+      assertNotNull(m.get(COLLECTION));
+    }
 
-      //now send the request to another node that does not serve the collection
+    //now send the request to another node that does not serve the collection
 
-      Set<String> allNodesOfColl = new HashSet<>();
-      for (Slice slice : coll.getSlices()) {
-        for (Replica replica : slice.getReplicas()) {
-          allNodesOfColl.add(replica.getStr(ZkStateReader.BASE_URL_PROP));
-        }
+    Set<String> allNodesOfColl = new HashSet<>();
+    for (Slice slice : coll.getSlices()) {
+      for (Replica replica : slice.getReplicas()) {
+        allNodesOfColl.add(replica.getStr(ZkStateReader.BASE_URL_PROP));
       }
-      String theNode = null;
-      Set<String> liveNodes = client.getZkStateReader().getClusterState().getLiveNodes();
-      for (String s : liveNodes) {
-        String n = client.getZkStateReader().getBaseUrlForNodeName(s);
-        if(!allNodesOfColl.contains(n)){
-          theNode = n;
-          break;
-        }
+    }
+    String theNode = null;
+    Set<String> liveNodes = cluster.getSolrClient().getZkStateReader().getClusterState().getLiveNodes();
+    for (String s : liveNodes) {
+      String n = cluster.getSolrClient().getZkStateReader().getBaseUrlForNodeName(s);
+      if(!allNodesOfColl.contains(n)){
+        theNode = n;
+        break;
       }
-      log.info("the node which does not serve this collection{} ",theNode);
-      assertNotNull(theNode);
+    }
+    log.info("the node which does not serve this collection{} ",theNode);
+    assertNotNull(theNode);
 
-      
-      final String solrClientUrl = theNode + "/" + collectionName;
-      try (SolrClient solrClient = getHttpSolrClient(solrClientUrl)) {
-
-        q.setParam(CloudSolrClient.STATE_VERSION, collectionName + ":" + (coll.getZNodeVersion()-1));
-        try {
-          QueryResponse rsp = solrClient.query(q);
-          log.info("error was expected");
-        } catch (HttpSolrClient.RemoteSolrException e) {
-          sse = e;
-        }
-        assertNotNull(sse);
-        assertEquals(" Error code should be 510", SolrException.ErrorCode.INVALID_STATE.code, sse.code());
+
+    final String solrClientUrl = theNode + "/" + COLLECTION;
+    try (SolrClient solrClient = getHttpSolrClient(solrClientUrl)) {
+
+      q.setParam(CloudSolrClient.STATE_VERSION, COLLECTION + ":" + (coll.getZNodeVersion()-1));
+      try {
+        QueryResponse rsp = solrClient.query(q);
+        log.info("error was expected");
+      } catch (HttpSolrClient.RemoteSolrException e) {
+        sse = e;
       }
+      assertNotNull(sse);
+      assertEquals(" Error code should be 510", SolrException.ErrorCode.INVALID_STATE.code, sse.code());
     }
 
   }
 
+  @Test
   public void testShutdown() throws IOException {
     try (CloudSolrClient client = getCloudSolrClient("[ff01::114]:33332")) {
       client.setZkConnectTimeout(100);
@@ -620,22 +516,23 @@ public class CloudSolrClientTest extends AbstractFullDistribZkTestBase {
   @Rule
   public ExpectedException exception = ExpectedException.none();
 
+  @Test
   public void testWrongZkChrootTest() throws IOException {
 
     exception.expect(SolrException.class);
     exception.expectMessage("cluster not found/not ready");
 
-    try (CloudSolrClient client = getCloudSolrClient(zkServer.getZkAddress() + "/xyz/foo")) {
-      client.setDefaultCollection(DEFAULT_COLLECTION);
+    try (CloudSolrClient client = getCloudSolrClient(cluster.getZkServer().getZkAddress() + "/xyz/foo")) {
       client.setZkClientTimeout(1000 * 60);
       client.connect();
       fail("Expected exception");
     }
   }
 
+  @Test
   public void customHttpClientTest() throws IOException {
     CloseableHttpClient client = HttpClientUtil.createClient(null);
-    try (CloudSolrClient solrClient = getCloudSolrClient(zkServer.getZkAddress(), client)) {
+    try (CloudSolrClient solrClient = getCloudSolrClient(cluster.getZkServer().getZkAddress(), client)) {
 
       assertTrue(solrClient.getLbClient().getHttpClient() == client);
 
diff --git a/solr/solrj/src/test/org/apache/solr/client/solrj/io/graph/GraphExpressionTest.java b/solr/solrj/src/test/org/apache/solr/client/solrj/io/graph/GraphExpressionTest.java
index 7c1f97d..53f7126 100644
--- a/solr/solrj/src/test/org/apache/solr/client/solrj/io/graph/GraphExpressionTest.java
+++ b/solr/solrj/src/test/org/apache/solr/client/solrj/io/graph/GraphExpressionTest.java
@@ -17,14 +17,12 @@ package org.apache.solr.client.solrj.io.graph;
  * limitations under the License.
  */
 
-import java.io.File;
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Collections;
 import java.util.HashMap;
 import java.util.HashSet;
 import java.util.List;
-import java.util.Locale;
 import java.util.Map;
 import java.util.Set;
 
@@ -34,18 +32,20 @@ import org.apache.solr.client.solrj.io.SolrClientCache;
 import org.apache.solr.client.solrj.io.Tuple;
 import org.apache.solr.client.solrj.io.comp.ComparatorOrder;
 import org.apache.solr.client.solrj.io.comp.FieldComparator;
-import org.apache.solr.client.solrj.io.stream.*;
+import org.apache.solr.client.solrj.io.stream.CloudSolrStream;
+import org.apache.solr.client.solrj.io.stream.HashJoinStream;
+import org.apache.solr.client.solrj.io.stream.StreamContext;
+import org.apache.solr.client.solrj.io.stream.TupleStream;
 import org.apache.solr.client.solrj.io.stream.expr.StreamFactory;
 import org.apache.solr.client.solrj.io.stream.metrics.CountMetric;
 import org.apache.solr.client.solrj.io.stream.metrics.MaxMetric;
 import org.apache.solr.client.solrj.io.stream.metrics.MeanMetric;
 import org.apache.solr.client.solrj.io.stream.metrics.MinMetric;
 import org.apache.solr.client.solrj.io.stream.metrics.SumMetric;
-import org.apache.solr.cloud.AbstractFullDistribZkTestBase;
-import org.apache.solr.cloud.AbstractZkTestCase;
-import org.apache.solr.common.SolrInputDocument;
-import org.junit.After;
-import org.junit.AfterClass;
+import org.apache.solr.client.solrj.request.CollectionAdminRequest;
+import org.apache.solr.client.solrj.request.UpdateRequest;
+import org.apache.solr.cloud.AbstractDistribZkTestBase;
+import org.apache.solr.cloud.SolrCloudTestCase;
 import org.junit.Before;
 import org.junit.BeforeClass;
 import org.junit.Test;
@@ -58,96 +58,52 @@ import org.junit.Test;
 
 @Slow
 @LuceneTestCase.SuppressCodecs({"Lucene3x", "Lucene40","Lucene41","Lucene42","Lucene45"})
-public class GraphExpressionTest extends AbstractFullDistribZkTestBase {
+public class GraphExpressionTest extends SolrCloudTestCase {
 
-  private static final String SOLR_HOME = getFile("solrj" + File.separator + "solr").getAbsolutePath();
+  private static final String COLLECTION = "collection1";
 
-  static {
-    schemaString = "schema-streaming.xml";
-  }
-
-  @BeforeClass
-  public static void beforeSuperClass() {
-    AbstractZkTestCase.SOLRHOME = new File(SOLR_HOME());
-  }
-
-  @AfterClass
-  public static void afterSuperClass() {
+  private static final String id = "id";
 
-  }
-
-  protected String getCloudSolrConfig() {
-    return "solrconfig-streaming.xml";
-  }
-
-
-  @Override
-  public String getSolrHome() {
-    return SOLR_HOME;
-  }
+  private static final int TIMEOUT = 30;
 
-  public static String SOLR_HOME() {
-    return SOLR_HOME;
+  @BeforeClass
+  public static void setupCluster() throws Exception {
+    configureCluster(2)
+        .addConfig("conf", getFile("solrj").toPath().resolve("solr").resolve("configsets").resolve("streaming").resolve("conf"))
+        .configure();
+
+    CollectionAdminRequest.createCollection(COLLECTION, "conf", 2, 1).process(cluster.getSolrClient());
+    AbstractDistribZkTestBase.waitForRecoveriesToFinish(COLLECTION, cluster.getSolrClient().getZkStateReader(),
+        false, true, TIMEOUT);
   }
 
   @Before
-  @Override
-  public void setUp() throws Exception {
-    super.setUp();
-    // we expect this time of exception as shards go up and down...
-    //ignoreException(".*");
-
-    System.setProperty("numShards", Integer.toString(sliceCount));
-  }
-
-  @Override
-  @After
-  public void tearDown() throws Exception {
-    super.tearDown();
-    resetExceptionIgnores();
-  }
-
-  public GraphExpressionTest() {
-    super();
-    sliceCount = 2;
+  public void cleanIndex() throws Exception {
+    new UpdateRequest()
+        .deleteByQuery("*:*")
+        .commit(cluster.getSolrClient(), COLLECTION);
   }
 
   @Test
-  public void testAll() throws Exception{
-    assertNotNull(cloudClient);
-
-    handle.clear();
-    handle.put("timestamp", SKIPVAL);
-
-    waitForRecoveriesToFinish(false);
-
-    del("*:*");
-    commit();
-
-    testShortestPathStream();
-    testGatherNodesStream();
-    testGatherNodesFriendsStream();
-  }
-
-  private void testShortestPathStream() throws Exception {
-
-    indexr(id, "0", "from_s", "jim", "to_s", "mike", "predicate_s", "knows");
-    indexr(id, "1", "from_s", "jim", "to_s", "dave", "predicate_s", "knows");
-    indexr(id, "2", "from_s", "jim", "to_s", "stan", "predicate_s", "knows");
-    indexr(id, "3", "from_s", "dave", "to_s", "stan", "predicate_s", "knows");
-    indexr(id, "4", "from_s", "dave", "to_s", "bill", "predicate_s", "knows");
-    indexr(id, "5", "from_s", "dave", "to_s", "mike", "predicate_s", "knows");
-    indexr(id, "20", "from_s", "dave", "to_s", "alex", "predicate_s", "knows");
-    indexr(id, "21", "from_s", "alex", "to_s", "steve", "predicate_s", "knows");
-    indexr(id, "6", "from_s", "stan", "to_s", "alice", "predicate_s", "knows");
-    indexr(id, "7", "from_s", "stan", "to_s", "mary", "predicate_s", "knows");
-    indexr(id, "8", "from_s", "stan", "to_s", "dave", "predicate_s", "knows");
-    indexr(id, "10", "from_s", "mary", "to_s", "mike", "predicate_s", "knows");
-    indexr(id, "11", "from_s", "mary", "to_s", "max", "predicate_s", "knows");
-    indexr(id, "12", "from_s", "mary", "to_s", "jim", "predicate_s", "knows");
-    indexr(id, "13", "from_s", "mary", "to_s", "steve", "predicate_s", "knows");
-
-    commit();
+  public void testShortestPathStream() throws Exception {
+
+    new UpdateRequest()
+        .add(id, "0", "from_s", "jim", "to_s", "mike", "predicate_s", "knows")
+        .add(id, "1", "from_s", "jim", "to_s", "dave", "predicate_s", "knows")
+        .add(id, "2", "from_s", "jim", "to_s", "stan", "predicate_s", "knows")
+        .add(id, "3", "from_s", "dave", "to_s", "stan", "predicate_s", "knows")
+        .add(id, "4", "from_s", "dave", "to_s", "bill", "predicate_s", "knows")
+        .add(id, "5", "from_s", "dave", "to_s", "mike", "predicate_s", "knows")
+        .add(id, "20", "from_s", "dave", "to_s", "alex", "predicate_s", "knows")
+        .add(id, "21", "from_s", "alex", "to_s", "steve", "predicate_s", "knows")
+        .add(id, "6", "from_s", "stan", "to_s", "alice", "predicate_s", "knows")
+        .add(id, "7", "from_s", "stan", "to_s", "mary", "predicate_s", "knows")
+        .add(id, "8", "from_s", "stan", "to_s", "dave", "predicate_s", "knows")
+        .add(id, "10", "from_s", "mary", "to_s", "mike", "predicate_s", "knows")
+        .add(id, "11", "from_s", "mary", "to_s", "max", "predicate_s", "knows")
+        .add(id, "12", "from_s", "mary", "to_s", "jim", "predicate_s", "knows")
+        .add(id, "13", "from_s", "mary", "to_s", "steve", "predicate_s", "knows")
+        .commit(cluster.getSolrClient(), COLLECTION);
 
     List<Tuple> tuples = null;
     Set<String> paths = null;
@@ -157,7 +113,7 @@ public class GraphExpressionTest extends AbstractFullDistribZkTestBase {
     context.setSolrClientCache(cache);
 
     StreamFactory factory = new StreamFactory()
-        .withCollectionZkHost("collection1", zkServer.getZkAddress())
+        .withCollectionZkHost("collection1", cluster.getZkServer().getZkAddress())
         .withFunctionName("shortestPath", ShortestPathStream.class);
 
     Map params = new HashMap();
@@ -271,27 +227,26 @@ public class GraphExpressionTest extends AbstractFullDistribZkTestBase {
     assertTrue(paths.contains("[jim, stan, mary, steve]"));
 
     cache.close();
-    del("*:*");
-    commit();
-  }
-
 
-  private void testGatherNodesStream() throws Exception {
-
-    indexr(id, "0", "basket_s", "basket1", "product_s", "product1", "price_f", "20");
-    indexr(id, "1", "basket_s", "basket1", "product_s", "product3", "price_f", "30");
-    indexr(id, "2", "basket_s", "basket1", "product_s", "product5", "price_f", "1");
-    indexr(id, "3", "basket_s", "basket2", "product_s", "product1", "price_f", "2");
-    indexr(id, "4", "basket_s", "basket2", "product_s", "product6", "price_f", "5");
-    indexr(id, "5", "basket_s", "basket2", "product_s", "product7", "price_f", "10");
-    indexr(id, "6", "basket_s", "basket3", "product_s", "product4", "price_f", "20");
-    indexr(id, "7", "basket_s", "basket3", "product_s", "product3", "price_f", "10");
-    indexr(id, "8", "basket_s", "basket3", "product_s", "product1", "price_f", "10");
-    indexr(id, "9", "basket_s", "basket4", "product_s", "product4", "price_f", "40");
-    indexr(id, "10", "basket_s", "basket4", "product_s", "product3", "price_f", "10");
-    indexr(id, "11", "basket_s", "basket4", "product_s", "product1", "price_f", "10");
+  }
 
-    commit();
+  @Test
+  public void testGatherNodesStream() throws Exception {
+
+    new UpdateRequest()
+        .add(id, "0", "basket_s", "basket1", "product_s", "product1", "price_f", "20")
+        .add(id, "1", "basket_s", "basket1", "product_s", "product3", "price_f", "30")
+        .add(id, "2", "basket_s", "basket1", "product_s", "product5", "price_f", "1")
+        .add(id, "3", "basket_s", "basket2", "product_s", "product1", "price_f", "2")
+        .add(id, "4", "basket_s", "basket2", "product_s", "product6", "price_f", "5")
+        .add(id, "5", "basket_s", "basket2", "product_s", "product7", "price_f", "10")
+        .add(id, "6", "basket_s", "basket3", "product_s", "product4", "price_f", "20")
+        .add(id, "7", "basket_s", "basket3", "product_s", "product3", "price_f", "10")
+        .add(id, "8", "basket_s", "basket3", "product_s", "product1", "price_f", "10")
+        .add(id, "9", "basket_s", "basket4", "product_s", "product4", "price_f", "40")
+        .add(id, "10", "basket_s", "basket4", "product_s", "product3", "price_f", "10")
+        .add(id, "11", "basket_s", "basket4", "product_s", "product1", "price_f", "10")
+        .commit(cluster.getSolrClient(), COLLECTION);
 
     List<Tuple> tuples = null;
     Set<String> paths = null;
@@ -301,7 +256,7 @@ public class GraphExpressionTest extends AbstractFullDistribZkTestBase {
     context.setSolrClientCache(cache);
 
     StreamFactory factory = new StreamFactory()
-        .withCollectionZkHost("collection1", zkServer.getZkAddress())
+        .withCollectionZkHost("collection1", cluster.getZkServer().getZkAddress())
         .withFunctionName("gatherNodes", GatherNodesStream.class)
         .withFunctionName("search", CloudSolrStream.class)
         .withFunctionName("count", CountMetric.class)
@@ -417,20 +372,20 @@ public class GraphExpressionTest extends AbstractFullDistribZkTestBase {
     assertTrue(tuples.get(1).getString("node").equals("basket3"));
 
     cache.close();
-    del("*:*");
-    commit();
-  }
 
-  private void testGatherNodesFriendsStream() throws Exception {
+  }
 
-    indexr(id, "0", "from_s", "bill", "to_s", "jim", "message_t", "Hello jim");
-    indexr(id, "1", "from_s", "bill", "to_s", "sam", "message_t", "Hello sam");
-    indexr(id, "2", "from_s", "bill", "to_s", "max", "message_t", "Hello max");
-    indexr(id, "3", "from_s", "max",  "to_s", "kip", "message_t", "Hello kip");
-    indexr(id, "4", "from_s", "sam",  "to_s", "steve", "message_t", "Hello steve");
-    indexr(id, "5", "from_s", "jim",  "to_s", "ann", "message_t", "Hello steve");
+  @Test
+  public void testGatherNodesFriendsStream() throws Exception {
 
-    commit();
+    new UpdateRequest()
+        .add(id, "0", "from_s", "bill", "to_s", "jim", "message_t", "Hello jim")
+        .add(id, "1", "from_s", "bill", "to_s", "sam", "message_t", "Hello sam")
+        .add(id, "2", "from_s", "bill", "to_s", "max", "message_t", "Hello max")
+        .add(id, "3", "from_s", "max",  "to_s", "kip", "message_t", "Hello kip")
+        .add(id, "4", "from_s", "sam",  "to_s", "steve", "message_t", "Hello steve")
+        .add(id, "5", "from_s", "jim",  "to_s", "ann", "message_t", "Hello steve")
+        .commit(cluster.getSolrClient(), COLLECTION);
 
     List<Tuple> tuples = null;
     Set<String> paths = null;
@@ -440,7 +395,7 @@ public class GraphExpressionTest extends AbstractFullDistribZkTestBase {
     context.setSolrClientCache(cache);
 
     StreamFactory factory = new StreamFactory()
-        .withCollectionZkHost("collection1", zkServer.getZkAddress())
+        .withCollectionZkHost("collection1", cluster.getZkServer().getZkAddress())
         .withFunctionName("gatherNodes", GatherNodesStream.class)
         .withFunctionName("search", CloudSolrStream.class)
         .withFunctionName("count", CountMetric.class)
@@ -628,10 +583,10 @@ public class GraphExpressionTest extends AbstractFullDistribZkTestBase {
     assertTrue(tuples.get(6).getLong("level").equals(new Long(2)));
 
     //Add a cycle from jim to bill
-    indexr(id, "6", "from_s", "jim", "to_s", "bill", "message_t", "Hello steve");
-    indexr(id, "7", "from_s", "sam", "to_s", "bill", "message_t", "Hello steve");
-
-    commit();
+    new UpdateRequest()
+        .add(id, "6", "from_s", "jim", "to_s", "bill", "message_t", "Hello steve")
+        .add(id, "7", "from_s", "sam", "to_s", "bill", "message_t", "Hello steve")
+        .commit(cluster.getSolrClient(), COLLECTION);
 
     expr = "gatherNodes(collection1, " +
            "search(collection1, q=\"message_t:jim\", fl=\"from_s\", sort=\"from_s asc\"),"+
@@ -676,11 +631,8 @@ public class GraphExpressionTest extends AbstractFullDistribZkTestBase {
     assertTrue(tuples.get(6).getLong("level").equals(new Long(2)));
 
     cache.close();
-    del("*:*");
-    commit();
-  }
-
 
+  }
 
   protected List<Tuple> getTuples(TupleStream tupleStream) throws IOException {
     tupleStream.open();
@@ -691,9 +643,7 @@ public class GraphExpressionTest extends AbstractFullDistribZkTestBase {
     tupleStream.close();
     return tuples;
   }
-  protected boolean assertOrder(List<Tuple> tuples, int... ids) throws Exception {
-    return assertOrderOf(tuples, "id", ids);
-  }
+
   protected boolean assertOrderOf(List<Tuple> tuples, String fieldName, int... ids) throws Exception {
     int i = 0;
     for(int val : ids) {
@@ -707,56 +657,6 @@ public class GraphExpressionTest extends AbstractFullDistribZkTestBase {
     return true;
   }
 
-  protected boolean assertMapOrder(List<Tuple> tuples, int... ids) throws Exception {
-    int i = 0;
-    for(int val : ids) {
-      Tuple t = tuples.get(i);
-      List<Map> tip = t.getMaps("group");
-      int id = (int)tip.get(0).get("id");
-      if(id != val) {
-        throw new Exception("Found value:"+id+" expecting:"+val);
-      }
-      ++i;
-    }
-    return true;
-  }
-
-
-  protected boolean assertFields(List<Tuple> tuples, String ... fields) throws Exception{
-    for(Tuple tuple : tuples){
-      for(String field : fields){
-        if(!tuple.fields.containsKey(field)){
-          throw new Exception(String.format(Locale.ROOT, "Expected field '%s' not found", field));
-        }
-      }
-    }
-    return true;
-  }
-  protected boolean assertNotFields(List<Tuple> tuples, String ... fields) throws Exception{
-    for(Tuple tuple : tuples){
-      for(String field : fields){
-        if(tuple.fields.containsKey(field)){
-          throw new Exception(String.format(Locale.ROOT, "Unexpected field '%s' found", field));
-        }
-      }
-    }
-    return true;
-  }
-
-  protected boolean assertGroupOrder(Tuple tuple, int... ids) throws Exception {
-    List<?> group = (List<?>)tuple.get("tuples");
-    int i=0;
-    for(int val : ids) {
-      Map<?,?> t = (Map<?,?>)group.get(i);
-      Long tip = (Long)t.get("id");
-      if(tip.intValue() != val) {
-        throw new Exception("Found value:"+tip.intValue()+" expecting:"+val);
-      }
-      ++i;
-    }
-    return true;
-  }
-
   public boolean assertLong(Tuple tuple, String fieldName, long l) throws Exception {
     long lv = (long)tuple.get(fieldName);
     if(lv != l) {
@@ -778,44 +678,4 @@ public class GraphExpressionTest extends AbstractFullDistribZkTestBase {
     return true;
   }
 
-  protected boolean assertMaps(List<Map> maps, int... ids) throws Exception {
-    if(maps.size() != ids.length) {
-      throw new Exception("Expected id count != actual map count:"+ids.length+":"+maps.size());
-    }
-
-    int i=0;
-    for(int val : ids) {
-      Map t = maps.get(i);
-      Long tip = (Long)t.get("id");
-      if(tip.intValue() != val) {
-        throw new Exception("Found value:"+tip.intValue()+" expecting:"+val);
-      }
-      ++i;
-    }
-    return true;
-  }
-
-  private boolean assertList(List list, Object... vals) throws Exception {
-
-    if(list.size() != vals.length) {
-      throw new Exception("Lists are not the same size:"+list.size() +" : "+vals.length);
-    }
-
-    for(int i=0; i<list.size(); i++) {
-      Object a = list.get(i);
-      Object b = vals[i];
-      if(!a.equals(b)) {
-        throw new Exception("List items not equals:"+a+" : "+b);
-      }
-    }
-
-    return true;
-  }
-
-
-  @Override
-  protected void indexr(Object... fields) throws Exception {
-    SolrInputDocument doc = getDoc(fields);
-    indexDoc(doc);
-  }
 }
diff --git a/solr/solrj/src/test/org/apache/solr/client/solrj/io/graph/GraphTest.java b/solr/solrj/src/test/org/apache/solr/client/solrj/io/graph/GraphTest.java
index 77f04e7..27c9dca 100644
--- a/solr/solrj/src/test/org/apache/solr/client/solrj/io/graph/GraphTest.java
+++ b/solr/solrj/src/test/org/apache/solr/client/solrj/io/graph/GraphTest.java
@@ -17,30 +17,26 @@ package org.apache.solr.client.solrj.io.graph;
  * limitations under the License.
  */
 
-import java.io.File;
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.HashMap;
+import java.util.HashSet;
 import java.util.List;
 import java.util.Map;
+import java.util.Set;
 
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.solr.client.solrj.io.SolrClientCache;
 import org.apache.solr.client.solrj.io.Tuple;
 import org.apache.solr.client.solrj.io.stream.StreamContext;
 import org.apache.solr.client.solrj.io.stream.TupleStream;
-import org.apache.solr.client.solrj.io.stream.expr.StreamFactory;
-import org.apache.solr.cloud.AbstractFullDistribZkTestBase;
-import org.apache.solr.cloud.AbstractZkTestCase;
-import org.apache.solr.common.SolrInputDocument;
-import org.junit.After;
-import org.junit.AfterClass;
+import org.apache.solr.client.solrj.request.CollectionAdminRequest;
+import org.apache.solr.client.solrj.request.UpdateRequest;
+import org.apache.solr.cloud.AbstractDistribZkTestBase;
+import org.apache.solr.cloud.SolrCloudTestCase;
 import org.junit.Before;
 import org.junit.BeforeClass;
 import org.junit.Test;
-
-import java.util.Set;
-import java.util.HashSet;
 /**
  *  All base tests will be done with CloudSolrStream. Under the covers CloudSolrStream uses SolrStream so
  *  SolrStream will get fully exercised through these tests.
@@ -49,86 +45,57 @@ import java.util.HashSet;
 
 @LuceneTestCase.Slow
 @LuceneTestCase.SuppressCodecs({"Lucene3x", "Lucene40","Lucene41","Lucene42","Lucene45"})
-public class GraphTest extends AbstractFullDistribZkTestBase {
-
-  private static final String SOLR_HOME = getFile("solrj" + File.separator + "solr").getAbsolutePath();
-  private StreamFactory streamFactory;
-
-  static {
-    schemaString = "schema-streaming.xml";
-  }
+public class GraphTest extends SolrCloudTestCase {
 
-  @BeforeClass
-  public static void beforeSuperClass() {
-    AbstractZkTestCase.SOLRHOME = new File(SOLR_HOME());
-  }
-
-  @AfterClass
-  public static void afterSuperClass() {
-
-  }
-
-  protected String getCloudSolrConfig() {
-    return "solrconfig-streaming.xml";
-  }
+  private static final String COLLECTION = "collection1";
 
+  private static final String id = "id";
 
-  @Override
-  public String getSolrHome() {
-    return SOLR_HOME;
-  }
+  private static final int TIMEOUT = 30;
 
-  public static String SOLR_HOME() {
-    return SOLR_HOME;
+  @BeforeClass
+  public static void setupCluster() throws Exception {
+    configureCluster(2)
+        .addConfig("conf", getFile("solrj").toPath().resolve("solr").resolve("configsets").resolve("streaming").resolve("conf"))
+        .configure();
+
+    CollectionAdminRequest.createCollection(COLLECTION, "conf", 2, 1).process(cluster.getSolrClient());
+    AbstractDistribZkTestBase.waitForRecoveriesToFinish(COLLECTION, cluster.getSolrClient().getZkStateReader(),
+        false, true, TIMEOUT);
   }
 
   @Before
-  @Override
-  public void setUp() throws Exception {
-    super.setUp();
-    // we expect this time of exception as shards go up and down...
-    //ignoreException(".*");
-    //System.setProperty("export.test", "true");
-    System.setProperty("numShards", Integer.toString(sliceCount));
-  }
-
-  @Override
-  @After
-  public void tearDown() throws Exception {
-    super.tearDown();
-    resetExceptionIgnores();
-  }
-
-  public GraphTest() {
-    super();
-    sliceCount = 2;
-
+  public void cleanIndex() throws Exception {
+    new UpdateRequest()
+        .deleteByQuery("*:*")
+        .commit(cluster.getSolrClient(), COLLECTION);
   }
 
-  private void testShortestPathStream() throws Exception {
-
-    indexr(id, "0", "from_s", "jim", "to_s", "mike", "predicate_s", "knows");
-    indexr(id, "1", "from_s", "jim", "to_s", "dave", "predicate_s", "knows");
-    indexr(id, "2", "from_s", "jim", "to_s", "stan", "predicate_s", "knows");
-    indexr(id, "3", "from_s", "dave", "to_s", "stan", "predicate_s", "knows");
-    indexr(id, "4", "from_s", "dave", "to_s", "bill", "predicate_s", "knows");
-    indexr(id, "5", "from_s", "dave", "to_s", "mike", "predicate_s", "knows");
-    indexr(id, "20", "from_s", "dave", "to_s", "alex", "predicate_s", "knows");
-    indexr(id, "21", "from_s", "alex", "to_s", "steve", "predicate_s", "knows");
-    indexr(id, "6", "from_s", "stan", "to_s", "alice", "predicate_s", "knows");
-    indexr(id, "7", "from_s", "stan", "to_s", "mary", "predicate_s", "knows");
-    indexr(id, "8", "from_s", "stan", "to_s", "dave", "predicate_s", "knows");
-    indexr(id, "10", "from_s", "mary", "to_s", "mike", "predicate_s", "knows");
-    indexr(id, "11", "from_s", "mary", "to_s", "max", "predicate_s", "knows");
-    indexr(id, "12", "from_s", "mary", "to_s", "jim", "predicate_s", "knows");
-    indexr(id, "13", "from_s", "mary", "to_s", "steve", "predicate_s", "knows");
-
-    commit();
+  @Test
+  public void testShortestPathStream() throws Exception {
+
+    new UpdateRequest()
+        .add(id, "0", "from_s", "jim", "to_s", "mike", "predicate_s", "knows")
+        .add(id, "1", "from_s", "jim", "to_s", "dave", "predicate_s", "knows")
+        .add(id, "2", "from_s", "jim", "to_s", "stan", "predicate_s", "knows")
+        .add(id, "3", "from_s", "dave", "to_s", "stan", "predicate_s", "knows")
+        .add(id, "4", "from_s", "dave", "to_s", "bill", "predicate_s", "knows")
+        .add(id, "5", "from_s", "dave", "to_s", "mike", "predicate_s", "knows")
+        .add(id, "20", "from_s", "dave", "to_s", "alex", "predicate_s", "knows")
+        .add(id, "21", "from_s", "alex", "to_s", "steve", "predicate_s", "knows")
+        .add(id, "6", "from_s", "stan", "to_s", "alice", "predicate_s", "knows")
+        .add(id, "7", "from_s", "stan", "to_s", "mary", "predicate_s", "knows")
+        .add(id, "8", "from_s", "stan", "to_s", "dave", "predicate_s", "knows")
+        .add(id, "10", "from_s", "mary", "to_s", "mike", "predicate_s", "knows")
+        .add(id, "11", "from_s", "mary", "to_s", "max", "predicate_s", "knows")
+        .add(id, "12", "from_s", "mary", "to_s", "jim", "predicate_s", "knows")
+        .add(id, "13", "from_s", "mary", "to_s", "steve", "predicate_s", "knows")
+        .commit(cluster.getSolrClient(), COLLECTION);
 
     List<Tuple> tuples = null;
     Set<String> paths = null;
     ShortestPathStream stream = null;
-    String zkHost = zkServer.getZkAddress();
+    String zkHost = cluster.getZkServer().getZkAddress();
     StreamContext context = new StreamContext();
     SolrClientCache cache = new SolrClientCache();
     context.setSolrClientCache(cache);
@@ -260,40 +227,6 @@ public class GraphTest extends AbstractFullDistribZkTestBase {
     assertTrue(paths.contains("[jim, stan, mary, steve]"));
 
     cache.close();
-    del("*:*");
-    commit();
-  }
-
-  @Test
-  public void streamTests() throws Exception {
-    assertNotNull(cloudClient);
-
-    handle.clear();
-    handle.put("timestamp", SKIPVAL);
-
-    waitForRecoveriesToFinish(false);
-
-    del("*:*");
-
-    commit();
-
-    testShortestPathStream();
-
-  }
-
-  protected Map mapParams(String... vals) {
-    Map params = new HashMap();
-    String k = null;
-    for(String val : vals) {
-      if(k == null) {
-        k = val;
-      } else {
-        params.put(k, val);
-        k = null;
-      }
-    }
-
-    return params;
   }
 
   protected List<Tuple> getTuples(TupleStream tupleStream) throws IOException {
@@ -311,58 +244,6 @@ public class GraphTest extends AbstractFullDistribZkTestBase {
     return tuples;
   }
 
-  protected Tuple getTuple(TupleStream tupleStream) throws IOException {
-    tupleStream.open();
-    Tuple t = tupleStream.read();
-    tupleStream.close();
-    return t;
-  }
-
-
-  protected boolean assertOrder(List<Tuple> tuples, int... ids) throws Exception {
-    int i = 0;
-    for(int val : ids) {
-      Tuple t = tuples.get(i);
-      Long tip = (Long)t.get("id");
-      if(tip.intValue() != val) {
-        throw new Exception("Found value:"+tip.intValue()+" expecting:"+val);
-      }
-      ++i;
-    }
-    return true;
-  }
-
-  protected boolean assertGroupOrder(Tuple tuple, int... ids) throws Exception {
-    List group = (List)tuple.get("tuples");
-    int i=0;
-    for(int val : ids) {
-      Map t = (Map)group.get(i);
-      Long tip = (Long)t.get("id");
-      if(tip.intValue() != val) {
-        throw new Exception("Found value:"+tip.intValue()+" expecting:"+val);
-      }
-      ++i;
-    }
-    return true;
-  }
-
-  protected boolean assertMaps(List<Map> maps, int... ids) throws Exception {
-    if(maps.size() != ids.length) {
-      throw new Exception("Expected id count != actual map count:"+ids.length+":"+maps.size());
-    }
-
-    int i=0;
-    for(int val : ids) {
-      Map t = maps.get(i);
-      Long tip = (Long)t.get("id");
-      if(tip.intValue() != val) {
-        throw new Exception("Found value:"+tip.intValue()+" expecting:"+val);
-      }
-      ++i;
-    }
-    return true;
-  }
-
   public boolean assertLong(Tuple tuple, String fieldName, long l) throws Exception {
     long lv = (long)tuple.get(fieldName);
     if(lv != l) {
@@ -372,16 +253,5 @@ public class GraphTest extends AbstractFullDistribZkTestBase {
     return true;
   }
 
-  @Override
-  protected void indexr(Object... fields) throws Exception {
-    SolrInputDocument doc = getDoc(fields);
-    indexDoc(doc);
-  }
-
-  private void attachStreamFactory(TupleStream tupleStream) {
-    StreamContext streamContext = new StreamContext();
-    streamContext.setStreamFactory(streamFactory);
-    tupleStream.setStreamContext(streamContext);
-  }
 }
 
diff --git a/solr/solrj/src/test/org/apache/solr/client/solrj/io/sql/JdbcTest.java b/solr/solrj/src/test/org/apache/solr/client/solrj/io/sql/JdbcTest.java
index d38661e..8280eae 100644
--- a/solr/solrj/src/test/org/apache/solr/client/solrj/io/sql/JdbcTest.java
+++ b/solr/solrj/src/test/org/apache/solr/client/solrj/io/sql/JdbcTest.java
@@ -16,7 +16,6 @@
  */
 package org.apache.solr.client.solrj.io.sql;
 
-import java.io.File;
 import java.sql.Connection;
 import java.sql.DatabaseMetaData;
 import java.sql.DriverManager;
@@ -32,11 +31,10 @@ import java.util.Properties;
 
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.util.LuceneTestCase.Slow;
-import org.apache.solr.cloud.AbstractFullDistribZkTestBase;
-import org.apache.solr.cloud.AbstractZkTestCase;
-import org.apache.solr.common.cloud.DocCollection;
-import org.junit.After;
-import org.junit.AfterClass;
+import org.apache.solr.client.solrj.request.CollectionAdminRequest;
+import org.apache.solr.client.solrj.request.UpdateRequest;
+import org.apache.solr.cloud.AbstractDistribZkTestBase;
+import org.apache.solr.cloud.SolrCloudTestCase;
 import org.junit.BeforeClass;
 import org.junit.Test;
 
@@ -47,68 +45,45 @@ import org.junit.Test;
 
 @Slow
 @LuceneTestCase.SuppressCodecs({"Lucene3x", "Lucene40", "Lucene41", "Lucene42", "Lucene45"})
-public class JdbcTest extends AbstractFullDistribZkTestBase {
+public class JdbcTest extends SolrCloudTestCase {
 
-  private static final String SOLR_HOME = getFile("solrj" + File.separator + "solr").getAbsolutePath();
+  private static final String COLLECTION = "collection1";
 
+  private static final String id = "id";
 
-  static {
-    schemaString = "schema-sql.xml";
-  }
-
-  @BeforeClass
-  public static void beforeSuperClass() {
-    AbstractZkTestCase.SOLRHOME = new File(SOLR_HOME);
-  }
-
-  @AfterClass
-  public static void afterSuperClass() {
-
-  }
-
-  protected String getCloudSolrConfig() {
-    return "solrconfig-sql.xml";
-  }
-
-  @Override
-  public String getSolrHome() {
-    return SOLR_HOME;
-  }
+  private static final int TIMEOUT = 30;
 
+  private static String zkHost;
 
-  @Override
-  public void distribSetUp() throws Exception {
-    super.distribSetUp();
-  }
-
-  @Override
-  @After
-  public void tearDown() throws Exception {
-    super.tearDown();
-    resetExceptionIgnores();
+  @BeforeClass
+  public static void setupCluster() throws Exception {
+    configureCluster(2)
+        .addConfig("conf", getFile("solrj").toPath().resolve("solr").resolve("configsets").resolve("streaming").resolve("conf"))
+        .configure();
+
+    CollectionAdminRequest.createCollection(COLLECTION, "conf", 2, 1).process(cluster.getSolrClient());
+    AbstractDistribZkTestBase.waitForRecoveriesToFinish(COLLECTION, cluster.getSolrClient().getZkStateReader(),
+        false, true, TIMEOUT);
+
+    new UpdateRequest()
+        .add(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "1", "testnull_i", null)
+        .add(id, "2", "a_s", "hello0", "a_i", "2", "a_f", "2", "testnull_i", "2")
+        .add(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3", "testnull_i", null)
+        .add(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4", "testnull_i", "4")
+        .add(id, "1", "a_s", "hello0", "a_i", "1", "a_f", "5", "testnull_i", null)
+        .add(id, "5", "a_s", "hello3", "a_i", "10", "a_f", "6", "testnull_i", "6")
+        .add(id, "6", "a_s", "hello4", "a_i", "11", "a_f", "7", "testnull_i", null)
+        .add(id, "7", "a_s", "hello3", "a_i", "12", "a_f", "8", "testnull_i", "8")
+        .add(id, "8", "a_s", "hello3", "a_i", "13", "a_f", "9", "testnull_i", null)
+        .add(id, "9", "a_s", "hello0", "a_i", "14", "a_f", "10", "testnull_i", "10")
+        .commit(cluster.getSolrClient(), COLLECTION);
+
+    zkHost = cluster.getZkServer().getZkAddress();
   }
 
   @Test
-  @ShardsFixed(num = 2)
   public void doTest() throws Exception {
 
-    waitForRecoveriesToFinish(false);
-
-    indexr(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "1", "testnull_i", null);
-    indexr(id, "2", "a_s", "hello0", "a_i", "2", "a_f", "2", "testnull_i", "2");
-    indexr(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3", "testnull_i", null);
-    indexr(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4", "testnull_i", "4");
-    indexr(id, "1", "a_s", "hello0", "a_i", "1", "a_f", "5", "testnull_i", null);
-    indexr(id, "5", "a_s", "hello3", "a_i", "10", "a_f", "6", "testnull_i", "6");
-    indexr(id, "6", "a_s", "hello4", "a_i", "11", "a_f", "7", "testnull_i", null);
-    indexr(id, "7", "a_s", "hello3", "a_i", "12", "a_f", "8", "testnull_i", "8");
-    indexr(id, "8", "a_s", "hello3", "a_i", "13", "a_f", "9", "testnull_i", null);
-    indexr(id, "9", "a_s", "hello0", "a_i", "14", "a_f", "10", "testnull_i", "10");
-
-    commit();
-
-    String zkHost = zkServer.getZkAddress();
-
     Properties props = new Properties();
 
     try (Connection con = DriverManager.getConnection("jdbc:solr://" + zkHost + "?collection=collection1", props)) {
@@ -202,8 +177,13 @@ public class JdbcTest extends AbstractFullDistribZkTestBase {
       }
     }
 
+  }
+
+  @Test
+  public void testFacetAggregation() throws Exception {
+
     //Test facet aggregation
-    props = new Properties();
+    Properties props = new Properties();
     props.put("aggregationMode", "facet");
     try (Connection con = DriverManager.getConnection("jdbc:solr://" + zkHost + "?collection=collection1", props)) {
       try (Statement stmt = con.createStatement()) {
@@ -236,8 +216,13 @@ public class JdbcTest extends AbstractFullDistribZkTestBase {
       }
     }
 
+  }
+
+  @Test
+  public void testMapReduceAggregation() throws Exception {
+
     //Test map / reduce aggregation
-    props = new Properties();
+    Properties props = new Properties();
     props.put("aggregationMode", "map_reduce");
     props.put("numWorkers", "2");
     try (Connection con = DriverManager.getConnection("jdbc:solr://" + zkHost + "?collection=collection1", props)) {
@@ -270,15 +255,20 @@ public class JdbcTest extends AbstractFullDistribZkTestBase {
         }
       }
     }
-    
+
+  }
+
+  @Test
+  public void testConnectionParams() throws Exception {
+
     //Test params on the url
-    try (Connection con = DriverManager.getConnection("jdbc:solr://" + zkHost + 
+    try (Connection con = DriverManager.getConnection("jdbc:solr://" + zkHost +
         "?collection=collection1&aggregationMode=map_reduce&numWorkers=2")) {
 
       Properties p = ((ConnectionImpl) con).getProperties();
 
-      assert(p.getProperty("aggregationMode").equals("map_reduce"));
-      assert(p.getProperty("numWorkers").equals("2"));
+      assert (p.getProperty("aggregationMode").equals("map_reduce"));
+      assert (p.getProperty("numWorkers").equals("2"));
 
       try (Statement stmt = con.createStatement()) {
         try (ResultSet rs = stmt.executeQuery("select a_s, sum(a_f) from collection1 group by a_s " +
@@ -310,6 +300,11 @@ public class JdbcTest extends AbstractFullDistribZkTestBase {
       }
     }
 
+  }
+
+  @Test
+  public void testJDBCUrlParameters() throws Exception {
+
     // Test JDBC paramters in URL
     try (Connection con = DriverManager.getConnection(
         "jdbc:solr://" + zkHost + "?collection=collection1&username=&password=&testKey1=testValue&testKey2")) {
@@ -350,6 +345,11 @@ public class JdbcTest extends AbstractFullDistribZkTestBase {
       }
     }
 
+  }
+
+  @Test
+  public void testJDBCPropertiesParameters() throws Exception {
+
     // Test JDBC paramters in properties
     Properties providedProperties = new Properties();
     providedProperties.put("collection", "collection1");
@@ -360,10 +360,10 @@ public class JdbcTest extends AbstractFullDistribZkTestBase {
 
     try (Connection con = DriverManager.getConnection("jdbc:solr://" + zkHost, providedProperties)) {
       Properties p = ((ConnectionImpl) con).getProperties();
-      assert(p.getProperty("username").equals(""));
-      assert(p.getProperty("password").equals(""));
-      assert(p.getProperty("testKey1").equals("testValue"));
-      assert(p.getProperty("testKey2").equals(""));
+      assert (p.getProperty("username").equals(""));
+      assert (p.getProperty("password").equals(""));
+      assert (p.getProperty("testKey1").equals("testValue"));
+      assert (p.getProperty("testKey2").equals(""));
 
       try (Statement stmt = con.createStatement()) {
         try (ResultSet rs = stmt.executeQuery("select a_s, sum(a_f) from collection1 group by a_s " +
@@ -394,10 +394,13 @@ public class JdbcTest extends AbstractFullDistribZkTestBase {
         }
       }
     }
+  }
 
+  @Test
+  public void testErrorPropagation() throws Exception {
 
     //Test error propagation
-    props = new Properties();
+    Properties props = new Properties();
     props.put("aggregationMode", "facet");
     try (Connection con = DriverManager.getConnection("jdbc:solr://" + zkHost + "?collection=collection1", props)) {
       try (Statement stmt = con.createStatement()) {
@@ -410,20 +413,20 @@ public class JdbcTest extends AbstractFullDistribZkTestBase {
       }
     }
 
-    testDriverMetadata();
   }
 
-  private void testDriverMetadata() throws Exception {
-    String collection = DEFAULT_COLLECTION;
+  @Test
+  public void testDriverMetadata() throws Exception {
+    String collection = COLLECTION;
 
-    String connectionString1 = "jdbc:solr://" + zkServer.getZkAddress() + "?collection=" + collection +
+    String connectionString1 = "jdbc:solr://" + zkHost + "?collection=" + collection +
         "&username=&password=&testKey1=testValue&testKey2";
     Properties properties1 = new Properties();
 
     String sql = "select id, a_i, a_s, a_f as my_float_col, testnull_i from " + collection +
         " order by a_i desc";
 
-    String connectionString2 = "jdbc:solr://" + zkServer.getZkAddress() + "?collection=" + collection +
+    String connectionString2 = "jdbc:solr://" + zkHost + "?collection=" + collection +
         "&aggregationMode=map_reduce&numWorkers=2&username=&password=&testKey1=testValue&testKey2";
     Properties properties2 = new Properties();
 
@@ -439,9 +442,9 @@ public class JdbcTest extends AbstractFullDistribZkTestBase {
     try (Connection con = DriverManager.getConnection(connectionString, properties)) {
       assertTrue(con.isValid(DEFAULT_CONNECTION_TIMEOUT));
 
-      assertEquals(zkServer.getZkAddress(), con.getCatalog());
-      con.setCatalog(zkServer.getZkAddress());
-      assertEquals(zkServer.getZkAddress(), con.getCatalog());
+      assertEquals(zkHost, con.getCatalog());
+      con.setCatalog(zkHost);
+      assertEquals(zkHost, con.getCatalog());
 
       assertEquals(null, con.getSchema());
       con.setSchema("myschema");
@@ -470,22 +473,22 @@ public class JdbcTest extends AbstractFullDistribZkTestBase {
 
       try(ResultSet rs = databaseMetaData.getCatalogs()) {
         assertTrue(rs.next());
-        assertEquals(zkServer.getZkAddress(), rs.getString("TABLE_CAT"));
+        assertEquals(zkHost, rs.getString("TABLE_CAT"));
         assertFalse(rs.next());
       }
 
       List<String> collections = new ArrayList<>();
-      collections.addAll(cloudClient.getZkStateReader().getClusterState().getCollections());
+      collections.addAll(cluster.getSolrClient().getZkStateReader().getClusterState().getCollections());
       Collections.sort(collections);
 
       try(ResultSet rs = databaseMetaData.getSchemas()) {
         assertFalse(rs.next());
       }
 
-      try(ResultSet rs = databaseMetaData.getTables(zkServer.getZkAddress(), null, "%", null)) {
+      try(ResultSet rs = databaseMetaData.getTables(zkHost, null, "%", null)) {
         for(String acollection : collections) {
           assertTrue(rs.next());
-          assertEquals(zkServer.getZkAddress(), rs.getString("TABLE_CAT"));
+          assertEquals(zkHost, rs.getString("TABLE_CAT"));
           assertNull(rs.getString("TABLE_SCHEM"));
           assertEquals(acollection, rs.getString("TABLE_NAME"));
           assertEquals("TABLE", rs.getString("TABLE_TYPE"));
diff --git a/solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/JDBCStreamTest.java b/solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/JDBCStreamTest.java
index f330c11..0315cfe 100644
--- a/solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/JDBCStreamTest.java
+++ b/solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/JDBCStreamTest.java
@@ -16,7 +16,6 @@
  */
 package org.apache.solr.client.solrj.io.stream;
 
-import java.io.File;
 import java.io.IOException;
 import java.sql.Connection;
 import java.sql.DriverManager;
@@ -26,10 +25,8 @@ import java.util.ArrayList;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Locale;
-import java.util.Properties;
 
 import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util.LuceneTestCase.Slow;
 import org.apache.solr.client.solrj.io.Tuple;
 import org.apache.solr.client.solrj.io.comp.ComparatorOrder;
 import org.apache.solr.client.solrj.io.comp.FieldComparator;
@@ -38,10 +35,10 @@ import org.apache.solr.client.solrj.io.stream.metrics.CountMetric;
 import org.apache.solr.client.solrj.io.stream.metrics.MaxMetric;
 import org.apache.solr.client.solrj.io.stream.metrics.MeanMetric;
 import org.apache.solr.client.solrj.io.stream.metrics.MinMetric;
-import org.apache.solr.cloud.AbstractFullDistribZkTestBase;
-import org.apache.solr.cloud.AbstractZkTestCase;
-import org.apache.solr.common.SolrInputDocument;
-import org.junit.After;
+import org.apache.solr.client.solrj.request.CollectionAdminRequest;
+import org.apache.solr.client.solrj.request.UpdateRequest;
+import org.apache.solr.cloud.AbstractDistribZkTestBase;
+import org.apache.solr.cloud.SolrCloudTestCase;
 import org.junit.AfterClass;
 import org.junit.Before;
 import org.junit.BeforeClass;
@@ -50,19 +47,28 @@ import org.junit.Test;
 /**
 */
 
-@Slow
 @LuceneTestCase.SuppressCodecs({"Lucene3x", "Lucene40","Lucene41","Lucene42","Lucene45"})
-public class JDBCStreamTest extends AbstractFullDistribZkTestBase {
+public class JDBCStreamTest extends SolrCloudTestCase {
 
-  private static final String SOLR_HOME = getFile("solrj" + File.separator + "solr").getAbsolutePath();
+  private static final String COLLECTION = "jdbc";
 
-  static {
-    schemaString = "schema-streaming.xml";
+  private static final int TIMEOUT = 30;
+
+  private static final String id = "id";
+
+  @BeforeClass
+  public static void setupCluster() throws Exception {
+    configureCluster(4)
+        .addConfig("conf", getFile("solrj").toPath().resolve("solr").resolve("configsets").resolve("streaming").resolve("conf"))
+        .configure();
+
+    CollectionAdminRequest.createCollection(COLLECTION, "conf", 2, 1).process(cluster.getSolrClient());
+    AbstractDistribZkTestBase.waitForRecoveriesToFinish(COLLECTION, cluster.getSolrClient().getZkStateReader(),
+        false, true, TIMEOUT);
   }
 
   @BeforeClass
-  public static void beforeSuperClass() throws Exception {
-    AbstractZkTestCase.SOLRHOME = new File(SOLR_HOME());
+  public static void setupDatabase() throws Exception {
     
     // Initialize Database
     // Ok, so.....hsqldb is doing something totally weird so I thought I'd take a moment to explain it.
@@ -74,8 +80,7 @@ public class JDBCStreamTest extends AbstractFullDistribZkTestBase {
     // JDBCStream and is only a carryover from the driver we are testing with.
     Class.forName("org.hsqldb.jdbcDriver").newInstance();
     Connection connection = DriverManager.getConnection("jdbc:hsqldb:mem:.");
-    Statement statement = connection.createStatement();
-    statement = connection.createStatement();
+    Statement statement  = connection.createStatement();
     statement.executeUpdate("create table COUNTRIES(CODE varchar(3) not null primary key, COUNTRY_NAME varchar(50), DELETED char(1) default 'N')");
     statement.executeUpdate("create table PEOPLE(ID int not null primary key, NAME varchar(50), COUNTRY_CODE char(2), DELETED char(1) default 'N')");
     statement.executeUpdate("create table PEOPLE_SPORTS(ID int not null primary key, PERSON_ID int, SPORT_NAME varchar(50), DELETED char(1) default 'N')");
@@ -83,107 +88,48 @@ public class JDBCStreamTest extends AbstractFullDistribZkTestBase {
   }
 
   @AfterClass
-  public static void afterSuperClass() throws SQLException {
+  public static void teardownDatabase() throws SQLException {
     Connection connection = DriverManager.getConnection("jdbc:hsqldb:mem:.");
     Statement statement = connection.createStatement();
     statement.executeUpdate("shutdown");
   }
 
-  protected String getCloudSolrConfig() {
-    return "solrconfig-streaming.xml";
-  }
-
-
-  @Override
-  public String getSolrHome() {
-    return SOLR_HOME;
-  }
-
-  public static String SOLR_HOME() {
-    return SOLR_HOME;
-  }
-
   @Before
-  @Override
-  public void setUp() throws Exception {
-    super.setUp();
-    // we expect this time of exception as shards go up and down...
-    //ignoreException(".*");
-
-    System.setProperty("numShards", Integer.toString(sliceCount));
-  }
-
-  @Override
-  @After
-  public void tearDown() throws Exception {
-    super.tearDown();
-    resetExceptionIgnores();
+  public void cleanIndex() throws Exception {
+    new UpdateRequest()
+        .deleteByQuery("*:*")
+        .commit(cluster.getSolrClient(), COLLECTION);
   }
 
-  public JDBCStreamTest() {
-    super();
-    sliceCount = 2;
+  @Before
+  public void cleanDatabase() throws Exception {
+    // Clear database
+    try (Connection connection = DriverManager.getConnection("jdbc:hsqldb:mem:.");
+         Statement statement = connection.createStatement()) {
+      statement.executeUpdate("delete from COUNTRIES WHERE 1=1");
+      statement.executeUpdate("delete from PEOPLE WHERE 1=1");
+      statement.executeUpdate("delete from PEOPLE_SPORTS WHERE 1=1");
+    }
   }
 
   @Test
-  public void testAll() throws Exception{
-    assertNotNull(cloudClient);
+  public void testJDBCSelect() throws Exception {
 
-    handle.clear();
-    handle.put("timestamp", SKIPVAL);
-
-    waitForRecoveriesToFinish(false);
-
-    // Run JDBC Only tests
-    testJDBCSelect();
-    testJDBCJoin();
-    
-    // Run JDBC + Solr tests
-    testJDBCSolrMerge();
-    testJDBCSolrInnerJoinExpression();
-    testJDBCSolrInnerJoinRollupExpression();
-    testJDBCSolrInnerJoinExpressionWithProperties();
-    
-    // Clear all data
-    clearData();
-    
-    // Delete database
-    // done during afterSuperClass(...)
-  }
-  
-  private void clearData() throws Exception {
-    // Clear Solr index
-    del("*:*");
-    commit();
-    
-    // Clear database
-    Connection connection = DriverManager.getConnection("jdbc:hsqldb:mem:.");
-    Statement statement = connection.createStatement();
-    statement.executeUpdate("delete from COUNTRIES WHERE 1=1");
-    statement.executeUpdate("delete from PEOPLE WHERE 1=1");
-    statement.executeUpdate("delete from PEOPLE_SPORTS WHERE 1=1");
-    statement.close();
-    connection.close();
-  }
-  
-  private void testJDBCSelect() throws Exception {
-    clearData();
-    
     // Load Database Data
-    Connection connection = DriverManager.getConnection("jdbc:hsqldb:mem:.");
-    Statement statement = connection.createStatement();
-    statement.executeUpdate("insert into COUNTRIES (CODE,COUNTRY_NAME) values ('US', 'United States')");
-    statement.executeUpdate("insert into COUNTRIES (CODE,COUNTRY_NAME) values ('NL', 'Netherlands')");
-    statement.executeUpdate("insert into COUNTRIES (CODE,COUNTRY_NAME) values ('NP', 'Nepal')");
-    statement.executeUpdate("insert into COUNTRIES (CODE,COUNTRY_NAME) values ('NO', 'Norway')");
-    statement.close();
-    connection.close();
+    try (Connection connection = DriverManager.getConnection("jdbc:hsqldb:mem:.");
+         Statement statement = connection.createStatement()) {
+      statement.executeUpdate("insert into COUNTRIES (CODE,COUNTRY_NAME) values ('US', 'United States')");
+      statement.executeUpdate("insert into COUNTRIES (CODE,COUNTRY_NAME) values ('NL', 'Netherlands')");
+      statement.executeUpdate("insert into COUNTRIES (CODE,COUNTRY_NAME) values ('NP', 'Nepal')");
+      statement.executeUpdate("insert into COUNTRIES (CODE,COUNTRY_NAME) values ('NO', 'Norway')");
+    }
     
     TupleStream stream;
     List<Tuple> tuples;
     
     // Simple 1
-    stream = new JDBCStream("jdbc:hsqldb:mem:.", "select CODE,COUNTRY_NAME from COUNTRIES order by CODE", new FieldComparator("CODE", ComparatorOrder.ASCENDING));
+    stream = new JDBCStream("jdbc:hsqldb:mem:.", "select CODE,COUNTRY_NAME from COUNTRIES order by CODE",
+        new FieldComparator("CODE", ComparatorOrder.ASCENDING));
     tuples = getTuples(stream);
     
     assert(tuples.size() == 4);
@@ -191,7 +137,8 @@ public class JDBCStreamTest extends AbstractFullDistribZkTestBase {
     assertOrderOf(tuples, "COUNTRY_NAME", "Netherlands", "Norway", "Nepal", "United States");
     
     // Simple 2
-    stream = new JDBCStream("jdbc:hsqldb:mem:.", "select CODE,COUNTRY_NAME from COUNTRIES order by COUNTRY_NAME", new FieldComparator("COUNTRY_NAME", ComparatorOrder.ASCENDING));
+    stream = new JDBCStream("jdbc:hsqldb:mem:.", "select CODE,COUNTRY_NAME from COUNTRIES order by COUNTRY_NAME",
+        new FieldComparator("COUNTRY_NAME", ComparatorOrder.ASCENDING));
     tuples = getTuples(stream);
     
     assertEquals(4, tuples.size());
@@ -199,29 +146,28 @@ public class JDBCStreamTest extends AbstractFullDistribZkTestBase {
     assertOrderOf(tuples, "COUNTRY_NAME", "Nepal", "Netherlands", "Norway", "United States");
     
   }
-  
-  private void testJDBCJoin() throws Exception {
-    clearData();
+
+  @Test
+  public void testJDBCJoin() throws Exception {
     
     // Load Database Data
-    Connection connection = DriverManager.getConnection("jdbc:hsqldb:mem:.");
-    Statement statement = connection.createStatement();
-    statement.executeUpdate("insert into COUNTRIES (CODE,COUNTRY_NAME) values ('US', 'United States')");
-    statement.executeUpdate("insert into COUNTRIES (CODE,COUNTRY_NAME) values ('NL', 'Netherlands')");
-    statement.executeUpdate("insert into COUNTRIES (CODE,COUNTRY_NAME) values ('NP', 'Nepal')");
-    statement.executeUpdate("insert into COUNTRIES (CODE,COUNTRY_NAME) values ('NO', 'Norway')");
-    statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (11,'Emma','NL')");
-    statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (12,'Grace','NI')");
-    statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (13,'Hailey','NG')");
-    statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (14,'Isabella','NF')");
-    statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (15,'Lily','NE')");
-    statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (16,'Madison','NC')");
-    statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (17,'Mia','NL')");
-    statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (18,'Natalie','NZ')");
-    statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (19,'Olivia','NL')");
-    statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (20,'Samantha','NR')");
-    statement.close();
-    connection.close();
+    try (Connection connection = DriverManager.getConnection("jdbc:hsqldb:mem:.");
+          Statement statement = connection.createStatement()) {
+      statement.executeUpdate("insert into COUNTRIES (CODE,COUNTRY_NAME) values ('US', 'United States')");
+      statement.executeUpdate("insert into COUNTRIES (CODE,COUNTRY_NAME) values ('NL', 'Netherlands')");
+      statement.executeUpdate("insert into COUNTRIES (CODE,COUNTRY_NAME) values ('NP', 'Nepal')");
+      statement.executeUpdate("insert into COUNTRIES (CODE,COUNTRY_NAME) values ('NO', 'Norway')");
+      statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (11,'Emma','NL')");
+      statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (12,'Grace','NI')");
+      statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (13,'Hailey','NG')");
+      statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (14,'Isabella','NF')");
+      statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (15,'Lily','NE')");
+      statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (16,'Madison','NC')");
+      statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (17,'Mia','NL')");
+      statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (18,'Natalie','NZ')");
+      statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (19,'Olivia','NL')");
+      statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (20,'Samantha','NR')");
+    }
     
     TupleStream stream;
     List<Tuple> tuples;
@@ -234,28 +180,28 @@ public class JDBCStreamTest extends AbstractFullDistribZkTestBase {
     assertOrderOf(tuples, "ID", 11, 17, 19);
     assertOrderOf(tuples, "NAME", "Emma", "Mia", "Olivia");    
   }
-  
-  private void testJDBCSolrMerge() throws Exception {
-    clearData();
+
+  @Test
+  public void testJDBCSolrMerge() throws Exception {
     
     // Load Database Data
-    Connection connection = DriverManager.getConnection("jdbc:hsqldb:mem:.");
-    Statement statement = connection.createStatement();
-    statement.executeUpdate("insert into COUNTRIES (CODE,COUNTRY_NAME) values ('US', 'United States')");
-    statement.executeUpdate("insert into COUNTRIES (CODE,COUNTRY_NAME) values ('NL', 'Netherlands')");
-    statement.executeUpdate("insert into COUNTRIES (CODE,COUNTRY_NAME) values ('NP', 'Nepal')");
-    statement.executeUpdate("insert into COUNTRIES (CODE,COUNTRY_NAME) values ('NO', 'Norway')");
-    statement.executeUpdate("insert into COUNTRIES (CODE,COUNTRY_NAME) values ('AL', 'Algeria')");
-    statement.close();
-    connection.close();
+    try (Connection connection = DriverManager.getConnection("jdbc:hsqldb:mem:.");
+         Statement statement = connection.createStatement()) {
+      statement.executeUpdate("insert into COUNTRIES (CODE,COUNTRY_NAME) values ('US', 'United States')");
+      statement.executeUpdate("insert into COUNTRIES (CODE,COUNTRY_NAME) values ('NL', 'Netherlands')");
+      statement.executeUpdate("insert into COUNTRIES (CODE,COUNTRY_NAME) values ('NP', 'Nepal')");
+      statement.executeUpdate("insert into COUNTRIES (CODE,COUNTRY_NAME) values ('NO', 'Norway')");
+      statement.executeUpdate("insert into COUNTRIES (CODE,COUNTRY_NAME) values ('AL', 'Algeria')");
+    }
     
     // Load Solr
-    indexr(id, "0", "code_s", "GB", "name_s", "Great Britian");
-    indexr(id, "1", "code_s", "CA", "name_s", "Canada");
-    commit();
+    new UpdateRequest()
+        .add(id, "0", "code_s", "GB", "name_s", "Great Britian")
+        .add(id, "1", "code_s", "CA", "name_s", "Canada")
+        .commit(cluster.getSolrClient(), COLLECTION);
     
     StreamFactory factory = new StreamFactory()
-      .withCollectionZkHost("collection1", zkServer.getZkAddress())
+      .withCollectionZkHost(COLLECTION, cluster.getZkServer().getZkAddress())
       .withFunctionName("search", CloudSolrStream.class);
     
     List<Tuple> tuples;
@@ -263,7 +209,7 @@ public class JDBCStreamTest extends AbstractFullDistribZkTestBase {
     // Simple 1
     TupleStream jdbcStream = new JDBCStream("jdbc:hsqldb:mem:.", "select CODE,COUNTRY_NAME from COUNTRIES order by CODE", new FieldComparator("CODE", ComparatorOrder.ASCENDING));
     TupleStream selectStream = new SelectStream(jdbcStream, new HashMap<String, String>(){{ put("CODE", "code_s"); put("COUNTRY_NAME", "name_s"); }});
-    TupleStream searchStream = factory.constructStream("search(collection1, fl=\"code_s,name_s\",q=\"*:*\",sort=\"code_s asc\")");
+    TupleStream searchStream = factory.constructStream("search(" + COLLECTION + ", fl=\"code_s,name_s\",q=\"*:*\",sort=\"code_s asc\")");
     TupleStream mergeStream = new MergeStream(new FieldComparator("code_s", ComparatorOrder.ASCENDING), new TupleStream[]{selectStream,searchStream});
     
     tuples = getTuples(mergeStream);
@@ -272,49 +218,49 @@ public class JDBCStreamTest extends AbstractFullDistribZkTestBase {
     assertOrderOf(tuples, "code_s", "AL","CA","GB","NL","NO","NP","US");
     assertOrderOf(tuples, "name_s", "Algeria", "Canada", "Great Britian", "Netherlands", "Norway", "Nepal", "United States");
   }
-  
-  private void testJDBCSolrInnerJoinExpression() throws Exception{
-    clearData();
+
+  @Test
+  public void testJDBCSolrInnerJoinExpression() throws Exception{
     
     StreamFactory factory = new StreamFactory()
-      .withCollectionZkHost("collection1", zkServer.getZkAddress())
+      .withCollectionZkHost(COLLECTION, cluster.getZkServer().getZkAddress())
       .withFunctionName("search", CloudSolrStream.class)
       .withFunctionName("select", SelectStream.class)
       .withFunctionName("innerJoin", InnerJoinStream.class)
       .withFunctionName("jdbc", JDBCStream.class);
     
     // Load Database Data
-    Connection connection = DriverManager.getConnection("jdbc:hsqldb:mem:.");
-    Statement statement = connection.createStatement();
-    statement.executeUpdate("insert into COUNTRIES (CODE,COUNTRY_NAME) values ('US', 'United States')");
-    statement.executeUpdate("insert into COUNTRIES (CODE,COUNTRY_NAME) values ('NL', 'Netherlands')");
-    statement.executeUpdate("insert into COUNTRIES (CODE,COUNTRY_NAME) values ('NP', 'Nepal')");
-    statement.executeUpdate("insert into COUNTRIES (CODE,COUNTRY_NAME) values ('NO', 'Norway')");
-    statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (11,'Emma','NL')");
-    statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (12,'Grace','US')");
-    statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (13,'Hailey','NL')");
-    statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (14,'Isabella','NL')");
-    statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (15,'Lily','NL')");
-    statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (16,'Madison','US')");
-    statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (17,'Mia','US')");
-    statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (18,'Natalie','NL')");
-    statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (19,'Olivia','NL')");
-    statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (20,'Samantha','US')");
-    statement.close();
-    connection.close();
+    try (Connection connection = DriverManager.getConnection("jdbc:hsqldb:mem:.");
+         Statement statement = connection.createStatement()) {
+      statement.executeUpdate("insert into COUNTRIES (CODE,COUNTRY_NAME) values ('US', 'United States')");
+      statement.executeUpdate("insert into COUNTRIES (CODE,COUNTRY_NAME) values ('NL', 'Netherlands')");
+      statement.executeUpdate("insert into COUNTRIES (CODE,COUNTRY_NAME) values ('NP', 'Nepal')");
+      statement.executeUpdate("insert into COUNTRIES (CODE,COUNTRY_NAME) values ('NO', 'Norway')");
+      statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (11,'Emma','NL')");
+      statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (12,'Grace','US')");
+      statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (13,'Hailey','NL')");
+      statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (14,'Isabella','NL')");
+      statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (15,'Lily','NL')");
+      statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (16,'Madison','US')");
+      statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (17,'Mia','US')");
+      statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (18,'Natalie','NL')");
+      statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (19,'Olivia','NL')");
+      statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (20,'Samantha','US')");
+    }
     
     // Load solr data
-    indexr(id, "1", "rating_f", "3.5", "personId_i", "11");
-    indexr(id, "2", "rating_f", "5", "personId_i", "12");
-    indexr(id, "3", "rating_f", "2.2", "personId_i", "13");
-    indexr(id, "4", "rating_f", "4.3", "personId_i", "14");
-    indexr(id, "5", "rating_f", "3.5", "personId_i", "15");
-    indexr(id, "6", "rating_f", "3", "personId_i", "16");
-    indexr(id, "7", "rating_f", "3", "personId_i", "17");
-    indexr(id, "8", "rating_f", "4", "personId_i", "18");
-    indexr(id, "9", "rating_f", "4.1", "personId_i", "19");
-    indexr(id, "10", "rating_f", "4.8", "personId_i", "20");
-    commit();
+    new UpdateRequest()
+        .add(id, "1", "rating_f", "3.5", "personId_i", "11")
+        .add(id, "2", "rating_f", "5", "personId_i", "12")
+        .add(id, "3", "rating_f", "2.2", "personId_i", "13")
+        .add(id, "4", "rating_f", "4.3", "personId_i", "14")
+        .add(id, "5", "rating_f", "3.5", "personId_i", "15")
+        .add(id, "6", "rating_f", "3", "personId_i", "16")
+        .add(id, "7", "rating_f", "3", "personId_i", "17")
+        .add(id, "8", "rating_f", "4", "personId_i", "18")
+        .add(id, "9", "rating_f", "4.1", "personId_i", "19")
+        .add(id, "10", "rating_f", "4.8", "personId_i", "20")
+        .commit(cluster.getSolrClient(), COLLECTION);
 
     String expression;
     TupleStream stream;
@@ -324,7 +270,7 @@ public class JDBCStreamTest extends AbstractFullDistribZkTestBase {
     expression =   
               "innerJoin("
             + "  select("
-            + "    search(collection1, fl=\"personId_i,rating_f\", q=\"rating_f:*\", sort=\"personId_i asc\"),"
+            + "    search(" + COLLECTION + ", fl=\"personId_i,rating_f\", q=\"rating_f:*\", sort=\"personId_i asc\"),"
             + "    personId_i as personId,"
             + "    rating_f as rating"
             + "  ),"
@@ -347,48 +293,48 @@ public class JDBCStreamTest extends AbstractFullDistribZkTestBase {
     assertOrderOf(tuples, "country", "Netherlands","United States","Netherlands","Netherlands","Netherlands","United States","United States","Netherlands","Netherlands","United States");
   }
 
-  private void testJDBCSolrInnerJoinExpressionWithProperties() throws Exception{
-    clearData();
+  @Test
+  public void testJDBCSolrInnerJoinExpressionWithProperties() throws Exception{
     
     StreamFactory factory = new StreamFactory()
-      .withCollectionZkHost("collection1", zkServer.getZkAddress())
+      .withCollectionZkHost(COLLECTION, cluster.getZkServer().getZkAddress())
       .withFunctionName("search", CloudSolrStream.class)
       .withFunctionName("select", SelectStream.class)
       .withFunctionName("innerJoin", InnerJoinStream.class)
       .withFunctionName("jdbc", JDBCStream.class);
     
     // Load Database Data
-    Connection connection = DriverManager.getConnection("jdbc:hsqldb:mem:.");
-    Statement statement = connection.createStatement();
-    statement.executeUpdate("insert into COUNTRIES (CODE,COUNTRY_NAME) values ('US', 'United States')");
-    statement.executeUpdate("insert into COUNTRIES (CODE,COUNTRY_NAME) values ('NL', 'Netherlands')");
-    statement.executeUpdate("insert into COUNTRIES (CODE,COUNTRY_NAME) values ('NP', 'Nepal')");
-    statement.executeUpdate("insert into COUNTRIES (CODE,COUNTRY_NAME) values ('NO', 'Norway')");
-    statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (11,'Emma','NL')");
-    statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (12,'Grace','US')");
-    statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (13,'Hailey','NL')");
-    statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (14,'Isabella','NL')");
-    statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (15,'Lily','NL')");
-    statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (16,'Madison','US')");
-    statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (17,'Mia','US')");
-    statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (18,'Natalie','NL')");
-    statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (19,'Olivia','NL')");
-    statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (20,'Samantha','US')");
-    statement.close();
-    connection.close();
+    try (Connection connection = DriverManager.getConnection("jdbc:hsqldb:mem:.");
+         Statement statement = connection.createStatement()) {
+      statement.executeUpdate("insert into COUNTRIES (CODE,COUNTRY_NAME) values ('US', 'United States')");
+      statement.executeUpdate("insert into COUNTRIES (CODE,COUNTRY_NAME) values ('NL', 'Netherlands')");
+      statement.executeUpdate("insert into COUNTRIES (CODE,COUNTRY_NAME) values ('NP', 'Nepal')");
+      statement.executeUpdate("insert into COUNTRIES (CODE,COUNTRY_NAME) values ('NO', 'Norway')");
+      statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (11,'Emma','NL')");
+      statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (12,'Grace','US')");
+      statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (13,'Hailey','NL')");
+      statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (14,'Isabella','NL')");
+      statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (15,'Lily','NL')");
+      statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (16,'Madison','US')");
+      statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (17,'Mia','US')");
+      statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (18,'Natalie','NL')");
+      statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (19,'Olivia','NL')");
+      statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (20,'Samantha','US')");
+    }
     
     // Load solr data
-    indexr(id, "1", "rating_f", "3.5", "personId_i", "11");
-    indexr(id, "2", "rating_f", "5", "personId_i", "12");
-    indexr(id, "3", "rating_f", "2.2", "personId_i", "13");
-    indexr(id, "4", "rating_f", "4.3", "personId_i", "14");
-    indexr(id, "5", "rating_f", "3.5", "personId_i", "15");
-    indexr(id, "6", "rating_f", "3", "personId_i", "16");
-    indexr(id, "7", "rating_f", "3", "personId_i", "17");
-    indexr(id, "8", "rating_f", "4", "personId_i", "18");
-    indexr(id, "9", "rating_f", "4.1", "personId_i", "19");
-    indexr(id, "10", "rating_f", "4.8", "personId_i", "20");
-    commit();
+    new UpdateRequest()
+        .add(id, "1", "rating_f", "3.5", "personId_i", "11")
+        .add(id, "2", "rating_f", "5", "personId_i", "12")
+        .add(id, "3", "rating_f", "2.2", "personId_i", "13")
+        .add(id, "4", "rating_f", "4.3", "personId_i", "14")
+        .add(id, "5", "rating_f", "3.5", "personId_i", "15")
+        .add(id, "6", "rating_f", "3", "personId_i", "16")
+        .add(id, "7", "rating_f", "3", "personId_i", "17")
+        .add(id, "8", "rating_f", "4", "personId_i", "18")
+        .add(id, "9", "rating_f", "4.1", "personId_i", "19")
+        .add(id, "10", "rating_f", "4.8", "personId_i", "20")
+        .commit(cluster.getSolrClient(), COLLECTION);
 
     String expression;
     TupleStream stream;
@@ -401,7 +347,7 @@ public class JDBCStreamTest extends AbstractFullDistribZkTestBase {
     expression =   
               "innerJoin("
             + "  select("
-            + "    search(collection1, fl=\"personId_i,rating_f\", q=\"rating_f:*\", sort=\"personId_i asc\"),"
+            + "    search(" + COLLECTION + ", fl=\"personId_i,rating_f\", q=\"rating_f:*\", sort=\"personId_i asc\"),"
             + "    personId_i as personId,"
             + "    rating_f as rating"
             + "  ),"
@@ -430,7 +376,7 @@ public class JDBCStreamTest extends AbstractFullDistribZkTestBase {
     expression =   
               "innerJoin("
             + "  select("
-            + "    search(collection1, fl=\"personId_i,rating_f\", q=\"rating_f:*\", sort=\"personId_i asc\"),"
+            + "    search(" + COLLECTION + ", fl=\"personId_i,rating_f\", q=\"rating_f:*\", sort=\"personId_i asc\"),"
             + "    personId_i as personId,"
             + "    rating_f as rating"
             + "  ),"
@@ -453,12 +399,11 @@ public class JDBCStreamTest extends AbstractFullDistribZkTestBase {
     assertOrderOf(tuples, "country", "Netherlands","United States","Netherlands","Netherlands","Netherlands","United States","United States","Netherlands","Netherlands","United States");
   }
 
-  
-  private void testJDBCSolrInnerJoinRollupExpression() throws Exception{
-    clearData();
+  @Test
+  public void testJDBCSolrInnerJoinRollupExpression() throws Exception{
     
     StreamFactory factory = new StreamFactory()
-      .withCollectionZkHost("collection1", zkServer.getZkAddress())
+      .withCollectionZkHost(COLLECTION, cluster.getZkServer().getZkAddress())
       .withFunctionName("search", CloudSolrStream.class)
       .withFunctionName("select", SelectStream.class)
       .withFunctionName("hashJoin", HashJoinStream.class)
@@ -471,38 +416,37 @@ public class JDBCStreamTest extends AbstractFullDistribZkTestBase {
       ;
     
     // Load Database Data
-    Connection connection = DriverManager.getConnection("jdbc:hsqldb:mem:.");
-    Statement statement = connection.createStatement();
-    statement.executeUpdate("insert into COUNTRIES (CODE,COUNTRY_NAME) values ('US', 'United States')");
-    statement.executeUpdate("insert into COUNTRIES (CODE,COUNTRY_NAME) values ('NL', 'Netherlands')");
-    statement.executeUpdate("insert into COUNTRIES (CODE,COUNTRY_NAME) values ('NP', 'Nepal')");
-    statement.executeUpdate("insert into COUNTRIES (CODE,COUNTRY_NAME) values ('NO', 'Norway')");
-    statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (11,'Emma','NL')");
-    statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (12,'Grace','US')");
-    statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (13,'Hailey','NL')");
-    statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (14,'Isabella','NL')");
-    statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (15,'Lily','NL')");
-    statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (16,'Madison','US')");
-    statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (17,'Mia','US')");
-    statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (18,'Natalie','NL')");
-    statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (19,'Olivia','NL')");
-    statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (20,'Samantha','US')");
-    statement.close();
-    connection.close();
+    try (Connection connection = DriverManager.getConnection("jdbc:hsqldb:mem:.");
+         Statement statement = connection.createStatement()) {
+      statement.executeUpdate("insert into COUNTRIES (CODE,COUNTRY_NAME) values ('US', 'United States')");
+      statement.executeUpdate("insert into COUNTRIES (CODE,COUNTRY_NAME) values ('NL', 'Netherlands')");
+      statement.executeUpdate("insert into COUNTRIES (CODE,COUNTRY_NAME) values ('NP', 'Nepal')");
+      statement.executeUpdate("insert into COUNTRIES (CODE,COUNTRY_NAME) values ('NO', 'Norway')");
+      statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (11,'Emma','NL')");
+      statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (12,'Grace','US')");
+      statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (13,'Hailey','NL')");
+      statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (14,'Isabella','NL')");
+      statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (15,'Lily','NL')");
+      statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (16,'Madison','US')");
+      statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (17,'Mia','US')");
+      statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (18,'Natalie','NL')");
+      statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (19,'Olivia','NL')");
+      statement.executeUpdate("insert into PEOPLE (ID, NAME, COUNTRY_CODE) values (20,'Samantha','US')");
+    }
     
     // Load solr data
-    indexr(id, "1", "rating_f", "3.5", "personId_i", "11");
-    indexr(id, "3", "rating_f", "2.2", "personId_i", "13");
-    indexr(id, "4", "rating_f", "4.3", "personId_i", "14");
-    indexr(id, "5", "rating_f", "3.5", "personId_i", "15");
-    indexr(id, "8", "rating_f", "4", "personId_i", "18");
-    indexr(id, "9", "rating_f", "4.1", "personId_i", "19");
-    
-    indexr(id, "2", "rating_f", "5", "personId_i", "12");
-    indexr(id, "6", "rating_f", "3", "personId_i", "16");
-    indexr(id, "7", "rating_f", "3", "personId_i", "17");
-    indexr(id, "10", "rating_f", "4.8", "personId_i", "20");
-    commit();
+    new UpdateRequest()
+        .add(id, "1", "rating_f", "3.5", "personId_i", "11")
+        .add(id, "3", "rating_f", "2.2", "personId_i", "13")
+        .add(id, "4", "rating_f", "4.3", "personId_i", "14")
+        .add(id, "5", "rating_f", "3.5", "personId_i", "15")
+        .add(id, "8", "rating_f", "4", "personId_i", "18")
+        .add(id, "9", "rating_f", "4.1", "personId_i", "19")
+        .add(id, "2", "rating_f", "5", "personId_i", "12")
+        .add(id, "6", "rating_f", "3", "personId_i", "16")
+        .add(id, "7", "rating_f", "3", "personId_i", "17")
+        .add(id, "10", "rating_f", "4.8", "personId_i", "20")
+        .commit(cluster.getSolrClient(), COLLECTION);
 
     String expression;
     TupleStream stream;
@@ -513,7 +457,7 @@ public class JDBCStreamTest extends AbstractFullDistribZkTestBase {
               "rollup("
             + "  hashJoin("
             + "    hashed=select("
-            + "      search(collection1, fl=\"personId_i,rating_f\", q=\"rating_f:*\", sort=\"personId_i asc\"),"
+            + "      search(" + COLLECTION + ", fl=\"personId_i,rating_f\", q=\"rating_f:*\", sort=\"personId_i asc\"),"
             + "      personId_i as personId,"
             + "      rating_f as rating"
             + "    ),"
@@ -562,6 +506,7 @@ public class JDBCStreamTest extends AbstractFullDistribZkTestBase {
     tupleStream.close();
     return tuples;
   }
+
   protected boolean assertOrderOf(List<Tuple> tuples, String fieldName, int... values) throws Exception {
     int i = 0;
     for(int val : values) {
@@ -574,6 +519,7 @@ public class JDBCStreamTest extends AbstractFullDistribZkTestBase {
     }
     return true;
   }
+
   protected boolean assertOrderOf(List<Tuple> tuples, String fieldName, double... values) throws Exception {
     int i = 0;
     for(double val : values) {
@@ -586,6 +532,7 @@ public class JDBCStreamTest extends AbstractFullDistribZkTestBase {
     }
     return true;
   }
+
   protected boolean assertOrderOf(List<Tuple> tuples, String fieldName, String... values) throws Exception {
     int i = 0;
     for(String val : values) {
@@ -617,6 +564,7 @@ public class JDBCStreamTest extends AbstractFullDistribZkTestBase {
     }
     return true;
   }
+
   protected boolean assertNotFields(List<Tuple> tuples, String ... fields) throws Exception{
     for(Tuple tuple : tuples){
       for(String field : fields){
@@ -649,9 +597,4 @@ public class JDBCStreamTest extends AbstractFullDistribZkTestBase {
     return true;
   }
 
-  @Override
-  protected void indexr(Object... fields) throws Exception {
-    SolrInputDocument doc = getDoc(fields);
-    indexDoc(doc);
-  }
 }
diff --git a/solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamExpressionTest.java b/solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamExpressionTest.java
index 868afd5..9a0653a 100644
--- a/solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamExpressionTest.java
+++ b/solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamExpressionTest.java
@@ -16,41 +16,37 @@
  */
 package org.apache.solr.client.solrj.io.stream;
 
-import java.io.File;
 import java.io.IOException;
 import java.util.ArrayList;
+import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Locale;
 import java.util.Map;
-import java.util.Collections;
 
 import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.util.LuceneTestCase.Slow;
-import org.apache.solr.client.solrj.impl.CloudSolrClient;
+import org.apache.solr.client.solrj.embedded.JettySolrRunner;
 import org.apache.solr.client.solrj.io.SolrClientCache;
 import org.apache.solr.client.solrj.io.Tuple;
-import org.apache.solr.client.solrj.io.ops.ConcatOperation;
-import org.apache.solr.client.solrj.io.ops.GroupOperation;
 import org.apache.solr.client.solrj.io.comp.ComparatorOrder;
 import org.apache.solr.client.solrj.io.comp.FieldComparator;
+import org.apache.solr.client.solrj.io.ops.ConcatOperation;
+import org.apache.solr.client.solrj.io.ops.GroupOperation;
 import org.apache.solr.client.solrj.io.ops.ReplaceOperation;
 import org.apache.solr.client.solrj.io.stream.expr.StreamExpression;
 import org.apache.solr.client.solrj.io.stream.expr.StreamExpressionParser;
 import org.apache.solr.client.solrj.io.stream.expr.StreamFactory;
-import org.apache.solr.client.solrj.io.stream.metrics.Bucket;
 import org.apache.solr.client.solrj.io.stream.metrics.CountMetric;
 import org.apache.solr.client.solrj.io.stream.metrics.MaxMetric;
 import org.apache.solr.client.solrj.io.stream.metrics.MeanMetric;
-import org.apache.solr.client.solrj.io.stream.metrics.Metric;
 import org.apache.solr.client.solrj.io.stream.metrics.MinMetric;
 import org.apache.solr.client.solrj.io.stream.metrics.SumMetric;
-import org.apache.solr.cloud.AbstractFullDistribZkTestBase;
-import org.apache.solr.cloud.AbstractZkTestCase;
-import org.apache.solr.common.SolrInputDocument;
+import org.apache.solr.client.solrj.request.CollectionAdminRequest;
+import org.apache.solr.client.solrj.request.UpdateRequest;
+import org.apache.solr.cloud.AbstractDistribZkTestBase;
+import org.apache.solr.cloud.SolrCloudTestCase;
 import org.apache.solr.common.params.CommonParams;
-import org.junit.After;
-import org.junit.AfterClass;
 import org.junit.Before;
 import org.junit.BeforeClass;
 import org.junit.Test;
@@ -63,122 +59,50 @@ import org.junit.Test;
 
 @Slow
 @LuceneTestCase.SuppressCodecs({"Lucene3x", "Lucene40","Lucene41","Lucene42","Lucene45"})
-public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
-
-  private static final String SOLR_HOME = getFile("solrj" + File.separator + "solr").getAbsolutePath();
-
-  static {
-    schemaString = "schema-streaming.xml";
-  }
-
-  @BeforeClass
-  public static void beforeSuperClass() {
-    AbstractZkTestCase.SOLRHOME = new File(SOLR_HOME());
-  }
-
-  @AfterClass
-  public static void afterSuperClass() {
-
-  }
+public class StreamExpressionTest extends SolrCloudTestCase {
 
-  protected String getCloudSolrConfig() {
-    return "solrconfig-streaming.xml";
-  }
+  private static final String COLLECTION = "collection1";
 
+  private static final int TIMEOUT = 30;
 
-  @Override
-  public String getSolrHome() {
-    return SOLR_HOME;
-  }
+  private static final String id = "id";
 
-  public static String SOLR_HOME() {
-    return SOLR_HOME;
+  @BeforeClass
+  public static void setupCluster() throws Exception {
+    configureCluster(4)
+        .addConfig("conf", getFile("solrj").toPath().resolve("solr").resolve("configsets").resolve("streaming").resolve("conf"))
+        .configure();
+
+    CollectionAdminRequest.createCollection(COLLECTION, "conf", 2, 1).process(cluster.getSolrClient());
+    AbstractDistribZkTestBase.waitForRecoveriesToFinish(COLLECTION, cluster.getSolrClient().getZkStateReader(),
+        false, true, TIMEOUT);
   }
 
   @Before
-  @Override
-  public void setUp() throws Exception {
-    super.setUp();
-    // we expect this time of exception as shards go up and down...
-    //ignoreException(".*");
-
-    System.setProperty("numShards", Integer.toString(sliceCount));
-  }
-
-  @Override
-  @After
-  public void tearDown() throws Exception {
-    super.tearDown();
-    resetExceptionIgnores();
-  }
-
-  public StreamExpressionTest() {
-    super();
-    sliceCount = 2;
+  public void cleanIndex() throws Exception {
+    new UpdateRequest()
+        .deleteByQuery("*:*")
+        .commit(cluster.getSolrClient(), COLLECTION);
   }
 
   @Test
-  public void testAll() throws Exception{
-    assertNotNull(cloudClient);
-
-    handle.clear();
-    handle.put("timestamp", SKIPVAL);
-
-    waitForRecoveriesToFinish(false);
-
-    del("*:*");
-    commit();
-    
-    testCloudSolrStream();
-    testCloudSolrStreamWithZkHost();
-    testMergeStream();
-    testRankStream();
-    testReducerStream();
-    testUniqueStream();
-    testSortStream();
-    testRollupStream();
-    testStatsStream();
-    testNulls();
-    testTopicStream();
-    testDaemonStream();
-    testRandomStream();
-    testParallelUniqueStream();
-    testParallelReducerStream();
-    testParallelRankStream();
-    testParallelMergeStream();
-    testParallelRollupStream();
-    testInnerJoinStream();
-    testLeftOuterJoinStream();
-    testHashJoinStream();
-    testOuterHashJoinStream();
-    testSelectStream();
-    testFacetStream();
-    testSubFacetStream();
-    testUpdateStream();
-    testParallelUpdateStream();
-    testParallelDaemonUpdateStream();
-    testIntersectStream();
-    testParallelIntersectStream();
-    testComplementStream();
-    testParallelComplementStream();
-  }
-
-  private void testCloudSolrStream() throws Exception {
+  public void testCloudSolrStream() throws Exception {
 
-    indexr(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "0");
-    indexr(id, "2", "a_s", "hello2", "a_i", "2", "a_f", "0");
-    indexr(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3");
-    indexr(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4");
-    indexr(id, "1", "a_s", "hello1", "a_i", "1", "a_f", "1");
-    commit();
+    new UpdateRequest()
+        .add(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "0")
+        .add(id, "2", "a_s", "hello2", "a_i", "2", "a_f", "0")
+        .add(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3")
+        .add(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4")
+        .add(id, "1", "a_s", "hello1", "a_i", "1", "a_f", "1")
+        .commit(cluster.getSolrClient(), COLLECTION);
 
-    StreamFactory factory = new StreamFactory().withCollectionZkHost("collection1", zkServer.getZkAddress());
+    StreamFactory factory = new StreamFactory().withCollectionZkHost(COLLECTION, cluster.getZkServer().getZkAddress());
     StreamExpression expression;
     CloudSolrStream stream;
     List<Tuple> tuples;
     
     // Basic test
-    expression = StreamExpressionParser.parse("search(collection1, q=*:*, fl=\"id,a_s,a_i,a_f\", sort=\"a_f asc, a_i asc\")");
+    expression = StreamExpressionParser.parse("search(" + COLLECTION + ", q=*:*, fl=\"id,a_s,a_i,a_f\", sort=\"a_f asc, a_i asc\")");
     stream = new CloudSolrStream(expression, factory);
     tuples = getTuples(stream);
 
@@ -187,7 +111,7 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
     assertLong(tuples.get(0), "a_i", 0);
 
     // Basic w/aliases
-    expression = StreamExpressionParser.parse("search(collection1, q=*:*, fl=\"id,a_s,a_i,a_f\", sort=\"a_f asc, a_i asc\", aliases=\"a_i=alias.a_i, a_s=name\")");
+    expression = StreamExpressionParser.parse("search(" + COLLECTION + ", q=*:*, fl=\"id,a_s,a_i,a_f\", sort=\"a_f asc, a_i asc\", aliases=\"a_i=alias.a_i, a_s=name\")");
     stream = new CloudSolrStream(expression, factory);
     tuples = getTuples(stream);
 
@@ -197,26 +121,26 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
     assertString(tuples.get(0), "name", "hello0");
 
     // Basic filtered test
-    expression = StreamExpressionParser.parse("search(collection1, q=\"id:(0 3 4)\", fl=\"id,a_s,a_i,a_f\", sort=\"a_f asc, a_i asc\")");
+    expression = StreamExpressionParser.parse("search(" + COLLECTION + ", q=\"id:(0 3 4)\", fl=\"id,a_s,a_i,a_f\", sort=\"a_f asc, a_i asc\")");
     stream = new CloudSolrStream(expression, factory);
     tuples = getTuples(stream);
 
     assert(tuples.size() == 3);
     assertOrder(tuples, 0, 3, 4);
     assertLong(tuples.get(1), "a_i", 3);
-    
-    del("*:*");
-    commit();
+
   }
 
-  private void testCloudSolrStreamWithZkHost() throws Exception {
+  @Test
+  public void testCloudSolrStreamWithZkHost() throws Exception {
 
-    indexr(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "0");
-    indexr(id, "2", "a_s", "hello2", "a_i", "2", "a_f", "0");
-    indexr(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3");
-    indexr(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4");
-    indexr(id, "1", "a_s", "hello1", "a_i", "1", "a_f", "1");
-    commit();
+    new UpdateRequest()
+        .add(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "0")
+        .add(id, "2", "a_s", "hello2", "a_i", "2", "a_f", "0")
+        .add(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3")
+        .add(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4")
+        .add(id, "1", "a_s", "hello1", "a_i", "1", "a_f", "1")
+        .commit(cluster.getSolrClient(), COLLECTION);
 
     StreamFactory factory = new StreamFactory();
     StreamExpression expression;
@@ -224,7 +148,7 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
     List<Tuple> tuples;
     
     // Basic test
-    expression = StreamExpressionParser.parse("search(collection1, zkHost=" + zkServer.getZkAddress() + ", q=*:*, fl=\"id,a_s,a_i,a_f\", sort=\"a_f asc, a_i asc\")");
+    expression = StreamExpressionParser.parse("search(" + COLLECTION + ", zkHost=" + cluster.getZkServer().getZkAddress() + ", q=*:*, fl=\"id,a_s,a_i,a_f\", sort=\"a_f asc, a_i asc\")");
     stream = new CloudSolrStream(expression, factory);
     tuples = getTuples(stream);
 
@@ -233,7 +157,7 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
     assertLong(tuples.get(0), "a_i", 0);
 
     // Basic w/aliases
-    expression = StreamExpressionParser.parse("search(collection1, q=*:*, fl=\"id,a_s,a_i,a_f\", sort=\"a_f asc, a_i asc\", aliases=\"a_i=alias.a_i, a_s=name\", zkHost=" + zkServer.getZkAddress() + ")");
+    expression = StreamExpressionParser.parse("search(" + COLLECTION + ", q=*:*, fl=\"id,a_s,a_i,a_f\", sort=\"a_f asc, a_i asc\", aliases=\"a_i=alias.a_i, a_s=name\", zkHost=" + cluster.getZkServer().getZkAddress() + ")");
     stream = new CloudSolrStream(expression, factory);
     tuples = getTuples(stream);
 
@@ -243,39 +167,39 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
     assertString(tuples.get(0), "name", "hello0");
 
     // Basic filtered test
-    expression = StreamExpressionParser.parse("search(collection1, q=\"id:(0 3 4)\", fl=\"id,a_s,a_i,a_f\", zkHost=" + zkServer.getZkAddress() + ", sort=\"a_f asc, a_i asc\")");
+    expression = StreamExpressionParser.parse("search(" + COLLECTION + ", q=\"id:(0 3 4)\", fl=\"id,a_s,a_i,a_f\", zkHost="
+        + cluster.getZkServer().getZkAddress() + ", sort=\"a_f asc, a_i asc\")");
     stream = new CloudSolrStream(expression, factory);
     tuples = getTuples(stream);
 
     assert(tuples.size() == 3);
     assertOrder(tuples, 0, 3, 4);
     assertLong(tuples.get(1), "a_i", 3);
-    
-    del("*:*");
-    commit();
+
   }
 
-  
-  private void testUniqueStream() throws Exception {
+  @Test
+  public void testUniqueStream() throws Exception {
 
-    indexr(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "0");
-    indexr(id, "2", "a_s", "hello2", "a_i", "2", "a_f", "0");
-    indexr(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3");
-    indexr(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4");
-    indexr(id, "1", "a_s", "hello1", "a_i", "1", "a_f", "1");
-    commit();
+    new UpdateRequest()
+        .add(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "0")
+        .add(id, "2", "a_s", "hello2", "a_i", "2", "a_f", "0")
+        .add(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3")
+        .add(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4")
+        .add(id, "1", "a_s", "hello1", "a_i", "1", "a_f", "1")
+        .commit(cluster.getSolrClient(), COLLECTION);
 
     StreamExpression expression;
     TupleStream stream;
     List<Tuple> tuples;
     
     StreamFactory factory = new StreamFactory()
-      .withCollectionZkHost("collection1", zkServer.getZkAddress())
+      .withCollectionZkHost(COLLECTION, cluster.getZkServer().getZkAddress())
       .withFunctionName("search", CloudSolrStream.class)
       .withFunctionName("unique", UniqueStream.class);
     
     // Basic test
-    expression = StreamExpressionParser.parse("unique(search(collection1, q=*:*, fl=\"id,a_s,a_i,a_f\", sort=\"a_f asc, a_i asc\"), over=\"a_f\")");
+    expression = StreamExpressionParser.parse("unique(search(" + COLLECTION + ", q=*:*, fl=\"id,a_s,a_i,a_f\", sort=\"a_f asc, a_i asc\"), over=\"a_f\")");
     stream = new UniqueStream(expression, factory);
     tuples = getTuples(stream);
     
@@ -283,7 +207,7 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
     assertOrder(tuples, 0, 1, 3, 4);
 
     // Basic test desc
-    expression = StreamExpressionParser.parse("unique(search(collection1, q=*:*, fl=\"id,a_s,a_i,a_f\", sort=\"a_f desc, a_i desc\"), over=\"a_f\")");
+    expression = StreamExpressionParser.parse("unique(search(" + COLLECTION + ", q=*:*, fl=\"id,a_s,a_i,a_f\", sort=\"a_f desc, a_i desc\"), over=\"a_f\")");
     stream = new UniqueStream(expression, factory);
     tuples = getTuples(stream);
     
@@ -291,7 +215,7 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
     assertOrder(tuples, 4,3,1,2);
     
     // Basic w/multi comp
-    expression = StreamExpressionParser.parse("unique(search(collection1, q=*:*, fl=\"id,a_s,a_i,a_f\", sort=\"a_f asc, a_i asc\"), over=\"a_f, a_i\")");
+    expression = StreamExpressionParser.parse("unique(search(" + COLLECTION + ", q=*:*, fl=\"id,a_s,a_i,a_f\", sort=\"a_f asc, a_i asc\"), over=\"a_f, a_i\")");
     stream = new UniqueStream(expression, factory);
     tuples = getTuples(stream);
     
@@ -299,76 +223,75 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
     assertOrder(tuples, 0,2,1,3,4);
     
     // full factory w/multi comp
-    stream = factory.constructStream("unique(search(collection1, q=*:*, fl=\"id,a_s,a_i,a_f\", sort=\"a_f asc, a_i asc\"), over=\"a_f, a_i\")");
+    stream = factory.constructStream("unique(search(" + COLLECTION + ", q=*:*, fl=\"id,a_s,a_i,a_f\", sort=\"a_f asc, a_i asc\"), over=\"a_f, a_i\")");
     tuples = getTuples(stream);
     
     assert(tuples.size() == 5);
     assertOrder(tuples, 0, 2, 1, 3, 4);
-    
-    del("*:*");
-    commit();
+
   }
 
-  private void testSortStream() throws Exception {
+  @Test
+  public void testSortStream() throws Exception {
 
-    indexr(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "0");
-    indexr(id, "2", "a_s", "hello2", "a_i", "2", "a_f", "0");
-    indexr(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3");
-    indexr(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4");
-    indexr(id, "1", "a_s", "hello1", "a_i", "1", "a_f", "1");
-    indexr(id, "5", "a_s", "hello1", "a_i", "1", "a_f", "2");
-    commit();
+    new UpdateRequest()
+        .add(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "0")
+        .add(id, "2", "a_s", "hello2", "a_i", "2", "a_f", "0")
+        .add(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3")
+        .add(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4")
+        .add(id, "1", "a_s", "hello1", "a_i", "1", "a_f", "1")
+        .add(id, "5", "a_s", "hello1", "a_i", "1", "a_f", "2")
+        .commit(cluster.getSolrClient(), COLLECTION);
 
     StreamExpression expression;
     TupleStream stream;
     List<Tuple> tuples;
     
     StreamFactory factory = new StreamFactory()
-      .withCollectionZkHost("collection1", zkServer.getZkAddress())
+      .withCollectionZkHost(COLLECTION, cluster.getZkServer().getZkAddress())
       .withFunctionName("search", CloudSolrStream.class)
       .withFunctionName("sort", SortStream.class);
     
     // Basic test
-    stream = factory.constructStream("sort(search(collection1, q=*:*, fl=\"id,a_s,a_i,a_f\", sort=\"a_f asc\"), by=\"a_i asc\")");
+    stream = factory.constructStream("sort(search(" + COLLECTION + ", q=*:*, fl=\"id,a_s,a_i,a_f\", sort=\"a_f asc\"), by=\"a_i asc\")");
     tuples = getTuples(stream);
     assert(tuples.size() == 6);
     assertOrder(tuples, 0,1,5,2,3,4);
 
     // Basic test desc
-    stream = factory.constructStream("sort(search(collection1, q=*:*, fl=\"id,a_s,a_i,a_f\", sort=\"a_f asc\"), by=\"a_i desc\")");
+    stream = factory.constructStream("sort(search(" + COLLECTION + ", q=*:*, fl=\"id,a_s,a_i,a_f\", sort=\"a_f asc\"), by=\"a_i desc\")");
     tuples = getTuples(stream);
     assert(tuples.size() == 6);
     assertOrder(tuples, 4,3,2,1,5,0);
     
     // Basic w/multi comp
-    stream = factory.constructStream("sort(search(collection1, q=*:*, fl=\"id,a_s,a_i,a_f\", sort=\"a_f asc\"), by=\"a_i asc, a_f desc\")");
+    stream = factory.constructStream("sort(search(" + COLLECTION + ", q=*:*, fl=\"id,a_s,a_i,a_f\", sort=\"a_f asc\"), by=\"a_i asc, a_f desc\")");
     tuples = getTuples(stream);
     assert(tuples.size() == 6);
     assertOrder(tuples, 0,5,1,2,3,4);
-        
-    del("*:*");
-    commit();
-  }
 
+  }
 
-  private void testNulls() throws Exception {
+  @Test
+  public void testNulls() throws Exception {
 
-    indexr(id, "0",                  "a_i", "1", "a_f", "0", "s_multi", "aaa", "s_multi", "bbb", "i_multi", "100", "i_multi", "200");
-    indexr(id, "2", "a_s", "hello2", "a_i", "3", "a_f", "0");
-    indexr(id, "3", "a_s", "hello3", "a_i", "4", "a_f", "3");
-    indexr(id, "4", "a_s", "hello4",             "a_f", "4");
-    indexr(id, "1", "a_s", "hello1", "a_i", "2", "a_f", "1");
-    commit();
+    new UpdateRequest()
+        .add(id, "0",                  "a_i", "1", "a_f", "0", "s_multi", "aaa", "s_multi", "bbb", "i_multi", "100", "i_multi", "200")
+        .add(id, "2", "a_s", "hello2", "a_i", "3", "a_f", "0")
+        .add(id, "3", "a_s", "hello3", "a_i", "4", "a_f", "3")
+        .add(id, "4", "a_s", "hello4",             "a_f", "4")
+        .add(id, "1", "a_s", "hello1", "a_i", "2", "a_f", "1")
+        .commit(cluster.getSolrClient(), COLLECTION);
 
     StreamExpression expression;
     TupleStream stream;
     List<Tuple> tuples;
     Tuple tuple;
     StreamFactory factory = new StreamFactory()
-        .withCollectionZkHost("collection1", zkServer.getZkAddress())
+        .withCollectionZkHost(COLLECTION, cluster.getZkServer().getZkAddress())
         .withFunctionName("search", CloudSolrStream.class);
     // Basic test
-    expression = StreamExpressionParser.parse("search(collection1, q=*:*, fl=\"id,a_s,a_i,a_f, s_multi, i_multi\", qt=\"/export\", sort=\"a_i asc\")");
+    expression = StreamExpressionParser.parse("search(" + COLLECTION + ", q=*:*, fl=\"id,a_s,a_i,a_f, s_multi, i_multi\", qt=\"/export\", sort=\"a_i asc\")");
     stream = new CloudSolrStream(expression, factory);
     tuples = getTuples(stream);
 
@@ -392,7 +315,7 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
     assertNotNull(longs);
 
     //test sort (asc) with null string field. Null should sort to the top.
-    expression = StreamExpressionParser.parse("search(collection1, q=*:*, fl=\"id,a_s,a_i,a_f, s_multi, i_multi\", qt=\"/export\", sort=\"a_s asc\")");
+    expression = StreamExpressionParser.parse("search(" + COLLECTION + ", q=*:*, fl=\"id,a_s,a_i,a_f, s_multi, i_multi\", qt=\"/export\", sort=\"a_s asc\")");
     stream = new CloudSolrStream(expression, factory);
     tuples = getTuples(stream);
 
@@ -400,41 +323,40 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
     assertOrder(tuples, 0, 1, 2, 3, 4);
 
     //test sort(desc) with null string field.  Null should sort to the bottom.
-    expression = StreamExpressionParser.parse("search(collection1, q=*:*, fl=\"id,a_s,a_i,a_f, s_multi, i_multi\", qt=\"/export\", sort=\"a_s desc\")");
+    expression = StreamExpressionParser.parse("search(" + COLLECTION + ", q=*:*, fl=\"id,a_s,a_i,a_f, s_multi, i_multi\", qt=\"/export\", sort=\"a_s desc\")");
     stream = new CloudSolrStream(expression, factory);
     tuples = getTuples(stream);
 
     assert(tuples.size() == 5);
     assertOrder(tuples, 4, 3, 2, 1, 0);
 
-    del("*:*");
-    commit();
   }
 
-  
-  private void testMergeStream() throws Exception {
+  @Test
+  public void testMergeStream() throws Exception {
 
-    indexr(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "0");
-    indexr(id, "2", "a_s", "hello2", "a_i", "2", "a_f", "0");
-    indexr(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3");
-    indexr(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4");
-    indexr(id, "1", "a_s", "hello1", "a_i", "1", "a_f", "1");
-    commit();
+    new UpdateRequest()
+        .add(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "0")
+        .add(id, "2", "a_s", "hello2", "a_i", "2", "a_f", "0")
+        .add(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3")
+        .add(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4")
+        .add(id, "1", "a_s", "hello1", "a_i", "1", "a_f", "1")
+        .commit(cluster.getSolrClient(), COLLECTION);
 
     StreamExpression expression;
     TupleStream stream;
     List<Tuple> tuples;
     
     StreamFactory factory = new StreamFactory()
-      .withCollectionZkHost("collection1", zkServer.getZkAddress())
+      .withCollectionZkHost(COLLECTION, cluster.getZkServer().getZkAddress())
       .withFunctionName("search", CloudSolrStream.class)
       .withFunctionName("unique", UniqueStream.class)
       .withFunctionName("merge", MergeStream.class);
     
     // Basic test
     expression = StreamExpressionParser.parse("merge("
-        + "search(collection1, q=\"id:(0 3 4)\", fl=\"id,a_s,a_i,a_f\", sort=\"a_f asc\"),"
-        + "search(collection1, q=\"id:(1)\", fl=\"id,a_s,a_i,a_f\", sort=\"a_f asc\"),"
+        + "search(" + COLLECTION + ", q=\"id:(0 3 4)\", fl=\"id,a_s,a_i,a_f\", sort=\"a_f asc\"),"
+        + "search(" + COLLECTION + ", q=\"id:(1)\", fl=\"id,a_s,a_i,a_f\", sort=\"a_f asc\"),"
         + "on=\"a_f asc\")");
     stream = new MergeStream(expression, factory);
     tuples = getTuples(stream);
@@ -444,8 +366,8 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
 
     // Basic test desc
     expression = StreamExpressionParser.parse("merge("
-        + "search(collection1, q=\"id:(0 3 4)\", fl=\"id,a_s,a_i,a_f\", sort=\"a_f desc\"),"
-        + "search(collection1, q=\"id:(1)\", fl=\"id,a_s,a_i,a_f\", sort=\"a_f desc\"),"
+        + "search(" + COLLECTION + ", q=\"id:(0 3 4)\", fl=\"id,a_s,a_i,a_f\", sort=\"a_f desc\"),"
+        + "search(" + COLLECTION + ", q=\"id:(1)\", fl=\"id,a_s,a_i,a_f\", sort=\"a_f desc\"),"
         + "on=\"a_f desc\")");
     stream = new MergeStream(expression, factory);
     tuples = getTuples(stream);
@@ -455,8 +377,8 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
     
     // Basic w/multi comp
     expression = StreamExpressionParser.parse("merge("
-        + "search(collection1, q=\"id:(0 3 4)\", fl=\"id,a_s,a_i,a_f\", sort=\"a_f asc, a_s asc\"),"
-        + "search(collection1, q=\"id:(1 2)\", fl=\"id,a_s,a_i,a_f\", sort=\"a_f asc, a_s asc\"),"
+        + "search(" + COLLECTION + ", q=\"id:(0 3 4)\", fl=\"id,a_s,a_i,a_f\", sort=\"a_f asc, a_s asc\"),"
+        + "search(" + COLLECTION + ", q=\"id:(1 2)\", fl=\"id,a_s,a_i,a_f\", sort=\"a_f asc, a_s asc\"),"
         + "on=\"a_f asc, a_s asc\")");
     stream = new MergeStream(expression, factory);
     tuples = getTuples(stream);
@@ -466,8 +388,8 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
     
     // full factory w/multi comp
     stream = factory.constructStream("merge("
-        + "search(collection1, q=\"id:(0 3 4)\", fl=\"id,a_s,a_i,a_f\", sort=\"a_f asc, a_s asc\"),"
-        + "search(collection1, q=\"id:(1 2)\", fl=\"id,a_s,a_i,a_f\", sort=\"a_f asc, a_s asc\"),"
+        + "search(" + COLLECTION + ", q=\"id:(0 3 4)\", fl=\"id,a_s,a_i,a_f\", sort=\"a_f asc, a_s asc\"),"
+        + "search(" + COLLECTION + ", q=\"id:(1 2)\", fl=\"id,a_s,a_i,a_f\", sort=\"a_f asc, a_s asc\"),"
         + "on=\"a_f asc, a_s asc\")");
     tuples = getTuples(stream);
     
@@ -476,34 +398,34 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
     
     // full factory w/multi streams
     stream = factory.constructStream("merge("
-        + "search(collection1, q=\"id:(0 4)\", fl=\"id,a_s,a_i,a_f\", sort=\"a_f asc, a_s asc\"),"
-        + "search(collection1, q=\"id:(1)\", fl=\"id,a_s,a_i,a_f\", sort=\"a_f asc, a_s asc\"),"
-        + "search(collection1, q=\"id:(2)\", fl=\"id,a_s,a_i,a_f\", sort=\"a_f asc, a_s asc\"),"
+        + "search(" + COLLECTION + ", q=\"id:(0 4)\", fl=\"id,a_s,a_i,a_f\", sort=\"a_f asc, a_s asc\"),"
+        + "search(" + COLLECTION + ", q=\"id:(1)\", fl=\"id,a_s,a_i,a_f\", sort=\"a_f asc, a_s asc\"),"
+        + "search(" + COLLECTION + ", q=\"id:(2)\", fl=\"id,a_s,a_i,a_f\", sort=\"a_f asc, a_s asc\"),"
         + "on=\"a_f asc\")");
     tuples = getTuples(stream);
     
     assert(tuples.size() == 4);
     assertOrder(tuples, 0, 2, 1, 4);
-    
-    del("*:*");
-    commit();
+
   }
-  
-  private void testRankStream() throws Exception {
 
-    indexr(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "0");
-    indexr(id, "2", "a_s", "hello2", "a_i", "2", "a_f", "0");
-    indexr(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3");
-    indexr(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4");
-    indexr(id, "1", "a_s", "hello1", "a_i", "1", "a_f", "1");
-    commit();
+  @Test
+  public void testRankStream() throws Exception {
+
+    new UpdateRequest()
+        .add(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "0")
+        .add(id, "2", "a_s", "hello2", "a_i", "2", "a_f", "0")
+        .add(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3")
+        .add(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4")
+        .add(id, "1", "a_s", "hello1", "a_i", "1", "a_f", "1")
+        .commit(cluster.getSolrClient(), COLLECTION);
 
     StreamExpression expression;
     TupleStream stream;
     List<Tuple> tuples;
     
     StreamFactory factory = new StreamFactory()
-      .withCollectionZkHost("collection1", zkServer.getZkAddress())
+      .withCollectionZkHost(COLLECTION, cluster.getZkServer().getZkAddress())
       .withFunctionName("search", CloudSolrStream.class)
       .withFunctionName("unique", UniqueStream.class)
       .withFunctionName("top", RankStream.class);
@@ -511,7 +433,7 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
     // Basic test
     expression = StreamExpressionParser.parse("top("
                                               + "n=3,"
-                                              + "search(collection1, q=*:*, fl=\"id,a_s,a_i,a_f\", sort=\"a_f asc, a_i asc\"),"
+                                              + "search(" + COLLECTION + ", q=*:*, fl=\"id,a_s,a_i,a_f\", sort=\"a_f asc, a_i asc\"),"
                                               + "sort=\"a_f asc, a_i asc\")");
     stream = new RankStream(expression, factory);
     tuples = getTuples(stream);
@@ -523,7 +445,7 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
     expression = StreamExpressionParser.parse("top("
                                               + "n=2,"
                                               + "unique("
-                                              +   "search(collection1, q=*:*, fl=\"id,a_s,a_i,a_f\", sort=\"a_f desc\"),"
+                                              +   "search(" + COLLECTION + ", q=*:*, fl=\"id,a_s,a_i,a_f\", sort=\"a_f desc\"),"
                                               +   "over=\"a_f\"),"
                                               + "sort=\"a_f desc\")");
     stream = new RankStream(expression, factory);
@@ -536,7 +458,7 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
     stream = factory.constructStream("top("
                                     + "n=4,"
                                     + "unique("
-                                    +   "search(collection1, q=*:*, fl=\"id,a_s,a_i,a_f\", sort=\"a_f asc, a_i asc\"),"
+                                    +   "search(" + COLLECTION + ", q=*:*, fl=\"id,a_s,a_i,a_f\", sort=\"a_f asc, a_i asc\"),"
                                     +   "over=\"a_f\"),"
                                     + "sort=\"a_f asc\")");
     tuples = getTuples(stream);
@@ -548,32 +470,32 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
     stream = factory.constructStream("top("
             + "n=4,"
             + "unique("
-            +   "search(collection1, q=*:*, fl=\"id,a_s,a_i,a_f\", sort=\"a_f desc, a_i desc\"),"
+            +   "search(" + COLLECTION + ", q=*:*, fl=\"id,a_s,a_i,a_f\", sort=\"a_f desc, a_i desc\"),"
             +   "over=\"a_f\"),"
             + "sort=\"a_f asc\")");
     tuples = getTuples(stream);
     
     assert(tuples.size() == 4);
     assertOrder(tuples, 2,1,3,4);
-    
-    del("*:*");
-    commit();
+
   }
 
-  private void testRandomStream() throws Exception {
+  @Test
+  public void testRandomStream() throws Exception {
 
-    indexr(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "0");
-    indexr(id, "2", "a_s", "hello2", "a_i", "2", "a_f", "0");
-    indexr(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3");
-    indexr(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4");
-    indexr(id, "1", "a_s", "hello1", "a_i", "1", "a_f", "1");
-    commit();
+    new UpdateRequest()
+        .add(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "0")
+        .add(id, "2", "a_s", "hello2", "a_i", "2", "a_f", "0")
+        .add(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3")
+        .add(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4")
+        .add(id, "1", "a_s", "hello1", "a_i", "1", "a_f", "1")
+        .commit(cluster.getSolrClient(), COLLECTION);
 
     StreamExpression expression;
     TupleStream stream;
 
     StreamFactory factory = new StreamFactory()
-        .withCollectionZkHost("collection1", zkServer.getZkAddress())
+        .withCollectionZkHost(COLLECTION, cluster.getZkServer().getZkAddress())
         .withFunctionName("random", RandomStream.class);
 
 
@@ -582,13 +504,13 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
     try {
       context.setSolrClientCache(cache);
 
-      expression = StreamExpressionParser.parse("random(collection1, q=\"*:*\", rows=\"10\", fl=\"id, a_i\")");
+      expression = StreamExpressionParser.parse("random(" + COLLECTION + ", q=\"*:*\", rows=\"10\", fl=\"id, a_i\")");
       stream = factory.constructStream(expression);
       stream.setStreamContext(context);
       List<Tuple> tuples1 = getTuples(stream);
       assert (tuples1.size() == 5);
 
-      expression = StreamExpressionParser.parse("random(collection1, q=\"*:*\", rows=\"10\", fl=\"id, a_i\")");
+      expression = StreamExpressionParser.parse("random(" + COLLECTION + ", q=\"*:*\", rows=\"10\", fl=\"id, a_i\")");
       stream = factory.constructStream(expression);
       stream.setStreamContext(context);
       List<Tuple> tuples2 = getTuples(stream);
@@ -617,7 +539,7 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
         }
       }
 
-      expression = StreamExpressionParser.parse("random(collection1, q=\"*:*\", rows=\"1\", fl=\"id, a_i\")");
+      expression = StreamExpressionParser.parse("random(" + COLLECTION + ", q=\"*:*\", rows=\"1\", fl=\"id, a_i\")");
       stream = factory.constructStream(expression);
       stream.setStreamContext(context);
       List<Tuple> tuples3 = getTuples(stream);
@@ -625,23 +547,24 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
 
     } finally {
       cache.close();
-      del("*:*");
-      commit();
     }
   }
-  
-  private void testReducerStream() throws Exception{
-    indexr(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "1");
-    indexr(id, "2", "a_s", "hello0", "a_i", "2", "a_f", "2");
-    indexr(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3");
-    indexr(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4");
-    indexr(id, "1", "a_s", "hello0", "a_i", "1", "a_f", "5");
-    indexr(id, "5", "a_s", "hello3", "a_i", "10", "a_f", "6");
-    indexr(id, "6", "a_s", "hello4", "a_i", "11", "a_f", "7");
-    indexr(id, "7", "a_s", "hello3", "a_i", "12", "a_f", "8");
-    indexr(id, "8", "a_s", "hello3", "a_i", "13", "a_f", "9");
-    indexr(id, "9", "a_s", "hello0", "a_i", "14", "a_f", "10");
-    commit();
+
+  @Test
+  public void testReducerStream() throws Exception {
+
+    new UpdateRequest()
+        .add(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "1")
+        .add(id, "2", "a_s", "hello0", "a_i", "2", "a_f", "2")
+        .add(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3")
+        .add(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4")
+        .add(id, "1", "a_s", "hello0", "a_i", "1", "a_f", "5")
+        .add(id, "5", "a_s", "hello3", "a_i", "10", "a_f", "6")
+        .add(id, "6", "a_s", "hello4", "a_i", "11", "a_f", "7")
+        .add(id, "7", "a_s", "hello3", "a_i", "12", "a_f", "8")
+        .add(id, "8", "a_s", "hello3", "a_i", "13", "a_f", "9")
+        .add(id, "9", "a_s", "hello0", "a_i", "14", "a_f", "10")
+        .commit(cluster.getSolrClient(), COLLECTION);
     
     StreamExpression expression;
     TupleStream stream;
@@ -650,14 +573,14 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
     List<Map> maps0, maps1, maps2;
     
     StreamFactory factory = new StreamFactory()
-        .withCollectionZkHost("collection1", zkServer.getZkAddress())
+        .withCollectionZkHost(COLLECTION, cluster.getZkServer().getZkAddress())
         .withFunctionName("search", CloudSolrStream.class)
         .withFunctionName("reduce", ReducerStream.class)
         .withFunctionName("group", GroupOperation.class);
 
     // basic
     expression = StreamExpressionParser.parse("reduce("
-        + "search(collection1, q=*:*, fl=\"id,a_s,a_i,a_f\", sort=\"a_s asc, a_f asc\"),"
+        + "search(" + COLLECTION + ", q=*:*, fl=\"id,a_s,a_i,a_f\", sort=\"a_s asc, a_f asc\"),"
         + "by=\"a_s\","
         + "group(sort=\"a_f desc\", n=\"4\"))");
 
@@ -681,7 +604,7 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
     
     // basic w/spaces
     expression = StreamExpressionParser.parse("reduce("
-        + "search(collection1, q=*:*, fl=\"id,a_s,a_i,a_f\", sort=\"a_s asc, a_f       asc\"),"
+        + "search(" + COLLECTION + ", q=*:*, fl=\"id,a_s,a_i,a_f\", sort=\"a_s asc, a_f       asc\"),"
         + "by=\"a_s\"," +
         "group(sort=\"a_i asc\", n=\"2\"))");
     stream = factory.constructStream(expression);
@@ -703,27 +626,26 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
     maps2 = t2.getMaps("group");
     assertMaps(maps2, 4, 6);
 
-    del("*:*");
-    commit();
   }
 
-  private void testDaemonStream() throws Exception {
-
-    indexr(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "1");
-    indexr(id, "2", "a_s", "hello0", "a_i", "2", "a_f", "2");
-    indexr(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3");
-    indexr(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4");
-    indexr(id, "1", "a_s", "hello0", "a_i", "1", "a_f", "5");
-    indexr(id, "5", "a_s", "hello3", "a_i", "10", "a_f", "6");
-    indexr(id, "6", "a_s", "hello4", "a_i", "11", "a_f", "7");
-    indexr(id, "7", "a_s", "hello3", "a_i", "12", "a_f", "8");
-    indexr(id, "8", "a_s", "hello3", "a_i", "13", "a_f", "9");
-    indexr(id, "9", "a_s", "hello0", "a_i", "14", "a_f", "10");
-
-    commit();
+  @Test
+  public void testDaemonStream() throws Exception {
+
+    new UpdateRequest()
+        .add(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "1")
+        .add(id, "2", "a_s", "hello0", "a_i", "2", "a_f", "2")
+        .add(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3")
+        .add(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4")
+        .add(id, "1", "a_s", "hello0", "a_i", "1", "a_f", "5")
+        .add(id, "5", "a_s", "hello3", "a_i", "10", "a_f", "6")
+        .add(id, "6", "a_s", "hello4", "a_i", "11", "a_f", "7")
+        .add(id, "7", "a_s", "hello3", "a_i", "12", "a_f", "8")
+        .add(id, "8", "a_s", "hello3", "a_i", "13", "a_f", "9")
+        .add(id, "9", "a_s", "hello0", "a_i", "14", "a_f", "10")
+        .commit(cluster.getSolrClient(), COLLECTION);
 
     StreamFactory factory = new StreamFactory()
-        .withCollectionZkHost("collection1", zkServer.getZkAddress())
+        .withCollectionZkHost(COLLECTION, cluster.getZkServer().getZkAddress())
         .withFunctionName("search", CloudSolrStream.class)
         .withFunctionName("rollup", RollupStream.class)
         .withFunctionName("sum", SumMetric.class)
@@ -737,7 +659,7 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
     DaemonStream daemonStream;
 
     expression = StreamExpressionParser.parse("daemon(rollup("
-        + "search(collection1, q=\"*:*\", fl=\"a_i,a_s\", sort=\"a_s asc\"),"
+        + "search(" + COLLECTION + ", q=\"*:*\", fl=\"a_i,a_s\", sort=\"a_s asc\"),"
         + "over=\"a_s\","
         + "sum(a_i)"
         + "), id=\"test\", runInterval=\"1000\", queueSize=\"9\")");
@@ -785,8 +707,9 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
 
     //OK capacity is full, let's index a new doc
 
-    indexr(id, "10", "a_s", "hello0", "a_i", "1", "a_f", "10");
-    commit();
+    new UpdateRequest()
+        .add(id, "10", "a_s", "hello0", "a_i", "1", "a_f", "10")
+        .commit(cluster.getSolrClient(), COLLECTION);
 
     //Now lets clear the existing docs in the queue 9, plus 3 more to get passed the run that was blocked. The next run should
     //have the tuples with the updated count.
@@ -822,32 +745,26 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
 
     daemonStream.close(); //This should stop the daemon thread
 
-    del("*:*");
-    commit();
   }
 
-
-
-
-
-
-  private void testRollupStream() throws Exception {
-
-    indexr(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "1");
-    indexr(id, "2", "a_s", "hello0", "a_i", "2", "a_f", "2");
-    indexr(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3");
-    indexr(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4");
-    indexr(id, "1", "a_s", "hello0", "a_i", "1", "a_f", "5");
-    indexr(id, "5", "a_s", "hello3", "a_i", "10", "a_f", "6");
-    indexr(id, "6", "a_s", "hello4", "a_i", "11", "a_f", "7");
-    indexr(id, "7", "a_s", "hello3", "a_i", "12", "a_f", "8");
-    indexr(id, "8", "a_s", "hello3", "a_i", "13", "a_f", "9");
-    indexr(id, "9", "a_s", "hello0", "a_i", "14", "a_f", "10");
-
-    commit();
+  @Test
+  public void testRollupStream() throws Exception {
+
+    new UpdateRequest()
+        .add(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "1")
+        .add(id, "2", "a_s", "hello0", "a_i", "2", "a_f", "2")
+        .add(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3")
+        .add(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4")
+        .add(id, "1", "a_s", "hello0", "a_i", "1", "a_f", "5")
+        .add(id, "5", "a_s", "hello3", "a_i", "10", "a_f", "6")
+        .add(id, "6", "a_s", "hello4", "a_i", "11", "a_f", "7")
+        .add(id, "7", "a_s", "hello3", "a_i", "12", "a_f", "8")
+        .add(id, "8", "a_s", "hello3", "a_i", "13", "a_f", "9")
+        .add(id, "9", "a_s", "hello0", "a_i", "14", "a_f", "10")
+        .commit(cluster.getSolrClient(), COLLECTION);
 
     StreamFactory factory = new StreamFactory()
-      .withCollectionZkHost("collection1", zkServer.getZkAddress())
+      .withCollectionZkHost(COLLECTION, cluster.getZkServer().getZkAddress())
       .withFunctionName("search", CloudSolrStream.class)
       .withFunctionName("rollup", RollupStream.class)
       .withFunctionName("sum", SumMetric.class)
@@ -861,7 +778,7 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
     List<Tuple> tuples;
 
     expression = StreamExpressionParser.parse("rollup("
-                                              + "search(collection1, q=*:*, fl=\"a_s,a_i,a_f\", sort=\"a_s asc\"),"
+                                              + "search(" + COLLECTION + ", q=*:*, fl=\"a_s,a_i,a_f\", sort=\"a_s asc\"),"
                                               + "over=\"a_s\","
                                               + "sum(a_i),"
                                               + "sum(a_f),"
@@ -949,27 +866,26 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
     assertTrue(avgf.doubleValue() == 5.5D);
     assertTrue(count.doubleValue() == 2);
 
-    del("*:*");
-    commit();
   }
-  
-  private void testStatsStream() throws Exception {
-
-    indexr(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "1");
-    indexr(id, "2", "a_s", "hello0", "a_i", "2", "a_f", "2");
-    indexr(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3");
-    indexr(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4");
-    indexr(id, "1", "a_s", "hello0", "a_i", "1", "a_f", "5");
-    indexr(id, "5", "a_s", "hello3", "a_i", "10", "a_f", "6");
-    indexr(id, "6", "a_s", "hello4", "a_i", "11", "a_f", "7");
-    indexr(id, "7", "a_s", "hello3", "a_i", "12", "a_f", "8");
-    indexr(id, "8", "a_s", "hello3", "a_i", "13", "a_f", "9");
-    indexr(id, "9", "a_s", "hello0", "a_i", "14", "a_f", "10");
 
-    commit();
+  @Test
+  public void testStatsStream() throws Exception {
+
+    new UpdateRequest()
+        .add(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "1")
+        .add(id, "2", "a_s", "hello0", "a_i", "2", "a_f", "2")
+        .add(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3")
+        .add(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4")
+        .add(id, "1", "a_s", "hello0", "a_i", "1", "a_f", "5")
+        .add(id, "5", "a_s", "hello3", "a_i", "10", "a_f", "6")
+        .add(id, "6", "a_s", "hello4", "a_i", "11", "a_f", "7")
+        .add(id, "7", "a_s", "hello3", "a_i", "12", "a_f", "8")
+        .add(id, "8", "a_s", "hello3", "a_i", "13", "a_f", "9")
+        .add(id, "9", "a_s", "hello0", "a_i", "14", "a_f", "10")
+        .commit(cluster.getSolrClient(), COLLECTION);
 
     StreamFactory factory = new StreamFactory()
-    .withCollectionZkHost("collection1", zkServer.getZkAddress())
+    .withCollectionZkHost(COLLECTION, cluster.getZkServer().getZkAddress())
     .withFunctionName("stats", StatsStream.class)
     .withFunctionName("sum", SumMetric.class)
     .withFunctionName("min", MinMetric.class)
@@ -1012,33 +928,32 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
     assertTrue(avgf.doubleValue() == 5.5D);
     assertTrue(count.doubleValue() == 10);
 
-    del("*:*");
-    commit();
   }
-  
-  private void testParallelUniqueStream() throws Exception {
-
-    indexr(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "0");
-    indexr(id, "2", "a_s", "hello2", "a_i", "2", "a_f", "0");
-    indexr(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3");
-    indexr(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4");
-    indexr(id, "1", "a_s", "hello1", "a_i", "1", "a_f", "1");
-    indexr(id, "5", "a_s", "hello1", "a_i", "10", "a_f", "1");
-    indexr(id, "6", "a_s", "hello1", "a_i", "11", "a_f", "5");
-    indexr(id, "7", "a_s", "hello1", "a_i", "12", "a_f", "5");
-    indexr(id, "8", "a_s", "hello1", "a_i", "13", "a_f", "4");
-
-    commit();
-
-    String zkHost = zkServer.getZkAddress();
-    StreamFactory streamFactory = new StreamFactory().withCollectionZkHost("collection1", zkServer.getZkAddress())
+
+  @Test
+  public void testParallelUniqueStream() throws Exception {
+
+    new UpdateRequest()
+        .add(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "0")
+        .add(id, "2", "a_s", "hello2", "a_i", "2", "a_f", "0")
+        .add(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3")
+        .add(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4")
+        .add(id, "1", "a_s", "hello1", "a_i", "1", "a_f", "1")
+        .add(id, "5", "a_s", "hello1", "a_i", "10", "a_f", "1")
+        .add(id, "6", "a_s", "hello1", "a_i", "11", "a_f", "5")
+        .add(id, "7", "a_s", "hello1", "a_i", "12", "a_f", "5")
+        .add(id, "8", "a_s", "hello1", "a_i", "13", "a_f", "4")
+        .commit(cluster.getSolrClient(), COLLECTION);
+
+    String zkHost = cluster.getZkServer().getZkAddress();
+    StreamFactory streamFactory = new StreamFactory().withCollectionZkHost(COLLECTION, zkHost)
         .withFunctionName("search", CloudSolrStream.class)
         .withFunctionName("unique", UniqueStream.class)
         .withFunctionName("top", RankStream.class)
         .withFunctionName("group", ReducerStream.class)
         .withFunctionName("parallel", ParallelStream.class);
 
-    ParallelStream pstream = (ParallelStream)streamFactory.constructStream("parallel(collection1, unique(search(collection1, q=*:*, fl=\"id,a_s,a_i,a_f\", sort=\"a_f asc, a_i asc\", partitionKeys=\"a_f\"), over=\"a_f\"), workers=\"2\", zkHost=\""+zkHost+"\", sort=\"a_f asc\")");
+    ParallelStream pstream = (ParallelStream)streamFactory.constructStream("parallel(" + COLLECTION + ", unique(search(collection1, q=*:*, fl=\"id,a_s,a_i,a_f\", sort=\"a_f asc, a_i asc\", partitionKeys=\"a_f\"), over=\"a_f\"), workers=\"2\", zkHost=\""+zkHost+"\", sort=\"a_f asc\")");
 
     List<Tuple> tuples = getTuples(pstream);
     assert(tuples.size() == 5);
@@ -1049,36 +964,34 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
     Map<String,Tuple> eofTuples = pstream.getEofTuples();
     assert(eofTuples.size() == 2); //There should be an EOF tuple for each worker.
 
-    del("*:*");
-    commit();
-
   }
 
-  private void testParallelReducerStream() throws Exception {
-
-    indexr(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "1");
-    indexr(id, "2", "a_s", "hello0", "a_i", "2", "a_f", "2");
-    indexr(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3");
-    indexr(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4");
-    indexr(id, "1", "a_s", "hello0", "a_i", "1", "a_f", "5");
-    indexr(id, "5", "a_s", "hello3", "a_i", "10", "a_f", "6");
-    indexr(id, "6", "a_s", "hello4", "a_i", "11", "a_f", "7");
-    indexr(id, "7", "a_s", "hello3", "a_i", "12", "a_f", "8");
-    indexr(id, "8", "a_s", "hello3", "a_i", "13", "a_f", "9");
-    indexr(id, "9", "a_s", "hello0", "a_i", "14", "a_f", "10");
-
-    commit();
-
-    String zkHost = zkServer.getZkAddress();
-    StreamFactory streamFactory = new StreamFactory().withCollectionZkHost("collection1", zkServer.getZkAddress())
+  @Test
+  public void testParallelReducerStream() throws Exception {
+
+    new UpdateRequest()
+        .add(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "1")
+        .add(id, "2", "a_s", "hello0", "a_i", "2", "a_f", "2")
+        .add(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3")
+        .add(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4")
+        .add(id, "1", "a_s", "hello0", "a_i", "1", "a_f", "5")
+        .add(id, "5", "a_s", "hello3", "a_i", "10", "a_f", "6")
+        .add(id, "6", "a_s", "hello4", "a_i", "11", "a_f", "7")
+        .add(id, "7", "a_s", "hello3", "a_i", "12", "a_f", "8")
+        .add(id, "8", "a_s", "hello3", "a_i", "13", "a_f", "9")
+        .add(id, "9", "a_s", "hello0", "a_i", "14", "a_f", "10")
+        .commit(cluster.getSolrClient(), COLLECTION);
+
+    String zkHost = cluster.getZkServer().getZkAddress();
+    StreamFactory streamFactory = new StreamFactory().withCollectionZkHost(COLLECTION, zkHost)
         .withFunctionName("search", CloudSolrStream.class)
         .withFunctionName("group", GroupOperation.class)
         .withFunctionName("reduce", ReducerStream.class)
         .withFunctionName("parallel", ParallelStream.class);
 
-    ParallelStream pstream = (ParallelStream)streamFactory.constructStream("parallel(collection1," +
+    ParallelStream pstream = (ParallelStream)streamFactory.constructStream("parallel(" + COLLECTION + ", " +
                                                                                     "reduce(" +
-                                                                                              "search(collection1, q=\"*:*\", fl=\"id,a_s,a_i,a_f\", sort=\"a_s asc,a_f asc\", partitionKeys=\"a_s\"), " +
+                                                                                              "search(" + COLLECTION + ", q=\"*:*\", fl=\"id,a_s,a_i,a_f\", sort=\"a_s asc,a_f asc\", partitionKeys=\"a_s\"), " +
                                                                                               "by=\"a_s\"," +
                                                                                               "group(sort=\"a_i asc\", n=\"5\")), " +
                                                                                     "workers=\"2\", zkHost=\""+zkHost+"\", sort=\"a_s asc\")");
@@ -1100,9 +1013,9 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
     assertMaps(maps2, 4, 6);
 
 
-    pstream = (ParallelStream)streamFactory.constructStream("parallel(collection1, " +
+    pstream = (ParallelStream)streamFactory.constructStream("parallel(" + COLLECTION + ", " +
                                                                       "reduce(" +
-                                                                              "search(collection1, q=\"*:*\", fl=\"id,a_s,a_i,a_f\", sort=\"a_s desc,a_f asc\", partitionKeys=\"a_s\"), " +
+                                                                              "search(" + COLLECTION + ", q=\"*:*\", fl=\"id,a_s,a_i,a_f\", sort=\"a_s desc,a_f asc\", partitionKeys=\"a_s\"), " +
                                                                               "by=\"a_s\", " +
                                                                               "group(sort=\"a_i desc\", n=\"5\")),"+
                                                                       "workers=\"2\", zkHost=\""+zkHost+"\", sort=\"a_s desc\")");
@@ -1125,27 +1038,26 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
     maps2 = t2.getMaps("group");
     assertMaps(maps2, 9, 2, 1, 0);
 
-    del("*:*");
-    commit();
   }
 
-  private void testParallelRankStream() throws Exception {
-
-    indexr(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "0");
-    indexr(id, "2", "a_s", "hello2", "a_i", "2", "a_f", "0");
-    indexr(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3");
-    indexr(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4");
-    indexr(id, "5", "a_s", "hello1", "a_i", "5", "a_f", "1");
-    indexr(id, "6", "a_s", "hello1", "a_i", "6", "a_f", "1");
-    indexr(id, "7", "a_s", "hello1", "a_i", "7", "a_f", "1");
-    indexr(id, "8", "a_s", "hello1", "a_i", "8", "a_f", "1");
-    indexr(id, "9", "a_s", "hello1", "a_i", "9", "a_f", "1");
-    indexr(id, "10", "a_s", "hello1", "a_i", "10", "a_f", "1");
-
-    commit();
-
-    String zkHost = zkServer.getZkAddress();
-    StreamFactory streamFactory = new StreamFactory().withCollectionZkHost("collection1", zkServer.getZkAddress())
+  @Test
+  public void testParallelRankStream() throws Exception {
+
+    new UpdateRequest()
+        .add(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "0")
+        .add(id, "2", "a_s", "hello2", "a_i", "2", "a_f", "0")
+        .add(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3")
+        .add(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4")
+        .add(id, "5", "a_s", "hello1", "a_i", "5", "a_f", "1")
+        .add(id, "6", "a_s", "hello1", "a_i", "6", "a_f", "1")
+        .add(id, "7", "a_s", "hello1", "a_i", "7", "a_f", "1")
+        .add(id, "8", "a_s", "hello1", "a_i", "8", "a_f", "1")
+        .add(id, "9", "a_s", "hello1", "a_i", "9", "a_f", "1")
+        .add(id, "10", "a_s", "hello1", "a_i", "10", "a_f", "1")
+        .commit(cluster.getSolrClient(), COLLECTION);
+
+    String zkHost = cluster.getZkServer().getZkAddress();
+    StreamFactory streamFactory = new StreamFactory().withCollectionZkHost(COLLECTION, zkHost)
         .withFunctionName("search", CloudSolrStream.class)
         .withFunctionName("unique", UniqueStream.class)
         .withFunctionName("top", RankStream.class)
@@ -1153,9 +1065,9 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
         .withFunctionName("parallel", ParallelStream.class);
 
     ParallelStream pstream = (ParallelStream)streamFactory.constructStream("parallel("
-        + "collection1, "
+        + COLLECTION + ", "
         + "top("
-          + "search(collection1, q=\"*:*\", fl=\"id,a_s,a_i\", sort=\"a_i asc\", partitionKeys=\"a_i\"), "
+          + "search(" + COLLECTION + ", q=\"*:*\", fl=\"id,a_s,a_i\", sort=\"a_i asc\", partitionKeys=\"a_i\"), "
           + "n=\"11\", "
           + "sort=\"a_i desc\"), workers=\"2\", zkHost=\""+zkHost+"\", sort=\"a_i desc\")");
 
@@ -1164,27 +1076,26 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
     assert(tuples.size() == 10);
     assertOrder(tuples, 10,9,8,7,6,5,4,3,2,0);
 
-    del("*:*");
-    commit();
   }
 
-  private void testParallelMergeStream() throws Exception {
-
-    indexr(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "0");
-    indexr(id, "2", "a_s", "hello2", "a_i", "2", "a_f", "0");
-    indexr(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3");
-    indexr(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4");
-    indexr(id, "1", "a_s", "hello1", "a_i", "1", "a_f", "1");
-    indexr(id, "5", "a_s", "hello0", "a_i", "10", "a_f", "0");
-    indexr(id, "6", "a_s", "hello2", "a_i", "8", "a_f", "0");
-    indexr(id, "7", "a_s", "hello3", "a_i", "7", "a_f", "3");
-    indexr(id, "8", "a_s", "hello4", "a_i", "11", "a_f", "4");
-    indexr(id, "9", "a_s", "hello1", "a_i", "100", "a_f", "1");
-
-    commit();
-
-    String zkHost = zkServer.getZkAddress();
-    StreamFactory streamFactory = new StreamFactory().withCollectionZkHost("collection1", zkServer.getZkAddress())
+  @Test
+  public void testParallelMergeStream() throws Exception {
+
+    new UpdateRequest()
+        .add(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "0")
+        .add(id, "2", "a_s", "hello2", "a_i", "2", "a_f", "0")
+        .add(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3")
+        .add(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4")
+        .add(id, "1", "a_s", "hello1", "a_i", "1", "a_f", "1")
+        .add(id, "5", "a_s", "hello0", "a_i", "10", "a_f", "0")
+        .add(id, "6", "a_s", "hello2", "a_i", "8", "a_f", "0")
+        .add(id, "7", "a_s", "hello3", "a_i", "7", "a_f", "3")
+        .add(id, "8", "a_s", "hello4", "a_i", "11", "a_f", "4")
+        .add(id, "9", "a_s", "hello1", "a_i", "100", "a_f", "1")
+        .commit(cluster.getSolrClient(), COLLECTION);
+
+    String zkHost = cluster.getZkServer().getZkAddress();
+    StreamFactory streamFactory = new StreamFactory().withCollectionZkHost(COLLECTION, zkHost)
         .withFunctionName("search", CloudSolrStream.class)
         .withFunctionName("unique", UniqueStream.class)
         .withFunctionName("top", RankStream.class)
@@ -1193,7 +1104,7 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
         .withFunctionName("parallel", ParallelStream.class);
 
     //Test ascending
-    ParallelStream pstream = (ParallelStream)streamFactory.constructStream("parallel(collection1, merge(search(collection1, q=\"id:(4 1 8 7 9)\", fl=\"id,a_s,a_i\", sort=\"a_i asc\", partitionKeys=\"a_i\"), search(collection1, q=\"id:(0 2 3 6)\", fl=\"id,a_s,a_i\", sort=\"a_i asc\", partitionKeys=\"a_i\"), on=\"a_i asc\"), workers=\"2\", zkHost=\""+zkHost+"\", sort=\"a_i asc\")");
+    ParallelStream pstream = (ParallelStream)streamFactory.constructStream("parallel(" + COLLECTION + ", merge(search(" + COLLECTION + ", q=\"id:(4 1 8 7 9)\", fl=\"id,a_s,a_i\", sort=\"a_i asc\", partitionKeys=\"a_i\"), search(" + COLLECTION + ", q=\"id:(0 2 3 6)\", fl=\"id,a_s,a_i\", sort=\"a_i asc\", partitionKeys=\"a_i\"), on=\"a_i asc\"), workers=\"2\", zkHost=\""+zkHost+"\", sort=\"a_i asc\")");
 
     List<Tuple> tuples = getTuples(pstream);
 
@@ -1204,34 +1115,33 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
 
     //Test descending
 
-    pstream = (ParallelStream)streamFactory.constructStream("parallel(collection1, merge(search(collection1, q=\"id:(4 1 8 9)\", fl=\"id,a_s,a_i\", sort=\"a_i desc\", partitionKeys=\"a_i\"), search(collection1, q=\"id:(0 2 3 6)\", fl=\"id,a_s,a_i\", sort=\"a_i desc\", partitionKeys=\"a_i\"), on=\"a_i desc\"), workers=\"2\", zkHost=\""+zkHost+"\", sort=\"a_i desc\")");
+    pstream = (ParallelStream)streamFactory.constructStream("parallel(" + COLLECTION + ", merge(search(" + COLLECTION + ", q=\"id:(4 1 8 9)\", fl=\"id,a_s,a_i\", sort=\"a_i desc\", partitionKeys=\"a_i\"), search(" + COLLECTION + ", q=\"id:(0 2 3 6)\", fl=\"id,a_s,a_i\", sort=\"a_i desc\", partitionKeys=\"a_i\"), on=\"a_i desc\"), workers=\"2\", zkHost=\""+zkHost+"\", sort=\"a_i desc\")");
 
     tuples = getTuples(pstream);
 
     assert(tuples.size() == 8);
     assertOrder(tuples, 9,8,6,4,3,2,1,0);
 
-    del("*:*");
-    commit();
   }
-  
-  private void testParallelRollupStream() throws Exception {
 
-    indexr(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "1");
-    indexr(id, "2", "a_s", "hello0", "a_i", "2", "a_f", "2");
-    indexr(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3");
-    indexr(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4");
-    indexr(id, "1", "a_s", "hello0", "a_i", "1", "a_f", "5");
-    indexr(id, "5", "a_s", "hello3", "a_i", "10", "a_f", "6");
-    indexr(id, "6", "a_s", "hello4", "a_i", "11", "a_f", "7");
-    indexr(id, "7", "a_s", "hello3", "a_i", "12", "a_f", "8");
-    indexr(id, "8", "a_s", "hello3", "a_i", "13", "a_f", "9");
-    indexr(id, "9", "a_s", "hello0", "a_i", "14", "a_f", "10");
-
-    commit();
+  @Test
+  public void testParallelRollupStream() throws Exception {
+
+    new UpdateRequest()
+        .add(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "1")
+        .add(id, "2", "a_s", "hello0", "a_i", "2", "a_f", "2")
+        .add(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3")
+        .add(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4")
+        .add(id, "1", "a_s", "hello0", "a_i", "1", "a_f", "5")
+        .add(id, "5", "a_s", "hello3", "a_i", "10", "a_f", "6")
+        .add(id, "6", "a_s", "hello4", "a_i", "11", "a_f", "7")
+        .add(id, "7", "a_s", "hello3", "a_i", "12", "a_f", "8")
+        .add(id, "8", "a_s", "hello3", "a_i", "13", "a_f", "9")
+        .add(id, "9", "a_s", "hello0", "a_i", "14", "a_f", "10")
+        .commit(cluster.getSolrClient(), COLLECTION);
 
     StreamFactory factory = new StreamFactory()
-      .withCollectionZkHost("collection1", zkServer.getZkAddress())
+      .withCollectionZkHost(COLLECTION, cluster.getZkServer().getZkAddress())
       .withFunctionName("search", CloudSolrStream.class)
       .withFunctionName("parallel", ParallelStream.class)
       .withFunctionName("rollup", RollupStream.class)
@@ -1245,9 +1155,9 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
     TupleStream stream;
     List<Tuple> tuples;
 
-    expression = StreamExpressionParser.parse("parallel(collection1,"
+    expression = StreamExpressionParser.parse("parallel(" + COLLECTION + ","
                                               + "rollup("
-                                                + "search(collection1, q=*:*, fl=\"a_s,a_i,a_f\", sort=\"a_s asc\", partitionKeys=\"a_s\"),"
+                                                + "search(" + COLLECTION + ", q=*:*, fl=\"a_s,a_i,a_f\", sort=\"a_s asc\", partitionKeys=\"a_s\"),"
                                                 + "over=\"a_s\","
                                                 + "sum(a_i),"
                                                 + "sum(a_f),"
@@ -1259,7 +1169,7 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
                                                 + "avg(a_f),"
                                                 + "count(*)"
                                               + "),"
-                                              + "workers=\"2\", zkHost=\""+zkServer.getZkAddress()+"\", sort=\"a_s asc\")"
+                                              + "workers=\"2\", zkHost=\""+cluster.getZkServer().getZkAddress()+"\", sort=\"a_s asc\")"
                                               );
     stream = factory.constructStream(expression);
     tuples = getTuples(stream);
@@ -1337,43 +1247,43 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
     assertTrue(avgf.doubleValue() == 5.5D);
     assertTrue(count.doubleValue() == 2);
 
-    del("*:*");
-    commit();
   }
 
-  private void testInnerJoinStream() throws Exception {
-
-    indexr(id, "1", "side_s", "left", "join1_i", "0", "join2_s", "a", "ident_s", "left_1"); // 8, 9
-    indexr(id, "15", "side_s", "left", "join1_i", "0", "join2_s", "a", "ident_s", "left_1"); // 8, 9
-    indexr(id, "2", "side_s", "left", "join1_i", "0", "join2_s", "b", "ident_s", "left_2");
-    indexr(id, "3", "side_s", "left", "join1_i", "1", "join2_s", "a", "ident_s", "left_3"); // 10
-    indexr(id, "4", "side_s", "left", "join1_i", "1", "join2_s", "b", "ident_s", "left_4"); // 11
-    indexr(id, "5", "side_s", "left", "join1_i", "1", "join2_s", "c", "ident_s", "left_5"); // 12
-    indexr(id, "6", "side_s", "left", "join1_i", "2", "join2_s", "d", "ident_s", "left_6");
-    indexr(id, "7", "side_s", "left", "join1_i", "3", "join2_s", "e", "ident_s", "left_7"); // 14
-
-    indexr(id, "8", "side_s", "right", "join1_i", "0", "join2_s", "a", "ident_s", "right_1", "join3_i", "0"); // 1,15
-    indexr(id, "9", "side_s", "right", "join1_i", "0", "join2_s", "a", "ident_s", "right_2", "join3_i", "0"); // 1,15
-    indexr(id, "10", "side_s", "right", "join1_i", "1", "join2_s", "a", "ident_s", "right_3", "join3_i", "1"); // 3
-    indexr(id, "11", "side_s", "right", "join1_i", "1", "join2_s", "b", "ident_s", "right_4", "join3_i", "1"); // 4
-    indexr(id, "12", "side_s", "right", "join1_i", "1", "join2_s", "c", "ident_s", "right_5", "join3_i", "1"); // 5
-    indexr(id, "13", "side_s", "right", "join1_i", "2", "join2_s", "dad", "ident_s", "right_6", "join3_i", "2"); 
-    indexr(id, "14", "side_s", "right", "join1_i", "3", "join2_s", "e", "ident_s", "right_7", "join3_i", "3"); // 7
-    commit();
+  @Test
+  public void testInnerJoinStream() throws Exception {
+
+    new UpdateRequest()
+        .add(id, "1", "side_s", "left", "join1_i", "0", "join2_s", "a", "ident_s", "left_1") // 8, 9
+        .add(id, "15", "side_s", "left", "join1_i", "0", "join2_s", "a", "ident_s", "left_1") // 8, 9
+        .add(id, "2", "side_s", "left", "join1_i", "0", "join2_s", "b", "ident_s", "left_2")
+        .add(id, "3", "side_s", "left", "join1_i", "1", "join2_s", "a", "ident_s", "left_3") // 10
+        .add(id, "4", "side_s", "left", "join1_i", "1", "join2_s", "b", "ident_s", "left_4") // 11
+        .add(id, "5", "side_s", "left", "join1_i", "1", "join2_s", "c", "ident_s", "left_5") // 12
+        .add(id, "6", "side_s", "left", "join1_i", "2", "join2_s", "d", "ident_s", "left_6")
+        .add(id, "7", "side_s", "left", "join1_i", "3", "join2_s", "e", "ident_s", "left_7") // 14
+
+        .add(id, "8", "side_s", "right", "join1_i", "0", "join2_s", "a", "ident_s", "right_1", "join3_i", "0") // 1,15
+        .add(id, "9", "side_s", "right", "join1_i", "0", "join2_s", "a", "ident_s", "right_2", "join3_i", "0") // 1,15
+        .add(id, "10", "side_s", "right", "join1_i", "1", "join2_s", "a", "ident_s", "right_3", "join3_i", "1") // 3
+        .add(id, "11", "side_s", "right", "join1_i", "1", "join2_s", "b", "ident_s", "right_4", "join3_i", "1") // 4
+        .add(id, "12", "side_s", "right", "join1_i", "1", "join2_s", "c", "ident_s", "right_5", "join3_i", "1") // 5
+        .add(id, "13", "side_s", "right", "join1_i", "2", "join2_s", "dad", "ident_s", "right_6", "join3_i", "2")
+        .add(id, "14", "side_s", "right", "join1_i", "3", "join2_s", "e", "ident_s", "right_7", "join3_i", "3") // 7
+        .commit(cluster.getSolrClient(), COLLECTION);
 
     StreamExpression expression;
     TupleStream stream;
     List<Tuple> tuples;
     
     StreamFactory factory = new StreamFactory()
-      .withCollectionZkHost("collection1", zkServer.getZkAddress())
+      .withCollectionZkHost(COLLECTION, cluster.getZkServer().getZkAddress())
       .withFunctionName("search", CloudSolrStream.class)
       .withFunctionName("innerJoin", InnerJoinStream.class);
     
     // Basic test
     expression = StreamExpressionParser.parse("innerJoin("
-                                                + "search(collection1, q=\"side_s:left\", fl=\"id,join1_i,join2_s,ident_s\", sort=\"join1_i asc, join2_s asc, id asc\"),"
-                                                + "search(collection1, q=\"side_s:right\", fl=\"join1_i,join2_s,ident_s\", sort=\"join1_i asc, join2_s asc\"),"
+                                                + "search(" + COLLECTION + ", q=\"side_s:left\", fl=\"id,join1_i,join2_s,ident_s\", sort=\"join1_i asc, join2_s asc, id asc\"),"
+                                                + "search(" + COLLECTION + ", q=\"side_s:right\", fl=\"join1_i,join2_s,ident_s\", sort=\"join1_i asc, join2_s asc\"),"
                                                 + "on=\"join1_i=join1_i, join2_s=join2_s\")");
     stream = new InnerJoinStream(expression, factory);
     tuples = getTuples(stream);    
@@ -1382,8 +1292,8 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
 
     // Basic desc
     expression = StreamExpressionParser.parse("innerJoin("
-                                                + "search(collection1, q=\"side_s:left\", fl=\"id,join1_i,join2_s,ident_s\", sort=\"join1_i desc, join2_s asc\"),"
-                                                + "search(collection1, q=\"side_s:right\", fl=\"join1_i,join2_s,ident_s\", sort=\"join1_i desc, join2_s asc\"),"
+                                                + "search(" + COLLECTION + ", q=\"side_s:left\", fl=\"id,join1_i,join2_s,ident_s\", sort=\"join1_i desc, join2_s asc\"),"
+                                                + "search(" + COLLECTION + ", q=\"side_s:right\", fl=\"join1_i,join2_s,ident_s\", sort=\"join1_i desc, join2_s asc\"),"
                                                 + "on=\"join1_i=join1_i, join2_s=join2_s\")");
     stream = new InnerJoinStream(expression, factory);
     tuples = getTuples(stream);    
@@ -1392,8 +1302,8 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
     
     // Results in both searches, no join matches
     expression = StreamExpressionParser.parse("innerJoin("
-                                                + "search(collection1, q=\"side_s:left\", fl=\"id,join1_i,join2_s,ident_s\", sort=\"ident_s asc\"),"
-                                                + "search(collection1, q=\"side_s:right\", fl=\"id,join1_i,join2_s,ident_s\", sort=\"ident_s asc\", aliases=\"id=right.id, join1_i=right.join1_i, join2_s=right.join2_s, ident_s=right.ident_s\"),"
+                                                + "search(" + COLLECTION + ", q=\"side_s:left\", fl=\"id,join1_i,join2_s,ident_s\", sort=\"ident_s asc\"),"
+                                                + "search(" + COLLECTION + ", q=\"side_s:right\", fl=\"id,join1_i,join2_s,ident_s\", sort=\"ident_s asc\", aliases=\"id=right.id, join1_i=right.join1_i, join2_s=right.join2_s, ident_s=right.ident_s\"),"
                                                 + "on=\"ident_s=right.ident_s\")");
     stream = new InnerJoinStream(expression, factory);
     tuples = getTuples(stream);    
@@ -1401,52 +1311,52 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
     
     // Differing field names
     expression = StreamExpressionParser.parse("innerJoin("
-                                                + "search(collection1, q=\"side_s:left\", fl=\"id,join1_i,join2_s,ident_s\", sort=\"join1_i asc, join2_s asc, id asc\"),"
-                                                + "search(collection1, q=\"side_s:right\", fl=\"join3_i,join2_s,ident_s\", sort=\"join3_i asc, join2_s asc\", aliases=\"join3_i=aliasesField\"),"
+                                                + "search(" + COLLECTION + ", q=\"side_s:left\", fl=\"id,join1_i,join2_s,ident_s\", sort=\"join1_i asc, join2_s asc, id asc\"),"
+                                                + "search(" + COLLECTION + ", q=\"side_s:right\", fl=\"join3_i,join2_s,ident_s\", sort=\"join3_i asc, join2_s asc\", aliases=\"join3_i=aliasesField\"),"
                                                 + "on=\"join1_i=aliasesField, join2_s=join2_s\")");
     stream = new InnerJoinStream(expression, factory);
     tuples = getTuples(stream);
     
     assert(tuples.size() == 8);
     assertOrder(tuples, 1,1,15,15,3,4,5,7);
-    
-    del("*:*");
-    commit();
+
   }
-  
-  private void testLeftOuterJoinStream() throws Exception {
-
-    indexr(id, "1", "side_s", "left", "join1_i", "0", "join2_s", "a", "ident_s", "left_1"); // 8, 9
-    indexr(id, "15", "side_s", "left", "join1_i", "0", "join2_s", "a", "ident_s", "left_1"); // 8, 9
-    indexr(id, "2", "side_s", "left", "join1_i", "0", "join2_s", "b", "ident_s", "left_2");
-    indexr(id, "3", "side_s", "left", "join1_i", "1", "join2_s", "a", "ident_s", "left_3"); // 10
-    indexr(id, "4", "side_s", "left", "join1_i", "1", "join2_s", "b", "ident_s", "left_4"); // 11
-    indexr(id, "5", "side_s", "left", "join1_i", "1", "join2_s", "c", "ident_s", "left_5"); // 12
-    indexr(id, "6", "side_s", "left", "join1_i", "2", "join2_s", "d", "ident_s", "left_6");
-    indexr(id, "7", "side_s", "left", "join1_i", "3", "join2_s", "e", "ident_s", "left_7"); // 14
-
-    indexr(id, "8", "side_s", "right", "join1_i", "0", "join2_s", "a", "ident_s", "right_1", "join3_i", "0"); // 1,15
-    indexr(id, "9", "side_s", "right", "join1_i", "0", "join2_s", "a", "ident_s", "right_2", "join3_i", "0"); // 1,15
-    indexr(id, "10", "side_s", "right", "join1_i", "1", "join2_s", "a", "ident_s", "right_3", "join3_i", "1"); // 3
-    indexr(id, "11", "side_s", "right", "join1_i", "1", "join2_s", "b", "ident_s", "right_4", "join3_i", "1"); // 4
-    indexr(id, "12", "side_s", "right", "join1_i", "1", "join2_s", "c", "ident_s", "right_5", "join3_i", "1"); // 5
-    indexr(id, "13", "side_s", "right", "join1_i", "2", "join2_s", "dad", "ident_s", "right_6", "join3_i", "2"); 
-    indexr(id, "14", "side_s", "right", "join1_i", "3", "join2_s", "e", "ident_s", "right_7", "join3_i", "3"); // 7
-    commit();
+
+  @Test
+  public void testLeftOuterJoinStream() throws Exception {
+
+    new UpdateRequest()
+        .add(id, "1", "side_s", "left", "join1_i", "0", "join2_s", "a", "ident_s", "left_1") // 8, 9
+        .add(id, "15", "side_s", "left", "join1_i", "0", "join2_s", "a", "ident_s", "left_1") // 8, 9
+        .add(id, "2", "side_s", "left", "join1_i", "0", "join2_s", "b", "ident_s", "left_2")
+        .add(id, "3", "side_s", "left", "join1_i", "1", "join2_s", "a", "ident_s", "left_3") // 10
+        .add(id, "4", "side_s", "left", "join1_i", "1", "join2_s", "b", "ident_s", "left_4") // 11
+        .add(id, "5", "side_s", "left", "join1_i", "1", "join2_s", "c", "ident_s", "left_5") // 12
+        .add(id, "6", "side_s", "left", "join1_i", "2", "join2_s", "d", "ident_s", "left_6")
+        .add(id, "7", "side_s", "left", "join1_i", "3", "join2_s", "e", "ident_s", "left_7") // 14
+
+        .add(id, "8", "side_s", "right", "join1_i", "0", "join2_s", "a", "ident_s", "right_1", "join3_i", "0") // 1,15
+        .add(id, "9", "side_s", "right", "join1_i", "0", "join2_s", "a", "ident_s", "right_2", "join3_i", "0") // 1,15
+        .add(id, "10", "side_s", "right", "join1_i", "1", "join2_s", "a", "ident_s", "right_3", "join3_i", "1") // 3
+        .add(id, "11", "side_s", "right", "join1_i", "1", "join2_s", "b", "ident_s", "right_4", "join3_i", "1") // 4
+        .add(id, "12", "side_s", "right", "join1_i", "1", "join2_s", "c", "ident_s", "right_5", "join3_i", "1") // 5
+        .add(id, "13", "side_s", "right", "join1_i", "2", "join2_s", "dad", "ident_s", "right_6", "join3_i", "2")
+        .add(id, "14", "side_s", "right", "join1_i", "3", "join2_s", "e", "ident_s", "right_7", "join3_i", "3") // 7
+        .commit(cluster.getSolrClient(), COLLECTION);
 
     StreamExpression expression;
     TupleStream stream;
     List<Tuple> tuples;
     
     StreamFactory factory = new StreamFactory()
-      .withCollectionZkHost("collection1", zkServer.getZkAddress())
+      .withCollectionZkHost(COLLECTION, cluster.getZkServer().getZkAddress())
       .withFunctionName("search", CloudSolrStream.class)
       .withFunctionName("leftOuterJoin", LeftOuterJoinStream.class);
     
     // Basic test
     expression = StreamExpressionParser.parse("leftOuterJoin("
-                                                + "search(collection1, q=\"side_s:left\", fl=\"id,join1_i,join2_s,ident_s\", sort=\"join1_i asc, join2_s asc, id asc\"),"
-                                                + "search(collection1, q=\"side_s:right\", fl=\"join1_i,join2_s,ident_s\", sort=\"join1_i asc, join2_s asc\"),"
+                                                + "search(" + COLLECTION + ", q=\"side_s:left\", fl=\"id,join1_i,join2_s,ident_s\", sort=\"join1_i asc, join2_s asc, id asc\"),"
+                                                + "search(" + COLLECTION + ", q=\"side_s:right\", fl=\"join1_i,join2_s,ident_s\", sort=\"join1_i asc, join2_s asc\"),"
                                                 + "on=\"join1_i=join1_i, join2_s=join2_s\")");
     stream = new LeftOuterJoinStream(expression, factory);
     tuples = getTuples(stream);    
@@ -1455,8 +1365,8 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
 
     // Basic desc
     expression = StreamExpressionParser.parse("leftOuterJoin("
-                                                + "search(collection1, q=\"side_s:left\", fl=\"id,join1_i,join2_s,ident_s\", sort=\"join1_i desc, join2_s asc\"),"
-                                                + "search(collection1, q=\"side_s:right\", fl=\"join1_i,join2_s,ident_s\", sort=\"join1_i desc, join2_s asc\"),"
+                                                + "search(" + COLLECTION + ", q=\"side_s:left\", fl=\"id,join1_i,join2_s,ident_s\", sort=\"join1_i desc, join2_s asc\"),"
+                                                + "search(" + COLLECTION + ", q=\"side_s:right\", fl=\"join1_i,join2_s,ident_s\", sort=\"join1_i desc, join2_s asc\"),"
                                                 + "on=\"join1_i=join1_i, join2_s=join2_s\")");
     stream = new LeftOuterJoinStream(expression, factory);
     tuples = getTuples(stream);    
@@ -1465,8 +1375,8 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
     
     // Results in both searches, no join matches
     expression = StreamExpressionParser.parse("leftOuterJoin("
-                                                + "search(collection1, q=\"side_s:left\", fl=\"id,join1_i,join2_s,ident_s\", sort=\"ident_s asc\"),"
-                                                + "search(collection1, q=\"side_s:right\", fl=\"id,join1_i,join2_s,ident_s\", sort=\"ident_s asc\", aliases=\"id=right.id, join1_i=right.join1_i, join2_s=right.join2_s, ident_s=right.ident_s\"),"
+                                                + "search(" + COLLECTION + ", q=\"side_s:left\", fl=\"id,join1_i,join2_s,ident_s\", sort=\"ident_s asc\"),"
+                                                + "search(" + COLLECTION + ", q=\"side_s:right\", fl=\"id,join1_i,join2_s,ident_s\", sort=\"ident_s asc\", aliases=\"id=right.id, join1_i=right.join1_i, join2_s=right.join2_s, ident_s=right.ident_s\"),"
                                                 + "on=\"ident_s=right.ident_s\")");
     stream = new LeftOuterJoinStream(expression, factory);
     tuples = getTuples(stream);    
@@ -1475,44 +1385,44 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
     
     // Differing field names
     expression = StreamExpressionParser.parse("leftOuterJoin("
-                                                + "search(collection1, q=\"side_s:left\", fl=\"id,join1_i,join2_s,ident_s\", sort=\"join1_i asc, join2_s asc, id asc\"),"
-                                                + "search(collection1, q=\"side_s:right\", fl=\"join3_i,join2_s,ident_s\", sort=\"join3_i asc, join2_s asc\", aliases=\"join3_i=aliasesField\"),"
+                                                + "search(" + COLLECTION + ", q=\"side_s:left\", fl=\"id,join1_i,join2_s,ident_s\", sort=\"join1_i asc, join2_s asc, id asc\"),"
+                                                + "search(" + COLLECTION + ", q=\"side_s:right\", fl=\"join3_i,join2_s,ident_s\", sort=\"join3_i asc, join2_s asc\", aliases=\"join3_i=aliasesField\"),"
                                                 + "on=\"join1_i=aliasesField, join2_s=join2_s\")");
     stream = new LeftOuterJoinStream(expression, factory);
     tuples = getTuples(stream);
     assert(tuples.size() == 10);
     assertOrder(tuples, 1,1,15,15,2,3,4,5,6,7);
-    
-    del("*:*");
-    commit();
+
   }
 
-  private void testHashJoinStream() throws Exception {
-
-    indexr(id, "1", "side_s", "left", "join1_i", "0", "join2_s", "a", "ident_s", "left_1"); // 8, 9
-    indexr(id, "15", "side_s", "left", "join1_i", "0", "join2_s", "a", "ident_s", "left_1"); // 8, 9
-    indexr(id, "2", "side_s", "left", "join1_i", "0", "join2_s", "b", "ident_s", "left_2");
-    indexr(id, "3", "side_s", "left", "join1_i", "1", "join2_s", "a", "ident_s", "left_3"); // 10
-    indexr(id, "4", "side_s", "left", "join1_i", "1", "join2_s", "b", "ident_s", "left_4"); // 11
-    indexr(id, "5", "side_s", "left", "join1_i", "1", "join2_s", "c", "ident_s", "left_5"); // 12
-    indexr(id, "6", "side_s", "left", "join1_i", "2", "join2_s", "d", "ident_s", "left_6");
-    indexr(id, "7", "side_s", "left", "join1_i", "3", "join2_s", "e", "ident_s", "left_7"); // 14
-
-    indexr(id, "8", "side_s", "right", "join1_i", "0", "join2_s", "a", "ident_s", "right_1", "join3_i", "0"); // 1,15
-    indexr(id, "9", "side_s", "right", "join1_i", "0", "join2_s", "a", "ident_s", "right_2", "join3_i", "0"); // 1,15
-    indexr(id, "10", "side_s", "right", "join1_i", "1", "join2_s", "a", "ident_s", "right_3", "join3_i", "1"); // 3
-    indexr(id, "11", "side_s", "right", "join1_i", "1", "join2_s", "b", "ident_s", "right_4", "join3_i", "1"); // 4
-    indexr(id, "12", "side_s", "right", "join1_i", "1", "join2_s", "c", "ident_s", "right_5", "join3_i", "1"); // 5
-    indexr(id, "13", "side_s", "right", "join1_i", "2", "join2_s", "dad", "ident_s", "right_6", "join3_i", "2"); 
-    indexr(id, "14", "side_s", "right", "join1_i", "3", "join2_s", "e", "ident_s", "right_7", "join3_i", "3"); // 7
-    commit();
+  @Test
+  public void testHashJoinStream() throws Exception {
+
+    new UpdateRequest()
+        .add(id, "1", "side_s", "left", "join1_i", "0", "join2_s", "a", "ident_s", "left_1") // 8, 9
+        .add(id, "15", "side_s", "left", "join1_i", "0", "join2_s", "a", "ident_s", "left_1") // 8, 9
+        .add(id, "2", "side_s", "left", "join1_i", "0", "join2_s", "b", "ident_s", "left_2")
+        .add(id, "3", "side_s", "left", "join1_i", "1", "join2_s", "a", "ident_s", "left_3") // 10
+        .add(id, "4", "side_s", "left", "join1_i", "1", "join2_s", "b", "ident_s", "left_4") // 11
+        .add(id, "5", "side_s", "left", "join1_i", "1", "join2_s", "c", "ident_s", "left_5") // 12
+        .add(id, "6", "side_s", "left", "join1_i", "2", "join2_s", "d", "ident_s", "left_6")
+        .add(id, "7", "side_s", "left", "join1_i", "3", "join2_s", "e", "ident_s", "left_7") // 14
+
+        .add(id, "8", "side_s", "right", "join1_i", "0", "join2_s", "a", "ident_s", "right_1", "join3_i", "0") // 1,15
+        .add(id, "9", "side_s", "right", "join1_i", "0", "join2_s", "a", "ident_s", "right_2", "join3_i", "0") // 1,15
+        .add(id, "10", "side_s", "right", "join1_i", "1", "join2_s", "a", "ident_s", "right_3", "join3_i", "1") // 3
+        .add(id, "11", "side_s", "right", "join1_i", "1", "join2_s", "b", "ident_s", "right_4", "join3_i", "1") // 4
+        .add(id, "12", "side_s", "right", "join1_i", "1", "join2_s", "c", "ident_s", "right_5", "join3_i", "1") // 5
+        .add(id, "13", "side_s", "right", "join1_i", "2", "join2_s", "dad", "ident_s", "right_6", "join3_i", "2")
+        .add(id, "14", "side_s", "right", "join1_i", "3", "join2_s", "e", "ident_s", "right_7", "join3_i", "3") // 7
+        .commit(cluster.getSolrClient(), COLLECTION);
 
     StreamExpression expression;
     TupleStream stream;
     List<Tuple> tuples;
     
     StreamFactory factory = new StreamFactory()
-      .withCollectionZkHost("collection1", zkServer.getZkAddress())
+      .withCollectionZkHost(COLLECTION, cluster.getZkServer().getZkAddress())
       .withFunctionName("search", CloudSolrStream.class)
       .withFunctionName("hashJoin", HashJoinStream.class);
     
@@ -1544,37 +1454,37 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
     stream = new HashJoinStream(expression, factory);
     tuples = getTuples(stream);    
     assert(tuples.size() == 0);
-    
-    del("*:*");
-    commit();
+
   }
-  
-  private void testOuterHashJoinStream() throws Exception {
-
-    indexr(id, "1", "side_s", "left", "join1_i", "0", "join2_s", "a", "ident_s", "left_1"); // 8, 9
-    indexr(id, "15", "side_s", "left", "join1_i", "0", "join2_s", "a", "ident_s", "left_1"); // 8, 9
-    indexr(id, "2", "side_s", "left", "join1_i", "0", "join2_s", "b", "ident_s", "left_2");
-    indexr(id, "3", "side_s", "left", "join1_i", "1", "join2_s", "a", "ident_s", "left_3"); // 10
-    indexr(id, "4", "side_s", "left", "join1_i", "1", "join2_s", "b", "ident_s", "left_4"); // 11
-    indexr(id, "5", "side_s", "left", "join1_i", "1", "join2_s", "c", "ident_s", "left_5"); // 12
-    indexr(id, "6", "side_s", "left", "join1_i", "2", "join2_s", "d", "ident_s", "left_6");
-    indexr(id, "7", "side_s", "left", "join1_i", "3", "join2_s", "e", "ident_s", "left_7"); // 14
-
-    indexr(id, "8", "side_s", "right", "join1_i", "0", "join2_s", "a", "ident_s", "right_1", "join3_i", "0"); // 1,15
-    indexr(id, "9", "side_s", "right", "join1_i", "0", "join2_s", "a", "ident_s", "right_2", "join3_i", "0"); // 1,15
-    indexr(id, "10", "side_s", "right", "join1_i", "1", "join2_s", "a", "ident_s", "right_3", "join3_i", "1"); // 3
-    indexr(id, "11", "side_s", "right", "join1_i", "1", "join2_s", "b", "ident_s", "right_4", "join3_i", "1"); // 4
-    indexr(id, "12", "side_s", "right", "join1_i", "1", "join2_s", "c", "ident_s", "right_5", "join3_i", "1"); // 5
-    indexr(id, "13", "side_s", "right", "join1_i", "2", "join2_s", "dad", "ident_s", "right_6", "join3_i", "2"); 
-    indexr(id, "14", "side_s", "right", "join1_i", "3", "join2_s", "e", "ident_s", "right_7", "join3_i", "3"); // 7
-    commit();
+
+  @Test
+  public void testOuterHashJoinStream() throws Exception {
+
+    new UpdateRequest()
+        .add(id, "1", "side_s", "left", "join1_i", "0", "join2_s", "a", "ident_s", "left_1") // 8, 9
+        .add(id, "15", "side_s", "left", "join1_i", "0", "join2_s", "a", "ident_s", "left_1") // 8, 9
+        .add(id, "2", "side_s", "left", "join1_i", "0", "join2_s", "b", "ident_s", "left_2")
+        .add(id, "3", "side_s", "left", "join1_i", "1", "join2_s", "a", "ident_s", "left_3") // 10
+        .add(id, "4", "side_s", "left", "join1_i", "1", "join2_s", "b", "ident_s", "left_4") // 11
+        .add(id, "5", "side_s", "left", "join1_i", "1", "join2_s", "c", "ident_s", "left_5") // 12
+        .add(id, "6", "side_s", "left", "join1_i", "2", "join2_s", "d", "ident_s", "left_6")
+        .add(id, "7", "side_s", "left", "join1_i", "3", "join2_s", "e", "ident_s", "left_7") // 14
+
+        .add(id, "8", "side_s", "right", "join1_i", "0", "join2_s", "a", "ident_s", "right_1", "join3_i", "0") // 1,15
+        .add(id, "9", "side_s", "right", "join1_i", "0", "join2_s", "a", "ident_s", "right_2", "join3_i", "0") // 1,15
+        .add(id, "10", "side_s", "right", "join1_i", "1", "join2_s", "a", "ident_s", "right_3", "join3_i", "1") // 3
+        .add(id, "11", "side_s", "right", "join1_i", "1", "join2_s", "b", "ident_s", "right_4", "join3_i", "1") // 4
+        .add(id, "12", "side_s", "right", "join1_i", "1", "join2_s", "c", "ident_s", "right_5", "join3_i", "1") // 5
+        .add(id, "13", "side_s", "right", "join1_i", "2", "join2_s", "dad", "ident_s", "right_6", "join3_i", "2")
+        .add(id, "14", "side_s", "right", "join1_i", "3", "join2_s", "e", "ident_s", "right_7", "join3_i", "3") // 7
+        .commit(cluster.getSolrClient(), COLLECTION);
 
     StreamExpression expression;
     TupleStream stream;
     List<Tuple> tuples;
     
     StreamFactory factory = new StreamFactory()
-        .withCollectionZkHost("collection1", zkServer.getZkAddress())
+        .withCollectionZkHost("collection1", cluster.getZkServer().getZkAddress())
       .withFunctionName("search", CloudSolrStream.class)
       .withFunctionName("outerHashJoin", OuterHashJoinStream.class);
     
@@ -1607,37 +1517,37 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
     tuples = getTuples(stream);    
     assert(tuples.size() == 8);
     assertOrder(tuples, 1,15,2,3,4,5,6,7);
-        
-    del("*:*");
-    commit();
+
   }
-  
-  private void testSelectStream() throws Exception {
-
-    indexr(id, "1", "side_s", "left", "join1_i", "0", "join2_s", "a", "ident_s", "left_1"); // 8, 9
-    indexr(id, "15", "side_s", "left", "join1_i", "0", "join2_s", "a", "ident_s", "left_1"); // 8, 9
-    indexr(id, "2", "side_s", "left", "join1_i", "0", "join2_s", "b", "ident_s", "left_2");
-    indexr(id, "3", "side_s", "left", "join1_i", "1", "join2_s", "a", "ident_s", "left_3"); // 10
-    indexr(id, "4", "side_s", "left", "join1_i", "1", "join2_s", "b", "ident_s", "left_4"); // 11
-    indexr(id, "5", "side_s", "left", "join1_i", "1", "join2_s", "c", "ident_s", "left_5"); // 12
-    indexr(id, "6", "side_s", "left", "join1_i", "2", "join2_s", "d", "ident_s", "left_6");
-    indexr(id, "7", "side_s", "left", "join1_i", "3", "join2_s", "e", "ident_s", "left_7"); // 14
-
-    indexr(id, "8", "side_s", "right", "join1_i", "0", "join2_s", "a", "ident_s", "right_1", "join3_i", "0"); // 1,15
-    indexr(id, "9", "side_s", "right", "join1_i", "0", "join2_s", "a", "ident_s", "right_2", "join3_i", "0"); // 1,15
-    indexr(id, "10", "side_s", "right", "join1_i", "1", "join2_s", "a", "ident_s", "right_3", "join3_i", "1"); // 3
-    indexr(id, "11", "side_s", "right", "join1_i", "1", "join2_s", "b", "ident_s", "right_4", "join3_i", "1"); // 4
-    indexr(id, "12", "side_s", "right", "join1_i", "1", "join2_s", "c", "ident_s", "right_5", "join3_i", "1"); // 5
-    indexr(id, "13", "side_s", "right", "join1_i", "2", "join2_s", "dad", "ident_s", "right_6", "join3_i", "2"); 
-    indexr(id, "14", "side_s", "right", "join1_i", "3", "join2_s", "e", "ident_s", "right_7", "join3_i", "3"); // 7
-    commit();
+
+  @Test
+  public void testSelectStream() throws Exception {
+
+    new UpdateRequest()
+        .add(id, "1", "side_s", "left", "join1_i", "0", "join2_s", "a", "ident_s", "left_1") // 8, 9
+        .add(id, "15", "side_s", "left", "join1_i", "0", "join2_s", "a", "ident_s", "left_1") // 8, 9
+        .add(id, "2", "side_s", "left", "join1_i", "0", "join2_s", "b", "ident_s", "left_2")
+        .add(id, "3", "side_s", "left", "join1_i", "1", "join2_s", "a", "ident_s", "left_3") // 10
+        .add(id, "4", "side_s", "left", "join1_i", "1", "join2_s", "b", "ident_s", "left_4") // 11
+        .add(id, "5", "side_s", "left", "join1_i", "1", "join2_s", "c", "ident_s", "left_5") // 12
+        .add(id, "6", "side_s", "left", "join1_i", "2", "join2_s", "d", "ident_s", "left_6")
+        .add(id, "7", "side_s", "left", "join1_i", "3", "join2_s", "e", "ident_s", "left_7") // 14
+
+        .add(id, "8", "side_s", "right", "join1_i", "0", "join2_s", "a", "ident_s", "right_1", "join3_i", "0") // 1,15
+        .add(id, "9", "side_s", "right", "join1_i", "0", "join2_s", "a", "ident_s", "right_2", "join3_i", "0") // 1,15
+        .add(id, "10", "side_s", "right", "join1_i", "1", "join2_s", "a", "ident_s", "right_3", "join3_i", "1") // 3
+        .add(id, "11", "side_s", "right", "join1_i", "1", "join2_s", "b", "ident_s", "right_4", "join3_i", "1") // 4
+        .add(id, "12", "side_s", "right", "join1_i", "1", "join2_s", "c", "ident_s", "right_5", "join3_i", "1") // 5
+        .add(id, "13", "side_s", "right", "join1_i", "2", "join2_s", "dad", "ident_s", "right_6", "join3_i", "2")
+        .add(id, "14", "side_s", "right", "join1_i", "3", "join2_s", "e", "ident_s", "right_7", "join3_i", "3") // 7
+        .commit(cluster.getSolrClient(), COLLECTION);
 
     String clause;
     TupleStream stream;
     List<Tuple> tuples;
     
     StreamFactory factory = new StreamFactory()
-      .withCollectionZkHost("collection1", zkServer.getZkAddress())
+      .withCollectionZkHost("collection1", cluster.getZkServer().getZkAddress())
       .withFunctionName("search", CloudSolrStream.class)
       .withFunctionName("innerJoin", InnerJoinStream.class)
       .withFunctionName("select", SelectStream.class)
@@ -1729,32 +1639,31 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
     tuples = getTuples(stream);
     assertFields(tuples, "id", "left.ident", "right.ident");
     assertNotFields(tuples, "left.join1", "left.join2", "right.join1", "right.join2");
-    
-    del("*:*");
-    commit();
-  }
-  
-  private void testFacetStream() throws Exception {
 
-    indexr(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "1");
-    indexr(id, "2", "a_s", "hello0", "a_i", "2", "a_f", "2");
-    indexr(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3");
-    indexr(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4");
-    indexr(id, "1", "a_s", "hello0", "a_i", "1", "a_f", "5");
-    indexr(id, "5", "a_s", "hello3", "a_i", "10", "a_f", "6");
-    indexr(id, "6", "a_s", "hello4", "a_i", "11", "a_f", "7");
-    indexr(id, "7", "a_s", "hello3", "a_i", "12", "a_f", "8");
-    indexr(id, "8", "a_s", "hello3", "a_i", "13", "a_f", "9");
-    indexr(id, "9", "a_s", "hello0", "a_i", "14", "a_f", "10");
+  }
 
-    commit();
+  @Test
+  public void testFacetStream() throws Exception {
+
+    new UpdateRequest()
+        .add(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "1")
+        .add(id, "2", "a_s", "hello0", "a_i", "2", "a_f", "2")
+        .add(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3")
+        .add(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4")
+        .add(id, "1", "a_s", "hello0", "a_i", "1", "a_f", "5")
+        .add(id, "5", "a_s", "hello3", "a_i", "10", "a_f", "6")
+        .add(id, "6", "a_s", "hello4", "a_i", "11", "a_f", "7")
+        .add(id, "7", "a_s", "hello3", "a_i", "12", "a_f", "8")
+        .add(id, "8", "a_s", "hello3", "a_i", "13", "a_f", "9")
+        .add(id, "9", "a_s", "hello0", "a_i", "14", "a_f", "10")
+        .commit(cluster.getSolrClient(), COLLECTION);
     
     String clause;
     TupleStream stream;
     List<Tuple> tuples;
     
     StreamFactory factory = new StreamFactory()
-      .withCollectionZkHost("collection1", zkServer.getZkAddress())
+      .withCollectionZkHost("collection1", cluster.getZkServer().getZkAddress())
       .withFunctionName("facet", FacetStream.class)
       .withFunctionName("sum", SumMetric.class)
       .withFunctionName("min", MinMetric.class)
@@ -2136,32 +2045,30 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
     assertTrue(avgf.doubleValue() == 5.5D);
     assertTrue(count.doubleValue() == 2);
 
-    del("*:*");
-    commit();
   }
 
-
-  private void testSubFacetStream() throws Exception {
-
-    indexr(id, "0", "level1_s", "hello0", "level2_s", "a", "a_i", "0", "a_f", "1");
-    indexr(id, "2", "level1_s", "hello0", "level2_s", "a", "a_i", "2", "a_f", "2");
-    indexr(id, "3", "level1_s", "hello3", "level2_s", "a", "a_i", "3", "a_f", "3");
-    indexr(id, "4", "level1_s", "hello4", "level2_s", "a", "a_i", "4", "a_f", "4");
-    indexr(id, "1", "level1_s", "hello0", "level2_s", "b", "a_i", "1", "a_f", "5");
-    indexr(id, "5", "level1_s", "hello3", "level2_s", "b", "a_i", "10", "a_f", "6");
-    indexr(id, "6", "level1_s", "hello4", "level2_s", "b", "a_i", "11", "a_f", "7");
-    indexr(id, "7", "level1_s", "hello3", "level2_s", "b", "a_i", "12", "a_f", "8");
-    indexr(id, "8", "level1_s", "hello3", "level2_s", "b", "a_i", "13", "a_f", "9");
-    indexr(id, "9", "level1_s", "hello0", "level2_s", "b", "a_i", "14", "a_f", "10");
-
-    commit();
+  @Test
+  public void testSubFacetStream() throws Exception {
+
+    new UpdateRequest()
+        .add(id, "0", "level1_s", "hello0", "level2_s", "a", "a_i", "0", "a_f", "1")
+        .add(id, "2", "level1_s", "hello0", "level2_s", "a", "a_i", "2", "a_f", "2")
+        .add(id, "3", "level1_s", "hello3", "level2_s", "a", "a_i", "3", "a_f", "3")
+        .add(id, "4", "level1_s", "hello4", "level2_s", "a", "a_i", "4", "a_f", "4")
+        .add(id, "1", "level1_s", "hello0", "level2_s", "b", "a_i", "1", "a_f", "5")
+        .add(id, "5", "level1_s", "hello3", "level2_s", "b", "a_i", "10", "a_f", "6")
+        .add(id, "6", "level1_s", "hello4", "level2_s", "b", "a_i", "11", "a_f", "7")
+        .add(id, "7", "level1_s", "hello3", "level2_s", "b", "a_i", "12", "a_f", "8")
+        .add(id, "8", "level1_s", "hello3", "level2_s", "b", "a_i", "13", "a_f", "9")
+        .add(id, "9", "level1_s", "hello0", "level2_s", "b", "a_i", "14", "a_f", "10")
+        .commit(cluster.getSolrClient(), COLLECTION);
 
     String clause;
     TupleStream stream;
     List<Tuple> tuples;
     
     StreamFactory factory = new StreamFactory()
-      .withCollectionZkHost("collection1", zkServer.getZkAddress())
+      .withCollectionZkHost("collection1", cluster.getZkServer().getZkAddress())
       .withFunctionName("facet", FacetStream.class)
       .withFunctionName("sum", SumMetric.class)
       .withFunctionName("min", MinMetric.class)
@@ -2330,26 +2237,26 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
     assertTrue(sumi.longValue() == 2);
     assertTrue(count.doubleValue() == 2);
 
-    del("*:*");
-    commit();
   }
 
-  private void testTopicStream() throws Exception{
-    indexr(id, "0", "a_s", "hello", "a_i", "0", "a_f", "1");
-    indexr(id, "2", "a_s", "hello", "a_i", "2", "a_f", "2");
-    indexr(id, "3", "a_s", "hello", "a_i", "3", "a_f", "3");
-    indexr(id, "4", "a_s", "hello", "a_i", "4", "a_f", "4");
-    indexr(id, "1", "a_s", "hello", "a_i", "1", "a_f", "5");
-    indexr(id, "5", "a_s", "hello", "a_i", "10", "a_f", "6");
-    indexr(id, "6", "a_s", "hello", "a_i", "11", "a_f", "7");
-    indexr(id, "7", "a_s", "hello", "a_i", "12", "a_f", "8");
-    indexr(id, "8", "a_s", "hello", "a_i", "13", "a_f", "9");
-    indexr(id, "9", "a_s", "hello", "a_i", "14", "a_f", "10");
-
-    commit();
+  @Test
+  public void testTopicStream() throws Exception {
+
+    new UpdateRequest()
+        .add(id, "0", "a_s", "hello", "a_i", "0", "a_f", "1")
+        .add(id, "2", "a_s", "hello", "a_i", "2", "a_f", "2")
+        .add(id, "3", "a_s", "hello", "a_i", "3", "a_f", "3")
+        .add(id, "4", "a_s", "hello", "a_i", "4", "a_f", "4")
+        .add(id, "1", "a_s", "hello", "a_i", "1", "a_f", "5")
+        .add(id, "5", "a_s", "hello", "a_i", "10", "a_f", "6")
+        .add(id, "6", "a_s", "hello", "a_i", "11", "a_f", "7")
+        .add(id, "7", "a_s", "hello", "a_i", "12", "a_f", "8")
+        .add(id, "8", "a_s", "hello", "a_i", "13", "a_f", "9")
+        .add(id, "9", "a_s", "hello", "a_i", "14", "a_f", "10")
+        .commit(cluster.getSolrClient(), COLLECTION);
 
     StreamFactory factory = new StreamFactory()
-        .withCollectionZkHost("collection1", zkServer.getZkAddress())
+        .withCollectionZkHost("collection1", cluster.getZkServer().getZkAddress())
         .withFunctionName("topic", TopicStream.class)
         .withFunctionName("search", CloudSolrStream.class)
         .withFunctionName("daemon", DaemonStream.class);
@@ -2373,7 +2280,7 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
       //Should be zero because the checkpoints will be set to the highest vesion on the shards.
       assertEquals(tuples.size(), 0);
 
-      commit();
+      cluster.getSolrClient().commit("collection1");
       //Now check to see if the checkpoints are present
 
               expression = StreamExpressionParser.parse("search(collection1, q=\"id:1000000\", fl=\"id, checkpoint_ss, _version_\", sort=\"id asc\")");
@@ -2388,10 +2295,10 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
               Long version1 = tuples.get(0).getLong("_version_");
 
       //Index a few more documents
-      indexr(id, "10", "a_s", "hello", "a_i", "13", "a_f", "9");
-      indexr(id, "11", "a_s", "hello", "a_i", "14", "a_f", "10");
-
-      commit();
+      new UpdateRequest()
+          .add(id, "10", "a_s", "hello", "a_i", "13", "a_f", "9")
+          .add(id, "11", "a_s", "hello", "a_i", "14", "a_f", "10")
+          .commit(cluster.getSolrClient(), COLLECTION);
 
       expression = StreamExpressionParser.parse("topic(collection1, collection1, fl=\"id\", q=\"a_s:hello\", id=\"1000000\", checkpointEvery=2)");
 
@@ -2404,7 +2311,7 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
         stream.open();
         Tuple tuple1 = stream.read();
         assertEquals((long) tuple1.getLong("id"), 10l);
-        commit();
+        cluster.getSolrClient().commit("collection1");
 
                 // Checkpoint should not have changed.
                 expression = StreamExpressionParser.parse("search(collection1, q=\"id:1000000\", fl=\"id, checkpoint_ss, _version_\", sort=\"id asc\")");
@@ -2421,7 +2328,7 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
                 assertEquals(version1, version2);
 
         Tuple tuple2 = stream.read();
-        commit();
+        cluster.getSolrClient().commit("collection1");
         assertEquals((long) tuple2.getLong("id"), 11l);
 
                 //Checkpoint should have changed.
@@ -2455,9 +2362,10 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
         dstream.setStreamContext(context);
 
         //Index a few more documents
-        indexr(id, "12", "a_s", "hello", "a_i", "13", "a_f", "9");
-        indexr(id, "13", "a_s", "hello", "a_i", "14", "a_f", "10");
-        commit();
+        new UpdateRequest()
+            .add(id, "12", "a_s", "hello", "a_i", "13", "a_f", "9")
+            .add(id, "13", "a_s", "hello", "a_i", "14", "a_f", "10")
+            .commit(cluster.getSolrClient(), COLLECTION);
 
         //Start reading from the DaemonStream
         Tuple tuple = null;
@@ -2467,12 +2375,13 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
         assertEquals(12, (long) tuple.getLong(id));
         tuple = dstream.read();
         assertEquals(13, (long) tuple.getLong(id));
-        commit(); // We want to see if the version has been updated after reading two tuples
+        cluster.getSolrClient().commit("collection1"); // We want to see if the version has been updated after reading two tuples
 
         //Index a few more documents
-        indexr(id, "14", "a_s", "hello", "a_i", "13", "a_f", "9");
-        indexr(id, "15", "a_s", "hello", "a_i", "14", "a_f", "10");
-        commit();
+        new UpdateRequest()
+            .add(id, "14", "a_s", "hello", "a_i", "13", "a_f", "9")
+            .add(id, "15", "a_s", "hello", "a_i", "14", "a_f", "10")
+            .commit(cluster.getSolrClient(), COLLECTION);
 
         //Read from the same DaemonStream stream
 
@@ -2488,31 +2397,32 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
         dstream.close();
       }
     } finally {
-      del("*:*");
-      commit();
       cache.close();
     }
   }
 
-  private void testUpdateStream() throws Exception {
-    CloudSolrClient destinationCollectionClient = createCloudClient("destinationCollection");
-    createCollection("destinationCollection", destinationCollectionClient, 2, 2);
-    
-    indexr(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "0", "s_multi", "aaaa",  "s_multi", "bbbb",  "i_multi", "4", "i_multi", "7");
-    indexr(id, "2", "a_s", "hello2", "a_i", "2", "a_f", "0", "s_multi", "aaaa1", "s_multi", "bbbb1", "i_multi", "44", "i_multi", "77");
-    indexr(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3", "s_multi", "aaaa2", "s_multi", "bbbb2", "i_multi", "444", "i_multi", "777");
-    indexr(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4", "s_multi", "aaaa3", "s_multi", "bbbb3", "i_multi", "4444", "i_multi", "7777");
-    indexr(id, "1", "a_s", "hello1", "a_i", "1", "a_f", "1", "s_multi", "aaaa4", "s_multi", "bbbb4", "i_multi", "44444", "i_multi", "77777");
-    commit();
-    waitForRecoveriesToFinish("destinationCollection", false);
+  @Test
+  public void testUpdateStream() throws Exception {
+
+    CollectionAdminRequest.createCollection("destinationCollection", "conf", 2, 1).process(cluster.getSolrClient());
+    AbstractDistribZkTestBase.waitForRecoveriesToFinish("destinationCollection", cluster.getSolrClient().getZkStateReader(),
+        false, true, TIMEOUT);
+
+    new UpdateRequest()
+        .add(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "0", "s_multi", "aaaa",  "s_multi", "bbbb",  "i_multi", "4", "i_multi", "7")
+        .add(id, "2", "a_s", "hello2", "a_i", "2", "a_f", "0", "s_multi", "aaaa1", "s_multi", "bbbb1", "i_multi", "44", "i_multi", "77")
+        .add(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3", "s_multi", "aaaa2", "s_multi", "bbbb2", "i_multi", "444", "i_multi", "777")
+        .add(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4", "s_multi", "aaaa3", "s_multi", "bbbb3", "i_multi", "4444", "i_multi", "7777")
+        .add(id, "1", "a_s", "hello1", "a_i", "1", "a_f", "1", "s_multi", "aaaa4", "s_multi", "bbbb4", "i_multi", "44444", "i_multi", "77777")
+        .commit(cluster.getSolrClient(), "collection1");
     
     StreamExpression expression;
     TupleStream stream;
     Tuple t;
     
     StreamFactory factory = new StreamFactory()
-      .withCollectionZkHost("collection1", zkServer.getZkAddress())
-      .withCollectionZkHost("destinationCollection", zkServer.getZkAddress())
+      .withCollectionZkHost("collection1", cluster.getZkServer().getZkAddress())
+      .withCollectionZkHost("destinationCollection", cluster.getZkServer().getZkAddress())
       .withFunctionName("search", CloudSolrStream.class)
       .withFunctionName("update", UpdateStream.class);
     
@@ -2520,7 +2430,7 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
     expression = StreamExpressionParser.parse("update(destinationCollection, batchSize=5, search(collection1, q=*:*, fl=\"id,a_s,a_i,a_f,s_multi,i_multi\", sort=\"a_f asc, a_i asc\"))");
     stream = new UpdateStream(expression, factory);
     List<Tuple> tuples = getTuples(stream);
-    destinationCollectionClient.commit();
+    cluster.getSolrClient().commit("destinationCollection");
     
     //Ensure that all UpdateStream tuples indicate the correct number of copied/indexed docs
     assert(tuples.size() == 1);
@@ -2573,34 +2483,31 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
     assert(tuple.getDouble("a_f") == 4.0);
     assertList(tuple.getStrings("s_multi"), "aaaa3", "bbbb3");
     assertList(tuple.getLongs("i_multi"), Long.parseLong("4444"), Long.parseLong("7777"));
-
-    destinationCollectionClient.deleteByQuery("*:*");
-    destinationCollectionClient.commit();
-    destinationCollectionClient.close();
-    del("*:*");
-    commit();
   }
-  
-  private void testParallelUpdateStream() throws Exception {
-    CloudSolrClient destinationCollectionClient = createCloudClient("parallelDestinationCollection");
-    createCollection("parallelDestinationCollection", destinationCollectionClient, 2, 2);
-
-    indexr(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "0", "s_multi", "aaaa",  "s_multi", "bbbb",  "i_multi", "4", "i_multi", "7");
-    indexr(id, "2", "a_s", "hello2", "a_i", "2", "a_f", "0", "s_multi", "aaaa1", "s_multi", "bbbb1", "i_multi", "44", "i_multi", "77");
-    indexr(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3", "s_multi", "aaaa2", "s_multi", "bbbb2", "i_multi", "444", "i_multi", "777");
-    indexr(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4", "s_multi", "aaaa3", "s_multi", "bbbb3", "i_multi", "4444", "i_multi", "7777");
-    indexr(id, "1", "a_s", "hello1", "a_i", "1", "a_f", "1", "s_multi", "aaaa4", "s_multi", "bbbb4", "i_multi", "44444", "i_multi", "77777");
-    commit();
-    waitForRecoveriesToFinish("parallelDestinationCollection", false);
+
+  @Test
+  public void testParallelUpdateStream() throws Exception {
+
+    CollectionAdminRequest.createCollection("parallelDestinationCollection", "conf", 2, 1).process(cluster.getSolrClient());
+    AbstractDistribZkTestBase.waitForRecoveriesToFinish("parallelDestinationCollection", cluster.getSolrClient().getZkStateReader(),
+        false, true, TIMEOUT);
+
+    new UpdateRequest()
+        .add(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "0", "s_multi", "aaaa",  "s_multi", "bbbb",  "i_multi", "4", "i_multi", "7")
+        .add(id, "2", "a_s", "hello2", "a_i", "2", "a_f", "0", "s_multi", "aaaa1", "s_multi", "bbbb1", "i_multi", "44", "i_multi", "77")
+        .add(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3", "s_multi", "aaaa2", "s_multi", "bbbb2", "i_multi", "444", "i_multi", "777")
+        .add(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4", "s_multi", "aaaa3", "s_multi", "bbbb3", "i_multi", "4444", "i_multi", "7777")
+        .add(id, "1", "a_s", "hello1", "a_i", "1", "a_f", "1", "s_multi", "aaaa4", "s_multi", "bbbb4", "i_multi", "44444", "i_multi", "77777")
+        .commit(cluster.getSolrClient(), "collection1");
     
     StreamExpression expression;
     TupleStream stream;
     Tuple t;
     
-    String zkHost = zkServer.getZkAddress();
+    String zkHost = cluster.getZkServer().getZkAddress();
     StreamFactory factory = new StreamFactory()
-      .withCollectionZkHost("collection1", zkServer.getZkAddress())
-      .withCollectionZkHost("parallelDestinationCollection", zkServer.getZkAddress())
+      .withCollectionZkHost("collection1", cluster.getZkServer().getZkAddress())
+      .withCollectionZkHost("parallelDestinationCollection", cluster.getZkServer().getZkAddress())
       .withFunctionName("search", CloudSolrStream.class)
       .withFunctionName("update", UpdateStream.class)
       .withFunctionName("parallel", ParallelStream.class);
@@ -2609,7 +2516,7 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
     String updateExpression = "update(parallelDestinationCollection, batchSize=2, search(collection1, q=*:*, fl=\"id,a_s,a_i,a_f,s_multi,i_multi\", sort=\"a_f asc, a_i asc\", partitionKeys=\"a_f\"))";
     TupleStream parallelUpdateStream = factory.constructStream("parallel(collection1, " + updateExpression + ", workers=\"2\", zkHost=\""+zkHost+"\", sort=\"batchNumber asc\")");
     List<Tuple> tuples = getTuples(parallelUpdateStream);
-    destinationCollectionClient.commit();
+    cluster.getSolrClient().commit("parallelDestinationCollection");
     
     //Ensure that all UpdateStream tuples indicate the correct number of copied/indexed docs
     long count = 0;
@@ -2666,33 +2573,31 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
     assertList(tuple.getStrings("s_multi"), "aaaa3", "bbbb3");
     assertList(tuple.getLongs("i_multi"), Long.parseLong("4444"), Long.parseLong("7777"));
 
-    destinationCollectionClient.deleteByQuery("*:*");
-    destinationCollectionClient.commit();
-    destinationCollectionClient.close();
-    del("*:*");
-    commit();
   }
 
-  private void testParallelDaemonUpdateStream() throws Exception {
-    CloudSolrClient destinationCollectionClient = createCloudClient("parallelDestinationCollection1");
-    createCollection("parallelDestinationCollection1", destinationCollectionClient, 2, 2);
+  @Test
+  public void testParallelDaemonUpdateStream() throws Exception {
+
+    CollectionAdminRequest.createCollection("parallelDestinationCollection1", "conf", 2, 1).process(cluster.getSolrClient());
+    AbstractDistribZkTestBase.waitForRecoveriesToFinish("parallelDestinationCollection1", cluster.getSolrClient().getZkStateReader(),
+        false, true, TIMEOUT);
 
-    indexr(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "0", "s_multi", "aaaa",  "s_multi", "bbbb",  "i_multi", "4", "i_multi", "7");
-    indexr(id, "2", "a_s", "hello2", "a_i", "2", "a_f", "0", "s_multi", "aaaa1", "s_multi", "bbbb1", "i_multi", "44", "i_multi", "77");
-    indexr(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3", "s_multi", "aaaa2", "s_multi", "bbbb2", "i_multi", "444", "i_multi", "777");
-    indexr(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4", "s_multi", "aaaa3", "s_multi", "bbbb3", "i_multi", "4444", "i_multi", "7777");
-    indexr(id, "1", "a_s", "hello1", "a_i", "1", "a_f", "1", "s_multi", "aaaa4", "s_multi", "bbbb4", "i_multi", "44444", "i_multi", "77777");
-    commit();
-    waitForRecoveriesToFinish("parallelDestinationCollection1", false);
+    new UpdateRequest()
+        .add(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "0", "s_multi", "aaaa",  "s_multi", "bbbb",  "i_multi", "4", "i_multi", "7")
+        .add(id, "2", "a_s", "hello2", "a_i", "2", "a_f", "0", "s_multi", "aaaa1", "s_multi", "bbbb1", "i_multi", "44", "i_multi", "77")
+        .add(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3", "s_multi", "aaaa2", "s_multi", "bbbb2", "i_multi", "444", "i_multi", "777")
+        .add(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4", "s_multi", "aaaa3", "s_multi", "bbbb3", "i_multi", "4444", "i_multi", "7777")
+        .add(id, "1", "a_s", "hello1", "a_i", "1", "a_f", "1", "s_multi", "aaaa4", "s_multi", "bbbb4", "i_multi", "44444", "i_multi", "77777")
+        .commit(cluster.getSolrClient(), "collection1");
 
     StreamExpression expression;
     TupleStream stream;
     Tuple t;
 
-    String zkHost = zkServer.getZkAddress();
+    String zkHost = cluster.getZkServer().getZkAddress();
     StreamFactory factory = new StreamFactory()
-        .withCollectionZkHost("collection1", zkServer.getZkAddress())
-        .withCollectionZkHost("parallelDestinationCollection1", zkServer.getZkAddress())
+        .withCollectionZkHost("collection1", cluster.getZkServer().getZkAddress())
+        .withCollectionZkHost("parallelDestinationCollection1", cluster.getZkServer().getZkAddress())
         .withFunctionName("search", CloudSolrStream.class)
         .withFunctionName("update", UpdateStream.class)
         .withFunctionName("parallel", ParallelStream.class)
@@ -2711,11 +2616,11 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
     params.put("action","list");
 
     int workersComplete = 0;
-    for(CloudJettyRunner jetty : this.cloudJettys) {
+    for(JettySolrRunner jetty : cluster.getJettySolrRunners()) {
       int iterations = 0;
       INNER:
       while(iterations == 0) {
-        SolrStream solrStream = new SolrStream(jetty.url, params);
+        SolrStream solrStream = new SolrStream(jetty.getBaseUrl().toString() + "/collection1", params);
         solrStream.open();
         Tuple tupleResponse = solrStream.read();
         if (tupleResponse.EOF) {
@@ -2738,17 +2643,17 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
       }
     }
 
-    assert(workersComplete == 2);
+    assertEquals(cluster.getJettySolrRunners().size(), workersComplete);
 
-    destinationCollectionClient.commit();
+    cluster.getSolrClient().commit("parallelDestinationCollection1");
 
     //Lets stop the daemons
     params = new HashMap();
     params.put(CommonParams.QT,"/stream");
     params.put("action", "stop");
     params.put("id", "test");
-    for(CloudJettyRunner jetty : this.cloudJettys) {
-      SolrStream solrStream = new SolrStream(jetty.url, params);
+    for (JettySolrRunner jetty : cluster.getJettySolrRunners()) {
+      SolrStream solrStream = new SolrStream(jetty.getBaseUrl() + "/collection1", params);
       solrStream.open();
       Tuple tupleResponse = solrStream.read();
       solrStream.close();
@@ -2759,11 +2664,11 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
     params.put("action","list");
 
     workersComplete = 0;
-    for(CloudJettyRunner jetty : this.cloudJettys) {
+    for (JettySolrRunner jetty : cluster.getJettySolrRunners()) {
       long stopTime = 0;
       INNER:
       while(stopTime == 0) {
-        SolrStream solrStream = new SolrStream(jetty.url, params);
+        SolrStream solrStream = new SolrStream(jetty.getBaseUrl() + "/collection1", params);
         solrStream.open();
         Tuple tupleResponse = solrStream.read();
         if (tupleResponse.EOF) {
@@ -2785,7 +2690,7 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
       }
     }
 
-    assertTrue(workersComplete == 2);
+    assertEquals(cluster.getJettySolrRunners().size(), workersComplete);
     //Ensure that destinationCollection actually has the new docs.
     expression = StreamExpressionParser.parse("search(parallelDestinationCollection1, q=*:*, fl=\"id,a_s,a_i,a_f,s_multi,i_multi\", sort=\"a_i asc\")");
     stream = new CloudSolrStream(expression, factory);
@@ -2832,34 +2737,30 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
     assertList(tuple.getStrings("s_multi"), "aaaa3", "bbbb3");
     assertList(tuple.getLongs("i_multi"), Long.parseLong("4444"), Long.parseLong("7777"));
 
-    destinationCollectionClient.deleteByQuery("*:*");
-    destinationCollectionClient.commit();
-    destinationCollectionClient.close();
-    del("*:*");
-    commit();
   }
 
+  @Test
+  public void testIntersectStream() throws Exception {
 
-  
-  private void testIntersectStream() throws Exception{
-    indexr(id, "0", "a_s", "setA", "a_i", "0");
-    indexr(id, "2", "a_s", "setA", "a_i", "1");
-    indexr(id, "3", "a_s", "setA", "a_i", "2");
-    indexr(id, "4", "a_s", "setA", "a_i", "3");
-    
-    indexr(id, "5", "a_s", "setB", "a_i", "2");
-    indexr(id, "6", "a_s", "setB", "a_i", "3");
-    
-    indexr(id, "7", "a_s", "setAB", "a_i", "0");
-    indexr(id, "8", "a_s", "setAB", "a_i", "6");
-    commit();
+    new UpdateRequest()
+        .add(id, "0", "a_s", "setA", "a_i", "0")
+        .add(id, "2", "a_s", "setA", "a_i", "1")
+        .add(id, "3", "a_s", "setA", "a_i", "2")
+        .add(id, "4", "a_s", "setA", "a_i", "3")
+
+        .add(id, "5", "a_s", "setB", "a_i", "2")
+        .add(id, "6", "a_s", "setB", "a_i", "3")
+
+        .add(id, "7", "a_s", "setAB", "a_i", "0")
+        .add(id, "8", "a_s", "setAB", "a_i", "6")
+        .commit(cluster.getSolrClient(), COLLECTION);
     
     StreamExpression expression;
     TupleStream stream;
     List<Tuple> tuples;
     
     StreamFactory factory = new StreamFactory()
-      .withCollectionZkHost("collection1", zkServer.getZkAddress())
+      .withCollectionZkHost("collection1", cluster.getZkServer().getZkAddress())
       .withFunctionName("search", CloudSolrStream.class)
       .withFunctionName("intersect", IntersectStream.class);
       
@@ -2873,32 +2774,33 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
     
     assert(tuples.size() == 5);
     assertOrder(tuples, 0,7,3,4,8);
-    
-    del("*:*");
-    commit();
+
   }
-  
-  private void testParallelIntersectStream() throws Exception {
-    indexr(id, "0", "a_s", "setA", "a_i", "0");
-    indexr(id, "2", "a_s", "setA", "a_i", "1");
-    indexr(id, "3", "a_s", "setA", "a_i", "2");
-    indexr(id, "4", "a_s", "setA", "a_i", "3");
-    
-    indexr(id, "5", "a_s", "setB", "a_i", "2");
-    indexr(id, "6", "a_s", "setB", "a_i", "3");
-    
-    indexr(id, "7", "a_s", "setAB", "a_i", "0");
-    indexr(id, "8", "a_s", "setAB", "a_i", "6");
-    commit();
+
+  @Test
+  public void testParallelIntersectStream() throws Exception {
+
+    new UpdateRequest()
+        .add(id, "0", "a_s", "setA", "a_i", "0")
+        .add(id, "2", "a_s", "setA", "a_i", "1")
+        .add(id, "3", "a_s", "setA", "a_i", "2")
+        .add(id, "4", "a_s", "setA", "a_i", "3")
+
+        .add(id, "5", "a_s", "setB", "a_i", "2")
+        .add(id, "6", "a_s", "setB", "a_i", "3")
+
+        .add(id, "7", "a_s", "setAB", "a_i", "0")
+        .add(id, "8", "a_s", "setAB", "a_i", "6")
+        .commit(cluster.getSolrClient(), COLLECTION);
     
     StreamFactory streamFactory = new StreamFactory()
-      .withCollectionZkHost("collection1", zkServer.getZkAddress())
+      .withCollectionZkHost("collection1", cluster.getZkServer().getZkAddress())
       .withFunctionName("search", CloudSolrStream.class)
       .withFunctionName("intersect", IntersectStream.class)
       .withFunctionName("parallel", ParallelStream.class);
     // basic
     
-    String zkHost = zkServer.getZkAddress();
+    String zkHost = cluster.getZkServer().getZkAddress();
     final TupleStream stream = streamFactory.constructStream("parallel("
         + "collection1, "
         + "intersect("
@@ -2910,31 +2812,32 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
     
     assert(tuples.size() == 5);
     assertOrder(tuples, 0,7,3,4,8);
-    
-    del("*:*");
-    commit();
+
   }
-  
-  private void testComplementStream() throws Exception{
-    indexr(id, "0", "a_s", "setA", "a_i", "0");
-    indexr(id, "2", "a_s", "setA", "a_i", "1");
-    indexr(id, "3", "a_s", "setA", "a_i", "2");
-    indexr(id, "4", "a_s", "setA", "a_i", "3");
-    
-    indexr(id, "5", "a_s", "setB", "a_i", "2");
-    indexr(id, "6", "a_s", "setB", "a_i", "3");
-    indexr(id, "9", "a_s", "setB", "a_i", "5");
-    
-    indexr(id, "7", "a_s", "setAB", "a_i", "0");
-    indexr(id, "8", "a_s", "setAB", "a_i", "6");
-    commit();
+
+  @Test
+  public void testComplementStream() throws Exception {
+
+    new UpdateRequest()
+        .add(id, "0", "a_s", "setA", "a_i", "0")
+        .add(id, "2", "a_s", "setA", "a_i", "1")
+        .add(id, "3", "a_s", "setA", "a_i", "2")
+        .add(id, "4", "a_s", "setA", "a_i", "3")
+
+        .add(id, "5", "a_s", "setB", "a_i", "2")
+        .add(id, "6", "a_s", "setB", "a_i", "3")
+        .add(id, "9", "a_s", "setB", "a_i", "5")
+
+        .add(id, "7", "a_s", "setAB", "a_i", "0")
+        .add(id, "8", "a_s", "setAB", "a_i", "6")
+        .commit(cluster.getSolrClient(), COLLECTION);
     
     StreamExpression expression;
     TupleStream stream;
     List<Tuple> tuples;
     
     StreamFactory factory = new StreamFactory()
-      .withCollectionZkHost("collection1", zkServer.getZkAddress())
+      .withCollectionZkHost("collection1", cluster.getZkServer().getZkAddress())
       .withFunctionName("search", CloudSolrStream.class)
       .withFunctionName("complement", ComplementStream.class);
       
@@ -2948,32 +2851,33 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
    
     assert(tuples.size() == 1);
     assertOrder(tuples, 2);
-    
-    del("*:*");
-    commit();
+
   }
-  
-  private void testParallelComplementStream() throws Exception {
-    indexr(id, "0", "a_s", "setA", "a_i", "0");
-    indexr(id, "2", "a_s", "setA", "a_i", "1");
-    indexr(id, "3", "a_s", "setA", "a_i", "2");
-    indexr(id, "4", "a_s", "setA", "a_i", "3");
-    
-    indexr(id, "5", "a_s", "setB", "a_i", "2");
-    indexr(id, "6", "a_s", "setB", "a_i", "3");
-    indexr(id, "9", "a_s", "setB", "a_i", "5");
-    
-    indexr(id, "7", "a_s", "setAB", "a_i", "0");
-    indexr(id, "8", "a_s", "setAB", "a_i", "6");
-    commit();
+
+  @Test
+  public void testParallelComplementStream() throws Exception {
+
+    new UpdateRequest()
+        .add(id, "0", "a_s", "setA", "a_i", "0")
+        .add(id, "2", "a_s", "setA", "a_i", "1")
+        .add(id, "3", "a_s", "setA", "a_i", "2")
+        .add(id, "4", "a_s", "setA", "a_i", "3")
+
+        .add(id, "5", "a_s", "setB", "a_i", "2")
+        .add(id, "6", "a_s", "setB", "a_i", "3")
+        .add(id, "9", "a_s", "setB", "a_i", "5")
+
+        .add(id, "7", "a_s", "setAB", "a_i", "0")
+        .add(id, "8", "a_s", "setAB", "a_i", "6")
+        .commit(cluster.getSolrClient(), COLLECTION);
     
     StreamFactory streamFactory = new StreamFactory()
-      .withCollectionZkHost("collection1", zkServer.getZkAddress())
+      .withCollectionZkHost("collection1", cluster.getZkServer().getZkAddress())
       .withFunctionName("search", CloudSolrStream.class)
       .withFunctionName("complement", ComplementStream.class)
       .withFunctionName("parallel", ParallelStream.class);
     
-    final String zkHost = zkServer.getZkAddress();
+    final String zkHost = cluster.getZkServer().getZkAddress();
     final TupleStream stream = streamFactory.constructStream("parallel("
       + "collection1, "
       + "complement("
@@ -2985,9 +2889,7 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
     
     assert(tuples.size() == 1);
     assertOrder(tuples, 2);
-    
-    del("*:*");
-    commit();
+
   }
   
   protected List<Tuple> getTuples(TupleStream tupleStream) throws IOException {
@@ -3120,10 +3022,4 @@ public class StreamExpressionTest extends AbstractFullDistribZkTestBase {
     return true;
   }
 
-
-  @Override
-  protected void indexr(Object... fields) throws Exception {
-    SolrInputDocument doc = getDoc(fields);
-    indexDoc(doc);
-  }
 }
diff --git a/solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamingTest.java b/solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamingTest.java
index 61253e1..9db02eb 100644
--- a/solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamingTest.java
+++ b/solr/solrj/src/test/org/apache/solr/client/solrj/io/stream/StreamingTest.java
@@ -16,22 +16,19 @@
  */
 package org.apache.solr.client.solrj.io.stream;
 
-import java.io.File;
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.HashMap;
-import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
 
 import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util.LuceneTestCase.Slow;
-import org.apache.solr.client.solrj.io.Tuple;
+import org.apache.solr.client.solrj.embedded.JettySolrRunner;
 import org.apache.solr.client.solrj.io.SolrClientCache;
+import org.apache.solr.client.solrj.io.Tuple;
 import org.apache.solr.client.solrj.io.comp.ComparatorOrder;
-import org.apache.solr.client.solrj.io.comp.MultipleFieldComparator;
 import org.apache.solr.client.solrj.io.comp.FieldComparator;
-import org.apache.solr.client.solrj.io.comp.StreamComparator;
+import org.apache.solr.client.solrj.io.comp.MultipleFieldComparator;
 import org.apache.solr.client.solrj.io.eq.FieldEqualitor;
 import org.apache.solr.client.solrj.io.ops.GroupOperation;
 import org.apache.solr.client.solrj.io.stream.expr.StreamFactory;
@@ -42,13 +39,13 @@ import org.apache.solr.client.solrj.io.stream.metrics.MeanMetric;
 import org.apache.solr.client.solrj.io.stream.metrics.Metric;
 import org.apache.solr.client.solrj.io.stream.metrics.MinMetric;
 import org.apache.solr.client.solrj.io.stream.metrics.SumMetric;
-import org.apache.solr.cloud.AbstractFullDistribZkTestBase;
-import org.apache.solr.cloud.AbstractZkTestCase;
-import org.apache.solr.common.SolrInputDocument;
-import org.junit.After;
-import org.junit.AfterClass;
+import org.apache.solr.client.solrj.request.CollectionAdminRequest;
+import org.apache.solr.client.solrj.request.UpdateRequest;
+import org.apache.solr.cloud.AbstractDistribZkTestBase;
+import org.apache.solr.cloud.SolrCloudTestCase;
 import org.junit.Before;
 import org.junit.BeforeClass;
+import org.junit.Ignore;
 import org.junit.Test;
 
 /**
@@ -57,106 +54,70 @@ import org.junit.Test;
  *
  **/
 
-@Slow
 @LuceneTestCase.SuppressCodecs({"Lucene3x", "Lucene40","Lucene41","Lucene42","Lucene45"})
-public class StreamingTest extends AbstractFullDistribZkTestBase {
-
-  private static final String SOLR_HOME = getFile("solrj" + File.separator + "solr").getAbsolutePath();
-  private StreamFactory streamFactory;
+public class StreamingTest extends SolrCloudTestCase {
 
-  static {
-    schemaString = "schema-streaming.xml";
-  }
+  public static final int TIMEOUT = 30;
 
-  @BeforeClass
-  public static void beforeSuperClass() {
-    AbstractZkTestCase.SOLRHOME = new File(SOLR_HOME());
-  }
+  public static final String COLLECTION = "streams";
 
-  @AfterClass
-  public static void afterSuperClass() {
+  private static final StreamFactory streamFactory = new StreamFactory()
+      .withFunctionName("search", CloudSolrStream.class)
+      .withFunctionName("merge", MergeStream.class)
+      .withFunctionName("unique", UniqueStream.class)
+      .withFunctionName("top", RankStream.class)
+      .withFunctionName("reduce", ReducerStream.class)
+      .withFunctionName("group", GroupOperation.class)
+      .withFunctionName("rollup", RollupStream.class)
+      .withFunctionName("parallel", ParallelStream.class);
 
-  }
+  private static String zkHost;
 
-  protected String getCloudSolrConfig() {
-    return "solrconfig-streaming.xml";
-  }
+  @BeforeClass
+  public static void configureCluster() throws Exception {
+    configureCluster(2)
+        .addConfig("conf", getFile("solrj").toPath().resolve("solr").resolve("configsets").resolve("streaming").resolve("conf"))
+        .configure();
 
+    CollectionAdminRequest.createCollection(COLLECTION, "conf", 2, 1).process(cluster.getSolrClient());
+    AbstractDistribZkTestBase.waitForRecoveriesToFinish(COLLECTION, cluster.getSolrClient().getZkStateReader(), false, true, TIMEOUT);
 
-  @Override
-  public String getSolrHome() {
-    return SOLR_HOME;
+    zkHost = cluster.getZkServer().getZkAddress();
+    streamFactory.withCollectionZkHost(COLLECTION, zkHost);
   }
 
-  public static String SOLR_HOME() {
-    return SOLR_HOME;
-  }
+  private static final String id = "id";
 
   @Before
-  @Override
-  public void setUp() throws Exception {
-    super.setUp();
-    // we expect this time of exception as shards go up and down...
-    //ignoreException(".*");
-    //System.setProperty("export.test", "true");
-    System.setProperty("numShards", Integer.toString(sliceCount));
+  public void clearCollection() throws Exception {
+    new UpdateRequest()
+        .deleteByQuery("*:*")
+        .commit(cluster.getSolrClient(), COLLECTION);
   }
 
-  @Override
-  @After
-  public void tearDown() throws Exception {
-    super.tearDown();
-    resetExceptionIgnores();
-  }
-
-  public StreamingTest() {
-    super();
-    sliceCount = 2;
-
-    streamFactory = new StreamFactory()
-                    .withFunctionName("search", CloudSolrStream.class)
-                    .withFunctionName("merge", MergeStream.class)
-                    .withFunctionName("unique", UniqueStream.class)
-                    .withFunctionName("top", RankStream.class)
-                    .withFunctionName("reduce", ReducerStream.class)
-                    .withFunctionName("group", GroupOperation.class)
-                    .withFunctionName("rollup", RollupStream.class)
-                    .withFunctionName("parallel", ParallelStream.class);
-  }
-
-  private void testUniqueStream() throws Exception {
+  @Test
+  public void testUniqueStream() throws Exception {
 
     //Test CloudSolrStream and UniqueStream
-
-    indexr(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "0");
-    indexr(id, "2", "a_s", "hello2", "a_i", "2", "a_f", "0");
-    indexr(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3");
-    indexr(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4");
-    indexr(id, "1", "a_s", "hello1", "a_i", "1", "a_f", "1");
-
-    commit();
-
-
-    String zkHost = zkServer.getZkAddress();
-    streamFactory.withCollectionZkHost("collection1", zkHost);
+    new UpdateRequest()
+        .add(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "0")
+        .add(id, "2", "a_s", "hello2", "a_i", "2", "a_f", "0")
+        .add(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3")
+        .add(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4")
+        .add(id, "1", "a_s", "hello1", "a_i", "1", "a_f", "1")
+        .commit(cluster.getSolrClient(), COLLECTION);
 
     Map params = mapParams("q","*:*","fl","id,a_s,a_i,a_f","sort", "a_f asc,a_i asc");
-    CloudSolrStream stream = new CloudSolrStream(zkHost, "collection1", params);
+    CloudSolrStream stream = new CloudSolrStream(zkHost, COLLECTION, params);
     UniqueStream ustream = new UniqueStream(stream, new FieldEqualitor("a_f"));
     List<Tuple> tuples = getTuples(ustream);
-    assert(tuples.size() == 4);
+    assertEquals(4, tuples.size());
     assertOrder(tuples, 0,1,3,4);
 
-    del("*:*");
-    commit();
-
   }
 
-
-  private void testSpacesInParams() throws Exception {
-
-    String zkHost = zkServer.getZkAddress();
-    streamFactory.withCollectionZkHost("collection1", zkHost);
+  @Test
+  public void testSpacesInParams() throws Exception {
 
     Map params = mapParams("q", "*:*", "fl", "id , a_s , a_i , a_f", "sort", "a_f  asc , a_i  asc");
 
@@ -164,66 +125,55 @@ public class StreamingTest extends AbstractFullDistribZkTestBase {
     //The constructor will throw an exception if the sort fields do not the
     //a value in the field list.
 
-    CloudSolrStream stream = new CloudSolrStream(zkHost, "collection1", params);
-
-    del("*:*");
-    commit();
-
+    CloudSolrStream stream = new CloudSolrStream("", "collection1", params);
   }
 
-  private void testNonePartitionKeys() throws Exception {
-
-    indexr(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "1");
-    indexr(id, "2", "a_s", "hello0", "a_i", "2", "a_f", "2");
-    indexr(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3");
-    indexr(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4");
-    indexr(id, "1", "a_s", "hello0", "a_i", "1", "a_f", "5");
-    indexr(id, "5", "a_s", "hello3", "a_i", "10", "a_f", "6");
-    indexr(id, "6", "a_s", "hello4", "a_i", "11", "a_f", "7");
-    indexr(id, "7", "a_s", "hello3", "a_i", "12", "a_f", "8");
-    indexr(id, "8", "a_s", "hello3", "a_i", "13", "a_f", "9");
-    indexr(id, "9", "a_s", "hello0", "a_i", "14", "a_f", "10");
-
-    commit();
-
-    String zkHost = zkServer.getZkAddress();
-    streamFactory.withCollectionZkHost("collection1", zkHost);
+  @Test
+  public void testNonePartitionKeys() throws Exception {
+
+    new UpdateRequest()
+        .add(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "1")
+        .add(id, "2", "a_s", "hello0", "a_i", "2", "a_f", "2")
+        .add(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3")
+        .add(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4")
+        .add(id, "1", "a_s", "hello0", "a_i", "1", "a_f", "5")
+        .add(id, "5", "a_s", "hello3", "a_i", "10", "a_f", "6")
+        .add(id, "6", "a_s", "hello4", "a_i", "11", "a_f", "7")
+        .add(id, "7", "a_s", "hello3", "a_i", "12", "a_f", "8")
+        .add(id, "8", "a_s", "hello3", "a_i", "13", "a_f", "9")
+        .add(id, "9", "a_s", "hello0", "a_i", "14", "a_f", "10")
+        .commit(cluster.getSolrClient(), COLLECTION);
 
     Map paramsA = mapParams("q", "*:*", "fl", "id,a_s,a_i,a_f", "sort", "a_s asc,a_f asc", "partitionKeys", "none");
-    CloudSolrStream stream = new CloudSolrStream(zkHost, "collection1", paramsA);
-    ParallelStream pstream = new ParallelStream(zkHost, "collection1", stream, 2, new FieldComparator("a_s",ComparatorOrder.ASCENDING));
+    CloudSolrStream stream = new CloudSolrStream(zkHost, COLLECTION, paramsA);
+    ParallelStream pstream = new ParallelStream(zkHost, COLLECTION, stream, 2, new FieldComparator("a_s",ComparatorOrder.ASCENDING));
 
     attachStreamFactory(pstream);
     List<Tuple> tuples = getTuples(pstream);
 
     assert(tuples.size() == 20); // Each tuple will be double counted.
 
-    del("*:*");
-    commit();
-
   }
 
-  private void testParallelUniqueStream() throws Exception {
-
-    indexr(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "0");
-    indexr(id, "2", "a_s", "hello2", "a_i", "2", "a_f", "0");
-    indexr(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3");
-    indexr(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4");
-    indexr(id, "1", "a_s", "hello1", "a_i", "1", "a_f", "1");
-    indexr(id, "5", "a_s", "hello1", "a_i", "10", "a_f", "1");
-    indexr(id, "6", "a_s", "hello1", "a_i", "11", "a_f", "5");
-    indexr(id, "7", "a_s", "hello1", "a_i", "12", "a_f", "5");
-    indexr(id, "8", "a_s", "hello1", "a_i", "13", "a_f", "4");
-
-    commit();
-
-    String zkHost = zkServer.getZkAddress();
-    streamFactory.withCollectionZkHost("collection1", zkHost);
+  @Test
+  public void testParallelUniqueStream() throws Exception {
+
+    new UpdateRequest()
+        .add(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "0")
+        .add(id, "2", "a_s", "hello2", "a_i", "2", "a_f", "0")
+        .add(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3")
+        .add(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4")
+        .add(id, "1", "a_s", "hello1", "a_i", "1", "a_f", "1")
+        .add(id, "5", "a_s", "hello1", "a_i", "10", "a_f", "1")
+        .add(id, "6", "a_s", "hello1", "a_i", "11", "a_f", "5")
+        .add(id, "7", "a_s", "hello1", "a_i", "12", "a_f", "5")
+        .add(id, "8", "a_s", "hello1", "a_i", "13", "a_f", "4")
+        .commit(cluster.getSolrClient(), COLLECTION);
 
     Map params = mapParams("q","*:*","fl","id,a_s,a_i,a_f","sort", "a_f asc,a_i asc", "partitionKeys", "a_f");
-    CloudSolrStream stream = new CloudSolrStream(zkHost, "collection1", params);
+    CloudSolrStream stream = new CloudSolrStream(zkHost, COLLECTION, params);
     UniqueStream ustream = new UniqueStream(stream, new FieldEqualitor("a_f"));
-    ParallelStream pstream = new ParallelStream(zkHost, "collection1", ustream, 2, new FieldComparator("a_f",ComparatorOrder.ASCENDING));
+    ParallelStream pstream = new ParallelStream(zkHost, COLLECTION, ustream, 2, new FieldComparator("a_f",ComparatorOrder.ASCENDING));
     attachStreamFactory(pstream);
     List<Tuple> tuples = getTuples(pstream);
     assert(tuples.size() == 5);
@@ -234,129 +184,104 @@ public class StreamingTest extends AbstractFullDistribZkTestBase {
     Map<String,Tuple> eofTuples = pstream.getEofTuples();
     assert(eofTuples.size() == 2); //There should be an EOF tuple for each worker.
 
-    del("*:*");
-    commit();
-
   }
 
+  @Test
+  public void testRankStream() throws Exception {
 
+    new UpdateRequest()
+        .add(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "0")
+        .add(id, "2", "a_s", "hello2", "a_i", "2", "a_f", "0")
+        .add(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3")
+        .add(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4")
+        .add(id, "1", "a_s", "hello1", "a_i", "1", "a_f", "1")
+        .commit(cluster.getSolrClient(), COLLECTION);
 
-  private void testRankStream() throws Exception {
-
-
-    indexr(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "0");
-    indexr(id, "2", "a_s", "hello2", "a_i", "2", "a_f", "0");
-    indexr(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3");
-    indexr(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4");
-    indexr(id, "1", "a_s", "hello1", "a_i", "1", "a_f", "1");
-
-    commit();
-
-    String zkHost = zkServer.getZkAddress();
-    streamFactory.withCollectionZkHost("collection1", zkHost);
 
     Map params = mapParams("q", "*:*", "fl", "id,a_s,a_i", "sort", "a_i asc");
-    CloudSolrStream stream = new CloudSolrStream(zkHost, "collection1", params);
+    CloudSolrStream stream = new CloudSolrStream(zkHost, COLLECTION, params);
     RankStream rstream = new RankStream(stream, 3, new FieldComparator("a_i",ComparatorOrder.DESCENDING));
     List<Tuple> tuples = getTuples(rstream);
 
-
     assert(tuples.size() == 3);
     assertOrder(tuples, 4,3,2);
 
-    del("*:*");
-    commit();
   }
 
-  private void testParallelRankStream() throws Exception {
-
-
-    indexr(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "0");
-    indexr(id, "2", "a_s", "hello2", "a_i", "2", "a_f", "0");
-    indexr(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3");
-    indexr(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4");
-    indexr(id, "5", "a_s", "hello1", "a_i", "5", "a_f", "1");
-    indexr(id, "6", "a_s", "hello1", "a_i", "6", "a_f", "1");
-    indexr(id, "7", "a_s", "hello1", "a_i", "7", "a_f", "1");
-    indexr(id, "8", "a_s", "hello1", "a_i", "8", "a_f", "1");
-    indexr(id, "9", "a_s", "hello1", "a_i", "9", "a_f", "1");
-    indexr(id, "10", "a_s", "hello1", "a_i", "10", "a_f", "1");
-
-    commit();
-
-    String zkHost = zkServer.getZkAddress();
-    streamFactory.withCollectionZkHost("collection1", zkHost);
+  @Test
+  public void testParallelRankStream() throws Exception {
+
+    new UpdateRequest()
+        .add(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "0")
+        .add(id, "2", "a_s", "hello2", "a_i", "2", "a_f", "0")
+        .add(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3")
+        .add(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4")
+        .add(id, "5", "a_s", "hello1", "a_i", "5", "a_f", "1")
+        .add(id, "6", "a_s", "hello1", "a_i", "6", "a_f", "1")
+        .add(id, "7", "a_s", "hello1", "a_i", "7", "a_f", "1")
+        .add(id, "8", "a_s", "hello1", "a_i", "8", "a_f", "1")
+        .add(id, "9", "a_s", "hello1", "a_i", "9", "a_f", "1")
+        .add(id, "10", "a_s", "hello1", "a_i", "10", "a_f", "1")
+        .commit(cluster.getSolrClient(), COLLECTION);
 
     Map params = mapParams("q", "*:*", "fl", "id,a_s,a_i", "sort", "a_i asc", "partitionKeys", "a_i");
-    CloudSolrStream stream = new CloudSolrStream(zkHost, "collection1", params);
+    CloudSolrStream stream = new CloudSolrStream(zkHost, COLLECTION, params);
     RankStream rstream = new RankStream(stream, 11, new FieldComparator("a_i",ComparatorOrder.DESCENDING));
-    ParallelStream pstream = new ParallelStream(zkHost, "collection1", rstream, 2, new FieldComparator("a_i",ComparatorOrder.DESCENDING));
+    ParallelStream pstream = new ParallelStream(zkHost, COLLECTION, rstream, 2, new FieldComparator("a_i",ComparatorOrder.DESCENDING));
     attachStreamFactory(pstream);
     List<Tuple> tuples = getTuples(pstream);
 
     assert(tuples.size() == 10);
     assertOrder(tuples, 10,9,8,7,6,5,4,3,2,0);
 
-    del("*:*");
-    commit();
   }
 
-  private void testTrace() throws Exception {
-
-    indexr(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "1");
-    indexr(id, "2", "a_s", "hello0", "a_i", "2", "a_f", "2");
-    indexr(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3");
-    indexr(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4");
-    indexr(id, "1", "a_s", "hello0", "a_i", "1", "a_f", "5");
-    indexr(id, "5", "a_s", "hello3", "a_i", "10", "a_f", "6");
-    indexr(id, "6", "a_s", "hello4", "a_i", "11", "a_f", "7");
-    indexr(id, "7", "a_s", "hello3", "a_i", "12", "a_f", "8");
-    indexr(id, "8", "a_s", "hello3", "a_i", "13", "a_f", "9");
-    indexr(id, "9", "a_s", "hello0", "a_i", "14", "a_f", "10");
-
-    commit();
-
-    String zkHost = zkServer.getZkAddress();
-    streamFactory.withCollectionZkHost("collection1", zkHost);
+  @Test
+  public void testTrace() throws Exception {
+
+    new UpdateRequest()
+        .add(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "1")
+        .add(id, "2", "a_s", "hello0", "a_i", "2", "a_f", "2")
+        .add(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3")
+        .add(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4")
+        .add(id, "1", "a_s", "hello0", "a_i", "1", "a_f", "5")
+        .add(id, "5", "a_s", "hello3", "a_i", "10", "a_f", "6")
+        .add(id, "6", "a_s", "hello4", "a_i", "11", "a_f", "7")
+        .add(id, "7", "a_s", "hello3", "a_i", "12", "a_f", "8")
+        .add(id, "8", "a_s", "hello3", "a_i", "13", "a_f", "9")
+        .add(id, "9", "a_s", "hello0", "a_i", "14", "a_f", "10")
+        .commit(cluster.getSolrClient(), COLLECTION);
 
     //Test with spaces in the parameter lists.
     Map paramsA = mapParams("q","*:*","fl","id,a_s, a_i,  a_f","sort", "a_s asc  ,  a_f   asc");
-    CloudSolrStream stream = new CloudSolrStream(zkHost, "collection1", paramsA);
+    CloudSolrStream stream = new CloudSolrStream(zkHost, COLLECTION, paramsA);
     stream.setTrace(true);
     List<Tuple> tuples = getTuples(stream);
-    assert(tuples.get(0).get("_COLLECTION_").equals("collection1"));
-    assert(tuples.get(1).get("_COLLECTION_").equals("collection1"));
-    assert(tuples.get(2).get("_COLLECTION_").equals("collection1"));
-    assert(tuples.get(3).get("_COLLECTION_").equals("collection1"));
-
-    del("*:*");
-    commit();
+    assert(tuples.get(0).get("_COLLECTION_").equals(COLLECTION));
+    assert(tuples.get(1).get("_COLLECTION_").equals(COLLECTION));
+    assert(tuples.get(2).get("_COLLECTION_").equals(COLLECTION));
+    assert(tuples.get(3).get("_COLLECTION_").equals(COLLECTION));
   }
 
-
-
-
-  private void testReducerStream() throws Exception {
-
-    indexr(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "1");
-    indexr(id, "2", "a_s", "hello0", "a_i", "2", "a_f", "2");
-    indexr(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3");
-    indexr(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4");
-    indexr(id, "1", "a_s", "hello0", "a_i", "1", "a_f", "5");
-    indexr(id, "5", "a_s", "hello3", "a_i", "10", "a_f", "6");
-    indexr(id, "6", "a_s", "hello4", "a_i", "11", "a_f", "7");
-    indexr(id, "7", "a_s", "hello3", "a_i", "12", "a_f", "8");
-    indexr(id, "8", "a_s", "hello3", "a_i", "13", "a_f", "9");
-    indexr(id, "9", "a_s", "hello0", "a_i", "14", "a_f", "10");
-
-    commit();
-
-    String zkHost = zkServer.getZkAddress();
-    streamFactory.withCollectionZkHost("collection1", zkHost);
+  @Test
+  public void testReducerStream() throws Exception {
+
+    new UpdateRequest()
+        .add(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "1")
+        .add(id, "2", "a_s", "hello0", "a_i", "2", "a_f", "2")
+        .add(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3")
+        .add(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4")
+        .add(id, "1", "a_s", "hello0", "a_i", "1", "a_f", "5")
+        .add(id, "5", "a_s", "hello3", "a_i", "10", "a_f", "6")
+        .add(id, "6", "a_s", "hello4", "a_i", "11", "a_f", "7")
+        .add(id, "7", "a_s", "hello3", "a_i", "12", "a_f", "8")
+        .add(id, "8", "a_s", "hello3", "a_i", "13", "a_f", "9")
+        .add(id, "9", "a_s", "hello0", "a_i", "14", "a_f", "10")
+        .commit(cluster.getSolrClient(), COLLECTION);
 
     //Test with spaces in the parameter lists.
     Map paramsA = mapParams("q","*:*","fl","id,a_s, a_i,  a_f","sort", "a_s asc  ,  a_f   asc");
-    CloudSolrStream stream = new CloudSolrStream(zkHost, "collection1", paramsA);
+    CloudSolrStream stream = new CloudSolrStream(zkHost, COLLECTION, paramsA);
     ReducerStream rstream  = new ReducerStream(stream,
                                                new FieldEqualitor("a_s"),
                                                new GroupOperation(new FieldComparator("a_f", ComparatorOrder.ASCENDING), 5));
@@ -379,7 +304,7 @@ public class StreamingTest extends AbstractFullDistribZkTestBase {
 
     //Test with spaces in the parameter lists using a comparator
     paramsA = mapParams("q","*:*","fl","id,a_s, a_i,  a_f","sort", "a_s asc  ,  a_f   asc");
-    stream  = new CloudSolrStream(zkHost, "collection1", paramsA);
+    stream  = new CloudSolrStream(zkHost, COLLECTION, paramsA);
     rstream = new ReducerStream(stream,
                                 new FieldComparator("a_s", ComparatorOrder.ASCENDING),
                                 new GroupOperation(new FieldComparator("a_f", ComparatorOrder.DESCENDING), 5));
@@ -400,32 +325,28 @@ public class StreamingTest extends AbstractFullDistribZkTestBase {
     maps2 = t2.getMaps("group");
     assertMaps(maps2, 6, 4);
 
-    del("*:*");
-    commit();
   }
 
-  private void testZeroReducerStream() throws Exception {
+  @Test
+  public void testZeroReducerStream() throws Exception {
 
     //Gracefully handle zero results
-    indexr(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "1");
-    indexr(id, "2", "a_s", "hello0", "a_i", "2", "a_f", "2");
-    indexr(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3");
-    indexr(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4");
-    indexr(id, "1", "a_s", "hello0", "a_i", "1", "a_f", "5");
-    indexr(id, "5", "a_s", "hello3", "a_i", "10", "a_f", "6");
-    indexr(id, "6", "a_s", "hello4", "a_i", "11", "a_f", "7");
-    indexr(id, "7", "a_s", "hello3", "a_i", "12", "a_f", "8");
-    indexr(id, "8", "a_s", "hello3", "a_i", "13", "a_f", "9");
-    indexr(id, "9", "a_s", "hello0", "a_i", "14", "a_f", "10");
-
-    commit();
-
-    String zkHost = zkServer.getZkAddress();
-    streamFactory.withCollectionZkHost("collection1", zkHost);
+    new UpdateRequest()
+        .add(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "1")
+        .add(id, "2", "a_s", "hello0", "a_i", "2", "a_f", "2")
+        .add(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3")
+        .add(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4")
+        .add(id, "1", "a_s", "hello0", "a_i", "1", "a_f", "5")
+        .add(id, "5", "a_s", "hello3", "a_i", "10", "a_f", "6")
+        .add(id, "6", "a_s", "hello4", "a_i", "11", "a_f", "7")
+        .add(id, "7", "a_s", "hello3", "a_i", "12", "a_f", "8")
+        .add(id, "8", "a_s", "hello3", "a_i", "13", "a_f", "9")
+        .add(id, "9", "a_s", "hello0", "a_i", "14", "a_f", "10")
+        .commit(cluster.getSolrClient(), COLLECTION);
 
     //Test with spaces in the parameter lists.
     Map paramsA = mapParams("q", "blah", "fl", "id,a_s, a_i,  a_f", "sort", "a_s asc  ,  a_f   asc");
-    CloudSolrStream stream = new CloudSolrStream(zkHost, "collection1", paramsA);
+    CloudSolrStream stream = new CloudSolrStream(zkHost, COLLECTION, paramsA);
     ReducerStream rstream = new ReducerStream(stream,
                                               new FieldEqualitor("a_s"),
                                               new GroupOperation(new FieldComparator("a_f", ComparatorOrder.ASCENDING), 5));
@@ -434,37 +355,32 @@ public class StreamingTest extends AbstractFullDistribZkTestBase {
 
     assert(tuples.size() == 0);
 
-    del("*:*");
-    commit();
   }
 
-
-  private void testParallelReducerStream() throws Exception {
-
-    indexr(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "1");
-    indexr(id, "2", "a_s", "hello0", "a_i", "2", "a_f", "2");
-    indexr(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3");
-    indexr(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4");
-    indexr(id, "1", "a_s", "hello0", "a_i", "1", "a_f", "5");
-    indexr(id, "5", "a_s", "hello3", "a_i", "10", "a_f", "6");
-    indexr(id, "6", "a_s", "hello4", "a_i", "11", "a_f", "7");
-    indexr(id, "7", "a_s", "hello3", "a_i", "12", "a_f", "8");
-    indexr(id, "8", "a_s", "hello3", "a_i", "13", "a_f", "9");
-    indexr(id, "9", "a_s", "hello0", "a_i", "14", "a_f", "10");
-
-    commit();
-
-    String zkHost = zkServer.getZkAddress();
-    streamFactory.withCollectionZkHost("collection1", zkHost);
+  @Test
+  public void testParallelReducerStream() throws Exception {
+
+    new UpdateRequest()
+        .add(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "1")
+        .add(id, "2", "a_s", "hello0", "a_i", "2", "a_f", "2")
+        .add(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3")
+        .add(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4")
+        .add(id, "1", "a_s", "hello0", "a_i", "1", "a_f", "5")
+        .add(id, "5", "a_s", "hello3", "a_i", "10", "a_f", "6")
+        .add(id, "6", "a_s", "hello4", "a_i", "11", "a_f", "7")
+        .add(id, "7", "a_s", "hello3", "a_i", "12", "a_f", "8")
+        .add(id, "8", "a_s", "hello3", "a_i", "13", "a_f", "9")
+        .add(id, "9", "a_s", "hello0", "a_i", "14", "a_f", "10")
+        .commit(cluster.getSolrClient(), COLLECTION);
 
     Map paramsA = mapParams("q","*:*","fl","id,a_s,a_i,a_f","sort", "a_s asc,a_f asc", "partitionKeys", "a_s");
-    CloudSolrStream stream = new CloudSolrStream(zkHost, "collection1", paramsA);
+    CloudSolrStream stream = new CloudSolrStream(zkHost, COLLECTION, paramsA);
 
     ReducerStream rstream = new ReducerStream(stream,
                                               new FieldEqualitor("a_s"),
                                               new GroupOperation(new FieldComparator("a_f", ComparatorOrder.DESCENDING), 5));
 
-    ParallelStream pstream = new ParallelStream(zkHost, "collection1", rstream, 2, new FieldComparator("a_s",ComparatorOrder.ASCENDING));
+    ParallelStream pstream = new ParallelStream(zkHost, COLLECTION, rstream, 2, new FieldComparator("a_s",ComparatorOrder.ASCENDING));
 
     attachStreamFactory(pstream);
     List<Tuple> tuples = getTuples(pstream);
@@ -486,13 +402,13 @@ public class StreamingTest extends AbstractFullDistribZkTestBase {
     //Test Descending with Ascending subsort
 
     paramsA = mapParams("q","*:*","fl","id,a_s,a_i,a_f","sort", "a_s desc,a_f asc", "partitionKeys", "a_s");
-    stream = new CloudSolrStream(zkHost, "collection1", paramsA);
+    stream = new CloudSolrStream(zkHost, COLLECTION, paramsA);
 
     rstream = new ReducerStream(stream,
                                 new FieldEqualitor("a_s"),
                                 new GroupOperation(new FieldComparator("a_f", ComparatorOrder.ASCENDING), 3));
 
-    pstream = new ParallelStream(zkHost, "collection1", rstream, 2, new FieldComparator("a_s",ComparatorOrder.DESCENDING));
+    pstream = new ParallelStream(zkHost, COLLECTION, rstream, 2, new FieldComparator("a_s",ComparatorOrder.DESCENDING));
 
     attachStreamFactory(pstream);
     tuples = getTuples(pstream);
@@ -511,34 +427,28 @@ public class StreamingTest extends AbstractFullDistribZkTestBase {
     maps2 = t2.getMaps("group");
     assertMaps(maps2, 0, 2, 1);
 
-
-
-    del("*:*");
-    commit();
   }
 
-
-  private void testExceptionStream() throws Exception {
-
-    indexr(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "1");
-    indexr(id, "2", "a_s", "hello0", "a_i", "2", "a_f", "2");
-    indexr(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3");
-    indexr(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4");
-    indexr(id, "1", "a_s", "hello0", "a_i", "1", "a_f", "5");
-    indexr(id, "5", "a_s", "hello3", "a_i", "10", "a_f", "6");
-    indexr(id, "6", "a_s", "hello4", "a_i", "11", "a_f", "7");
-    indexr(id, "7", "a_s", "hello3", "a_i", "12", "a_f", "8");
-    indexr(id, "8", "a_s", "hello3", "a_i", "13", "a_f", "9");
-    indexr(id, "9", "a_s", "hello0", "a_i", "14", "a_f", "10");
-
-    commit();
-
-    String zkHost = zkServer.getZkAddress();
-
+  @Test
+  @Ignore
+  public void testExceptionStream() throws Exception {
+
+    new UpdateRequest()
+        .add(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "1")
+        .add(id, "2", "a_s", "hello0", "a_i", "2", "a_f", "2")
+        .add(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3")
+        .add(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4")
+        .add(id, "1", "a_s", "hello0", "a_i", "1", "a_f", "5")
+        .add(id, "5", "a_s", "hello3", "a_i", "10", "a_f", "6")
+        .add(id, "6", "a_s", "hello4", "a_i", "11", "a_f", "7")
+        .add(id, "7", "a_s", "hello3", "a_i", "12", "a_f", "8")
+        .add(id, "8", "a_s", "hello3", "a_i", "13", "a_f", "9")
+        .add(id, "9", "a_s", "hello0", "a_i", "14", "a_f", "10")
+        .commit(cluster.getSolrClient(), COLLECTION);
 
     //Test an error that comes originates from the /select handler
     Map paramsA = mapParams("q", "*:*", "fl", "a_s,a_i,a_f,blah", "sort", "blah asc");
-    CloudSolrStream stream = new CloudSolrStream(zkHost, "collection1", paramsA);
+    CloudSolrStream stream = new CloudSolrStream(zkHost, COLLECTION, paramsA);
     ExceptionStream estream = new ExceptionStream(stream);
     Tuple t = getTuple(estream);
     assert(t.EOF);
@@ -547,7 +457,7 @@ public class StreamingTest extends AbstractFullDistribZkTestBase {
 
     //Test an error that comes originates from the /export handler
     paramsA = mapParams("q", "*:*", "fl", "a_s,a_i,a_f,score", "sort", "a_s asc", "qt","/export");
-    stream = new CloudSolrStream(zkHost, "collection1", paramsA);
+    stream = new CloudSolrStream(zkHost, COLLECTION, paramsA);
     estream = new ExceptionStream(stream);
     t = getTuple(estream);
     assert(t.EOF);
@@ -556,26 +466,26 @@ public class StreamingTest extends AbstractFullDistribZkTestBase {
     assert(t.getException().contains("undefined field:"));
   }
 
-  private void testParallelExceptionStream() throws Exception {
-
-    indexr(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "1");
-    indexr(id, "2", "a_s", "hello0", "a_i", "2", "a_f", "2");
-    indexr(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3");
-    indexr(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4");
-    indexr(id, "1", "a_s", "hello0", "a_i", "1", "a_f", "5");
-    indexr(id, "5", "a_s", "hello3", "a_i", "10", "a_f", "6");
-    indexr(id, "6", "a_s", "hello4", "a_i", "11", "a_f", "7");
-    indexr(id, "7", "a_s", "hello3", "a_i", "12", "a_f", "8");
-    indexr(id, "8", "a_s", "hello3", "a_i", "13", "a_f", "9");
-    indexr(id, "9", "a_s", "hello0", "a_i", "14", "a_f", "10");
-
-    commit();
-
-    String zkHost = zkServer.getZkAddress();
+  @Test
+  @Ignore
+  public void testParallelExceptionStream() throws Exception {
+
+    new UpdateRequest()
+        .add(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "1")
+        .add(id, "2", "a_s", "hello0", "a_i", "2", "a_f", "2")
+        .add(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3")
+        .add(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4")
+        .add(id, "1", "a_s", "hello0", "a_i", "1", "a_f", "5")
+        .add(id, "5", "a_s", "hello3", "a_i", "10", "a_f", "6")
+        .add(id, "6", "a_s", "hello4", "a_i", "11", "a_f", "7")
+        .add(id, "7", "a_s", "hello3", "a_i", "12", "a_f", "8")
+        .add(id, "8", "a_s", "hello3", "a_i", "13", "a_f", "9")
+        .add(id, "9", "a_s", "hello0", "a_i", "14", "a_f", "10")
+        .commit(cluster.getSolrClient(), COLLECTION);
 
     Map paramsA = mapParams("q", "*:*", "fl", "a_s,a_i,a_f,blah", "sort", "blah asc");
-    CloudSolrStream stream = new CloudSolrStream(zkHost, "collection1", paramsA);
-    ParallelStream pstream = new ParallelStream(zkHost,"collection1", stream, 2, new FieldComparator("blah", ComparatorOrder.ASCENDING));
+    CloudSolrStream stream = new CloudSolrStream(zkHost, COLLECTION, paramsA);
+    ParallelStream pstream = new ParallelStream(zkHost, COLLECTION, stream, 2, new FieldComparator("blah", ComparatorOrder.ASCENDING));
     ExceptionStream estream = new ExceptionStream(pstream);
     Tuple t = getTuple(estream);
     assert(t.EOF);
@@ -586,8 +496,8 @@ public class StreamingTest extends AbstractFullDistribZkTestBase {
 
     //Test an error that originates from the /select handler
     paramsA = mapParams("q", "*:*", "fl", "a_s,a_i,a_f,blah", "sort", "blah asc", "partitionKeys","a_s");
-    stream = new CloudSolrStream(zkHost, "collection1", paramsA);
-    pstream = new ParallelStream(zkHost,"collection1", stream, 2, new FieldComparator("blah", ComparatorOrder.ASCENDING));
+    stream = new CloudSolrStream(zkHost, COLLECTION, paramsA);
+    pstream = new ParallelStream(zkHost, COLLECTION, stream, 2, new FieldComparator("blah", ComparatorOrder.ASCENDING));
     estream = new ExceptionStream(pstream);
     t = getTuple(estream);
     assert(t.EOF);
@@ -597,8 +507,8 @@ public class StreamingTest extends AbstractFullDistribZkTestBase {
 
     //Test an error that originates from the /export handler
     paramsA = mapParams("q", "*:*", "fl", "a_s,a_i,a_f,score", "sort", "a_s asc", "qt","/export", "partitionKeys","a_s");
-    stream = new CloudSolrStream(zkHost, "collection1", paramsA);
-    pstream = new ParallelStream(zkHost,"collection1", stream, 2, new FieldComparator("a_s", ComparatorOrder.ASCENDING));
+    stream = new CloudSolrStream(zkHost, COLLECTION, paramsA);
+    pstream = new ParallelStream(zkHost, COLLECTION, stream, 2, new FieldComparator("a_s", ComparatorOrder.ASCENDING));
     estream = new ExceptionStream(pstream);
     t = getTuple(estream);
     assert(t.EOF);
@@ -607,23 +517,21 @@ public class StreamingTest extends AbstractFullDistribZkTestBase {
     assert(t.getException().contains("undefined field:"));
   }
 
-
-  private void testStatsStream() throws Exception {
-
-    indexr(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "1");
-    indexr(id, "2", "a_s", "hello0", "a_i", "2", "a_f", "2");
-    indexr(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3");
-    indexr(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4");
-    indexr(id, "1", "a_s", "hello0", "a_i", "1", "a_f", "5");
-    indexr(id, "5", "a_s", "hello3", "a_i", "10", "a_f", "6");
-    indexr(id, "6", "a_s", "hello4", "a_i", "11", "a_f", "7");
-    indexr(id, "7", "a_s", "hello3", "a_i", "12", "a_f", "8");
-    indexr(id, "8", "a_s", "hello3", "a_i", "13", "a_f", "9");
-    indexr(id, "9", "a_s", "hello0", "a_i", "14", "a_f", "10");
-
-    commit();
-
-    String zkHost = zkServer.getZkAddress();
+  @Test
+  public void testStatsStream() throws Exception {
+
+    new UpdateRequest()
+        .add(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "1")
+        .add(id, "2", "a_s", "hello0", "a_i", "2", "a_f", "2")
+        .add(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3")
+        .add(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4")
+        .add(id, "1", "a_s", "hello0", "a_i", "1", "a_f", "5")
+        .add(id, "5", "a_s", "hello3", "a_i", "10", "a_f", "6")
+        .add(id, "6", "a_s", "hello4", "a_i", "11", "a_f", "7")
+        .add(id, "7", "a_s", "hello3", "a_i", "12", "a_f", "8")
+        .add(id, "8", "a_s", "hello3", "a_i", "13", "a_f", "9")
+        .add(id, "9", "a_s", "hello0", "a_i", "14", "a_f", "10")
+        .commit(cluster.getSolrClient(), COLLECTION);
 
     Map paramsA = mapParams("q", "*:*");
 
@@ -637,10 +545,7 @@ public class StreamingTest extends AbstractFullDistribZkTestBase {
                         new MeanMetric("a_f"),
                         new CountMetric()};
 
-    StatsStream statsStream = new StatsStream(zkHost,
-                                              "collection1",
-                                              paramsA,
-                                              metrics);
+    StatsStream statsStream = new StatsStream(zkHost, COLLECTION, paramsA, metrics);
 
     List<Tuple> tuples = getTuples(statsStream);
 
@@ -670,26 +575,23 @@ public class StreamingTest extends AbstractFullDistribZkTestBase {
     assertTrue(avgf.doubleValue() == 5.5D);
     assertTrue(count.doubleValue() == 10);
 
-    del("*:*");
-    commit();
   }
 
-  private void testFacetStream() throws Exception {
-
-    indexr(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "1");
-    indexr(id, "2", "a_s", "hello0", "a_i", "2", "a_f", "2");
-    indexr(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3");
-    indexr(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4");
-    indexr(id, "1", "a_s", "hello0", "a_i", "1", "a_f", "5");
-    indexr(id, "5", "a_s", "hello3", "a_i", "10", "a_f", "6");
-    indexr(id, "6", "a_s", "hello4", "a_i", "11", "a_f", "7");
-    indexr(id, "7", "a_s", "hello3", "a_i", "12", "a_f", "8");
-    indexr(id, "8", "a_s", "hello3", "a_i", "13", "a_f", "9");
-    indexr(id, "9", "a_s", "hello0", "a_i", "14", "a_f", "10");
-
-    commit();
-
-    String zkHost = zkServer.getZkAddress();
+  @Test
+  public void testFacetStream() throws Exception {
+
+    new UpdateRequest()
+        .add(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "1")
+        .add(id, "2", "a_s", "hello0", "a_i", "2", "a_f", "2")
+        .add(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3")
+        .add(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4")
+        .add(id, "1", "a_s", "hello0", "a_i", "1", "a_f", "5")
+        .add(id, "5", "a_s", "hello3", "a_i", "10", "a_f", "6")
+        .add(id, "6", "a_s", "hello4", "a_i", "11", "a_f", "7")
+        .add(id, "7", "a_s", "hello3", "a_i", "12", "a_f", "8")
+        .add(id, "8", "a_s", "hello3", "a_i", "13", "a_f", "9")
+        .add(id, "9", "a_s", "hello0", "a_i", "14", "a_f", "10")
+        .commit(cluster.getSolrClient(), COLLECTION);
 
     Map paramsA = mapParams("q", "*:*", "fl", "a_s,a_i,a_f", "sort", "a_s asc");
 
@@ -708,13 +610,7 @@ public class StreamingTest extends AbstractFullDistribZkTestBase {
     FieldComparator[] sorts = {new FieldComparator("sum(a_i)",
                                                    ComparatorOrder.ASCENDING)};
 
-    FacetStream facetStream = new FacetStream(zkHost,
-                                              "collection1",
-                                              paramsA,
-                                              buckets,
-                                              metrics,
-                                              sorts,
-                                              100);
+    FacetStream facetStream = new FacetStream(zkHost, COLLECTION, paramsA, buckets, metrics, sorts, 100);
 
     List<Tuple> tuples = getTuples(facetStream);
 
@@ -796,13 +692,7 @@ public class StreamingTest extends AbstractFullDistribZkTestBase {
 
     sorts[0] = new FieldComparator("sum(a_i)", ComparatorOrder.DESCENDING);
 
-    facetStream = new FacetStream(zkHost,
-                                  "collection1",
-                                  paramsA,
-                                  buckets,
-                                  metrics,
-                                  sorts,
-                                  100);
+    facetStream = new FacetStream(zkHost, COLLECTION, paramsA, buckets, metrics, sorts, 100);
 
     tuples = getTuples(facetStream);
 
@@ -885,13 +775,7 @@ public class StreamingTest extends AbstractFullDistribZkTestBase {
     sorts[0] = new FieldComparator("a_s", ComparatorOrder.DESCENDING);
 
 
-    facetStream = new FacetStream(zkHost,
-                                  "collection1",
-                                  paramsA,
-                                  buckets,
-                                  metrics,
-                                  sorts,
-                                  100);
+    facetStream = new FacetStream(zkHost, COLLECTION, paramsA, buckets, metrics, sorts, 100);
 
     tuples = getTuples(facetStream);
 
@@ -922,7 +806,6 @@ public class StreamingTest extends AbstractFullDistribZkTestBase {
     assertTrue(avgf.doubleValue() == 5.5D);
     assertTrue(count.doubleValue() == 2);
 
-
     tuple = tuples.get(1);
     bucket = tuple.getString("a_s");
     sumi = tuple.getDouble("sum(a_i)");
@@ -946,7 +829,6 @@ public class StreamingTest extends AbstractFullDistribZkTestBase {
     assertTrue(avgf.doubleValue() == 6.5D);
     assertTrue(count.doubleValue() == 4);
 
-
     tuple = tuples.get(2);
     bucket = tuple.getString("a_s");
     sumi = tuple.getDouble("sum(a_i)");
@@ -974,19 +856,12 @@ public class StreamingTest extends AbstractFullDistribZkTestBase {
 
     sorts[0] = new FieldComparator("a_s", ComparatorOrder.ASCENDING);
 
-    facetStream = new FacetStream(zkHost,
-                                  "collection1",
-                                  paramsA,
-                                  buckets,
-                                  metrics,
-                                  sorts,
-                                  100);
+    facetStream = new FacetStream(zkHost, COLLECTION, paramsA, buckets, metrics, sorts, 100);
 
     tuples = getTuples(facetStream);
 
     assert(tuples.size() == 3);
 
-
     tuple = tuples.get(0);
     bucket = tuple.getString("a_s");
     sumi = tuple.getDouble("sum(a_i)");
@@ -1010,7 +885,6 @@ public class StreamingTest extends AbstractFullDistribZkTestBase {
     assertTrue(avgf.doubleValue() == 4.5D);
     assertTrue(count.doubleValue() == 4);
 
-
     tuple = tuples.get(1);
     bucket = tuple.getString("a_s");
     sumi = tuple.getDouble("sum(a_i)");
@@ -1034,7 +908,6 @@ public class StreamingTest extends AbstractFullDistribZkTestBase {
     assertTrue(avgf.doubleValue() == 6.5D);
     assertTrue(count.doubleValue() == 4);
 
-
     tuple = tuples.get(2);
     bucket = tuple.getString("a_s");
     sumi = tuple.getDouble("sum(a_i)");
@@ -1058,27 +931,23 @@ public class StreamingTest extends AbstractFullDistribZkTestBase {
     assertTrue(avgf.doubleValue() == 5.5D);
     assertTrue(count.doubleValue() == 2);
 
-    del("*:*");
-    commit();
   }
 
-
-  private void testSubFacetStream() throws Exception {
-
-    indexr(id, "0", "level1_s", "hello0", "level2_s", "a", "a_i", "0", "a_f", "1");
-    indexr(id, "2", "level1_s", "hello0", "level2_s", "a", "a_i", "2", "a_f", "2");
-    indexr(id, "3", "level1_s", "hello3", "level2_s", "a", "a_i", "3", "a_f", "3");
-    indexr(id, "4", "level1_s", "hello4", "level2_s", "a", "a_i", "4", "a_f", "4");
-    indexr(id, "1", "level1_s", "hello0", "level2_s", "b", "a_i", "1", "a_f", "5");
-    indexr(id, "5", "level1_s", "hello3", "level2_s", "b", "a_i", "10", "a_f", "6");
-    indexr(id, "6", "level1_s", "hello4", "level2_s", "b", "a_i", "11", "a_f", "7");
-    indexr(id, "7", "level1_s", "hello3", "level2_s", "b", "a_i", "12", "a_f", "8");
-    indexr(id, "8", "level1_s", "hello3", "level2_s", "b", "a_i", "13", "a_f", "9");
-    indexr(id, "9", "level1_s", "hello0", "level2_s", "b", "a_i", "14", "a_f", "10");
-
-    commit();
-
-    String zkHost = zkServer.getZkAddress();
+  @Test
+  public void testSubFacetStream() throws Exception {
+
+    new UpdateRequest()
+        .add(id, "0", "level1_s", "hello0", "level2_s", "a", "a_i", "0", "a_f", "1")
+        .add(id, "2", "level1_s", "hello0", "level2_s", "a", "a_i", "2", "a_f", "2")
+        .add(id, "3", "level1_s", "hello3", "level2_s", "a", "a_i", "3", "a_f", "3")
+        .add(id, "4", "level1_s", "hello4", "level2_s", "a", "a_i", "4", "a_f", "4")
+        .add(id, "1", "level1_s", "hello0", "level2_s", "b", "a_i", "1", "a_f", "5")
+        .add(id, "5", "level1_s", "hello3", "level2_s", "b", "a_i", "10", "a_f", "6")
+        .add(id, "6", "level1_s", "hello4", "level2_s", "b", "a_i", "11", "a_f", "7")
+        .add(id, "7", "level1_s", "hello3", "level2_s", "b", "a_i", "12", "a_f", "8")
+        .add(id, "8", "level1_s", "hello3", "level2_s", "b", "a_i", "13", "a_f", "9")
+        .add(id, "9", "level1_s", "hello0", "level2_s", "b", "a_i", "14", "a_f", "10")
+        .commit(cluster.getSolrClient(), COLLECTION);
 
     Map paramsA = mapParams("q","*:*","fl","a_i,a_f");
 
@@ -1089,10 +958,9 @@ public class StreamingTest extends AbstractFullDistribZkTestBase {
 
     FieldComparator[] sorts = {new FieldComparator("sum(a_i)", ComparatorOrder.DESCENDING), new FieldComparator("sum(a_i)", ComparatorOrder.DESCENDING)};
 
-
     FacetStream facetStream = new FacetStream(
         zkHost,
-        "collection1",
+        COLLECTION,
         paramsA,
         buckets,
         metrics,
@@ -1172,7 +1040,7 @@ public class StreamingTest extends AbstractFullDistribZkTestBase {
     sorts[1] =  new FieldComparator("level2_s", ComparatorOrder.DESCENDING );
     facetStream = new FacetStream(
         zkHost,
-        "collection1",
+        COLLECTION,
         paramsA,
         buckets,
         metrics,
@@ -1248,29 +1116,26 @@ public class StreamingTest extends AbstractFullDistribZkTestBase {
     assertTrue(sumi.longValue() == 2);
     assertTrue(count.doubleValue() == 2);
 
-    del("*:*");
-    commit();
   }
 
-  private void testRollupStream() throws Exception {
-
-    indexr(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "1");
-    indexr(id, "2", "a_s", "hello0", "a_i", "2", "a_f", "2");
-    indexr(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3");
-    indexr(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4");
-    indexr(id, "1", "a_s", "hello0", "a_i", "1", "a_f", "5");
-    indexr(id, "5", "a_s", "hello3", "a_i", "10", "a_f", "6");
-    indexr(id, "6", "a_s", "hello4", "a_i", "11", "a_f", "7");
-    indexr(id, "7", "a_s", "hello3", "a_i", "12", "a_f", "8");
-    indexr(id, "8", "a_s", "hello3", "a_i", "13", "a_f", "9");
-    indexr(id, "9", "a_s", "hello0", "a_i", "14", "a_f", "10");
-
-    commit();
-
-    String zkHost = zkServer.getZkAddress();
+  @Test
+  public void testRollupStream() throws Exception {
+
+    new UpdateRequest()
+        .add(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "1")
+        .add(id, "2", "a_s", "hello0", "a_i", "2", "a_f", "2")
+        .add(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3")
+        .add(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4")
+        .add(id, "1", "a_s", "hello0", "a_i", "1", "a_f", "5")
+        .add(id, "5", "a_s", "hello3", "a_i", "10", "a_f", "6")
+        .add(id, "6", "a_s", "hello4", "a_i", "11", "a_f", "7")
+        .add(id, "7", "a_s", "hello3", "a_i", "12", "a_f", "8")
+        .add(id, "8", "a_s", "hello3", "a_i", "13", "a_f", "9")
+        .add(id, "9", "a_s", "hello0", "a_i", "14", "a_f", "10")
+        .commit(cluster.getSolrClient(), COLLECTION);
 
     Map paramsA = mapParams("q","*:*","fl","a_s,a_i,a_f","sort", "a_s asc");
-    CloudSolrStream stream = new CloudSolrStream(zkHost, "collection1", paramsA);
+    CloudSolrStream stream = new CloudSolrStream(zkHost, COLLECTION, paramsA);
 
     Bucket[] buckets =  {new Bucket("a_s")};
 
@@ -1365,11 +1230,12 @@ public class StreamingTest extends AbstractFullDistribZkTestBase {
 
 
     //Test will null value in the grouping field
-    indexr(id, "12", "a_s", null, "a_i", "14", "a_f", "10");
-    commit();
+    new UpdateRequest()
+        .add(id, "12", "a_s", null, "a_i", "14", "a_f", "10")
+        .commit(cluster.getSolrClient(), COLLECTION);
 
     paramsA = mapParams("q","*:*","fl","a_s,a_i,a_f","sort", "a_s asc", "qt", "/export");
-    stream = new CloudSolrStream(zkHost, "collection1", paramsA);
+    stream = new CloudSolrStream(zkHost, COLLECTION, paramsA);
 
     Bucket[] buckets1 =  {new Bucket("a_s")};
 
@@ -1410,15 +1276,10 @@ public class StreamingTest extends AbstractFullDistribZkTestBase {
     assertTrue(avgf.doubleValue() == 10.0D);
     assertTrue(count.doubleValue() == 1);
 
-
-    del("*:*");
-    commit();
   }
 
-
-  private void testDaemonTopicStream() throws Exception {
-
-    String zkHost = zkServer.getZkAddress();
+  @Test
+  public void testDaemonTopicStream() throws Exception {
 
     StreamContext context = new StreamContext();
     SolrClientCache cache = new SolrClientCache();
@@ -1429,7 +1290,7 @@ public class StreamingTest extends AbstractFullDistribZkTestBase {
     params.put("rows", "500");
     params.put("fl", "id");
 
-    TopicStream topicStream = new TopicStream(zkHost, "collection1", "collection1", "50000000", 1000000, params);
+    TopicStream topicStream = new TopicStream(zkHost, COLLECTION, COLLECTION, "50000000", 1000000, params);
 
     DaemonStream daemonStream = new DaemonStream(topicStream, "daemon1", 1000, 500);
     daemonStream.setStreamContext(context);
@@ -1437,7 +1298,7 @@ public class StreamingTest extends AbstractFullDistribZkTestBase {
     daemonStream.open();
 
     // Wait for the checkpoint
-    CloudJettyRunner jetty = this.cloudJettys.get(0);
+    JettySolrRunner jetty = cluster.getJettySolrRunners().get(0);
 
     Map params1 = new HashMap();
     params1.put("qt","/get");
@@ -1445,7 +1306,7 @@ public class StreamingTest extends AbstractFullDistribZkTestBase {
     params1.put("fl","id");
     int count = 0;
     while(count == 0) {
-      SolrStream solrStream = new SolrStream(jetty.url, params1);
+      SolrStream solrStream = new SolrStream(jetty.getBaseUrl().toString() + "/" + COLLECTION, params1);
       List<Tuple> tuples = getTuples(solrStream);
       count = tuples.size();
       if(count > 0) {
@@ -1456,24 +1317,22 @@ public class StreamingTest extends AbstractFullDistribZkTestBase {
       }
     }
 
-    indexr(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "1");
-    indexr(id, "2", "a_s", "hello0", "a_i", "2", "a_f", "2");
-    indexr(id, "3", "a_s", "hello0", "a_i", "3", "a_f", "3");
-    indexr(id, "4", "a_s", "hello0", "a_i", "4", "a_f", "4");
-    indexr(id, "1", "a_s", "hello0", "a_i", "1", "a_f", "5");
-
-    commit();
-
+    new UpdateRequest()
+        .add(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "1")
+        .add(id, "2", "a_s", "hello0", "a_i", "2", "a_f", "2")
+        .add(id, "3", "a_s", "hello0", "a_i", "3", "a_f", "3")
+        .add(id, "4", "a_s", "hello0", "a_i", "4", "a_f", "4")
+        .add(id, "1", "a_s", "hello0", "a_i", "1", "a_f", "5")
+        .commit(cluster.getSolrClient(), COLLECTION);
 
     for(int i=0; i<5; i++) {
       daemonStream.read();
     }
 
-
-    indexr(id, "5", "a_s", "hello0", "a_i", "4", "a_f", "4");
-    indexr(id, "6", "a_s", "hello0", "a_i", "4", "a_f", "4");
-
-    commit();
+    new UpdateRequest()
+        .add(id, "5", "a_s", "hello0", "a_i", "4", "a_f", "4")
+        .add(id, "6", "a_s", "hello0", "a_i", "4", "a_f", "4")
+        .commit(cluster.getSolrClient(), COLLECTION);
 
     for(int i=0; i<2; i++) {
       daemonStream.read();
@@ -1486,30 +1345,27 @@ public class StreamingTest extends AbstractFullDistribZkTestBase {
     assertTrue(tuple.EOF);
     daemonStream.close();
     cache.close();
-    del("*:*");
-    commit();
-  }
-
-  private void testParallelRollupStream() throws Exception {
 
-    indexr(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "1");
-    indexr(id, "2", "a_s", "hello0", "a_i", "2", "a_f", "2");
-    indexr(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3");
-    indexr(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4");
-    indexr(id, "1", "a_s", "hello0", "a_i", "1", "a_f", "5");
-    indexr(id, "5", "a_s", "hello3", "a_i", "10", "a_f", "6");
-    indexr(id, "6", "a_s", "hello4", "a_i", "11", "a_f", "7");
-    indexr(id, "7", "a_s", "hello3", "a_i", "12", "a_f", "8");
-    indexr(id, "8", "a_s", "hello3", "a_i", "13", "a_f", "9");
-    indexr(id, "9", "a_s", "hello0", "a_i", "14", "a_f", "10");
-
-    commit();
+  }
 
-    String zkHost = zkServer.getZkAddress();
-    streamFactory.withCollectionZkHost("collection1", zkHost);
+  @Test
+  public void testParallelRollupStream() throws Exception {
+
+    new UpdateRequest()
+        .add(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "1")
+        .add(id, "2", "a_s", "hello0", "a_i", "2", "a_f", "2")
+        .add(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3")
+        .add(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4")
+        .add(id, "1", "a_s", "hello0", "a_i", "1", "a_f", "5")
+        .add(id, "5", "a_s", "hello3", "a_i", "10", "a_f", "6")
+        .add(id, "6", "a_s", "hello4", "a_i", "11", "a_f", "7")
+        .add(id, "7", "a_s", "hello3", "a_i", "12", "a_f", "8")
+        .add(id, "8", "a_s", "hello3", "a_i", "13", "a_f", "9")
+        .add(id, "9", "a_s", "hello0", "a_i", "14", "a_f", "10")
+        .commit(cluster.getSolrClient(), COLLECTION);
 
     Map paramsA = mapParams("q", "*:*", "fl", "a_s,a_i,a_f", "sort", "a_s asc", "partitionKeys", "a_s");
-    CloudSolrStream stream = new CloudSolrStream(zkHost, "collection1", paramsA);
+    CloudSolrStream stream = new CloudSolrStream(zkHost, COLLECTION, paramsA);
 
     Bucket[] buckets =  {new Bucket("a_s")};
 
@@ -1524,7 +1380,7 @@ public class StreamingTest extends AbstractFullDistribZkTestBase {
                         new CountMetric()};
 
     RollupStream rollupStream = new RollupStream(stream, buckets, metrics);
-    ParallelStream parallelStream = new ParallelStream(zkHost, "collection1", rollupStream, 2, new FieldComparator("a_s", ComparatorOrder.ASCENDING));
+    ParallelStream parallelStream = new ParallelStream(zkHost, COLLECTION, rollupStream, 2, new FieldComparator("a_s", ComparatorOrder.ASCENDING));
     attachStreamFactory(parallelStream);
     List<Tuple> tuples = getTuples(parallelStream);
 
@@ -1601,55 +1457,48 @@ public class StreamingTest extends AbstractFullDistribZkTestBase {
     assertTrue(avgf.doubleValue() == 5.5D);
     assertTrue(count.doubleValue() == 2);
 
-    del("*:*");
-    commit();
   }
 
-  private void testZeroParallelReducerStream() throws Exception {
-
-    indexr(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "1");
-    indexr(id, "2", "a_s", "hello0", "a_i", "2", "a_f", "2");
-    indexr(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3");
-    indexr(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4");
-    indexr(id, "1", "a_s", "hello0", "a_i", "1", "a_f", "5");
-    indexr(id, "5", "a_s", "hello3", "a_i", "10", "a_f", "6");
-    indexr(id, "6", "a_s", "hello4", "a_i", "11", "a_f", "7");
-    indexr(id, "7", "a_s", "hello3", "a_i", "12", "a_f", "8");
-    indexr(id, "8", "a_s", "hello3", "a_i", "13", "a_f", "9");
-    indexr(id, "9", "a_s", "hello0", "a_i", "14", "a_f", "10");
-
-    commit();
-
-    String zkHost = zkServer.getZkAddress();
-    streamFactory.withCollectionZkHost("collection1", zkHost);
+  @Test
+  public void testZeroParallelReducerStream() throws Exception {
+
+    new UpdateRequest()
+        .add(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "1")
+        .add(id, "2", "a_s", "hello0", "a_i", "2", "a_f", "2")
+        .add(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3")
+        .add(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4")
+        .add(id, "1", "a_s", "hello0", "a_i", "1", "a_f", "5")
+        .add(id, "5", "a_s", "hello3", "a_i", "10", "a_f", "6")
+        .add(id, "6", "a_s", "hello4", "a_i", "11", "a_f", "7")
+        .add(id, "7", "a_s", "hello3", "a_i", "12", "a_f", "8")
+        .add(id, "8", "a_s", "hello3", "a_i", "13", "a_f", "9")
+        .add(id, "9", "a_s", "hello0", "a_i", "14", "a_f", "10")
+        .commit(cluster.getSolrClient(), COLLECTION);
 
     Map paramsA = mapParams("q", "blah", "fl", "id,a_s,a_i,a_f","sort", "a_s asc,a_f asc", "partitionKeys", "a_s");
-    CloudSolrStream stream = new CloudSolrStream(zkHost, "collection1", paramsA);
+    CloudSolrStream stream = new CloudSolrStream(zkHost, COLLECTION, paramsA);
     ReducerStream rstream = new ReducerStream(stream,
                                               new FieldEqualitor("a_s"),
                                               new GroupOperation(new FieldComparator("a_s", ComparatorOrder.ASCENDING), 2));
 
-    ParallelStream pstream = new ParallelStream(zkHost, "collection1", rstream, 2, new FieldComparator("a_s", ComparatorOrder.ASCENDING));
+    ParallelStream pstream = new ParallelStream(zkHost, COLLECTION, rstream, 2, new FieldComparator("a_s", ComparatorOrder.ASCENDING));
 
     attachStreamFactory(pstream);
     List<Tuple> tuples = getTuples(pstream);
     assert(tuples.size() == 0);
-    del("*:*");
-    commit();
-  }
-
 
-  private void testTuple() throws Exception {
-
-    indexr(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "5.1", "s_multi", "a", "s_multi", "b", "i_multi", "1", "i_multi", "2", "f_multi", "1.2", "f_multi", "1.3");
+  }
 
-    commit();
+  @Test
+  public void testTuple() throws Exception {
 
-    String zkHost = zkServer.getZkAddress();
-    streamFactory.withCollectionZkHost("collection1", zkHost);
+    new UpdateRequest()
+        .add(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "5.1", "s_multi", "a", "s_multi", "b", "i_multi",
+                 "1", "i_multi", "2", "f_multi", "1.2", "f_multi", "1.3")
+        .commit(cluster.getSolrClient(), COLLECTION);
 
     Map params = mapParams("q","*:*","fl","id,a_s,a_i,a_f,s_multi,i_multi,f_multi","sort", "a_s asc");
-    CloudSolrStream stream = new CloudSolrStream(zkHost, "collection1", params);
+    CloudSolrStream stream = new CloudSolrStream(zkHost, COLLECTION, params);
     List<Tuple> tuples = getTuples(stream);
     Tuple tuple = tuples.get(0);
 
@@ -1675,29 +1524,25 @@ public class StreamingTest extends AbstractFullDistribZkTestBase {
     assert(doubleList.get(0).doubleValue() == 1.2);
     assert(doubleList.get(1).doubleValue() == 1.3);
 
-    del("*:*");
-    commit();
   }
 
-  private void testMergeStream() throws Exception {
-
-    indexr(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "0");
-    indexr(id, "2", "a_s", "hello2", "a_i", "2", "a_f", "0");
-    indexr(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3");
-    indexr(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4");
-    indexr(id, "1", "a_s", "hello1", "a_i", "1", "a_f", "1");
-
-    commit();
+  @Test
+  public void testMergeStream() throws Exception {
 
-    String zkHost = zkServer.getZkAddress();
-    streamFactory.withCollectionZkHost("collection1", zkHost);
+    new UpdateRequest()
+        .add(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "0")
+        .add(id, "2", "a_s", "hello2", "a_i", "2", "a_f", "0")
+        .add(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3")
+        .add(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4")
+        .add(id, "1", "a_s", "hello1", "a_i", "1", "a_f", "1")
+        .commit(cluster.getSolrClient(), COLLECTION);
 
     //Test ascending
     Map paramsA = mapParams("q","id:(4 1)","fl","id,a_s,a_i","sort", "a_i asc");
-    CloudSolrStream streamA = new CloudSolrStream(zkHost, "collection1", paramsA);
+    CloudSolrStream streamA = new CloudSolrStream(zkHost, COLLECTION, paramsA);
 
     Map paramsB = mapParams("q","id:(0 2 3)","fl","id,a_s,a_i","sort", "a_i asc");
-    CloudSolrStream streamB = new CloudSolrStream(zkHost, "collection1", paramsB);
+    CloudSolrStream streamB = new CloudSolrStream(zkHost, COLLECTION, paramsB);
 
     MergeStream mstream = new MergeStream(streamA, streamB, new FieldComparator("a_i",ComparatorOrder.ASCENDING));
     List<Tuple> tuples = getTuples(mstream);
@@ -1707,10 +1552,10 @@ public class StreamingTest extends AbstractFullDistribZkTestBase {
 
     //Test descending
     paramsA = mapParams("q","id:(4 1)","fl","id,a_s,a_i","sort", "a_i desc");
-    streamA = new CloudSolrStream(zkHost, "collection1", paramsA);
+    streamA = new CloudSolrStream(zkHost, COLLECTION, paramsA);
 
     paramsB = mapParams("q","id:(0 2 3)","fl","id,a_s,a_i","sort", "a_i desc");
-    streamB = new CloudSolrStream(zkHost, "collection1", paramsB);
+    streamB = new CloudSolrStream(zkHost, COLLECTION, paramsB);
 
     mstream = new MergeStream(streamA, streamB, new FieldComparator("a_i",ComparatorOrder.DESCENDING));
     tuples = getTuples(mstream);
@@ -1721,10 +1566,10 @@ public class StreamingTest extends AbstractFullDistribZkTestBase {
     //Test compound sort
 
     paramsA = mapParams("q","id:(2 4 1)","fl","id,a_s,a_i,a_f","sort", "a_f asc,a_i asc");
-    streamA = new CloudSolrStream(zkHost, "collection1", paramsA);
+    streamA = new CloudSolrStream(zkHost, COLLECTION, paramsA);
 
     paramsB = mapParams("q","id:(0 3)","fl","id,a_s,a_i,a_f","sort", "a_f asc,a_i asc");
-    streamB = new CloudSolrStream(zkHost, "collection1", paramsB);
+    streamB = new CloudSolrStream(zkHost, COLLECTION, paramsB);
 
     mstream = new MergeStream(streamA, streamB, new MultipleFieldComparator(new FieldComparator("a_f",ComparatorOrder.ASCENDING),new FieldComparator("a_i",ComparatorOrder.ASCENDING)));
     tuples = getTuples(mstream);
@@ -1733,10 +1578,10 @@ public class StreamingTest extends AbstractFullDistribZkTestBase {
     assertOrder(tuples, 0,2,1,3,4);
 
     paramsA = mapParams("q","id:(2 4 1)","fl","id,a_s,a_i,a_f","sort", "a_f asc,a_i desc");
-    streamA = new CloudSolrStream(zkHost, "collection1", paramsA);
+    streamA = new CloudSolrStream(zkHost, COLLECTION, paramsA);
 
     paramsB = mapParams("q","id:(0 3)","fl","id,a_s,a_i,a_f","sort", "a_f asc,a_i desc");
-    streamB = new CloudSolrStream(zkHost, "collection1", paramsB);
+    streamB = new CloudSolrStream(zkHost, COLLECTION, paramsB);
 
     mstream = new MergeStream(streamA, streamB, new MultipleFieldComparator(new FieldComparator("a_f",ComparatorOrder.ASCENDING),new FieldComparator("a_i",ComparatorOrder.DESCENDING)));
     tuples = getTuples(mstream);
@@ -1744,38 +1589,33 @@ public class StreamingTest extends AbstractFullDistribZkTestBase {
     assert(tuples.size() == 5);
     assertOrder(tuples, 2,0,1,3,4);
 
-    del("*:*");
-    commit();
   }
 
-
-  private void testParallelMergeStream() throws Exception {
-
-    indexr(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "0");
-    indexr(id, "2", "a_s", "hello2", "a_i", "2", "a_f", "0");
-    indexr(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3");
-    indexr(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4");
-    indexr(id, "1", "a_s", "hello1", "a_i", "1", "a_f", "1");
-    indexr(id, "5", "a_s", "hello0", "a_i", "10", "a_f", "0");
-    indexr(id, "6", "a_s", "hello2", "a_i", "8", "a_f", "0");
-    indexr(id, "7", "a_s", "hello3", "a_i", "7", "a_f", "3");
-    indexr(id, "8", "a_s", "hello4", "a_i", "11", "a_f", "4");
-    indexr(id, "9", "a_s", "hello1", "a_i", "100", "a_f", "1");
-
-    commit();
-
-    String zkHost = zkServer.getZkAddress();
-    streamFactory.withCollectionZkHost("collection1", zkHost);
+  @Test
+  public void testParallelMergeStream() throws Exception {
+
+    new UpdateRequest()
+        .add(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "0")
+        .add(id, "2", "a_s", "hello2", "a_i", "2", "a_f", "0")
+        .add(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3")
+        .add(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4")
+        .add(id, "1", "a_s", "hello1", "a_i", "1", "a_f", "1")
+        .add(id, "5", "a_s", "hello0", "a_i", "10", "a_f", "0")
+        .add(id, "6", "a_s", "hello2", "a_i", "8", "a_f", "0")
+        .add(id, "7", "a_s", "hello3", "a_i", "7", "a_f", "3")
+        .add(id, "8", "a_s", "hello4", "a_i", "11", "a_f", "4")
+        .add(id, "9", "a_s", "hello1", "a_i", "100", "a_f", "1")
+        .commit(cluster.getSolrClient(), COLLECTION);
 
     //Test ascending
     Map paramsA = mapParams("q","id:(4 1 8 7 9)","fl","id,a_s,a_i","sort", "a_i asc", "partitionKeys", "a_i");
-    CloudSolrStream streamA = new CloudSolrStream(zkHost, "collection1", paramsA);
+    CloudSolrStream streamA = new CloudSolrStream(zkHost, COLLECTION, paramsA);
 
     Map paramsB = mapParams("q","id:(0 2 3 6)","fl","id,a_s,a_i","sort", "a_i asc", "partitionKeys", "a_i");
-    CloudSolrStream streamB = new CloudSolrStream(zkHost, "collection1", paramsB);
+    CloudSolrStream streamB = new CloudSolrStream(zkHost, COLLECTION, paramsB);
 
     MergeStream mstream = new MergeStream(streamA, streamB, new FieldComparator("a_i",ComparatorOrder.ASCENDING));
-    ParallelStream pstream = new ParallelStream(zkHost, "collection1", mstream, 2, new FieldComparator("a_i",ComparatorOrder.ASCENDING));
+    ParallelStream pstream = new ParallelStream(zkHost, COLLECTION, mstream, 2, new FieldComparator("a_i",ComparatorOrder.ASCENDING));
     attachStreamFactory(pstream);
     List<Tuple> tuples = getTuples(pstream);
 
@@ -1784,50 +1624,46 @@ public class StreamingTest extends AbstractFullDistribZkTestBase {
 
     //Test descending
     paramsA = mapParams("q", "id:(4 1 8 9)", "fl", "id,a_s,a_i", "sort", "a_i desc", "partitionKeys", "a_i");
-    streamA = new CloudSolrStream(zkHost, "collection1", paramsA);
+    streamA = new CloudSolrStream(zkHost, COLLECTION, paramsA);
 
     paramsB = mapParams("q","id:(0 2 3 6)","fl","id,a_s,a_i","sort", "a_i desc", "partitionKeys", "a_i");
-    streamB = new CloudSolrStream(zkHost, "collection1", paramsB);
+    streamB = new CloudSolrStream(zkHost, COLLECTION, paramsB);
 
     mstream = new MergeStream(streamA, streamB, new FieldComparator("a_i",ComparatorOrder.DESCENDING));
-    pstream = new ParallelStream(zkHost, "collection1", mstream, 2, new FieldComparator("a_i",ComparatorOrder.DESCENDING));
+    pstream = new ParallelStream(zkHost, COLLECTION, mstream, 2, new FieldComparator("a_i",ComparatorOrder.DESCENDING));
     attachStreamFactory(pstream);
     tuples = getTuples(pstream);
 
     assert(tuples.size() == 8);
     assertOrder(tuples, 9,8,6,4,3,2,1,0);
 
-    del("*:*");
-    commit();
   }
 
-  private void testParallelEOF() throws Exception {
-
-    indexr(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "0");
-    indexr(id, "2", "a_s", "hello2", "a_i", "2", "a_f", "0");
-    indexr(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3");
-    indexr(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4");
-    indexr(id, "1", "a_s", "hello1", "a_i", "1", "a_f", "1");
-    indexr(id, "5", "a_s", "hello0", "a_i", "10", "a_f", "0");
-    indexr(id, "6", "a_s", "hello2", "a_i", "8", "a_f", "0");
-    indexr(id, "7", "a_s", "hello3", "a_i", "7", "a_f", "3");
-    indexr(id, "8", "a_s", "hello4", "a_i", "11", "a_f", "4");
-    indexr(id, "9", "a_s", "hello1", "a_i", "100", "a_f", "1");
-
-    commit();
-
-    String zkHost = zkServer.getZkAddress();
-    streamFactory.withCollectionZkHost("collection1", zkHost);
+  @Test
+  public void testParallelEOF() throws Exception {
+
+    new UpdateRequest()
+        .add(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "0")
+        .add(id, "2", "a_s", "hello2", "a_i", "2", "a_f", "0")
+        .add(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3")
+        .add(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4")
+        .add(id, "1", "a_s", "hello1", "a_i", "1", "a_f", "1")
+        .add(id, "5", "a_s", "hello0", "a_i", "10", "a_f", "0")
+        .add(id, "6", "a_s", "hello2", "a_i", "8", "a_f", "0")
+        .add(id, "7", "a_s", "hello3", "a_i", "7", "a_f", "3")
+        .add(id, "8", "a_s", "hello4", "a_i", "11", "a_f", "4")
+        .add(id, "9", "a_s", "hello1", "a_i", "100", "a_f", "1")
+        .commit(cluster.getSolrClient(), COLLECTION);
 
     //Test ascending
     Map paramsA = mapParams("q","id:(4 1 8 7 9)","fl","id,a_s,a_i","sort", "a_i asc", "partitionKeys", "a_i");
-    CloudSolrStream streamA = new CloudSolrStream(zkHost, "collection1", paramsA);
+    CloudSolrStream streamA = new CloudSolrStream(zkHost, COLLECTION, paramsA);
 
     Map paramsB = mapParams("q","id:(0 2 3 6)","fl","id,a_s,a_i","sort", "a_i asc", "partitionKeys", "a_i");
-    CloudSolrStream streamB = new CloudSolrStream(zkHost, "collection1", paramsB);
+    CloudSolrStream streamB = new CloudSolrStream(zkHost, COLLECTION, paramsB);
 
     MergeStream mstream = new MergeStream(streamA, streamB, new FieldComparator("a_i",ComparatorOrder.ASCENDING));
-    ParallelStream pstream = new ParallelStream(zkHost, "collection1", mstream, 2, new FieldComparator("a_i",ComparatorOrder.ASCENDING));
+    ParallelStream pstream = new ParallelStream(zkHost, COLLECTION, mstream, 2, new FieldComparator("a_i",ComparatorOrder.ASCENDING));
     
     attachStreamFactory(pstream);
     List<Tuple> tuples = getTuples(pstream);
@@ -1836,41 +1672,25 @@ public class StreamingTest extends AbstractFullDistribZkTestBase {
     Map<String, Tuple> eofTuples = pstream.getEofTuples();
     assert(eofTuples.size() == 2); // There should be an EOF Tuple for each worker.
 
-    del("*:*");
-    commit();
   }
 
-
-
   @Test
   public void streamTests() throws Exception {
-    assertNotNull(cloudClient);
-
-    handle.clear();
-    handle.put("timestamp", SKIPVAL);
-
-    waitForRecoveriesToFinish(false);
-
-    del("*:*");
 
-    commit();
+    new UpdateRequest()
+        .add(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "0")
+        .add(id, "2", "a_s", "hello2", "a_i", "2", "a_f", "0")
+        .add(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3")
+        .add(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4")
+        .add(id, "1", "a_s", "hello1", "a_i", "1", "a_f", "1")
+        .commit(cluster.getSolrClient(), COLLECTION);
 
-    indexr(id, "0", "a_s", "hello0", "a_i", "0", "a_f", "0");
-    indexr(id, "2", "a_s", "hello2", "a_i", "2", "a_f", "0");
-    indexr(id, "3", "a_s", "hello3", "a_i", "3", "a_f", "3");
-    indexr(id, "4", "a_s", "hello4", "a_i", "4", "a_f", "4");
-    indexr(id, "1", "a_s", "hello1", "a_i", "1", "a_f", "1");
-
-    commit();
-
-    String zkHost = zkServer.getZkAddress();
-    streamFactory.withCollectionZkHost("collection1", zkHost);
     Map params = null;
 
     //Basic CloudSolrStream Test with Descending Sort
 
     params = mapParams("q","*:*","fl","id,a_s,a_i","sort", "a_i desc");
-    CloudSolrStream stream = new CloudSolrStream(zkHost, "collection1", params);
+    CloudSolrStream stream = new CloudSolrStream(zkHost, COLLECTION, params);
     List<Tuple> tuples = getTuples(stream);
 
     assert(tuples.size() == 5);
@@ -1878,7 +1698,7 @@ public class StreamingTest extends AbstractFullDistribZkTestBase {
 
     //With Ascending Sort
     params = mapParams("q","*:*","fl","id,a_s,a_i","sort", "a_i asc");
-    stream = new CloudSolrStream(zkHost, "collection1", params);
+    stream = new CloudSolrStream(zkHost, COLLECTION, params);
     tuples = getTuples(stream);
 
     assert(tuples.size() == 5);
@@ -1887,7 +1707,7 @@ public class StreamingTest extends AbstractFullDistribZkTestBase {
 
     //Test compound sort
     params = mapParams("q","*:*","fl","id,a_s,a_i,a_f","sort", "a_f asc,a_i desc");
-    stream = new CloudSolrStream(zkHost, "collection1", params);
+    stream = new CloudSolrStream(zkHost, COLLECTION, params);
     tuples = getTuples(stream);
 
     assert(tuples.size() == 5);
@@ -1895,38 +1715,12 @@ public class StreamingTest extends AbstractFullDistribZkTestBase {
 
 
     params = mapParams("q","*:*","fl","id,a_s,a_i,a_f","sort", "a_f asc,a_i asc");
-    stream = new CloudSolrStream(zkHost, "collection1", params);
+    stream = new CloudSolrStream(zkHost, COLLECTION, params);
     tuples = getTuples(stream);
 
     assert (tuples.size() == 5);
     assertOrder(tuples, 0, 2, 1, 3, 4);
 
-    del("*:*");
-    commit();
-
-    testTuple();
-    testSpacesInParams();
-    testNonePartitionKeys();
-    testTrace();
-    testUniqueStream();
-    testRankStream();
-    testMergeStream();
-    testReducerStream();
-    testRollupStream();
-    testZeroReducerStream();
-    testFacetStream();
-    testSubFacetStream();
-    testStatsStream();
-    //testExceptionStream();
-    testDaemonTopicStream();
-    testParallelEOF();
-    testParallelUniqueStream();
-    testParallelRankStream();
-    testParallelMergeStream();
-    testParallelRollupStream();
-    testParallelReducerStream();
-    //testParallelExceptionStream();
-    testZeroParallelReducerStream();
   }
 
   protected Map mapParams(String... vals) {
@@ -2019,12 +1813,6 @@ public class StreamingTest extends AbstractFullDistribZkTestBase {
 
     return true;
   }
-
-  @Override
-  protected void indexr(Object... fields) throws Exception {
-    SolrInputDocument doc = getDoc(fields);
-    indexDoc(doc);
-  }
   
   private void attachStreamFactory(TupleStream tupleStream) {
     StreamContext streamContext = new StreamContext();

