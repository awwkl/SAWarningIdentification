GitDiffStart: 8a136e3113e6538ad9efd89d746092dfb7c2ed9d | Thu May 22 19:28:27 2014 +0000
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/idversion/IDVersionPostingsFormat.java b/lucene/codecs/src/java/org/apache/lucene/codecs/idversion/IDVersionPostingsFormat.java
deleted file mode 100644
index ec54677..0000000
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/idversion/IDVersionPostingsFormat.java
+++ /dev/null
@@ -1,136 +0,0 @@
-package org.apache.lucene.codecs.idversion;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.FieldsConsumer;
-import org.apache.lucene.codecs.FieldsProducer;
-import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.codecs.PostingsReaderBase;
-import org.apache.lucene.codecs.PostingsWriterBase;
-import org.apache.lucene.codecs.blocktree.BlockTreeTermsWriter;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.search.LiveFieldValues;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-
-/** A PostingsFormat optimized for primary-key (ID) fields that also
- *  record a version (long) for each ID, delivered as a payload
- *  created by {@link #longToBytes} during indexing.  At search time,
- *  the TermsEnum implementation {@link IDVersionSegmentTermsEnum}
- *  enables fast (using only the terms index when possible) lookup for
- *  whether a given ID was previously indexed with version > N (see
- *  {@link IDVersionSegmentTermsEnum#seekExact(BytesRef,long)}.
- *
- *  <p>This is most effective if the app assigns monotonically
- *  increasing global version to each indexed doc.  Then, during
- *  indexing, use {@link
- *  IDVersionSegmentTermsEnum#seekExact(BytesRef,long)} (along with
- *  {@link LiveFieldValues}) to decide whether the document you are
- *  about to index was already indexed with a higher version, and skip
- *  it if so.
- *
- *  <p>The field is effectively indexed as DOCS_ONLY and the docID is
- *  pulsed into the terms dictionary, but the user must feed in the
- *  version as a payload on the first token.
- *
- *  <p>NOTE: term vectors cannot be indexed with this field (not that
- *  you should really ever want to do this).
- *
- *  @lucene.experimental */
-
-public class IDVersionPostingsFormat extends PostingsFormat {
-
-  private final int minTermsInBlock;
-  private final int maxTermsInBlock;
-
-  public IDVersionPostingsFormat() {
-    this(BlockTreeTermsWriter.DEFAULT_MIN_BLOCK_SIZE, BlockTreeTermsWriter.DEFAULT_MAX_BLOCK_SIZE);
-  }
-
-  public IDVersionPostingsFormat(int minTermsInBlock, int maxTermsInBlock) {
-    super("IDVersion");
-    this.minTermsInBlock = minTermsInBlock;
-    this.maxTermsInBlock = maxTermsInBlock;
-  }
-
-  @Override
-  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-    PostingsWriterBase postingsWriter = new IDVersionPostingsWriter(state);
-    boolean success = false;
-    try {
-      FieldsConsumer ret = new VersionBlockTreeTermsWriter(state, 
-                                                           postingsWriter,
-                                                           minTermsInBlock, 
-                                                           maxTermsInBlock);
-      success = true;
-      return ret;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(postingsWriter);
-      }
-    }
-  }
-
-  @Override
-  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
-    PostingsReaderBase postingsReader = new IDVersionPostingsReader();
-    boolean success = false;
-     try {
-       FieldsProducer ret = new VersionBlockTreeTermsReader(state.directory,
-                                                            state.fieldInfos,
-                                                            state.segmentInfo,
-                                                            postingsReader,
-                                                            state.context,
-                                                            state.segmentSuffix);
-       success = true;
-       return ret;
-     } finally {
-       if (!success) {
-         IOUtils.closeWhileHandlingException(postingsReader);
-       }
-     }
-  }
-
-  public static long bytesToLong(BytesRef bytes) {
-    return ((bytes.bytes[bytes.offset]&0xFFL) << 56) |
-      ((bytes.bytes[bytes.offset+1]&0xFFL) << 48) |
-      ((bytes.bytes[bytes.offset+2]&0xFFL) << 40) |
-      ((bytes.bytes[bytes.offset+3]&0xFFL) << 32) |
-      ((bytes.bytes[bytes.offset+4]&0xFFL) << 24) |
-      ((bytes.bytes[bytes.offset+5]&0xFFL) << 16) |
-      ((bytes.bytes[bytes.offset+6]&0xFFL) << 8) |
-      (bytes.bytes[bytes.offset+7]&0xFFL);
-  }
-
-  public static void longToBytes(long v, BytesRef bytes) {
-    bytes.offset = 0;
-    bytes.length = 8;
-    bytes.bytes[0] = (byte) (v >> 56);
-    bytes.bytes[1] = (byte) (v >> 48);
-    bytes.bytes[2] = (byte) (v >> 40);
-    bytes.bytes[3] = (byte) (v >> 32);
-    bytes.bytes[4] = (byte) (v >> 24);
-    bytes.bytes[5] = (byte) (v >> 16);
-    bytes.bytes[6] = (byte) (v >> 8);
-    bytes.bytes[7] = (byte) v;
-    assert bytesToLong(bytes) == v: bytesToLong(bytes) + " vs " + v + " bytes=" + bytes;
-  }
-}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/idversion/IDVersionPostingsReader.java b/lucene/codecs/src/java/org/apache/lucene/codecs/idversion/IDVersionPostingsReader.java
deleted file mode 100644
index 3a1ba6c..0000000
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/idversion/IDVersionPostingsReader.java
+++ /dev/null
@@ -1,97 +0,0 @@
-package org.apache.lucene.codecs.idversion;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.BlockTermState;
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.PostingsReaderBase;
-import org.apache.lucene.index.DocsAndPositionsEnum;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.store.DataInput;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.Bits;
-
-final class IDVersionPostingsReader extends PostingsReaderBase {
-
-  @Override
-  public void init(IndexInput termsIn) throws IOException {
-    // Make sure we are talking to the matching postings writer
-    CodecUtil.checkHeader(termsIn,
-                          IDVersionPostingsWriter.TERMS_CODEC,
-                          IDVersionPostingsWriter.VERSION_START,
-                          IDVersionPostingsWriter.VERSION_CURRENT);
-  }
-
-  @Override
-  public BlockTermState newTermState() {
-    return new IDVersionTermState();
-  }
-
-  @Override
-  public void close() throws IOException {
-  }
-
-  @Override
-  public void decodeTerm(long[] longs, DataInput in, FieldInfo fieldInfo, BlockTermState _termState, boolean absolute)
-    throws IOException {
-    final IDVersionTermState termState = (IDVersionTermState) _termState;
-    termState.docID = in.readVInt();
-    termState.idVersion = in.readVLong();
-  }
-
-  @Override
-  public DocsEnum docs(FieldInfo fieldInfo, BlockTermState termState, Bits liveDocs, DocsEnum reuse, int flags) throws IOException {
-    SingleDocsEnum docsEnum;
-
-    if (reuse instanceof SingleDocsEnum) {
-      docsEnum = (SingleDocsEnum) reuse;
-    } else {
-      docsEnum = new SingleDocsEnum();
-    }
-    docsEnum.reset(((IDVersionTermState) termState).docID, liveDocs);
-
-    return docsEnum;
-  }
-
-  @Override
-  public DocsAndPositionsEnum docsAndPositions(FieldInfo fieldInfo, BlockTermState _termState, Bits liveDocs,
-                                               DocsAndPositionsEnum reuse, int flags) {
-    SingleDocsAndPositionsEnum posEnum;
-
-    if (reuse instanceof SingleDocsAndPositionsEnum) {
-      posEnum = (SingleDocsAndPositionsEnum) reuse;
-    } else {
-      posEnum = new SingleDocsAndPositionsEnum();
-    }
-    IDVersionTermState termState = (IDVersionTermState) _termState;
-    posEnum.reset(termState.docID, termState.idVersion, liveDocs);
-    return posEnum;
-  }
-
-  @Override
-  public long ramBytesUsed() {
-    return 0;
-  }
-
-  @Override
-  public void checkIntegrity() throws IOException {
-  }
-}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/idversion/IDVersionPostingsWriter.java b/lucene/codecs/src/java/org/apache/lucene/codecs/idversion/IDVersionPostingsWriter.java
deleted file mode 100644
index 4184069..0000000
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/idversion/IDVersionPostingsWriter.java
+++ /dev/null
@@ -1,157 +0,0 @@
-package org.apache.lucene.codecs.idversion;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.BlockTermState;
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.PushPostingsWriterBase;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.store.DataOutput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.BytesRef;
-
-final class IDVersionPostingsWriter extends PushPostingsWriterBase {
-
-  final static String TERMS_CODEC = "IDVersionPostingsWriterTerms";
-
-  // Increment version to change it
-  final static int VERSION_START = 0;
-  final static int VERSION_CURRENT = VERSION_START;
-
-  final static IDVersionTermState emptyState = new IDVersionTermState();
-  IDVersionTermState lastState;
-
-  int lastDocID;
-  private int lastPosition;
-  private long lastVersion;
-
-  private final SegmentWriteState state;
-
-  public IDVersionPostingsWriter(SegmentWriteState state) {
-    this.state = state;
-  }
-
-  @Override
-  public BlockTermState newTermState() {
-    return new IDVersionTermState();
-  }
-
-  @Override
-  public void init(IndexOutput termsOut) throws IOException {
-    CodecUtil.writeHeader(termsOut, TERMS_CODEC, VERSION_CURRENT);
-  }
-
-  @Override
-  public int setField(FieldInfo fieldInfo) {
-    super.setField(fieldInfo);
-    if (fieldInfo.getIndexOptions() != FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
-      throw new IllegalArgumentException("field must be index using IndexOptions.DOCS_AND_FREQS_AND_POSITIONS");
-    }
-    // LUCENE-5693: because CheckIndex cross-checks term vectors with postings even for deleted docs, and because our PF only indexes the
-    // non-deleted documents on flush, CheckIndex will see this as corruption:
-    if (fieldInfo.hasVectors()) {
-      throw new IllegalArgumentException("field cannot index term vectors: CheckIndex will report this as index corruption");
-    }
-    lastState = emptyState;
-    return 0;
-  }
-
-  @Override
-  public void startTerm() {
-    lastDocID = -1;
-  }
-
-  @Override
-  public void startDoc(int docID, int termDocFreq) throws IOException {
-    // TODO: LUCENE-5693: we don't need this check if we fix IW to not send deleted docs to us on flush:
-    if (state.liveDocs != null && state.liveDocs.get(docID) == false) {
-      return;
-    }
-    if (lastDocID != -1) {
-      throw new IllegalArgumentException("term appears in more than one document");
-    }
-    if (termDocFreq != 1) {
-      throw new IllegalArgumentException("term appears more than once in the document");
-    }
-
-    lastDocID = docID;
-    lastPosition = -1;
-    lastVersion = -1;
-  }
-
-  @Override
-  public void addPosition(int position, BytesRef payload, int startOffset, int endOffset) throws IOException {
-    if (lastDocID == -1) {
-      // Doc is deleted; skip it
-      return;
-    }
-    if (lastPosition != -1) {
-      throw new IllegalArgumentException("term appears more than once in document");
-    }
-    lastPosition = position;
-    if (payload == null) {
-      throw new IllegalArgumentException("token doens't have a payload");
-    }
-    if (payload.length != 8) {
-      throw new IllegalArgumentException("payload.length != 8 (got " + payload.length + ")");
-    }
-
-    lastVersion = IDVersionPostingsFormat.bytesToLong(payload);
-    if (lastVersion < 0) {
-      throw new IllegalArgumentException("version must be >= 0 (got: " + lastVersion + "; payload=" + payload + ")");
-    }
-  }
-
-  @Override
-  public void finishDoc() throws IOException {
-    if (lastDocID == -1) {
-      // Doc is deleted; skip it
-      return;
-    }
-    if (lastPosition == -1) {
-      throw new IllegalArgumentException("missing addPosition");
-    }
-  }
-
-  /** Called when we are done adding docs to this term */
-  @Override
-  public void finishTerm(BlockTermState _state) throws IOException {
-    if (lastDocID == -1) {
-      return;
-    }
-    IDVersionTermState state = (IDVersionTermState) _state;
-    assert state.docFreq > 0;
-
-    state.docID = lastDocID;
-    state.idVersion = lastVersion;
-  }
-  
-  @Override
-  public void encodeTerm(long[] longs, DataOutput out, FieldInfo fieldInfo, BlockTermState _state, boolean absolute) throws IOException {
-    IDVersionTermState state = (IDVersionTermState) _state;
-    out.writeVInt(state.docID);
-    out.writeVLong(state.idVersion);
-  }
-
-  @Override
-  public void close() throws IOException {
-  }
-}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/idversion/IDVersionSegmentTermsEnum.java b/lucene/codecs/src/java/org/apache/lucene/codecs/idversion/IDVersionSegmentTermsEnum.java
deleted file mode 100644
index bca8027..0000000
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/idversion/IDVersionSegmentTermsEnum.java
+++ /dev/null
@@ -1,1071 +0,0 @@
-package org.apache.lucene.codecs.idversion;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.io.PrintStream;
-
-import org.apache.lucene.codecs.BlockTermState;
-import org.apache.lucene.index.DocsAndPositionsEnum;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.TermState;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.store.ByteArrayDataInput;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.RamUsageEstimator;
-import org.apache.lucene.util.fst.FST;
-import org.apache.lucene.util.fst.PairOutputs.Pair;
-import org.apache.lucene.util.fst.Util;
-
-/** Iterates through terms in this field; this class is public so users
- *  can cast it to call {@link #seekExact(BytesRef, long)} for
- *  optimistic-concurreny, and also {@link #getVersion} to get the
- *  version of the currently seek'd term. */
-public final class IDVersionSegmentTermsEnum extends TermsEnum {
-
-  // Lazy init:
-  IndexInput in;
-
-  // static boolean DEBUG = false;
-
-  private IDVersionSegmentTermsEnumFrame[] stack;
-  private final IDVersionSegmentTermsEnumFrame staticFrame;
-  IDVersionSegmentTermsEnumFrame currentFrame;
-  boolean termExists;
-  final VersionFieldReader fr;
-
-  private int targetBeforeCurrentLength;
-
-  private final ByteArrayDataInput scratchReader = new ByteArrayDataInput();
-
-  // What prefix of the current term was present in the index:
-  private int validIndexPrefix;
-
-  // assert only:
-  private boolean eof;
-
-  final BytesRef term = new BytesRef();
-  private final FST.BytesReader fstReader;
-
-  @SuppressWarnings({"rawtypes","unchecked"}) private FST.Arc<Pair<BytesRef,Long>>[] arcs =
-  new FST.Arc[1];
-
-  IDVersionSegmentTermsEnum(VersionFieldReader fr) throws IOException {
-    this.fr = fr;
-
-    //if (DEBUG) System.out.println("BTTR.init seg=" + segment);
-    stack = new IDVersionSegmentTermsEnumFrame[0];
-        
-    // Used to hold seek by TermState, or cached seek
-    staticFrame = new IDVersionSegmentTermsEnumFrame(this, -1);
-
-    if (fr.index == null) {
-      fstReader = null;
-    } else {
-      fstReader = fr.index.getBytesReader();
-    }
-
-    // Init w/ root block; don't use index since it may
-    // not (and need not) have been loaded
-    for(int arcIdx=0;arcIdx<arcs.length;arcIdx++) {
-      arcs[arcIdx] = new FST.Arc<>();
-    }
-
-    currentFrame = staticFrame;
-    final FST.Arc<Pair<BytesRef,Long>> arc;
-    if (fr.index != null) {
-      arc = fr.index.getFirstArc(arcs[0]);
-      // Empty string prefix must have an output in the index!
-      assert arc.isFinal();
-    } else {
-      arc = null;
-    }
-    currentFrame = staticFrame;
-    //currentFrame = pushFrame(arc, rootCode, 0);
-    //currentFrame.loadBlock();
-    validIndexPrefix = 0;
-    // if (DEBUG) {
-    //   System.out.println("init frame state " + currentFrame.ord);
-    //   printSeekState();
-    // }
-
-    //System.out.println();
-    // computeBlockStats().print(System.out);
-  }
-      
-  // Not private to avoid synthetic access$NNN methods
-  void initIndexInput() {
-    if (this.in == null) {
-      this.in = fr.parent.in.clone();
-    }
-  }
-
-  private IDVersionSegmentTermsEnumFrame getFrame(int ord) throws IOException {
-    if (ord >= stack.length) {
-      final IDVersionSegmentTermsEnumFrame[] next = new IDVersionSegmentTermsEnumFrame[ArrayUtil.oversize(1+ord, RamUsageEstimator.NUM_BYTES_OBJECT_REF)];
-      System.arraycopy(stack, 0, next, 0, stack.length);
-      for(int stackOrd=stack.length;stackOrd<next.length;stackOrd++) {
-        next[stackOrd] = new IDVersionSegmentTermsEnumFrame(this, stackOrd);
-      }
-      stack = next;
-    }
-    assert stack[ord].ord == ord;
-    return stack[ord];
-  }
-
-  private FST.Arc<Pair<BytesRef,Long>> getArc(int ord) {
-    if (ord >= arcs.length) {
-      @SuppressWarnings({"rawtypes","unchecked"}) final FST.Arc<Pair<BytesRef,Long>>[] next =
-      new FST.Arc[ArrayUtil.oversize(1+ord, RamUsageEstimator.NUM_BYTES_OBJECT_REF)];
-      System.arraycopy(arcs, 0, next, 0, arcs.length);
-      for(int arcOrd=arcs.length;arcOrd<next.length;arcOrd++) {
-        next[arcOrd] = new FST.Arc<>();
-      }
-      arcs = next;
-    }
-    return arcs[ord];
-  }
-
-  // Pushes a frame we seek'd to
-  IDVersionSegmentTermsEnumFrame pushFrame(FST.Arc<Pair<BytesRef,Long>> arc, Pair<BytesRef,Long> frameData, int length) throws IOException {
-    scratchReader.reset(frameData.output1.bytes, frameData.output1.offset, frameData.output1.length);
-    final long code = scratchReader.readVLong();
-    final long fpSeek = code >>> VersionBlockTreeTermsWriter.OUTPUT_FLAGS_NUM_BITS;
-    final IDVersionSegmentTermsEnumFrame f = getFrame(1+currentFrame.ord);
-    f.maxIDVersion = Long.MAX_VALUE - frameData.output2;
-    f.hasTerms = (code & VersionBlockTreeTermsWriter.OUTPUT_FLAG_HAS_TERMS) != 0;
-    f.hasTermsOrig = f.hasTerms;
-    f.isFloor = (code & VersionBlockTreeTermsWriter.OUTPUT_FLAG_IS_FLOOR) != 0;
-    if (f.isFloor) {
-      f.setFloorData(scratchReader, frameData.output1);
-    }
-    pushFrame(arc, fpSeek, length);
-
-    return f;
-  }
-
-  // Pushes next'd frame or seek'd frame; we later
-  // lazy-load the frame only when needed
-  IDVersionSegmentTermsEnumFrame pushFrame(FST.Arc<Pair<BytesRef,Long>> arc, long fp, int length) throws IOException {
-    final IDVersionSegmentTermsEnumFrame f = getFrame(1+currentFrame.ord);
-    f.arc = arc;
-    if (f.fpOrig == fp && f.nextEnt != -1) {
-      //if (DEBUG) System.out.println("      push reused frame ord=" + f.ord + " fp=" + f.fp + " isFloor?=" + f.isFloor + " hasTerms=" + f.hasTerms + " pref=" + term + " nextEnt=" + f.nextEnt + " targetBeforeCurrentLength=" + targetBeforeCurrentLength + " term.length=" + term.length + " vs prefix=" + f.prefix);
-      if (f.prefix > targetBeforeCurrentLength) {
-        f.rewind();
-      } else {
-        // if (DEBUG) {
-        //   System.out.println("        skip rewind!");
-        // }
-      }
-      assert length == f.prefix;
-    } else {
-      f.nextEnt = -1;
-      f.prefix = length;
-      f.state.termBlockOrd = 0;
-      f.fpOrig = f.fp = fp;
-      f.lastSubFP = -1;
-      // if (DEBUG) {
-      //   final int sav = term.length;
-      //   term.length = length;
-      //   System.out.println("      push new frame ord=" + f.ord + " fp=" + f.fp + " hasTerms=" + f.hasTerms + " isFloor=" + f.isFloor + " pref=" + brToString(term));
-      //   term.length = sav;
-      // }
-    }
-
-    return f;
-  }
-
-  // asserts only
-  private boolean clearEOF() {
-    eof = false;
-    return true;
-  }
-
-  // asserts only
-  private boolean setEOF() {
-    eof = true;
-    return true;
-  }
-
-  @Override
-  public boolean seekExact(final BytesRef target) throws IOException {
-    return seekExact(target, 0);
-  }
-
-  // for debugging
-  @SuppressWarnings("unused")
-  static String brToString(BytesRef b) {
-    try {
-      return b.utf8ToString() + " " + b;
-    } catch (Throwable t) {
-      // If BytesRef isn't actually UTF8, or it's eg a
-      // prefix of UTF8 that ends mid-unicode-char, we
-      // fallback to hex:
-      return b.toString();
-    }
-  }
-
-  /** Get the version of the currently seek'd term; only valid if we are
-   *  positioned. */
-  public long getVersion() {
-    return ((IDVersionTermState) currentFrame.state).idVersion;
-  }
-
-  /** Optimized version of {@link #seekExact(BytesRef)} that can
-   *  sometimes fail-fast if the version indexed with the requested ID
-   *  is less than the specified minIDVersion.  Applications that index
-   *  a monotonically increasing global version with each document can
-   *  use this for fast optimistic concurrency. */
-  public boolean seekExact(final BytesRef target, long minIDVersion) throws IOException {
-
-    if (fr.index == null) {
-      throw new IllegalStateException("terms index was not loaded");
-    }
-
-    if (term.bytes.length <= target.length) {
-      term.bytes = ArrayUtil.grow(term.bytes, 1+target.length);
-    }
-
-    assert clearEOF();
-
-    //  if (DEBUG) {
-    //    System.out.println("\nBTTR.seekExact seg=" + fr.parent.segment + " target=" + fr.fieldInfo.name + ":" + brToString(target) + " minIDVersion=" + minIDVersion + " current=" + brToString(term) + " (exists?=" + termExists + ") validIndexPrefix=" + validIndexPrefix);
-    //   printSeekState(System.out);
-    //  }
-
-    FST.Arc<Pair<BytesRef,Long>> arc;
-    int targetUpto;
-    Pair<BytesRef,Long> output;
-
-    long startFrameFP = currentFrame.fp;
-
-    targetBeforeCurrentLength = currentFrame.ord;
-
-    boolean changed = false;
-
-    // TODO: we could stop earlier w/ the version check, every time we traverse an index arc we can check?
-
-    if (currentFrame != staticFrame) {
-
-      // We are already seek'd; find the common
-      // prefix of new seek term vs current term and
-      // re-use the corresponding seek state.  For
-      // example, if app first seeks to foobar, then
-      // seeks to foobaz, we can re-use the seek state
-      // for the first 5 bytes.
-
-      // if (DEBUG) {
-      //    System.out.println("  re-use current seek state validIndexPrefix=" + validIndexPrefix);
-      //  }
-
-      arc = arcs[0];
-      assert arc.isFinal();
-      output = arc.output;
-      targetUpto = 0;
-
-      IDVersionSegmentTermsEnumFrame lastFrame = stack[0];
-      assert validIndexPrefix <= term.length: "validIndexPrefix=" + validIndexPrefix + " term.length=" + term.length + " seg=" + fr.parent.segment;
-
-      final int targetLimit = Math.min(target.length, validIndexPrefix);
-
-      int cmp = 0;
-
-      // TODO: reverse vLong byte order for better FST
-      // prefix output sharing
-
-      // First compare up to valid seek frames:
-      while (targetUpto < targetLimit) {
-        cmp = (term.bytes[targetUpto]&0xFF) - (target.bytes[target.offset + targetUpto]&0xFF);
-        // if (DEBUG) {
-        //    System.out.println("    cycle targetUpto=" + targetUpto + " (vs limit=" + targetLimit + ") cmp=" + cmp + " (targetLabel=" + (char) (target.bytes[target.offset + targetUpto]) + " vs termLabel=" + (char) (term.bytes[targetUpto]) + ")"   + " arc.output=" + arc.output + " output=" + output);
-        // }
-        if (cmp != 0) {
-          break;
-        }
-        arc = arcs[1+targetUpto];
-        //if (arc.label != (target.bytes[target.offset + targetUpto] & 0xFF)) {
-        //System.out.println("FAIL: arc.label=" + (char) arc.label + " targetLabel=" + (char) (target.bytes[target.offset + targetUpto] & 0xFF));
-        //}
-        assert arc.label == (target.bytes[target.offset + targetUpto] & 0xFF): "arc.label=" + (char) arc.label + " targetLabel=" + (char) (target.bytes[target.offset + targetUpto] & 0xFF);
-        if (arc.output != VersionBlockTreeTermsWriter.NO_OUTPUT) {
-          output = VersionBlockTreeTermsWriter.FST_OUTPUTS.add(output, arc.output);
-        }
-        if (arc.isFinal()) {
-          lastFrame = stack[1+lastFrame.ord];
-        }
-        targetUpto++;
-      }
-
-      if (cmp == 0) {
-        final int targetUptoMid = targetUpto;
-
-        // Second compare the rest of the term, but
-        // don't save arc/output/frame; we only do this
-        // to find out if the target term is before,
-        // equal or after the current term
-        final int targetLimit2 = Math.min(target.length, term.length);
-        while (targetUpto < targetLimit2) {
-          cmp = (term.bytes[targetUpto]&0xFF) - (target.bytes[target.offset + targetUpto]&0xFF);
-          // if (DEBUG) {
-          //    System.out.println("    cycle2 targetUpto=" + targetUpto + " (vs limit=" + targetLimit + ") cmp=" + cmp + " (targetLabel=" + (char) (target.bytes[target.offset + targetUpto]) + " vs termLabel=" + (char) (term.bytes[targetUpto]) + ")");
-          // }
-          if (cmp != 0) {
-            break;
-          }
-          targetUpto++;
-        }
-
-        if (cmp == 0) {
-          cmp = term.length - target.length;
-        }
-        targetUpto = targetUptoMid;
-      }
-
-      if (cmp < 0) {
-        // Common case: target term is after current
-        // term, ie, app is seeking multiple terms
-        // in sorted order
-        // if (DEBUG) {
-        //    System.out.println("  target is after current (shares prefixLen=" + targetUpto + "); frame.ord=" + lastFrame.ord + "; targetUpto=" + targetUpto);
-        //  }
-        currentFrame = lastFrame;
-
-      } else if (cmp > 0) {
-        // Uncommon case: target term
-        // is before current term; this means we can
-        // keep the currentFrame but we must rewind it
-        // (so we scan from the start)
-        targetBeforeCurrentLength = 0;
-        changed = true;
-        // if (DEBUG) {
-        //    System.out.println("  target is before current (shares prefixLen=" + targetUpto + "); rewind frame ord=" + lastFrame.ord);
-        //  }
-        currentFrame = lastFrame;
-        currentFrame.rewind();
-      } else {
-        // Target is exactly the same as current term
-        assert term.length == target.length;
-        if (termExists) {
-
-          if (currentFrame.maxIDVersion < minIDVersion) {
-            // The max version for all terms in this block is lower than the minVersion
-            // if (DEBUG) {
-            //   System.out.println("  target is same as current maxIDVersion=" + currentFrame.maxIDVersion + " is < minIDVersion=" + minIDVersion + "; return false");
-            // }
-            return false;
-          }
-
-          currentFrame.decodeMetaData();
-          if (((IDVersionTermState) currentFrame.state).idVersion < minIDVersion) {
-            // This term's version is lower than the minVersion
-            // if (DEBUG) {
-            //   System.out.println("  target is same as current but version=" + ((IDVersionTermState) currentFrame.state).idVersion + " is < minIDVersion=" + minIDVersion + "; return false");
-            // }
-            return false;
-          }
-          // System.out.println("  term version=" + ((IDVersionTermState) currentFrame.state).idVersion + " frame version=" + currentFrame.maxIDVersion + " frame ord=" + currentFrame.ord);
-
-          // if (DEBUG) {
-          //    System.out.println("  target is same as current; return true");
-          //  }
-          return true;
-        } else {
-          // if (DEBUG) {
-          //    System.out.println("  target is same as current but term doesn't exist");
-          //  }
-        }
-        //validIndexPrefix = currentFrame.depth;
-        //term.length = target.length;
-        //return termExists;
-      }
-
-    } else {
-
-      targetBeforeCurrentLength = -1;
-      arc = fr.index.getFirstArc(arcs[0]);
-      //System.out.println("first arc=" + arc);
-
-      // Empty string prefix must have an output (block) in the index!
-      assert arc.isFinal();
-      assert arc.output != null;
-
-      // if (DEBUG) {
-      //    System.out.println("    no seek state; push root frame");
-      //  }
-
-      output = arc.output;
-
-      currentFrame = staticFrame;
-
-      //term.length = 0;
-      targetUpto = 0;
-      currentFrame = pushFrame(arc, VersionBlockTreeTermsWriter.FST_OUTPUTS.add(output, arc.nextFinalOutput), 0);
-    }
-
-    // if (DEBUG) {
-    //   System.out.println("  start index loop targetUpto=" + targetUpto + " output=" + output + " currentFrame.ord=" + currentFrame.ord + " targetBeforeCurrentLength=" + targetBeforeCurrentLength + " termExists=" + termExists);
-    // }
-
-    // We are done sharing the common prefix with the incoming target and where we are currently seek'd; now continue walking the index:
-    while (targetUpto < target.length) {
-
-      final int targetLabel = target.bytes[target.offset + targetUpto] & 0xFF;
-
-      final FST.Arc<Pair<BytesRef,Long>> nextArc = fr.index.findTargetArc(targetLabel, arc, getArc(1+targetUpto), fstReader);
-
-      if (nextArc == null) {
-
-        // Index is exhausted
-        // if (DEBUG) {
-        //    System.out.println("    index: index exhausted label=" + ((char) targetLabel) + " " + Integer.toHexString(targetLabel) + " termExists=" + termExists);
-        //  }
-            
-        validIndexPrefix = currentFrame.prefix;
-        //validIndexPrefix = targetUpto;
-
-        currentFrame.scanToFloorFrame(target);
-
-        if (!currentFrame.hasTerms) {
-          termExists = false;
-          term.bytes[targetUpto] = (byte) targetLabel;
-          term.length = 1+targetUpto;
-          // if (DEBUG) {
-          //    System.out.println("  FAST NOT_FOUND term=" + brToString(term));
-          //  }
-          return false;
-        }
-        //System.out.println("  check maxVersion=" + currentFrame.maxIDVersion + " vs " + minIDVersion);
-
-        // if (DEBUG) {
-        //   System.out.println("  frame.maxIDVersion=" + currentFrame.maxIDVersion +  " vs minIDVersion=" + minIDVersion);
-        // }
-
-        if (currentFrame.maxIDVersion < minIDVersion) {
-          // The max version for all terms in this block is lower than the minVersion
-          if (currentFrame.fp != startFrameFP || changed) {
-          //if (targetUpto+1 > term.length) {
-            termExists = false;
-            term.bytes[targetUpto] = (byte) targetLabel;
-            term.length = 1+targetUpto;
-            // if (DEBUG) {
-            //   System.out.println("    reset current term");
-            // }
-            validIndexPrefix = Math.min(validIndexPrefix, term.length);
-          }
-            //if (currentFrame.ord != startFrameOrd) {
-            //termExists = false;
-            //}
-          // if (DEBUG) {
-          //   System.out.println("    FAST version NOT_FOUND term=" + brToString(term) + " targetUpto=" + targetUpto + " currentFrame.maxIDVersion=" + currentFrame.maxIDVersion + " validIndexPrefix=" + validIndexPrefix + " startFrameFP=" + startFrameFP + " vs " + currentFrame.fp + " termExists=" + termExists);
-          // }
-          return false;
-        }
-
-        currentFrame.loadBlock();
-
-        // if (DEBUG) {
-        //   System.out.println("    scan currentFrame ord=" + currentFrame.ord);
-        // }
-        final SeekStatus result = currentFrame.scanToTerm(target, true);            
-        if (result == SeekStatus.FOUND) {
-          currentFrame.decodeMetaData();
-          if (((IDVersionTermState) currentFrame.state).idVersion < minIDVersion) {
-            // This term's version is lower than the minVersion
-            // if (DEBUG) {
-            //   System.out.println("    return NOT_FOUND: idVersion=" + ((IDVersionTermState) currentFrame.state).idVersion + " vs minIDVersion=" + minIDVersion);
-            // }
-            return false;
-          }
-
-          // if (DEBUG) {
-          //    System.out.println("  return FOUND term=" + term.utf8ToString() + " " + term);
-          //  }
-
-          return true;
-        } else {
-          // if (DEBUG) {
-          //    System.out.println("  got " + result + "; return NOT_FOUND term=" + brToString(term));
-          // }
-          return false;
-        }
-      } else {
-        // Follow this arc
-        arc = nextArc;
-        if (term.bytes[targetUpto] != (byte) targetLabel) {
-          // if (DEBUG) {
-          //   System.out.println("  now set termExists=false targetUpto=" + targetUpto + " term=" + term.bytes[targetUpto] + " targetLabel=" + targetLabel);
-          // }
-          changed = true;
-          term.bytes[targetUpto] = (byte) targetLabel;
-          termExists = false;
-        }
-        // Aggregate output as we go:
-        assert arc.output != null;
-        if (arc.output != VersionBlockTreeTermsWriter.NO_OUTPUT) {
-          output = VersionBlockTreeTermsWriter.FST_OUTPUTS.add(output, arc.output);
-        }
-
-        // if (DEBUG) {
-        //    System.out.println("    index: follow label=" + (char) ((target.bytes[target.offset + targetUpto]&0xff)) + " arc.output=" + arc.output + " arc.nfo=" + arc.nextFinalOutput);
-        //  }
-        targetUpto++;
-
-        if (arc.isFinal()) {
-          // if (DEBUG) System.out.println("    arc is final!");
-          currentFrame = pushFrame(arc, VersionBlockTreeTermsWriter.FST_OUTPUTS.add(output, arc.nextFinalOutput), targetUpto);
-          // if (DEBUG) System.out.println("    curFrame.ord=" + currentFrame.ord + " hasTerms=" + currentFrame.hasTerms);
-        }
-      }
-    }
-
-    //validIndexPrefix = targetUpto;
-    validIndexPrefix = currentFrame.prefix;
-
-    currentFrame.scanToFloorFrame(target);
-
-    // Target term is entirely contained in the index:
-    if (!currentFrame.hasTerms) {
-      termExists = false;
-      term.length = targetUpto;
-      // if (DEBUG) {
-      //    System.out.println("  FAST NOT_FOUND term=" + brToString(term));
-      //  }
-      return false;
-    }
-
-    // if (DEBUG) {
-    //   System.out.println("  frame.maxIDVersion=" + currentFrame.maxIDVersion +  " vs minIDVersion=" + minIDVersion);
-    // }
-
-    if (currentFrame.maxIDVersion < minIDVersion) {
-      // The max version for all terms in this block is lower than the minVersion
-      termExists = false;
-      term.length = targetUpto;
-      return false;
-    }
-
-    currentFrame.loadBlock();
-
-    final SeekStatus result = currentFrame.scanToTerm(target, true);            
-    if (result == SeekStatus.FOUND) {
-      // if (DEBUG) {
-      //    System.out.println("  return FOUND term=" + term.utf8ToString() + " " + term);
-      //  }
-      currentFrame.decodeMetaData();
-      if (((IDVersionTermState) currentFrame.state).idVersion < minIDVersion) {
-        // This term's version is lower than the minVersion
-        return false;
-      }
-      return true;
-    } else {
-      // if (DEBUG) {
-      //    System.out.println("  got result " + result + "; return NOT_FOUND term=" + term.utf8ToString());
-      //  }
-
-      return false;
-    }
-  }
-
-  @Override
-  public SeekStatus seekCeil(final BytesRef target) throws IOException {
-    if (fr.index == null) {
-      throw new IllegalStateException("terms index was not loaded");
-    }
-   
-    if (term.bytes.length <= target.length) {
-      term.bytes = ArrayUtil.grow(term.bytes, 1+target.length);
-    }
-
-    assert clearEOF();
-
-    //if (DEBUG) {
-    //System.out.println("\nBTTR.seekCeil seg=" + segment + " target=" + fieldInfo.name + ":" + target.utf8ToString() + " " + target + " current=" + brToString(term) + " (exists?=" + termExists + ") validIndexPrefix=  " + validIndexPrefix);
-    //printSeekState();
-    //}
-
-    FST.Arc<Pair<BytesRef,Long>> arc;
-    int targetUpto;
-    Pair<BytesRef,Long> output;
-
-    targetBeforeCurrentLength = currentFrame.ord;
-
-    if (currentFrame != staticFrame) {
-
-      // We are already seek'd; find the common
-      // prefix of new seek term vs current term and
-      // re-use the corresponding seek state.  For
-      // example, if app first seeks to foobar, then
-      // seeks to foobaz, we can re-use the seek state
-      // for the first 5 bytes.
-
-      //if (DEBUG) {
-      //System.out.println("  re-use current seek state validIndexPrefix=" + validIndexPrefix);
-      //}
-
-      arc = arcs[0];
-      assert arc.isFinal();
-      output = arc.output;
-      targetUpto = 0;
-          
-      IDVersionSegmentTermsEnumFrame lastFrame = stack[0];
-      assert validIndexPrefix <= term.length;
-
-      final int targetLimit = Math.min(target.length, validIndexPrefix);
-
-      int cmp = 0;
-
-      // TOOD: we should write our vLong backwards (MSB
-      // first) to get better sharing from the FST
-
-      // First compare up to valid seek frames:
-      while (targetUpto < targetLimit) {
-        cmp = (term.bytes[targetUpto]&0xFF) - (target.bytes[target.offset + targetUpto]&0xFF);
-        //if (DEBUG) {
-        //System.out.println("    cycle targetUpto=" + targetUpto + " (vs limit=" + targetLimit + ") cmp=" + cmp + " (targetLabel=" + (char) (target.bytes[target.offset + targetUpto]) + " vs termLabel=" + (char) (term.bytes[targetUpto]) + ")"   + " arc.output=" + arc.output + " output=" + output);
-        //}
-        if (cmp != 0) {
-          break;
-        }
-        arc = arcs[1+targetUpto];
-        assert arc.label == (target.bytes[target.offset + targetUpto] & 0xFF): "arc.label=" + (char) arc.label + " targetLabel=" + (char) (target.bytes[target.offset + targetUpto] & 0xFF);
-        // TOOD: we could save the outputs in local
-        // byte[][] instead of making new objs ever
-        // seek; but, often the FST doesn't have any
-        // shared bytes (but this could change if we
-        // reverse vLong byte order)
-        if (arc.output != VersionBlockTreeTermsWriter.NO_OUTPUT) {
-          output = VersionBlockTreeTermsWriter.FST_OUTPUTS.add(output, arc.output);
-        }
-        if (arc.isFinal()) {
-          lastFrame = stack[1+lastFrame.ord];
-        }
-        targetUpto++;
-      }
-
-
-      if (cmp == 0) {
-        final int targetUptoMid = targetUpto;
-        // Second compare the rest of the term, but
-        // don't save arc/output/frame:
-        final int targetLimit2 = Math.min(target.length, term.length);
-        while (targetUpto < targetLimit2) {
-          cmp = (term.bytes[targetUpto]&0xFF) - (target.bytes[target.offset + targetUpto]&0xFF);
-          //if (DEBUG) {
-          //System.out.println("    cycle2 targetUpto=" + targetUpto + " (vs limit=" + targetLimit + ") cmp=" + cmp + " (targetLabel=" + (char) (target.bytes[target.offset + targetUpto]) + " vs termLabel=" + (char) (term.bytes[targetUpto]) + ")");
-          //}
-          if (cmp != 0) {
-            break;
-          }
-          targetUpto++;
-        }
-
-        if (cmp == 0) {
-          cmp = term.length - target.length;
-        }
-        targetUpto = targetUptoMid;
-      }
-
-      if (cmp < 0) {
-        // Common case: target term is after current
-        // term, ie, app is seeking multiple terms
-        // in sorted order
-        //if (DEBUG) {
-        //System.out.println("  target is after current (shares prefixLen=" + targetUpto + "); clear frame.scanned ord=" + lastFrame.ord);
-        //}
-        currentFrame = lastFrame;
-
-      } else if (cmp > 0) {
-        // Uncommon case: target term
-        // is before current term; this means we can
-        // keep the currentFrame but we must rewind it
-        // (so we scan from the start)
-        targetBeforeCurrentLength = 0;
-        //if (DEBUG) {
-        //System.out.println("  target is before current (shares prefixLen=" + targetUpto + "); rewind frame ord=" + lastFrame.ord);
-        //}
-        currentFrame = lastFrame;
-        currentFrame.rewind();
-      } else {
-        // Target is exactly the same as current term
-        assert term.length == target.length;
-        if (termExists) {
-          //if (DEBUG) {
-          //System.out.println("  target is same as current; return FOUND");
-          //}
-          return SeekStatus.FOUND;
-        } else {
-          //if (DEBUG) {
-          //System.out.println("  target is same as current but term doesn't exist");
-          //}
-        }
-      }
-
-    } else {
-
-      targetBeforeCurrentLength = -1;
-      arc = fr.index.getFirstArc(arcs[0]);
-
-      // Empty string prefix must have an output (block) in the index!
-      assert arc.isFinal();
-      assert arc.output != null;
-
-      //if (DEBUG) {
-      //System.out.println("    no seek state; push root frame");
-      //}
-
-      output = arc.output;
-
-      currentFrame = staticFrame;
-
-      //term.length = 0;
-      targetUpto = 0;
-      currentFrame = pushFrame(arc, VersionBlockTreeTermsWriter.FST_OUTPUTS.add(output, arc.nextFinalOutput), 0);
-    }
-
-    //if (DEBUG) {
-    //System.out.println("  start index loop targetUpto=" + targetUpto + " output=" + output + " currentFrame.ord+1=" + currentFrame.ord + " targetBeforeCurrentLength=" + targetBeforeCurrentLength);
-    //}
-
-    // We are done sharing the common prefix with the incoming target and where we are currently seek'd; now continue walking the index:
-    while (targetUpto < target.length) {
-
-      final int targetLabel = target.bytes[target.offset + targetUpto] & 0xFF;
-
-      final FST.Arc<Pair<BytesRef,Long>> nextArc = fr.index.findTargetArc(targetLabel, arc, getArc(1+targetUpto), fstReader);
-
-      if (nextArc == null) {
-
-        // Index is exhausted
-        // if (DEBUG) {
-        //   System.out.println("    index: index exhausted label=" + ((char) targetLabel) + " " + toHex(targetLabel));
-        // }
-            
-        validIndexPrefix = currentFrame.prefix;
-        //validIndexPrefix = targetUpto;
-
-        currentFrame.scanToFloorFrame(target);
-
-        currentFrame.loadBlock();
-
-        final SeekStatus result = currentFrame.scanToTerm(target, false);
-        if (result == SeekStatus.END) {
-          term.copyBytes(target);
-          termExists = false;
-
-          if (next() != null) {
-            //if (DEBUG) {
-            //System.out.println("  return NOT_FOUND term=" + brToString(term) + " " + term);
-            //}
-            return SeekStatus.NOT_FOUND;
-          } else {
-            //if (DEBUG) {
-            //System.out.println("  return END");
-            //}
-            return SeekStatus.END;
-          }
-        } else {
-          //if (DEBUG) {
-          //System.out.println("  return " + result + " term=" + brToString(term) + " " + term);
-          //}
-          return result;
-        }
-      } else {
-        // Follow this arc
-        term.bytes[targetUpto] = (byte) targetLabel;
-        arc = nextArc;
-        // Aggregate output as we go:
-        assert arc.output != null;
-        if (arc.output != VersionBlockTreeTermsWriter.NO_OUTPUT) {
-          output = VersionBlockTreeTermsWriter.FST_OUTPUTS.add(output, arc.output);
-        }
-
-        //if (DEBUG) {
-        //System.out.println("    index: follow label=" + toHex(target.bytes[target.offset + targetUpto]&0xff) + " arc.output=" + arc.output + " arc.nfo=" + arc.nextFinalOutput);
-        //}
-        targetUpto++;
-
-        if (arc.isFinal()) {
-          //if (DEBUG) System.out.println("    arc is final!");
-          currentFrame = pushFrame(arc, VersionBlockTreeTermsWriter.FST_OUTPUTS.add(output, arc.nextFinalOutput), targetUpto);
-          //if (DEBUG) System.out.println("    curFrame.ord=" + currentFrame.ord + " hasTerms=" + currentFrame.hasTerms);
-        }
-      }
-    }
-
-    //validIndexPrefix = targetUpto;
-    validIndexPrefix = currentFrame.prefix;
-
-    currentFrame.scanToFloorFrame(target);
-
-    currentFrame.loadBlock();
-
-    final SeekStatus result = currentFrame.scanToTerm(target, false);
-
-    if (result == SeekStatus.END) {
-      term.copyBytes(target);
-      termExists = false;
-      if (next() != null) {
-        //if (DEBUG) {
-        //System.out.println("  return NOT_FOUND term=" + term.utf8ToString() + " " + term);
-        //}
-        return SeekStatus.NOT_FOUND;
-      } else {
-        //if (DEBUG) {
-        //System.out.println("  return END");
-        //}
-        return SeekStatus.END;
-      }
-    } else {
-      return result;
-    }
-  }
-
-  @SuppressWarnings("unused")
-  private void printSeekState(PrintStream out) throws IOException {
-    if (currentFrame == staticFrame) {
-      out.println("  no prior seek");
-    } else {
-      out.println("  prior seek state:");
-      int ord = 0;
-      boolean isSeekFrame = true;
-      while(true) {
-        IDVersionSegmentTermsEnumFrame f = getFrame(ord);
-        assert f != null;
-        final BytesRef prefix = new BytesRef(term.bytes, 0, f.prefix);
-        if (f.nextEnt == -1) {
-          out.println("    frame " + (isSeekFrame ? "(seek)" : "(next)") + " ord=" + ord + " fp=" + f.fp + (f.isFloor ? (" (fpOrig=" + f.fpOrig + ")") : "") + " prefixLen=" + f.prefix + " prefix=" + brToString(prefix) + (f.nextEnt == -1 ? "" : (" (of " + f.entCount + ")")) + " hasTerms=" + f.hasTerms + " isFloor=" + f.isFloor + " code=" + ((f.fp<<VersionBlockTreeTermsWriter.OUTPUT_FLAGS_NUM_BITS) + (f.hasTerms ? VersionBlockTreeTermsWriter.OUTPUT_FLAG_HAS_TERMS:0) + (f.isFloor ? VersionBlockTreeTermsWriter.OUTPUT_FLAG_IS_FLOOR:0)) + " isLastInFloor=" + f.isLastInFloor + " mdUpto=" + f.metaDataUpto + " tbOrd=" + f.getTermBlockOrd());
-        } else {
-          out.println("    frame " + (isSeekFrame ? "(seek, loaded)" : "(next, loaded)") + " ord=" + ord + " fp=" + f.fp + (f.isFloor ? (" (fpOrig=" + f.fpOrig + ")") : "") + " prefixLen=" + f.prefix + " prefix=" + brToString(prefix) + " nextEnt=" + f.nextEnt + (f.nextEnt == -1 ? "" : (" (of " + f.entCount + ")")) + " hasTerms=" + f.hasTerms + " isFloor=" + f.isFloor + " code=" + ((f.fp<<VersionBlockTreeTermsWriter.OUTPUT_FLAGS_NUM_BITS) + (f.hasTerms ? VersionBlockTreeTermsWriter.OUTPUT_FLAG_HAS_TERMS:0) + (f.isFloor ? VersionBlockTreeTermsWriter.OUTPUT_FLAG_IS_FLOOR:0)) + " lastSubFP=" + f.lastSubFP + " isLastInFloor=" + f.isLastInFloor + " mdUpto=" + f.metaDataUpto + " tbOrd=" + f.getTermBlockOrd());
-        }
-        if (fr.index != null) {
-          assert !isSeekFrame || f.arc != null: "isSeekFrame=" + isSeekFrame + " f.arc=" + f.arc;
-          if (f.prefix > 0 && isSeekFrame && f.arc.label != (term.bytes[f.prefix-1]&0xFF)) {
-            out.println("      broken seek state: arc.label=" + (char) f.arc.label + " vs term byte=" + (char) (term.bytes[f.prefix-1]&0xFF));
-            throw new RuntimeException("seek state is broken");
-          }
-          Pair<BytesRef,Long> output = Util.get(fr.index, prefix);
-          if (output == null) {
-            out.println("      broken seek state: prefix is not final in index");
-            throw new RuntimeException("seek state is broken");
-          } else if (isSeekFrame && !f.isFloor) {
-            final ByteArrayDataInput reader = new ByteArrayDataInput(output.output1.bytes, output.output1.offset, output.output1.length);
-            final long codeOrig = reader.readVLong();
-            final long code = (f.fp << VersionBlockTreeTermsWriter.OUTPUT_FLAGS_NUM_BITS) | (f.hasTerms ? VersionBlockTreeTermsWriter.OUTPUT_FLAG_HAS_TERMS:0) | (f.isFloor ? VersionBlockTreeTermsWriter.OUTPUT_FLAG_IS_FLOOR:0);
-            if (codeOrig != code) {
-              out.println("      broken seek state: output code=" + codeOrig + " doesn't match frame code=" + code);
-              throw new RuntimeException("seek state is broken");
-            }
-          }
-        }
-        if (f == currentFrame) {
-          break;
-        }
-        if (f.prefix == validIndexPrefix) {
-          isSeekFrame = false;
-        }
-        ord++;
-      }
-    }
-  }
-
-  /* Decodes only the term bytes of the next term.  If caller then asks for
-     metadata, ie docFreq, totalTermFreq or pulls a D/&PEnum, we then (lazily)
-     decode all metadata up to the current term. */
-  @Override
-  public BytesRef next() throws IOException {
-
-    if (in == null) {
-      // Fresh TermsEnum; seek to first term:
-      final FST.Arc<Pair<BytesRef,Long>> arc;
-      if (fr.index != null) {
-        arc = fr.index.getFirstArc(arcs[0]);
-        // Empty string prefix must have an output in the index!
-        assert arc.isFinal();
-      } else {
-        arc = null;
-      }
-      currentFrame = pushFrame(arc, fr.rootCode, 0);
-      currentFrame.loadBlock();
-    }
-
-    targetBeforeCurrentLength = currentFrame.ord;
-
-    assert !eof;
-    //if (DEBUG) {
-    //System.out.println("\nBTTR.next seg=" + segment + " term=" + brToString(term) + " termExists?=" + termExists + " field=" + fieldInfo.name + " termBlockOrd=" + currentFrame.state.termBlockOrd + " validIndexPrefix=" + validIndexPrefix);
-    //printSeekState();
-    //}
-
-    if (currentFrame == staticFrame) {
-      // If seek was previously called and the term was
-      // cached, or seek(TermState) was called, usually
-      // caller is just going to pull a D/&PEnum or get
-      // docFreq, etc.  But, if they then call next(),
-      // this method catches up all internal state so next()
-      // works properly:
-      //if (DEBUG) System.out.println("  re-seek to pending term=" + term.utf8ToString() + " " + term);
-      final boolean result = seekExact(term);
-      assert result;
-    }
-
-    // Pop finished blocks
-    while (currentFrame.nextEnt == currentFrame.entCount) {
-      if (!currentFrame.isLastInFloor) {
-        currentFrame.loadNextFloorBlock();
-      } else {
-        //if (DEBUG) System.out.println("  pop frame");
-        if (currentFrame.ord == 0) {
-          //if (DEBUG) System.out.println("  return null");
-          assert setEOF();
-          term.length = 0;
-          validIndexPrefix = 0;
-          currentFrame.rewind();
-          termExists = false;
-          return null;
-        }
-        final long lastFP = currentFrame.fpOrig;
-        currentFrame = stack[currentFrame.ord-1];
-
-        if (currentFrame.nextEnt == -1 || currentFrame.lastSubFP != lastFP) {
-          // We popped into a frame that's not loaded
-          // yet or not scan'd to the right entry
-          currentFrame.scanToFloorFrame(term);
-          currentFrame.loadBlock();
-          currentFrame.scanToSubBlock(lastFP);
-        }
-
-        // Note that the seek state (last seek) has been
-        // invalidated beyond this depth
-        validIndexPrefix = Math.min(validIndexPrefix, currentFrame.prefix);
-        //if (DEBUG) {
-        //System.out.println("  reset validIndexPrefix=" + validIndexPrefix);
-        //}
-      }
-    }
-
-    while(true) {
-      if (currentFrame.next()) {
-        // Push to new block:
-        //if (DEBUG) System.out.println("  push frame");
-        currentFrame = pushFrame(null, currentFrame.lastSubFP, term.length);
-        // This is a "next" frame -- even if it's
-        // floor'd we must pretend it isn't so we don't
-        // try to scan to the right floor frame:
-        currentFrame.isFloor = false;
-        //currentFrame.hasTerms = true;
-        currentFrame.loadBlock();
-      } else {
-        //if (DEBUG) System.out.println("  return term=" + term.utf8ToString() + " " + term + " currentFrame.ord=" + currentFrame.ord);
-        return term;
-      }
-    }
-  }
-
-  @Override
-  public BytesRef term() {
-    assert !eof;
-    return term;
-  }
-
-  @Override
-  public int docFreq() throws IOException {
-    assert !eof;
-    return 1;
-  }
-
-  @Override
-  public long totalTermFreq() throws IOException {
-    assert !eof;
-    return 1;
-  }
-
-  @Override
-  public DocsEnum docs(Bits skipDocs, DocsEnum reuse, int flags) throws IOException {
-    assert !eof;
-    //if (DEBUG) {
-    //System.out.println("BTTR.docs seg=" + segment);
-    //}
-    currentFrame.decodeMetaData();
-    //if (DEBUG) {
-    //System.out.println("  state=" + currentFrame.state);
-    //}
-    return fr.parent.postingsReader.docs(fr.fieldInfo, currentFrame.state, skipDocs, reuse, flags);
-  }
-
-  @Override
-  public DocsAndPositionsEnum docsAndPositions(Bits skipDocs, DocsAndPositionsEnum reuse, int flags) throws IOException {
-    if (fr.fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) < 0) {
-      // Positions were not indexed:
-      return null;
-    }
-
-    assert !eof;
-    currentFrame.decodeMetaData();
-    return fr.parent.postingsReader.docsAndPositions(fr.fieldInfo, currentFrame.state, skipDocs, reuse, flags);
-  }
-
-  @Override
-  public void seekExact(BytesRef target, TermState otherState) {
-    // if (DEBUG) {
-    //   System.out.println("BTTR.seekExact termState seg=" + segment + " target=" + target.utf8ToString() + " " + target + " state=" + otherState);
-    // }
-    assert clearEOF();
-    if (target.compareTo(term) != 0 || !termExists) {
-      assert otherState != null && otherState instanceof BlockTermState;
-      currentFrame = staticFrame;
-      currentFrame.state.copyFrom(otherState);
-      term.copyBytes(target);
-      currentFrame.metaDataUpto = currentFrame.getTermBlockOrd();
-      assert currentFrame.metaDataUpto > 0;
-      validIndexPrefix = 0;
-    } else {
-      // if (DEBUG) {
-      //   System.out.println("  skip seek: already on target state=" + currentFrame.state);
-      // }
-    }
-  }
-      
-  @Override
-  public TermState termState() throws IOException {
-    assert !eof;
-    currentFrame.decodeMetaData();
-    TermState ts = currentFrame.state.clone();
-    //if (DEBUG) System.out.println("BTTR.termState seg=" + segment + " state=" + ts);
-    return ts;
-  }
-
-  @Override
-  public void seekExact(long ord) {
-    throw new UnsupportedOperationException();
-  }
-
-  @Override
-  public long ord() {
-    throw new UnsupportedOperationException();
-  }
-
-  @Override
-  public String toString() {
-    return "IDVersionSegmentTermsEnum(seg=" + fr.parent.segment + ")";
-  }
-}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/idversion/IDVersionSegmentTermsEnumFrame.java b/lucene/codecs/src/java/org/apache/lucene/codecs/idversion/IDVersionSegmentTermsEnumFrame.java
deleted file mode 100644
index fa64056..0000000
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/idversion/IDVersionSegmentTermsEnumFrame.java
+++ /dev/null
@@ -1,738 +0,0 @@
-package org.apache.lucene.codecs.idversion;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.BlockTermState;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.TermsEnum.SeekStatus;
-import org.apache.lucene.store.ByteArrayDataInput;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.fst.FST;
-import org.apache.lucene.util.fst.PairOutputs.Pair;
-
-final class IDVersionSegmentTermsEnumFrame {
-  // Our index in stack[]:
-  final int ord;
-
-  boolean hasTerms;
-  boolean hasTermsOrig;
-  boolean isFloor;
-
-  // static boolean DEBUG = IDVersionSegmentTermsEnum.DEBUG;
-
-  /** Highest version of any term in this block. */
-  long maxIDVersion;
-
-  FST.Arc<Pair<BytesRef,Long>> arc;
-
-  // File pointer where this block was loaded from
-  long fp;
-  long fpOrig;
-  long fpEnd;
-
-  byte[] suffixBytes = new byte[128];
-  final ByteArrayDataInput suffixesReader = new ByteArrayDataInput();
-
-  byte[] floorData = new byte[32];
-  final ByteArrayDataInput floorDataReader = new ByteArrayDataInput();
-
-  // Length of prefix shared by all terms in this block
-  int prefix;
-
-  // Number of entries (term or sub-block) in this block
-  int entCount;
-
-  // Which term we will next read, or -1 if the block
-  // isn't loaded yet
-  int nextEnt;
-
-  // True if this block is either not a floor block,
-  // or, it's the last sub-block of a floor block
-  boolean isLastInFloor;
-
-  // True if all entries are terms
-  boolean isLeafBlock;
-
-  long lastSubFP;
-
-  int nextFloorLabel;
-  int numFollowFloorBlocks;
-
-  // Next term to decode metaData; we decode metaData
-  // lazily so that scanning to find the matching term is
-  // fast and only if you find a match and app wants the
-  // stats or docs/positions enums, will we decode the
-  // metaData
-  int metaDataUpto;
-
-  final BlockTermState state;
-
-  // metadata buffer, holding monotonic values
-  public long[] longs;
-  // metadata buffer, holding general values
-  public byte[] bytes;
-  ByteArrayDataInput bytesReader;
-
-  private final IDVersionSegmentTermsEnum ste;
-
-  public IDVersionSegmentTermsEnumFrame(IDVersionSegmentTermsEnum ste, int ord) throws IOException {
-    this.ste = ste;
-    this.ord = ord;
-    this.state = ste.fr.parent.postingsReader.newTermState();
-    this.state.totalTermFreq = -1;
-    this.longs = new long[ste.fr.longsSize];
-  }
-
-  public void setFloorData(ByteArrayDataInput in, BytesRef source) {
-    final int numBytes = source.length - (in.getPosition() - source.offset);
-    if (numBytes > floorData.length) {
-      floorData = new byte[ArrayUtil.oversize(numBytes, 1)];
-    }
-    System.arraycopy(source.bytes, source.offset+in.getPosition(), floorData, 0, numBytes);
-    floorDataReader.reset(floorData, 0, numBytes);
-    numFollowFloorBlocks = floorDataReader.readVInt();
-    nextFloorLabel = floorDataReader.readByte() & 0xff;
-    //if (DEBUG) {
-    //System.out.println("    setFloorData fpOrig=" + fpOrig + " bytes=" + new BytesRef(source.bytes, source.offset + in.getPosition(), numBytes) + " numFollowFloorBlocks=" + numFollowFloorBlocks + " nextFloorLabel=" + toHex(nextFloorLabel));
-    //}
-  }
-
-  public int getTermBlockOrd() {
-    return isLeafBlock ? nextEnt : state.termBlockOrd;
-  }
-
-  void loadNextFloorBlock() throws IOException {
-    //if (DEBUG) {
-    //System.out.println("    loadNextFloorBlock fp=" + fp + " fpEnd=" + fpEnd);
-    //}
-    assert arc == null || isFloor: "arc=" + arc + " isFloor=" + isFloor;
-    fp = fpEnd;
-    nextEnt = -1;
-    loadBlock();
-  }
-
-  /* Does initial decode of next block of terms; this
-     doesn't actually decode the docFreq, totalTermFreq,
-     postings details (frq/prx offset, etc.) metadata;
-     it just loads them as byte[] blobs which are then      
-     decoded on-demand if the metadata is ever requested
-     for any term in this block.  This enables terms-only
-     intensive consumes (eg certain MTQs, respelling) to
-     not pay the price of decoding metadata they won't
-     use. */
-  void loadBlock() throws IOException {
-
-    // Clone the IndexInput lazily, so that consumers
-    // that just pull a TermsEnum to
-    // seekExact(TermState) don't pay this cost:
-    ste.initIndexInput();
-
-    if (nextEnt != -1) {
-      // Already loaded
-      return;
-    }
-    //System.out.println("blc=" + blockLoadCount);
-
-    ste.in.seek(fp);
-    int code = ste.in.readVInt();
-    entCount = code >>> 1;
-    assert entCount > 0;
-    isLastInFloor = (code & 1) != 0;
-    assert arc == null || (isLastInFloor || isFloor);
-
-    // TODO: if suffixes were stored in random-access
-    // array structure, then we could do binary search
-    // instead of linear scan to find target term; eg
-    // we could have simple array of offsets
-
-    // term suffixes:
-    code = ste.in.readVInt();
-    isLeafBlock = (code & 1) != 0;
-    int numBytes = code >>> 1;
-    if (suffixBytes.length < numBytes) {
-      suffixBytes = new byte[ArrayUtil.oversize(numBytes, 1)];
-    }
-    ste.in.readBytes(suffixBytes, 0, numBytes);
-    suffixesReader.reset(suffixBytes, 0, numBytes);
-
-    /*if (DEBUG) {
-      if (arc == null) {
-      System.out.println("    loadBlock (next) fp=" + fp + " entCount=" + entCount + " prefixLen=" + prefix + " isLastInFloor=" + isLastInFloor + " leaf?=" + isLeafBlock);
-      } else {
-      System.out.println("    loadBlock (seek) fp=" + fp + " entCount=" + entCount + " prefixLen=" + prefix + " hasTerms?=" + hasTerms + " isFloor?=" + isFloor + " isLastInFloor=" + isLastInFloor + " leaf?=" + isLeafBlock);
-      }
-      }*/
-
-    metaDataUpto = 0;
-
-    state.termBlockOrd = 0;
-    nextEnt = 0;
-    lastSubFP = -1;
-
-    // TODO: we could skip this if !hasTerms; but
-    // that's rare so won't help much
-    // metadata
-    numBytes = ste.in.readVInt();
-    if (bytes == null) {
-      bytes = new byte[ArrayUtil.oversize(numBytes, 1)];
-      bytesReader = new ByteArrayDataInput();
-    } else if (bytes.length < numBytes) {
-      bytes = new byte[ArrayUtil.oversize(numBytes, 1)];
-    }
-    ste.in.readBytes(bytes, 0, numBytes);
-    bytesReader.reset(bytes, 0, numBytes);
-
-    // Sub-blocks of a single floor block are always
-    // written one after another -- tail recurse:
-    fpEnd = ste.in.getFilePointer();
-    // if (DEBUG) {
-    //   System.out.println("      fpEnd=" + fpEnd);
-    // }
-  }
-
-  void rewind() {
-
-    // Force reload:
-    fp = fpOrig;
-    nextEnt = -1;
-    hasTerms = hasTermsOrig;
-    if (isFloor) {
-      floorDataReader.rewind();
-      numFollowFloorBlocks = floorDataReader.readVInt();
-      nextFloorLabel = floorDataReader.readByte() & 0xff;
-    }
-
-    /*
-    //System.out.println("rewind");
-    // Keeps the block loaded, but rewinds its state:
-    if (nextEnt > 0 || fp != fpOrig) {
-    if (DEBUG) {
-    System.out.println("      rewind frame ord=" + ord + " fpOrig=" + fpOrig + " fp=" + fp + " hasTerms?=" + hasTerms + " isFloor?=" + isFloor + " nextEnt=" + nextEnt + " prefixLen=" + prefix);
-    }
-    if (fp != fpOrig) {
-    fp = fpOrig;
-    nextEnt = -1;
-    } else {
-    nextEnt = 0;
-    }
-    hasTerms = hasTermsOrig;
-    if (isFloor) {
-    floorDataReader.rewind();
-    numFollowFloorBlocks = floorDataReader.readVInt();
-    nextFloorLabel = floorDataReader.readByte() & 0xff;
-    }
-    assert suffixBytes != null;
-    suffixesReader.rewind();
-    assert statBytes != null;
-    statsReader.rewind();
-    metaDataUpto = 0;
-    state.termBlockOrd = 0;
-    // TODO: skip this if !hasTerms?  Then postings
-    // impl wouldn't have to write useless 0 byte
-    postingsReader.resetTermsBlock(fieldInfo, state);
-    lastSubFP = -1;
-    } else if (DEBUG) {
-    System.out.println("      skip rewind fp=" + fp + " fpOrig=" + fpOrig + " nextEnt=" + nextEnt + " ord=" + ord);
-    }
-    */
-  }
-
-  public boolean next() {
-    return isLeafBlock ? nextLeaf() : nextNonLeaf();
-  }
-
-  // Decodes next entry; returns true if it's a sub-block
-  public boolean nextLeaf() {
-    //if (DEBUG) System.out.println("  frame.next ord=" + ord + " nextEnt=" + nextEnt + " entCount=" + entCount);
-    assert nextEnt != -1 && nextEnt < entCount: "nextEnt=" + nextEnt + " entCount=" + entCount + " fp=" + fp;
-    nextEnt++;
-    suffix = suffixesReader.readVInt();
-    startBytePos = suffixesReader.getPosition();
-    ste.term.length = prefix + suffix;
-    if (ste.term.bytes.length < ste.term.length) {
-      ste.term.grow(ste.term.length);
-    }
-    suffixesReader.readBytes(ste.term.bytes, prefix, suffix);
-    // A normal term
-    ste.termExists = true;
-    return false;
-  }
-
-  public boolean nextNonLeaf() {
-    // if (DEBUG) System.out.println("  frame.next ord=" + ord + " nextEnt=" + nextEnt + " entCount=" + entCount);
-    assert nextEnt != -1 && nextEnt < entCount: "nextEnt=" + nextEnt + " entCount=" + entCount + " fp=" + fp;
-    nextEnt++;
-    final int code = suffixesReader.readVInt();
-    suffix = code >>> 1;
-    startBytePos = suffixesReader.getPosition();
-    ste.term.length = prefix + suffix;
-    if (ste.term.bytes.length < ste.term.length) {
-      ste.term.grow(ste.term.length);
-    }
-    suffixesReader.readBytes(ste.term.bytes, prefix, suffix);
-    if ((code & 1) == 0) {
-      // A normal term
-      ste.termExists = true;
-      subCode = 0;
-      state.termBlockOrd++;
-      return false;
-    } else {
-      // A sub-block; make sub-FP absolute:
-      ste.termExists = false;
-      subCode = suffixesReader.readVLong();
-      lastSubFP = fp - subCode;
-      // if (DEBUG) {
-      //   System.out.println("    lastSubFP=" + lastSubFP);
-      // }
-      return true;
-    }
-  }
-        
-  // TODO: make this array'd so we can do bin search?
-  // likely not worth it?  need to measure how many
-  // floor blocks we "typically" get
-  public void scanToFloorFrame(BytesRef target) {
-
-    if (!isFloor || target.length <= prefix) {
-      // if (DEBUG) {
-      //    System.out.println("    scanToFloorFrame skip: isFloor=" + isFloor + " target.length=" + target.length + " vs prefix=" + prefix);
-      //  }
-      return;
-    }
-
-    final int targetLabel = target.bytes[target.offset + prefix] & 0xFF;
-
-    // if (DEBUG) {
-    //    System.out.println("    scanToFloorFrame fpOrig=" + fpOrig + " targetLabel=" + ((char) targetLabel) + " vs nextFloorLabel=" + ((char) nextFloorLabel) + " numFollowFloorBlocks=" + numFollowFloorBlocks);
-    //  }
-
-    if (targetLabel < nextFloorLabel) {
-      // if (DEBUG) {
-      //    System.out.println("      already on correct block");
-      //  }
-      return;
-    }
-
-    assert numFollowFloorBlocks != 0;
-
-    long newFP = fpOrig;
-    while (true) {
-      final long code = floorDataReader.readVLong();
-      newFP = fpOrig + (code >>> 1);
-      hasTerms = (code & 1) != 0;
-      // if (DEBUG) {
-      //    System.out.println("      label=" + ((char) nextFloorLabel) + " fp=" + newFP + " hasTerms?=" + hasTerms + " numFollowFloor=" + numFollowFloorBlocks);
-      //  }
-            
-      isLastInFloor = numFollowFloorBlocks == 1;
-      numFollowFloorBlocks--;
-
-      if (isLastInFloor) {
-        nextFloorLabel = 256;
-        // if (DEBUG) {
-        //    System.out.println("        stop!  last block nextFloorLabel=" + ((char) nextFloorLabel));
-        //  }
-        break;
-      } else {
-        nextFloorLabel = floorDataReader.readByte() & 0xff;
-        if (targetLabel < nextFloorLabel) {
-          // if (DEBUG) {
-          //    System.out.println("        stop!  nextFloorLabel=" + ((char) nextFloorLabel));
-          //  }
-          break;
-        }
-      }
-    }
-
-    if (newFP != fp) {
-      // Force re-load of the block:
-      // if (DEBUG) {
-      //    System.out.println("      force switch to fp=" + newFP + " oldFP=" + fp);
-      //  }
-      nextEnt = -1;
-      fp = newFP;
-    } else {
-      // if (DEBUG) {
-      //    System.out.println("      stay on same fp=" + newFP);
-      //  }
-    }
-  }
-    
-  public void decodeMetaData() throws IOException {
-
-    //if (DEBUG) System.out.println("\nBTTR.decodeMetadata seg=" + ste.fr.parent.segment + " mdUpto=" + metaDataUpto + " vs termBlockOrd=" + state.termBlockOrd);
-
-    assert nextEnt >= 0;
-
-    // lazily catch up on metadata decode:
-    final int limit = getTermBlockOrd();
-    boolean absolute = metaDataUpto == 0;
-
-    // TODO: better API would be "jump straight to term=N"???
-    while (metaDataUpto < limit) {
-
-      // TODO: we could make "tiers" of metadata, ie,
-      // decode docFreq/totalTF but don't decode postings
-      // metadata; this way caller could get
-      // docFreq/totalTF w/o paying decode cost for
-      // postings
-
-      // TODO: if docFreq were bulk decoded we could
-      // just skipN here:
-
-      // stats
-      state.docFreq = 1;
-      state.totalTermFreq = 1;
-      //if (DEBUG) System.out.println("    dF=" + state.docFreq);
-      // metadata 
-      for (int i = 0; i < ste.fr.longsSize; i++) {
-        longs[i] = bytesReader.readVLong();
-      }
-      ste.fr.parent.postingsReader.decodeTerm(longs, bytesReader, ste.fr.fieldInfo, state, absolute);
-
-      metaDataUpto++;
-      absolute = false;
-    }
-    state.termBlockOrd = metaDataUpto;
-  }
-
-  // Used only by assert
-  private boolean prefixMatches(BytesRef target) {
-    for(int bytePos=0;bytePos<prefix;bytePos++) {
-      if (target.bytes[target.offset + bytePos] != ste.term.bytes[bytePos]) {
-        return false;
-      }
-    }
-
-    return true;
-  }
-
-  // Scans to sub-block that has this target fp; only
-  // called by next(); NOTE: does not set
-  // startBytePos/suffix as a side effect
-  public void scanToSubBlock(long subFP) {
-    assert !isLeafBlock;
-    //if (DEBUG) System.out.println("  scanToSubBlock fp=" + fp + " subFP=" + subFP + " entCount=" + entCount + " lastSubFP=" + lastSubFP);
-    //assert nextEnt == 0;
-    if (lastSubFP == subFP) {
-      //if (DEBUG) System.out.println("    already positioned");
-      return;
-    }
-    assert subFP < fp : "fp=" + fp + " subFP=" + subFP;
-    final long targetSubCode = fp - subFP;
-    //if (DEBUG) System.out.println("    targetSubCode=" + targetSubCode);
-    while(true) {
-      assert nextEnt < entCount;
-      nextEnt++;
-      final int code = suffixesReader.readVInt();
-      suffixesReader.skipBytes(isLeafBlock ? code : code >>> 1);
-      //if (DEBUG) System.out.println("    " + nextEnt + " (of " + entCount + ") ent isSubBlock=" + ((code&1)==1));
-      if ((code & 1) != 0) {
-        final long subCode = suffixesReader.readVLong();
-        //if (DEBUG) System.out.println("      subCode=" + subCode);
-        if (targetSubCode == subCode) {
-          //if (DEBUG) System.out.println("        match!");
-          lastSubFP = subFP;
-          return;
-        }
-      } else {
-        state.termBlockOrd++;
-      }
-    }
-  }
-
-  // NOTE: sets startBytePos/suffix as a side effect
-  public SeekStatus scanToTerm(BytesRef target, boolean exactOnly) throws IOException {
-    return isLeafBlock ? scanToTermLeaf(target, exactOnly) : scanToTermNonLeaf(target, exactOnly);
-  }
-
-  private int startBytePos;
-  private int suffix;
-  private long subCode;
-
-  // Target's prefix matches this block's prefix; we
-  // scan the entries check if the suffix matches.
-  public SeekStatus scanToTermLeaf(BytesRef target, boolean exactOnly) throws IOException {
-
-    // if (DEBUG) System.out.println("    scanToTermLeaf: block fp=" + fp + " prefix=" + prefix + " nextEnt=" + nextEnt + " (of " + entCount + ") target=" + IDVersionSegmentTermsEnum.brToString(target) + " term=" + IDVersionSegmentTermsEnum.brToString(ste.term));
-
-    assert nextEnt != -1;
-
-    ste.termExists = true;
-    subCode = 0;
-
-    if (nextEnt == entCount) {
-      if (exactOnly) {
-        fillTerm();
-      }
-      return SeekStatus.END;
-    }
-
-    assert prefixMatches(target);
-
-    // Loop over each entry (term or sub-block) in this block:
-    //nextTerm: while(nextEnt < entCount) {
-    nextTerm: while (true) {
-      nextEnt++;
-
-      suffix = suffixesReader.readVInt();
-
-      // if (DEBUG) {
-      //    BytesRef suffixBytesRef = new BytesRef();
-      //    suffixBytesRef.bytes = suffixBytes;
-      //    suffixBytesRef.offset = suffixesReader.getPosition();
-      //    suffixBytesRef.length = suffix;
-      //    System.out.println("      cycle: term " + (nextEnt-1) + " (of " + entCount + ") suffix=" + IDVersionSegmentTermsEnum.brToString(suffixBytesRef));
-      // }
-
-      final int termLen = prefix + suffix;
-      startBytePos = suffixesReader.getPosition();
-      suffixesReader.skipBytes(suffix);
-
-      final int targetLimit = target.offset + (target.length < termLen ? target.length : termLen);
-      int targetPos = target.offset + prefix;
-
-      // Loop over bytes in the suffix, comparing to
-      // the target
-      int bytePos = startBytePos;
-      while(true) {
-        final int cmp;
-        final boolean stop;
-        if (targetPos < targetLimit) {
-          cmp = (suffixBytes[bytePos++]&0xFF) - (target.bytes[targetPos++]&0xFF);
-          stop = false;
-        } else {
-          assert targetPos == targetLimit;
-          cmp = termLen - target.length;
-          stop = true;
-        }
-
-        if (cmp < 0) {
-          // Current entry is still before the target;
-          // keep scanning
-
-          if (nextEnt == entCount) {
-            if (exactOnly) {
-              fillTerm();
-            }
-            // We are done scanning this block
-            break nextTerm;
-          } else {
-            continue nextTerm;
-          }
-        } else if (cmp > 0) {
-
-          // Done!  Current entry is after target --
-          // return NOT_FOUND:
-          fillTerm();
-
-          if (!exactOnly && !ste.termExists) {
-            // We are on a sub-block, and caller wants
-            // us to position to the next term after
-            // the target, so we must recurse into the
-            // sub-frame(s):
-            ste.currentFrame = ste.pushFrame(null, ste.currentFrame.lastSubFP, termLen);
-            ste.currentFrame.loadBlock();
-            while (ste.currentFrame.next()) {
-              ste.currentFrame = ste.pushFrame(null, ste.currentFrame.lastSubFP, ste.term.length);
-              ste.currentFrame.loadBlock();
-            }
-          }
-                
-          //if (DEBUG) System.out.println("        not found");
-          return SeekStatus.NOT_FOUND;
-        } else if (stop) {
-          // Exact match!
-
-          // This cannot be a sub-block because we
-          // would have followed the index to this
-          // sub-block from the start:
-
-          assert ste.termExists;
-          fillTerm();
-          //if (DEBUG) System.out.println("        found!");
-          return SeekStatus.FOUND;
-        }
-      }
-    }
-
-    // It is possible (and OK) that terms index pointed us
-    // at this block, but, we scanned the entire block and
-    // did not find the term to position to.  This happens
-    // when the target is after the last term in the block
-    // (but, before the next term in the index).  EG
-    // target could be foozzz, and terms index pointed us
-    // to the foo* block, but the last term in this block
-    // was fooz (and, eg, first term in the next block will
-    // bee fop).
-    //if (DEBUG) System.out.println("      block end");
-    if (exactOnly) {
-      fillTerm();
-    }
-
-    // TODO: not consistent that in the
-    // not-exact case we don't next() into the next
-    // frame here
-    return SeekStatus.END;
-  }
-
-  // Target's prefix matches this block's prefix; we
-  // scan the entries check if the suffix matches.
-  public SeekStatus scanToTermNonLeaf(BytesRef target, boolean exactOnly) throws IOException {
-
-    // if (DEBUG) System.out.println("    scanToTermNonLeaf: block fp=" + fp + " prefix=" + prefix + " nextEnt=" + nextEnt + " (of " + entCount + ") target=" + IDVersionSegmentTermsEnum.brToString(target) + " term=" + IDVersionSegmentTermsEnum.brToString(ste.term));
-
-    assert nextEnt != -1;
-
-    if (nextEnt == entCount) {
-      if (exactOnly) {
-        fillTerm();
-        ste.termExists = subCode == 0;
-      }
-      return SeekStatus.END;
-    }
-
-    assert prefixMatches(target);
-
-    // Loop over each entry (term or sub-block) in this block:
-    //nextTerm: while(nextEnt < entCount) {
-    nextTerm: while (true) {
-      nextEnt++;
-
-      final int code = suffixesReader.readVInt();
-      suffix = code >>> 1;
-      // if (DEBUG) {
-      //   BytesRef suffixBytesRef = new BytesRef();
-      //   suffixBytesRef.bytes = suffixBytes;
-      //   suffixBytesRef.offset = suffixesReader.getPosition();
-      //   suffixBytesRef.length = suffix;
-      //   System.out.println("      cycle: " + ((code&1)==1 ? "sub-block" : "term") + " " + (nextEnt-1) + " (of " + entCount + ") suffix=" + brToString(suffixBytesRef));
-      // }
-
-      ste.termExists = (code & 1) == 0;
-      final int termLen = prefix + suffix;
-      startBytePos = suffixesReader.getPosition();
-      suffixesReader.skipBytes(suffix);
-      if (ste.termExists) {
-        state.termBlockOrd++;
-        subCode = 0;
-      } else {
-        subCode = suffixesReader.readVLong();
-        lastSubFP = fp - subCode;
-      }
-
-      final int targetLimit = target.offset + (target.length < termLen ? target.length : termLen);
-      int targetPos = target.offset + prefix;
-
-      // Loop over bytes in the suffix, comparing to
-      // the target
-      int bytePos = startBytePos;
-      while(true) {
-        final int cmp;
-        final boolean stop;
-        if (targetPos < targetLimit) {
-          cmp = (suffixBytes[bytePos++]&0xFF) - (target.bytes[targetPos++]&0xFF);
-          stop = false;
-        } else {
-          assert targetPos == targetLimit;
-          cmp = termLen - target.length;
-          stop = true;
-        }
-
-        if (cmp < 0) {
-          // Current entry is still before the target;
-          // keep scanning
-
-          if (nextEnt == entCount) {
-            if (exactOnly) {
-              fillTerm();
-              //termExists = true;
-            }
-            // We are done scanning this block
-            break nextTerm;
-          } else {
-            continue nextTerm;
-          }
-        } else if (cmp > 0) {
-
-          // Done!  Current entry is after target --
-          // return NOT_FOUND:
-          fillTerm();
-
-          if (!exactOnly && !ste.termExists) {
-            // We are on a sub-block, and caller wants
-            // us to position to the next term after
-            // the target, so we must recurse into the
-            // sub-frame(s):
-            ste.currentFrame = ste.pushFrame(null, ste.currentFrame.lastSubFP, termLen);
-            ste.currentFrame.loadBlock();
-            while (ste.currentFrame.next()) {
-              ste.currentFrame = ste.pushFrame(null, ste.currentFrame.lastSubFP, ste.term.length);
-              ste.currentFrame.loadBlock();
-            }
-          }
-                
-          //if (DEBUG) System.out.println("        not found");
-          return SeekStatus.NOT_FOUND;
-        } else if (stop) {
-          // Exact match!
-
-          // This cannot be a sub-block because we
-          // would have followed the index to this
-          // sub-block from the start:
-
-          assert ste.termExists;
-          fillTerm();
-          //if (DEBUG) System.out.println("        found!");
-          return SeekStatus.FOUND;
-        }
-      }
-    }
-
-    // It is possible (and OK) that terms index pointed us
-    // at this block, but, we scanned the entire block and
-    // did not find the term to position to.  This happens
-    // when the target is after the last term in the block
-    // (but, before the next term in the index).  EG
-    // target could be foozzz, and terms index pointed us
-    // to the foo* block, but the last term in this block
-    // was fooz (and, eg, first term in the next block will
-    // bee fop).
-    //if (DEBUG) System.out.println("      block end");
-    if (exactOnly) {
-      fillTerm();
-    }
-
-    // TODO: not consistent that in the
-    // not-exact case we don't next() into the next
-    // frame here
-    return SeekStatus.END;
-  }
-
-  private void fillTerm() {
-    final int termLength = prefix + suffix;
-    ste.term.length = prefix + suffix;
-    if (ste.term.bytes.length < termLength) {
-      ste.term.grow(termLength);
-    }
-    System.arraycopy(suffixBytes, startBytePos, ste.term.bytes, prefix, suffix);
-  }
-}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/idversion/IDVersionTermState.java b/lucene/codecs/src/java/org/apache/lucene/codecs/idversion/IDVersionTermState.java
deleted file mode 100644
index 227f3f8..0000000
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/idversion/IDVersionTermState.java
+++ /dev/null
@@ -1,41 +0,0 @@
-package org.apache.lucene.codecs.idversion;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.codecs.BlockTermState;
-import org.apache.lucene.index.TermState;
-
-final class IDVersionTermState extends BlockTermState {
-  long idVersion;
-  int docID;
-
-  @Override
-  public IDVersionTermState clone() {
-    IDVersionTermState other = new IDVersionTermState();
-    other.copyFrom(this);
-    return other;
-  }
-
-  @Override
-  public void copyFrom(TermState _other) {
-    super.copyFrom(_other);
-    IDVersionTermState other = (IDVersionTermState) _other;
-    idVersion = other.idVersion;
-    docID = other.docID;
-  }
-}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/idversion/SingleDocsAndPositionsEnum.java b/lucene/codecs/src/java/org/apache/lucene/codecs/idversion/SingleDocsAndPositionsEnum.java
deleted file mode 100644
index eecc700..0000000
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/idversion/SingleDocsAndPositionsEnum.java
+++ /dev/null
@@ -1,105 +0,0 @@
-package org.apache.lucene.codecs.idversion;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.index.DocsAndPositionsEnum;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-
-class SingleDocsAndPositionsEnum extends DocsAndPositionsEnum {
-  private int doc;
-  private int pos;
-  private int singleDocID;
-  private Bits liveDocs;
-  private long version;
-  private final BytesRef payload;
-
-  public SingleDocsAndPositionsEnum() {
-    payload = new BytesRef(8);
-    payload.length = 8;
-  }
-
-  /** For reuse */
-  public void reset(int singleDocID, long version, Bits liveDocs) {
-    doc = -1;
-    this.liveDocs = liveDocs;
-    this.singleDocID = singleDocID;
-    this.version = version;
-  }
-
-  @Override
-  public int nextDoc() {
-    if (doc == -1 && (liveDocs == null || liveDocs.get(singleDocID))) {
-      doc = singleDocID;
-    } else {
-      doc = NO_MORE_DOCS;
-    }
-    pos = -1;
-    
-    return doc;
-  }
-
-  @Override
-  public int docID() {
-    return doc;
-  }
-
-  @Override
-  public int advance(int target) {
-    if (doc == -1 && target <= singleDocID && (liveDocs == null || liveDocs.get(singleDocID))) {
-      doc = singleDocID;
-      pos = -1;
-    } else {
-      doc = NO_MORE_DOCS;
-    }
-    return doc;
-  }
-
-  @Override
-  public long cost() {
-    return 1;
-  }
-
-  @Override
-  public int freq() {
-    return 1;
-  }
-
-  @Override
-  public int nextPosition() {
-    assert pos == -1;
-    pos = 0;
-    IDVersionPostingsFormat.longToBytes(version, payload);
-    return pos;
-  }
-
-  @Override
-  public BytesRef getPayload() {
-    return payload;
-  }
-
-  @Override
-  public int startOffset() {
-    return -1;
-  }
-
-  @Override
-  public int endOffset() {
-    return -1;
-  }
-}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/idversion/SingleDocsEnum.java b/lucene/codecs/src/java/org/apache/lucene/codecs/idversion/SingleDocsEnum.java
deleted file mode 100644
index b29619c..0000000
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/idversion/SingleDocsEnum.java
+++ /dev/null
@@ -1,71 +0,0 @@
-package org.apache.lucene.codecs.idversion;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.util.Bits;
-
-class SingleDocsEnum extends DocsEnum {
-
-  private int doc;
-  private int singleDocID;
-  private Bits liveDocs;
-
-  /** For reuse */
-  public void reset(int singleDocID, Bits liveDocs) {
-    doc = -1;
-    this.liveDocs = liveDocs;
-    this.singleDocID = singleDocID;
-  }
-
-  @Override
-  public int nextDoc() {
-    if (doc == -1 && (liveDocs == null || liveDocs.get(singleDocID))) {
-      doc = singleDocID;
-    } else {
-      doc = NO_MORE_DOCS;
-    }
-    
-    return doc;
-  }
-
-  @Override
-  public int docID() {
-    return doc;
-  }
-
-  @Override
-  public int advance(int target) {
-    if (doc == -1 && target <= singleDocID && (liveDocs == null || liveDocs.get(singleDocID))) {
-      doc = singleDocID;
-    } else {
-      doc = NO_MORE_DOCS;
-    }
-    return doc;
-  }
-
-  @Override
-  public long cost() {
-    return 1;
-  }
-
-  @Override
-  public int freq() {
-    return 1;
-  }
-}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/idversion/VersionBlockTreeTermsReader.java b/lucene/codecs/src/java/org/apache/lucene/codecs/idversion/VersionBlockTreeTermsReader.java
deleted file mode 100644
index b4e69e1..0000000
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/idversion/VersionBlockTreeTermsReader.java
+++ /dev/null
@@ -1,274 +0,0 @@
-package org.apache.lucene.codecs.idversion;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.io.PrintStream;
-import java.util.Collections;
-import java.util.Iterator;
-import java.util.TreeMap;
-
-import org.apache.lucene.codecs.BlockTermState;
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.FieldsProducer;
-import org.apache.lucene.codecs.PostingsReaderBase;
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.DocsAndPositionsEnum;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.TermState;
-import org.apache.lucene.index.Terms;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.store.ByteArrayDataInput;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.RamUsageEstimator;
-import org.apache.lucene.util.StringHelper;
-import org.apache.lucene.util.automaton.CompiledAutomaton;
-import org.apache.lucene.util.automaton.RunAutomaton;
-import org.apache.lucene.util.automaton.Transition;
-import org.apache.lucene.util.fst.ByteSequenceOutputs;
-import org.apache.lucene.util.fst.FST;
-import org.apache.lucene.util.fst.Outputs;
-import org.apache.lucene.util.fst.PairOutputs.Pair;
-import org.apache.lucene.util.fst.PairOutputs;
-import org.apache.lucene.util.fst.Util;
-
-/**
- * See {@link VersionBlockTreeTermsWriter}.
- *
- * @lucene.experimental
- */
-
-final class VersionBlockTreeTermsReader extends FieldsProducer {
-
-  // Open input to the main terms dict file (_X.tiv)
-  final IndexInput in;
-
-  //private static final boolean DEBUG = BlockTreeTermsWriter.DEBUG;
-
-  // Reads the terms dict entries, to gather state to
-  // produce DocsEnum on demand
-  final PostingsReaderBase postingsReader;
-
-  private final TreeMap<String,VersionFieldReader> fields = new TreeMap<>();
-
-  /** File offset where the directory starts in the terms file. */
-  private long dirOffset;
-
-  /** File offset where the directory starts in the index file. */
-  private long indexDirOffset;
-
-  final String segment;
-  
-  private final int version;
-
-  /** Sole constructor. */
-  public VersionBlockTreeTermsReader(Directory dir, FieldInfos fieldInfos, SegmentInfo info,
-                                     PostingsReaderBase postingsReader, IOContext ioContext,
-                                     String segmentSuffix)
-    throws IOException {
-    
-    this.postingsReader = postingsReader;
-
-    this.segment = info.name;
-    in = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, VersionBlockTreeTermsWriter.TERMS_EXTENSION),
-                       ioContext);
-
-    boolean success = false;
-    IndexInput indexIn = null;
-
-    try {
-      version = readHeader(in);
-      indexIn = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, VersionBlockTreeTermsWriter.TERMS_INDEX_EXTENSION),
-                                ioContext);
-      int indexVersion = readIndexHeader(indexIn);
-      if (indexVersion != version) {
-        throw new CorruptIndexException("mixmatched version files: " + in + "=" + version + "," + indexIn + "=" + indexVersion);
-      }
-      
-      // verify
-      CodecUtil.checksumEntireFile(indexIn);
-
-      // Have PostingsReader init itself
-      postingsReader.init(in);
-
-      // Read per-field details
-      seekDir(in, dirOffset);
-      seekDir(indexIn, indexDirOffset);
-
-      final int numFields = in.readVInt();
-      if (numFields < 0) {
-        throw new CorruptIndexException("invalid numFields: " + numFields + " (resource=" + in + ")");
-      }
-
-      for(int i=0;i<numFields;i++) {
-        final int field = in.readVInt();
-        final long numTerms = in.readVLong();
-        assert numTerms >= 0;
-        final int numBytes = in.readVInt();
-        final BytesRef code = new BytesRef(new byte[numBytes]);
-        in.readBytes(code.bytes, 0, numBytes);
-        code.length = numBytes;
-        final long version = in.readVLong();
-        final Pair<BytesRef,Long> rootCode = VersionBlockTreeTermsWriter.FST_OUTPUTS.newPair(code, version);
-        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);
-        assert fieldInfo != null: "field=" + field;
-        final long sumTotalTermFreq = numTerms;
-        final long sumDocFreq = numTerms;
-        assert numTerms <= Integer.MAX_VALUE;
-        final int docCount = (int) numTerms;
-        final int longsSize = in.readVInt();
-
-        BytesRef minTerm = readBytesRef(in);
-        BytesRef maxTerm = readBytesRef(in);
-        if (docCount < 0 || docCount > info.getDocCount()) { // #docs with field must be <= #docs
-          throw new CorruptIndexException("invalid docCount: " + docCount + " maxDoc: " + info.getDocCount() + " (resource=" + in + ")");
-        }
-        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field
-          throw new CorruptIndexException("invalid sumDocFreq: " + sumDocFreq + " docCount: " + docCount + " (resource=" + in + ")");
-        }
-        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings
-          throw new CorruptIndexException("invalid sumTotalTermFreq: " + sumTotalTermFreq + " sumDocFreq: " + sumDocFreq + " (resource=" + in + ")");
-        }
-        final long indexStartFP = indexIn.readVLong();
-        VersionFieldReader previous = fields.put(fieldInfo.name,       
-                                                 new VersionFieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,
-                                                                        indexStartFP, longsSize, indexIn, minTerm, maxTerm));
-        if (previous != null) {
-          throw new CorruptIndexException("duplicate field: " + fieldInfo.name + " (resource=" + in + ")");
-        }
-      }
-      indexIn.close();
-
-      success = true;
-    } finally {
-      if (!success) {
-        // this.close() will close in:
-        IOUtils.closeWhileHandlingException(indexIn, this);
-      }
-    }
-  }
-
-  private static BytesRef readBytesRef(IndexInput in) throws IOException {
-    BytesRef bytes = new BytesRef();
-    bytes.length = in.readVInt();
-    bytes.bytes = new byte[bytes.length];
-    in.readBytes(bytes.bytes, 0, bytes.length);
-    return bytes;
-  }
-
-  /** Reads terms file header. */
-  private int readHeader(IndexInput input) throws IOException {
-    int version = CodecUtil.checkHeader(input, VersionBlockTreeTermsWriter.TERMS_CODEC_NAME,
-                          VersionBlockTreeTermsWriter.VERSION_START,
-                          VersionBlockTreeTermsWriter.VERSION_CURRENT);
-    return version;
-  }
-
-  /** Reads index file header. */
-  private int readIndexHeader(IndexInput input) throws IOException {
-    int version = CodecUtil.checkHeader(input, VersionBlockTreeTermsWriter.TERMS_INDEX_CODEC_NAME,
-                          VersionBlockTreeTermsWriter.VERSION_START,
-                          VersionBlockTreeTermsWriter.VERSION_CURRENT);
-    return version;
-  }
-
-  /** Seek {@code input} to the directory offset. */
-  private void seekDir(IndexInput input, long dirOffset)
-      throws IOException {
-    input.seek(input.length() - CodecUtil.footerLength() - 8);
-    dirOffset = input.readLong();
-    input.seek(dirOffset);
-  }
-
-  // for debugging
-  // private static String toHex(int v) {
-  //   return "0x" + Integer.toHexString(v);
-  // }
-
-  @Override
-  public void close() throws IOException {
-    try {
-      IOUtils.close(in, postingsReader);
-    } finally { 
-      // Clear so refs to terms index is GCable even if
-      // app hangs onto us:
-      fields.clear();
-    }
-  }
-
-  @Override
-  public Iterator<String> iterator() {
-    return Collections.unmodifiableSet(fields.keySet()).iterator();
-  }
-
-  @Override
-  public Terms terms(String field) throws IOException {
-    assert field != null;
-    return fields.get(field);
-  }
-
-  @Override
-  public int size() {
-    return fields.size();
-  }
-
-  // for debugging
-  String brToString(BytesRef b) {
-    if (b == null) {
-      return "null";
-    } else {
-      try {
-        return b.utf8ToString() + " " + b;
-      } catch (Throwable t) {
-        // If BytesRef isn't actually UTF8, or it's eg a
-        // prefix of UTF8 that ends mid-unicode-char, we
-        // fallback to hex:
-        return b.toString();
-      }
-    }
-  }
-
-  @Override
-  public long ramBytesUsed() {
-    long sizeInByes = ((postingsReader!=null) ? postingsReader.ramBytesUsed() : 0);
-    for(VersionFieldReader reader : fields.values()) {
-      sizeInByes += reader.ramBytesUsed();
-    }
-    return sizeInByes;
-  }
-
-  @Override
-  public void checkIntegrity() throws IOException {
-    // term dictionary
-    CodecUtil.checksumEntireFile(in);
-      
-    // postings
-    postingsReader.checkIntegrity();
-  }
-}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/idversion/VersionBlockTreeTermsWriter.java b/lucene/codecs/src/java/org/apache/lucene/codecs/idversion/VersionBlockTreeTermsWriter.java
deleted file mode 100644
index 24b656f..0000000
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/idversion/VersionBlockTreeTermsWriter.java
+++ /dev/null
@@ -1,1032 +0,0 @@
-package org.apache.lucene.codecs.idversion;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.lucene.codecs.BlockTermState;
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.FieldsConsumer;
-import org.apache.lucene.codecs.PostingsWriterBase;
-import org.apache.lucene.codecs.blocktree.BlockTreeTermsWriter;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.Fields;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.index.Terms;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.store.RAMOutputStream;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.FixedBitSet;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.IntsRef;
-import org.apache.lucene.util.fst.Builder;
-import org.apache.lucene.util.fst.ByteSequenceOutputs;
-import org.apache.lucene.util.fst.BytesRefFSTEnum;
-import org.apache.lucene.util.fst.FST;
-import org.apache.lucene.util.fst.NoOutputs;
-import org.apache.lucene.util.fst.PairOutputs.Pair;
-import org.apache.lucene.util.fst.PairOutputs;
-import org.apache.lucene.util.fst.PositiveIntOutputs;
-import org.apache.lucene.util.fst.Util;
-import org.apache.lucene.util.packed.PackedInts;
-
-/*
-  TODO:
-  
-    - Currently there is a one-to-one mapping of indexed
-      term to term block, but we could decouple the two, ie,
-      put more terms into the index than there are blocks.
-      The index would take up more RAM but then it'd be able
-      to avoid seeking more often and could make PK/FuzzyQ
-      faster if the additional indexed terms could store
-      the offset into the terms block.
-
-    - The blocks are not written in true depth-first
-      order, meaning if you just next() the file pointer will
-      sometimes jump backwards.  For example, block foo* will
-      be written before block f* because it finished before.
-      This could possibly hurt performance if the terms dict is
-      not hot, since OSs anticipate sequential file access.  We
-      could fix the writer to re-order the blocks as a 2nd
-      pass.
-
-    - Each block encodes the term suffixes packed
-      sequentially using a separate vInt per term, which is
-      1) wasteful and 2) slow (must linear scan to find a
-      particular suffix).  We should instead 1) make
-      random-access array so we can directly access the Nth
-      suffix, and 2) bulk-encode this array using bulk int[]
-      codecs; then at search time we can binary search when
-      we seek a particular term.
-*/
-
-/**
- * This is just like {@link BlockTreeTermsWriter}, except it also stores a version per term, and adds a method to its TermsEnum
- * implementation to seekExact only if the version is >= the specified version.  The version is added to the terms index to avoid seeking if
- * no term in the block has a high enough version.  The term blocks file is .tiv and the terms index extension is .tipv.
- *
- * @lucene.experimental
- */
-
-final class VersionBlockTreeTermsWriter extends FieldsConsumer {
-
-  // private static boolean DEBUG = IDVersionSegmentTermsEnum.DEBUG;
-
-  static final PairOutputs<BytesRef,Long> FST_OUTPUTS = new PairOutputs<>(ByteSequenceOutputs.getSingleton(),
-                                                                          PositiveIntOutputs.getSingleton());
-
-  static final Pair<BytesRef,Long> NO_OUTPUT = FST_OUTPUTS.getNoOutput();
-
-  /** Suggested default value for the {@code
-   *  minItemsInBlock} parameter to {@link
-   *  #VersionBlockTreeTermsWriter(SegmentWriteState,PostingsWriterBase,int,int)}. */
-  public final static int DEFAULT_MIN_BLOCK_SIZE = 25;
-
-  /** Suggested default value for the {@code
-   *  maxItemsInBlock} parameter to {@link
-   *  #VersionBlockTreeTermsWriter(SegmentWriteState,PostingsWriterBase,int,int)}. */
-  public final static int DEFAULT_MAX_BLOCK_SIZE = 48;
-
-  //public final static boolean DEBUG = false;
-  //private final static boolean SAVE_DOT_FILES = false;
-
-  static final int OUTPUT_FLAGS_NUM_BITS = 2;
-  static final int OUTPUT_FLAGS_MASK = 0x3;
-  static final int OUTPUT_FLAG_IS_FLOOR = 0x1;
-  static final int OUTPUT_FLAG_HAS_TERMS = 0x2;
-
-  /** Extension of terms file */
-  static final String TERMS_EXTENSION = "tiv";
-  final static String TERMS_CODEC_NAME = "VERSION_BLOCK_TREE_TERMS_DICT";
-
-  /** Initial terms format. */
-  public static final int VERSION_START = 0;
-
-  /** Current terms format. */
-  public static final int VERSION_CURRENT = VERSION_START;
-
-  /** Extension of terms index file */
-  static final String TERMS_INDEX_EXTENSION = "tipv";
-  final static String TERMS_INDEX_CODEC_NAME = "VERSION_BLOCK_TREE_TERMS_INDEX";
-
-  private final IndexOutput out;
-  private final IndexOutput indexOut;
-  final int maxDoc;
-  final int minItemsInBlock;
-  final int maxItemsInBlock;
-
-  final PostingsWriterBase postingsWriter;
-  final FieldInfos fieldInfos;
-
-  private static class FieldMetaData {
-    public final FieldInfo fieldInfo;
-    public final Pair<BytesRef,Long> rootCode;
-    public final long numTerms;
-    public final long indexStartFP;
-    private final int longsSize;
-    public final BytesRef minTerm;
-    public final BytesRef maxTerm;
-
-    public FieldMetaData(FieldInfo fieldInfo, Pair<BytesRef,Long> rootCode, long numTerms, long indexStartFP, int longsSize,
-                         BytesRef minTerm, BytesRef maxTerm) {
-      assert numTerms > 0;
-      this.fieldInfo = fieldInfo;
-      assert rootCode != null: "field=" + fieldInfo.name + " numTerms=" + numTerms;
-      this.rootCode = rootCode;
-      this.indexStartFP = indexStartFP;
-      this.numTerms = numTerms;
-      this.longsSize = longsSize;
-      this.minTerm = minTerm;
-      this.maxTerm = maxTerm;
-    }
-  }
-
-  private final List<FieldMetaData> fields = new ArrayList<>();
-  // private final String segment;
-
-  /** Create a new writer.  The number of items (terms or
-   *  sub-blocks) per block will aim to be between
-   *  minItemsPerBlock and maxItemsPerBlock, though in some
-   *  cases the blocks may be smaller than the min. */
-  public VersionBlockTreeTermsWriter(
-                                     SegmentWriteState state,
-                                     PostingsWriterBase postingsWriter,
-                                     int minItemsInBlock,
-                                     int maxItemsInBlock)
-    throws IOException
-  {
-    if (minItemsInBlock <= 1) {
-      throw new IllegalArgumentException("minItemsInBlock must be >= 2; got " + minItemsInBlock);
-    }
-    if (maxItemsInBlock <= 0) {
-      throw new IllegalArgumentException("maxItemsInBlock must be >= 1; got " + maxItemsInBlock);
-    }
-    if (minItemsInBlock > maxItemsInBlock) {
-      throw new IllegalArgumentException("maxItemsInBlock must be >= minItemsInBlock; got maxItemsInBlock=" + maxItemsInBlock + " minItemsInBlock=" + minItemsInBlock);
-    }
-    if (2*(minItemsInBlock-1) > maxItemsInBlock) {
-      throw new IllegalArgumentException("maxItemsInBlock must be at least 2*(minItemsInBlock-1); got maxItemsInBlock=" + maxItemsInBlock + " minItemsInBlock=" + minItemsInBlock);
-    }
-
-    maxDoc = state.segmentInfo.getDocCount();
-
-    final String termsFileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, TERMS_EXTENSION);
-    out = state.directory.createOutput(termsFileName, state.context);
-    boolean success = false;
-    IndexOutput indexOut = null;
-    try {
-      fieldInfos = state.fieldInfos;
-      this.minItemsInBlock = minItemsInBlock;
-      this.maxItemsInBlock = maxItemsInBlock;
-      CodecUtil.writeHeader(out, TERMS_CODEC_NAME, VERSION_CURRENT);   
-
-      //DEBUG = state.segmentName.equals("_4a");
-
-      final String termsIndexFileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, TERMS_INDEX_EXTENSION);
-      indexOut = state.directory.createOutput(termsIndexFileName, state.context);
-      CodecUtil.writeHeader(indexOut, TERMS_INDEX_CODEC_NAME, VERSION_CURRENT); 
-
-      this.postingsWriter = postingsWriter;
-      // segment = state.segmentInfo.name;
-
-      // System.out.println("BTW.init seg=" + state.segmentName);
-
-      postingsWriter.init(out);                          // have consumer write its format/header
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(out, indexOut);
-      }
-    }
-    this.indexOut = indexOut;
-  }
-
-  /** Writes the terms file trailer. */
-  private void writeTrailer(IndexOutput out, long dirStart) throws IOException {
-    out.writeLong(dirStart);    
-  }
-
-  /** Writes the index file trailer. */
-  private void writeIndexTrailer(IndexOutput indexOut, long dirStart) throws IOException {
-    indexOut.writeLong(dirStart);    
-  }
-
-  @Override
-  public void write(Fields fields) throws IOException {
-
-    String lastField = null;
-    for(String field : fields) {
-      assert lastField == null || lastField.compareTo(field) < 0;
-      lastField = field;
-
-      Terms terms = fields.terms(field);
-      if (terms == null) {
-        continue;
-      }
-
-      TermsEnum termsEnum = terms.iterator(null);
-
-      TermsWriter termsWriter = new TermsWriter(fieldInfos.fieldInfo(field));
-      BytesRef minTerm = null;
-      BytesRef maxTerm = new BytesRef();
-      while (true) {
-        BytesRef term = termsEnum.next();
-        if (term == null) {
-          break;
-        }
-        if (minTerm == null) {
-          minTerm = BytesRef.deepCopyOf(term);
-        }
-        maxTerm.copyBytes(term);
-        termsWriter.write(term, termsEnum);
-      }
-
-      termsWriter.finish(minTerm, minTerm == null ? null : maxTerm);
-    }
-  }
-  
-  static long encodeOutput(long fp, boolean hasTerms, boolean isFloor) {
-    assert fp < (1L << 62);
-    return (fp << 2) | (hasTerms ? OUTPUT_FLAG_HAS_TERMS : 0) | (isFloor ? OUTPUT_FLAG_IS_FLOOR : 0);
-  }
-
-  private static class PendingEntry {
-    public final boolean isTerm;
-
-    protected PendingEntry(boolean isTerm) {
-      this.isTerm = isTerm;
-    }
-  }
-
-  private static final class PendingTerm extends PendingEntry {
-    public final BytesRef term;
-    // stats + metadata
-    public final BlockTermState state;
-
-    public PendingTerm(BytesRef term, BlockTermState state) {
-      super(true);
-      this.term = term;
-      this.state = state;
-    }
-
-    @Override
-    public String toString() {
-      return term.utf8ToString();
-    }
-  }
-
-  private static final class PendingBlock extends PendingEntry {
-    public final BytesRef prefix;
-    public final long fp;
-    public FST<Pair<BytesRef,Long>> index;
-    public List<FST<Pair<BytesRef,Long>>> subIndices;
-    public final boolean hasTerms;
-    public final boolean isFloor;
-    public final int floorLeadByte;
-    private final IntsRef scratchIntsRef = new IntsRef();
-    /** Max version for all terms in this block. */
-    private final long maxVersion;
-
-    public PendingBlock(BytesRef prefix, long maxVersion, long fp, boolean hasTerms, boolean isFloor, int floorLeadByte, List<FST<Pair<BytesRef,Long>>> subIndices) {
-      super(false);
-      this.prefix = prefix;
-      this.maxVersion = maxVersion;
-      this.fp = fp;
-      this.hasTerms = hasTerms;
-      this.isFloor = isFloor;
-      this.floorLeadByte = floorLeadByte;
-      this.subIndices = subIndices;
-    }
-
-    @Override
-    public String toString() {
-      return "BLOCK: " + prefix.utf8ToString();
-    }
-
-    public void compileIndex(List<PendingBlock> floorBlocks, RAMOutputStream scratchBytes) throws IOException {
-
-      assert (isFloor && floorBlocks != null && floorBlocks.size() != 0) || (!isFloor && floorBlocks == null): "isFloor=" + isFloor + " floorBlocks=" + floorBlocks;
-
-      assert scratchBytes.getFilePointer() == 0;
-
-      long maxVersionIndex = maxVersion;
-
-      // TODO: try writing the leading vLong in MSB order
-      // (opposite of what Lucene does today), for better
-      // outputs sharing in the FST
-      scratchBytes.writeVLong(encodeOutput(fp, hasTerms, isFloor));
-      if (isFloor) {
-        scratchBytes.writeVInt(floorBlocks.size());
-        for (PendingBlock sub : floorBlocks) {
-          assert sub.floorLeadByte != -1;
-          maxVersionIndex = Math.max(maxVersionIndex, sub.maxVersion);
-          //if (DEBUG) {
-          //  System.out.println("    write floorLeadByte=" + Integer.toHexString(sub.floorLeadByte&0xff));
-          //}
-          scratchBytes.writeByte((byte) sub.floorLeadByte);
-          assert sub.fp > fp;
-          scratchBytes.writeVLong((sub.fp - fp) << 1 | (sub.hasTerms ? 1 : 0));
-        }
-      }
-
-      final Builder<Pair<BytesRef,Long>> indexBuilder = new Builder<>(FST.INPUT_TYPE.BYTE1,
-                                                                      0, 0, true, false, Integer.MAX_VALUE,
-                                                                      FST_OUTPUTS, null, false,
-                                                                      PackedInts.COMPACT, true, 15);
-      //if (DEBUG) {
-      //  System.out.println("  compile index for prefix=" + prefix);
-      //}
-      //indexBuilder.DEBUG = false;
-      final byte[] bytes = new byte[(int) scratchBytes.getFilePointer()];
-      assert bytes.length > 0;
-      scratchBytes.writeTo(bytes, 0);
-      indexBuilder.add(Util.toIntsRef(prefix, scratchIntsRef), FST_OUTPUTS.newPair(new BytesRef(bytes, 0, bytes.length), Long.MAX_VALUE - maxVersionIndex));
-      scratchBytes.reset();
-
-      // Copy over index for all sub-blocks
-
-      if (subIndices != null) {
-        for(FST<Pair<BytesRef,Long>> subIndex : subIndices) {
-          append(indexBuilder, subIndex);
-        }
-      }
-
-      if (floorBlocks != null) {
-        for (PendingBlock sub : floorBlocks) {
-          if (sub.subIndices != null) {
-            for(FST<Pair<BytesRef,Long>> subIndex : sub.subIndices) {
-              append(indexBuilder, subIndex);
-            }
-          }
-          sub.subIndices = null;
-        }
-      }
-
-      index = indexBuilder.finish();
-      subIndices = null;
-
-      /*
-      Writer w = new OutputStreamWriter(new FileOutputStream("out.dot"));
-      Util.toDot(index, w, false, false);
-      System.out.println("SAVED to out.dot");
-      w.close();
-      */
-    }
-
-    // TODO: maybe we could add bulk-add method to
-    // Builder?  Takes FST and unions it w/ current
-    // FST.
-    private void append(Builder<Pair<BytesRef,Long>> builder, FST<Pair<BytesRef,Long>> subIndex) throws IOException {
-      final BytesRefFSTEnum<Pair<BytesRef,Long>> subIndexEnum = new BytesRefFSTEnum<>(subIndex);
-      BytesRefFSTEnum.InputOutput<Pair<BytesRef,Long>> indexEnt;
-      while((indexEnt = subIndexEnum.next()) != null) {
-        //if (DEBUG) {
-        //  System.out.println("      add sub=" + indexEnt.input + " " + indexEnt.input + " output=" + indexEnt.output);
-        //}
-        builder.add(Util.toIntsRef(indexEnt.input, scratchIntsRef), indexEnt.output);
-      }
-    }
-  }
-
-  final RAMOutputStream scratchBytes = new RAMOutputStream();
-
-  class TermsWriter {
-    private final FieldInfo fieldInfo;
-    private final int longsSize;
-    private long numTerms;
-    final FixedBitSet docsSeen;
-    long indexStartFP;
-
-    // Used only to partition terms into the block tree; we
-    // don't pull an FST from this builder:
-    private final NoOutputs noOutputs;
-    private final Builder<Object> blockBuilder;
-
-    // PendingTerm or PendingBlock:
-    private final List<PendingEntry> pending = new ArrayList<>();
-
-    // Index into pending of most recently written block
-    private int lastBlockIndex = -1;
-
-    // Re-used when segmenting a too-large block into floor
-    // blocks:
-    private int[] subBytes = new int[10];
-    private int[] subTermCounts = new int[10];
-    private int[] subTermCountSums = new int[10];
-    private int[] subSubCounts = new int[10];
-
-    // This class assigns terms to blocks "naturally", ie,
-    // according to the number of terms under a given prefix
-    // that we encounter:
-    private class FindBlocks extends Builder.FreezeTail<Object> {
-
-      @Override
-      public void freeze(final Builder.UnCompiledNode<Object>[] frontier, int prefixLenPlus1, final IntsRef lastInput) throws IOException {
-
-        //if (DEBUG) System.out.println("  freeze prefixLenPlus1=" + prefixLenPlus1);
-
-        for(int idx=lastInput.length; idx >= prefixLenPlus1; idx--) {
-          final Builder.UnCompiledNode<Object> node = frontier[idx];
-
-          long totCount = 0;
-
-          if (node.isFinal) {
-            totCount++;
-          }
-
-          for(int arcIdx=0;arcIdx<node.numArcs;arcIdx++) {
-            @SuppressWarnings("unchecked") final Builder.UnCompiledNode<Object> target = (Builder.UnCompiledNode<Object>) node.arcs[arcIdx].target;
-            totCount += target.inputCount;
-            target.clear();
-            node.arcs[arcIdx].target = null;
-          }
-          node.numArcs = 0;
-
-          if (totCount >= minItemsInBlock || idx == 0) {
-            // We are on a prefix node that has enough
-            // entries (terms or sub-blocks) under it to let
-            // us write a new block or multiple blocks (main
-            // block + follow on floor blocks):
-            //if (DEBUG) {
-            //  if (totCount < minItemsInBlock && idx != 0) {
-            //    System.out.println("  force block has terms");
-            //  }
-            //}
-            writeBlocks(lastInput, idx, (int) totCount);
-            node.inputCount = 1;
-          } else {
-            // stragglers!  carry count upwards
-            node.inputCount = totCount;
-          }
-          frontier[idx] = new Builder.UnCompiledNode<>(blockBuilder, idx);
-        }
-      }
-    }
-
-    // Write the top count entries on the pending stack as
-    // one or more blocks.  Returns how many blocks were
-    // written.  If the entry count is <= maxItemsPerBlock
-    // we just write a single block; else we break into
-    // primary (initial) block and then one or more
-    // following floor blocks:
-
-    void writeBlocks(IntsRef prevTerm, int prefixLength, int count) throws IOException {
-      if (count <= maxItemsInBlock) {
-        // Easy case: not floor block.  Eg, prefix is "foo",
-        // and we found 30 terms/sub-blocks starting w/ that
-        // prefix, and minItemsInBlock <= 30 <=
-        // maxItemsInBlock.
-        final PendingBlock nonFloorBlock = writeBlock(prevTerm, prefixLength, prefixLength, count, count, 0, false, -1, true);
-        nonFloorBlock.compileIndex(null, scratchBytes);
-        pending.add(nonFloorBlock);
-      } else {
-        // Floor block case.  Eg, prefix is "foo" but we
-        // have 100 terms/sub-blocks starting w/ that
-        // prefix.  We segment the entries into a primary
-        // block and following floor blocks using the first
-        // label in the suffix to assign to floor blocks.
-
-        // TODO: we could store min & max suffix start byte
-        // in each block, to make floor blocks authoritative
-
-        /*
-        if (DEBUG) {
-          final BytesRef prefix = new BytesRef(prefixLength);
-          for(int m=0;m<prefixLength;m++) {
-            prefix.bytes[m] = (byte) prevTerm.ints[m];
-          }
-          prefix.length = prefixLength;
-          //System.out.println("\nWBS count=" + count + " prefix=" + prefix.utf8ToString() + " " + prefix);
-          System.out.println("writeBlocks: prefix=" + toString(prefix) + " " + prefix + " count=" + count + " pending.size()=" + pending.size());
-        }
-        */
-        //System.out.println("\nwbs count=" + count);
-
-        final int savLabel = prevTerm.ints[prevTerm.offset + prefixLength];
-
-        // Count up how many items fall under
-        // each unique label after the prefix.
-        
-        // TODO: this is wasteful since the builder had
-        // already done this (partitioned these sub-terms
-        // according to their leading prefix byte)
-        
-        final List<PendingEntry> slice = pending.subList(pending.size()-count, pending.size());
-        int lastSuffixLeadLabel = -1;
-        int termCount = 0;
-        int subCount = 0;
-        int numSubs = 0;
-
-        for(PendingEntry ent : slice) {
-
-          // First byte in the suffix of this term
-          final int suffixLeadLabel;
-          if (ent.isTerm) {
-            PendingTerm term = (PendingTerm) ent;
-            if (term.term.length == prefixLength) {
-              // Suffix is 0, ie prefix 'foo' and term is
-              // 'foo' so the term has empty string suffix
-              // in this block
-              assert lastSuffixLeadLabel == -1;
-              assert numSubs == 0;
-              suffixLeadLabel = -1;
-            } else {
-              suffixLeadLabel = term.term.bytes[term.term.offset + prefixLength] & 0xff;
-            }
-          } else {
-            PendingBlock block = (PendingBlock) ent;
-            assert block.prefix.length > prefixLength;
-            suffixLeadLabel = block.prefix.bytes[block.prefix.offset + prefixLength] & 0xff;
-          }
-
-          if (suffixLeadLabel != lastSuffixLeadLabel && (termCount + subCount) != 0) {
-            if (subBytes.length == numSubs) {
-              subBytes = ArrayUtil.grow(subBytes);
-              subTermCounts = ArrayUtil.grow(subTermCounts);
-              subSubCounts = ArrayUtil.grow(subSubCounts);
-            }
-            subBytes[numSubs] = lastSuffixLeadLabel;
-            lastSuffixLeadLabel = suffixLeadLabel;
-            subTermCounts[numSubs] = termCount;
-            subSubCounts[numSubs] = subCount;
-            /*
-            if (suffixLeadLabel == -1) {
-              System.out.println("  sub " + -1 + " termCount=" + termCount + " subCount=" + subCount);
-            } else {
-              System.out.println("  sub " + Integer.toHexString(suffixLeadLabel) + " termCount=" + termCount + " subCount=" + subCount);
-            }
-            */
-            termCount = subCount = 0;
-            numSubs++;
-          }
-
-          if (ent.isTerm) {
-            termCount++;
-          } else {
-            subCount++;
-          }
-        }
-
-        if (subBytes.length == numSubs) {
-          subBytes = ArrayUtil.grow(subBytes);
-          subTermCounts = ArrayUtil.grow(subTermCounts);
-          subSubCounts = ArrayUtil.grow(subSubCounts);
-        }
-
-        subBytes[numSubs] = lastSuffixLeadLabel;
-        subTermCounts[numSubs] = termCount;
-        subSubCounts[numSubs] = subCount;
-        numSubs++;
-        /*
-        if (lastSuffixLeadLabel == -1) {
-          System.out.println("  sub " + -1 + " termCount=" + termCount + " subCount=" + subCount);
-        } else {
-          System.out.println("  sub " + Integer.toHexString(lastSuffixLeadLabel) + " termCount=" + termCount + " subCount=" + subCount);
-        }
-        */
-
-        if (subTermCountSums.length < numSubs) {
-          subTermCountSums = ArrayUtil.grow(subTermCountSums, numSubs);
-        }
-
-        // Roll up (backwards) the termCounts; postings impl
-        // needs this to know where to pull the term slice
-        // from its pending terms stack:
-        int sum = 0;
-        for(int idx=numSubs-1;idx>=0;idx--) {
-          sum += subTermCounts[idx];
-          subTermCountSums[idx] = sum;
-        }
-
-        // TODO: make a better segmenter?  It'd have to
-        // absorb the too-small end blocks backwards into
-        // the previous blocks
-
-        // Naive greedy segmentation; this is not always
-        // best (it can produce a too-small block as the
-        // last block):
-        int pendingCount = 0;
-        int startLabel = subBytes[0];
-        int curStart = count;
-        subCount = 0;
-
-        final List<PendingBlock> floorBlocks = new ArrayList<>();
-        PendingBlock firstBlock = null;
-
-        for(int sub=0;sub<numSubs;sub++) {
-          pendingCount += subTermCounts[sub] + subSubCounts[sub];
-          //System.out.println("  " + (subTermCounts[sub] + subSubCounts[sub]));
-          subCount++;
-
-          // Greedily make a floor block as soon as we've
-          // crossed the min count
-          if (pendingCount >= minItemsInBlock) {
-            final int curPrefixLength;
-            if (startLabel == -1) {
-              curPrefixLength = prefixLength;
-            } else {
-              curPrefixLength = 1+prefixLength;
-              // floor term:
-              prevTerm.ints[prevTerm.offset + prefixLength] = startLabel;
-            }
-            //System.out.println("  " + subCount + " subs");
-            final PendingBlock floorBlock = writeBlock(prevTerm, prefixLength, curPrefixLength, curStart, pendingCount, subTermCountSums[1+sub], true, startLabel, curStart == pendingCount);
-            if (firstBlock == null) {
-              firstBlock = floorBlock;
-            } else {
-              floorBlocks.add(floorBlock);
-            }
-            curStart -= pendingCount;
-            //System.out.println("    = " + pendingCount);
-            pendingCount = 0;
-
-            assert minItemsInBlock == 1 || subCount > 1: "minItemsInBlock=" + minItemsInBlock + " subCount=" + subCount + " sub=" + sub + " of " + numSubs + " subTermCount=" + subTermCountSums[sub] + " subSubCount=" + subSubCounts[sub] + " depth=" + prefixLength;
-            subCount = 0;
-            startLabel = subBytes[sub+1];
-
-            if (curStart == 0) {
-              break;
-            }
-
-            if (curStart <= maxItemsInBlock) {
-              // remainder is small enough to fit into a
-              // block.  NOTE that this may be too small (<
-              // minItemsInBlock); need a true segmenter
-              // here
-              assert startLabel != -1;
-              assert firstBlock != null;
-              prevTerm.ints[prevTerm.offset + prefixLength] = startLabel;
-              //System.out.println("  final " + (numSubs-sub-1) + " subs");
-              /*
-              for(sub++;sub < numSubs;sub++) {
-                System.out.println("  " + (subTermCounts[sub] + subSubCounts[sub]));
-              }
-              System.out.println("    = " + curStart);
-              if (curStart < minItemsInBlock) {
-                System.out.println("      **");
-              }
-              */
-              floorBlocks.add(writeBlock(prevTerm, prefixLength, prefixLength+1, curStart, curStart, 0, true, startLabel, true));
-              break;
-            }
-          }
-        }
-
-        prevTerm.ints[prevTerm.offset + prefixLength] = savLabel;
-
-        assert firstBlock != null;
-        firstBlock.compileIndex(floorBlocks, scratchBytes);
-
-        pending.add(firstBlock);
-        //if (DEBUG) System.out.println("  done pending.size()=" + pending.size());
-      }
-      lastBlockIndex = pending.size()-1;
-    }
-
-    // for debugging
-    @SuppressWarnings("unused")
-    private String toString(BytesRef b) {
-      try {
-        return b.utf8ToString() + " " + b;
-      } catch (Throwable t) {
-        // If BytesRef isn't actually UTF8, or it's eg a
-        // prefix of UTF8 that ends mid-unicode-char, we
-        // fallback to hex:
-        return b.toString();
-      }
-    }
-
-    // Writes all entries in the pending slice as a single
-    // block: 
-    private PendingBlock writeBlock(IntsRef prevTerm, int prefixLength, int indexPrefixLength, int startBackwards, int length,
-                                    int futureTermCount, boolean isFloor, int floorLeadByte, boolean isLastInFloor) throws IOException {
-
-      assert length > 0;
-
-      final int start = pending.size()-startBackwards;
-
-      assert start >= 0: "pending.size()=" + pending.size() + " startBackwards=" + startBackwards + " length=" + length;
-
-      final List<PendingEntry> slice = pending.subList(start, start + length);
-
-      final long startFP = out.getFilePointer();
-
-      final BytesRef prefix = new BytesRef(indexPrefixLength);
-      for(int m=0;m<indexPrefixLength;m++) {
-        prefix.bytes[m] = (byte) prevTerm.ints[m];
-      }
-      prefix.length = indexPrefixLength;
-
-      // Write block header:
-      out.writeVInt((length<<1)|(isLastInFloor ? 1:0));
-
-      // if (DEBUG) {
-      //  System.out.println("  writeBlock " + (isFloor ? "(floor) " : "") + "seg=" + segment + " pending.size()=" + pending.size() + " prefixLength=" + prefixLength + " indexPrefix=" + toString(prefix) + " entCount=" + length + " startFP=" + startFP + " futureTermCount=" + futureTermCount + (isFloor ? (" floorLeadByte=" + Integer.toHexString(floorLeadByte&0xff)) : "") + " isLastInFloor=" + isLastInFloor);
-      // }
-
-      // 1st pass: pack term suffix bytes into byte[] blob
-      // TODO: cutover to bulk int codec... simple64?
-
-      final boolean isLeafBlock;
-      if (lastBlockIndex < start) {
-        // This block definitely does not contain sub-blocks:
-        isLeafBlock = true;
-        //System.out.println("no scan true isFloor=" + isFloor);
-      } else if (!isFloor) {
-        // This block definitely does contain at least one sub-block:
-        isLeafBlock = false;
-        //System.out.println("no scan false " + lastBlockIndex + " vs start=" + start + " len=" + length);
-      } else {
-        // Must scan up-front to see if there is a sub-block
-        boolean v = true;
-        //System.out.println("scan " + lastBlockIndex + " vs start=" + start + " len=" + length);
-        for (PendingEntry ent : slice) {
-          if (!ent.isTerm) {
-            v = false;
-            break;
-          }
-        }
-        isLeafBlock = v;
-      }
-
-      final List<FST<Pair<BytesRef,Long>>> subIndices;
-
-      int termCount;
-
-      long[] longs = new long[longsSize];
-      boolean absolute = true;
-      long maxVersionInBlock = -1;
-
-      // int countx = 0;
-      if (isLeafBlock) {
-        subIndices = null;
-        for (PendingEntry ent : slice) {
-          assert ent.isTerm;
-          PendingTerm term = (PendingTerm) ent;
-          BlockTermState state = term.state;
-          maxVersionInBlock = Math.max(maxVersionInBlock, ((IDVersionTermState) state).idVersion);
-          final int suffix = term.term.length - prefixLength;
-          // if (DEBUG) {
-          //    BytesRef suffixBytes = new BytesRef(suffix);
-          //    System.arraycopy(term.term.bytes, prefixLength, suffixBytes.bytes, 0, suffix);
-          //    suffixBytes.length = suffix;
-          //    System.out.println("    " + (countx++) + ": write term suffix=" + toString(suffixBytes));
-          // }
-          // For leaf block we write suffix straight
-          suffixWriter.writeVInt(suffix);
-          suffixWriter.writeBytes(term.term.bytes, prefixLength, suffix);
-
-          // Write term meta data
-          postingsWriter.encodeTerm(longs, bytesWriter, fieldInfo, state, absolute);
-          for (int pos = 0; pos < longsSize; pos++) {
-            assert longs[pos] >= 0;
-            metaWriter.writeVLong(longs[pos]);
-          }
-          bytesWriter.writeTo(metaWriter);
-          bytesWriter.reset();
-          absolute = false;
-        }
-        termCount = length;
-      } else {
-        subIndices = new ArrayList<>();
-        termCount = 0;
-        for (PendingEntry ent : slice) {
-          if (ent.isTerm) {
-            PendingTerm term = (PendingTerm) ent;
-            BlockTermState state = term.state;
-            maxVersionInBlock = Math.max(maxVersionInBlock, ((IDVersionTermState) state).idVersion);
-            final int suffix = term.term.length - prefixLength;
-            // if (DEBUG) {
-            //    BytesRef suffixBytes = new BytesRef(suffix);
-            //    System.arraycopy(term.term.bytes, prefixLength, suffixBytes.bytes, 0, suffix);
-            //    suffixBytes.length = suffix;
-            //    System.out.println("    " + (countx++) + ": write term suffix=" + toString(suffixBytes));
-            // }
-            // For non-leaf block we borrow 1 bit to record
-            // if entry is term or sub-block
-            suffixWriter.writeVInt(suffix<<1);
-            suffixWriter.writeBytes(term.term.bytes, prefixLength, suffix);
-
-            // TODO: now that terms dict "sees" these longs,
-            // we can explore better column-stride encodings
-            // to encode all long[0]s for this block at
-            // once, all long[1]s, etc., e.g. using
-            // Simple64.  Alternatively, we could interleave
-            // stats + meta ... no reason to have them
-            // separate anymore:
-
-            // Write term meta data
-            postingsWriter.encodeTerm(longs, bytesWriter, fieldInfo, state, absolute);
-            for (int pos = 0; pos < longsSize; pos++) {
-              assert longs[pos] >= 0;
-              metaWriter.writeVLong(longs[pos]);
-            }
-            bytesWriter.writeTo(metaWriter);
-            bytesWriter.reset();
-            absolute = false;
-
-            termCount++;
-          } else {
-            PendingBlock block = (PendingBlock) ent;
-            maxVersionInBlock = Math.max(maxVersionInBlock, block.maxVersion);
-            final int suffix = block.prefix.length - prefixLength;
-
-            assert suffix > 0;
-
-            // For non-leaf block we borrow 1 bit to record
-            // if entry is term or sub-block
-            suffixWriter.writeVInt((suffix<<1)|1);
-            suffixWriter.writeBytes(block.prefix.bytes, prefixLength, suffix);
-            assert block.fp < startFP;
-
-            // if (DEBUG) {
-            //    BytesRef suffixBytes = new BytesRef(suffix);
-            //    System.arraycopy(block.prefix.bytes, prefixLength, suffixBytes.bytes, 0, suffix);
-            //    suffixBytes.length = suffix;
-            //    System.out.println("    " + (countx++) + ": write sub-block suffix=" + toString(suffixBytes) + " subFP=" + block.fp + " subCode=" + (startFP-block.fp) + " floor=" + block.isFloor);
-            // }
-
-            suffixWriter.writeVLong(startFP - block.fp);
-            subIndices.add(block.index);
-          }
-        }
-
-        assert subIndices.size() != 0;
-      }
-
-      // TODO: we could block-write the term suffix pointers;
-      // this would take more space but would enable binary
-      // search on lookup
-
-      // Write suffixes byte[] blob to terms dict output:
-      out.writeVInt((int) (suffixWriter.getFilePointer() << 1) | (isLeafBlock ? 1:0));
-      suffixWriter.writeTo(out);
-      suffixWriter.reset();
-
-      // Write term meta data byte[] blob
-      out.writeVInt((int) metaWriter.getFilePointer());
-      metaWriter.writeTo(out);
-      metaWriter.reset();
-
-      // Remove slice replaced by block:
-      slice.clear();
-
-      if (lastBlockIndex >= start) {
-        if (lastBlockIndex < start+length) {
-          lastBlockIndex = start;
-        } else {
-          lastBlockIndex -= length;
-        }
-      }
-
-      // if (DEBUG) {
-      //   System.out.println("      fpEnd=" + out.getFilePointer());
-      // }
-
-      return new PendingBlock(prefix, maxVersionInBlock, startFP, termCount != 0, isFloor, floorLeadByte, subIndices);
-    }
-
-    TermsWriter(FieldInfo fieldInfo) {
-      this.fieldInfo = fieldInfo;
-      docsSeen = new FixedBitSet(maxDoc);
-
-      noOutputs = NoOutputs.getSingleton();
-
-      // This Builder is just used transiently to fragment
-      // terms into "good" blocks; we don't save the
-      // resulting FST:
-      blockBuilder = new Builder<>(FST.INPUT_TYPE.BYTE1,
-                                   0, 0, true,
-                                   true, Integer.MAX_VALUE,
-                                   noOutputs,
-                                   new FindBlocks(), false,
-                                   PackedInts.COMPACT,
-                                   true, 15);
-
-      this.longsSize = postingsWriter.setField(fieldInfo);
-    }
-    
-    private final IntsRef scratchIntsRef = new IntsRef();
-
-    /** Writes one term's worth of postings. */
-    public void write(BytesRef text, TermsEnum termsEnum) throws IOException {
-
-      BlockTermState state = postingsWriter.writeTerm(text, termsEnum, docsSeen);
-      // TODO: LUCENE-5693: we don't need this check if we fix IW to not send deleted docs to us on flush:
-      if (state != null && ((IDVersionPostingsWriter) postingsWriter).lastDocID != -1) {
-        assert state.docFreq != 0;
-        assert fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY || state.totalTermFreq >= state.docFreq: "postingsWriter=" + postingsWriter;
-        blockBuilder.add(Util.toIntsRef(text, scratchIntsRef), noOutputs.getNoOutput());
-
-        PendingTerm term = new PendingTerm(BytesRef.deepCopyOf(text), state);
-        pending.add(term);
-        numTerms++;
-      }
-    }
-
-    // Finishes all terms in this field
-    public void finish(BytesRef minTerm, BytesRef maxTerm) throws IOException {
-      if (numTerms > 0) {
-        blockBuilder.finish();
-
-        // We better have one final "root" block:
-        assert pending.size() == 1 && !pending.get(0).isTerm: "pending.size()=" + pending.size() + " pending=" + pending;
-        final PendingBlock root = (PendingBlock) pending.get(0);
-        assert root.prefix.length == 0;
-        assert root.index.getEmptyOutput() != null;
-
-        // Write FST to index
-        indexStartFP = indexOut.getFilePointer();
-        root.index.save(indexOut);
-        //System.out.println("  write FST " + indexStartFP + " field=" + fieldInfo.name);
-
-        // if (SAVE_DOT_FILES || DEBUG) {
-        //   final String dotFileName = segment + "_" + fieldInfo.name + ".dot";
-        //   Writer w = new OutputStreamWriter(new FileOutputStream(dotFileName));
-        //   Util.toDot(root.index, w, false, false);
-        //   System.out.println("SAVED to " + dotFileName);
-        //   w.close();
-        // }
-
-        fields.add(new FieldMetaData(fieldInfo,
-                                     ((PendingBlock) pending.get(0)).index.getEmptyOutput(),
-                                     numTerms,
-                                     indexStartFP,
-                                     longsSize,
-                                     minTerm, maxTerm));
-      } else {
-        // cannot assert this: we skip deleted docIDs in the postings:
-        // assert docsSeen.cardinality() == 0;
-      }
-    }
-
-    private final RAMOutputStream suffixWriter = new RAMOutputStream();
-    private final RAMOutputStream metaWriter = new RAMOutputStream();
-    private final RAMOutputStream bytesWriter = new RAMOutputStream();
-  }
-
-  @Override
-  public void close() throws IOException {
-
-    boolean success = false;
-    try {
-      
-      final long dirStart = out.getFilePointer();
-      final long indexDirStart = indexOut.getFilePointer();
-
-      out.writeVInt(fields.size());
-      
-      for(FieldMetaData field : fields) {
-        //System.out.println("  field " + field.fieldInfo.name + " " + field.numTerms + " terms");
-        out.writeVInt(field.fieldInfo.number);
-        assert field.numTerms > 0;
-        out.writeVLong(field.numTerms);
-        out.writeVInt(field.rootCode.output1.length);
-        out.writeBytes(field.rootCode.output1.bytes, field.rootCode.output1.offset, field.rootCode.output1.length);
-        out.writeVLong(field.rootCode.output2);
-        out.writeVInt(field.longsSize);
-        indexOut.writeVLong(field.indexStartFP);
-        writeBytesRef(out, field.minTerm);
-        writeBytesRef(out, field.maxTerm);
-      }
-      writeTrailer(out, dirStart);
-      CodecUtil.writeFooter(out);
-      writeIndexTrailer(indexOut, indexDirStart);
-      CodecUtil.writeFooter(indexOut);
-      success = true;
-    } finally {
-      if (success) {
-        IOUtils.close(out, indexOut, postingsWriter);
-      } else {
-        IOUtils.closeWhileHandlingException(out, indexOut, postingsWriter);
-      }
-    }
-  }
-
-  private static void writeBytesRef(IndexOutput out, BytesRef bytes) throws IOException {
-    out.writeVInt(bytes.length);
-    out.writeBytes(bytes.bytes, bytes.offset, bytes.length);
-  }
-}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/idversion/VersionFieldReader.java b/lucene/codecs/src/java/org/apache/lucene/codecs/idversion/VersionFieldReader.java
deleted file mode 100644
index 417ce11..0000000
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/idversion/VersionFieldReader.java
+++ /dev/null
@@ -1,163 +0,0 @@
-package org.apache.lucene.codecs.idversion;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.Terms;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.store.ByteArrayDataInput;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.automaton.CompiledAutomaton;
-import org.apache.lucene.util.fst.ByteSequenceOutputs;
-import org.apache.lucene.util.fst.FST;
-import org.apache.lucene.util.fst.PairOutputs.Pair;
-
-/** BlockTree's implementation of {@link Terms}. */
-// public for CheckIndex:
-final class VersionFieldReader extends Terms {
-  final long numTerms;
-  final FieldInfo fieldInfo;
-  final long sumTotalTermFreq;
-  final long sumDocFreq;
-  final int docCount;
-  final long indexStartFP;
-  final long rootBlockFP;
-  final Pair<BytesRef,Long> rootCode;
-  final BytesRef minTerm;
-  final BytesRef maxTerm;
-  final int longsSize;
-  final VersionBlockTreeTermsReader parent;
-
-  final FST<Pair<BytesRef,Long>> index;
-  //private boolean DEBUG;
-
-  VersionFieldReader(VersionBlockTreeTermsReader parent, FieldInfo fieldInfo, long numTerms, Pair<BytesRef,Long> rootCode, long sumTotalTermFreq, long sumDocFreq, int docCount,
-              long indexStartFP, int longsSize, IndexInput indexIn, BytesRef minTerm, BytesRef maxTerm) throws IOException {
-    assert numTerms > 0;
-    this.fieldInfo = fieldInfo;
-    //DEBUG = BlockTreeTermsReader.DEBUG && fieldInfo.name.equals("id");
-    this.parent = parent;
-    this.numTerms = numTerms;
-    this.sumTotalTermFreq = sumTotalTermFreq; 
-    this.sumDocFreq = sumDocFreq; 
-    this.docCount = docCount;
-    this.indexStartFP = indexStartFP;
-    this.rootCode = rootCode;
-    this.longsSize = longsSize;
-    this.minTerm = minTerm;
-    this.maxTerm = maxTerm;
-    // if (DEBUG) {
-    //   System.out.println("BTTR: seg=" + segment + " field=" + fieldInfo.name + " rootBlockCode=" + rootCode + " divisor=" + indexDivisor);
-    // }
-
-    rootBlockFP = (new ByteArrayDataInput(rootCode.output1.bytes, rootCode.output1.offset, rootCode.output1.length)).readVLong() >>> VersionBlockTreeTermsWriter.OUTPUT_FLAGS_NUM_BITS;
-
-    if (indexIn != null) {
-      final IndexInput clone = indexIn.clone();
-      //System.out.println("start=" + indexStartFP + " field=" + fieldInfo.name);
-      clone.seek(indexStartFP);
-      index = new FST<>(clone, VersionBlockTreeTermsWriter.FST_OUTPUTS);
-        
-      /*
-        if (false) {
-        final String dotFileName = segment + "_" + fieldInfo.name + ".dot";
-        Writer w = new OutputStreamWriter(new FileOutputStream(dotFileName));
-        Util.toDot(index, w, false, false);
-        System.out.println("FST INDEX: SAVED to " + dotFileName);
-        w.close();
-        }
-      */
-    } else {
-      index = null;
-    }
-  }
-
-  @Override
-  public BytesRef getMin() throws IOException {
-    if (minTerm == null) {
-      // Older index that didn't store min/maxTerm
-      return super.getMin();
-    } else {
-      return minTerm;
-    }
-  }
-
-  @Override
-  public BytesRef getMax() throws IOException {
-    if (maxTerm == null) {
-      // Older index that didn't store min/maxTerm
-      return super.getMax();
-    } else {
-      return maxTerm;
-    }
-  }
-
-  @Override
-  public boolean hasFreqs() {
-    return fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;
-  }
-
-  @Override
-  public boolean hasOffsets() {
-    return fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
-  }
-
-  @Override
-  public boolean hasPositions() {
-    return fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;
-  }
-    
-  @Override
-  public boolean hasPayloads() {
-    return fieldInfo.hasPayloads();
-  }
-
-  @Override
-  public TermsEnum iterator(TermsEnum reuse) throws IOException {
-    return new IDVersionSegmentTermsEnum(this);
-  }
-
-  @Override
-  public long size() {
-    return numTerms;
-  }
-
-  @Override
-  public long getSumTotalTermFreq() {
-    return sumTotalTermFreq;
-  }
-
-  @Override
-  public long getSumDocFreq() {
-    return sumDocFreq;
-  }
-
-  @Override
-  public int getDocCount() {
-    return docCount;
-  }
-
-  /** Returns approximate RAM bytes used */
-  public long ramBytesUsed() {
-    return ((index!=null)? index.sizeInBytes() : 0);
-  }
-}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/idversion/package.html b/lucene/codecs/src/java/org/apache/lucene/codecs/idversion/package.html
deleted file mode 100644
index 6764dbe..0000000
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/idversion/package.html
+++ /dev/null
@@ -1,25 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
-</head>
-<body>
-A primary-key postings format that associations a monotonically increasing version with each term.
-</body>
-</html>
diff --git a/lucene/codecs/src/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat b/lucene/codecs/src/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat
index e4d1935..40a0c35 100644
--- a/lucene/codecs/src/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat
+++ b/lucene/codecs/src/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat
@@ -22,5 +22,3 @@ org.apache.lucene.codecs.memory.FSTPulsing41PostingsFormat
 org.apache.lucene.codecs.memory.FSTOrdPulsing41PostingsFormat
 org.apache.lucene.codecs.memory.FSTPostingsFormat
 org.apache.lucene.codecs.memory.FSTOrdPostingsFormat
-
-org.apache.lucene.codecs.idversion.IDVersionPostingsFormat
diff --git a/lucene/codecs/src/test/org/apache/lucene/codecs/idversion/StringAndPayloadField.java b/lucene/codecs/src/test/org/apache/lucene/codecs/idversion/StringAndPayloadField.java
deleted file mode 100644
index 19ff31a..0000000
--- a/lucene/codecs/src/test/org/apache/lucene/codecs/idversion/StringAndPayloadField.java
+++ /dev/null
@@ -1,104 +0,0 @@
-package org.apache.lucene.codecs.idversion;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.FieldType;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.util.BytesRef;
-
-// TODO: can we take a BytesRef token instead?
-
-/** Produces a single String token from the provided value, with the provided payload. */
-class StringAndPayloadField extends Field {
-
-  public static final FieldType TYPE = new FieldType();
-
-  static {
-    TYPE.setIndexed(true);
-    TYPE.setOmitNorms(true);
-    TYPE.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);
-    TYPE.setTokenized(true);
-    TYPE.freeze();
-  }
-
-  private final BytesRef payload;
-
-  public StringAndPayloadField(String name, String value, BytesRef payload) {
-    super(name, value, TYPE);
-    this.payload = payload;
-  }
-
-  @Override
-  public TokenStream tokenStream(Analyzer analyzer, TokenStream reuse) throws IOException {
-    SingleTokenWithPayloadTokenStream ts;
-    if (reuse instanceof SingleTokenWithPayloadTokenStream) {
-      ts = (SingleTokenWithPayloadTokenStream) reuse;
-    } else {
-      ts = new SingleTokenWithPayloadTokenStream();
-    }
-    ts.setValue((String) fieldsData, payload);
-    return ts;
-  }
-
-  static final class SingleTokenWithPayloadTokenStream extends TokenStream {
-
-    private final CharTermAttribute termAttribute = addAttribute(CharTermAttribute.class);
-    private final PayloadAttribute payloadAttribute = addAttribute(PayloadAttribute.class);
-    private boolean used = false;
-    private String value = null;
-    private BytesRef payload;
-    
-    /** Sets the string value. */
-    void setValue(String value, BytesRef payload) {
-      this.value = value;
-      this.payload = payload;
-    }
-
-    @Override
-    public boolean incrementToken() {
-      if (used) {
-        return false;
-      }
-      clearAttributes();
-      termAttribute.append(value);
-      payloadAttribute.setPayload(payload);
-      used = true;
-      return true;
-    }
-
-    @Override
-    public void reset() {
-      used = false;
-    }
-
-    @Override
-    public void close() {
-      value = null;
-      payload = null;
-    }
-  }
-}
-
-
diff --git a/lucene/codecs/src/test/org/apache/lucene/codecs/idversion/TestIDVersionPostingsFormat.java b/lucene/codecs/src/test/org/apache/lucene/codecs/idversion/TestIDVersionPostingsFormat.java
deleted file mode 100644
index 1f8f7d2..0000000
--- a/lucene/codecs/src/test/org/apache/lucene/codecs/idversion/TestIDVersionPostingsFormat.java
+++ /dev/null
@@ -1,797 +0,0 @@
-package org.apache.lucene.codecs.idversion;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Locale;
-import java.util.Map;
-import java.util.Set;
-import java.util.concurrent.ConcurrentHashMap;
-import java.util.concurrent.CountDownLatch;
-import java.util.concurrent.atomic.AtomicLong;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.codecs.idversion.StringAndPayloadField.SingleTokenWithPayloadTokenStream;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.FieldType;
-import org.apache.lucene.index.ConcurrentMergeScheduler;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.MergeScheduler;
-import org.apache.lucene.index.PerThreadPKLookup;
-import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.index.TieredMergePolicy;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.LiveFieldValues;
-import org.apache.lucene.search.SearcherFactory;
-import org.apache.lucene.search.SearcherManager;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util.TestUtil;
-
-/**
- * Basic tests for IDVersionPostingsFormat
- */
-// Cannot extend BasePostingsFormatTestCase because this PF is not
-// general (it requires payloads, only allows 1 doc per term, etc.)
-public class TestIDVersionPostingsFormat extends LuceneTestCase {
-
-  public void testBasic() throws Exception {
-    Directory dir = newDirectory();
-    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
-    iwc.setCodec(TestUtil.alwaysPostingsFormat(new IDVersionPostingsFormat()));
-    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
-    Document doc = new Document();
-    doc.add(makeIDField("id0", 100));
-    w.addDocument(doc);
-    doc = new Document();
-    doc.add(makeIDField("id1", 110));
-    w.addDocument(doc);
-    IndexReader r = w.getReader();
-    IDVersionSegmentTermsEnum termsEnum = (IDVersionSegmentTermsEnum) r.leaves().get(0).reader().fields().terms("id").iterator(null);
-    assertTrue(termsEnum.seekExact(new BytesRef("id0"), 50));
-    assertTrue(termsEnum.seekExact(new BytesRef("id0"), 100));
-    assertFalse(termsEnum.seekExact(new BytesRef("id0"), 101));
-    assertTrue(termsEnum.seekExact(new BytesRef("id1"), 50));
-    assertTrue(termsEnum.seekExact(new BytesRef("id1"), 110));
-    assertFalse(termsEnum.seekExact(new BytesRef("id1"), 111));
-    r.close();
-
-    w.close();
-    dir.close();
-  }
-
-  private interface IDSource {
-    String next();
-  }
-
-  private IDSource getRandomIDs() {
-    IDSource ids;
-    switch (random().nextInt(6)) {
-    case 0:
-      // random simple
-      if (VERBOSE) {
-        System.out.println("TEST: use random simple ids");
-      }
-      ids = new IDSource() {
-          @Override
-          public String next() {
-            return TestUtil.randomSimpleString(random());
-          }
-        };
-      break;
-    case 1:
-      // random realistic unicode
-      if (VERBOSE) {
-        System.out.println("TEST: use random realistic unicode ids");
-      }
-      ids = new IDSource() {
-          @Override
-          public String next() {
-            return TestUtil.randomRealisticUnicodeString(random());
-          }
-        };
-      break;
-    case 2:
-      // sequential
-      if (VERBOSE) {
-        System.out.println("TEST: use seuquential ids");
-      }
-      ids = new IDSource() {
-          int upto;
-          @Override
-          public String next() {
-            return Integer.toString(upto++);
-          }
-        };
-      break;
-    case 3:
-      // zero-pad sequential
-      if (VERBOSE) {
-        System.out.println("TEST: use zero-pad seuquential ids");
-      }
-      ids = new IDSource() {
-          final int radix = TestUtil.nextInt(random(), Character.MIN_RADIX, Character.MAX_RADIX);
-          final String zeroPad = String.format(Locale.ROOT, "%0" + TestUtil.nextInt(random(), 4, 20) + "d", 0);
-          int upto;
-          @Override
-          public String next() {
-            String s = Integer.toString(upto++);
-            return zeroPad.substring(zeroPad.length() - s.length()) + s;
-          }
-        };
-      break;
-    case 4:
-      // random long
-      if (VERBOSE) {
-        System.out.println("TEST: use random long ids");
-      }
-      ids = new IDSource() {
-          final int radix = TestUtil.nextInt(random(), Character.MIN_RADIX, Character.MAX_RADIX);
-          int upto;
-          @Override
-          public String next() {
-            return Long.toString(random().nextLong() & 0x7ffffffffffffffL, radix);
-          }
-        };
-      break;
-    case 5:
-      // zero-pad random long
-      if (VERBOSE) {
-        System.out.println("TEST: use zero-pad random long ids");
-      }
-      ids = new IDSource() {
-          final int radix = TestUtil.nextInt(random(), Character.MIN_RADIX, Character.MAX_RADIX);
-          final String zeroPad = String.format(Locale.ROOT, "%015d", 0);
-          int upto;
-          @Override
-          public String next() {
-            return Long.toString(random().nextLong() & 0x7ffffffffffffffL, radix);
-          }
-        };
-      break;
-    default:
-      throw new AssertionError();
-    }
-
-    return ids;
-  }
-
-  // TODO make a similar test for BT, w/ varied IDs:
-
-  public void testRandom() throws Exception {
-    Directory dir = newDirectory();
-    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
-    int minItemsInBlock = TestUtil.nextInt(random(), 2, 50);
-    int maxItemsInBlock = 2*(minItemsInBlock-1) + random().nextInt(50);
-    iwc.setCodec(TestUtil.alwaysPostingsFormat(new IDVersionPostingsFormat(minItemsInBlock, maxItemsInBlock)));
-    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
-    //IndexWriter w = new IndexWriter(dir, iwc);
-    int numDocs = atLeast(1000);
-    Map<String,Long> idValues = new HashMap<String,Long>();
-    int docUpto = 0;
-    if (VERBOSE) {
-      System.out.println("TEST: numDocs=" + numDocs);
-    }
-
-    IDSource ids = getRandomIDs();
-    String idPrefix;
-    if (random().nextBoolean()) {
-      idPrefix = "";
-    } else {
-      idPrefix = TestUtil.randomSimpleString(random());
-      if (VERBOSE) {
-        System.out.println("TEST: use id prefix: " + idPrefix);
-      }
-    }
-
-    boolean useMonotonicVersion = random().nextBoolean();
-    if (VERBOSE) {
-      System.out.println("TEST: useMonotonicVersion=" + useMonotonicVersion);
-    }
-
-    List<String> idsList = new ArrayList<>();
-
-    long version = 0;
-    while (docUpto < numDocs) {
-      String idValue = idPrefix + ids.next();
-      if (idValues.containsKey(idValue)) {
-        continue;
-      }
-      if (useMonotonicVersion) {
-        version += TestUtil.nextInt(random(), 1, 10);
-      } else {
-        version = random().nextLong() & 0x7fffffffffffffffL;
-      }
-      idValues.put(idValue, version);
-      if (VERBOSE) {
-        System.out.println("  " + idValue + " -> " + version);
-      }
-      Document doc = new Document();
-      doc.add(makeIDField(idValue, version));
-      w.addDocument(doc);
-      idsList.add(idValue);
-
-      if (idsList.size() > 0 && random().nextInt(7) == 5) {
-        // Randomly delete or update a previous ID
-        idValue = idsList.get(random().nextInt(idsList.size()));
-        if (random().nextBoolean()) {
-          if (useMonotonicVersion) {
-            version += TestUtil.nextInt(random(), 1, 10);
-          } else {
-            version = random().nextLong() & 0x7fffffffffffffffL;
-          }
-          doc = new Document();
-          doc.add(makeIDField(idValue, version));
-          if (VERBOSE) {
-            System.out.println("  update " + idValue + " -> " + version);
-          }
-          w.updateDocument(new Term("id", idValue), doc);
-          idValues.put(idValue, version);
-        } else {
-          if (VERBOSE) {
-            System.out.println("  delete " + idValue);
-          }
-          w.deleteDocuments(new Term("id", idValue));
-          idValues.remove(idValue);
-        }        
-      }
-
-      docUpto++;
-    }
-
-    IndexReader r = w.getReader();
-    //IndexReader r = DirectoryReader.open(w, true);
-    PerThreadVersionPKLookup lookup = new PerThreadVersionPKLookup(r, "id");
-
-    List<Map.Entry<String,Long>> idValuesList = new ArrayList<>(idValues.entrySet());
-    int iters = numDocs * 5;
-    for(int iter=0;iter<iters;iter++) {
-      String idValue;
-
-      if (random().nextBoolean()) {
-        idValue = idValuesList.get(random().nextInt(idValuesList.size())).getKey();
-      } else if (random().nextBoolean()) {
-        idValue = ids.next();
-      } else {
-        idValue = idPrefix + TestUtil.randomSimpleString(random());
-      }
-
-      BytesRef idValueBytes = new BytesRef(idValue);
-
-      Long expectedVersion = idValues.get(idValue);
-
-      if (VERBOSE) {
-        System.out.println("\nTEST: iter=" + iter + " id=" + idValue + " expectedVersion=" + expectedVersion);
-      }
-      
-      if (expectedVersion == null) {
-        assertEquals("term should not have been found (doesn't exist)", -1, lookup.lookup(idValueBytes));
-      } else {
-        if (random().nextBoolean()) {
-          if (VERBOSE) {
-            System.out.println("  lookup exact version (should be found)");
-          }
-          assertTrue("term should have been found (version too old)", lookup.lookup(idValueBytes, expectedVersion.longValue()) != -1);
-          assertEquals(expectedVersion.longValue(), lookup.getVersion());
-        } else {
-          if (VERBOSE) {
-            System.out.println("  lookup version+1 (should not be found)");
-          }
-          assertEquals("term should not have been found (version newer)", -1, lookup.lookup(idValueBytes, expectedVersion.longValue()+1));
-        }
-      }
-    }
-
-    r.close();
-    w.close();
-    dir.close();
-  }
-
-  private static class PerThreadVersionPKLookup extends PerThreadPKLookup {
-    public PerThreadVersionPKLookup(IndexReader r, String field) throws IOException {
-      super(r, field);
-    }
-
-    long lastVersion;
-
-    /** Returns docID if found, else -1. */
-    public int lookup(BytesRef id, long version) throws IOException {
-      for(int seg=0;seg<numSegs;seg++) {
-        if (((IDVersionSegmentTermsEnum) termsEnums[seg]).seekExact(id, version)) {
-          if (VERBOSE) {
-            System.out.println("  found in seg=" + termsEnums[seg]);
-          }
-          docsEnums[seg] = termsEnums[seg].docs(liveDocs[seg], docsEnums[seg], 0);
-          int docID = docsEnums[seg].nextDoc();
-          if (docID != DocsEnum.NO_MORE_DOCS) {
-            lastVersion = ((IDVersionSegmentTermsEnum) termsEnums[seg]).getVersion();
-            return docBases[seg] + docID;
-          }
-          assert hasDeletions;
-        }
-      }
-
-      return -1;
-    }
-
-    /** Only valid if lookup returned a valid docID. */
-    public long getVersion() {
-      return lastVersion;
-    }
-  }
-
-  private static Field makeIDField(String id, long version) {
-    BytesRef payload = new BytesRef(8);
-    payload.length = 8;
-    IDVersionPostingsFormat.longToBytes(version, payload);
-    return new StringAndPayloadField("id", id, payload);
-  }
-
-  public void testMoreThanOneDocPerIDOneSegment() throws Exception {
-    Directory dir = newDirectory();
-    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
-    iwc.setCodec(TestUtil.alwaysPostingsFormat(new IDVersionPostingsFormat()));
-    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
-    Document doc = new Document();
-    doc.add(makeIDField("id", 17));
-    w.addDocument(doc);
-    doc = new Document();
-    doc.add(makeIDField("id", 17));
-    w.addDocument(doc);
-    try {
-      w.commit();
-      fail("didn't hit expected exception");
-    } catch (IllegalArgumentException iae) {
-      // expected
-    }
-    w.close();
-    dir.close();
-  }
-
-  public void testMoreThanOneDocPerIDTwoSegments() throws Exception {
-    Directory dir = newDirectory();
-    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
-    iwc.setCodec(TestUtil.alwaysPostingsFormat(new IDVersionPostingsFormat()));
-    iwc.setMergePolicy(new TieredMergePolicy());
-    MergeScheduler ms = iwc.getMergeScheduler();
-    if (ms instanceof ConcurrentMergeScheduler) {
-      iwc.setMergeScheduler(new ConcurrentMergeScheduler() {
-          @Override
-          protected void handleMergeException(Throwable exc) {
-            assertTrue(exc instanceof IllegalArgumentException);
-          }
-        });
-    }
-    IndexWriter w = new IndexWriter(dir, iwc);
-    Document doc = new Document();
-    doc.add(makeIDField("id", 17));
-    w.addDocument(doc);
-    w.commit();
-    doc = new Document();
-    doc.add(makeIDField("id", 17));
-    try {
-      w.addDocument(doc);
-      w.commit();
-      w.forceMerge(1);
-      fail("didn't hit exception");
-    } catch (IllegalArgumentException iae) {
-      // expected: SMS will hit this
-    } catch (IOException ioe) {
-      // expected
-      assertTrue(ioe.getCause() instanceof IllegalArgumentException);
-    }
-    w.close();
-    dir.close();
-  }
-
-  public void testMoreThanOneDocPerIDWithUpdates() throws Exception {
-    Directory dir = newDirectory();
-    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
-    iwc.setCodec(TestUtil.alwaysPostingsFormat(new IDVersionPostingsFormat()));
-    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
-    Document doc = new Document();
-    doc.add(makeIDField("id", 17));
-    w.addDocument(doc);
-    doc = new Document();
-    doc.add(makeIDField("id", 17));
-    // Replaces the doc we just indexed:
-    w.updateDocument(new Term("id", "id"), doc);
-    w.commit();
-    w.close();
-    dir.close();
-  }
-
-  public void testMoreThanOneDocPerIDWithDeletes() throws Exception {
-    Directory dir = newDirectory();
-    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
-    iwc.setCodec(TestUtil.alwaysPostingsFormat(new IDVersionPostingsFormat()));
-    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
-    Document doc = new Document();
-    doc.add(makeIDField("id", 17));
-    w.addDocument(doc);
-    w.deleteDocuments(new Term("id", "id"));
-    doc = new Document();
-    doc.add(makeIDField("id", 17));
-    w.addDocument(doc);
-    w.commit();
-    w.close();
-    dir.close();
-  }
-
-  public void testMissingPayload() throws Exception {
-    Directory dir = newDirectory();
-    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
-    iwc.setCodec(TestUtil.alwaysPostingsFormat(new IDVersionPostingsFormat()));
-    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
-    Document doc = new Document();
-    doc.add(newTextField("id", "id", Field.Store.NO));
-    try {
-      w.addDocument(doc);
-      w.commit();
-      fail("didn't hit expected exception");
-    } catch (IllegalArgumentException iae) {
-      // expected
-    }
-             
-    w.close();
-    dir.close();
-  }
-
-  public void testMissingPositions() throws Exception {
-    Directory dir = newDirectory();
-    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
-    iwc.setCodec(TestUtil.alwaysPostingsFormat(new IDVersionPostingsFormat()));
-    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
-    Document doc = new Document();
-    doc.add(newStringField("id", "id", Field.Store.NO));
-    try {
-      w.addDocument(doc);
-      w.commit();
-      fail("didn't hit expected exception");
-    } catch (IllegalArgumentException iae) {
-      // expected
-    }
-             
-    w.close();
-    dir.close();
-  }
-
-  public void testInvalidPayload() throws Exception {
-    Directory dir = newDirectory();
-    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
-    iwc.setCodec(TestUtil.alwaysPostingsFormat(new IDVersionPostingsFormat()));
-    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
-    Document doc = new Document();
-    doc.add(new StringAndPayloadField("id", "id", new BytesRef("foo")));
-    try {
-      w.addDocument(doc);
-      w.commit();
-      fail("didn't hit expected exception");
-    } catch (IllegalArgumentException iae) {
-      // expected
-    }
-             
-    w.close();
-    dir.close();
-  }
-
-  public void testMoreThanOneDocPerIDWithDeletesAcrossSegments() throws IOException {
-    Directory dir = newDirectory();
-    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
-    iwc.setCodec(TestUtil.alwaysPostingsFormat(new IDVersionPostingsFormat()));
-    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
-    Document doc = new Document();
-    doc.add(makeIDField("id", 17));
-    w.addDocument(doc);
-    w.commit();
-    doc = new Document();
-    doc.add(makeIDField("id", 17));
-    // Replaces the doc we just indexed:
-    w.updateDocument(new Term("id", "id"), doc);
-    w.forceMerge(1);
-    w.close();
-    dir.close();
-  }
-
-  // LUCENE-5693: because CheckIndex cross-checks term vectors with postings even for deleted docs, and because our PF only indexes the
-  // non-deleted documents on flush, CheckIndex will see this as corruption:
-  public void testCannotIndexTermVectors() throws Exception {
-    Directory dir = newDirectory();
-    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
-    iwc.setCodec(TestUtil.alwaysPostingsFormat(new IDVersionPostingsFormat()));
-    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
-    Document doc = new Document();
-
-    FieldType ft = new FieldType(StringAndPayloadField.TYPE);
-    ft.setStoreTermVectors(true);
-    SingleTokenWithPayloadTokenStream ts = new SingleTokenWithPayloadTokenStream();
-    BytesRef payload = new BytesRef(8);
-    payload.length = 8;
-    IDVersionPostingsFormat.longToBytes(17, payload);
-    ts.setValue("foo", payload);
-    Field field = new Field("id", ts, ft);
-    doc.add(new Field("id", ts, ft));
-    try {
-      w.addDocument(doc);
-      w.commit();
-      fail("didn't hit expected exception");
-    } catch (IllegalArgumentException iae) {
-      // expected
-      // iae.printStackTrace(System.out);
-    }
-    w.close();
-    dir.close();
-  }
-
-  public void testMoreThanOnceInSingleDoc() throws IOException {
-    Directory dir = newDirectory();
-    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
-    iwc.setCodec(TestUtil.alwaysPostingsFormat(new IDVersionPostingsFormat()));
-    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
-    Document doc = new Document();
-    doc.add(makeIDField("id", 17));
-    doc.add(makeIDField("id", 17));
-    try {
-      w.addDocument(doc);
-      w.commit();
-      fail("didn't hit expected exception");
-    } catch (IllegalArgumentException iae) {
-      // expected
-    }
-    w.close();
-    dir.close();
-  }
-
-  // Simulates optimistic concurrency in a distributed indexing app and confirms the latest version always wins:
-  public void testGlobalVersions() throws Exception {
-    Directory dir = newDirectory();
-    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
-    iwc.setCodec(TestUtil.alwaysPostingsFormat(new IDVersionPostingsFormat()));
-    final RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
-
-    IDSource idsSource = getRandomIDs();
-    int numIDs = atLeast(100);
-    System.out.println("ids=" + numIDs);
-    if (VERBOSE) {
-      System.out.println("TEST: " + numIDs + " ids");
-    }
-    Set<String> idsSeen = new HashSet<String>();
-    while (idsSeen.size() < numIDs) {
-      idsSeen.add(idsSource.next());
-    }
-    final String[] ids = idsSeen.toArray(new String[numIDs]);
-
-    final Object[] locks = new Object[ids.length];
-    for(int i=0;i<locks.length;i++) {
-      locks[i] = new Object();
-    }
-
-    final AtomicLong nextVersion = new AtomicLong();
-
-    final SearcherManager mgr = new SearcherManager(w.w, true, new SearcherFactory());
-
-    final Long missingValue = -1L;
-
-    final LiveFieldValues<IndexSearcher,Long> versionValues = new LiveFieldValues<IndexSearcher,Long>(mgr, missingValue) {
-      @Override
-      protected Long lookupFromSearcher(IndexSearcher s, String id) {
-        // TODO: would be cleaner if we could do our PerThreadLookup here instead of "up above":
-        // We always return missing: the caller then does a lookup against the current reader
-        return missingValue;
-      }
-    };
-
-    // Maps to the version the id was lasted indexed with:
-    final Map<String,Long> truth = new ConcurrentHashMap<>();
-
-    final CountDownLatch startingGun = new CountDownLatch(1);
-
-    Thread[] threads = new Thread[TestUtil.nextInt(random(), 2, 7)];
-
-    final int versionType = random().nextInt(3);
-
-    if (VERBOSE) {
-      if (versionType == 0) {
-        System.out.println("TEST: use random versions");
-      } else if (versionType == 1) {
-        System.out.println("TEST: use monotonic versions");
-      } else {
-        System.out.println("TEST: use nanotime versions");
-      }
-    }
-
-    // Run for 3 sec in normal tests, else 60 seconds for nightly:
-    final long stopTime = System.currentTimeMillis() + (TEST_NIGHTLY ? 60000 : 3000);
-
-    for(int i=0;i<threads.length;i++) {
-      threads[i] = new Thread() {
-          @Override
-          public void run() {
-            try {
-              runForReal();
-            } catch (Exception e) {
-              throw new RuntimeException(e);
-            }
-          }
-
-          private void runForReal() throws IOException, InterruptedException {
-            startingGun.await();
-            PerThreadVersionPKLookup lookup = null;
-            IndexReader lookupReader = null;
-            while (System.currentTimeMillis() < stopTime) {
-
-              // Intentionally pull version first, and then sleep/yield, to provoke version conflicts:
-              long newVersion;
-              if (versionType == 0) {
-                // Random:
-                newVersion = random().nextLong() & 0x7fffffffffffffffL;
-              } else if (versionType == 1) {
-                // Monotonic
-                newVersion = nextVersion.getAndIncrement();
-              } else {
-                newVersion = System.nanoTime();
-              }
-
-              if (versionType != 0) {
-                if (random().nextBoolean()) {
-                  Thread.yield();
-                } else {
-                  Thread.sleep(TestUtil.nextInt(random(), 1, 4));
-                }
-              }
-
-              int x = random().nextInt(ids.length);
-
-              // TODO: we could relax this, if e.g. we assign indexer thread based on ID.  This would ensure a given ID cannot be indexed at
-              // the same time in multiple threads:
-
-              // Only one thread can update an ID at once:
-              synchronized (locks[x]) {
-
-                String id = ids[x];
-
-                // We will attempt to index id with newVersion, but only do so if id wasn't yet indexed, or it was indexed with an older
-                // version (< newVersion):
-
-                // Must lookup the RT value before pulling from the index, in case a reopen happens just after we lookup:
-                Long currentVersion = versionValues.get(id);
-
-                IndexSearcher s = mgr.acquire();
-                try {
-                  if (VERBOSE) System.out.println("\n" + Thread.currentThread().getName() + ": update id=" + id + " newVersion=" + newVersion);
-
-                  if (lookup == null || lookupReader != s.getIndexReader()) {
-                    // TODO: sort of messy; we could add reopen to PerThreadVersionPKLookup?
-                    // TODO: this is thin ice .... that we don't incRef/decRef this reader we are implicitly holding onto:
-                    lookupReader = s.getIndexReader();
-                    if (VERBOSE) System.out.println(Thread.currentThread().getName() + ": open new PK lookup reader=" + lookupReader);
-                    lookup = new PerThreadVersionPKLookup(lookupReader, "id");
-                  }
-
-                  Long truthVersion = truth.get(id);
-                  if (VERBOSE) System.out.println(Thread.currentThread().getName() + ":   truthVersion=" + truthVersion);
-
-                  boolean doIndex;
-                  if (currentVersion == missingValue) {
-                    if (VERBOSE) System.out.println(Thread.currentThread().getName() + ":   id not in RT cache");
-                    int otherDocID = lookup.lookup(new BytesRef(id), newVersion+1);
-                    if (otherDocID == -1) {
-                      if (VERBOSE) System.out.println(Thread.currentThread().getName() + ":   id not in index, or version is <= newVersion; will index");
-                      doIndex = true;
-                    } else {
-                      if (VERBOSE) System.out.println(Thread.currentThread().getName() + ":   id is in index with version=" + lookup.getVersion() + "; will not index");
-                      doIndex = false;
-                      if (truthVersion.longValue() !=lookup.getVersion()) {
-                        System.out.println(Thread.currentThread() + ": now fail0!");
-                      }
-                      assertEquals(truthVersion.longValue(), lookup.getVersion());
-                    }
-                  } else {
-                    if (VERBOSE) System.out.println(Thread.currentThread().getName() + ":   id is in RT cache: currentVersion=" + currentVersion);
-                    doIndex = newVersion > currentVersion;
-                  }
-
-                  if (doIndex) {
-                    if (VERBOSE) System.out.println(Thread.currentThread().getName() + ":   now index");
-                    boolean passes = truthVersion == null || truthVersion.longValue() <= newVersion;
-                    if (passes == false) {
-                      System.out.println(Thread.currentThread() + ": now fail!");
-                    }
-                    assertTrue(passes);
-                    Document doc = new Document();
-                    doc.add(makeIDField(id, newVersion));
-                    w.updateDocument(new Term("id", id), doc);
-                    truth.put(id, newVersion);
-                    versionValues.add(id, newVersion);
-                  } else {
-                    if (VERBOSE) System.out.println(Thread.currentThread().getName() + ":   skip index");
-                    assertNotNull(truthVersion);
-                    assertTrue(truthVersion.longValue() >= newVersion);
-                  }
-                } finally {
-                  mgr.release(s);
-                }
-              }
-            }
-          }
-        };
-      threads[i].start();
-    }
-
-    startingGun.countDown();
-
-    // Keep reopening the NRT reader until all indexing threads are done:
-    refreshLoop:
-    while (true) {
-      Thread.sleep(TestUtil.nextInt(random(), 1, 10));
-      mgr.maybeRefresh();
-      for (Thread thread : threads) {
-        if (thread.isAlive()) {
-          continue refreshLoop;
-        }
-      }
-
-      break;
-    }
-
-    // Verify final index against truth:
-    for(int i=0;i<2;i++) {
-      mgr.maybeRefresh();
-      IndexSearcher s = mgr.acquire();
-      try {
-        IndexReader r = s.getIndexReader();
-        // cannot assert this: maybe not all IDs were indexed
-        /*
-        assertEquals(numIDs, r.numDocs());
-        if (i == 1) {
-          // After forceMerge no deleted docs:
-          assertEquals(numIDs, r.maxDoc());
-        }
-        */
-        PerThreadVersionPKLookup lookup = new PerThreadVersionPKLookup(r, "id");
-        for(Map.Entry<String,Long> ent : truth.entrySet()) {
-          assertTrue(lookup.lookup(new BytesRef(ent.getKey()), -1L) != -1);
-          assertEquals(ent.getValue().longValue(), lookup.getVersion());
-        }
-      } finally {
-        mgr.release(s);
-      }
-
-      if (i == 1) {
-        break;
-      }
-
-      // forceMerge and verify again
-      w.forceMerge(1);
-    }
-
-    mgr.close();
-    w.close();
-    dir.close();
-  }
-}
diff --git a/lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/IDVersionPostingsFormat.java b/lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/IDVersionPostingsFormat.java
new file mode 100644
index 0000000..ec54677
--- /dev/null
+++ b/lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/IDVersionPostingsFormat.java
@@ -0,0 +1,136 @@
+package org.apache.lucene.codecs.idversion;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.FieldsConsumer;
+import org.apache.lucene.codecs.FieldsProducer;
+import org.apache.lucene.codecs.PostingsFormat;
+import org.apache.lucene.codecs.PostingsReaderBase;
+import org.apache.lucene.codecs.PostingsWriterBase;
+import org.apache.lucene.codecs.blocktree.BlockTreeTermsWriter;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.search.LiveFieldValues;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+
+/** A PostingsFormat optimized for primary-key (ID) fields that also
+ *  record a version (long) for each ID, delivered as a payload
+ *  created by {@link #longToBytes} during indexing.  At search time,
+ *  the TermsEnum implementation {@link IDVersionSegmentTermsEnum}
+ *  enables fast (using only the terms index when possible) lookup for
+ *  whether a given ID was previously indexed with version > N (see
+ *  {@link IDVersionSegmentTermsEnum#seekExact(BytesRef,long)}.
+ *
+ *  <p>This is most effective if the app assigns monotonically
+ *  increasing global version to each indexed doc.  Then, during
+ *  indexing, use {@link
+ *  IDVersionSegmentTermsEnum#seekExact(BytesRef,long)} (along with
+ *  {@link LiveFieldValues}) to decide whether the document you are
+ *  about to index was already indexed with a higher version, and skip
+ *  it if so.
+ *
+ *  <p>The field is effectively indexed as DOCS_ONLY and the docID is
+ *  pulsed into the terms dictionary, but the user must feed in the
+ *  version as a payload on the first token.
+ *
+ *  <p>NOTE: term vectors cannot be indexed with this field (not that
+ *  you should really ever want to do this).
+ *
+ *  @lucene.experimental */
+
+public class IDVersionPostingsFormat extends PostingsFormat {
+
+  private final int minTermsInBlock;
+  private final int maxTermsInBlock;
+
+  public IDVersionPostingsFormat() {
+    this(BlockTreeTermsWriter.DEFAULT_MIN_BLOCK_SIZE, BlockTreeTermsWriter.DEFAULT_MAX_BLOCK_SIZE);
+  }
+
+  public IDVersionPostingsFormat(int minTermsInBlock, int maxTermsInBlock) {
+    super("IDVersion");
+    this.minTermsInBlock = minTermsInBlock;
+    this.maxTermsInBlock = maxTermsInBlock;
+  }
+
+  @Override
+  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
+    PostingsWriterBase postingsWriter = new IDVersionPostingsWriter(state);
+    boolean success = false;
+    try {
+      FieldsConsumer ret = new VersionBlockTreeTermsWriter(state, 
+                                                           postingsWriter,
+                                                           minTermsInBlock, 
+                                                           maxTermsInBlock);
+      success = true;
+      return ret;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(postingsWriter);
+      }
+    }
+  }
+
+  @Override
+  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
+    PostingsReaderBase postingsReader = new IDVersionPostingsReader();
+    boolean success = false;
+     try {
+       FieldsProducer ret = new VersionBlockTreeTermsReader(state.directory,
+                                                            state.fieldInfos,
+                                                            state.segmentInfo,
+                                                            postingsReader,
+                                                            state.context,
+                                                            state.segmentSuffix);
+       success = true;
+       return ret;
+     } finally {
+       if (!success) {
+         IOUtils.closeWhileHandlingException(postingsReader);
+       }
+     }
+  }
+
+  public static long bytesToLong(BytesRef bytes) {
+    return ((bytes.bytes[bytes.offset]&0xFFL) << 56) |
+      ((bytes.bytes[bytes.offset+1]&0xFFL) << 48) |
+      ((bytes.bytes[bytes.offset+2]&0xFFL) << 40) |
+      ((bytes.bytes[bytes.offset+3]&0xFFL) << 32) |
+      ((bytes.bytes[bytes.offset+4]&0xFFL) << 24) |
+      ((bytes.bytes[bytes.offset+5]&0xFFL) << 16) |
+      ((bytes.bytes[bytes.offset+6]&0xFFL) << 8) |
+      (bytes.bytes[bytes.offset+7]&0xFFL);
+  }
+
+  public static void longToBytes(long v, BytesRef bytes) {
+    bytes.offset = 0;
+    bytes.length = 8;
+    bytes.bytes[0] = (byte) (v >> 56);
+    bytes.bytes[1] = (byte) (v >> 48);
+    bytes.bytes[2] = (byte) (v >> 40);
+    bytes.bytes[3] = (byte) (v >> 32);
+    bytes.bytes[4] = (byte) (v >> 24);
+    bytes.bytes[5] = (byte) (v >> 16);
+    bytes.bytes[6] = (byte) (v >> 8);
+    bytes.bytes[7] = (byte) v;
+    assert bytesToLong(bytes) == v: bytesToLong(bytes) + " vs " + v + " bytes=" + bytes;
+  }
+}
diff --git a/lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/IDVersionPostingsReader.java b/lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/IDVersionPostingsReader.java
new file mode 100644
index 0000000..3a1ba6c
--- /dev/null
+++ b/lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/IDVersionPostingsReader.java
@@ -0,0 +1,97 @@
+package org.apache.lucene.codecs.idversion;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.BlockTermState;
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.PostingsReaderBase;
+import org.apache.lucene.index.DocsAndPositionsEnum;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.store.DataInput;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.Bits;
+
+final class IDVersionPostingsReader extends PostingsReaderBase {
+
+  @Override
+  public void init(IndexInput termsIn) throws IOException {
+    // Make sure we are talking to the matching postings writer
+    CodecUtil.checkHeader(termsIn,
+                          IDVersionPostingsWriter.TERMS_CODEC,
+                          IDVersionPostingsWriter.VERSION_START,
+                          IDVersionPostingsWriter.VERSION_CURRENT);
+  }
+
+  @Override
+  public BlockTermState newTermState() {
+    return new IDVersionTermState();
+  }
+
+  @Override
+  public void close() throws IOException {
+  }
+
+  @Override
+  public void decodeTerm(long[] longs, DataInput in, FieldInfo fieldInfo, BlockTermState _termState, boolean absolute)
+    throws IOException {
+    final IDVersionTermState termState = (IDVersionTermState) _termState;
+    termState.docID = in.readVInt();
+    termState.idVersion = in.readVLong();
+  }
+
+  @Override
+  public DocsEnum docs(FieldInfo fieldInfo, BlockTermState termState, Bits liveDocs, DocsEnum reuse, int flags) throws IOException {
+    SingleDocsEnum docsEnum;
+
+    if (reuse instanceof SingleDocsEnum) {
+      docsEnum = (SingleDocsEnum) reuse;
+    } else {
+      docsEnum = new SingleDocsEnum();
+    }
+    docsEnum.reset(((IDVersionTermState) termState).docID, liveDocs);
+
+    return docsEnum;
+  }
+
+  @Override
+  public DocsAndPositionsEnum docsAndPositions(FieldInfo fieldInfo, BlockTermState _termState, Bits liveDocs,
+                                               DocsAndPositionsEnum reuse, int flags) {
+    SingleDocsAndPositionsEnum posEnum;
+
+    if (reuse instanceof SingleDocsAndPositionsEnum) {
+      posEnum = (SingleDocsAndPositionsEnum) reuse;
+    } else {
+      posEnum = new SingleDocsAndPositionsEnum();
+    }
+    IDVersionTermState termState = (IDVersionTermState) _termState;
+    posEnum.reset(termState.docID, termState.idVersion, liveDocs);
+    return posEnum;
+  }
+
+  @Override
+  public long ramBytesUsed() {
+    return 0;
+  }
+
+  @Override
+  public void checkIntegrity() throws IOException {
+  }
+}
diff --git a/lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/IDVersionPostingsWriter.java b/lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/IDVersionPostingsWriter.java
new file mode 100644
index 0000000..4184069
--- /dev/null
+++ b/lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/IDVersionPostingsWriter.java
@@ -0,0 +1,157 @@
+package org.apache.lucene.codecs.idversion;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.BlockTermState;
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.PushPostingsWriterBase;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.store.DataOutput;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.BytesRef;
+
+final class IDVersionPostingsWriter extends PushPostingsWriterBase {
+
+  final static String TERMS_CODEC = "IDVersionPostingsWriterTerms";
+
+  // Increment version to change it
+  final static int VERSION_START = 0;
+  final static int VERSION_CURRENT = VERSION_START;
+
+  final static IDVersionTermState emptyState = new IDVersionTermState();
+  IDVersionTermState lastState;
+
+  int lastDocID;
+  private int lastPosition;
+  private long lastVersion;
+
+  private final SegmentWriteState state;
+
+  public IDVersionPostingsWriter(SegmentWriteState state) {
+    this.state = state;
+  }
+
+  @Override
+  public BlockTermState newTermState() {
+    return new IDVersionTermState();
+  }
+
+  @Override
+  public void init(IndexOutput termsOut) throws IOException {
+    CodecUtil.writeHeader(termsOut, TERMS_CODEC, VERSION_CURRENT);
+  }
+
+  @Override
+  public int setField(FieldInfo fieldInfo) {
+    super.setField(fieldInfo);
+    if (fieldInfo.getIndexOptions() != FieldInfo.IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
+      throw new IllegalArgumentException("field must be index using IndexOptions.DOCS_AND_FREQS_AND_POSITIONS");
+    }
+    // LUCENE-5693: because CheckIndex cross-checks term vectors with postings even for deleted docs, and because our PF only indexes the
+    // non-deleted documents on flush, CheckIndex will see this as corruption:
+    if (fieldInfo.hasVectors()) {
+      throw new IllegalArgumentException("field cannot index term vectors: CheckIndex will report this as index corruption");
+    }
+    lastState = emptyState;
+    return 0;
+  }
+
+  @Override
+  public void startTerm() {
+    lastDocID = -1;
+  }
+
+  @Override
+  public void startDoc(int docID, int termDocFreq) throws IOException {
+    // TODO: LUCENE-5693: we don't need this check if we fix IW to not send deleted docs to us on flush:
+    if (state.liveDocs != null && state.liveDocs.get(docID) == false) {
+      return;
+    }
+    if (lastDocID != -1) {
+      throw new IllegalArgumentException("term appears in more than one document");
+    }
+    if (termDocFreq != 1) {
+      throw new IllegalArgumentException("term appears more than once in the document");
+    }
+
+    lastDocID = docID;
+    lastPosition = -1;
+    lastVersion = -1;
+  }
+
+  @Override
+  public void addPosition(int position, BytesRef payload, int startOffset, int endOffset) throws IOException {
+    if (lastDocID == -1) {
+      // Doc is deleted; skip it
+      return;
+    }
+    if (lastPosition != -1) {
+      throw new IllegalArgumentException("term appears more than once in document");
+    }
+    lastPosition = position;
+    if (payload == null) {
+      throw new IllegalArgumentException("token doens't have a payload");
+    }
+    if (payload.length != 8) {
+      throw new IllegalArgumentException("payload.length != 8 (got " + payload.length + ")");
+    }
+
+    lastVersion = IDVersionPostingsFormat.bytesToLong(payload);
+    if (lastVersion < 0) {
+      throw new IllegalArgumentException("version must be >= 0 (got: " + lastVersion + "; payload=" + payload + ")");
+    }
+  }
+
+  @Override
+  public void finishDoc() throws IOException {
+    if (lastDocID == -1) {
+      // Doc is deleted; skip it
+      return;
+    }
+    if (lastPosition == -1) {
+      throw new IllegalArgumentException("missing addPosition");
+    }
+  }
+
+  /** Called when we are done adding docs to this term */
+  @Override
+  public void finishTerm(BlockTermState _state) throws IOException {
+    if (lastDocID == -1) {
+      return;
+    }
+    IDVersionTermState state = (IDVersionTermState) _state;
+    assert state.docFreq > 0;
+
+    state.docID = lastDocID;
+    state.idVersion = lastVersion;
+  }
+  
+  @Override
+  public void encodeTerm(long[] longs, DataOutput out, FieldInfo fieldInfo, BlockTermState _state, boolean absolute) throws IOException {
+    IDVersionTermState state = (IDVersionTermState) _state;
+    out.writeVInt(state.docID);
+    out.writeVLong(state.idVersion);
+  }
+
+  @Override
+  public void close() throws IOException {
+  }
+}
diff --git a/lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/IDVersionSegmentTermsEnum.java b/lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/IDVersionSegmentTermsEnum.java
new file mode 100644
index 0000000..bca8027
--- /dev/null
+++ b/lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/IDVersionSegmentTermsEnum.java
@@ -0,0 +1,1071 @@
+package org.apache.lucene.codecs.idversion;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.io.PrintStream;
+
+import org.apache.lucene.codecs.BlockTermState;
+import org.apache.lucene.index.DocsAndPositionsEnum;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.TermState;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.store.ByteArrayDataInput;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.RamUsageEstimator;
+import org.apache.lucene.util.fst.FST;
+import org.apache.lucene.util.fst.PairOutputs.Pair;
+import org.apache.lucene.util.fst.Util;
+
+/** Iterates through terms in this field; this class is public so users
+ *  can cast it to call {@link #seekExact(BytesRef, long)} for
+ *  optimistic-concurreny, and also {@link #getVersion} to get the
+ *  version of the currently seek'd term. */
+public final class IDVersionSegmentTermsEnum extends TermsEnum {
+
+  // Lazy init:
+  IndexInput in;
+
+  // static boolean DEBUG = false;
+
+  private IDVersionSegmentTermsEnumFrame[] stack;
+  private final IDVersionSegmentTermsEnumFrame staticFrame;
+  IDVersionSegmentTermsEnumFrame currentFrame;
+  boolean termExists;
+  final VersionFieldReader fr;
+
+  private int targetBeforeCurrentLength;
+
+  private final ByteArrayDataInput scratchReader = new ByteArrayDataInput();
+
+  // What prefix of the current term was present in the index:
+  private int validIndexPrefix;
+
+  // assert only:
+  private boolean eof;
+
+  final BytesRef term = new BytesRef();
+  private final FST.BytesReader fstReader;
+
+  @SuppressWarnings({"rawtypes","unchecked"}) private FST.Arc<Pair<BytesRef,Long>>[] arcs =
+  new FST.Arc[1];
+
+  IDVersionSegmentTermsEnum(VersionFieldReader fr) throws IOException {
+    this.fr = fr;
+
+    //if (DEBUG) System.out.println("BTTR.init seg=" + segment);
+    stack = new IDVersionSegmentTermsEnumFrame[0];
+        
+    // Used to hold seek by TermState, or cached seek
+    staticFrame = new IDVersionSegmentTermsEnumFrame(this, -1);
+
+    if (fr.index == null) {
+      fstReader = null;
+    } else {
+      fstReader = fr.index.getBytesReader();
+    }
+
+    // Init w/ root block; don't use index since it may
+    // not (and need not) have been loaded
+    for(int arcIdx=0;arcIdx<arcs.length;arcIdx++) {
+      arcs[arcIdx] = new FST.Arc<>();
+    }
+
+    currentFrame = staticFrame;
+    final FST.Arc<Pair<BytesRef,Long>> arc;
+    if (fr.index != null) {
+      arc = fr.index.getFirstArc(arcs[0]);
+      // Empty string prefix must have an output in the index!
+      assert arc.isFinal();
+    } else {
+      arc = null;
+    }
+    currentFrame = staticFrame;
+    //currentFrame = pushFrame(arc, rootCode, 0);
+    //currentFrame.loadBlock();
+    validIndexPrefix = 0;
+    // if (DEBUG) {
+    //   System.out.println("init frame state " + currentFrame.ord);
+    //   printSeekState();
+    // }
+
+    //System.out.println();
+    // computeBlockStats().print(System.out);
+  }
+      
+  // Not private to avoid synthetic access$NNN methods
+  void initIndexInput() {
+    if (this.in == null) {
+      this.in = fr.parent.in.clone();
+    }
+  }
+
+  private IDVersionSegmentTermsEnumFrame getFrame(int ord) throws IOException {
+    if (ord >= stack.length) {
+      final IDVersionSegmentTermsEnumFrame[] next = new IDVersionSegmentTermsEnumFrame[ArrayUtil.oversize(1+ord, RamUsageEstimator.NUM_BYTES_OBJECT_REF)];
+      System.arraycopy(stack, 0, next, 0, stack.length);
+      for(int stackOrd=stack.length;stackOrd<next.length;stackOrd++) {
+        next[stackOrd] = new IDVersionSegmentTermsEnumFrame(this, stackOrd);
+      }
+      stack = next;
+    }
+    assert stack[ord].ord == ord;
+    return stack[ord];
+  }
+
+  private FST.Arc<Pair<BytesRef,Long>> getArc(int ord) {
+    if (ord >= arcs.length) {
+      @SuppressWarnings({"rawtypes","unchecked"}) final FST.Arc<Pair<BytesRef,Long>>[] next =
+      new FST.Arc[ArrayUtil.oversize(1+ord, RamUsageEstimator.NUM_BYTES_OBJECT_REF)];
+      System.arraycopy(arcs, 0, next, 0, arcs.length);
+      for(int arcOrd=arcs.length;arcOrd<next.length;arcOrd++) {
+        next[arcOrd] = new FST.Arc<>();
+      }
+      arcs = next;
+    }
+    return arcs[ord];
+  }
+
+  // Pushes a frame we seek'd to
+  IDVersionSegmentTermsEnumFrame pushFrame(FST.Arc<Pair<BytesRef,Long>> arc, Pair<BytesRef,Long> frameData, int length) throws IOException {
+    scratchReader.reset(frameData.output1.bytes, frameData.output1.offset, frameData.output1.length);
+    final long code = scratchReader.readVLong();
+    final long fpSeek = code >>> VersionBlockTreeTermsWriter.OUTPUT_FLAGS_NUM_BITS;
+    final IDVersionSegmentTermsEnumFrame f = getFrame(1+currentFrame.ord);
+    f.maxIDVersion = Long.MAX_VALUE - frameData.output2;
+    f.hasTerms = (code & VersionBlockTreeTermsWriter.OUTPUT_FLAG_HAS_TERMS) != 0;
+    f.hasTermsOrig = f.hasTerms;
+    f.isFloor = (code & VersionBlockTreeTermsWriter.OUTPUT_FLAG_IS_FLOOR) != 0;
+    if (f.isFloor) {
+      f.setFloorData(scratchReader, frameData.output1);
+    }
+    pushFrame(arc, fpSeek, length);
+
+    return f;
+  }
+
+  // Pushes next'd frame or seek'd frame; we later
+  // lazy-load the frame only when needed
+  IDVersionSegmentTermsEnumFrame pushFrame(FST.Arc<Pair<BytesRef,Long>> arc, long fp, int length) throws IOException {
+    final IDVersionSegmentTermsEnumFrame f = getFrame(1+currentFrame.ord);
+    f.arc = arc;
+    if (f.fpOrig == fp && f.nextEnt != -1) {
+      //if (DEBUG) System.out.println("      push reused frame ord=" + f.ord + " fp=" + f.fp + " isFloor?=" + f.isFloor + " hasTerms=" + f.hasTerms + " pref=" + term + " nextEnt=" + f.nextEnt + " targetBeforeCurrentLength=" + targetBeforeCurrentLength + " term.length=" + term.length + " vs prefix=" + f.prefix);
+      if (f.prefix > targetBeforeCurrentLength) {
+        f.rewind();
+      } else {
+        // if (DEBUG) {
+        //   System.out.println("        skip rewind!");
+        // }
+      }
+      assert length == f.prefix;
+    } else {
+      f.nextEnt = -1;
+      f.prefix = length;
+      f.state.termBlockOrd = 0;
+      f.fpOrig = f.fp = fp;
+      f.lastSubFP = -1;
+      // if (DEBUG) {
+      //   final int sav = term.length;
+      //   term.length = length;
+      //   System.out.println("      push new frame ord=" + f.ord + " fp=" + f.fp + " hasTerms=" + f.hasTerms + " isFloor=" + f.isFloor + " pref=" + brToString(term));
+      //   term.length = sav;
+      // }
+    }
+
+    return f;
+  }
+
+  // asserts only
+  private boolean clearEOF() {
+    eof = false;
+    return true;
+  }
+
+  // asserts only
+  private boolean setEOF() {
+    eof = true;
+    return true;
+  }
+
+  @Override
+  public boolean seekExact(final BytesRef target) throws IOException {
+    return seekExact(target, 0);
+  }
+
+  // for debugging
+  @SuppressWarnings("unused")
+  static String brToString(BytesRef b) {
+    try {
+      return b.utf8ToString() + " " + b;
+    } catch (Throwable t) {
+      // If BytesRef isn't actually UTF8, or it's eg a
+      // prefix of UTF8 that ends mid-unicode-char, we
+      // fallback to hex:
+      return b.toString();
+    }
+  }
+
+  /** Get the version of the currently seek'd term; only valid if we are
+   *  positioned. */
+  public long getVersion() {
+    return ((IDVersionTermState) currentFrame.state).idVersion;
+  }
+
+  /** Optimized version of {@link #seekExact(BytesRef)} that can
+   *  sometimes fail-fast if the version indexed with the requested ID
+   *  is less than the specified minIDVersion.  Applications that index
+   *  a monotonically increasing global version with each document can
+   *  use this for fast optimistic concurrency. */
+  public boolean seekExact(final BytesRef target, long minIDVersion) throws IOException {
+
+    if (fr.index == null) {
+      throw new IllegalStateException("terms index was not loaded");
+    }
+
+    if (term.bytes.length <= target.length) {
+      term.bytes = ArrayUtil.grow(term.bytes, 1+target.length);
+    }
+
+    assert clearEOF();
+
+    //  if (DEBUG) {
+    //    System.out.println("\nBTTR.seekExact seg=" + fr.parent.segment + " target=" + fr.fieldInfo.name + ":" + brToString(target) + " minIDVersion=" + minIDVersion + " current=" + brToString(term) + " (exists?=" + termExists + ") validIndexPrefix=" + validIndexPrefix);
+    //   printSeekState(System.out);
+    //  }
+
+    FST.Arc<Pair<BytesRef,Long>> arc;
+    int targetUpto;
+    Pair<BytesRef,Long> output;
+
+    long startFrameFP = currentFrame.fp;
+
+    targetBeforeCurrentLength = currentFrame.ord;
+
+    boolean changed = false;
+
+    // TODO: we could stop earlier w/ the version check, every time we traverse an index arc we can check?
+
+    if (currentFrame != staticFrame) {
+
+      // We are already seek'd; find the common
+      // prefix of new seek term vs current term and
+      // re-use the corresponding seek state.  For
+      // example, if app first seeks to foobar, then
+      // seeks to foobaz, we can re-use the seek state
+      // for the first 5 bytes.
+
+      // if (DEBUG) {
+      //    System.out.println("  re-use current seek state validIndexPrefix=" + validIndexPrefix);
+      //  }
+
+      arc = arcs[0];
+      assert arc.isFinal();
+      output = arc.output;
+      targetUpto = 0;
+
+      IDVersionSegmentTermsEnumFrame lastFrame = stack[0];
+      assert validIndexPrefix <= term.length: "validIndexPrefix=" + validIndexPrefix + " term.length=" + term.length + " seg=" + fr.parent.segment;
+
+      final int targetLimit = Math.min(target.length, validIndexPrefix);
+
+      int cmp = 0;
+
+      // TODO: reverse vLong byte order for better FST
+      // prefix output sharing
+
+      // First compare up to valid seek frames:
+      while (targetUpto < targetLimit) {
+        cmp = (term.bytes[targetUpto]&0xFF) - (target.bytes[target.offset + targetUpto]&0xFF);
+        // if (DEBUG) {
+        //    System.out.println("    cycle targetUpto=" + targetUpto + " (vs limit=" + targetLimit + ") cmp=" + cmp + " (targetLabel=" + (char) (target.bytes[target.offset + targetUpto]) + " vs termLabel=" + (char) (term.bytes[targetUpto]) + ")"   + " arc.output=" + arc.output + " output=" + output);
+        // }
+        if (cmp != 0) {
+          break;
+        }
+        arc = arcs[1+targetUpto];
+        //if (arc.label != (target.bytes[target.offset + targetUpto] & 0xFF)) {
+        //System.out.println("FAIL: arc.label=" + (char) arc.label + " targetLabel=" + (char) (target.bytes[target.offset + targetUpto] & 0xFF));
+        //}
+        assert arc.label == (target.bytes[target.offset + targetUpto] & 0xFF): "arc.label=" + (char) arc.label + " targetLabel=" + (char) (target.bytes[target.offset + targetUpto] & 0xFF);
+        if (arc.output != VersionBlockTreeTermsWriter.NO_OUTPUT) {
+          output = VersionBlockTreeTermsWriter.FST_OUTPUTS.add(output, arc.output);
+        }
+        if (arc.isFinal()) {
+          lastFrame = stack[1+lastFrame.ord];
+        }
+        targetUpto++;
+      }
+
+      if (cmp == 0) {
+        final int targetUptoMid = targetUpto;
+
+        // Second compare the rest of the term, but
+        // don't save arc/output/frame; we only do this
+        // to find out if the target term is before,
+        // equal or after the current term
+        final int targetLimit2 = Math.min(target.length, term.length);
+        while (targetUpto < targetLimit2) {
+          cmp = (term.bytes[targetUpto]&0xFF) - (target.bytes[target.offset + targetUpto]&0xFF);
+          // if (DEBUG) {
+          //    System.out.println("    cycle2 targetUpto=" + targetUpto + " (vs limit=" + targetLimit + ") cmp=" + cmp + " (targetLabel=" + (char) (target.bytes[target.offset + targetUpto]) + " vs termLabel=" + (char) (term.bytes[targetUpto]) + ")");
+          // }
+          if (cmp != 0) {
+            break;
+          }
+          targetUpto++;
+        }
+
+        if (cmp == 0) {
+          cmp = term.length - target.length;
+        }
+        targetUpto = targetUptoMid;
+      }
+
+      if (cmp < 0) {
+        // Common case: target term is after current
+        // term, ie, app is seeking multiple terms
+        // in sorted order
+        // if (DEBUG) {
+        //    System.out.println("  target is after current (shares prefixLen=" + targetUpto + "); frame.ord=" + lastFrame.ord + "; targetUpto=" + targetUpto);
+        //  }
+        currentFrame = lastFrame;
+
+      } else if (cmp > 0) {
+        // Uncommon case: target term
+        // is before current term; this means we can
+        // keep the currentFrame but we must rewind it
+        // (so we scan from the start)
+        targetBeforeCurrentLength = 0;
+        changed = true;
+        // if (DEBUG) {
+        //    System.out.println("  target is before current (shares prefixLen=" + targetUpto + "); rewind frame ord=" + lastFrame.ord);
+        //  }
+        currentFrame = lastFrame;
+        currentFrame.rewind();
+      } else {
+        // Target is exactly the same as current term
+        assert term.length == target.length;
+        if (termExists) {
+
+          if (currentFrame.maxIDVersion < minIDVersion) {
+            // The max version for all terms in this block is lower than the minVersion
+            // if (DEBUG) {
+            //   System.out.println("  target is same as current maxIDVersion=" + currentFrame.maxIDVersion + " is < minIDVersion=" + minIDVersion + "; return false");
+            // }
+            return false;
+          }
+
+          currentFrame.decodeMetaData();
+          if (((IDVersionTermState) currentFrame.state).idVersion < minIDVersion) {
+            // This term's version is lower than the minVersion
+            // if (DEBUG) {
+            //   System.out.println("  target is same as current but version=" + ((IDVersionTermState) currentFrame.state).idVersion + " is < minIDVersion=" + minIDVersion + "; return false");
+            // }
+            return false;
+          }
+          // System.out.println("  term version=" + ((IDVersionTermState) currentFrame.state).idVersion + " frame version=" + currentFrame.maxIDVersion + " frame ord=" + currentFrame.ord);
+
+          // if (DEBUG) {
+          //    System.out.println("  target is same as current; return true");
+          //  }
+          return true;
+        } else {
+          // if (DEBUG) {
+          //    System.out.println("  target is same as current but term doesn't exist");
+          //  }
+        }
+        //validIndexPrefix = currentFrame.depth;
+        //term.length = target.length;
+        //return termExists;
+      }
+
+    } else {
+
+      targetBeforeCurrentLength = -1;
+      arc = fr.index.getFirstArc(arcs[0]);
+      //System.out.println("first arc=" + arc);
+
+      // Empty string prefix must have an output (block) in the index!
+      assert arc.isFinal();
+      assert arc.output != null;
+
+      // if (DEBUG) {
+      //    System.out.println("    no seek state; push root frame");
+      //  }
+
+      output = arc.output;
+
+      currentFrame = staticFrame;
+
+      //term.length = 0;
+      targetUpto = 0;
+      currentFrame = pushFrame(arc, VersionBlockTreeTermsWriter.FST_OUTPUTS.add(output, arc.nextFinalOutput), 0);
+    }
+
+    // if (DEBUG) {
+    //   System.out.println("  start index loop targetUpto=" + targetUpto + " output=" + output + " currentFrame.ord=" + currentFrame.ord + " targetBeforeCurrentLength=" + targetBeforeCurrentLength + " termExists=" + termExists);
+    // }
+
+    // We are done sharing the common prefix with the incoming target and where we are currently seek'd; now continue walking the index:
+    while (targetUpto < target.length) {
+
+      final int targetLabel = target.bytes[target.offset + targetUpto] & 0xFF;
+
+      final FST.Arc<Pair<BytesRef,Long>> nextArc = fr.index.findTargetArc(targetLabel, arc, getArc(1+targetUpto), fstReader);
+
+      if (nextArc == null) {
+
+        // Index is exhausted
+        // if (DEBUG) {
+        //    System.out.println("    index: index exhausted label=" + ((char) targetLabel) + " " + Integer.toHexString(targetLabel) + " termExists=" + termExists);
+        //  }
+            
+        validIndexPrefix = currentFrame.prefix;
+        //validIndexPrefix = targetUpto;
+
+        currentFrame.scanToFloorFrame(target);
+
+        if (!currentFrame.hasTerms) {
+          termExists = false;
+          term.bytes[targetUpto] = (byte) targetLabel;
+          term.length = 1+targetUpto;
+          // if (DEBUG) {
+          //    System.out.println("  FAST NOT_FOUND term=" + brToString(term));
+          //  }
+          return false;
+        }
+        //System.out.println("  check maxVersion=" + currentFrame.maxIDVersion + " vs " + minIDVersion);
+
+        // if (DEBUG) {
+        //   System.out.println("  frame.maxIDVersion=" + currentFrame.maxIDVersion +  " vs minIDVersion=" + minIDVersion);
+        // }
+
+        if (currentFrame.maxIDVersion < minIDVersion) {
+          // The max version for all terms in this block is lower than the minVersion
+          if (currentFrame.fp != startFrameFP || changed) {
+          //if (targetUpto+1 > term.length) {
+            termExists = false;
+            term.bytes[targetUpto] = (byte) targetLabel;
+            term.length = 1+targetUpto;
+            // if (DEBUG) {
+            //   System.out.println("    reset current term");
+            // }
+            validIndexPrefix = Math.min(validIndexPrefix, term.length);
+          }
+            //if (currentFrame.ord != startFrameOrd) {
+            //termExists = false;
+            //}
+          // if (DEBUG) {
+          //   System.out.println("    FAST version NOT_FOUND term=" + brToString(term) + " targetUpto=" + targetUpto + " currentFrame.maxIDVersion=" + currentFrame.maxIDVersion + " validIndexPrefix=" + validIndexPrefix + " startFrameFP=" + startFrameFP + " vs " + currentFrame.fp + " termExists=" + termExists);
+          // }
+          return false;
+        }
+
+        currentFrame.loadBlock();
+
+        // if (DEBUG) {
+        //   System.out.println("    scan currentFrame ord=" + currentFrame.ord);
+        // }
+        final SeekStatus result = currentFrame.scanToTerm(target, true);            
+        if (result == SeekStatus.FOUND) {
+          currentFrame.decodeMetaData();
+          if (((IDVersionTermState) currentFrame.state).idVersion < minIDVersion) {
+            // This term's version is lower than the minVersion
+            // if (DEBUG) {
+            //   System.out.println("    return NOT_FOUND: idVersion=" + ((IDVersionTermState) currentFrame.state).idVersion + " vs minIDVersion=" + minIDVersion);
+            // }
+            return false;
+          }
+
+          // if (DEBUG) {
+          //    System.out.println("  return FOUND term=" + term.utf8ToString() + " " + term);
+          //  }
+
+          return true;
+        } else {
+          // if (DEBUG) {
+          //    System.out.println("  got " + result + "; return NOT_FOUND term=" + brToString(term));
+          // }
+          return false;
+        }
+      } else {
+        // Follow this arc
+        arc = nextArc;
+        if (term.bytes[targetUpto] != (byte) targetLabel) {
+          // if (DEBUG) {
+          //   System.out.println("  now set termExists=false targetUpto=" + targetUpto + " term=" + term.bytes[targetUpto] + " targetLabel=" + targetLabel);
+          // }
+          changed = true;
+          term.bytes[targetUpto] = (byte) targetLabel;
+          termExists = false;
+        }
+        // Aggregate output as we go:
+        assert arc.output != null;
+        if (arc.output != VersionBlockTreeTermsWriter.NO_OUTPUT) {
+          output = VersionBlockTreeTermsWriter.FST_OUTPUTS.add(output, arc.output);
+        }
+
+        // if (DEBUG) {
+        //    System.out.println("    index: follow label=" + (char) ((target.bytes[target.offset + targetUpto]&0xff)) + " arc.output=" + arc.output + " arc.nfo=" + arc.nextFinalOutput);
+        //  }
+        targetUpto++;
+
+        if (arc.isFinal()) {
+          // if (DEBUG) System.out.println("    arc is final!");
+          currentFrame = pushFrame(arc, VersionBlockTreeTermsWriter.FST_OUTPUTS.add(output, arc.nextFinalOutput), targetUpto);
+          // if (DEBUG) System.out.println("    curFrame.ord=" + currentFrame.ord + " hasTerms=" + currentFrame.hasTerms);
+        }
+      }
+    }
+
+    //validIndexPrefix = targetUpto;
+    validIndexPrefix = currentFrame.prefix;
+
+    currentFrame.scanToFloorFrame(target);
+
+    // Target term is entirely contained in the index:
+    if (!currentFrame.hasTerms) {
+      termExists = false;
+      term.length = targetUpto;
+      // if (DEBUG) {
+      //    System.out.println("  FAST NOT_FOUND term=" + brToString(term));
+      //  }
+      return false;
+    }
+
+    // if (DEBUG) {
+    //   System.out.println("  frame.maxIDVersion=" + currentFrame.maxIDVersion +  " vs minIDVersion=" + minIDVersion);
+    // }
+
+    if (currentFrame.maxIDVersion < minIDVersion) {
+      // The max version for all terms in this block is lower than the minVersion
+      termExists = false;
+      term.length = targetUpto;
+      return false;
+    }
+
+    currentFrame.loadBlock();
+
+    final SeekStatus result = currentFrame.scanToTerm(target, true);            
+    if (result == SeekStatus.FOUND) {
+      // if (DEBUG) {
+      //    System.out.println("  return FOUND term=" + term.utf8ToString() + " " + term);
+      //  }
+      currentFrame.decodeMetaData();
+      if (((IDVersionTermState) currentFrame.state).idVersion < minIDVersion) {
+        // This term's version is lower than the minVersion
+        return false;
+      }
+      return true;
+    } else {
+      // if (DEBUG) {
+      //    System.out.println("  got result " + result + "; return NOT_FOUND term=" + term.utf8ToString());
+      //  }
+
+      return false;
+    }
+  }
+
+  @Override
+  public SeekStatus seekCeil(final BytesRef target) throws IOException {
+    if (fr.index == null) {
+      throw new IllegalStateException("terms index was not loaded");
+    }
+   
+    if (term.bytes.length <= target.length) {
+      term.bytes = ArrayUtil.grow(term.bytes, 1+target.length);
+    }
+
+    assert clearEOF();
+
+    //if (DEBUG) {
+    //System.out.println("\nBTTR.seekCeil seg=" + segment + " target=" + fieldInfo.name + ":" + target.utf8ToString() + " " + target + " current=" + brToString(term) + " (exists?=" + termExists + ") validIndexPrefix=  " + validIndexPrefix);
+    //printSeekState();
+    //}
+
+    FST.Arc<Pair<BytesRef,Long>> arc;
+    int targetUpto;
+    Pair<BytesRef,Long> output;
+
+    targetBeforeCurrentLength = currentFrame.ord;
+
+    if (currentFrame != staticFrame) {
+
+      // We are already seek'd; find the common
+      // prefix of new seek term vs current term and
+      // re-use the corresponding seek state.  For
+      // example, if app first seeks to foobar, then
+      // seeks to foobaz, we can re-use the seek state
+      // for the first 5 bytes.
+
+      //if (DEBUG) {
+      //System.out.println("  re-use current seek state validIndexPrefix=" + validIndexPrefix);
+      //}
+
+      arc = arcs[0];
+      assert arc.isFinal();
+      output = arc.output;
+      targetUpto = 0;
+          
+      IDVersionSegmentTermsEnumFrame lastFrame = stack[0];
+      assert validIndexPrefix <= term.length;
+
+      final int targetLimit = Math.min(target.length, validIndexPrefix);
+
+      int cmp = 0;
+
+      // TOOD: we should write our vLong backwards (MSB
+      // first) to get better sharing from the FST
+
+      // First compare up to valid seek frames:
+      while (targetUpto < targetLimit) {
+        cmp = (term.bytes[targetUpto]&0xFF) - (target.bytes[target.offset + targetUpto]&0xFF);
+        //if (DEBUG) {
+        //System.out.println("    cycle targetUpto=" + targetUpto + " (vs limit=" + targetLimit + ") cmp=" + cmp + " (targetLabel=" + (char) (target.bytes[target.offset + targetUpto]) + " vs termLabel=" + (char) (term.bytes[targetUpto]) + ")"   + " arc.output=" + arc.output + " output=" + output);
+        //}
+        if (cmp != 0) {
+          break;
+        }
+        arc = arcs[1+targetUpto];
+        assert arc.label == (target.bytes[target.offset + targetUpto] & 0xFF): "arc.label=" + (char) arc.label + " targetLabel=" + (char) (target.bytes[target.offset + targetUpto] & 0xFF);
+        // TOOD: we could save the outputs in local
+        // byte[][] instead of making new objs ever
+        // seek; but, often the FST doesn't have any
+        // shared bytes (but this could change if we
+        // reverse vLong byte order)
+        if (arc.output != VersionBlockTreeTermsWriter.NO_OUTPUT) {
+          output = VersionBlockTreeTermsWriter.FST_OUTPUTS.add(output, arc.output);
+        }
+        if (arc.isFinal()) {
+          lastFrame = stack[1+lastFrame.ord];
+        }
+        targetUpto++;
+      }
+
+
+      if (cmp == 0) {
+        final int targetUptoMid = targetUpto;
+        // Second compare the rest of the term, but
+        // don't save arc/output/frame:
+        final int targetLimit2 = Math.min(target.length, term.length);
+        while (targetUpto < targetLimit2) {
+          cmp = (term.bytes[targetUpto]&0xFF) - (target.bytes[target.offset + targetUpto]&0xFF);
+          //if (DEBUG) {
+          //System.out.println("    cycle2 targetUpto=" + targetUpto + " (vs limit=" + targetLimit + ") cmp=" + cmp + " (targetLabel=" + (char) (target.bytes[target.offset + targetUpto]) + " vs termLabel=" + (char) (term.bytes[targetUpto]) + ")");
+          //}
+          if (cmp != 0) {
+            break;
+          }
+          targetUpto++;
+        }
+
+        if (cmp == 0) {
+          cmp = term.length - target.length;
+        }
+        targetUpto = targetUptoMid;
+      }
+
+      if (cmp < 0) {
+        // Common case: target term is after current
+        // term, ie, app is seeking multiple terms
+        // in sorted order
+        //if (DEBUG) {
+        //System.out.println("  target is after current (shares prefixLen=" + targetUpto + "); clear frame.scanned ord=" + lastFrame.ord);
+        //}
+        currentFrame = lastFrame;
+
+      } else if (cmp > 0) {
+        // Uncommon case: target term
+        // is before current term; this means we can
+        // keep the currentFrame but we must rewind it
+        // (so we scan from the start)
+        targetBeforeCurrentLength = 0;
+        //if (DEBUG) {
+        //System.out.println("  target is before current (shares prefixLen=" + targetUpto + "); rewind frame ord=" + lastFrame.ord);
+        //}
+        currentFrame = lastFrame;
+        currentFrame.rewind();
+      } else {
+        // Target is exactly the same as current term
+        assert term.length == target.length;
+        if (termExists) {
+          //if (DEBUG) {
+          //System.out.println("  target is same as current; return FOUND");
+          //}
+          return SeekStatus.FOUND;
+        } else {
+          //if (DEBUG) {
+          //System.out.println("  target is same as current but term doesn't exist");
+          //}
+        }
+      }
+
+    } else {
+
+      targetBeforeCurrentLength = -1;
+      arc = fr.index.getFirstArc(arcs[0]);
+
+      // Empty string prefix must have an output (block) in the index!
+      assert arc.isFinal();
+      assert arc.output != null;
+
+      //if (DEBUG) {
+      //System.out.println("    no seek state; push root frame");
+      //}
+
+      output = arc.output;
+
+      currentFrame = staticFrame;
+
+      //term.length = 0;
+      targetUpto = 0;
+      currentFrame = pushFrame(arc, VersionBlockTreeTermsWriter.FST_OUTPUTS.add(output, arc.nextFinalOutput), 0);
+    }
+
+    //if (DEBUG) {
+    //System.out.println("  start index loop targetUpto=" + targetUpto + " output=" + output + " currentFrame.ord+1=" + currentFrame.ord + " targetBeforeCurrentLength=" + targetBeforeCurrentLength);
+    //}
+
+    // We are done sharing the common prefix with the incoming target and where we are currently seek'd; now continue walking the index:
+    while (targetUpto < target.length) {
+
+      final int targetLabel = target.bytes[target.offset + targetUpto] & 0xFF;
+
+      final FST.Arc<Pair<BytesRef,Long>> nextArc = fr.index.findTargetArc(targetLabel, arc, getArc(1+targetUpto), fstReader);
+
+      if (nextArc == null) {
+
+        // Index is exhausted
+        // if (DEBUG) {
+        //   System.out.println("    index: index exhausted label=" + ((char) targetLabel) + " " + toHex(targetLabel));
+        // }
+            
+        validIndexPrefix = currentFrame.prefix;
+        //validIndexPrefix = targetUpto;
+
+        currentFrame.scanToFloorFrame(target);
+
+        currentFrame.loadBlock();
+
+        final SeekStatus result = currentFrame.scanToTerm(target, false);
+        if (result == SeekStatus.END) {
+          term.copyBytes(target);
+          termExists = false;
+
+          if (next() != null) {
+            //if (DEBUG) {
+            //System.out.println("  return NOT_FOUND term=" + brToString(term) + " " + term);
+            //}
+            return SeekStatus.NOT_FOUND;
+          } else {
+            //if (DEBUG) {
+            //System.out.println("  return END");
+            //}
+            return SeekStatus.END;
+          }
+        } else {
+          //if (DEBUG) {
+          //System.out.println("  return " + result + " term=" + brToString(term) + " " + term);
+          //}
+          return result;
+        }
+      } else {
+        // Follow this arc
+        term.bytes[targetUpto] = (byte) targetLabel;
+        arc = nextArc;
+        // Aggregate output as we go:
+        assert arc.output != null;
+        if (arc.output != VersionBlockTreeTermsWriter.NO_OUTPUT) {
+          output = VersionBlockTreeTermsWriter.FST_OUTPUTS.add(output, arc.output);
+        }
+
+        //if (DEBUG) {
+        //System.out.println("    index: follow label=" + toHex(target.bytes[target.offset + targetUpto]&0xff) + " arc.output=" + arc.output + " arc.nfo=" + arc.nextFinalOutput);
+        //}
+        targetUpto++;
+
+        if (arc.isFinal()) {
+          //if (DEBUG) System.out.println("    arc is final!");
+          currentFrame = pushFrame(arc, VersionBlockTreeTermsWriter.FST_OUTPUTS.add(output, arc.nextFinalOutput), targetUpto);
+          //if (DEBUG) System.out.println("    curFrame.ord=" + currentFrame.ord + " hasTerms=" + currentFrame.hasTerms);
+        }
+      }
+    }
+
+    //validIndexPrefix = targetUpto;
+    validIndexPrefix = currentFrame.prefix;
+
+    currentFrame.scanToFloorFrame(target);
+
+    currentFrame.loadBlock();
+
+    final SeekStatus result = currentFrame.scanToTerm(target, false);
+
+    if (result == SeekStatus.END) {
+      term.copyBytes(target);
+      termExists = false;
+      if (next() != null) {
+        //if (DEBUG) {
+        //System.out.println("  return NOT_FOUND term=" + term.utf8ToString() + " " + term);
+        //}
+        return SeekStatus.NOT_FOUND;
+      } else {
+        //if (DEBUG) {
+        //System.out.println("  return END");
+        //}
+        return SeekStatus.END;
+      }
+    } else {
+      return result;
+    }
+  }
+
+  @SuppressWarnings("unused")
+  private void printSeekState(PrintStream out) throws IOException {
+    if (currentFrame == staticFrame) {
+      out.println("  no prior seek");
+    } else {
+      out.println("  prior seek state:");
+      int ord = 0;
+      boolean isSeekFrame = true;
+      while(true) {
+        IDVersionSegmentTermsEnumFrame f = getFrame(ord);
+        assert f != null;
+        final BytesRef prefix = new BytesRef(term.bytes, 0, f.prefix);
+        if (f.nextEnt == -1) {
+          out.println("    frame " + (isSeekFrame ? "(seek)" : "(next)") + " ord=" + ord + " fp=" + f.fp + (f.isFloor ? (" (fpOrig=" + f.fpOrig + ")") : "") + " prefixLen=" + f.prefix + " prefix=" + brToString(prefix) + (f.nextEnt == -1 ? "" : (" (of " + f.entCount + ")")) + " hasTerms=" + f.hasTerms + " isFloor=" + f.isFloor + " code=" + ((f.fp<<VersionBlockTreeTermsWriter.OUTPUT_FLAGS_NUM_BITS) + (f.hasTerms ? VersionBlockTreeTermsWriter.OUTPUT_FLAG_HAS_TERMS:0) + (f.isFloor ? VersionBlockTreeTermsWriter.OUTPUT_FLAG_IS_FLOOR:0)) + " isLastInFloor=" + f.isLastInFloor + " mdUpto=" + f.metaDataUpto + " tbOrd=" + f.getTermBlockOrd());
+        } else {
+          out.println("    frame " + (isSeekFrame ? "(seek, loaded)" : "(next, loaded)") + " ord=" + ord + " fp=" + f.fp + (f.isFloor ? (" (fpOrig=" + f.fpOrig + ")") : "") + " prefixLen=" + f.prefix + " prefix=" + brToString(prefix) + " nextEnt=" + f.nextEnt + (f.nextEnt == -1 ? "" : (" (of " + f.entCount + ")")) + " hasTerms=" + f.hasTerms + " isFloor=" + f.isFloor + " code=" + ((f.fp<<VersionBlockTreeTermsWriter.OUTPUT_FLAGS_NUM_BITS) + (f.hasTerms ? VersionBlockTreeTermsWriter.OUTPUT_FLAG_HAS_TERMS:0) + (f.isFloor ? VersionBlockTreeTermsWriter.OUTPUT_FLAG_IS_FLOOR:0)) + " lastSubFP=" + f.lastSubFP + " isLastInFloor=" + f.isLastInFloor + " mdUpto=" + f.metaDataUpto + " tbOrd=" + f.getTermBlockOrd());
+        }
+        if (fr.index != null) {
+          assert !isSeekFrame || f.arc != null: "isSeekFrame=" + isSeekFrame + " f.arc=" + f.arc;
+          if (f.prefix > 0 && isSeekFrame && f.arc.label != (term.bytes[f.prefix-1]&0xFF)) {
+            out.println("      broken seek state: arc.label=" + (char) f.arc.label + " vs term byte=" + (char) (term.bytes[f.prefix-1]&0xFF));
+            throw new RuntimeException("seek state is broken");
+          }
+          Pair<BytesRef,Long> output = Util.get(fr.index, prefix);
+          if (output == null) {
+            out.println("      broken seek state: prefix is not final in index");
+            throw new RuntimeException("seek state is broken");
+          } else if (isSeekFrame && !f.isFloor) {
+            final ByteArrayDataInput reader = new ByteArrayDataInput(output.output1.bytes, output.output1.offset, output.output1.length);
+            final long codeOrig = reader.readVLong();
+            final long code = (f.fp << VersionBlockTreeTermsWriter.OUTPUT_FLAGS_NUM_BITS) | (f.hasTerms ? VersionBlockTreeTermsWriter.OUTPUT_FLAG_HAS_TERMS:0) | (f.isFloor ? VersionBlockTreeTermsWriter.OUTPUT_FLAG_IS_FLOOR:0);
+            if (codeOrig != code) {
+              out.println("      broken seek state: output code=" + codeOrig + " doesn't match frame code=" + code);
+              throw new RuntimeException("seek state is broken");
+            }
+          }
+        }
+        if (f == currentFrame) {
+          break;
+        }
+        if (f.prefix == validIndexPrefix) {
+          isSeekFrame = false;
+        }
+        ord++;
+      }
+    }
+  }
+
+  /* Decodes only the term bytes of the next term.  If caller then asks for
+     metadata, ie docFreq, totalTermFreq or pulls a D/&PEnum, we then (lazily)
+     decode all metadata up to the current term. */
+  @Override
+  public BytesRef next() throws IOException {
+
+    if (in == null) {
+      // Fresh TermsEnum; seek to first term:
+      final FST.Arc<Pair<BytesRef,Long>> arc;
+      if (fr.index != null) {
+        arc = fr.index.getFirstArc(arcs[0]);
+        // Empty string prefix must have an output in the index!
+        assert arc.isFinal();
+      } else {
+        arc = null;
+      }
+      currentFrame = pushFrame(arc, fr.rootCode, 0);
+      currentFrame.loadBlock();
+    }
+
+    targetBeforeCurrentLength = currentFrame.ord;
+
+    assert !eof;
+    //if (DEBUG) {
+    //System.out.println("\nBTTR.next seg=" + segment + " term=" + brToString(term) + " termExists?=" + termExists + " field=" + fieldInfo.name + " termBlockOrd=" + currentFrame.state.termBlockOrd + " validIndexPrefix=" + validIndexPrefix);
+    //printSeekState();
+    //}
+
+    if (currentFrame == staticFrame) {
+      // If seek was previously called and the term was
+      // cached, or seek(TermState) was called, usually
+      // caller is just going to pull a D/&PEnum or get
+      // docFreq, etc.  But, if they then call next(),
+      // this method catches up all internal state so next()
+      // works properly:
+      //if (DEBUG) System.out.println("  re-seek to pending term=" + term.utf8ToString() + " " + term);
+      final boolean result = seekExact(term);
+      assert result;
+    }
+
+    // Pop finished blocks
+    while (currentFrame.nextEnt == currentFrame.entCount) {
+      if (!currentFrame.isLastInFloor) {
+        currentFrame.loadNextFloorBlock();
+      } else {
+        //if (DEBUG) System.out.println("  pop frame");
+        if (currentFrame.ord == 0) {
+          //if (DEBUG) System.out.println("  return null");
+          assert setEOF();
+          term.length = 0;
+          validIndexPrefix = 0;
+          currentFrame.rewind();
+          termExists = false;
+          return null;
+        }
+        final long lastFP = currentFrame.fpOrig;
+        currentFrame = stack[currentFrame.ord-1];
+
+        if (currentFrame.nextEnt == -1 || currentFrame.lastSubFP != lastFP) {
+          // We popped into a frame that's not loaded
+          // yet or not scan'd to the right entry
+          currentFrame.scanToFloorFrame(term);
+          currentFrame.loadBlock();
+          currentFrame.scanToSubBlock(lastFP);
+        }
+
+        // Note that the seek state (last seek) has been
+        // invalidated beyond this depth
+        validIndexPrefix = Math.min(validIndexPrefix, currentFrame.prefix);
+        //if (DEBUG) {
+        //System.out.println("  reset validIndexPrefix=" + validIndexPrefix);
+        //}
+      }
+    }
+
+    while(true) {
+      if (currentFrame.next()) {
+        // Push to new block:
+        //if (DEBUG) System.out.println("  push frame");
+        currentFrame = pushFrame(null, currentFrame.lastSubFP, term.length);
+        // This is a "next" frame -- even if it's
+        // floor'd we must pretend it isn't so we don't
+        // try to scan to the right floor frame:
+        currentFrame.isFloor = false;
+        //currentFrame.hasTerms = true;
+        currentFrame.loadBlock();
+      } else {
+        //if (DEBUG) System.out.println("  return term=" + term.utf8ToString() + " " + term + " currentFrame.ord=" + currentFrame.ord);
+        return term;
+      }
+    }
+  }
+
+  @Override
+  public BytesRef term() {
+    assert !eof;
+    return term;
+  }
+
+  @Override
+  public int docFreq() throws IOException {
+    assert !eof;
+    return 1;
+  }
+
+  @Override
+  public long totalTermFreq() throws IOException {
+    assert !eof;
+    return 1;
+  }
+
+  @Override
+  public DocsEnum docs(Bits skipDocs, DocsEnum reuse, int flags) throws IOException {
+    assert !eof;
+    //if (DEBUG) {
+    //System.out.println("BTTR.docs seg=" + segment);
+    //}
+    currentFrame.decodeMetaData();
+    //if (DEBUG) {
+    //System.out.println("  state=" + currentFrame.state);
+    //}
+    return fr.parent.postingsReader.docs(fr.fieldInfo, currentFrame.state, skipDocs, reuse, flags);
+  }
+
+  @Override
+  public DocsAndPositionsEnum docsAndPositions(Bits skipDocs, DocsAndPositionsEnum reuse, int flags) throws IOException {
+    if (fr.fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) < 0) {
+      // Positions were not indexed:
+      return null;
+    }
+
+    assert !eof;
+    currentFrame.decodeMetaData();
+    return fr.parent.postingsReader.docsAndPositions(fr.fieldInfo, currentFrame.state, skipDocs, reuse, flags);
+  }
+
+  @Override
+  public void seekExact(BytesRef target, TermState otherState) {
+    // if (DEBUG) {
+    //   System.out.println("BTTR.seekExact termState seg=" + segment + " target=" + target.utf8ToString() + " " + target + " state=" + otherState);
+    // }
+    assert clearEOF();
+    if (target.compareTo(term) != 0 || !termExists) {
+      assert otherState != null && otherState instanceof BlockTermState;
+      currentFrame = staticFrame;
+      currentFrame.state.copyFrom(otherState);
+      term.copyBytes(target);
+      currentFrame.metaDataUpto = currentFrame.getTermBlockOrd();
+      assert currentFrame.metaDataUpto > 0;
+      validIndexPrefix = 0;
+    } else {
+      // if (DEBUG) {
+      //   System.out.println("  skip seek: already on target state=" + currentFrame.state);
+      // }
+    }
+  }
+      
+  @Override
+  public TermState termState() throws IOException {
+    assert !eof;
+    currentFrame.decodeMetaData();
+    TermState ts = currentFrame.state.clone();
+    //if (DEBUG) System.out.println("BTTR.termState seg=" + segment + " state=" + ts);
+    return ts;
+  }
+
+  @Override
+  public void seekExact(long ord) {
+    throw new UnsupportedOperationException();
+  }
+
+  @Override
+  public long ord() {
+    throw new UnsupportedOperationException();
+  }
+
+  @Override
+  public String toString() {
+    return "IDVersionSegmentTermsEnum(seg=" + fr.parent.segment + ")";
+  }
+}
diff --git a/lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/IDVersionSegmentTermsEnumFrame.java b/lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/IDVersionSegmentTermsEnumFrame.java
new file mode 100644
index 0000000..fa64056
--- /dev/null
+++ b/lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/IDVersionSegmentTermsEnumFrame.java
@@ -0,0 +1,738 @@
+package org.apache.lucene.codecs.idversion;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.BlockTermState;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.TermsEnum.SeekStatus;
+import org.apache.lucene.store.ByteArrayDataInput;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.fst.FST;
+import org.apache.lucene.util.fst.PairOutputs.Pair;
+
+final class IDVersionSegmentTermsEnumFrame {
+  // Our index in stack[]:
+  final int ord;
+
+  boolean hasTerms;
+  boolean hasTermsOrig;
+  boolean isFloor;
+
+  // static boolean DEBUG = IDVersionSegmentTermsEnum.DEBUG;
+
+  /** Highest version of any term in this block. */
+  long maxIDVersion;
+
+  FST.Arc<Pair<BytesRef,Long>> arc;
+
+  // File pointer where this block was loaded from
+  long fp;
+  long fpOrig;
+  long fpEnd;
+
+  byte[] suffixBytes = new byte[128];
+  final ByteArrayDataInput suffixesReader = new ByteArrayDataInput();
+
+  byte[] floorData = new byte[32];
+  final ByteArrayDataInput floorDataReader = new ByteArrayDataInput();
+
+  // Length of prefix shared by all terms in this block
+  int prefix;
+
+  // Number of entries (term or sub-block) in this block
+  int entCount;
+
+  // Which term we will next read, or -1 if the block
+  // isn't loaded yet
+  int nextEnt;
+
+  // True if this block is either not a floor block,
+  // or, it's the last sub-block of a floor block
+  boolean isLastInFloor;
+
+  // True if all entries are terms
+  boolean isLeafBlock;
+
+  long lastSubFP;
+
+  int nextFloorLabel;
+  int numFollowFloorBlocks;
+
+  // Next term to decode metaData; we decode metaData
+  // lazily so that scanning to find the matching term is
+  // fast and only if you find a match and app wants the
+  // stats or docs/positions enums, will we decode the
+  // metaData
+  int metaDataUpto;
+
+  final BlockTermState state;
+
+  // metadata buffer, holding monotonic values
+  public long[] longs;
+  // metadata buffer, holding general values
+  public byte[] bytes;
+  ByteArrayDataInput bytesReader;
+
+  private final IDVersionSegmentTermsEnum ste;
+
+  public IDVersionSegmentTermsEnumFrame(IDVersionSegmentTermsEnum ste, int ord) throws IOException {
+    this.ste = ste;
+    this.ord = ord;
+    this.state = ste.fr.parent.postingsReader.newTermState();
+    this.state.totalTermFreq = -1;
+    this.longs = new long[ste.fr.longsSize];
+  }
+
+  public void setFloorData(ByteArrayDataInput in, BytesRef source) {
+    final int numBytes = source.length - (in.getPosition() - source.offset);
+    if (numBytes > floorData.length) {
+      floorData = new byte[ArrayUtil.oversize(numBytes, 1)];
+    }
+    System.arraycopy(source.bytes, source.offset+in.getPosition(), floorData, 0, numBytes);
+    floorDataReader.reset(floorData, 0, numBytes);
+    numFollowFloorBlocks = floorDataReader.readVInt();
+    nextFloorLabel = floorDataReader.readByte() & 0xff;
+    //if (DEBUG) {
+    //System.out.println("    setFloorData fpOrig=" + fpOrig + " bytes=" + new BytesRef(source.bytes, source.offset + in.getPosition(), numBytes) + " numFollowFloorBlocks=" + numFollowFloorBlocks + " nextFloorLabel=" + toHex(nextFloorLabel));
+    //}
+  }
+
+  public int getTermBlockOrd() {
+    return isLeafBlock ? nextEnt : state.termBlockOrd;
+  }
+
+  void loadNextFloorBlock() throws IOException {
+    //if (DEBUG) {
+    //System.out.println("    loadNextFloorBlock fp=" + fp + " fpEnd=" + fpEnd);
+    //}
+    assert arc == null || isFloor: "arc=" + arc + " isFloor=" + isFloor;
+    fp = fpEnd;
+    nextEnt = -1;
+    loadBlock();
+  }
+
+  /* Does initial decode of next block of terms; this
+     doesn't actually decode the docFreq, totalTermFreq,
+     postings details (frq/prx offset, etc.) metadata;
+     it just loads them as byte[] blobs which are then      
+     decoded on-demand if the metadata is ever requested
+     for any term in this block.  This enables terms-only
+     intensive consumes (eg certain MTQs, respelling) to
+     not pay the price of decoding metadata they won't
+     use. */
+  void loadBlock() throws IOException {
+
+    // Clone the IndexInput lazily, so that consumers
+    // that just pull a TermsEnum to
+    // seekExact(TermState) don't pay this cost:
+    ste.initIndexInput();
+
+    if (nextEnt != -1) {
+      // Already loaded
+      return;
+    }
+    //System.out.println("blc=" + blockLoadCount);
+
+    ste.in.seek(fp);
+    int code = ste.in.readVInt();
+    entCount = code >>> 1;
+    assert entCount > 0;
+    isLastInFloor = (code & 1) != 0;
+    assert arc == null || (isLastInFloor || isFloor);
+
+    // TODO: if suffixes were stored in random-access
+    // array structure, then we could do binary search
+    // instead of linear scan to find target term; eg
+    // we could have simple array of offsets
+
+    // term suffixes:
+    code = ste.in.readVInt();
+    isLeafBlock = (code & 1) != 0;
+    int numBytes = code >>> 1;
+    if (suffixBytes.length < numBytes) {
+      suffixBytes = new byte[ArrayUtil.oversize(numBytes, 1)];
+    }
+    ste.in.readBytes(suffixBytes, 0, numBytes);
+    suffixesReader.reset(suffixBytes, 0, numBytes);
+
+    /*if (DEBUG) {
+      if (arc == null) {
+      System.out.println("    loadBlock (next) fp=" + fp + " entCount=" + entCount + " prefixLen=" + prefix + " isLastInFloor=" + isLastInFloor + " leaf?=" + isLeafBlock);
+      } else {
+      System.out.println("    loadBlock (seek) fp=" + fp + " entCount=" + entCount + " prefixLen=" + prefix + " hasTerms?=" + hasTerms + " isFloor?=" + isFloor + " isLastInFloor=" + isLastInFloor + " leaf?=" + isLeafBlock);
+      }
+      }*/
+
+    metaDataUpto = 0;
+
+    state.termBlockOrd = 0;
+    nextEnt = 0;
+    lastSubFP = -1;
+
+    // TODO: we could skip this if !hasTerms; but
+    // that's rare so won't help much
+    // metadata
+    numBytes = ste.in.readVInt();
+    if (bytes == null) {
+      bytes = new byte[ArrayUtil.oversize(numBytes, 1)];
+      bytesReader = new ByteArrayDataInput();
+    } else if (bytes.length < numBytes) {
+      bytes = new byte[ArrayUtil.oversize(numBytes, 1)];
+    }
+    ste.in.readBytes(bytes, 0, numBytes);
+    bytesReader.reset(bytes, 0, numBytes);
+
+    // Sub-blocks of a single floor block are always
+    // written one after another -- tail recurse:
+    fpEnd = ste.in.getFilePointer();
+    // if (DEBUG) {
+    //   System.out.println("      fpEnd=" + fpEnd);
+    // }
+  }
+
+  void rewind() {
+
+    // Force reload:
+    fp = fpOrig;
+    nextEnt = -1;
+    hasTerms = hasTermsOrig;
+    if (isFloor) {
+      floorDataReader.rewind();
+      numFollowFloorBlocks = floorDataReader.readVInt();
+      nextFloorLabel = floorDataReader.readByte() & 0xff;
+    }
+
+    /*
+    //System.out.println("rewind");
+    // Keeps the block loaded, but rewinds its state:
+    if (nextEnt > 0 || fp != fpOrig) {
+    if (DEBUG) {
+    System.out.println("      rewind frame ord=" + ord + " fpOrig=" + fpOrig + " fp=" + fp + " hasTerms?=" + hasTerms + " isFloor?=" + isFloor + " nextEnt=" + nextEnt + " prefixLen=" + prefix);
+    }
+    if (fp != fpOrig) {
+    fp = fpOrig;
+    nextEnt = -1;
+    } else {
+    nextEnt = 0;
+    }
+    hasTerms = hasTermsOrig;
+    if (isFloor) {
+    floorDataReader.rewind();
+    numFollowFloorBlocks = floorDataReader.readVInt();
+    nextFloorLabel = floorDataReader.readByte() & 0xff;
+    }
+    assert suffixBytes != null;
+    suffixesReader.rewind();
+    assert statBytes != null;
+    statsReader.rewind();
+    metaDataUpto = 0;
+    state.termBlockOrd = 0;
+    // TODO: skip this if !hasTerms?  Then postings
+    // impl wouldn't have to write useless 0 byte
+    postingsReader.resetTermsBlock(fieldInfo, state);
+    lastSubFP = -1;
+    } else if (DEBUG) {
+    System.out.println("      skip rewind fp=" + fp + " fpOrig=" + fpOrig + " nextEnt=" + nextEnt + " ord=" + ord);
+    }
+    */
+  }
+
+  public boolean next() {
+    return isLeafBlock ? nextLeaf() : nextNonLeaf();
+  }
+
+  // Decodes next entry; returns true if it's a sub-block
+  public boolean nextLeaf() {
+    //if (DEBUG) System.out.println("  frame.next ord=" + ord + " nextEnt=" + nextEnt + " entCount=" + entCount);
+    assert nextEnt != -1 && nextEnt < entCount: "nextEnt=" + nextEnt + " entCount=" + entCount + " fp=" + fp;
+    nextEnt++;
+    suffix = suffixesReader.readVInt();
+    startBytePos = suffixesReader.getPosition();
+    ste.term.length = prefix + suffix;
+    if (ste.term.bytes.length < ste.term.length) {
+      ste.term.grow(ste.term.length);
+    }
+    suffixesReader.readBytes(ste.term.bytes, prefix, suffix);
+    // A normal term
+    ste.termExists = true;
+    return false;
+  }
+
+  public boolean nextNonLeaf() {
+    // if (DEBUG) System.out.println("  frame.next ord=" + ord + " nextEnt=" + nextEnt + " entCount=" + entCount);
+    assert nextEnt != -1 && nextEnt < entCount: "nextEnt=" + nextEnt + " entCount=" + entCount + " fp=" + fp;
+    nextEnt++;
+    final int code = suffixesReader.readVInt();
+    suffix = code >>> 1;
+    startBytePos = suffixesReader.getPosition();
+    ste.term.length = prefix + suffix;
+    if (ste.term.bytes.length < ste.term.length) {
+      ste.term.grow(ste.term.length);
+    }
+    suffixesReader.readBytes(ste.term.bytes, prefix, suffix);
+    if ((code & 1) == 0) {
+      // A normal term
+      ste.termExists = true;
+      subCode = 0;
+      state.termBlockOrd++;
+      return false;
+    } else {
+      // A sub-block; make sub-FP absolute:
+      ste.termExists = false;
+      subCode = suffixesReader.readVLong();
+      lastSubFP = fp - subCode;
+      // if (DEBUG) {
+      //   System.out.println("    lastSubFP=" + lastSubFP);
+      // }
+      return true;
+    }
+  }
+        
+  // TODO: make this array'd so we can do bin search?
+  // likely not worth it?  need to measure how many
+  // floor blocks we "typically" get
+  public void scanToFloorFrame(BytesRef target) {
+
+    if (!isFloor || target.length <= prefix) {
+      // if (DEBUG) {
+      //    System.out.println("    scanToFloorFrame skip: isFloor=" + isFloor + " target.length=" + target.length + " vs prefix=" + prefix);
+      //  }
+      return;
+    }
+
+    final int targetLabel = target.bytes[target.offset + prefix] & 0xFF;
+
+    // if (DEBUG) {
+    //    System.out.println("    scanToFloorFrame fpOrig=" + fpOrig + " targetLabel=" + ((char) targetLabel) + " vs nextFloorLabel=" + ((char) nextFloorLabel) + " numFollowFloorBlocks=" + numFollowFloorBlocks);
+    //  }
+
+    if (targetLabel < nextFloorLabel) {
+      // if (DEBUG) {
+      //    System.out.println("      already on correct block");
+      //  }
+      return;
+    }
+
+    assert numFollowFloorBlocks != 0;
+
+    long newFP = fpOrig;
+    while (true) {
+      final long code = floorDataReader.readVLong();
+      newFP = fpOrig + (code >>> 1);
+      hasTerms = (code & 1) != 0;
+      // if (DEBUG) {
+      //    System.out.println("      label=" + ((char) nextFloorLabel) + " fp=" + newFP + " hasTerms?=" + hasTerms + " numFollowFloor=" + numFollowFloorBlocks);
+      //  }
+            
+      isLastInFloor = numFollowFloorBlocks == 1;
+      numFollowFloorBlocks--;
+
+      if (isLastInFloor) {
+        nextFloorLabel = 256;
+        // if (DEBUG) {
+        //    System.out.println("        stop!  last block nextFloorLabel=" + ((char) nextFloorLabel));
+        //  }
+        break;
+      } else {
+        nextFloorLabel = floorDataReader.readByte() & 0xff;
+        if (targetLabel < nextFloorLabel) {
+          // if (DEBUG) {
+          //    System.out.println("        stop!  nextFloorLabel=" + ((char) nextFloorLabel));
+          //  }
+          break;
+        }
+      }
+    }
+
+    if (newFP != fp) {
+      // Force re-load of the block:
+      // if (DEBUG) {
+      //    System.out.println("      force switch to fp=" + newFP + " oldFP=" + fp);
+      //  }
+      nextEnt = -1;
+      fp = newFP;
+    } else {
+      // if (DEBUG) {
+      //    System.out.println("      stay on same fp=" + newFP);
+      //  }
+    }
+  }
+    
+  public void decodeMetaData() throws IOException {
+
+    //if (DEBUG) System.out.println("\nBTTR.decodeMetadata seg=" + ste.fr.parent.segment + " mdUpto=" + metaDataUpto + " vs termBlockOrd=" + state.termBlockOrd);
+
+    assert nextEnt >= 0;
+
+    // lazily catch up on metadata decode:
+    final int limit = getTermBlockOrd();
+    boolean absolute = metaDataUpto == 0;
+
+    // TODO: better API would be "jump straight to term=N"???
+    while (metaDataUpto < limit) {
+
+      // TODO: we could make "tiers" of metadata, ie,
+      // decode docFreq/totalTF but don't decode postings
+      // metadata; this way caller could get
+      // docFreq/totalTF w/o paying decode cost for
+      // postings
+
+      // TODO: if docFreq were bulk decoded we could
+      // just skipN here:
+
+      // stats
+      state.docFreq = 1;
+      state.totalTermFreq = 1;
+      //if (DEBUG) System.out.println("    dF=" + state.docFreq);
+      // metadata 
+      for (int i = 0; i < ste.fr.longsSize; i++) {
+        longs[i] = bytesReader.readVLong();
+      }
+      ste.fr.parent.postingsReader.decodeTerm(longs, bytesReader, ste.fr.fieldInfo, state, absolute);
+
+      metaDataUpto++;
+      absolute = false;
+    }
+    state.termBlockOrd = metaDataUpto;
+  }
+
+  // Used only by assert
+  private boolean prefixMatches(BytesRef target) {
+    for(int bytePos=0;bytePos<prefix;bytePos++) {
+      if (target.bytes[target.offset + bytePos] != ste.term.bytes[bytePos]) {
+        return false;
+      }
+    }
+
+    return true;
+  }
+
+  // Scans to sub-block that has this target fp; only
+  // called by next(); NOTE: does not set
+  // startBytePos/suffix as a side effect
+  public void scanToSubBlock(long subFP) {
+    assert !isLeafBlock;
+    //if (DEBUG) System.out.println("  scanToSubBlock fp=" + fp + " subFP=" + subFP + " entCount=" + entCount + " lastSubFP=" + lastSubFP);
+    //assert nextEnt == 0;
+    if (lastSubFP == subFP) {
+      //if (DEBUG) System.out.println("    already positioned");
+      return;
+    }
+    assert subFP < fp : "fp=" + fp + " subFP=" + subFP;
+    final long targetSubCode = fp - subFP;
+    //if (DEBUG) System.out.println("    targetSubCode=" + targetSubCode);
+    while(true) {
+      assert nextEnt < entCount;
+      nextEnt++;
+      final int code = suffixesReader.readVInt();
+      suffixesReader.skipBytes(isLeafBlock ? code : code >>> 1);
+      //if (DEBUG) System.out.println("    " + nextEnt + " (of " + entCount + ") ent isSubBlock=" + ((code&1)==1));
+      if ((code & 1) != 0) {
+        final long subCode = suffixesReader.readVLong();
+        //if (DEBUG) System.out.println("      subCode=" + subCode);
+        if (targetSubCode == subCode) {
+          //if (DEBUG) System.out.println("        match!");
+          lastSubFP = subFP;
+          return;
+        }
+      } else {
+        state.termBlockOrd++;
+      }
+    }
+  }
+
+  // NOTE: sets startBytePos/suffix as a side effect
+  public SeekStatus scanToTerm(BytesRef target, boolean exactOnly) throws IOException {
+    return isLeafBlock ? scanToTermLeaf(target, exactOnly) : scanToTermNonLeaf(target, exactOnly);
+  }
+
+  private int startBytePos;
+  private int suffix;
+  private long subCode;
+
+  // Target's prefix matches this block's prefix; we
+  // scan the entries check if the suffix matches.
+  public SeekStatus scanToTermLeaf(BytesRef target, boolean exactOnly) throws IOException {
+
+    // if (DEBUG) System.out.println("    scanToTermLeaf: block fp=" + fp + " prefix=" + prefix + " nextEnt=" + nextEnt + " (of " + entCount + ") target=" + IDVersionSegmentTermsEnum.brToString(target) + " term=" + IDVersionSegmentTermsEnum.brToString(ste.term));
+
+    assert nextEnt != -1;
+
+    ste.termExists = true;
+    subCode = 0;
+
+    if (nextEnt == entCount) {
+      if (exactOnly) {
+        fillTerm();
+      }
+      return SeekStatus.END;
+    }
+
+    assert prefixMatches(target);
+
+    // Loop over each entry (term or sub-block) in this block:
+    //nextTerm: while(nextEnt < entCount) {
+    nextTerm: while (true) {
+      nextEnt++;
+
+      suffix = suffixesReader.readVInt();
+
+      // if (DEBUG) {
+      //    BytesRef suffixBytesRef = new BytesRef();
+      //    suffixBytesRef.bytes = suffixBytes;
+      //    suffixBytesRef.offset = suffixesReader.getPosition();
+      //    suffixBytesRef.length = suffix;
+      //    System.out.println("      cycle: term " + (nextEnt-1) + " (of " + entCount + ") suffix=" + IDVersionSegmentTermsEnum.brToString(suffixBytesRef));
+      // }
+
+      final int termLen = prefix + suffix;
+      startBytePos = suffixesReader.getPosition();
+      suffixesReader.skipBytes(suffix);
+
+      final int targetLimit = target.offset + (target.length < termLen ? target.length : termLen);
+      int targetPos = target.offset + prefix;
+
+      // Loop over bytes in the suffix, comparing to
+      // the target
+      int bytePos = startBytePos;
+      while(true) {
+        final int cmp;
+        final boolean stop;
+        if (targetPos < targetLimit) {
+          cmp = (suffixBytes[bytePos++]&0xFF) - (target.bytes[targetPos++]&0xFF);
+          stop = false;
+        } else {
+          assert targetPos == targetLimit;
+          cmp = termLen - target.length;
+          stop = true;
+        }
+
+        if (cmp < 0) {
+          // Current entry is still before the target;
+          // keep scanning
+
+          if (nextEnt == entCount) {
+            if (exactOnly) {
+              fillTerm();
+            }
+            // We are done scanning this block
+            break nextTerm;
+          } else {
+            continue nextTerm;
+          }
+        } else if (cmp > 0) {
+
+          // Done!  Current entry is after target --
+          // return NOT_FOUND:
+          fillTerm();
+
+          if (!exactOnly && !ste.termExists) {
+            // We are on a sub-block, and caller wants
+            // us to position to the next term after
+            // the target, so we must recurse into the
+            // sub-frame(s):
+            ste.currentFrame = ste.pushFrame(null, ste.currentFrame.lastSubFP, termLen);
+            ste.currentFrame.loadBlock();
+            while (ste.currentFrame.next()) {
+              ste.currentFrame = ste.pushFrame(null, ste.currentFrame.lastSubFP, ste.term.length);
+              ste.currentFrame.loadBlock();
+            }
+          }
+                
+          //if (DEBUG) System.out.println("        not found");
+          return SeekStatus.NOT_FOUND;
+        } else if (stop) {
+          // Exact match!
+
+          // This cannot be a sub-block because we
+          // would have followed the index to this
+          // sub-block from the start:
+
+          assert ste.termExists;
+          fillTerm();
+          //if (DEBUG) System.out.println("        found!");
+          return SeekStatus.FOUND;
+        }
+      }
+    }
+
+    // It is possible (and OK) that terms index pointed us
+    // at this block, but, we scanned the entire block and
+    // did not find the term to position to.  This happens
+    // when the target is after the last term in the block
+    // (but, before the next term in the index).  EG
+    // target could be foozzz, and terms index pointed us
+    // to the foo* block, but the last term in this block
+    // was fooz (and, eg, first term in the next block will
+    // bee fop).
+    //if (DEBUG) System.out.println("      block end");
+    if (exactOnly) {
+      fillTerm();
+    }
+
+    // TODO: not consistent that in the
+    // not-exact case we don't next() into the next
+    // frame here
+    return SeekStatus.END;
+  }
+
+  // Target's prefix matches this block's prefix; we
+  // scan the entries check if the suffix matches.
+  public SeekStatus scanToTermNonLeaf(BytesRef target, boolean exactOnly) throws IOException {
+
+    // if (DEBUG) System.out.println("    scanToTermNonLeaf: block fp=" + fp + " prefix=" + prefix + " nextEnt=" + nextEnt + " (of " + entCount + ") target=" + IDVersionSegmentTermsEnum.brToString(target) + " term=" + IDVersionSegmentTermsEnum.brToString(ste.term));
+
+    assert nextEnt != -1;
+
+    if (nextEnt == entCount) {
+      if (exactOnly) {
+        fillTerm();
+        ste.termExists = subCode == 0;
+      }
+      return SeekStatus.END;
+    }
+
+    assert prefixMatches(target);
+
+    // Loop over each entry (term or sub-block) in this block:
+    //nextTerm: while(nextEnt < entCount) {
+    nextTerm: while (true) {
+      nextEnt++;
+
+      final int code = suffixesReader.readVInt();
+      suffix = code >>> 1;
+      // if (DEBUG) {
+      //   BytesRef suffixBytesRef = new BytesRef();
+      //   suffixBytesRef.bytes = suffixBytes;
+      //   suffixBytesRef.offset = suffixesReader.getPosition();
+      //   suffixBytesRef.length = suffix;
+      //   System.out.println("      cycle: " + ((code&1)==1 ? "sub-block" : "term") + " " + (nextEnt-1) + " (of " + entCount + ") suffix=" + brToString(suffixBytesRef));
+      // }
+
+      ste.termExists = (code & 1) == 0;
+      final int termLen = prefix + suffix;
+      startBytePos = suffixesReader.getPosition();
+      suffixesReader.skipBytes(suffix);
+      if (ste.termExists) {
+        state.termBlockOrd++;
+        subCode = 0;
+      } else {
+        subCode = suffixesReader.readVLong();
+        lastSubFP = fp - subCode;
+      }
+
+      final int targetLimit = target.offset + (target.length < termLen ? target.length : termLen);
+      int targetPos = target.offset + prefix;
+
+      // Loop over bytes in the suffix, comparing to
+      // the target
+      int bytePos = startBytePos;
+      while(true) {
+        final int cmp;
+        final boolean stop;
+        if (targetPos < targetLimit) {
+          cmp = (suffixBytes[bytePos++]&0xFF) - (target.bytes[targetPos++]&0xFF);
+          stop = false;
+        } else {
+          assert targetPos == targetLimit;
+          cmp = termLen - target.length;
+          stop = true;
+        }
+
+        if (cmp < 0) {
+          // Current entry is still before the target;
+          // keep scanning
+
+          if (nextEnt == entCount) {
+            if (exactOnly) {
+              fillTerm();
+              //termExists = true;
+            }
+            // We are done scanning this block
+            break nextTerm;
+          } else {
+            continue nextTerm;
+          }
+        } else if (cmp > 0) {
+
+          // Done!  Current entry is after target --
+          // return NOT_FOUND:
+          fillTerm();
+
+          if (!exactOnly && !ste.termExists) {
+            // We are on a sub-block, and caller wants
+            // us to position to the next term after
+            // the target, so we must recurse into the
+            // sub-frame(s):
+            ste.currentFrame = ste.pushFrame(null, ste.currentFrame.lastSubFP, termLen);
+            ste.currentFrame.loadBlock();
+            while (ste.currentFrame.next()) {
+              ste.currentFrame = ste.pushFrame(null, ste.currentFrame.lastSubFP, ste.term.length);
+              ste.currentFrame.loadBlock();
+            }
+          }
+                
+          //if (DEBUG) System.out.println("        not found");
+          return SeekStatus.NOT_FOUND;
+        } else if (stop) {
+          // Exact match!
+
+          // This cannot be a sub-block because we
+          // would have followed the index to this
+          // sub-block from the start:
+
+          assert ste.termExists;
+          fillTerm();
+          //if (DEBUG) System.out.println("        found!");
+          return SeekStatus.FOUND;
+        }
+      }
+    }
+
+    // It is possible (and OK) that terms index pointed us
+    // at this block, but, we scanned the entire block and
+    // did not find the term to position to.  This happens
+    // when the target is after the last term in the block
+    // (but, before the next term in the index).  EG
+    // target could be foozzz, and terms index pointed us
+    // to the foo* block, but the last term in this block
+    // was fooz (and, eg, first term in the next block will
+    // bee fop).
+    //if (DEBUG) System.out.println("      block end");
+    if (exactOnly) {
+      fillTerm();
+    }
+
+    // TODO: not consistent that in the
+    // not-exact case we don't next() into the next
+    // frame here
+    return SeekStatus.END;
+  }
+
+  private void fillTerm() {
+    final int termLength = prefix + suffix;
+    ste.term.length = prefix + suffix;
+    if (ste.term.bytes.length < termLength) {
+      ste.term.grow(termLength);
+    }
+    System.arraycopy(suffixBytes, startBytePos, ste.term.bytes, prefix, suffix);
+  }
+}
diff --git a/lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/IDVersionTermState.java b/lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/IDVersionTermState.java
new file mode 100644
index 0000000..227f3f8
--- /dev/null
+++ b/lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/IDVersionTermState.java
@@ -0,0 +1,41 @@
+package org.apache.lucene.codecs.idversion;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.codecs.BlockTermState;
+import org.apache.lucene.index.TermState;
+
+final class IDVersionTermState extends BlockTermState {
+  long idVersion;
+  int docID;
+
+  @Override
+  public IDVersionTermState clone() {
+    IDVersionTermState other = new IDVersionTermState();
+    other.copyFrom(this);
+    return other;
+  }
+
+  @Override
+  public void copyFrom(TermState _other) {
+    super.copyFrom(_other);
+    IDVersionTermState other = (IDVersionTermState) _other;
+    idVersion = other.idVersion;
+    docID = other.docID;
+  }
+}
diff --git a/lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/SingleDocsAndPositionsEnum.java b/lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/SingleDocsAndPositionsEnum.java
new file mode 100644
index 0000000..eecc700
--- /dev/null
+++ b/lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/SingleDocsAndPositionsEnum.java
@@ -0,0 +1,105 @@
+package org.apache.lucene.codecs.idversion;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.index.DocsAndPositionsEnum;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+
+class SingleDocsAndPositionsEnum extends DocsAndPositionsEnum {
+  private int doc;
+  private int pos;
+  private int singleDocID;
+  private Bits liveDocs;
+  private long version;
+  private final BytesRef payload;
+
+  public SingleDocsAndPositionsEnum() {
+    payload = new BytesRef(8);
+    payload.length = 8;
+  }
+
+  /** For reuse */
+  public void reset(int singleDocID, long version, Bits liveDocs) {
+    doc = -1;
+    this.liveDocs = liveDocs;
+    this.singleDocID = singleDocID;
+    this.version = version;
+  }
+
+  @Override
+  public int nextDoc() {
+    if (doc == -1 && (liveDocs == null || liveDocs.get(singleDocID))) {
+      doc = singleDocID;
+    } else {
+      doc = NO_MORE_DOCS;
+    }
+    pos = -1;
+    
+    return doc;
+  }
+
+  @Override
+  public int docID() {
+    return doc;
+  }
+
+  @Override
+  public int advance(int target) {
+    if (doc == -1 && target <= singleDocID && (liveDocs == null || liveDocs.get(singleDocID))) {
+      doc = singleDocID;
+      pos = -1;
+    } else {
+      doc = NO_MORE_DOCS;
+    }
+    return doc;
+  }
+
+  @Override
+  public long cost() {
+    return 1;
+  }
+
+  @Override
+  public int freq() {
+    return 1;
+  }
+
+  @Override
+  public int nextPosition() {
+    assert pos == -1;
+    pos = 0;
+    IDVersionPostingsFormat.longToBytes(version, payload);
+    return pos;
+  }
+
+  @Override
+  public BytesRef getPayload() {
+    return payload;
+  }
+
+  @Override
+  public int startOffset() {
+    return -1;
+  }
+
+  @Override
+  public int endOffset() {
+    return -1;
+  }
+}
diff --git a/lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/SingleDocsEnum.java b/lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/SingleDocsEnum.java
new file mode 100644
index 0000000..b29619c
--- /dev/null
+++ b/lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/SingleDocsEnum.java
@@ -0,0 +1,71 @@
+package org.apache.lucene.codecs.idversion;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.util.Bits;
+
+class SingleDocsEnum extends DocsEnum {
+
+  private int doc;
+  private int singleDocID;
+  private Bits liveDocs;
+
+  /** For reuse */
+  public void reset(int singleDocID, Bits liveDocs) {
+    doc = -1;
+    this.liveDocs = liveDocs;
+    this.singleDocID = singleDocID;
+  }
+
+  @Override
+  public int nextDoc() {
+    if (doc == -1 && (liveDocs == null || liveDocs.get(singleDocID))) {
+      doc = singleDocID;
+    } else {
+      doc = NO_MORE_DOCS;
+    }
+    
+    return doc;
+  }
+
+  @Override
+  public int docID() {
+    return doc;
+  }
+
+  @Override
+  public int advance(int target) {
+    if (doc == -1 && target <= singleDocID && (liveDocs == null || liveDocs.get(singleDocID))) {
+      doc = singleDocID;
+    } else {
+      doc = NO_MORE_DOCS;
+    }
+    return doc;
+  }
+
+  @Override
+  public long cost() {
+    return 1;
+  }
+
+  @Override
+  public int freq() {
+    return 1;
+  }
+}
diff --git a/lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/VersionBlockTreeTermsReader.java b/lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/VersionBlockTreeTermsReader.java
new file mode 100644
index 0000000..b4e69e1
--- /dev/null
+++ b/lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/VersionBlockTreeTermsReader.java
@@ -0,0 +1,274 @@
+package org.apache.lucene.codecs.idversion;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.io.PrintStream;
+import java.util.Collections;
+import java.util.Iterator;
+import java.util.TreeMap;
+
+import org.apache.lucene.codecs.BlockTermState;
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.FieldsProducer;
+import org.apache.lucene.codecs.PostingsReaderBase;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.DocsAndPositionsEnum;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.TermState;
+import org.apache.lucene.index.Terms;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.store.ByteArrayDataInput;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.RamUsageEstimator;
+import org.apache.lucene.util.StringHelper;
+import org.apache.lucene.util.automaton.CompiledAutomaton;
+import org.apache.lucene.util.automaton.RunAutomaton;
+import org.apache.lucene.util.automaton.Transition;
+import org.apache.lucene.util.fst.ByteSequenceOutputs;
+import org.apache.lucene.util.fst.FST;
+import org.apache.lucene.util.fst.Outputs;
+import org.apache.lucene.util.fst.PairOutputs.Pair;
+import org.apache.lucene.util.fst.PairOutputs;
+import org.apache.lucene.util.fst.Util;
+
+/**
+ * See {@link VersionBlockTreeTermsWriter}.
+ *
+ * @lucene.experimental
+ */
+
+final class VersionBlockTreeTermsReader extends FieldsProducer {
+
+  // Open input to the main terms dict file (_X.tiv)
+  final IndexInput in;
+
+  //private static final boolean DEBUG = BlockTreeTermsWriter.DEBUG;
+
+  // Reads the terms dict entries, to gather state to
+  // produce DocsEnum on demand
+  final PostingsReaderBase postingsReader;
+
+  private final TreeMap<String,VersionFieldReader> fields = new TreeMap<>();
+
+  /** File offset where the directory starts in the terms file. */
+  private long dirOffset;
+
+  /** File offset where the directory starts in the index file. */
+  private long indexDirOffset;
+
+  final String segment;
+  
+  private final int version;
+
+  /** Sole constructor. */
+  public VersionBlockTreeTermsReader(Directory dir, FieldInfos fieldInfos, SegmentInfo info,
+                                     PostingsReaderBase postingsReader, IOContext ioContext,
+                                     String segmentSuffix)
+    throws IOException {
+    
+    this.postingsReader = postingsReader;
+
+    this.segment = info.name;
+    in = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, VersionBlockTreeTermsWriter.TERMS_EXTENSION),
+                       ioContext);
+
+    boolean success = false;
+    IndexInput indexIn = null;
+
+    try {
+      version = readHeader(in);
+      indexIn = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, VersionBlockTreeTermsWriter.TERMS_INDEX_EXTENSION),
+                                ioContext);
+      int indexVersion = readIndexHeader(indexIn);
+      if (indexVersion != version) {
+        throw new CorruptIndexException("mixmatched version files: " + in + "=" + version + "," + indexIn + "=" + indexVersion);
+      }
+      
+      // verify
+      CodecUtil.checksumEntireFile(indexIn);
+
+      // Have PostingsReader init itself
+      postingsReader.init(in);
+
+      // Read per-field details
+      seekDir(in, dirOffset);
+      seekDir(indexIn, indexDirOffset);
+
+      final int numFields = in.readVInt();
+      if (numFields < 0) {
+        throw new CorruptIndexException("invalid numFields: " + numFields + " (resource=" + in + ")");
+      }
+
+      for(int i=0;i<numFields;i++) {
+        final int field = in.readVInt();
+        final long numTerms = in.readVLong();
+        assert numTerms >= 0;
+        final int numBytes = in.readVInt();
+        final BytesRef code = new BytesRef(new byte[numBytes]);
+        in.readBytes(code.bytes, 0, numBytes);
+        code.length = numBytes;
+        final long version = in.readVLong();
+        final Pair<BytesRef,Long> rootCode = VersionBlockTreeTermsWriter.FST_OUTPUTS.newPair(code, version);
+        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);
+        assert fieldInfo != null: "field=" + field;
+        final long sumTotalTermFreq = numTerms;
+        final long sumDocFreq = numTerms;
+        assert numTerms <= Integer.MAX_VALUE;
+        final int docCount = (int) numTerms;
+        final int longsSize = in.readVInt();
+
+        BytesRef minTerm = readBytesRef(in);
+        BytesRef maxTerm = readBytesRef(in);
+        if (docCount < 0 || docCount > info.getDocCount()) { // #docs with field must be <= #docs
+          throw new CorruptIndexException("invalid docCount: " + docCount + " maxDoc: " + info.getDocCount() + " (resource=" + in + ")");
+        }
+        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field
+          throw new CorruptIndexException("invalid sumDocFreq: " + sumDocFreq + " docCount: " + docCount + " (resource=" + in + ")");
+        }
+        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings
+          throw new CorruptIndexException("invalid sumTotalTermFreq: " + sumTotalTermFreq + " sumDocFreq: " + sumDocFreq + " (resource=" + in + ")");
+        }
+        final long indexStartFP = indexIn.readVLong();
+        VersionFieldReader previous = fields.put(fieldInfo.name,       
+                                                 new VersionFieldReader(this, fieldInfo, numTerms, rootCode, sumTotalTermFreq, sumDocFreq, docCount,
+                                                                        indexStartFP, longsSize, indexIn, minTerm, maxTerm));
+        if (previous != null) {
+          throw new CorruptIndexException("duplicate field: " + fieldInfo.name + " (resource=" + in + ")");
+        }
+      }
+      indexIn.close();
+
+      success = true;
+    } finally {
+      if (!success) {
+        // this.close() will close in:
+        IOUtils.closeWhileHandlingException(indexIn, this);
+      }
+    }
+  }
+
+  private static BytesRef readBytesRef(IndexInput in) throws IOException {
+    BytesRef bytes = new BytesRef();
+    bytes.length = in.readVInt();
+    bytes.bytes = new byte[bytes.length];
+    in.readBytes(bytes.bytes, 0, bytes.length);
+    return bytes;
+  }
+
+  /** Reads terms file header. */
+  private int readHeader(IndexInput input) throws IOException {
+    int version = CodecUtil.checkHeader(input, VersionBlockTreeTermsWriter.TERMS_CODEC_NAME,
+                          VersionBlockTreeTermsWriter.VERSION_START,
+                          VersionBlockTreeTermsWriter.VERSION_CURRENT);
+    return version;
+  }
+
+  /** Reads index file header. */
+  private int readIndexHeader(IndexInput input) throws IOException {
+    int version = CodecUtil.checkHeader(input, VersionBlockTreeTermsWriter.TERMS_INDEX_CODEC_NAME,
+                          VersionBlockTreeTermsWriter.VERSION_START,
+                          VersionBlockTreeTermsWriter.VERSION_CURRENT);
+    return version;
+  }
+
+  /** Seek {@code input} to the directory offset. */
+  private void seekDir(IndexInput input, long dirOffset)
+      throws IOException {
+    input.seek(input.length() - CodecUtil.footerLength() - 8);
+    dirOffset = input.readLong();
+    input.seek(dirOffset);
+  }
+
+  // for debugging
+  // private static String toHex(int v) {
+  //   return "0x" + Integer.toHexString(v);
+  // }
+
+  @Override
+  public void close() throws IOException {
+    try {
+      IOUtils.close(in, postingsReader);
+    } finally { 
+      // Clear so refs to terms index is GCable even if
+      // app hangs onto us:
+      fields.clear();
+    }
+  }
+
+  @Override
+  public Iterator<String> iterator() {
+    return Collections.unmodifiableSet(fields.keySet()).iterator();
+  }
+
+  @Override
+  public Terms terms(String field) throws IOException {
+    assert field != null;
+    return fields.get(field);
+  }
+
+  @Override
+  public int size() {
+    return fields.size();
+  }
+
+  // for debugging
+  String brToString(BytesRef b) {
+    if (b == null) {
+      return "null";
+    } else {
+      try {
+        return b.utf8ToString() + " " + b;
+      } catch (Throwable t) {
+        // If BytesRef isn't actually UTF8, or it's eg a
+        // prefix of UTF8 that ends mid-unicode-char, we
+        // fallback to hex:
+        return b.toString();
+      }
+    }
+  }
+
+  @Override
+  public long ramBytesUsed() {
+    long sizeInByes = ((postingsReader!=null) ? postingsReader.ramBytesUsed() : 0);
+    for(VersionFieldReader reader : fields.values()) {
+      sizeInByes += reader.ramBytesUsed();
+    }
+    return sizeInByes;
+  }
+
+  @Override
+  public void checkIntegrity() throws IOException {
+    // term dictionary
+    CodecUtil.checksumEntireFile(in);
+      
+    // postings
+    postingsReader.checkIntegrity();
+  }
+}
diff --git a/lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/VersionBlockTreeTermsWriter.java b/lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/VersionBlockTreeTermsWriter.java
new file mode 100644
index 0000000..24b656f
--- /dev/null
+++ b/lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/VersionBlockTreeTermsWriter.java
@@ -0,0 +1,1032 @@
+package org.apache.lucene.codecs.idversion;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.lucene.codecs.BlockTermState;
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.FieldsConsumer;
+import org.apache.lucene.codecs.PostingsWriterBase;
+import org.apache.lucene.codecs.blocktree.BlockTreeTermsWriter;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.Fields;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.index.Terms;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.store.RAMOutputStream;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.FixedBitSet;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.IntsRef;
+import org.apache.lucene.util.fst.Builder;
+import org.apache.lucene.util.fst.ByteSequenceOutputs;
+import org.apache.lucene.util.fst.BytesRefFSTEnum;
+import org.apache.lucene.util.fst.FST;
+import org.apache.lucene.util.fst.NoOutputs;
+import org.apache.lucene.util.fst.PairOutputs.Pair;
+import org.apache.lucene.util.fst.PairOutputs;
+import org.apache.lucene.util.fst.PositiveIntOutputs;
+import org.apache.lucene.util.fst.Util;
+import org.apache.lucene.util.packed.PackedInts;
+
+/*
+  TODO:
+  
+    - Currently there is a one-to-one mapping of indexed
+      term to term block, but we could decouple the two, ie,
+      put more terms into the index than there are blocks.
+      The index would take up more RAM but then it'd be able
+      to avoid seeking more often and could make PK/FuzzyQ
+      faster if the additional indexed terms could store
+      the offset into the terms block.
+
+    - The blocks are not written in true depth-first
+      order, meaning if you just next() the file pointer will
+      sometimes jump backwards.  For example, block foo* will
+      be written before block f* because it finished before.
+      This could possibly hurt performance if the terms dict is
+      not hot, since OSs anticipate sequential file access.  We
+      could fix the writer to re-order the blocks as a 2nd
+      pass.
+
+    - Each block encodes the term suffixes packed
+      sequentially using a separate vInt per term, which is
+      1) wasteful and 2) slow (must linear scan to find a
+      particular suffix).  We should instead 1) make
+      random-access array so we can directly access the Nth
+      suffix, and 2) bulk-encode this array using bulk int[]
+      codecs; then at search time we can binary search when
+      we seek a particular term.
+*/
+
+/**
+ * This is just like {@link BlockTreeTermsWriter}, except it also stores a version per term, and adds a method to its TermsEnum
+ * implementation to seekExact only if the version is >= the specified version.  The version is added to the terms index to avoid seeking if
+ * no term in the block has a high enough version.  The term blocks file is .tiv and the terms index extension is .tipv.
+ *
+ * @lucene.experimental
+ */
+
+final class VersionBlockTreeTermsWriter extends FieldsConsumer {
+
+  // private static boolean DEBUG = IDVersionSegmentTermsEnum.DEBUG;
+
+  static final PairOutputs<BytesRef,Long> FST_OUTPUTS = new PairOutputs<>(ByteSequenceOutputs.getSingleton(),
+                                                                          PositiveIntOutputs.getSingleton());
+
+  static final Pair<BytesRef,Long> NO_OUTPUT = FST_OUTPUTS.getNoOutput();
+
+  /** Suggested default value for the {@code
+   *  minItemsInBlock} parameter to {@link
+   *  #VersionBlockTreeTermsWriter(SegmentWriteState,PostingsWriterBase,int,int)}. */
+  public final static int DEFAULT_MIN_BLOCK_SIZE = 25;
+
+  /** Suggested default value for the {@code
+   *  maxItemsInBlock} parameter to {@link
+   *  #VersionBlockTreeTermsWriter(SegmentWriteState,PostingsWriterBase,int,int)}. */
+  public final static int DEFAULT_MAX_BLOCK_SIZE = 48;
+
+  //public final static boolean DEBUG = false;
+  //private final static boolean SAVE_DOT_FILES = false;
+
+  static final int OUTPUT_FLAGS_NUM_BITS = 2;
+  static final int OUTPUT_FLAGS_MASK = 0x3;
+  static final int OUTPUT_FLAG_IS_FLOOR = 0x1;
+  static final int OUTPUT_FLAG_HAS_TERMS = 0x2;
+
+  /** Extension of terms file */
+  static final String TERMS_EXTENSION = "tiv";
+  final static String TERMS_CODEC_NAME = "VERSION_BLOCK_TREE_TERMS_DICT";
+
+  /** Initial terms format. */
+  public static final int VERSION_START = 0;
+
+  /** Current terms format. */
+  public static final int VERSION_CURRENT = VERSION_START;
+
+  /** Extension of terms index file */
+  static final String TERMS_INDEX_EXTENSION = "tipv";
+  final static String TERMS_INDEX_CODEC_NAME = "VERSION_BLOCK_TREE_TERMS_INDEX";
+
+  private final IndexOutput out;
+  private final IndexOutput indexOut;
+  final int maxDoc;
+  final int minItemsInBlock;
+  final int maxItemsInBlock;
+
+  final PostingsWriterBase postingsWriter;
+  final FieldInfos fieldInfos;
+
+  private static class FieldMetaData {
+    public final FieldInfo fieldInfo;
+    public final Pair<BytesRef,Long> rootCode;
+    public final long numTerms;
+    public final long indexStartFP;
+    private final int longsSize;
+    public final BytesRef minTerm;
+    public final BytesRef maxTerm;
+
+    public FieldMetaData(FieldInfo fieldInfo, Pair<BytesRef,Long> rootCode, long numTerms, long indexStartFP, int longsSize,
+                         BytesRef minTerm, BytesRef maxTerm) {
+      assert numTerms > 0;
+      this.fieldInfo = fieldInfo;
+      assert rootCode != null: "field=" + fieldInfo.name + " numTerms=" + numTerms;
+      this.rootCode = rootCode;
+      this.indexStartFP = indexStartFP;
+      this.numTerms = numTerms;
+      this.longsSize = longsSize;
+      this.minTerm = minTerm;
+      this.maxTerm = maxTerm;
+    }
+  }
+
+  private final List<FieldMetaData> fields = new ArrayList<>();
+  // private final String segment;
+
+  /** Create a new writer.  The number of items (terms or
+   *  sub-blocks) per block will aim to be between
+   *  minItemsPerBlock and maxItemsPerBlock, though in some
+   *  cases the blocks may be smaller than the min. */
+  public VersionBlockTreeTermsWriter(
+                                     SegmentWriteState state,
+                                     PostingsWriterBase postingsWriter,
+                                     int minItemsInBlock,
+                                     int maxItemsInBlock)
+    throws IOException
+  {
+    if (minItemsInBlock <= 1) {
+      throw new IllegalArgumentException("minItemsInBlock must be >= 2; got " + minItemsInBlock);
+    }
+    if (maxItemsInBlock <= 0) {
+      throw new IllegalArgumentException("maxItemsInBlock must be >= 1; got " + maxItemsInBlock);
+    }
+    if (minItemsInBlock > maxItemsInBlock) {
+      throw new IllegalArgumentException("maxItemsInBlock must be >= minItemsInBlock; got maxItemsInBlock=" + maxItemsInBlock + " minItemsInBlock=" + minItemsInBlock);
+    }
+    if (2*(minItemsInBlock-1) > maxItemsInBlock) {
+      throw new IllegalArgumentException("maxItemsInBlock must be at least 2*(minItemsInBlock-1); got maxItemsInBlock=" + maxItemsInBlock + " minItemsInBlock=" + minItemsInBlock);
+    }
+
+    maxDoc = state.segmentInfo.getDocCount();
+
+    final String termsFileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, TERMS_EXTENSION);
+    out = state.directory.createOutput(termsFileName, state.context);
+    boolean success = false;
+    IndexOutput indexOut = null;
+    try {
+      fieldInfos = state.fieldInfos;
+      this.minItemsInBlock = minItemsInBlock;
+      this.maxItemsInBlock = maxItemsInBlock;
+      CodecUtil.writeHeader(out, TERMS_CODEC_NAME, VERSION_CURRENT);   
+
+      //DEBUG = state.segmentName.equals("_4a");
+
+      final String termsIndexFileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, TERMS_INDEX_EXTENSION);
+      indexOut = state.directory.createOutput(termsIndexFileName, state.context);
+      CodecUtil.writeHeader(indexOut, TERMS_INDEX_CODEC_NAME, VERSION_CURRENT); 
+
+      this.postingsWriter = postingsWriter;
+      // segment = state.segmentInfo.name;
+
+      // System.out.println("BTW.init seg=" + state.segmentName);
+
+      postingsWriter.init(out);                          // have consumer write its format/header
+      success = true;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(out, indexOut);
+      }
+    }
+    this.indexOut = indexOut;
+  }
+
+  /** Writes the terms file trailer. */
+  private void writeTrailer(IndexOutput out, long dirStart) throws IOException {
+    out.writeLong(dirStart);    
+  }
+
+  /** Writes the index file trailer. */
+  private void writeIndexTrailer(IndexOutput indexOut, long dirStart) throws IOException {
+    indexOut.writeLong(dirStart);    
+  }
+
+  @Override
+  public void write(Fields fields) throws IOException {
+
+    String lastField = null;
+    for(String field : fields) {
+      assert lastField == null || lastField.compareTo(field) < 0;
+      lastField = field;
+
+      Terms terms = fields.terms(field);
+      if (terms == null) {
+        continue;
+      }
+
+      TermsEnum termsEnum = terms.iterator(null);
+
+      TermsWriter termsWriter = new TermsWriter(fieldInfos.fieldInfo(field));
+      BytesRef minTerm = null;
+      BytesRef maxTerm = new BytesRef();
+      while (true) {
+        BytesRef term = termsEnum.next();
+        if (term == null) {
+          break;
+        }
+        if (minTerm == null) {
+          minTerm = BytesRef.deepCopyOf(term);
+        }
+        maxTerm.copyBytes(term);
+        termsWriter.write(term, termsEnum);
+      }
+
+      termsWriter.finish(minTerm, minTerm == null ? null : maxTerm);
+    }
+  }
+  
+  static long encodeOutput(long fp, boolean hasTerms, boolean isFloor) {
+    assert fp < (1L << 62);
+    return (fp << 2) | (hasTerms ? OUTPUT_FLAG_HAS_TERMS : 0) | (isFloor ? OUTPUT_FLAG_IS_FLOOR : 0);
+  }
+
+  private static class PendingEntry {
+    public final boolean isTerm;
+
+    protected PendingEntry(boolean isTerm) {
+      this.isTerm = isTerm;
+    }
+  }
+
+  private static final class PendingTerm extends PendingEntry {
+    public final BytesRef term;
+    // stats + metadata
+    public final BlockTermState state;
+
+    public PendingTerm(BytesRef term, BlockTermState state) {
+      super(true);
+      this.term = term;
+      this.state = state;
+    }
+
+    @Override
+    public String toString() {
+      return term.utf8ToString();
+    }
+  }
+
+  private static final class PendingBlock extends PendingEntry {
+    public final BytesRef prefix;
+    public final long fp;
+    public FST<Pair<BytesRef,Long>> index;
+    public List<FST<Pair<BytesRef,Long>>> subIndices;
+    public final boolean hasTerms;
+    public final boolean isFloor;
+    public final int floorLeadByte;
+    private final IntsRef scratchIntsRef = new IntsRef();
+    /** Max version for all terms in this block. */
+    private final long maxVersion;
+
+    public PendingBlock(BytesRef prefix, long maxVersion, long fp, boolean hasTerms, boolean isFloor, int floorLeadByte, List<FST<Pair<BytesRef,Long>>> subIndices) {
+      super(false);
+      this.prefix = prefix;
+      this.maxVersion = maxVersion;
+      this.fp = fp;
+      this.hasTerms = hasTerms;
+      this.isFloor = isFloor;
+      this.floorLeadByte = floorLeadByte;
+      this.subIndices = subIndices;
+    }
+
+    @Override
+    public String toString() {
+      return "BLOCK: " + prefix.utf8ToString();
+    }
+
+    public void compileIndex(List<PendingBlock> floorBlocks, RAMOutputStream scratchBytes) throws IOException {
+
+      assert (isFloor && floorBlocks != null && floorBlocks.size() != 0) || (!isFloor && floorBlocks == null): "isFloor=" + isFloor + " floorBlocks=" + floorBlocks;
+
+      assert scratchBytes.getFilePointer() == 0;
+
+      long maxVersionIndex = maxVersion;
+
+      // TODO: try writing the leading vLong in MSB order
+      // (opposite of what Lucene does today), for better
+      // outputs sharing in the FST
+      scratchBytes.writeVLong(encodeOutput(fp, hasTerms, isFloor));
+      if (isFloor) {
+        scratchBytes.writeVInt(floorBlocks.size());
+        for (PendingBlock sub : floorBlocks) {
+          assert sub.floorLeadByte != -1;
+          maxVersionIndex = Math.max(maxVersionIndex, sub.maxVersion);
+          //if (DEBUG) {
+          //  System.out.println("    write floorLeadByte=" + Integer.toHexString(sub.floorLeadByte&0xff));
+          //}
+          scratchBytes.writeByte((byte) sub.floorLeadByte);
+          assert sub.fp > fp;
+          scratchBytes.writeVLong((sub.fp - fp) << 1 | (sub.hasTerms ? 1 : 0));
+        }
+      }
+
+      final Builder<Pair<BytesRef,Long>> indexBuilder = new Builder<>(FST.INPUT_TYPE.BYTE1,
+                                                                      0, 0, true, false, Integer.MAX_VALUE,
+                                                                      FST_OUTPUTS, null, false,
+                                                                      PackedInts.COMPACT, true, 15);
+      //if (DEBUG) {
+      //  System.out.println("  compile index for prefix=" + prefix);
+      //}
+      //indexBuilder.DEBUG = false;
+      final byte[] bytes = new byte[(int) scratchBytes.getFilePointer()];
+      assert bytes.length > 0;
+      scratchBytes.writeTo(bytes, 0);
+      indexBuilder.add(Util.toIntsRef(prefix, scratchIntsRef), FST_OUTPUTS.newPair(new BytesRef(bytes, 0, bytes.length), Long.MAX_VALUE - maxVersionIndex));
+      scratchBytes.reset();
+
+      // Copy over index for all sub-blocks
+
+      if (subIndices != null) {
+        for(FST<Pair<BytesRef,Long>> subIndex : subIndices) {
+          append(indexBuilder, subIndex);
+        }
+      }
+
+      if (floorBlocks != null) {
+        for (PendingBlock sub : floorBlocks) {
+          if (sub.subIndices != null) {
+            for(FST<Pair<BytesRef,Long>> subIndex : sub.subIndices) {
+              append(indexBuilder, subIndex);
+            }
+          }
+          sub.subIndices = null;
+        }
+      }
+
+      index = indexBuilder.finish();
+      subIndices = null;
+
+      /*
+      Writer w = new OutputStreamWriter(new FileOutputStream("out.dot"));
+      Util.toDot(index, w, false, false);
+      System.out.println("SAVED to out.dot");
+      w.close();
+      */
+    }
+
+    // TODO: maybe we could add bulk-add method to
+    // Builder?  Takes FST and unions it w/ current
+    // FST.
+    private void append(Builder<Pair<BytesRef,Long>> builder, FST<Pair<BytesRef,Long>> subIndex) throws IOException {
+      final BytesRefFSTEnum<Pair<BytesRef,Long>> subIndexEnum = new BytesRefFSTEnum<>(subIndex);
+      BytesRefFSTEnum.InputOutput<Pair<BytesRef,Long>> indexEnt;
+      while((indexEnt = subIndexEnum.next()) != null) {
+        //if (DEBUG) {
+        //  System.out.println("      add sub=" + indexEnt.input + " " + indexEnt.input + " output=" + indexEnt.output);
+        //}
+        builder.add(Util.toIntsRef(indexEnt.input, scratchIntsRef), indexEnt.output);
+      }
+    }
+  }
+
+  final RAMOutputStream scratchBytes = new RAMOutputStream();
+
+  class TermsWriter {
+    private final FieldInfo fieldInfo;
+    private final int longsSize;
+    private long numTerms;
+    final FixedBitSet docsSeen;
+    long indexStartFP;
+
+    // Used only to partition terms into the block tree; we
+    // don't pull an FST from this builder:
+    private final NoOutputs noOutputs;
+    private final Builder<Object> blockBuilder;
+
+    // PendingTerm or PendingBlock:
+    private final List<PendingEntry> pending = new ArrayList<>();
+
+    // Index into pending of most recently written block
+    private int lastBlockIndex = -1;
+
+    // Re-used when segmenting a too-large block into floor
+    // blocks:
+    private int[] subBytes = new int[10];
+    private int[] subTermCounts = new int[10];
+    private int[] subTermCountSums = new int[10];
+    private int[] subSubCounts = new int[10];
+
+    // This class assigns terms to blocks "naturally", ie,
+    // according to the number of terms under a given prefix
+    // that we encounter:
+    private class FindBlocks extends Builder.FreezeTail<Object> {
+
+      @Override
+      public void freeze(final Builder.UnCompiledNode<Object>[] frontier, int prefixLenPlus1, final IntsRef lastInput) throws IOException {
+
+        //if (DEBUG) System.out.println("  freeze prefixLenPlus1=" + prefixLenPlus1);
+
+        for(int idx=lastInput.length; idx >= prefixLenPlus1; idx--) {
+          final Builder.UnCompiledNode<Object> node = frontier[idx];
+
+          long totCount = 0;
+
+          if (node.isFinal) {
+            totCount++;
+          }
+
+          for(int arcIdx=0;arcIdx<node.numArcs;arcIdx++) {
+            @SuppressWarnings("unchecked") final Builder.UnCompiledNode<Object> target = (Builder.UnCompiledNode<Object>) node.arcs[arcIdx].target;
+            totCount += target.inputCount;
+            target.clear();
+            node.arcs[arcIdx].target = null;
+          }
+          node.numArcs = 0;
+
+          if (totCount >= minItemsInBlock || idx == 0) {
+            // We are on a prefix node that has enough
+            // entries (terms or sub-blocks) under it to let
+            // us write a new block or multiple blocks (main
+            // block + follow on floor blocks):
+            //if (DEBUG) {
+            //  if (totCount < minItemsInBlock && idx != 0) {
+            //    System.out.println("  force block has terms");
+            //  }
+            //}
+            writeBlocks(lastInput, idx, (int) totCount);
+            node.inputCount = 1;
+          } else {
+            // stragglers!  carry count upwards
+            node.inputCount = totCount;
+          }
+          frontier[idx] = new Builder.UnCompiledNode<>(blockBuilder, idx);
+        }
+      }
+    }
+
+    // Write the top count entries on the pending stack as
+    // one or more blocks.  Returns how many blocks were
+    // written.  If the entry count is <= maxItemsPerBlock
+    // we just write a single block; else we break into
+    // primary (initial) block and then one or more
+    // following floor blocks:
+
+    void writeBlocks(IntsRef prevTerm, int prefixLength, int count) throws IOException {
+      if (count <= maxItemsInBlock) {
+        // Easy case: not floor block.  Eg, prefix is "foo",
+        // and we found 30 terms/sub-blocks starting w/ that
+        // prefix, and minItemsInBlock <= 30 <=
+        // maxItemsInBlock.
+        final PendingBlock nonFloorBlock = writeBlock(prevTerm, prefixLength, prefixLength, count, count, 0, false, -1, true);
+        nonFloorBlock.compileIndex(null, scratchBytes);
+        pending.add(nonFloorBlock);
+      } else {
+        // Floor block case.  Eg, prefix is "foo" but we
+        // have 100 terms/sub-blocks starting w/ that
+        // prefix.  We segment the entries into a primary
+        // block and following floor blocks using the first
+        // label in the suffix to assign to floor blocks.
+
+        // TODO: we could store min & max suffix start byte
+        // in each block, to make floor blocks authoritative
+
+        /*
+        if (DEBUG) {
+          final BytesRef prefix = new BytesRef(prefixLength);
+          for(int m=0;m<prefixLength;m++) {
+            prefix.bytes[m] = (byte) prevTerm.ints[m];
+          }
+          prefix.length = prefixLength;
+          //System.out.println("\nWBS count=" + count + " prefix=" + prefix.utf8ToString() + " " + prefix);
+          System.out.println("writeBlocks: prefix=" + toString(prefix) + " " + prefix + " count=" + count + " pending.size()=" + pending.size());
+        }
+        */
+        //System.out.println("\nwbs count=" + count);
+
+        final int savLabel = prevTerm.ints[prevTerm.offset + prefixLength];
+
+        // Count up how many items fall under
+        // each unique label after the prefix.
+        
+        // TODO: this is wasteful since the builder had
+        // already done this (partitioned these sub-terms
+        // according to their leading prefix byte)
+        
+        final List<PendingEntry> slice = pending.subList(pending.size()-count, pending.size());
+        int lastSuffixLeadLabel = -1;
+        int termCount = 0;
+        int subCount = 0;
+        int numSubs = 0;
+
+        for(PendingEntry ent : slice) {
+
+          // First byte in the suffix of this term
+          final int suffixLeadLabel;
+          if (ent.isTerm) {
+            PendingTerm term = (PendingTerm) ent;
+            if (term.term.length == prefixLength) {
+              // Suffix is 0, ie prefix 'foo' and term is
+              // 'foo' so the term has empty string suffix
+              // in this block
+              assert lastSuffixLeadLabel == -1;
+              assert numSubs == 0;
+              suffixLeadLabel = -1;
+            } else {
+              suffixLeadLabel = term.term.bytes[term.term.offset + prefixLength] & 0xff;
+            }
+          } else {
+            PendingBlock block = (PendingBlock) ent;
+            assert block.prefix.length > prefixLength;
+            suffixLeadLabel = block.prefix.bytes[block.prefix.offset + prefixLength] & 0xff;
+          }
+
+          if (suffixLeadLabel != lastSuffixLeadLabel && (termCount + subCount) != 0) {
+            if (subBytes.length == numSubs) {
+              subBytes = ArrayUtil.grow(subBytes);
+              subTermCounts = ArrayUtil.grow(subTermCounts);
+              subSubCounts = ArrayUtil.grow(subSubCounts);
+            }
+            subBytes[numSubs] = lastSuffixLeadLabel;
+            lastSuffixLeadLabel = suffixLeadLabel;
+            subTermCounts[numSubs] = termCount;
+            subSubCounts[numSubs] = subCount;
+            /*
+            if (suffixLeadLabel == -1) {
+              System.out.println("  sub " + -1 + " termCount=" + termCount + " subCount=" + subCount);
+            } else {
+              System.out.println("  sub " + Integer.toHexString(suffixLeadLabel) + " termCount=" + termCount + " subCount=" + subCount);
+            }
+            */
+            termCount = subCount = 0;
+            numSubs++;
+          }
+
+          if (ent.isTerm) {
+            termCount++;
+          } else {
+            subCount++;
+          }
+        }
+
+        if (subBytes.length == numSubs) {
+          subBytes = ArrayUtil.grow(subBytes);
+          subTermCounts = ArrayUtil.grow(subTermCounts);
+          subSubCounts = ArrayUtil.grow(subSubCounts);
+        }
+
+        subBytes[numSubs] = lastSuffixLeadLabel;
+        subTermCounts[numSubs] = termCount;
+        subSubCounts[numSubs] = subCount;
+        numSubs++;
+        /*
+        if (lastSuffixLeadLabel == -1) {
+          System.out.println("  sub " + -1 + " termCount=" + termCount + " subCount=" + subCount);
+        } else {
+          System.out.println("  sub " + Integer.toHexString(lastSuffixLeadLabel) + " termCount=" + termCount + " subCount=" + subCount);
+        }
+        */
+
+        if (subTermCountSums.length < numSubs) {
+          subTermCountSums = ArrayUtil.grow(subTermCountSums, numSubs);
+        }
+
+        // Roll up (backwards) the termCounts; postings impl
+        // needs this to know where to pull the term slice
+        // from its pending terms stack:
+        int sum = 0;
+        for(int idx=numSubs-1;idx>=0;idx--) {
+          sum += subTermCounts[idx];
+          subTermCountSums[idx] = sum;
+        }
+
+        // TODO: make a better segmenter?  It'd have to
+        // absorb the too-small end blocks backwards into
+        // the previous blocks
+
+        // Naive greedy segmentation; this is not always
+        // best (it can produce a too-small block as the
+        // last block):
+        int pendingCount = 0;
+        int startLabel = subBytes[0];
+        int curStart = count;
+        subCount = 0;
+
+        final List<PendingBlock> floorBlocks = new ArrayList<>();
+        PendingBlock firstBlock = null;
+
+        for(int sub=0;sub<numSubs;sub++) {
+          pendingCount += subTermCounts[sub] + subSubCounts[sub];
+          //System.out.println("  " + (subTermCounts[sub] + subSubCounts[sub]));
+          subCount++;
+
+          // Greedily make a floor block as soon as we've
+          // crossed the min count
+          if (pendingCount >= minItemsInBlock) {
+            final int curPrefixLength;
+            if (startLabel == -1) {
+              curPrefixLength = prefixLength;
+            } else {
+              curPrefixLength = 1+prefixLength;
+              // floor term:
+              prevTerm.ints[prevTerm.offset + prefixLength] = startLabel;
+            }
+            //System.out.println("  " + subCount + " subs");
+            final PendingBlock floorBlock = writeBlock(prevTerm, prefixLength, curPrefixLength, curStart, pendingCount, subTermCountSums[1+sub], true, startLabel, curStart == pendingCount);
+            if (firstBlock == null) {
+              firstBlock = floorBlock;
+            } else {
+              floorBlocks.add(floorBlock);
+            }
+            curStart -= pendingCount;
+            //System.out.println("    = " + pendingCount);
+            pendingCount = 0;
+
+            assert minItemsInBlock == 1 || subCount > 1: "minItemsInBlock=" + minItemsInBlock + " subCount=" + subCount + " sub=" + sub + " of " + numSubs + " subTermCount=" + subTermCountSums[sub] + " subSubCount=" + subSubCounts[sub] + " depth=" + prefixLength;
+            subCount = 0;
+            startLabel = subBytes[sub+1];
+
+            if (curStart == 0) {
+              break;
+            }
+
+            if (curStart <= maxItemsInBlock) {
+              // remainder is small enough to fit into a
+              // block.  NOTE that this may be too small (<
+              // minItemsInBlock); need a true segmenter
+              // here
+              assert startLabel != -1;
+              assert firstBlock != null;
+              prevTerm.ints[prevTerm.offset + prefixLength] = startLabel;
+              //System.out.println("  final " + (numSubs-sub-1) + " subs");
+              /*
+              for(sub++;sub < numSubs;sub++) {
+                System.out.println("  " + (subTermCounts[sub] + subSubCounts[sub]));
+              }
+              System.out.println("    = " + curStart);
+              if (curStart < minItemsInBlock) {
+                System.out.println("      **");
+              }
+              */
+              floorBlocks.add(writeBlock(prevTerm, prefixLength, prefixLength+1, curStart, curStart, 0, true, startLabel, true));
+              break;
+            }
+          }
+        }
+
+        prevTerm.ints[prevTerm.offset + prefixLength] = savLabel;
+
+        assert firstBlock != null;
+        firstBlock.compileIndex(floorBlocks, scratchBytes);
+
+        pending.add(firstBlock);
+        //if (DEBUG) System.out.println("  done pending.size()=" + pending.size());
+      }
+      lastBlockIndex = pending.size()-1;
+    }
+
+    // for debugging
+    @SuppressWarnings("unused")
+    private String toString(BytesRef b) {
+      try {
+        return b.utf8ToString() + " " + b;
+      } catch (Throwable t) {
+        // If BytesRef isn't actually UTF8, or it's eg a
+        // prefix of UTF8 that ends mid-unicode-char, we
+        // fallback to hex:
+        return b.toString();
+      }
+    }
+
+    // Writes all entries in the pending slice as a single
+    // block: 
+    private PendingBlock writeBlock(IntsRef prevTerm, int prefixLength, int indexPrefixLength, int startBackwards, int length,
+                                    int futureTermCount, boolean isFloor, int floorLeadByte, boolean isLastInFloor) throws IOException {
+
+      assert length > 0;
+
+      final int start = pending.size()-startBackwards;
+
+      assert start >= 0: "pending.size()=" + pending.size() + " startBackwards=" + startBackwards + " length=" + length;
+
+      final List<PendingEntry> slice = pending.subList(start, start + length);
+
+      final long startFP = out.getFilePointer();
+
+      final BytesRef prefix = new BytesRef(indexPrefixLength);
+      for(int m=0;m<indexPrefixLength;m++) {
+        prefix.bytes[m] = (byte) prevTerm.ints[m];
+      }
+      prefix.length = indexPrefixLength;
+
+      // Write block header:
+      out.writeVInt((length<<1)|(isLastInFloor ? 1:0));
+
+      // if (DEBUG) {
+      //  System.out.println("  writeBlock " + (isFloor ? "(floor) " : "") + "seg=" + segment + " pending.size()=" + pending.size() + " prefixLength=" + prefixLength + " indexPrefix=" + toString(prefix) + " entCount=" + length + " startFP=" + startFP + " futureTermCount=" + futureTermCount + (isFloor ? (" floorLeadByte=" + Integer.toHexString(floorLeadByte&0xff)) : "") + " isLastInFloor=" + isLastInFloor);
+      // }
+
+      // 1st pass: pack term suffix bytes into byte[] blob
+      // TODO: cutover to bulk int codec... simple64?
+
+      final boolean isLeafBlock;
+      if (lastBlockIndex < start) {
+        // This block definitely does not contain sub-blocks:
+        isLeafBlock = true;
+        //System.out.println("no scan true isFloor=" + isFloor);
+      } else if (!isFloor) {
+        // This block definitely does contain at least one sub-block:
+        isLeafBlock = false;
+        //System.out.println("no scan false " + lastBlockIndex + " vs start=" + start + " len=" + length);
+      } else {
+        // Must scan up-front to see if there is a sub-block
+        boolean v = true;
+        //System.out.println("scan " + lastBlockIndex + " vs start=" + start + " len=" + length);
+        for (PendingEntry ent : slice) {
+          if (!ent.isTerm) {
+            v = false;
+            break;
+          }
+        }
+        isLeafBlock = v;
+      }
+
+      final List<FST<Pair<BytesRef,Long>>> subIndices;
+
+      int termCount;
+
+      long[] longs = new long[longsSize];
+      boolean absolute = true;
+      long maxVersionInBlock = -1;
+
+      // int countx = 0;
+      if (isLeafBlock) {
+        subIndices = null;
+        for (PendingEntry ent : slice) {
+          assert ent.isTerm;
+          PendingTerm term = (PendingTerm) ent;
+          BlockTermState state = term.state;
+          maxVersionInBlock = Math.max(maxVersionInBlock, ((IDVersionTermState) state).idVersion);
+          final int suffix = term.term.length - prefixLength;
+          // if (DEBUG) {
+          //    BytesRef suffixBytes = new BytesRef(suffix);
+          //    System.arraycopy(term.term.bytes, prefixLength, suffixBytes.bytes, 0, suffix);
+          //    suffixBytes.length = suffix;
+          //    System.out.println("    " + (countx++) + ": write term suffix=" + toString(suffixBytes));
+          // }
+          // For leaf block we write suffix straight
+          suffixWriter.writeVInt(suffix);
+          suffixWriter.writeBytes(term.term.bytes, prefixLength, suffix);
+
+          // Write term meta data
+          postingsWriter.encodeTerm(longs, bytesWriter, fieldInfo, state, absolute);
+          for (int pos = 0; pos < longsSize; pos++) {
+            assert longs[pos] >= 0;
+            metaWriter.writeVLong(longs[pos]);
+          }
+          bytesWriter.writeTo(metaWriter);
+          bytesWriter.reset();
+          absolute = false;
+        }
+        termCount = length;
+      } else {
+        subIndices = new ArrayList<>();
+        termCount = 0;
+        for (PendingEntry ent : slice) {
+          if (ent.isTerm) {
+            PendingTerm term = (PendingTerm) ent;
+            BlockTermState state = term.state;
+            maxVersionInBlock = Math.max(maxVersionInBlock, ((IDVersionTermState) state).idVersion);
+            final int suffix = term.term.length - prefixLength;
+            // if (DEBUG) {
+            //    BytesRef suffixBytes = new BytesRef(suffix);
+            //    System.arraycopy(term.term.bytes, prefixLength, suffixBytes.bytes, 0, suffix);
+            //    suffixBytes.length = suffix;
+            //    System.out.println("    " + (countx++) + ": write term suffix=" + toString(suffixBytes));
+            // }
+            // For non-leaf block we borrow 1 bit to record
+            // if entry is term or sub-block
+            suffixWriter.writeVInt(suffix<<1);
+            suffixWriter.writeBytes(term.term.bytes, prefixLength, suffix);
+
+            // TODO: now that terms dict "sees" these longs,
+            // we can explore better column-stride encodings
+            // to encode all long[0]s for this block at
+            // once, all long[1]s, etc., e.g. using
+            // Simple64.  Alternatively, we could interleave
+            // stats + meta ... no reason to have them
+            // separate anymore:
+
+            // Write term meta data
+            postingsWriter.encodeTerm(longs, bytesWriter, fieldInfo, state, absolute);
+            for (int pos = 0; pos < longsSize; pos++) {
+              assert longs[pos] >= 0;
+              metaWriter.writeVLong(longs[pos]);
+            }
+            bytesWriter.writeTo(metaWriter);
+            bytesWriter.reset();
+            absolute = false;
+
+            termCount++;
+          } else {
+            PendingBlock block = (PendingBlock) ent;
+            maxVersionInBlock = Math.max(maxVersionInBlock, block.maxVersion);
+            final int suffix = block.prefix.length - prefixLength;
+
+            assert suffix > 0;
+
+            // For non-leaf block we borrow 1 bit to record
+            // if entry is term or sub-block
+            suffixWriter.writeVInt((suffix<<1)|1);
+            suffixWriter.writeBytes(block.prefix.bytes, prefixLength, suffix);
+            assert block.fp < startFP;
+
+            // if (DEBUG) {
+            //    BytesRef suffixBytes = new BytesRef(suffix);
+            //    System.arraycopy(block.prefix.bytes, prefixLength, suffixBytes.bytes, 0, suffix);
+            //    suffixBytes.length = suffix;
+            //    System.out.println("    " + (countx++) + ": write sub-block suffix=" + toString(suffixBytes) + " subFP=" + block.fp + " subCode=" + (startFP-block.fp) + " floor=" + block.isFloor);
+            // }
+
+            suffixWriter.writeVLong(startFP - block.fp);
+            subIndices.add(block.index);
+          }
+        }
+
+        assert subIndices.size() != 0;
+      }
+
+      // TODO: we could block-write the term suffix pointers;
+      // this would take more space but would enable binary
+      // search on lookup
+
+      // Write suffixes byte[] blob to terms dict output:
+      out.writeVInt((int) (suffixWriter.getFilePointer() << 1) | (isLeafBlock ? 1:0));
+      suffixWriter.writeTo(out);
+      suffixWriter.reset();
+
+      // Write term meta data byte[] blob
+      out.writeVInt((int) metaWriter.getFilePointer());
+      metaWriter.writeTo(out);
+      metaWriter.reset();
+
+      // Remove slice replaced by block:
+      slice.clear();
+
+      if (lastBlockIndex >= start) {
+        if (lastBlockIndex < start+length) {
+          lastBlockIndex = start;
+        } else {
+          lastBlockIndex -= length;
+        }
+      }
+
+      // if (DEBUG) {
+      //   System.out.println("      fpEnd=" + out.getFilePointer());
+      // }
+
+      return new PendingBlock(prefix, maxVersionInBlock, startFP, termCount != 0, isFloor, floorLeadByte, subIndices);
+    }
+
+    TermsWriter(FieldInfo fieldInfo) {
+      this.fieldInfo = fieldInfo;
+      docsSeen = new FixedBitSet(maxDoc);
+
+      noOutputs = NoOutputs.getSingleton();
+
+      // This Builder is just used transiently to fragment
+      // terms into "good" blocks; we don't save the
+      // resulting FST:
+      blockBuilder = new Builder<>(FST.INPUT_TYPE.BYTE1,
+                                   0, 0, true,
+                                   true, Integer.MAX_VALUE,
+                                   noOutputs,
+                                   new FindBlocks(), false,
+                                   PackedInts.COMPACT,
+                                   true, 15);
+
+      this.longsSize = postingsWriter.setField(fieldInfo);
+    }
+    
+    private final IntsRef scratchIntsRef = new IntsRef();
+
+    /** Writes one term's worth of postings. */
+    public void write(BytesRef text, TermsEnum termsEnum) throws IOException {
+
+      BlockTermState state = postingsWriter.writeTerm(text, termsEnum, docsSeen);
+      // TODO: LUCENE-5693: we don't need this check if we fix IW to not send deleted docs to us on flush:
+      if (state != null && ((IDVersionPostingsWriter) postingsWriter).lastDocID != -1) {
+        assert state.docFreq != 0;
+        assert fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY || state.totalTermFreq >= state.docFreq: "postingsWriter=" + postingsWriter;
+        blockBuilder.add(Util.toIntsRef(text, scratchIntsRef), noOutputs.getNoOutput());
+
+        PendingTerm term = new PendingTerm(BytesRef.deepCopyOf(text), state);
+        pending.add(term);
+        numTerms++;
+      }
+    }
+
+    // Finishes all terms in this field
+    public void finish(BytesRef minTerm, BytesRef maxTerm) throws IOException {
+      if (numTerms > 0) {
+        blockBuilder.finish();
+
+        // We better have one final "root" block:
+        assert pending.size() == 1 && !pending.get(0).isTerm: "pending.size()=" + pending.size() + " pending=" + pending;
+        final PendingBlock root = (PendingBlock) pending.get(0);
+        assert root.prefix.length == 0;
+        assert root.index.getEmptyOutput() != null;
+
+        // Write FST to index
+        indexStartFP = indexOut.getFilePointer();
+        root.index.save(indexOut);
+        //System.out.println("  write FST " + indexStartFP + " field=" + fieldInfo.name);
+
+        // if (SAVE_DOT_FILES || DEBUG) {
+        //   final String dotFileName = segment + "_" + fieldInfo.name + ".dot";
+        //   Writer w = new OutputStreamWriter(new FileOutputStream(dotFileName));
+        //   Util.toDot(root.index, w, false, false);
+        //   System.out.println("SAVED to " + dotFileName);
+        //   w.close();
+        // }
+
+        fields.add(new FieldMetaData(fieldInfo,
+                                     ((PendingBlock) pending.get(0)).index.getEmptyOutput(),
+                                     numTerms,
+                                     indexStartFP,
+                                     longsSize,
+                                     minTerm, maxTerm));
+      } else {
+        // cannot assert this: we skip deleted docIDs in the postings:
+        // assert docsSeen.cardinality() == 0;
+      }
+    }
+
+    private final RAMOutputStream suffixWriter = new RAMOutputStream();
+    private final RAMOutputStream metaWriter = new RAMOutputStream();
+    private final RAMOutputStream bytesWriter = new RAMOutputStream();
+  }
+
+  @Override
+  public void close() throws IOException {
+
+    boolean success = false;
+    try {
+      
+      final long dirStart = out.getFilePointer();
+      final long indexDirStart = indexOut.getFilePointer();
+
+      out.writeVInt(fields.size());
+      
+      for(FieldMetaData field : fields) {
+        //System.out.println("  field " + field.fieldInfo.name + " " + field.numTerms + " terms");
+        out.writeVInt(field.fieldInfo.number);
+        assert field.numTerms > 0;
+        out.writeVLong(field.numTerms);
+        out.writeVInt(field.rootCode.output1.length);
+        out.writeBytes(field.rootCode.output1.bytes, field.rootCode.output1.offset, field.rootCode.output1.length);
+        out.writeVLong(field.rootCode.output2);
+        out.writeVInt(field.longsSize);
+        indexOut.writeVLong(field.indexStartFP);
+        writeBytesRef(out, field.minTerm);
+        writeBytesRef(out, field.maxTerm);
+      }
+      writeTrailer(out, dirStart);
+      CodecUtil.writeFooter(out);
+      writeIndexTrailer(indexOut, indexDirStart);
+      CodecUtil.writeFooter(indexOut);
+      success = true;
+    } finally {
+      if (success) {
+        IOUtils.close(out, indexOut, postingsWriter);
+      } else {
+        IOUtils.closeWhileHandlingException(out, indexOut, postingsWriter);
+      }
+    }
+  }
+
+  private static void writeBytesRef(IndexOutput out, BytesRef bytes) throws IOException {
+    out.writeVInt(bytes.length);
+    out.writeBytes(bytes.bytes, bytes.offset, bytes.length);
+  }
+}
diff --git a/lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/VersionFieldReader.java b/lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/VersionFieldReader.java
new file mode 100644
index 0000000..417ce11
--- /dev/null
+++ b/lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/VersionFieldReader.java
@@ -0,0 +1,163 @@
+package org.apache.lucene.codecs.idversion;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.Terms;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.store.ByteArrayDataInput;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.automaton.CompiledAutomaton;
+import org.apache.lucene.util.fst.ByteSequenceOutputs;
+import org.apache.lucene.util.fst.FST;
+import org.apache.lucene.util.fst.PairOutputs.Pair;
+
+/** BlockTree's implementation of {@link Terms}. */
+// public for CheckIndex:
+final class VersionFieldReader extends Terms {
+  final long numTerms;
+  final FieldInfo fieldInfo;
+  final long sumTotalTermFreq;
+  final long sumDocFreq;
+  final int docCount;
+  final long indexStartFP;
+  final long rootBlockFP;
+  final Pair<BytesRef,Long> rootCode;
+  final BytesRef minTerm;
+  final BytesRef maxTerm;
+  final int longsSize;
+  final VersionBlockTreeTermsReader parent;
+
+  final FST<Pair<BytesRef,Long>> index;
+  //private boolean DEBUG;
+
+  VersionFieldReader(VersionBlockTreeTermsReader parent, FieldInfo fieldInfo, long numTerms, Pair<BytesRef,Long> rootCode, long sumTotalTermFreq, long sumDocFreq, int docCount,
+              long indexStartFP, int longsSize, IndexInput indexIn, BytesRef minTerm, BytesRef maxTerm) throws IOException {
+    assert numTerms > 0;
+    this.fieldInfo = fieldInfo;
+    //DEBUG = BlockTreeTermsReader.DEBUG && fieldInfo.name.equals("id");
+    this.parent = parent;
+    this.numTerms = numTerms;
+    this.sumTotalTermFreq = sumTotalTermFreq; 
+    this.sumDocFreq = sumDocFreq; 
+    this.docCount = docCount;
+    this.indexStartFP = indexStartFP;
+    this.rootCode = rootCode;
+    this.longsSize = longsSize;
+    this.minTerm = minTerm;
+    this.maxTerm = maxTerm;
+    // if (DEBUG) {
+    //   System.out.println("BTTR: seg=" + segment + " field=" + fieldInfo.name + " rootBlockCode=" + rootCode + " divisor=" + indexDivisor);
+    // }
+
+    rootBlockFP = (new ByteArrayDataInput(rootCode.output1.bytes, rootCode.output1.offset, rootCode.output1.length)).readVLong() >>> VersionBlockTreeTermsWriter.OUTPUT_FLAGS_NUM_BITS;
+
+    if (indexIn != null) {
+      final IndexInput clone = indexIn.clone();
+      //System.out.println("start=" + indexStartFP + " field=" + fieldInfo.name);
+      clone.seek(indexStartFP);
+      index = new FST<>(clone, VersionBlockTreeTermsWriter.FST_OUTPUTS);
+        
+      /*
+        if (false) {
+        final String dotFileName = segment + "_" + fieldInfo.name + ".dot";
+        Writer w = new OutputStreamWriter(new FileOutputStream(dotFileName));
+        Util.toDot(index, w, false, false);
+        System.out.println("FST INDEX: SAVED to " + dotFileName);
+        w.close();
+        }
+      */
+    } else {
+      index = null;
+    }
+  }
+
+  @Override
+  public BytesRef getMin() throws IOException {
+    if (minTerm == null) {
+      // Older index that didn't store min/maxTerm
+      return super.getMin();
+    } else {
+      return minTerm;
+    }
+  }
+
+  @Override
+  public BytesRef getMax() throws IOException {
+    if (maxTerm == null) {
+      // Older index that didn't store min/maxTerm
+      return super.getMax();
+    } else {
+      return maxTerm;
+    }
+  }
+
+  @Override
+  public boolean hasFreqs() {
+    return fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;
+  }
+
+  @Override
+  public boolean hasOffsets() {
+    return fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
+  }
+
+  @Override
+  public boolean hasPositions() {
+    return fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;
+  }
+    
+  @Override
+  public boolean hasPayloads() {
+    return fieldInfo.hasPayloads();
+  }
+
+  @Override
+  public TermsEnum iterator(TermsEnum reuse) throws IOException {
+    return new IDVersionSegmentTermsEnum(this);
+  }
+
+  @Override
+  public long size() {
+    return numTerms;
+  }
+
+  @Override
+  public long getSumTotalTermFreq() {
+    return sumTotalTermFreq;
+  }
+
+  @Override
+  public long getSumDocFreq() {
+    return sumDocFreq;
+  }
+
+  @Override
+  public int getDocCount() {
+    return docCount;
+  }
+
+  /** Returns approximate RAM bytes used */
+  public long ramBytesUsed() {
+    return ((index!=null)? index.sizeInBytes() : 0);
+  }
+}
diff --git a/lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/package.html b/lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/package.html
new file mode 100644
index 0000000..16f92ef
--- /dev/null
+++ b/lucene/sandbox/src/java/org/apache/lucene/codecs/idversion/package.html
@@ -0,0 +1,26 @@
+<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+<html>
+<head>
+   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
+</head>
+<body>
+A primary-key postings format that associates a version (long) with each term and
+can provide fail-fast lookups by ID and version.
+</body>
+</html>
diff --git a/lucene/sandbox/src/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat b/lucene/sandbox/src/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat
new file mode 100644
index 0000000..a319f24
--- /dev/null
+++ b/lucene/sandbox/src/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat
@@ -0,0 +1,16 @@
+#  Licensed to the Apache Software Foundation (ASF) under one or more
+#  contributor license agreements.  See the NOTICE file distributed with
+#  this work for additional information regarding copyright ownership.
+#  The ASF licenses this file to You under the Apache License, Version 2.0
+#  (the "License"); you may not use this file except in compliance with
+#  the License.  You may obtain a copy of the License at
+#
+#       http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+#  limitations under the License.
+
+org.apache.lucene.codecs.idversion.IDVersionPostingsFormat
diff --git a/lucene/sandbox/src/test/org/apache/lucene/codecs/idversion/StringAndPayloadField.java b/lucene/sandbox/src/test/org/apache/lucene/codecs/idversion/StringAndPayloadField.java
new file mode 100644
index 0000000..19ff31a
--- /dev/null
+++ b/lucene/sandbox/src/test/org/apache/lucene/codecs/idversion/StringAndPayloadField.java
@@ -0,0 +1,104 @@
+package org.apache.lucene.codecs.idversion;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.FieldType;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.util.BytesRef;
+
+// TODO: can we take a BytesRef token instead?
+
+/** Produces a single String token from the provided value, with the provided payload. */
+class StringAndPayloadField extends Field {
+
+  public static final FieldType TYPE = new FieldType();
+
+  static {
+    TYPE.setIndexed(true);
+    TYPE.setOmitNorms(true);
+    TYPE.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);
+    TYPE.setTokenized(true);
+    TYPE.freeze();
+  }
+
+  private final BytesRef payload;
+
+  public StringAndPayloadField(String name, String value, BytesRef payload) {
+    super(name, value, TYPE);
+    this.payload = payload;
+  }
+
+  @Override
+  public TokenStream tokenStream(Analyzer analyzer, TokenStream reuse) throws IOException {
+    SingleTokenWithPayloadTokenStream ts;
+    if (reuse instanceof SingleTokenWithPayloadTokenStream) {
+      ts = (SingleTokenWithPayloadTokenStream) reuse;
+    } else {
+      ts = new SingleTokenWithPayloadTokenStream();
+    }
+    ts.setValue((String) fieldsData, payload);
+    return ts;
+  }
+
+  static final class SingleTokenWithPayloadTokenStream extends TokenStream {
+
+    private final CharTermAttribute termAttribute = addAttribute(CharTermAttribute.class);
+    private final PayloadAttribute payloadAttribute = addAttribute(PayloadAttribute.class);
+    private boolean used = false;
+    private String value = null;
+    private BytesRef payload;
+    
+    /** Sets the string value. */
+    void setValue(String value, BytesRef payload) {
+      this.value = value;
+      this.payload = payload;
+    }
+
+    @Override
+    public boolean incrementToken() {
+      if (used) {
+        return false;
+      }
+      clearAttributes();
+      termAttribute.append(value);
+      payloadAttribute.setPayload(payload);
+      used = true;
+      return true;
+    }
+
+    @Override
+    public void reset() {
+      used = false;
+    }
+
+    @Override
+    public void close() {
+      value = null;
+      payload = null;
+    }
+  }
+}
+
+
diff --git a/lucene/sandbox/src/test/org/apache/lucene/codecs/idversion/TestIDVersionPostingsFormat.java b/lucene/sandbox/src/test/org/apache/lucene/codecs/idversion/TestIDVersionPostingsFormat.java
new file mode 100644
index 0000000..1f8f7d2
--- /dev/null
+++ b/lucene/sandbox/src/test/org/apache/lucene/codecs/idversion/TestIDVersionPostingsFormat.java
@@ -0,0 +1,797 @@
+package org.apache.lucene.codecs.idversion;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Locale;
+import java.util.Map;
+import java.util.Set;
+import java.util.concurrent.ConcurrentHashMap;
+import java.util.concurrent.CountDownLatch;
+import java.util.concurrent.atomic.AtomicLong;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.codecs.idversion.StringAndPayloadField.SingleTokenWithPayloadTokenStream;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.FieldType;
+import org.apache.lucene.index.ConcurrentMergeScheduler;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.MergeScheduler;
+import org.apache.lucene.index.PerThreadPKLookup;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.index.TieredMergePolicy;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.LiveFieldValues;
+import org.apache.lucene.search.SearcherFactory;
+import org.apache.lucene.search.SearcherManager;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.TestUtil;
+
+/**
+ * Basic tests for IDVersionPostingsFormat
+ */
+// Cannot extend BasePostingsFormatTestCase because this PF is not
+// general (it requires payloads, only allows 1 doc per term, etc.)
+public class TestIDVersionPostingsFormat extends LuceneTestCase {
+
+  public void testBasic() throws Exception {
+    Directory dir = newDirectory();
+    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
+    iwc.setCodec(TestUtil.alwaysPostingsFormat(new IDVersionPostingsFormat()));
+    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
+    Document doc = new Document();
+    doc.add(makeIDField("id0", 100));
+    w.addDocument(doc);
+    doc = new Document();
+    doc.add(makeIDField("id1", 110));
+    w.addDocument(doc);
+    IndexReader r = w.getReader();
+    IDVersionSegmentTermsEnum termsEnum = (IDVersionSegmentTermsEnum) r.leaves().get(0).reader().fields().terms("id").iterator(null);
+    assertTrue(termsEnum.seekExact(new BytesRef("id0"), 50));
+    assertTrue(termsEnum.seekExact(new BytesRef("id0"), 100));
+    assertFalse(termsEnum.seekExact(new BytesRef("id0"), 101));
+    assertTrue(termsEnum.seekExact(new BytesRef("id1"), 50));
+    assertTrue(termsEnum.seekExact(new BytesRef("id1"), 110));
+    assertFalse(termsEnum.seekExact(new BytesRef("id1"), 111));
+    r.close();
+
+    w.close();
+    dir.close();
+  }
+
+  private interface IDSource {
+    String next();
+  }
+
+  private IDSource getRandomIDs() {
+    IDSource ids;
+    switch (random().nextInt(6)) {
+    case 0:
+      // random simple
+      if (VERBOSE) {
+        System.out.println("TEST: use random simple ids");
+      }
+      ids = new IDSource() {
+          @Override
+          public String next() {
+            return TestUtil.randomSimpleString(random());
+          }
+        };
+      break;
+    case 1:
+      // random realistic unicode
+      if (VERBOSE) {
+        System.out.println("TEST: use random realistic unicode ids");
+      }
+      ids = new IDSource() {
+          @Override
+          public String next() {
+            return TestUtil.randomRealisticUnicodeString(random());
+          }
+        };
+      break;
+    case 2:
+      // sequential
+      if (VERBOSE) {
+        System.out.println("TEST: use seuquential ids");
+      }
+      ids = new IDSource() {
+          int upto;
+          @Override
+          public String next() {
+            return Integer.toString(upto++);
+          }
+        };
+      break;
+    case 3:
+      // zero-pad sequential
+      if (VERBOSE) {
+        System.out.println("TEST: use zero-pad seuquential ids");
+      }
+      ids = new IDSource() {
+          final int radix = TestUtil.nextInt(random(), Character.MIN_RADIX, Character.MAX_RADIX);
+          final String zeroPad = String.format(Locale.ROOT, "%0" + TestUtil.nextInt(random(), 4, 20) + "d", 0);
+          int upto;
+          @Override
+          public String next() {
+            String s = Integer.toString(upto++);
+            return zeroPad.substring(zeroPad.length() - s.length()) + s;
+          }
+        };
+      break;
+    case 4:
+      // random long
+      if (VERBOSE) {
+        System.out.println("TEST: use random long ids");
+      }
+      ids = new IDSource() {
+          final int radix = TestUtil.nextInt(random(), Character.MIN_RADIX, Character.MAX_RADIX);
+          int upto;
+          @Override
+          public String next() {
+            return Long.toString(random().nextLong() & 0x7ffffffffffffffL, radix);
+          }
+        };
+      break;
+    case 5:
+      // zero-pad random long
+      if (VERBOSE) {
+        System.out.println("TEST: use zero-pad random long ids");
+      }
+      ids = new IDSource() {
+          final int radix = TestUtil.nextInt(random(), Character.MIN_RADIX, Character.MAX_RADIX);
+          final String zeroPad = String.format(Locale.ROOT, "%015d", 0);
+          int upto;
+          @Override
+          public String next() {
+            return Long.toString(random().nextLong() & 0x7ffffffffffffffL, radix);
+          }
+        };
+      break;
+    default:
+      throw new AssertionError();
+    }
+
+    return ids;
+  }
+
+  // TODO make a similar test for BT, w/ varied IDs:
+
+  public void testRandom() throws Exception {
+    Directory dir = newDirectory();
+    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
+    int minItemsInBlock = TestUtil.nextInt(random(), 2, 50);
+    int maxItemsInBlock = 2*(minItemsInBlock-1) + random().nextInt(50);
+    iwc.setCodec(TestUtil.alwaysPostingsFormat(new IDVersionPostingsFormat(minItemsInBlock, maxItemsInBlock)));
+    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
+    //IndexWriter w = new IndexWriter(dir, iwc);
+    int numDocs = atLeast(1000);
+    Map<String,Long> idValues = new HashMap<String,Long>();
+    int docUpto = 0;
+    if (VERBOSE) {
+      System.out.println("TEST: numDocs=" + numDocs);
+    }
+
+    IDSource ids = getRandomIDs();
+    String idPrefix;
+    if (random().nextBoolean()) {
+      idPrefix = "";
+    } else {
+      idPrefix = TestUtil.randomSimpleString(random());
+      if (VERBOSE) {
+        System.out.println("TEST: use id prefix: " + idPrefix);
+      }
+    }
+
+    boolean useMonotonicVersion = random().nextBoolean();
+    if (VERBOSE) {
+      System.out.println("TEST: useMonotonicVersion=" + useMonotonicVersion);
+    }
+
+    List<String> idsList = new ArrayList<>();
+
+    long version = 0;
+    while (docUpto < numDocs) {
+      String idValue = idPrefix + ids.next();
+      if (idValues.containsKey(idValue)) {
+        continue;
+      }
+      if (useMonotonicVersion) {
+        version += TestUtil.nextInt(random(), 1, 10);
+      } else {
+        version = random().nextLong() & 0x7fffffffffffffffL;
+      }
+      idValues.put(idValue, version);
+      if (VERBOSE) {
+        System.out.println("  " + idValue + " -> " + version);
+      }
+      Document doc = new Document();
+      doc.add(makeIDField(idValue, version));
+      w.addDocument(doc);
+      idsList.add(idValue);
+
+      if (idsList.size() > 0 && random().nextInt(7) == 5) {
+        // Randomly delete or update a previous ID
+        idValue = idsList.get(random().nextInt(idsList.size()));
+        if (random().nextBoolean()) {
+          if (useMonotonicVersion) {
+            version += TestUtil.nextInt(random(), 1, 10);
+          } else {
+            version = random().nextLong() & 0x7fffffffffffffffL;
+          }
+          doc = new Document();
+          doc.add(makeIDField(idValue, version));
+          if (VERBOSE) {
+            System.out.println("  update " + idValue + " -> " + version);
+          }
+          w.updateDocument(new Term("id", idValue), doc);
+          idValues.put(idValue, version);
+        } else {
+          if (VERBOSE) {
+            System.out.println("  delete " + idValue);
+          }
+          w.deleteDocuments(new Term("id", idValue));
+          idValues.remove(idValue);
+        }        
+      }
+
+      docUpto++;
+    }
+
+    IndexReader r = w.getReader();
+    //IndexReader r = DirectoryReader.open(w, true);
+    PerThreadVersionPKLookup lookup = new PerThreadVersionPKLookup(r, "id");
+
+    List<Map.Entry<String,Long>> idValuesList = new ArrayList<>(idValues.entrySet());
+    int iters = numDocs * 5;
+    for(int iter=0;iter<iters;iter++) {
+      String idValue;
+
+      if (random().nextBoolean()) {
+        idValue = idValuesList.get(random().nextInt(idValuesList.size())).getKey();
+      } else if (random().nextBoolean()) {
+        idValue = ids.next();
+      } else {
+        idValue = idPrefix + TestUtil.randomSimpleString(random());
+      }
+
+      BytesRef idValueBytes = new BytesRef(idValue);
+
+      Long expectedVersion = idValues.get(idValue);
+
+      if (VERBOSE) {
+        System.out.println("\nTEST: iter=" + iter + " id=" + idValue + " expectedVersion=" + expectedVersion);
+      }
+      
+      if (expectedVersion == null) {
+        assertEquals("term should not have been found (doesn't exist)", -1, lookup.lookup(idValueBytes));
+      } else {
+        if (random().nextBoolean()) {
+          if (VERBOSE) {
+            System.out.println("  lookup exact version (should be found)");
+          }
+          assertTrue("term should have been found (version too old)", lookup.lookup(idValueBytes, expectedVersion.longValue()) != -1);
+          assertEquals(expectedVersion.longValue(), lookup.getVersion());
+        } else {
+          if (VERBOSE) {
+            System.out.println("  lookup version+1 (should not be found)");
+          }
+          assertEquals("term should not have been found (version newer)", -1, lookup.lookup(idValueBytes, expectedVersion.longValue()+1));
+        }
+      }
+    }
+
+    r.close();
+    w.close();
+    dir.close();
+  }
+
+  private static class PerThreadVersionPKLookup extends PerThreadPKLookup {
+    public PerThreadVersionPKLookup(IndexReader r, String field) throws IOException {
+      super(r, field);
+    }
+
+    long lastVersion;
+
+    /** Returns docID if found, else -1. */
+    public int lookup(BytesRef id, long version) throws IOException {
+      for(int seg=0;seg<numSegs;seg++) {
+        if (((IDVersionSegmentTermsEnum) termsEnums[seg]).seekExact(id, version)) {
+          if (VERBOSE) {
+            System.out.println("  found in seg=" + termsEnums[seg]);
+          }
+          docsEnums[seg] = termsEnums[seg].docs(liveDocs[seg], docsEnums[seg], 0);
+          int docID = docsEnums[seg].nextDoc();
+          if (docID != DocsEnum.NO_MORE_DOCS) {
+            lastVersion = ((IDVersionSegmentTermsEnum) termsEnums[seg]).getVersion();
+            return docBases[seg] + docID;
+          }
+          assert hasDeletions;
+        }
+      }
+
+      return -1;
+    }
+
+    /** Only valid if lookup returned a valid docID. */
+    public long getVersion() {
+      return lastVersion;
+    }
+  }
+
+  private static Field makeIDField(String id, long version) {
+    BytesRef payload = new BytesRef(8);
+    payload.length = 8;
+    IDVersionPostingsFormat.longToBytes(version, payload);
+    return new StringAndPayloadField("id", id, payload);
+  }
+
+  public void testMoreThanOneDocPerIDOneSegment() throws Exception {
+    Directory dir = newDirectory();
+    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
+    iwc.setCodec(TestUtil.alwaysPostingsFormat(new IDVersionPostingsFormat()));
+    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
+    Document doc = new Document();
+    doc.add(makeIDField("id", 17));
+    w.addDocument(doc);
+    doc = new Document();
+    doc.add(makeIDField("id", 17));
+    w.addDocument(doc);
+    try {
+      w.commit();
+      fail("didn't hit expected exception");
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+    w.close();
+    dir.close();
+  }
+
+  public void testMoreThanOneDocPerIDTwoSegments() throws Exception {
+    Directory dir = newDirectory();
+    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
+    iwc.setCodec(TestUtil.alwaysPostingsFormat(new IDVersionPostingsFormat()));
+    iwc.setMergePolicy(new TieredMergePolicy());
+    MergeScheduler ms = iwc.getMergeScheduler();
+    if (ms instanceof ConcurrentMergeScheduler) {
+      iwc.setMergeScheduler(new ConcurrentMergeScheduler() {
+          @Override
+          protected void handleMergeException(Throwable exc) {
+            assertTrue(exc instanceof IllegalArgumentException);
+          }
+        });
+    }
+    IndexWriter w = new IndexWriter(dir, iwc);
+    Document doc = new Document();
+    doc.add(makeIDField("id", 17));
+    w.addDocument(doc);
+    w.commit();
+    doc = new Document();
+    doc.add(makeIDField("id", 17));
+    try {
+      w.addDocument(doc);
+      w.commit();
+      w.forceMerge(1);
+      fail("didn't hit exception");
+    } catch (IllegalArgumentException iae) {
+      // expected: SMS will hit this
+    } catch (IOException ioe) {
+      // expected
+      assertTrue(ioe.getCause() instanceof IllegalArgumentException);
+    }
+    w.close();
+    dir.close();
+  }
+
+  public void testMoreThanOneDocPerIDWithUpdates() throws Exception {
+    Directory dir = newDirectory();
+    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
+    iwc.setCodec(TestUtil.alwaysPostingsFormat(new IDVersionPostingsFormat()));
+    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
+    Document doc = new Document();
+    doc.add(makeIDField("id", 17));
+    w.addDocument(doc);
+    doc = new Document();
+    doc.add(makeIDField("id", 17));
+    // Replaces the doc we just indexed:
+    w.updateDocument(new Term("id", "id"), doc);
+    w.commit();
+    w.close();
+    dir.close();
+  }
+
+  public void testMoreThanOneDocPerIDWithDeletes() throws Exception {
+    Directory dir = newDirectory();
+    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
+    iwc.setCodec(TestUtil.alwaysPostingsFormat(new IDVersionPostingsFormat()));
+    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
+    Document doc = new Document();
+    doc.add(makeIDField("id", 17));
+    w.addDocument(doc);
+    w.deleteDocuments(new Term("id", "id"));
+    doc = new Document();
+    doc.add(makeIDField("id", 17));
+    w.addDocument(doc);
+    w.commit();
+    w.close();
+    dir.close();
+  }
+
+  public void testMissingPayload() throws Exception {
+    Directory dir = newDirectory();
+    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
+    iwc.setCodec(TestUtil.alwaysPostingsFormat(new IDVersionPostingsFormat()));
+    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
+    Document doc = new Document();
+    doc.add(newTextField("id", "id", Field.Store.NO));
+    try {
+      w.addDocument(doc);
+      w.commit();
+      fail("didn't hit expected exception");
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+             
+    w.close();
+    dir.close();
+  }
+
+  public void testMissingPositions() throws Exception {
+    Directory dir = newDirectory();
+    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
+    iwc.setCodec(TestUtil.alwaysPostingsFormat(new IDVersionPostingsFormat()));
+    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
+    Document doc = new Document();
+    doc.add(newStringField("id", "id", Field.Store.NO));
+    try {
+      w.addDocument(doc);
+      w.commit();
+      fail("didn't hit expected exception");
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+             
+    w.close();
+    dir.close();
+  }
+
+  public void testInvalidPayload() throws Exception {
+    Directory dir = newDirectory();
+    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
+    iwc.setCodec(TestUtil.alwaysPostingsFormat(new IDVersionPostingsFormat()));
+    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
+    Document doc = new Document();
+    doc.add(new StringAndPayloadField("id", "id", new BytesRef("foo")));
+    try {
+      w.addDocument(doc);
+      w.commit();
+      fail("didn't hit expected exception");
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+             
+    w.close();
+    dir.close();
+  }
+
+  public void testMoreThanOneDocPerIDWithDeletesAcrossSegments() throws IOException {
+    Directory dir = newDirectory();
+    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
+    iwc.setCodec(TestUtil.alwaysPostingsFormat(new IDVersionPostingsFormat()));
+    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
+    Document doc = new Document();
+    doc.add(makeIDField("id", 17));
+    w.addDocument(doc);
+    w.commit();
+    doc = new Document();
+    doc.add(makeIDField("id", 17));
+    // Replaces the doc we just indexed:
+    w.updateDocument(new Term("id", "id"), doc);
+    w.forceMerge(1);
+    w.close();
+    dir.close();
+  }
+
+  // LUCENE-5693: because CheckIndex cross-checks term vectors with postings even for deleted docs, and because our PF only indexes the
+  // non-deleted documents on flush, CheckIndex will see this as corruption:
+  public void testCannotIndexTermVectors() throws Exception {
+    Directory dir = newDirectory();
+    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
+    iwc.setCodec(TestUtil.alwaysPostingsFormat(new IDVersionPostingsFormat()));
+    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
+    Document doc = new Document();
+
+    FieldType ft = new FieldType(StringAndPayloadField.TYPE);
+    ft.setStoreTermVectors(true);
+    SingleTokenWithPayloadTokenStream ts = new SingleTokenWithPayloadTokenStream();
+    BytesRef payload = new BytesRef(8);
+    payload.length = 8;
+    IDVersionPostingsFormat.longToBytes(17, payload);
+    ts.setValue("foo", payload);
+    Field field = new Field("id", ts, ft);
+    doc.add(new Field("id", ts, ft));
+    try {
+      w.addDocument(doc);
+      w.commit();
+      fail("didn't hit expected exception");
+    } catch (IllegalArgumentException iae) {
+      // expected
+      // iae.printStackTrace(System.out);
+    }
+    w.close();
+    dir.close();
+  }
+
+  public void testMoreThanOnceInSingleDoc() throws IOException {
+    Directory dir = newDirectory();
+    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
+    iwc.setCodec(TestUtil.alwaysPostingsFormat(new IDVersionPostingsFormat()));
+    RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
+    Document doc = new Document();
+    doc.add(makeIDField("id", 17));
+    doc.add(makeIDField("id", 17));
+    try {
+      w.addDocument(doc);
+      w.commit();
+      fail("didn't hit expected exception");
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+    w.close();
+    dir.close();
+  }
+
+  // Simulates optimistic concurrency in a distributed indexing app and confirms the latest version always wins:
+  public void testGlobalVersions() throws Exception {
+    Directory dir = newDirectory();
+    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
+    iwc.setCodec(TestUtil.alwaysPostingsFormat(new IDVersionPostingsFormat()));
+    final RandomIndexWriter w = new RandomIndexWriter(random(), dir, iwc);
+
+    IDSource idsSource = getRandomIDs();
+    int numIDs = atLeast(100);
+    System.out.println("ids=" + numIDs);
+    if (VERBOSE) {
+      System.out.println("TEST: " + numIDs + " ids");
+    }
+    Set<String> idsSeen = new HashSet<String>();
+    while (idsSeen.size() < numIDs) {
+      idsSeen.add(idsSource.next());
+    }
+    final String[] ids = idsSeen.toArray(new String[numIDs]);
+
+    final Object[] locks = new Object[ids.length];
+    for(int i=0;i<locks.length;i++) {
+      locks[i] = new Object();
+    }
+
+    final AtomicLong nextVersion = new AtomicLong();
+
+    final SearcherManager mgr = new SearcherManager(w.w, true, new SearcherFactory());
+
+    final Long missingValue = -1L;
+
+    final LiveFieldValues<IndexSearcher,Long> versionValues = new LiveFieldValues<IndexSearcher,Long>(mgr, missingValue) {
+      @Override
+      protected Long lookupFromSearcher(IndexSearcher s, String id) {
+        // TODO: would be cleaner if we could do our PerThreadLookup here instead of "up above":
+        // We always return missing: the caller then does a lookup against the current reader
+        return missingValue;
+      }
+    };
+
+    // Maps to the version the id was lasted indexed with:
+    final Map<String,Long> truth = new ConcurrentHashMap<>();
+
+    final CountDownLatch startingGun = new CountDownLatch(1);
+
+    Thread[] threads = new Thread[TestUtil.nextInt(random(), 2, 7)];
+
+    final int versionType = random().nextInt(3);
+
+    if (VERBOSE) {
+      if (versionType == 0) {
+        System.out.println("TEST: use random versions");
+      } else if (versionType == 1) {
+        System.out.println("TEST: use monotonic versions");
+      } else {
+        System.out.println("TEST: use nanotime versions");
+      }
+    }
+
+    // Run for 3 sec in normal tests, else 60 seconds for nightly:
+    final long stopTime = System.currentTimeMillis() + (TEST_NIGHTLY ? 60000 : 3000);
+
+    for(int i=0;i<threads.length;i++) {
+      threads[i] = new Thread() {
+          @Override
+          public void run() {
+            try {
+              runForReal();
+            } catch (Exception e) {
+              throw new RuntimeException(e);
+            }
+          }
+
+          private void runForReal() throws IOException, InterruptedException {
+            startingGun.await();
+            PerThreadVersionPKLookup lookup = null;
+            IndexReader lookupReader = null;
+            while (System.currentTimeMillis() < stopTime) {
+
+              // Intentionally pull version first, and then sleep/yield, to provoke version conflicts:
+              long newVersion;
+              if (versionType == 0) {
+                // Random:
+                newVersion = random().nextLong() & 0x7fffffffffffffffL;
+              } else if (versionType == 1) {
+                // Monotonic
+                newVersion = nextVersion.getAndIncrement();
+              } else {
+                newVersion = System.nanoTime();
+              }
+
+              if (versionType != 0) {
+                if (random().nextBoolean()) {
+                  Thread.yield();
+                } else {
+                  Thread.sleep(TestUtil.nextInt(random(), 1, 4));
+                }
+              }
+
+              int x = random().nextInt(ids.length);
+
+              // TODO: we could relax this, if e.g. we assign indexer thread based on ID.  This would ensure a given ID cannot be indexed at
+              // the same time in multiple threads:
+
+              // Only one thread can update an ID at once:
+              synchronized (locks[x]) {
+
+                String id = ids[x];
+
+                // We will attempt to index id with newVersion, but only do so if id wasn't yet indexed, or it was indexed with an older
+                // version (< newVersion):
+
+                // Must lookup the RT value before pulling from the index, in case a reopen happens just after we lookup:
+                Long currentVersion = versionValues.get(id);
+
+                IndexSearcher s = mgr.acquire();
+                try {
+                  if (VERBOSE) System.out.println("\n" + Thread.currentThread().getName() + ": update id=" + id + " newVersion=" + newVersion);
+
+                  if (lookup == null || lookupReader != s.getIndexReader()) {
+                    // TODO: sort of messy; we could add reopen to PerThreadVersionPKLookup?
+                    // TODO: this is thin ice .... that we don't incRef/decRef this reader we are implicitly holding onto:
+                    lookupReader = s.getIndexReader();
+                    if (VERBOSE) System.out.println(Thread.currentThread().getName() + ": open new PK lookup reader=" + lookupReader);
+                    lookup = new PerThreadVersionPKLookup(lookupReader, "id");
+                  }
+
+                  Long truthVersion = truth.get(id);
+                  if (VERBOSE) System.out.println(Thread.currentThread().getName() + ":   truthVersion=" + truthVersion);
+
+                  boolean doIndex;
+                  if (currentVersion == missingValue) {
+                    if (VERBOSE) System.out.println(Thread.currentThread().getName() + ":   id not in RT cache");
+                    int otherDocID = lookup.lookup(new BytesRef(id), newVersion+1);
+                    if (otherDocID == -1) {
+                      if (VERBOSE) System.out.println(Thread.currentThread().getName() + ":   id not in index, or version is <= newVersion; will index");
+                      doIndex = true;
+                    } else {
+                      if (VERBOSE) System.out.println(Thread.currentThread().getName() + ":   id is in index with version=" + lookup.getVersion() + "; will not index");
+                      doIndex = false;
+                      if (truthVersion.longValue() !=lookup.getVersion()) {
+                        System.out.println(Thread.currentThread() + ": now fail0!");
+                      }
+                      assertEquals(truthVersion.longValue(), lookup.getVersion());
+                    }
+                  } else {
+                    if (VERBOSE) System.out.println(Thread.currentThread().getName() + ":   id is in RT cache: currentVersion=" + currentVersion);
+                    doIndex = newVersion > currentVersion;
+                  }
+
+                  if (doIndex) {
+                    if (VERBOSE) System.out.println(Thread.currentThread().getName() + ":   now index");
+                    boolean passes = truthVersion == null || truthVersion.longValue() <= newVersion;
+                    if (passes == false) {
+                      System.out.println(Thread.currentThread() + ": now fail!");
+                    }
+                    assertTrue(passes);
+                    Document doc = new Document();
+                    doc.add(makeIDField(id, newVersion));
+                    w.updateDocument(new Term("id", id), doc);
+                    truth.put(id, newVersion);
+                    versionValues.add(id, newVersion);
+                  } else {
+                    if (VERBOSE) System.out.println(Thread.currentThread().getName() + ":   skip index");
+                    assertNotNull(truthVersion);
+                    assertTrue(truthVersion.longValue() >= newVersion);
+                  }
+                } finally {
+                  mgr.release(s);
+                }
+              }
+            }
+          }
+        };
+      threads[i].start();
+    }
+
+    startingGun.countDown();
+
+    // Keep reopening the NRT reader until all indexing threads are done:
+    refreshLoop:
+    while (true) {
+      Thread.sleep(TestUtil.nextInt(random(), 1, 10));
+      mgr.maybeRefresh();
+      for (Thread thread : threads) {
+        if (thread.isAlive()) {
+          continue refreshLoop;
+        }
+      }
+
+      break;
+    }
+
+    // Verify final index against truth:
+    for(int i=0;i<2;i++) {
+      mgr.maybeRefresh();
+      IndexSearcher s = mgr.acquire();
+      try {
+        IndexReader r = s.getIndexReader();
+        // cannot assert this: maybe not all IDs were indexed
+        /*
+        assertEquals(numIDs, r.numDocs());
+        if (i == 1) {
+          // After forceMerge no deleted docs:
+          assertEquals(numIDs, r.maxDoc());
+        }
+        */
+        PerThreadVersionPKLookup lookup = new PerThreadVersionPKLookup(r, "id");
+        for(Map.Entry<String,Long> ent : truth.entrySet()) {
+          assertTrue(lookup.lookup(new BytesRef(ent.getKey()), -1L) != -1);
+          assertEquals(ent.getValue().longValue(), lookup.getVersion());
+        }
+      } finally {
+        mgr.release(s);
+      }
+
+      if (i == 1) {
+        break;
+      }
+
+      // forceMerge and verify again
+      w.forceMerge(1);
+    }
+
+    mgr.close();
+    w.close();
+    dir.close();
+  }
+}

