GitDiffStart: 99ac97228163da26865e4682f27070f7aeac8461 | Wed Aug 17 16:43:13 2011 +0000
diff --git a/lucene/src/test-framework/org/apache/lucene/analysis/CollationTestBase.java b/lucene/src/test-framework/org/apache/lucene/analysis/CollationTestBase.java
new file mode 100644
index 0000000..d0cd139
--- /dev/null
+++ b/lucene/src/test-framework/org/apache/lucene/analysis/CollationTestBase.java
@@ -0,0 +1,312 @@
+package org.apache.lucene.analysis;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.analysis.MockTokenizer;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.tokenattributes.TermToBytesRefAttribute;
+import org.apache.lucene.store.RAMDirectory;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.search.ScoreDoc;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.TermRangeFilter;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.TermRangeQuery;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.Sort;
+import org.apache.lucene.search.SortField;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IndexableBinaryStringTools;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util._TestUtil;
+
+import java.io.StringReader;
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.Map;
+
+public abstract class CollationTestBase extends LuceneTestCase {
+
+  protected String firstRangeBeginningOriginal = "\u062F";
+  protected String firstRangeEndOriginal = "\u0698";
+  
+  protected String secondRangeBeginningOriginal = "\u0633";
+  protected String secondRangeEndOriginal = "\u0638";
+  
+  /**
+   * Convenience method to perform the same function as CollationKeyFilter.
+   *  
+   * @param keyBits the result from 
+   *  collator.getCollationKey(original).toByteArray()
+   * @return The encoded collation key for the original String
+   * @deprecated only for testing deprecated filters
+   */
+  @Deprecated
+  protected String encodeCollationKey(byte[] keyBits) {
+    // Ensure that the backing char[] array is large enough to hold the encoded
+    // Binary String
+    int encodedLength = IndexableBinaryStringTools.getEncodedLength(keyBits, 0, keyBits.length);
+    char[] encodedBegArray = new char[encodedLength];
+    IndexableBinaryStringTools.encode(keyBits, 0, keyBits.length, encodedBegArray, 0, encodedLength);
+    return new String(encodedBegArray);
+  }
+  
+  public void testFarsiRangeFilterCollating(Analyzer analyzer, BytesRef firstBeg, 
+                                            BytesRef firstEnd, BytesRef secondBeg,
+                                            BytesRef secondEnd) throws Exception {
+    RAMDirectory ramDir = new RAMDirectory();
+    IndexWriter writer = new IndexWriter(ramDir, new IndexWriterConfig(
+        TEST_VERSION_CURRENT, analyzer));
+    Document doc = new Document();
+    doc.add(new Field("content", "\u0633\u0627\u0628", 
+                      Field.Store.YES, Field.Index.ANALYZED));
+    doc.add(new Field("body", "body",
+                      Field.Store.YES, Field.Index.NOT_ANALYZED));
+    writer.addDocument(doc);
+    writer.close();
+    IndexSearcher searcher = new IndexSearcher(ramDir, true);
+    Query query = new TermQuery(new Term("body","body"));
+
+    // Unicode order would include U+0633 in [ U+062F - U+0698 ], but Farsi
+    // orders the U+0698 character before the U+0633 character, so the single
+    // index Term below should NOT be returned by a TermRangeFilter with a Farsi
+    // Collator (or an Arabic one for the case when Farsi searcher not
+    // supported).
+    ScoreDoc[] result = searcher.search
+      (query, new TermRangeFilter("content", firstBeg, firstEnd, true, true), 1).scoreDocs;
+    assertEquals("The index Term should not be included.", 0, result.length);
+
+    result = searcher.search
+      (query, new TermRangeFilter("content", secondBeg, secondEnd, true, true), 1).scoreDocs;
+    assertEquals("The index Term should be included.", 1, result.length);
+
+    searcher.close();
+  }
+ 
+  public void testFarsiRangeQueryCollating(Analyzer analyzer, BytesRef firstBeg, 
+                                            BytesRef firstEnd, BytesRef secondBeg,
+                                            BytesRef secondEnd) throws Exception {
+    RAMDirectory ramDir = new RAMDirectory();
+    IndexWriter writer = new IndexWriter(ramDir, new IndexWriterConfig(
+        TEST_VERSION_CURRENT, analyzer));
+    Document doc = new Document();
+
+    // Unicode order would include U+0633 in [ U+062F - U+0698 ], but Farsi
+    // orders the U+0698 character before the U+0633 character, so the single
+    // index Term below should NOT be returned by a TermRangeQuery with a Farsi
+    // Collator (or an Arabic one for the case when Farsi is not supported).
+    doc.add(new Field("content", "\u0633\u0627\u0628", 
+                      Field.Store.YES, Field.Index.ANALYZED));
+    writer.addDocument(doc);
+    writer.close();
+    IndexSearcher searcher = new IndexSearcher(ramDir, true);
+
+    Query query = new TermRangeQuery("content", firstBeg, firstEnd, true, true);
+    ScoreDoc[] hits = searcher.search(query, null, 1000).scoreDocs;
+    assertEquals("The index Term should not be included.", 0, hits.length);
+
+    query = new TermRangeQuery("content", secondBeg, secondEnd, true, true);
+    hits = searcher.search(query, null, 1000).scoreDocs;
+    assertEquals("The index Term should be included.", 1, hits.length);
+    searcher.close();
+  }
+
+  public void testFarsiTermRangeQuery(Analyzer analyzer, BytesRef firstBeg,
+      BytesRef firstEnd, BytesRef secondBeg, BytesRef secondEnd) throws Exception {
+
+    RAMDirectory farsiIndex = new RAMDirectory();
+    IndexWriter writer = new IndexWriter(farsiIndex, new IndexWriterConfig(
+        TEST_VERSION_CURRENT, analyzer));
+    Document doc = new Document();
+    doc.add(new Field("content", "\u0633\u0627\u0628", 
+                      Field.Store.YES, Field.Index.ANALYZED));
+    doc.add(new Field("body", "body",
+                      Field.Store.YES, Field.Index.NOT_ANALYZED));
+    writer.addDocument(doc);
+    writer.close();
+
+    IndexReader reader = IndexReader.open(farsiIndex, true);
+    IndexSearcher search = newSearcher(reader);
+        
+    // Unicode order would include U+0633 in [ U+062F - U+0698 ], but Farsi
+    // orders the U+0698 character before the U+0633 character, so the single
+    // index Term below should NOT be returned by a TermRangeQuery
+    // with a Farsi Collator (or an Arabic one for the case when Farsi is 
+    // not supported).
+    Query csrq 
+      = new TermRangeQuery("content", firstBeg, firstEnd, true, true);
+    ScoreDoc[] result = search.search(csrq, null, 1000).scoreDocs;
+    assertEquals("The index Term should not be included.", 0, result.length);
+
+    csrq = new TermRangeQuery
+      ("content", secondBeg, secondEnd, true, true);
+    result = search.search(csrq, null, 1000).scoreDocs;
+    assertEquals("The index Term should be included.", 1, result.length);
+    search.close();
+  }
+  
+  // Test using various international locales with accented characters (which
+  // sort differently depending on locale)
+  //
+  // Copied (and slightly modified) from 
+  // org.apache.lucene.search.TestSort.testInternationalSort()
+  //  
+  // TODO: this test is really fragile. there are already 3 different cases,
+  // depending upon unicode version.
+  public void testCollationKeySort(Analyzer usAnalyzer,
+                                   Analyzer franceAnalyzer,
+                                   Analyzer swedenAnalyzer,
+                                   Analyzer denmarkAnalyzer,
+                                   String usResult,
+                                   String frResult,
+                                   String svResult,
+                                   String dkResult) throws Exception {
+    RAMDirectory indexStore = new RAMDirectory();
+    IndexWriter writer = new IndexWriter(indexStore, new IndexWriterConfig(
+        TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));
+
+    // document data:
+    // the tracer field is used to determine which document was hit
+    String[][] sortData = new String[][] {
+      // tracer contents US                 France             Sweden (sv_SE)     Denmark (da_DK)
+      {  "A",   "x",     "p\u00EAche",      "p\u00EAche",      "p\u00EAche",      "p\u00EAche"      },
+      {  "B",   "y",     "HAT",             "HAT",             "HAT",             "HAT"             },
+      {  "C",   "x",     "p\u00E9ch\u00E9", "p\u00E9ch\u00E9", "p\u00E9ch\u00E9", "p\u00E9ch\u00E9" },
+      {  "D",   "y",     "HUT",             "HUT",             "HUT",             "HUT"             },
+      {  "E",   "x",     "peach",           "peach",           "peach",           "peach"           },
+      {  "F",   "y",     "H\u00C5T",        "H\u00C5T",        "H\u00C5T",        "H\u00C5T"        },
+      {  "G",   "x",     "sin",             "sin",             "sin",             "sin"             },
+      {  "H",   "y",     "H\u00D8T",        "H\u00D8T",        "H\u00D8T",        "H\u00D8T"        },
+      {  "I",   "x",     "s\u00EDn",        "s\u00EDn",        "s\u00EDn",        "s\u00EDn"        },
+      {  "J",   "y",     "HOT",             "HOT",             "HOT",             "HOT"             },
+    };
+
+    for (int i = 0 ; i < sortData.length ; ++i) {
+      Document doc = new Document();
+      doc.add(new Field("tracer", sortData[i][0], 
+                        Field.Store.YES, Field.Index.NO));
+      doc.add(new Field("contents", sortData[i][1], 
+                        Field.Store.NO, Field.Index.ANALYZED));
+      if (sortData[i][2] != null) 
+        doc.add(new Field("US", usAnalyzer.reusableTokenStream("US", new StringReader(sortData[i][2]))));
+      if (sortData[i][3] != null) 
+        doc.add(new Field("France", franceAnalyzer.reusableTokenStream("France", new StringReader(sortData[i][3]))));
+      if (sortData[i][4] != null)
+        doc.add(new Field("Sweden", swedenAnalyzer.reusableTokenStream("Sweden", new StringReader(sortData[i][4]))));
+      if (sortData[i][5] != null) 
+        doc.add(new Field("Denmark", denmarkAnalyzer.reusableTokenStream("Denmark", new StringReader(sortData[i][5]))));
+      writer.addDocument(doc);
+    }
+    writer.optimize();
+    writer.close();
+    IndexSearcher searcher = new IndexSearcher(indexStore, true);
+
+    Sort sort = new Sort();
+    Query queryX = new TermQuery(new Term ("contents", "x"));
+    Query queryY = new TermQuery(new Term ("contents", "y"));
+    
+    sort.setSort(new SortField("US", SortField.Type.STRING));
+    assertMatches(searcher, queryY, sort, usResult);
+
+    sort.setSort(new SortField("France", SortField.Type.STRING));
+    assertMatches(searcher, queryX, sort, frResult);
+
+    sort.setSort(new SortField("Sweden", SortField.Type.STRING));
+    assertMatches(searcher, queryY, sort, svResult);
+
+    sort.setSort(new SortField("Denmark", SortField.Type.STRING));
+    assertMatches(searcher, queryY, sort, dkResult);
+  }
+    
+  // Make sure the documents returned by the search match the expected list
+  // Copied from TestSort.java
+  private void assertMatches(IndexSearcher searcher, Query query, Sort sort, 
+                             String expectedResult) throws IOException {
+    ScoreDoc[] result = searcher.search(query, null, 1000, sort).scoreDocs;
+    StringBuilder buff = new StringBuilder(10);
+    int n = result.length;
+    for (int i = 0 ; i < n ; ++i) {
+      Document doc = searcher.doc(result[i].doc);
+      String[] v = doc.getValues("tracer");
+      for (int j = 0 ; j < v.length ; ++j) {
+        buff.append(v[j]);
+      }
+    }
+    assertEquals(expectedResult, buff.toString());
+  }
+
+  public void assertThreadSafe(final Analyzer analyzer) throws Exception {
+    int numTestPoints = 100;
+    int numThreads = _TestUtil.nextInt(random, 3, 5);
+    final HashMap<String,BytesRef> map = new HashMap<String,BytesRef>();
+    
+    // create a map<String,SortKey> up front.
+    // then with multiple threads, generate sort keys for all the keys in the map
+    // and ensure they are the same as the ones we produced in serial fashion.
+
+    for (int i = 0; i < numTestPoints; i++) {
+      String term = _TestUtil.randomSimpleString(random);
+      TokenStream ts = analyzer.reusableTokenStream("fake", new StringReader(term));
+      TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);
+      BytesRef bytes = termAtt.getBytesRef();
+      ts.reset();
+      assertTrue(ts.incrementToken());
+      termAtt.fillBytesRef();
+      // ensure we make a copy of the actual bytes too
+      map.put(term, new BytesRef(bytes));
+    }
+    
+    Thread threads[] = new Thread[numThreads];
+    for (int i = 0; i < numThreads; i++) {
+      threads[i] = new Thread() {
+        @Override
+        public void run() {
+          try {
+            for (Map.Entry<String,BytesRef> mapping : map.entrySet()) {
+              String term = mapping.getKey();
+              BytesRef expected = mapping.getValue();
+              TokenStream ts = analyzer.reusableTokenStream("fake", new StringReader(term));
+              TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);
+              BytesRef bytes = termAtt.getBytesRef();
+              ts.reset();
+              assertTrue(ts.incrementToken());
+              termAtt.fillBytesRef();
+              assertEquals(expected, bytes);
+            }
+          } catch (IOException e) {
+            throw new RuntimeException(e);
+          }
+        }
+      };
+    }
+    for (int i = 0; i < numThreads; i++) {
+      threads[i].start();
+    }
+    for (int i = 0; i < numThreads; i++) {
+      threads[i].join();
+    }
+  }
+}
diff --git a/modules/analysis/common/src/test/org/apache/lucene/collation/CollationTestBase.java b/modules/analysis/common/src/test/org/apache/lucene/collation/CollationTestBase.java
deleted file mode 100644
index 1c7084e..0000000
--- a/modules/analysis/common/src/test/org/apache/lucene/collation/CollationTestBase.java
+++ /dev/null
@@ -1,312 +0,0 @@
-package org.apache.lucene.collation;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.analysis.MockTokenizer;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.tokenattributes.TermToBytesRefAttribute;
-import org.apache.lucene.store.RAMDirectory;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.search.ScoreDoc;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TermRangeFilter;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.search.TermRangeQuery;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.Sort;
-import org.apache.lucene.search.SortField;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IndexableBinaryStringTools;
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util._TestUtil;
-
-import java.io.StringReader;
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-
-public abstract class CollationTestBase extends LuceneTestCase {
-
-  protected String firstRangeBeginningOriginal = "\u062F";
-  protected String firstRangeEndOriginal = "\u0698";
-  
-  protected String secondRangeBeginningOriginal = "\u0633";
-  protected String secondRangeEndOriginal = "\u0638";
-  
-  /**
-   * Convenience method to perform the same function as CollationKeyFilter.
-   *  
-   * @param keyBits the result from 
-   *  collator.getCollationKey(original).toByteArray()
-   * @return The encoded collation key for the original String
-   * @deprecated only for testing deprecated filters
-   */
-  @Deprecated
-  protected String encodeCollationKey(byte[] keyBits) {
-    // Ensure that the backing char[] array is large enough to hold the encoded
-    // Binary String
-    int encodedLength = IndexableBinaryStringTools.getEncodedLength(keyBits, 0, keyBits.length);
-    char[] encodedBegArray = new char[encodedLength];
-    IndexableBinaryStringTools.encode(keyBits, 0, keyBits.length, encodedBegArray, 0, encodedLength);
-    return new String(encodedBegArray);
-  }
-  
-  public void testFarsiRangeFilterCollating(Analyzer analyzer, BytesRef firstBeg, 
-                                            BytesRef firstEnd, BytesRef secondBeg,
-                                            BytesRef secondEnd) throws Exception {
-    RAMDirectory ramDir = new RAMDirectory();
-    IndexWriter writer = new IndexWriter(ramDir, new IndexWriterConfig(
-        TEST_VERSION_CURRENT, analyzer));
-    Document doc = new Document();
-    doc.add(new Field("content", "\u0633\u0627\u0628", 
-                      Field.Store.YES, Field.Index.ANALYZED));
-    doc.add(new Field("body", "body",
-                      Field.Store.YES, Field.Index.NOT_ANALYZED));
-    writer.addDocument(doc);
-    writer.close();
-    IndexSearcher searcher = new IndexSearcher(ramDir, true);
-    Query query = new TermQuery(new Term("body","body"));
-
-    // Unicode order would include U+0633 in [ U+062F - U+0698 ], but Farsi
-    // orders the U+0698 character before the U+0633 character, so the single
-    // index Term below should NOT be returned by a TermRangeFilter with a Farsi
-    // Collator (or an Arabic one for the case when Farsi searcher not
-    // supported).
-    ScoreDoc[] result = searcher.search
-      (query, new TermRangeFilter("content", firstBeg, firstEnd, true, true), 1).scoreDocs;
-    assertEquals("The index Term should not be included.", 0, result.length);
-
-    result = searcher.search
-      (query, new TermRangeFilter("content", secondBeg, secondEnd, true, true), 1).scoreDocs;
-    assertEquals("The index Term should be included.", 1, result.length);
-
-    searcher.close();
-  }
- 
-  public void testFarsiRangeQueryCollating(Analyzer analyzer, BytesRef firstBeg, 
-                                            BytesRef firstEnd, BytesRef secondBeg,
-                                            BytesRef secondEnd) throws Exception {
-    RAMDirectory ramDir = new RAMDirectory();
-    IndexWriter writer = new IndexWriter(ramDir, new IndexWriterConfig(
-        TEST_VERSION_CURRENT, analyzer));
-    Document doc = new Document();
-
-    // Unicode order would include U+0633 in [ U+062F - U+0698 ], but Farsi
-    // orders the U+0698 character before the U+0633 character, so the single
-    // index Term below should NOT be returned by a TermRangeQuery with a Farsi
-    // Collator (or an Arabic one for the case when Farsi is not supported).
-    doc.add(new Field("content", "\u0633\u0627\u0628", 
-                      Field.Store.YES, Field.Index.ANALYZED));
-    writer.addDocument(doc);
-    writer.close();
-    IndexSearcher searcher = new IndexSearcher(ramDir, true);
-
-    Query query = new TermRangeQuery("content", firstBeg, firstEnd, true, true);
-    ScoreDoc[] hits = searcher.search(query, null, 1000).scoreDocs;
-    assertEquals("The index Term should not be included.", 0, hits.length);
-
-    query = new TermRangeQuery("content", secondBeg, secondEnd, true, true);
-    hits = searcher.search(query, null, 1000).scoreDocs;
-    assertEquals("The index Term should be included.", 1, hits.length);
-    searcher.close();
-  }
-
-  public void testFarsiTermRangeQuery(Analyzer analyzer, BytesRef firstBeg,
-      BytesRef firstEnd, BytesRef secondBeg, BytesRef secondEnd) throws Exception {
-
-    RAMDirectory farsiIndex = new RAMDirectory();
-    IndexWriter writer = new IndexWriter(farsiIndex, new IndexWriterConfig(
-        TEST_VERSION_CURRENT, analyzer));
-    Document doc = new Document();
-    doc.add(new Field("content", "\u0633\u0627\u0628", 
-                      Field.Store.YES, Field.Index.ANALYZED));
-    doc.add(new Field("body", "body",
-                      Field.Store.YES, Field.Index.NOT_ANALYZED));
-    writer.addDocument(doc);
-    writer.close();
-
-    IndexReader reader = IndexReader.open(farsiIndex, true);
-    IndexSearcher search = newSearcher(reader);
-        
-    // Unicode order would include U+0633 in [ U+062F - U+0698 ], but Farsi
-    // orders the U+0698 character before the U+0633 character, so the single
-    // index Term below should NOT be returned by a TermRangeQuery
-    // with a Farsi Collator (or an Arabic one for the case when Farsi is 
-    // not supported).
-    Query csrq 
-      = new TermRangeQuery("content", firstBeg, firstEnd, true, true);
-    ScoreDoc[] result = search.search(csrq, null, 1000).scoreDocs;
-    assertEquals("The index Term should not be included.", 0, result.length);
-
-    csrq = new TermRangeQuery
-      ("content", secondBeg, secondEnd, true, true);
-    result = search.search(csrq, null, 1000).scoreDocs;
-    assertEquals("The index Term should be included.", 1, result.length);
-    search.close();
-  }
-  
-  // Test using various international locales with accented characters (which
-  // sort differently depending on locale)
-  //
-  // Copied (and slightly modified) from 
-  // org.apache.lucene.search.TestSort.testInternationalSort()
-  //  
-  // TODO: this test is really fragile. there are already 3 different cases,
-  // depending upon unicode version.
-  public void testCollationKeySort(Analyzer usAnalyzer,
-                                   Analyzer franceAnalyzer,
-                                   Analyzer swedenAnalyzer,
-                                   Analyzer denmarkAnalyzer,
-                                   String usResult,
-                                   String frResult,
-                                   String svResult,
-                                   String dkResult) throws Exception {
-    RAMDirectory indexStore = new RAMDirectory();
-    IndexWriter writer = new IndexWriter(indexStore, new IndexWriterConfig(
-        TEST_VERSION_CURRENT, new MockAnalyzer(random, MockTokenizer.WHITESPACE, false)));
-
-    // document data:
-    // the tracer field is used to determine which document was hit
-    String[][] sortData = new String[][] {
-      // tracer contents US                 France             Sweden (sv_SE)     Denmark (da_DK)
-      {  "A",   "x",     "p\u00EAche",      "p\u00EAche",      "p\u00EAche",      "p\u00EAche"      },
-      {  "B",   "y",     "HAT",             "HAT",             "HAT",             "HAT"             },
-      {  "C",   "x",     "p\u00E9ch\u00E9", "p\u00E9ch\u00E9", "p\u00E9ch\u00E9", "p\u00E9ch\u00E9" },
-      {  "D",   "y",     "HUT",             "HUT",             "HUT",             "HUT"             },
-      {  "E",   "x",     "peach",           "peach",           "peach",           "peach"           },
-      {  "F",   "y",     "H\u00C5T",        "H\u00C5T",        "H\u00C5T",        "H\u00C5T"        },
-      {  "G",   "x",     "sin",             "sin",             "sin",             "sin"             },
-      {  "H",   "y",     "H\u00D8T",        "H\u00D8T",        "H\u00D8T",        "H\u00D8T"        },
-      {  "I",   "x",     "s\u00EDn",        "s\u00EDn",        "s\u00EDn",        "s\u00EDn"        },
-      {  "J",   "y",     "HOT",             "HOT",             "HOT",             "HOT"             },
-    };
-
-    for (int i = 0 ; i < sortData.length ; ++i) {
-      Document doc = new Document();
-      doc.add(new Field("tracer", sortData[i][0], 
-                        Field.Store.YES, Field.Index.NO));
-      doc.add(new Field("contents", sortData[i][1], 
-                        Field.Store.NO, Field.Index.ANALYZED));
-      if (sortData[i][2] != null) 
-        doc.add(new Field("US", usAnalyzer.reusableTokenStream("US", new StringReader(sortData[i][2]))));
-      if (sortData[i][3] != null) 
-        doc.add(new Field("France", franceAnalyzer.reusableTokenStream("France", new StringReader(sortData[i][3]))));
-      if (sortData[i][4] != null)
-        doc.add(new Field("Sweden", swedenAnalyzer.reusableTokenStream("Sweden", new StringReader(sortData[i][4]))));
-      if (sortData[i][5] != null) 
-        doc.add(new Field("Denmark", denmarkAnalyzer.reusableTokenStream("Denmark", new StringReader(sortData[i][5]))));
-      writer.addDocument(doc);
-    }
-    writer.optimize();
-    writer.close();
-    IndexSearcher searcher = new IndexSearcher(indexStore, true);
-
-    Sort sort = new Sort();
-    Query queryX = new TermQuery(new Term ("contents", "x"));
-    Query queryY = new TermQuery(new Term ("contents", "y"));
-    
-    sort.setSort(new SortField("US", SortField.Type.STRING));
-    assertMatches(searcher, queryY, sort, usResult);
-
-    sort.setSort(new SortField("France", SortField.Type.STRING));
-    assertMatches(searcher, queryX, sort, frResult);
-
-    sort.setSort(new SortField("Sweden", SortField.Type.STRING));
-    assertMatches(searcher, queryY, sort, svResult);
-
-    sort.setSort(new SortField("Denmark", SortField.Type.STRING));
-    assertMatches(searcher, queryY, sort, dkResult);
-  }
-    
-  // Make sure the documents returned by the search match the expected list
-  // Copied from TestSort.java
-  private void assertMatches(IndexSearcher searcher, Query query, Sort sort, 
-                             String expectedResult) throws IOException {
-    ScoreDoc[] result = searcher.search(query, null, 1000, sort).scoreDocs;
-    StringBuilder buff = new StringBuilder(10);
-    int n = result.length;
-    for (int i = 0 ; i < n ; ++i) {
-      Document doc = searcher.doc(result[i].doc);
-      String[] v = doc.getValues("tracer");
-      for (int j = 0 ; j < v.length ; ++j) {
-        buff.append(v[j]);
-      }
-    }
-    assertEquals(expectedResult, buff.toString());
-  }
-
-  public void assertThreadSafe(final Analyzer analyzer) throws Exception {
-    int numTestPoints = 100;
-    int numThreads = _TestUtil.nextInt(random, 3, 5);
-    final HashMap<String,BytesRef> map = new HashMap<String,BytesRef>();
-    
-    // create a map<String,SortKey> up front.
-    // then with multiple threads, generate sort keys for all the keys in the map
-    // and ensure they are the same as the ones we produced in serial fashion.
-
-    for (int i = 0; i < numTestPoints; i++) {
-      String term = _TestUtil.randomSimpleString(random);
-      TokenStream ts = analyzer.reusableTokenStream("fake", new StringReader(term));
-      TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);
-      BytesRef bytes = termAtt.getBytesRef();
-      ts.reset();
-      assertTrue(ts.incrementToken());
-      termAtt.fillBytesRef();
-      // ensure we make a copy of the actual bytes too
-      map.put(term, new BytesRef(bytes));
-    }
-    
-    Thread threads[] = new Thread[numThreads];
-    for (int i = 0; i < numThreads; i++) {
-      threads[i] = new Thread() {
-        @Override
-        public void run() {
-          try {
-            for (Map.Entry<String,BytesRef> mapping : map.entrySet()) {
-              String term = mapping.getKey();
-              BytesRef expected = mapping.getValue();
-              TokenStream ts = analyzer.reusableTokenStream("fake", new StringReader(term));
-              TermToBytesRefAttribute termAtt = ts.addAttribute(TermToBytesRefAttribute.class);
-              BytesRef bytes = termAtt.getBytesRef();
-              ts.reset();
-              assertTrue(ts.incrementToken());
-              termAtt.fillBytesRef();
-              assertEquals(expected, bytes);
-            }
-          } catch (IOException e) {
-            throw new RuntimeException(e);
-          }
-        }
-      };
-    }
-    for (int i = 0; i < numThreads; i++) {
-      threads[i].start();
-    }
-    for (int i = 0; i < numThreads; i++) {
-      threads[i].join();
-    }
-  }
-}
diff --git a/modules/analysis/common/src/test/org/apache/lucene/collation/TestCollationKeyAnalyzer.java b/modules/analysis/common/src/test/org/apache/lucene/collation/TestCollationKeyAnalyzer.java
index bff441c..e3a0a29 100644
--- a/modules/analysis/common/src/test/org/apache/lucene/collation/TestCollationKeyAnalyzer.java
+++ b/modules/analysis/common/src/test/org/apache/lucene/collation/TestCollationKeyAnalyzer.java
@@ -19,6 +19,7 @@ package org.apache.lucene.collation;
 
 
 import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.CollationTestBase;
 import org.apache.lucene.index.codecs.CodecProvider;
 import org.apache.lucene.util.BytesRef;
 
diff --git a/modules/analysis/common/src/test/org/apache/lucene/collation/TestCollationKeyFilter.java b/modules/analysis/common/src/test/org/apache/lucene/collation/TestCollationKeyFilter.java
index c500df8..348cb30 100644
--- a/modules/analysis/common/src/test/org/apache/lucene/collation/TestCollationKeyFilter.java
+++ b/modules/analysis/common/src/test/org/apache/lucene/collation/TestCollationKeyFilter.java
@@ -18,6 +18,7 @@ package org.apache.lucene.collation;
  */
 
 
+import org.apache.lucene.analysis.CollationTestBase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.core.KeywordTokenizer;
diff --git a/modules/analysis/icu/build.xml b/modules/analysis/icu/build.xml
index 9ca66c8..b3a313a 100644
--- a/modules/analysis/icu/build.xml
+++ b/modules/analysis/icu/build.xml
@@ -43,16 +43,6 @@
     <path refid="base.classpath"/>
   </path>
 
-  <path id="test.classpath">
-  	<pathelement path="${analyzers-common.jar}"/>
-    <path refid="classpath"/>
-    <pathelement location="../../../lucene/build/classes/test-framework/"/>
-    <pathelement location="../../../lucene/build/classes/test/"/>
-  	<pathelement location="../build/common/classes/test/"/>
-    <path refid="junit-path"/>
-    <pathelement location="${build.dir}/classes/java"/>
-  </path>
-
   <target name="compile-core" depends="jar-analyzers-common, common.compile-core" />
 
   <property name="gennorm2.src.dir" value="src/data/utr30"/>
diff --git a/modules/analysis/icu/src/test/org/apache/lucene/collation/TestICUCollationKeyAnalyzer.java b/modules/analysis/icu/src/test/org/apache/lucene/collation/TestICUCollationKeyAnalyzer.java
index dc6a3e8..305ee27 100644
--- a/modules/analysis/icu/src/test/org/apache/lucene/collation/TestICUCollationKeyAnalyzer.java
+++ b/modules/analysis/icu/src/test/org/apache/lucene/collation/TestICUCollationKeyAnalyzer.java
@@ -21,6 +21,7 @@ package org.apache.lucene.collation;
 import com.ibm.icu.text.Collator;
 
 import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.CollationTestBase;
 import org.apache.lucene.index.codecs.CodecProvider;
 import org.apache.lucene.util.BytesRef;
 
diff --git a/modules/analysis/icu/src/test/org/apache/lucene/collation/TestICUCollationKeyFilter.java b/modules/analysis/icu/src/test/org/apache/lucene/collation/TestICUCollationKeyFilter.java
index d05d80a..68b0ec9 100644
--- a/modules/analysis/icu/src/test/org/apache/lucene/collation/TestICUCollationKeyFilter.java
+++ b/modules/analysis/icu/src/test/org/apache/lucene/collation/TestICUCollationKeyFilter.java
@@ -19,6 +19,8 @@ package org.apache.lucene.collation;
 
 
 import com.ibm.icu.text.Collator;
+
+import org.apache.lucene.analysis.CollationTestBase;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.core.KeywordTokenizer;

