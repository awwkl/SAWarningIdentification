GitDiffStart: b8ec4728349860af50500d598a6dca6e810e99f3 | Tue Oct 9 23:06:03 2012 +0000
diff --git a/lucene/CHANGES.txt b/lucene/CHANGES.txt
index d4125f9..8616104 100644
--- a/lucene/CHANGES.txt
+++ b/lucene/CHANGES.txt
@@ -54,6 +54,9 @@ Bug Fixes
 * LUCENE-1822: BaseFragListBuilder hard-coded 6 char margin is too naive.
   (Alex Vigdor, Arcadius Ahouansou, Koji Sekiguchi)
 
+* LUCENE-4468: Fix rareish integer overflows in Block and Lucene40 postings 
+  formats (Robert Muir)
+
 Optimizations
 
 * LUCENE-4443: BlockPostingsFormat no longer writes unnecessary offsets 
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/block/BlockPostingsFormat.java b/lucene/codecs/src/java/org/apache/lucene/codecs/block/BlockPostingsFormat.java
index c8d97a4..b8030d0 100644
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/block/BlockPostingsFormat.java
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/block/BlockPostingsFormat.java
@@ -130,8 +130,8 @@ import org.apache.lucene.util.packed.PackedInts;
  *   <li>Term Metadata --&gt; DocFPDelta, PosFPDelta?, PosVIntBlockFPDelta?, PayFPDelta?, 
  *                            SkipFPDelta?</li>
  *   <li>Header, --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
- *   <li>PackedBlockSize, PosVIntBlockFPDelta, SkipFPDelta --&gt; {@link DataOutput#writeVInt VInt}</li>
- *   <li>DocFPDelta, PosFPDelta, PayFPDelta --&gt; {@link DataOutput#writeVLong VLong}</li>
+ *   <li>PackedBlockSize --&gt; {@link DataOutput#writeVInt VInt}</li>
+ *   <li>DocFPDelta, PosFPDelta, PayFPDelta, PosVIntBlockFPDelta, SkipFPDelta --&gt; {@link DataOutput#writeVLong VLong}</li>
  * </ul>
  * <p>Notes:</p>
  * <ul>
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/block/BlockPostingsReader.java b/lucene/codecs/src/java/org/apache/lucene/codecs/block/BlockPostingsReader.java
index 23d684e..a6058c7 100644
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/block/BlockPostingsReader.java
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/block/BlockPostingsReader.java
@@ -145,8 +145,8 @@ final class BlockPostingsReader extends PostingsReaderBase {
     long docStartFP;
     long posStartFP;
     long payStartFP;
-    int skipOffset;
-    int lastPosBlockOffset;
+    long skipOffset;
+    long lastPosBlockOffset;
 
     // Only used by the "primary" TermState -- clones don't
     // copy this (basically they are "transient"):
@@ -226,7 +226,7 @@ final class BlockPostingsReader extends PostingsReaderBase {
       if (fieldHasPositions) {
         termState.posStartFP = in.readVLong();
         if (termState.totalTermFreq > BLOCK_SIZE) {
-          termState.lastPosBlockOffset = in.readVInt();
+          termState.lastPosBlockOffset = in.readVLong();
         } else {
           termState.lastPosBlockOffset = -1;
         }
@@ -241,7 +241,7 @@ final class BlockPostingsReader extends PostingsReaderBase {
       if (fieldHasPositions) {
         termState.posStartFP += in.readVLong();
         if (termState.totalTermFreq > BLOCK_SIZE) {
-          termState.lastPosBlockOffset = in.readVInt();
+          termState.lastPosBlockOffset = in.readVLong();
         } else {
           termState.lastPosBlockOffset = -1;
         }
@@ -257,7 +257,7 @@ final class BlockPostingsReader extends PostingsReaderBase {
     }
 
     if (termState.docFreq > BLOCK_SIZE) {
-      termState.skipOffset = in.readVInt();
+      termState.skipOffset = in.readVLong();
     } else {
       termState.skipOffset = -1;
     }
@@ -344,7 +344,7 @@ final class BlockPostingsReader extends PostingsReaderBase {
     // Where this term's skip data starts (after
     // docTermStartFP) in the .doc file (or -1 if there is
     // no skip data for this term):
-    private int skipOffset;
+    private long skipOffset;
 
     // docID for next skip point, we won't use skipper if 
     // target docID is not larger than this
@@ -621,7 +621,7 @@ final class BlockPostingsReader extends PostingsReaderBase {
     // Where this term's skip data starts (after
     // docTermStartFP) in the .doc file (or -1 if there is
     // no skip data for this term):
-    private int skipOffset;
+    private long skipOffset;
 
     private int nextSkipDoc;
 
@@ -1035,7 +1035,7 @@ final class BlockPostingsReader extends PostingsReaderBase {
     // Where this term's skip data starts (after
     // docTermStartFP) in the .doc file (or -1 if there is
     // no skip data for this term):
-    private int skipOffset;
+    private long skipOffset;
 
     private int nextSkipDoc;
 
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/block/BlockPostingsWriter.java b/lucene/codecs/src/java/org/apache/lucene/codecs/block/BlockPostingsWriter.java
index d07547e..30f4602 100644
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/block/BlockPostingsWriter.java
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/block/BlockPostingsWriter.java
@@ -350,10 +350,10 @@ final class BlockPostingsWriter extends PostingsWriterBase {
     public final long docStartFP;
     public final long posStartFP;
     public final long payStartFP;
-    public final int skipOffset;
-    public final int lastPosBlockOffset;
+    public final long skipOffset;
+    public final long lastPosBlockOffset;
 
-    public PendingTerm(long docStartFP, long posStartFP, long payStartFP, int skipOffset, int lastPosBlockOffset) {
+    public PendingTerm(long docStartFP, long posStartFP, long payStartFP, long skipOffset, long lastPosBlockOffset) {
       this.docStartFP = docStartFP;
       this.posStartFP = posStartFP;
       this.payStartFP = payStartFP;
@@ -397,7 +397,7 @@ final class BlockPostingsWriter extends PostingsWriterBase {
       }
     }
 
-    final int lastPosBlockOffset;
+    final long lastPosBlockOffset;
 
     if (fieldHasPositions) {
       // if (DEBUG) {
@@ -411,7 +411,7 @@ final class BlockPostingsWriter extends PostingsWriterBase {
       assert stats.totalTermFreq != -1;
       if (stats.totalTermFreq > BLOCK_SIZE) {
         // record file offset for last pos in last block
-        lastPosBlockOffset = (int) (posOut.getFilePointer() - posTermStartFP);
+        lastPosBlockOffset = posOut.getFilePointer() - posTermStartFP;
       } else {
         lastPosBlockOffset = -1;
       }
@@ -474,9 +474,9 @@ final class BlockPostingsWriter extends PostingsWriterBase {
       lastPosBlockOffset = -1;
     }
 
-    int skipOffset;
+    long skipOffset;
     if (docCount > BLOCK_SIZE) {
-      skipOffset = (int) (skipWriter.writeSkip(docOut) - docTermStartFP);
+      skipOffset = skipWriter.writeSkip(docOut) - docTermStartFP;
       
       // if (DEBUG) {
       //   System.out.println("skip packet " + (docOut.getFilePointer() - (docTermStartFP + skipOffset)) + " bytes");
@@ -534,7 +534,7 @@ final class BlockPostingsWriter extends PostingsWriterBase {
         bytesWriter.writeVLong(term.posStartFP - lastPosStartFP);
         lastPosStartFP = term.posStartFP;
         if (term.lastPosBlockOffset != -1) {
-          bytesWriter.writeVInt(term.lastPosBlockOffset);
+          bytesWriter.writeVLong(term.lastPosBlockOffset);
         }
         if ((fieldHasPayloads || fieldHasOffsets) && term.payStartFP != -1) {
           bytesWriter.writeVLong(term.payStartFP - lastPayStartFP);
@@ -543,7 +543,7 @@ final class BlockPostingsWriter extends PostingsWriterBase {
       }
 
       if (term.skipOffset != -1) {
-        bytesWriter.writeVInt(term.skipOffset);
+        bytesWriter.writeVLong(term.skipOffset);
       }
     }
 
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsFormat.java
index 8bc2032..16d9c47 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsFormat.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsFormat.java
@@ -65,8 +65,7 @@ import org.apache.lucene.util.fst.FST; // javadocs
  *    <li>Term Metadata --&gt; FreqDelta, SkipDelta?, ProxDelta?
  *    <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
  *    <li>SkipInterval,MaxSkipLevels,SkipMinimum --&gt; {@link DataOutput#writeInt Uint32}</li>
- *    <li>SkipDelta --&gt; {@link DataOutput#writeVInt VInt}</li>
- *    <li>FreqDelta,ProxDelta --&gt; {@link DataOutput#writeVLong VLong}</li>
+ *    <li>SkipDelta,FreqDelta,ProxDelta --&gt; {@link DataOutput#writeVLong VLong}</li>
  * </ul>
  * <p>Notes:</p>
  * <ul>
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsReader.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsReader.java
index c7bccd0..64d2e49 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsReader.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsReader.java
@@ -67,7 +67,7 @@ public class Lucene40PostingsReader extends PostingsReaderBase {
     try {
       freqIn = dir.openInput(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, Lucene40PostingsFormat.FREQ_EXTENSION),
                            ioContext);
-      CodecUtil.checkHeader(freqIn, Lucene40PostingsWriter.FRQ_CODEC, Lucene40PostingsWriter.VERSION_START,Lucene40PostingsWriter.VERSION_START);
+      CodecUtil.checkHeader(freqIn, Lucene40PostingsWriter.FRQ_CODEC, Lucene40PostingsWriter.VERSION_START,Lucene40PostingsWriter.VERSION_CURRENT);
       // TODO: hasProx should (somehow!) become codec private,
       // but it's tricky because 1) FIS.hasProx is global (it
       // could be all fields that have prox are written by a
@@ -79,7 +79,7 @@ public class Lucene40PostingsReader extends PostingsReaderBase {
       if (fieldInfos.hasProx()) {
         proxIn = dir.openInput(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, Lucene40PostingsFormat.PROX_EXTENSION),
                              ioContext);
-        CodecUtil.checkHeader(proxIn, Lucene40PostingsWriter.PRX_CODEC, Lucene40PostingsWriter.VERSION_START,Lucene40PostingsWriter.VERSION_START);
+        CodecUtil.checkHeader(proxIn, Lucene40PostingsWriter.PRX_CODEC, Lucene40PostingsWriter.VERSION_START,Lucene40PostingsWriter.VERSION_CURRENT);
       } else {
         proxIn = null;
       }
@@ -98,7 +98,7 @@ public class Lucene40PostingsReader extends PostingsReaderBase {
 
     // Make sure we are talking to the matching past writer
     CodecUtil.checkHeader(termsIn, Lucene40PostingsWriter.TERMS_CODEC,
-      Lucene40PostingsWriter.VERSION_START, Lucene40PostingsWriter.VERSION_START);
+      Lucene40PostingsWriter.VERSION_START, Lucene40PostingsWriter.VERSION_CURRENT);
 
     skipInterval = termsIn.readInt();
     maxSkipLevels = termsIn.readInt();
@@ -109,7 +109,7 @@ public class Lucene40PostingsReader extends PostingsReaderBase {
   private final static class StandardTermState extends BlockTermState {
     long freqOffset;
     long proxOffset;
-    int skipOffset;
+    long skipOffset;
 
     // Only used by the "primary" TermState -- clones don't
     // copy this (basically they are "transient"):
@@ -202,7 +202,7 @@ public class Lucene40PostingsReader extends PostingsReaderBase {
     assert termState.freqOffset < freqIn.length();
 
     if (termState.docFreq >= skipMinimum) {
-      termState.skipOffset = termState.bytesReader.readVInt();
+      termState.skipOffset = termState.bytesReader.readVLong();
       // if (DEBUG) System.out.println("  skipOffset=" + termState.skipOffset + " vs freqIn.length=" + freqIn.length());
       assert termState.freqOffset + termState.skipOffset < freqIn.length();
     } else {
@@ -319,7 +319,7 @@ public class Lucene40PostingsReader extends PostingsReaderBase {
 
 
     protected long freqOffset;
-    protected int skipOffset;
+    protected long skipOffset;
 
     protected boolean skipped;
     protected final Bits liveDocs;
@@ -695,7 +695,7 @@ public class Lucene40PostingsReader extends PostingsReaderBase {
     Bits liveDocs;
 
     long freqOffset;
-    int skipOffset;
+    long skipOffset;
     long proxOffset;
 
     int posPendingCount;
@@ -894,7 +894,7 @@ public class Lucene40PostingsReader extends PostingsReaderBase {
     Bits liveDocs;
 
     long freqOffset;
-    int skipOffset;
+    long skipOffset;
     long proxOffset;
 
     int posPendingCount;
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsWriter.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsWriter.java
index ecc2bf4..44b953b 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsWriter.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene40/Lucene40PostingsWriter.java
@@ -53,7 +53,8 @@ public final class Lucene40PostingsWriter extends PostingsWriterBase {
   
   // Increment version to change it:
   final static int VERSION_START = 0;
-  final static int VERSION_CURRENT = VERSION_START;
+  final static int VERSION_LONG_SKIP = 1;
+  final static int VERSION_CURRENT = VERSION_LONG_SKIP;
 
   final IndexOutput freqOut;
   final IndexOutput proxOut;
@@ -277,9 +278,9 @@ public final class Lucene40PostingsWriter extends PostingsWriterBase {
   private static class PendingTerm {
     public final long freqStart;
     public final long proxStart;
-    public final int skipOffset;
+    public final long skipOffset;
 
-    public PendingTerm(long freqStart, long proxStart, int skipOffset) {
+    public PendingTerm(long freqStart, long proxStart, long skipOffset) {
       this.freqStart = freqStart;
       this.proxStart = proxStart;
       this.skipOffset = skipOffset;
@@ -299,9 +300,9 @@ public final class Lucene40PostingsWriter extends PostingsWriterBase {
     // for this term) in two places?
     assert stats.docFreq == df;
 
-    final int skipOffset;
+    final long skipOffset;
     if (df >= skipMinimum) {
-      skipOffset = (int) (skipListWriter.writeSkip(freqOut)-freqStart);
+      skipOffset = skipListWriter.writeSkip(freqOut)-freqStart;
     } else {
       skipOffset = -1;
     }
@@ -333,7 +334,7 @@ public final class Lucene40PostingsWriter extends PostingsWriterBase {
 
     if (firstTerm.skipOffset != -1) {
       assert firstTerm.skipOffset > 0;
-      bytesWriter.writeVInt(firstTerm.skipOffset);
+      bytesWriter.writeVLong(firstTerm.skipOffset);
     }
     if (indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0) {
       bytesWriter.writeVLong(firstTerm.proxStart);
@@ -348,7 +349,7 @@ public final class Lucene40PostingsWriter extends PostingsWriterBase {
       lastFreqStart = term.freqStart;
       if (term.skipOffset != -1) {
         assert term.skipOffset > 0;
-        bytesWriter.writeVInt(term.skipOffset);
+        bytesWriter.writeVLong(term.skipOffset);
       }
       if (indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0) {
         bytesWriter.writeVLong(term.proxStart - lastProxStart);
diff --git a/lucene/core/src/test/org/apache/lucene/index/Test2BPositions.java b/lucene/core/src/test/org/apache/lucene/index/Test2BPositions.java
new file mode 100644
index 0000000..1d39c93
--- /dev/null
+++ b/lucene/core/src/test/org/apache/lucene/index/Test2BPositions.java
@@ -0,0 +1,111 @@
+package org.apache.lucene.index;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.FieldType;
+import org.apache.lucene.document.TextField;
+import org.apache.lucene.store.BaseDirectoryWrapper;
+import org.apache.lucene.store.MockDirectoryWrapper;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.TimeUnits;
+import org.apache.lucene.util._TestUtil;
+import org.apache.lucene.util.LuceneTestCase.SuppressCodecs;
+import org.junit.Ignore;
+
+import com.carrotsearch.randomizedtesting.annotations.TimeoutSuite;
+
+/**
+ * Test indexes ~82M docs with 52 positions each, so you get > Integer.MAX_VALUE positions
+ * @lucene.experimental
+ */
+@SuppressCodecs({ "SimpleText", "Memory", "Direct" })
+@TimeoutSuite(millis = 4 * TimeUnits.HOUR)
+public class Test2BPositions extends LuceneTestCase {
+
+  // uses lots of space and takes a few minutes
+  @Ignore("Very slow. Enable manually by removing @Ignore.")
+  public void test() throws Exception {
+    BaseDirectoryWrapper dir = newFSDirectory(_TestUtil.getTempDir("2BPositions"));
+    if (dir instanceof MockDirectoryWrapper) {
+      ((MockDirectoryWrapper)dir).setThrottling(MockDirectoryWrapper.Throttling.NEVER);
+    }
+    
+    IndexWriter w = new IndexWriter(dir,
+        new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))
+        .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)
+        .setRAMBufferSizeMB(256.0)
+        .setMergeScheduler(new ConcurrentMergeScheduler())
+        .setMergePolicy(newLogMergePolicy(false, 10))
+        .setOpenMode(IndexWriterConfig.OpenMode.CREATE));
+
+    MergePolicy mp = w.getConfig().getMergePolicy();
+    if (mp instanceof LogByteSizeMergePolicy) {
+     // 1 petabyte:
+     ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1024*1024*1024);
+    }
+
+    Document doc = new Document();
+    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);
+    ft.setOmitNorms(true);
+    Field field = new Field("field", new MyTokenStream(), ft);
+    doc.add(field);
+    
+    final int numDocs = (Integer.MAX_VALUE / 26) + 1;
+    for (int i = 0; i < numDocs; i++) {
+      w.addDocument(doc);
+      if (VERBOSE && i % 100000 == 0) {
+        System.out.println(i + " of " + numDocs + "...");
+      }
+    }
+    w.forceMerge(1);
+    w.close();
+    dir.close();
+  }
+  
+  public static final class MyTokenStream extends TokenStream {
+    private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
+    private final PositionIncrementAttribute posIncAtt = addAttribute(PositionIncrementAttribute.class);
+    int index;
+
+    public MyTokenStream() {
+      termAtt.setLength(1);
+      termAtt.buffer()[0] = 'a';
+    }
+    
+    @Override
+    public boolean incrementToken() {
+      if (index < 52) {
+        posIncAtt.setPositionIncrement(1+index);
+        index++;
+        return true;
+      }
+      return false;
+    }
+    
+    @Override
+    public void reset() {
+      index = 0;
+    }
+  }
+}
diff --git a/lucene/core/src/test/org/apache/lucene/index/Test2BPostingsBytes.java b/lucene/core/src/test/org/apache/lucene/index/Test2BPostingsBytes.java
new file mode 100644
index 0000000..9c67839
--- /dev/null
+++ b/lucene/core/src/test/org/apache/lucene/index/Test2BPostingsBytes.java
@@ -0,0 +1,151 @@
+package org.apache.lucene.index;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.Arrays;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.FieldType;
+import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.store.BaseDirectoryWrapper;
+import org.apache.lucene.store.MockDirectoryWrapper;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.TimeUnits;
+import org.apache.lucene.util._TestUtil;
+import org.apache.lucene.util.LuceneTestCase.SuppressCodecs;
+import org.junit.Ignore;
+
+import com.carrotsearch.randomizedtesting.annotations.TimeoutSuite;
+
+/**
+ * Test indexes 2B docs with 65k freqs each, 
+ * so you get > Integer.MAX_VALUE postings data for the term
+ * @lucene.experimental
+ */
+@SuppressCodecs({ "SimpleText", "Memory", "Direct" })
+@TimeoutSuite(millis = 4 * TimeUnits.HOUR)
+public class Test2BPostingsBytes extends LuceneTestCase {
+
+  // @Absurd @Ignore takes ~20GB-30GB of space and 10 minutes.
+  // with some codecs needs more heap space as well.
+  @Ignore("Very slow. Enable manually by removing @Ignore.")
+  public void test() throws Exception {
+    BaseDirectoryWrapper dir = newFSDirectory(_TestUtil.getTempDir("2BPostingsBytes1"));
+    if (dir instanceof MockDirectoryWrapper) {
+      ((MockDirectoryWrapper)dir).setThrottling(MockDirectoryWrapper.Throttling.NEVER);
+    }
+    
+    IndexWriter w = new IndexWriter(dir,
+        new IndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()))
+        .setMaxBufferedDocs(IndexWriterConfig.DISABLE_AUTO_FLUSH)
+        .setRAMBufferSizeMB(256.0)
+        .setMergeScheduler(new ConcurrentMergeScheduler())
+        .setMergePolicy(newLogMergePolicy(false, 10))
+        .setOpenMode(IndexWriterConfig.OpenMode.CREATE));
+
+    MergePolicy mp = w.getConfig().getMergePolicy();
+    if (mp instanceof LogByteSizeMergePolicy) {
+     // 1 petabyte:
+     ((LogByteSizeMergePolicy) mp).setMaxMergeMB(1024*1024*1024);
+    }
+
+    Document doc = new Document();
+    FieldType ft = new FieldType(TextField.TYPE_NOT_STORED);
+    ft.setIndexOptions(IndexOptions.DOCS_AND_FREQS);
+    ft.setOmitNorms(true);
+    MyTokenStream tokenStream = new MyTokenStream();
+    Field field = new Field("field", tokenStream, ft);
+    doc.add(field);
+    
+    final int numDocs = 1000;
+    for (int i = 0; i < numDocs; i++) {
+      if (i % 2 == 1) { // trick blockPF's little optimization
+        tokenStream.n = 65536;
+      } else {
+        tokenStream.n = 65537;
+      }
+      w.addDocument(doc);
+    }
+    w.forceMerge(1);
+    w.close();
+    
+    DirectoryReader oneThousand = DirectoryReader.open(dir);
+    IndexReader subReaders[] = new IndexReader[1000];
+    Arrays.fill(subReaders, oneThousand);
+    MultiReader mr = new MultiReader(subReaders);
+    BaseDirectoryWrapper dir2 = newFSDirectory(_TestUtil.getTempDir("2BPostingsBytes2"));
+    if (dir2 instanceof MockDirectoryWrapper) {
+      ((MockDirectoryWrapper)dir2).setThrottling(MockDirectoryWrapper.Throttling.NEVER);
+    }
+    IndexWriter w2 = new IndexWriter(dir2,
+        new IndexWriterConfig(TEST_VERSION_CURRENT, null));
+    w2.addIndexes(mr);
+    w2.forceMerge(1);
+    w2.close();
+    oneThousand.close();
+    
+    DirectoryReader oneMillion = DirectoryReader.open(dir2);
+    subReaders = new IndexReader[2000];
+    Arrays.fill(subReaders, oneMillion);
+    mr = new MultiReader(subReaders);
+    BaseDirectoryWrapper dir3 = newFSDirectory(_TestUtil.getTempDir("2BPostingsBytes3"));
+    if (dir3 instanceof MockDirectoryWrapper) {
+      ((MockDirectoryWrapper)dir3).setThrottling(MockDirectoryWrapper.Throttling.NEVER);
+    }
+    IndexWriter w3 = new IndexWriter(dir3,
+        new IndexWriterConfig(TEST_VERSION_CURRENT, null));
+    w3.addIndexes(mr);
+    w3.forceMerge(1);
+    w3.close();
+    oneMillion.close();
+    
+    dir.close();
+    dir2.close();
+    dir3.close();
+  }
+  
+  public static final class MyTokenStream extends TokenStream {
+    private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
+    int index;
+    int n;
+
+    public MyTokenStream() {
+      termAtt.setLength(1);
+      termAtt.buffer()[0] = 'a';
+    }
+    
+    @Override
+    public boolean incrementToken() {
+      if (index < n) {
+        index++;
+        return true;
+      }
+      return false;
+    }
+    
+    @Override
+    public void reset() {
+      index = 0;
+    }
+  }
+}

