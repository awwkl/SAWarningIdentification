GitDiffStart: f79712ce8294ae3d117e87a64f4355326b2919af | Wed Aug 12 18:53:49 2009 +0000
diff --git a/CHANGES.txt b/CHANGES.txt
index 190526e..cdbc31b 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -727,6 +727,8 @@ Optimizations
     strings, the StringHelper.intern() interface was added with a
     default implementation that uses a lockless cache.
     (Earwin Burrfoot, yonik)
+
+13. LUCENE-1800: QueryParser should use reusable TokenStreams. (yonik)
     
 
 Documentation
diff --git a/src/java/org/apache/lucene/queryParser/QueryParser.java b/src/java/org/apache/lucene/queryParser/QueryParser.java
index d1ed9ab..3d7dd5e 100644
--- a/src/java/org/apache/lucene/queryParser/QueryParser.java
+++ b/src/java/org/apache/lucene/queryParser/QueryParser.java
@@ -537,7 +537,13 @@ public class QueryParser implements QueryParserConstants {
     // Use the analyzer to get all the tokens, and then build a TermQuery,
     // PhraseQuery, or nothing based on the term count
 
-    TokenStream source = analyzer.tokenStream(field, new StringReader(queryText));
+    TokenStream source;
+    try {
+      source = analyzer.reusableTokenStream(field, new StringReader(queryText));
+      source.reset();
+    } catch (IOException e) {
+      source = analyzer.tokenStream(field, new StringReader(queryText));
+    }
     CachingTokenFilter buffer = new CachingTokenFilter(source);
     TermAttribute termAtt = null;
     PositionIncrementAttribute posIncrAtt = null;
diff --git a/src/java/org/apache/lucene/queryParser/QueryParser.jj b/src/java/org/apache/lucene/queryParser/QueryParser.jj
index 0c99696..309f28b 100644
--- a/src/java/org/apache/lucene/queryParser/QueryParser.jj
+++ b/src/java/org/apache/lucene/queryParser/QueryParser.jj
@@ -562,7 +562,13 @@ public class QueryParser {
     // Use the analyzer to get all the tokens, and then build a TermQuery,
     // PhraseQuery, or nothing based on the term count
 
-    TokenStream source = analyzer.tokenStream(field, new StringReader(queryText));
+    TokenStream source;
+    try {
+      source = analyzer.reusableTokenStream(field, new StringReader(queryText));
+      source.reset();
+    } catch (IOException e) {
+      source = analyzer.tokenStream(field, new StringReader(queryText));
+    }
     CachingTokenFilter buffer = new CachingTokenFilter(source);
     TermAttribute termAtt = null;
     PositionIncrementAttribute posIncrAtt = null;

