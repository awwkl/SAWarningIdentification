GitDiffStart: 5265dc1bb206a84a82db4129ac12962ef5260660 | Wed Jul 1 10:32:23 2009 +0000
diff --git a/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/SmartChineseAnalyzer.java b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/SmartChineseAnalyzer.java
index 94fddae..6f5a4f4 100644
--- a/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/SmartChineseAnalyzer.java
+++ b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/SmartChineseAnalyzer.java
@@ -33,23 +33,26 @@ import org.apache.lucene.analysis.cn.smart.SentenceTokenizer;
 import org.apache.lucene.analysis.cn.smart.WordSegmenter;
 import org.apache.lucene.analysis.cn.smart.WordTokenizer;
 
+import org.apache.lucene.analysis.cn.smart.AnalyzerProfile; // for javadoc
+
 /**
- * 
- * SmartChineseAnalyzer ???ä¸???½ä¸­???è¯?¨¡??? ?½å??©ç?æ¦??å¯¹æ?è¯??å­??è¡??ä¼?????
- * å¹¶å?åµ????okenizerï¼?????å¤??ä¸????··???????????
- * 
- * å®??????ºä????è¯??å¤??é¢?????é©??ç§?¤«æ¨¡å?(HMM)ï¼? ?©ç?å¤§é?è¯??åº??è®???¥ç?è®¡æ?è¯??æ±??è¯????·³è½?????
- * ä»???????äº??è®¡ç?????´ä¸ªæ±???¥å?è®¡ç???ä¼¼ç?(likelihood)???????
- * 
- * ??¸º?ºè??????è¦???¸æ?ä¿??è¯?????è®¡å?¼ï?SmartChineseAnalyzer???è¡??è¦??å®???¸ä?ç½??å¦?????è¯??ä½?½®è¯·å???
- * org.apache.lucene.analysis.cn.smart.AnalyzerProfile
- * 
- * SmartChineseAnalyzer???æ³??è¯??åº???¸æ????ictclas1.0é¡¹ç?(http://www.ictclas.org)ï¼?
- * ?¶ä¸­è¯??å·²è???ww.ictclas.org??pache license v2(APLv2)?????????µå¾ªAPLv2???ä»¶ä?ï¼??è¿???·ä½¿?¨ã??
- * ?¨æ???°¢www.ictclas.orgä»¥å?ictclas???è½?»¶??·¥ä½?ºº??????å¥??ï¼?
- * 
- * @see org.apache.lucene.analysis.cn.smart.AnalyzerProfile
- * 
+ * <p>
+ * SmartChineseAnalyzer is an analyzer for Chinese or mixed Chinese-English text.
+ * The analyzer uses probabilistic knowledge to find the optimal word segmentation for Simplified Chinese text.
+ * The text is first broken into sentences, then each sentence is segmented into words.
+ * </p>
+ * <p>
+ * Segmentation is based upon the <a href="http://en.wikipedia.org/wiki/Hidden_Markov_Model">Hidden Markov Model</a>. 
+ * A large training corpus was used to calculate Chinese word frequency probability.
+ * </p>
+ * <p>
+ * This analyzer requires a dictionary to provide statistical data. 
+ * To specify the location of the dictionary data, refer to {@link AnalyzerProfile}
+ * </p>
+ * <p>
+ * The included dictionary data is from <a href="http://www.ictclas.org">ICTCLAS1.0</a>.
+ * Thanks to ICTCLAS for their hard work, and for contributing the data under the Apache 2 License!
+ * </p>
  */
 public class SmartChineseAnalyzer extends Analyzer {
 
@@ -57,15 +60,23 @@ public class SmartChineseAnalyzer extends Analyzer {
 
   private WordSegmenter wordSegment;
 
+  /**
+   * Create a new SmartChineseAnalyzer, using the default stopword list.
+   */
   public SmartChineseAnalyzer() {
     this(true);
   }
 
   /**
-   * SmartChineseAnalyzer???å¸??é»?????è¯??ï¼?¸»è¦?????ç¬????????å¸??ç»??ä¸???°æ??¹ç??·ï?
-   * ??»¥å°?seDefaultStopWordsè®¾ä¸ºtrueï¼? useDefaultStopWordsä¸?alse?¶ä?ä½¿ç?ä»»ä????è¯?
+   * <p>
+   * Create a new SmartChineseAnalyzer, optionally using the default stopword list.
+   * </p>
+   * <p>
+   * The included default stopword list is simply a list of punctuation.
+   * If you do not use this list, punctuation will not be removed from the text!
+   * </p>
    * 
-   * @param useDefaultStopWords
+   * @param useDefaultStopWords true to use the default stopword list.
    */
   public SmartChineseAnalyzer(boolean useDefaultStopWords) {
     if (useDefaultStopWords) {
@@ -76,10 +87,14 @@ public class SmartChineseAnalyzer extends Analyzer {
   }
 
   /**
-   * ä½¿ç????ä¹?????ä½¿ç???½®???æ­¢è?åº?????è¯??ä»¥ä½¿??martChineseAnalyzer.loadStopWords(InputStream)??½½
-   * 
-   * @param stopWords
-   * @see SmartChineseAnalyzer.loadStopWords(InputStream)
+   * <p>
+   * Create a new SmartChineseAnalyzer, using the provided {@link Set} of stopwords.
+   * </p>
+   * <p>
+   * Note: the set should include punctuation, unless you want to index punctuation!
+   * </p>
+   * @param stopWords {@link Set} of stopwords to use.
+   * @see SmartChineseAnalyzer#loadStopWords(InputStream)
    */
   public SmartChineseAnalyzer(Set stopWords) {
     this.stopWords = stopWords;
@@ -90,8 +105,8 @@ public class SmartChineseAnalyzer extends Analyzer {
     TokenStream result = new SentenceTokenizer(reader);
     result = new WordTokenizer(result, wordSegment);
     // result = new LowerCaseFilter(result);
-    // ä¸????è¦?owerCaseFilterï¼??ä¸?egTokenFilterå·²ç?å°????????ç¬?½¬?¢æ?å°??
-    // stemå¤?¸¥?¼ä?, This is not bug, this feature:)
+    // LowerCaseFilter is not needed, as SegTokenFilter lowercases Basic Latin text.
+    // The porter stemming is too strict, this is not a bug, this is a feature:)
     result = new PorterStemFilter(result);
     if (stopWords != null) {
       result = new StopFilter(result, stopWords, false);
@@ -100,13 +115,17 @@ public class SmartChineseAnalyzer extends Analyzer {
   }
 
   /**
-   * ä»???¨è???»¶ä¸??è½½å??¨è?ï¼? ???è¯??ä»¶æ?????TF-8ç¼????????ä»¶ï? æ¯??è¡??ä¸?ä¸???¨è?ï¼?³¨????¨â??//??? ???è¯?¸­???ä¸?????ç¬??ï¼? ä¸??ç©ºæ?ï¼?
-   * ä»¥å?ä½¿ç???¤ªé«????ç´¢å????ä¸?¤§?????
+   * Utility function to return a {@link Set} of stopwords from a UTF-8 encoded {@link InputStream}.
+   * The comment "//" can be used in the stopword list.
    * 
-   * @param input ???è¯??ä»?
-   * @return ???è¯?????HashSet
+   * @param input {@link InputStream} of UTF-8 encoded stopwords
+   * @return {@link Set} of stopwords.
    */
   public static Set loadStopWords(InputStream input) {
+    /*
+     * Note: WordListLoader is not used here because this method allows for inline "//" comments.
+     * WordListLoader will only filter out these comments if they are on a separate line.
+     */
     String line;
     Set stopWords = new HashSet();
     try {
diff --git a/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/package.html b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/package.html
index 0a3201c..09d37d5 100644
--- a/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/package.html
+++ b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/package.html
@@ -1,51 +1,22 @@
 <html>
 <head></head>
 <body>
-Analyzer for Chinese.
-
-
-<h2>About SmartChineseAnalyzer</h2>
-<p>SmartChineseAnalyzer ???ä¸???½ä¸­???è¯?¨¡??? ä¸? ChineseAnalyzer ï¼?????ä¸??å­????
-CJKAnalyzer ï¼?????ä¸¤ä¸ªæ±??ï¼????? å®??å¤???¨æ????æ±???¥å?è¿????ä¼????? å¹¶å?åµ????okenizerï¼?
-?½æ??????¸­?±æ?æ··å???????å®¹ã?????martChineseAnalyzer????¸å???????ä½?¸­????</p>
-
-<p>å®??????ºä????è¯??å¤??é¢?????é©??ç§?¤«æ¨¡å?(HMM)ï¼? ?©ç?å¤§é?è¯??åº??è®???¥ç?è®¡æ?è¯??æ±??è¯????·³è½?????
-ä»???????äº??è®¡ç?????´ä¸ªæ±???¥å?è®¡ç???ä¼¼ç?(likelihood)???????</p>
-
-<p>ä¸?????æ¨¡å????è¯?????è¾?, ?±æ???»¥????ºè?????´ç????å­?????è¯??ï¼? ä»????é«??ç´¢ç???¡®????
-<pre>è¯??ï¼? ???ä¸??äº?</pre>
+Analyzers for Chinese.
+<p>
+Three analyzers are provided for Chinese, each of which treats Chinese text in a different way.
+<ul>
+	<li>ChineseAnalyzer: Index unigrams (individual Chinese characters) as a token.
+	<li>CJKAnalyzer: Index bigrams (overlapping groups of two adjacent Chinese characters) as tokens.
+	<li>SmartChineseAnalyzer: Index words (attempt to segment Chinese text into words) as tokens.
+</ul>
+
+Example phraseï¼? "???ä¸??äº?"
 <ol>
-	<li>SmartChineseAnalyzer: ??????ä¸??ï¼?ºº</li>
 	<li>ChineseAnalyzer: ??????ä¸???½ï?äº?</li>
 	<li>CJKAnalyzer: ???ï¼??ä¸??ä¸??ï¼??äº?</li>
+	<li>SmartChineseAnalyzer: ??????ä¸??ï¼?ºº</li>
 </ol>
 </p>
 
-<h3>???è¯?????ç½?</h3>
-<p>??¸º?ºè??????è¦???¸æ?ä¿??è¯?????è®¡å?¼ï?é»?????ä¸??SmartChineseAnalyzerä½¿ç???½®????¸å?ï¼????è¦??å®??è¯??åº??ï¼??è¦??å®???¸ä?ç½??å¦?????è¯??ä½?½®è¯·å???
-org.apache.lucene.analysis.cn.smart.AnalyzerProfile??</p>
-
-<p><b>è¯?????è½½å???ä¸ºï?<a
-	href="http://code.google.com/p/imdict-chinese-analyzer/downloads/list">http://code.google.com/p/imdict-chinese-analyzer/downloads/list</a>
-</b> ä¸?½½??»¶analysis-data.zipä¿???°æ??°ï?è§£å??³å?ä½¿ç???</p>
-
-<p>??ç®???????è¯??åº?????å°±æ?è¿???¶å?ä¸????-Danalysis.data.dir
-<pre>å¦?? java -Danalysis.data.dir=/path/to/analysis-data com.example.YourApplication</pre>
-</p>
-
-<h3>???è¦??</h3>
-<p>SmartChineseAnalyzer??VMè¦??java 1.4??»¥ä¸?????Lucene
-è¦??2.4.0??»¥ä¸?????Lucene 2.3.X???è¯¥ä???»¥ä½¿ç?ï¼?????æµ??ï¼????è¦???¨æ????è¡??è¯???</p>
-
-<h3>æº??ä»¶å????ç¼??</h3>
-?¤ç?å®??äº???¶ç???»¶å¤??SmartChineseAnalyzer?????????Javaæº???½é???TF-8ç¼??ï¼?
-????¨è???????ç¼??Javaæº?????æ³¨æ????æ­£ç¡®???å¼??ä»¥é???º§??¹±???è¯???
-
-<h3>SmartChineseAnalyzer?????</h3>
-<p>SmartChineseAnalyzer???æ³??è¯??åº???¸æ????ictclas1.0é¡¹ç?(<a
-	href="http://www.ictclas.org">http://www.ictclas.org</a>)ï¼?
-?¶ä¸­è¯??å·²ç??????ººwww.ictclas.org???ï¼?»¥apache license
-v2(APLv2)??????????µå¾ªAPLv2???ä»¶ä?ï¼??è¿???·ä½¿?¨ã??
-?¨æ???°¢www.ictclas.orgä»¥å?ictclas???è½?»¶??·¥ä½?ºº???è¾??å·¥ä????ç§?????</p>
 </body>
 </html>
diff --git a/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/AnalyzerProfile.java b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/AnalyzerProfile.java
index 3d766a6..d74e252 100644
--- a/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/AnalyzerProfile.java
+++ b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/AnalyzerProfile.java
@@ -23,38 +23,37 @@ import java.io.IOException;
 import java.util.Properties;
 
 /**
- * ?¨é?è®¤æ??µä?ï¼?martChineseAnalyzer??½®????¸å????è®¤å?æ­¢è?åº??å·²ç?ç»??å°??ï¼???·å?ä»¥ç??¥ä½¿?¨ã??
- * 
- * ?¹æ????ä¸???¨æ???è¦?½¿?¨æ?å®??è¯??åº?????è¯??ï¼???¶é?è¦????rg.apache.lucene.analysis.cn.smart. hhmmä¸??
- * coredict.mem ?? bigramdict.memï¼? ?¶å?ä½¿ç?AnalyzerProfile?¥æ?å®???¸å??????
- * 
- * AnalyzerProfile ?¨æ?å¯»æ?å­?????è¯???°æ? ????¨è??°æ????å½?? è¯¥ç?å½??åº???? bigramdict.dct, coredict.dct,
- * stopwords_utf8.txt, ?¥æ?è¿??ä¾??å¦??ï¼?
+ * Configure analysis data for SmartChineseAnalyzer
+ * <p>
+ * SmartChineseAnalyzer has a built-in dictionary and stopword list out-of-box.
+ * </p>
+ * <p>
+ * In special circumstances a user may wish to configure SmartChineseAnalyzer with a custom data directory location.
+ * </p>
+ * AnalyzerProfile is used to determine the location of the data directory containing bigramdict.dct and coredict.dct.
+ * The following order is used to determine the location of the data directory:
  * 
  * <ol>
- * <li>è¯»å?ç³»ç?è¿???¶å??°ï?-Danalysis.data.dir=/path/to/analysis-dataï¼????²¡???ç»§ç»­ä¸????</li>
- * <li>?§è??½ä»¤??????å½?¸­???å­??analysis-data???</li>
- * <li>?§è??½ä»¤??ib/???ä¸???????nalysis-data???</li>
- * <li>?§è??½ä»¤??????å½?¸­???å­??analysis.properties??»¶</li>
- * <li>?§è??½ä»¤??ib/???ä¸???????nalysis.properties??»¶</li>
+ * <li>System propertyï¼? -Danalysis.data.dir=/path/to/analysis-data</li>
+ * <li>Relative path: analysis-data</li>
+ * <li>Relative path: lib/analysis-data</li>
+ * <li>Property file: analysis.data.dir property from relative path analysis.properties</li>
+ * <li>Property file: analysis.data.dir property from relative path lib/analysis.properties</li>
  * </ol>
  * 
- * ?¶ä¸­analysis.properties??»¶analysis.data.dir???analysis-data??????¨ä?ç½?.
- * analysis.properties??»¶???å®¹ç¤ºä¾??
+ * Example property fileï¼?
  * 
  * <pre>
  * analysis.data.dir=D:/path/to/analysis-data/
  * </pre>
  * 
- * å½??ä¸??analysis-data????¶ï?ANALYSIS_DATA_DIRè®¾ç½®ä¸?""ï¼??æ­¤å?ä½¿ç????å¿?¡»?¨ç?åº???¾å????data???ï¼??å¦??
- * 
- * <pre>
- * AnalyzerProfile.ANALYSIS_DATA_DIR = &quot;/path/to/analysis-data&quot;;
- * </pre>
  * 
  */
 public class AnalyzerProfile {
 
+  /**
+   * Global indicating the configured analysis data directory
+   */
   public static String ANALYSIS_DATA_DIR = "";
 
   static {
@@ -65,7 +64,7 @@ public class AnalyzerProfile {
     String dirName = "analysis-data";
     String propName = "analysis.properties";
 
-    // è¯»å?ç³»ç?è®¾ç½®ï¼??è¿???¶å??¥å??°ï?-Danalysis.data.dir=/path/to/analysis-data
+    // Try the system propertyï¼?-Danalysis.data.dir=/path/to/analysis-data
     ANALYSIS_DATA_DIR = System.getProperty("analysis.data.dir", "");
     if (ANALYSIS_DATA_DIR.length() != 0)
       return;
@@ -86,9 +85,9 @@ public class AnalyzerProfile {
     }
 
     if (ANALYSIS_DATA_DIR.length() == 0) {
-      // ??¤º?¨æ?????°è??¸æ?ä»¶å¤¹
+      // Dictionary directory cannot be found.
       System.err
-          .println("WARNING: Can not found lexical dictionary directory!");
+          .println("WARNING: Can not find lexical dictionary directory!");
       System.err
           .println("WARNING: This will cause unpredictable exceptions in your application!");
       System.err
diff --git a/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/CharType.java b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/CharType.java
index 5fd18e6..72517de 100644
--- a/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/CharType.java
+++ b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/CharType.java
@@ -17,23 +17,49 @@
 
 package org.apache.lucene.analysis.cn.smart;
 
+/**
+ * Internal SmartChineseAnalyzer character type constants.
+ */
 public class CharType {
 
+  /**
+   * Punctuation Characters
+   */
   public final static int DELIMITER = 0;
 
+  /**
+   * Letters
+   */
   public final static int LETTER = 1;
 
+  /**
+   * Numeric Digits
+   */
   public final static int DIGIT = 2;
 
+  /**
+   * Han Ideographs
+   */
   public final static int HANZI = 3;
 
+  /**
+   * Characters that act as a space
+   */
   public final static int SPACE_LIKE = 4;
 
-  // (?¨è????)???ç¬??ï¼??è§??å­??ï¼??å­??ï¼??å­??ç©ºæ?ï¼?"\t\r\n"ç­?©º?¼æ??¢è?å­??
+  /**
+   * Full-Width letters
+   */
   public final static int FULLWIDTH_LETTER = 5;
 
-  public final static int FULLWIDTH_DIGIT = 6; // ?¨è?å­??ï¼??æ¯???°å?
+  /**
+   * Full-Width alphanumeric characters
+   */
+  public final static int FULLWIDTH_DIGIT = 6;
 
+  /**
+   * Other (not fitting any of the other categories)
+   */
   public final static int OTHER = 7;
 
 }
diff --git a/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/SentenceTokenizer.java b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/SentenceTokenizer.java
index e9a380b..c18fe9f 100644
--- a/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/SentenceTokenizer.java
+++ b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/SentenceTokenizer.java
@@ -25,14 +25,12 @@ import org.apache.lucene.analysis.Token;
 import org.apache.lucene.analysis.Tokenizer;
 
 /**
- * 
- * ???ä¸?ä¸???´å?å­??Tokenï¼????»¶ä¸???ºï????ä¸?æ­¥å?è¯??å¯¹è±¡
- * 
+ * Tokenizes input into sentences.
  */
 public class SentenceTokenizer extends Tokenizer {
 
   /**
-   * ?¨æ?????¥å?????¹ç??? ???ï¼??ï¼?,!?;
+   * End of sentence punctuation: ???ï¼??ï¼?,!?;
    */
   public final static String PUNCTION = "???ï¼??ï¼?,!?;";
 
@@ -62,7 +60,7 @@ public class SentenceTokenizer extends Tokenizer {
       if (ci == -1) {
         break;
       } else if (PUNCTION.indexOf(ch) != -1) {
-        // ?¾å?äº??å­??å°?
+        // End of a sentence
         buffer.append(ch);
         tokenEnd++;
         break;
@@ -78,8 +76,7 @@ public class SentenceTokenizer extends Tokenizer {
         pch = ch;
         ci = bufferInput.read();
         ch = (char) ci;
-        // å¦??ç¢°ä?äº?¸¤ä¸??ç»??skipå­??ï¼??å¦?¸¤ä¸??è½??ä¸¤ä¸ªç©ºæ??????
-        // ä¸?ä¸??è½??ä¸?ä¸?©º?¼ç?ç­??å°??è§?¸º?¥å?ç»??ï¼?»¥???å­?¤ª?¿è???å­??è¶?
+        // Two spaces, such as CR, LF
         if (Utility.SPACES.indexOf(ch) != -1
             && Utility.SPACES.indexOf(pch) != -1) {
           // buffer.append(ch);
diff --git a/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/Utility.java b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/Utility.java
index b3105d7..06f609c 100644
--- a/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/Utility.java
+++ b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/Utility.java
@@ -17,6 +17,12 @@
 
 package org.apache.lucene.analysis.cn.smart;
 
+import org.apache.lucene.analysis.cn.smart.hhmm.BiSegGraph; // for javadoc
+import org.apache.lucene.analysis.cn.smart.hhmm.SegTokenFilter; // for javadoc
+
+/**
+ * SmartChineseAnalyzer utility constants and methods
+ */
 public class Utility {
 
   public static final char[] STRING_CHAR_ARRAY = new String("??##ä¸?")
@@ -30,24 +36,29 @@ public class Utility {
 
   public static final char[] END_CHAR_ARRAY = new String("??##??").toCharArray();
 
+  /**
+   * Delimiters will be filtered to this character by {@link SegTokenFilter}
+   */
   public static final char[] COMMON_DELIMITER = new char[] { ',' };
 
   /**
-   * ??è¦?·³è¿??ç¬??ï¼??å¦??è¡¨ç?ï¼??è½???¢è?ç­????
+   * Space-like characters that need to be skipped: such as space, tab, newline, carriage return.
    */
   public static final String SPACES = " ??\t\r\n";
 
+  /**
+   * Maximum bigram frequency (used in the {@link BiSegGraph} smoothing function). 
+   */
   public static final int MAX_FREQUENCE = 2079997 + 80000;
 
   /**
-   * æ¯??ä¸¤ä¸ª?´æ??°ç???¤§å°?, ???ä»??ç»??ä¸?å®??ç½??å§???¸ªæ¯??, å½??æ¬¡ç?ç­???½å?è¾¾æ?å°¾æ?, è¿???¸ç?, ??????è¾¾æ?å°¾ç?å¤§ä??°è¾¾??°¾??;
-   * å½???°è¾¾??°¾?¶æ?ä¸?ä½???¸ç?, è¯¥ä?ç½???¼å¤§???ç»?¤§äº????
+   * compare two arrays starting at the specified offsets.
    * 
-   * @param larray
-   * @param lstartIndex larray??µ·å§??ç½?
-   * @param rarray
-   * @param rstartIndex rarray??µ·å§??ç½?
-   * @return 0è¡¨ç¤º?¸ç?ï¼?1è¡¨ç¤ºlarray > rarray, -1è¡¨ç¤ºlarray < rarray
+   * @param larray left array
+   * @param lstartIndex start offset into larray
+   * @param rarray right array
+   * @param rstartIndex start offset into rarray
+   * @return 0 if the arrays are equalï¼?1 if larray > rarray, -1 if larray < rarray
    */
   public static int compareArray(char[] larray, int lstartIndex, char[] rarray,
       int rstartIndex) {
@@ -74,21 +85,19 @@ public class Utility {
     }
     if (li == larray.length) {
       if (ri == rarray.length) {
-        // ä¸¤è????´ç?ç­????°¾ï¼??æ­¤è????ç­??ä¹?°±?????0
+        // Both arrays are equivalent, return 0.
         return 0;
       } else {
-        // æ­¤æ?ä¸????i>rarray.length??????ri<rarray.length
-        // è¡¨ç¤ºlarrayå·²ç?ç»??ï¼?arrayæ²¡æ?ç»??ï¼??æ­?array < rarrayï¼????-1
+        // larray < rarray because larray has ended first.
         return -1;
       }
     } else {
-      // æ­¤æ?ä¸????i>larray.length??????li < larray.lengthï¼?¡¨ç¤?iæ²¡æ??°è¾¾larray??°¾
+      // differing lengths
       if (ri == rarray.length) {
-        // larrayæ²¡æ?ç»??ï¼????arrayå·²ç?ç»??ï¼??æ­?array > rarray
+        // larray > rarray because rarray has ended first.
         return 1;
       } else {
-        // æ­¤æ?ä¸????i>rarray.length??????ri < rarray.length
-        // è¡¨ç¤ºlarray??array?½æ²¡????????????ä¸?ä¸????¤§å°????
+        // determine by comparison
         if (larray[li] > rarray[ri])
           return 1;
         else
@@ -98,18 +107,20 @@ public class Utility {
   }
 
   /**
-   * ?¹æ?????¥å???¸¤ä¸??ç¬??ç»??å¤§å?ï¼??????¸º?????????¶ï?è¡¨ç¤º?¸ç?ï¼??ä¸?¸º????¶ï?????????ç¬?¸²?¹å?æ¯??
+   * Compare two arrays, starting at the specified offsets, but treating shortArray as a prefix to longArray.
+   * As long as shortArray is a prefix of longArray, return 0.
+   * Otherwise, behave as {@link Utility#compareArray(char[], int, char[], int)}
    * 
-   * @param shortArray
-   * @param shortIndex
-   * @param longArray
-   * @param longIndex
-   * @return
+   * @param shortArray prefix array
+   * @param shortIndex offset into shortArray
+   * @param longArray long array (word)
+   * @param longIndex offset into longArray
+   * @return 0 if shortArray is a prefix of longArray, otherwise act as {@link Utility#compareArray(char[], int, char[], int)}
    */
   public static int compareArrayByPrefix(char[] shortArray, int shortIndex,
       char[] longArray, int longIndex) {
 
-    // ç©ºæ?ç»???????ç»?????ï¼?????index
+    // a null prefix is a prefix of longArray
     if (shortArray == null)
       return 0;
     else if (longArray == null)
@@ -122,24 +133,27 @@ public class Utility {
       li++;
     }
     if (si == shortArray.length) {
-      // shortArray ?? longArray??refix
+      // shortArray is a prefix of longArray
       return 0;
     } else {
-      // æ­¤æ?ä¸????i>shortArray.length??????si <
-      // shortArray.lengthï¼?¡¨ç¤?iæ²¡æ??°è¾¾shortArray??°¾
-
-      // shortArrayæ²¡æ?ç»??ï¼????ongArrayå·²ç?ç»??ï¼??æ­?hortArray > longArray
+      // shortArray > longArray because longArray ended first.
       if (li == longArray.length)
         return 1;
       else
-        // æ­¤æ?ä¸????i>longArray.length??????li < longArray.length
-        // è¡¨ç¤ºshortArray??ongArray?½æ²¡????????????ä¸?ä¸????¤§å°????
+        // determine by comparison
         return (shortArray[si] > longArray[li]) ? 1 : -1;
     }
   }
 
+  /**
+   * Return the internal {@link CharType} constant of a given character. 
+   * @param ch input character
+   * @return constant from {@link CharType} describing the character type.
+   * 
+   * @see CharType
+   */
   public static int getCharType(char ch) {
-    // ??å¤?????å­?
+    // Most (but not all!) of these are Han Ideographic Characters
     if (ch >= 0x4E00 && ch <= 0x9FA5)
       return CharType.HANZI;
     if ((ch >= 0x0041 && ch <= 0x005A) || (ch >= 0x0061 && ch <= 0x007A))
@@ -148,12 +162,12 @@ public class Utility {
       return CharType.DIGIT;
     if (ch == ' ' || ch == '\t' || ch == '\r' || ch == '\n' || ch == '??')
       return CharType.SPACE_LIKE;
-    // ????????å®???½æ????ç¬??äº?
+    // Punctuation Marks
     if ((ch >= 0x0021 && ch <= 0x00BB) || (ch >= 0x2010 && ch <= 0x2642)
         || (ch >= 0x3001 && ch <= 0x301E))
       return CharType.DELIMITER;
 
-    // ?¨è?å­???ºå?
+    // Full-Width range
     if ((ch >= 0xFF21 && ch <= 0xFF3A) || (ch >= 0xFF41 && ch <= 0xFF5A))
       return CharType.FULLWIDTH_LETTER;
     if (ch >= 0xFF10 && ch <= 0xFF19)
diff --git a/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/WordSegmenter.java b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/WordSegmenter.java
index bd159be..4a758a2 100644
--- a/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/WordSegmenter.java
+++ b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/WordSegmenter.java
@@ -25,6 +25,9 @@ import org.apache.lucene.analysis.cn.smart.hhmm.HHMMSegmenter;
 import org.apache.lucene.analysis.cn.smart.hhmm.SegToken;
 import org.apache.lucene.analysis.cn.smart.hhmm.SegTokenFilter;
 
+/**
+ * Segment a sentence of Chinese text into words.
+ */
 public class WordSegmenter {
 
   private HHMMSegmenter hhmmSegmenter = new HHMMSegmenter();
@@ -32,20 +35,19 @@ public class WordSegmenter {
   private SegTokenFilter tokenFilter = new SegTokenFilter();
 
   /**
-   * è°??HHMMSegmentç¨??å°?????sentence Token???ï¼?????è¯?????ä¿????oken Listä¸?
+   * Segment a sentence into words with {@link HHMMSegmenter}
    * 
-   * @param sentenceToken ?¥å???oken
-   * @param shortPathCount HHMMç®?????????è¦??ä¼?????????·¯å¾?¸ª?°ã??????å¤§å?è¯?????ç²¾ç¡®ï¼?????ç®?»£ä»·ä?è¾????
-   * @return ???ç»????oken List
+   * @param sentenceToken sentence {@link Token}
+   * @return {@link List} of {@link SegToken}
    */
-  public List segmentSentence(Token sentenceToken, int shortPathCount) {
+  public List segmentSentence(Token sentenceToken) {
     String sentence = sentenceToken.term();
 
     List segTokenList = hhmmSegmenter.process(sentence);
 
     List result = new ArrayList();
 
-    // iä»?1??awTokens.length-2ï¼??å°±æ?è¯´å????##å§???????##????¸¤ä¸?awToken?»æ?
+    // tokens from sentence, excluding WordType.SENTENCE_BEGIN and WordType.SENTENCE_END
     for (int i = 1; i < segTokenList.size() - 1; i++) {
       result.add(convertSegToken((SegToken) segTokenList.get(i), sentence,
           sentenceToken.startOffset(), "word"));
@@ -55,14 +57,13 @@ public class WordSegmenter {
   }
 
   /**
+   * Convert a {@link SegToken} to a Lucene {@link Token}
    * 
-   * å°?awTokenç±»å?è½????´¢å¼??è¦??Tokenç±»å?ï¼? ??¸ºç´¢å???è¦?awToken?¨å??¥ä¸­???å®¹ï? ???è½???¶é?è¦??å®???¥å???
-   * 
-   * @param rt
-   * @param sentence è½????è¦???¥å????
-   * @param sentenceStartOffset sentence?¨æ?ç«?¸­???å§??ç½?
-   * @param type tokenç±»å?ï¼??è®¤å?è¯¥æ?word
-   * @return
+   * @param st input {@link SegToken}
+   * @param sentence associated Sentence
+   * @param sentenceStartOffset offset into sentence
+   * @param type token type, default is word
+   * @return Lucene {@link Token}
    */
   public Token convertSegToken(SegToken st, String sentence,
       int sentenceStartOffset, String type) {
diff --git a/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/WordTokenizer.java b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/WordTokenizer.java
index 7a26f05..0972683 100644
--- a/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/WordTokenizer.java
+++ b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/WordTokenizer.java
@@ -25,11 +25,11 @@ import org.apache.lucene.analysis.Token;
 import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.analysis.Tokenizer;
 
+/**
+ * A {@link Tokenizer} that breaks sentences into words.
+ */
 public class WordTokenizer extends Tokenizer {
 
-  /**
-   * ???ä¸»ç?åº??WordTokenizer????????½½??
-   */
   private WordSegmenter wordSegmenter;
 
   private TokenStream in;
@@ -41,13 +41,10 @@ public class WordTokenizer extends Tokenizer {
   private Token sentenceToken = new Token();
 
   /**
-   * è®¾è?ä¸??SentenceTokenizer???ä¸?å¤??å±????SentenceTokenizer???å­???ºï?
-   * ?©ç?HHMMSegmentä¸»ç?åº???¥å????ï¼????????ç»??è¿????
+   * Construct a new WordTokenizer.
    * 
-   * @param in ?¥å???oken
-   * @param smooth å¹³æ??½æ?
-   * @param dataPath è£?½½?¸å?å­??ä¸??????¸ç????
-   * @see init()
+   * @param in {@link TokenStream} of sentences
+   * @param wordSegmenter {@link WordSegmenter} to break sentences into words 
    */
   public WordTokenizer(TokenStream in, WordSegmenter wordSegmenter) {
     this.in = in;
@@ -66,17 +63,16 @@ public class WordTokenizer extends Tokenizer {
   }
 
   /**
-   * å½??????¥å????å¹¶ç´¢å¼??æ¯??ï¼??è¦?????ä¸?ä¸??å­?okenï¼? ????°è?è´£è??¨ä?ä¸?å±??SentenceTokenizer?»å?è½½ä?ä¸?ä¸??å­?? å¹¶å??¶å?è¯??
-   * å°??è¯?????å­??Token?¾å?tokenBufferä¸?
+   * Process the next input sentence, placing tokens into tokenBuffer
    * 
-   * @return è¯»å?å¹¶å????ä¸?ä¸??å­????????å¦??æ²¡æ????ï¼?????ä»¶å????æ¯?????æ²¡æ?Tokenäº?
+   * @return true if more tokens were placed into tokenBuffer.
    * @throws IOException
    */
   private boolean processNextSentence() throws IOException {
     sentenceToken = in.next(sentenceToken);
     if (sentenceToken == null)
       return false;
-    tokenBuffer = wordSegmenter.segmentSentence(sentenceToken, 1);
+    tokenBuffer = wordSegmenter.segmentSentence(sentenceToken);
     tokenIter = tokenBuffer.iterator();
     return tokenBuffer != null && tokenIter.hasNext();
   }
diff --git a/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/WordType.java b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/WordType.java
index 808ee42..f870400 100644
--- a/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/WordType.java
+++ b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/WordType.java
@@ -17,22 +17,49 @@
 
 package org.apache.lucene.analysis.cn.smart;
 
+/**
+ * Internal SmartChineseAnalyzer token type constants
+ */
 public class WordType {
 
+  /**
+   * Start of a Sentence
+   */
   public final static int SENTENCE_BEGIN = 0;
 
-  public final static int SENTENCE_END = 1;// ?¥å????å¤´å?ç»??
+  /**
+   * End of a Sentence
+   */
+  public final static int SENTENCE_END = 1;
 
-  public final static int CHINESE_WORD = 2;// ä¸??è¯?
+  /**
+   * Chinese Word 
+   */
+  public final static int CHINESE_WORD = 2;
 
+  /**
+   * ASCII String
+   */
   public final static int STRING = 3;
 
-  public final static int NUMBER = 4; // asciiå­??ä¸²å??°å?
+  /**
+   * ASCII Alphanumeric 
+   */
+  public final static int NUMBER = 4;
 
-  public final static int DELIMITER = 5; // ??????¹ç???
+  /**
+   * Punctuation Symbol
+   */
+  public final static int DELIMITER = 5;
 
+  /**
+   * Full-Width String
+   */
   public final static int FULLWIDTH_STRING = 6;
 
-  public final static int FULLWIDTH_NUMBER = 7;// ????¨è?å­?????ç¬?¸²ï¼???¨è??°å????å­?
+  /**
+   * Full-Width Alphanumeric
+   */
+  public final static int FULLWIDTH_NUMBER = 7;
 
 }
diff --git a/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/AbstractDictionary.java b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/AbstractDictionary.java
index d0da76b..ff7d839 100644
--- a/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/AbstractDictionary.java
+++ b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/AbstractDictionary.java
@@ -19,19 +19,29 @@ package org.apache.lucene.analysis.cn.smart.hhmm;
 
 import java.io.UnsupportedEncodingException;
 
+/**
+ * <p>
+ * SmartChineseAnalyzer abstract dictionary implementation.
+ * </p>
+ * <p>
+ * Contains methods for dealing with GB2312 encoding.
+ * </p>
+ */
 public abstract class AbstractDictionary {
   /**
-   * ç¬??ä¸??å­?¸º??????ä»???¢æ?15ä¸??ï¼??15*94ä¸??ç¬?
+   * First Chinese Character in GB2312 (15 * 94)
+   * Characters in GB2312 are arranged in a grid of 94 * 94, 0-14 are unassigned or punctuation.
    */
   public static final int GB2312_FIRST_CHAR = 1410;
 
   /**
-   * GB2312å­????¸­01~87???ç¬??????½æ??????8178ä¸?
+   * Last Chinese Character in GB2312 (87 * 94). 
+   * Characters in GB2312 are arranged in a grid of 94 * 94, 88-94 are unassigned.
    */
   public static final int GB2312_CHAR_NUM = 87 * 94;
 
   /**
-   * è¯????»¶ä¸??å½??6768ä¸??å­??è¯??ç»??
+   * Dictionary data contains 6768 Chinese characters with frequency statistics.
    */
   public static final int CHAR_NUM_IN_FILE = 6768;
 
@@ -45,33 +55,33 @@ public abstract class AbstractDictionary {
   // B0F0 æ¢? æ¦? ?? ç»? æ£? ç£? ?? ?? ?? è°? ?? ?? ?? è¤? ??
   // =====================================================
   //
-  // GB2312 å­??????ºä????è¡??
-  // ?ºå? å­?? å­??ç±»å?
-  // 01 94 ä¸??????
-  // 02 72 é¡ºå??·ç?
-  // 03 94 ???å­??
-  // 04 83 ?¥æ????
+  // GB2312 character setï¼?
+  // 01 94 Symbols
+  // 02 72 Numbers
+  // 03 94 Latin
+  // 04 83 Kana
   // 05 86 Katakana
-  // 06 48 å¸??å­??
-  // 07 66 ä¿??å­??
-  // 08 63 æ±???¼é?ç¬??
-  // 09 76 ?¾å½¢ç¬??
-  // 10-15 å¤????
-  // 16-55 3755 ä¸?çº§æ?å­??ä»¥æ??³ä¸ºåº?
-  // 56-87 3008 äº?º§æ±??ï¼?»¥ç¬??ä¸ºå?
-  // 88-94 å¤????
+  // 06 48 Greek
+  // 07 66 Cyrillic
+  // 08 63 Phonetic Symbols
+  // 09 76 Drawing Symbols
+  // 10-15 Unassigned
+  // 16-55 3755 Plane 1, in pinyin order
+  // 56-87 3008 Plane 2, in radical/stroke order
+  // 88-94 Unassigned
   // ======================================================
 
   /**
-   * GB2312 ?±æ?å½?? 7445 ä¸??ç¬???¶ä¸­ç®????å­? 6763 ä¸??å­??????? 682 ä¸???
+   * <p>
+   * Transcode from GB2312 ID to Unicode
+   * </p>
+   * <p>
+   * GB2312 is divided into a 94 * 94 grid, containing 7445 characters consisting of 6763 Chinese characters and 682 symbols.
+   * Some regions are unassigned (reserved).
+   * </p>
    * 
-   * GB2312 å°???¶å????ç¬??ä¸? 94 ä¸??ï¼???·ä¸º 01 ?ºè? 94 ?ºï?æ¯?¸ª?ºæ?å½? 94 ä¸??ç¬??ç¼??ä¸? 01 ä½?? 94
-   * ä½??01ä¸ºèµ·å§??0xA1ï¼?94ä½??äº?0xFE??B2312 ???ä¸?ä¸??ç¬???±ä??¶å?ä¸?å¯¹å?????·å?ä½????ç¡?????å¦??æ±????????ç¼??ä¸? 16 ?? 01
-   * ä½???
-   */
-  /**
-   * @param ccid
-   * @return
+   * @param ccid GB2312 id
+   * @return unicode String
    */
   public String getCCByGB2312Id(int ccid) {
     if (ccid < 0 || ccid > WordDictionary.GB2312_CHAR_NUM)
@@ -90,16 +100,16 @@ public abstract class AbstractDictionary {
   }
 
   /**
-   * ?¹æ?è¾????nicodeå­??ï¼???????B2312ç¼??????sciiç¼??ï¼?
+   * Transcode from Unicode to GB2312
    * 
-   * @param ch è¾????B2312ä¸??å­??????SCIIå­??(128ä¸?)
-   * @return ch??B2312ä¸??ä½?½®ï¼?-1è¡¨ç¤ºè¯¥å?ç¬??è®¤è?
+   * @param ch input character in Unicode, or character in Basic Latin range.
+   * @return position in GB2312
    */
   public short getGB2312Id(char ch) {
     try {
       byte[] buffer = Character.toString(ch).getBytes("GB2312");
       if (buffer.length != 2) {
-        // æ­£å¸¸???ä¸?ufferåº????¸¤ä¸????????è¯´æ?chä¸??äº?B2312ç¼??ï¼??è¿??'?'ï¼???¶è????è®¤è?è¯¥å?ç¬?
+        // Should be a two-byte character
         return -1;
       }
       int b0 = (int) (buffer[0] & 0x0FF) - 161; // ç¼??ä»?1å¼?å§????????0xA1=161
@@ -112,12 +122,10 @@ public abstract class AbstractDictionary {
   }
 
   /**
-   * ?¹è???32ä½?NV hashç®??ï¼??ä½??ç¨??ä¸??ç¬??hash?½æ?.ç¬?????äº?ash?½æ??¨æ????è®¡ç?hashè¡?? ä½¿å???????ï¼?
-   * å¹¶è??¿å???ashè¡¨è?å¯?????´ç??¿æ??´è?ç®?????
+   * 32-bit FNV Hash Function
    * 
-   * @param c å¾?ash??nicodeå­??
-   * @return c???å¸???
-   * @see Utility.hash2()
+   * @param c input character
+   * @return hashcode
    */
   public long hash1(char c) {
     final long p = 1099511628211L;
@@ -133,9 +141,10 @@ public abstract class AbstractDictionary {
   }
 
   /**
-   * @see Utility.hash1(char[])
-   * @param carray
-   * @return
+   * 32-bit FNV Hash Function
+   * 
+   * @param carray character array
+   * @return hashcode
    */
   public long hash1(char carray[]) {
     final long p = 1099511628211L;
@@ -155,16 +164,14 @@ public abstract class AbstractDictionary {
   }
 
   /**
-   * djb2???ç®??ï¼??ä½??ç¨??ä¸??ç¬??hash?½æ?
-   * 
    * djb2 hash algorithmï¼?his algorithm (k=33) was first reported by dan
    * bernstein many years ago in comp.lang.c. another version of this algorithm
    * (now favored by bernstein) uses xor: hash(i) = hash(i - 1) * 33 ^ str[i];
    * the magic of number 33 (why it works better than many other constants,
    * prime or not) has never been adequately explained.
    * 
-   * @param c
-   * @return
+   * @param c character
+   * @return hashcode
    */
   public int hash2(char c) {
     int hash = 5381;
@@ -177,9 +184,14 @@ public abstract class AbstractDictionary {
   }
 
   /**
-   * @see Utility.hash2(char[])
-   * @param carray
-   * @return
+   * djb2 hash algorithmï¼?his algorithm (k=33) was first reported by dan
+   * bernstein many years ago in comp.lang.c. another version of this algorithm
+   * (now favored by bernstein) uses xor: hash(i) = hash(i - 1) * 33 ^ str[i];
+   * the magic of number 33 (why it works better than many other constants,
+   * prime or not) has never been adequately explained.
+   * 
+   * @param carray character array
+   * @return hashcode
    */
   public int hash2(char carray[]) {
     int hash = 5381;
diff --git a/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/BiSegGraph.java b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/BiSegGraph.java
index 1854b57..bd2c9a1 100644
--- a/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/BiSegGraph.java
+++ b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/BiSegGraph.java
@@ -26,6 +26,12 @@ import java.util.Map;
 
 import org.apache.lucene.analysis.cn.smart.Utility;
 
+/**
+ * Graph representing possible token pairs (bigrams) at each start offset in the sentence.
+ * <p>
+ * For each start offset, a list of possible token pairs is stored.
+ * </p>
+ */
 public class BiSegGraph {
 
   private Map tokenPairListTable = new HashMap();
@@ -39,15 +45,8 @@ public class BiSegGraph {
     generateBiSegGraph(segGraph);
   }
 
-  /**
-   * ???ä¸¤ä¸¤è¯???´ç?äº???¾è¡¨ï¼??ç»??ä¿???¨ä?ä¸?ultiTokenPairMapä¸?
-   * 
-   * @param segGraph ?????Token??¡¨
-   * @param smooth å¹³æ?ç³»æ?
-   * @param biDict äº??è¯??
-   * @return
-   * 
-   * @see MultiTokenPairMap
+  /*
+   * Generate a BiSegGraph based upon a SegGraph
    */
   private void generateBiSegGraph(SegGraph segGraph) {
     double smooth = 0.1;
@@ -57,7 +56,7 @@ public class BiSegGraph {
 
     int next;
     char[] idBuffer;
-    // ä¸?egGraphä¸??æ¯?¸ª???èµ?»¥ä¸?ä¸????
+    // get the list of tokens ordered and indexed
     segTokenList = segGraph.makeIndex();
     // ??¸ºstartTokenï¼?"å§?##å§?"ï¼??èµ·å?ä½?½®??-1???keyä¸?-1?¶å?ä»¥å???tartToken
     int key = -1;
@@ -119,31 +118,29 @@ public class BiSegGraph {
   }
 
   /**
-   * ?¥ç?SegTokenPair??????ç½?¸ºto(SegTokenPair.toä¸?o)???å­??SegTokenPairï¼?
-   * å¦??æ²¡æ??????oå¤?²¡??egTokenPair?????æ²¡æ?æ·»å?
+   * Returns true if their is a list of token pairs at this offset (index of the second token)
    * 
-   * @param to SegTokenPair.to
-   * @return
+   * @param to index of the second token in the token pair
+   * @return true if a token pair exists
    */
   public boolean isToExist(int to) {
     return tokenPairListTable.get(new Integer(to)) != null;
   }
 
   /**
-   * ???SegTokenPair.toä¸?o?????egTokenPairï¼????²¡???è¿??null
+   * Return a {@link List} of all token pairs at this offset (index of the second token)
    * 
-   * @param to
-   * @return ???????egTokenPair.to??egTokenPair?????
+   * @param to index of the second token in the token pair
+   * @return {@link List} of token pairs.
    */
   public List getToList(int to) {
     return (List) tokenPairListTable.get(new Integer(to));
   }
 
   /**
-   * ??iSegGraphä¸?????ä¸?egTokenPairï¼??äº?egTokenPair????¸å?SegTokenPair.
-   * to?¾å????ä¸?rrayListä¸?
+   * Add a {@link SegTokenPair}
    * 
-   * @param tokenPair
+   * @param tokenPair {@link SegTokenPair}
    */
   public void addSegTokenPair(SegTokenPair tokenPair) {
     int to = tokenPair.to;
@@ -158,16 +155,16 @@ public class BiSegGraph {
   }
 
   /**
-   * @return TokenPair????°ï?ä¹?°±??apä¸??????·ç?TokenPairç§????
+   * Get the number of {@link SegTokenPair} entries in the table.
+   * @return number of {@link SegTokenPair} entries
    */
   public int getToCount() {
     return tokenPairListTable.size();
   }
 
   /**
-   * ??eterbiç®??è®¡ç?ä»?µ·?¹å?ç»???????·¯å¾?
-   * 
-   * @return
+   * Find the shortest path with the Viterbi algorithm.
+   * @return {@link List}
    */
   public List getShortPath() {
     int current;
@@ -198,7 +195,7 @@ public class BiSegGraph {
       path.add(newNode);
     }
 
-    // ?¥ä??¥ä?nodePathsä¸??ç®??èµ·ç??°ç??¹ç????è·??
+    // Calculate PathNodes
     int preNode, lastNode;
     lastNode = path.size() - 1;
     current = lastNode;
diff --git a/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/BigramDictionary.java b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/BigramDictionary.java
index 0ae2b88..ce5f545 100644
--- a/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/BigramDictionary.java
+++ b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/BigramDictionary.java
@@ -32,6 +32,9 @@ import java.nio.ByteOrder;
 
 import org.apache.lucene.analysis.cn.smart.AnalyzerProfile;
 
+/**
+ * SmartChineseAnalyzer Bigram dictionary.
+ */
 public class BigramDictionary extends AbstractDictionary {
 
   private BigramDictionary() {
@@ -43,12 +46,8 @@ public class BigramDictionary extends AbstractDictionary {
 
   public static final int PRIME_BIGRAM_LENGTH = 402137;
 
-  /**
-   * bigramTable ?¥å??¨è?ä¸??ä¹????·³è½????? bigramHashTable ?? frequencyTable
-   * å°±æ??¨æ?å­??è¿??é¢???????????? ä¸ºä?????¥è???º¦??????å­?? ??? hash ?¼æ?ä»£æ??³è?è¯??ä¸ºæ?è¯????? ?³è?è¯?°±??
-   * (formWord+'@'+toWord) ï¼? ?©ç? FNV1 hash ç®???¥è?ç®???????ash?? ï¼?¹¶ä¿???? bigramHashTable
-   * ä¸???©ç? hash ?¼æ?ä»£æ??³è?è¯?????ä¼?º§???å°??????²ç?ï¼? ä½?? long ç±»å?
-   * (64bit)??ash?¼æ????å°??æ¦??????????igramHashTable[i]ä¸?requencyTable[i]ä¸?ä¸?å¯¹å?
+  /*
+   * The word associations are stored as FNV1 hashcodes, which have a small probability of collision, but save memory.  
    */
   private long[] bigramHashTable;
 
@@ -128,7 +127,7 @@ public class BigramDictionary extends AbstractDictionary {
         bigramHashTable = new long[PRIME_BIGRAM_LENGTH];
         frequencyTable = new int[PRIME_BIGRAM_LENGTH];
         for (int i = 0; i < PRIME_BIGRAM_LENGTH; i++) {
-          // å®??ä¸??0ä½?¸º????¼æ?ä¸??¹é?é¢????¸º??¸ªå­??ä¸²å???ash?¼ä¸º0ï¼????????å¸¸å?ï¼??æ­¤å½±???å¤?
+          // it is possible for a value to hash to 0, but the probability is extremely low
           bigramHashTable[i] = 0;
           frequencyTable[i] = 0;
         }
@@ -141,10 +140,9 @@ public class BigramDictionary extends AbstractDictionary {
   }
 
   /**
-   * å°??åº??ä»¶å?è½½å?WordDictionary????³æ??????¸­ï¼?????è½½ï?æ²¡æ?è¿????¹¶??¿®?¹æ?ä½?
+   * Load the datafile into this BigramDictionary
    * 
-   * @param dctFilePath
-   * @return
+   * @param dctFilePath path to the Bigramdictionary (bigramdict.mem)
    * @throws FileNotFoundException
    * @throws IOException
    * @throws UnsupportedEncodingException
@@ -159,14 +157,14 @@ public class BigramDictionary extends AbstractDictionary {
     String tmpword;
     RandomAccessFile dctFile = new RandomAccessFile(dctFilePath, "r");
 
-    // å­????»¶ä¸??ä¸?ä¸??å­???°ç?ä½?½®??0ï¼?????ä¸??6768
+    // GB2312 characters 0 - 6768
     for (i = GB2312_FIRST_CHAR; i < GB2312_FIRST_CHAR + CHAR_NUM_IN_FILE; i++) {
       String currentStr = getCCByGB2312Id(i);
       // if (i == 5231)
       // System.out.println(i);
 
-      dctFile.read(intBuffer);// ???åº??ä»¶å?cä¸???????ä»¥å??¥ç???»¶ä¸?ittle
-      // endianç¼??ï¼???avaä¸?ig endianï¼??é¡»è½¬?¢è???
+      dctFile.read(intBuffer);
+      // the dictionary was developed for C, and byte order must be converted to work with Java
       cnt = ByteBuffer.wrap(intBuffer).order(ByteOrder.LITTLE_ENDIAN).getInt();
       if (cnt <= 0) {
         continue;
@@ -272,9 +270,8 @@ public class BigramDictionary extends AbstractDictionary {
       return -1;
   }
 
-  /**
-   * @param c
-   * @return
+  /*
+   * lookup the index into the frequency array.
    */
   private int getBigramItemIndex(char carray[]) {
     long hashId = hash1(carray);
diff --git a/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/HHMMSegmenter.java b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/HHMMSegmenter.java
index c3532f2..4396a61 100644
--- a/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/HHMMSegmenter.java
+++ b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/HHMMSegmenter.java
@@ -23,18 +23,18 @@ import org.apache.lucene.analysis.cn.smart.CharType;
 import org.apache.lucene.analysis.cn.smart.Utility;
 import org.apache.lucene.analysis.cn.smart.WordType;
 
+/**
+ * Finds the optimal segmentation of a sentence into Chinese words
+ */
 public class HHMMSegmenter {
 
   private static WordDictionary wordDict = WordDictionary.getInstance();
 
   /**
-   * å¯»æ?sentenceä¸??????½ç?Tokenï¼?????æ·»å?ä¸¤ä¸ª?¹æ?Tokenï¼?"å§?##å§?",
-   * "??##??"ï¼?"å§?##å§?"Token??µ·å§??ç½??-1,"??##??"Token??µ·å§??ç½???¥å????åº?
+   * Create the {@link SegGraph} for a sentence.
    * 
-   * @param sentence è¾?????å­??ä¸????"å§?##å§?","??##??"ç­?
-   * @param coreDict ?¸å?å­??
-   * @return ??????½ç?Token
-   * @see MultiTokenMap
+   * @param sentence input sentence, without start and end markers
+   * @return {@link SegGraph} corresponding to the input sentence.
    */
   private SegGraph createSegGraph(String sentence) {
     int i = 0, j;
@@ -168,16 +168,16 @@ public class HHMMSegmenter {
   }
 
   /**
-   * ä¸?entenceä¸??æ¯?¸ªå­??ç¡????????ç¬?±»??
+   * Get the character types for every character in a sentence.
    * 
    * @see Utility.charType(char)
-   * @param sentence è¾????????å­?
-   * @return è¿?????ç¬?±»???ç»??å¦??è¾??ä¸?ullï¼???????ull
+   * @param sentence input sentence
+   * @return array of character types corresponding to character positions in the sentence
    */
   private static int[] getCharTypes(String sentence) {
     int length = sentence.length();
     int[] charTypeArray = new int[length];
-    // ???å¯¹å???¸ªæ±?????ç¬?±»???ç»?
+    // the type of each character by position
     for (int i = 0; i < length; i++) {
       charTypeArray[i] = Utility.getCharType(sentence.charAt(i));
     }
@@ -185,6 +185,11 @@ public class HHMMSegmenter {
     return charTypeArray;
   }
 
+  /**
+   * Return a list of {@link PathNode} representing the best segmentation of a sentence
+   * @param sentence input sentence
+   * @return best segmentation as a {@link List}
+   */
   public List process(String sentence) {
     SegGraph segGraph = createSegGraph(sentence);
     BiSegGraph biSegGraph = new BiSegGraph(segGraph);
diff --git a/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/PathNode.java b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/PathNode.java
index 4a224ce..960a7d6 100644
--- a/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/PathNode.java
+++ b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/PathNode.java
@@ -17,6 +17,12 @@
 
 package org.apache.lucene.analysis.cn.smart.hhmm;
 
+/**
+ * SmartChineseAnalyzer internal node representation
+ * <p>
+ * Used by {@link BiSegGraph} to maximize the segmentation with the Viterbi algorithm.
+ * </p>
+ */
 public class PathNode implements Comparable {
   public double weight;
 
diff --git a/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/SegGraph.java b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/SegGraph.java
index bdfac7d..49c902b 100644
--- a/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/SegGraph.java
+++ b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/SegGraph.java
@@ -23,42 +23,53 @@ import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
 
+/**
+ * Graph representing possible tokens at each start offset in the sentence.
+ * <p>
+ * For each start offset, a list of possible tokens is stored.
+ * </p>
+ */
 public class SegGraph {
 
   /**
-   * ?¨ä?ä¸?rrayListè®°å?startOffset?¸å???okenï¼??ä¸?tartOffsetå°±æ?Token??ey
+   * Map of start offsets to ArrayList of tokens at that position
    */
-  private Map tokenListTable = new HashMap();
+  private Map /* <Integer, ArrayList<SegToken>> */ tokenListTable = new HashMap();
 
   private int maxStart = -1;
 
   /**
-   * ?¥ç?startOffsetä¸???oken???å­??ï¼????²¡???è¯´æ?så¤?²¡??oken?????æ²¡æ?æ·»å?
+   * Returns true if a mapping for the specified start offset exists
    * 
    * @param s startOffset
-   * @return
+   * @return true if there are tokens for the startOffset
    */
   public boolean isStartExist(int s) {
     return tokenListTable.get(new Integer(s)) != null;
   }
 
   /**
-   * ???startOffsetä¸??????okensï¼????²¡???è¿??null
+   * Get the list of tokens at the specified start offset
    * 
-   * @param s
-   * @return ???????tartOffset??oken?????
+   * @param s startOffset
+   * @return List of tokens at the specified start offset.
    */
   public List getStartList(int s) {
     return (List) tokenListTable.get(new Integer(s));
   }
 
+  /**
+   * Get the highest start offset in the map
+   * 
+   * @return maximum start offset, or -1 if the map is empty.
+   */
   public int getMaxStart() {
     return maxStart;
   }
 
   /**
-   * ä¸?egGraphä¸??????okens???ä¸?ä¸??ä¸???ndexï¼?ndexä»?0å¼?å§??
-   * ???startOffset?????¡ºåº??åº???¸å?startOffset??okens????¾ç½®???é¡ºå????
+   * Set the {@link SegToken#index} for each token, based upon its order by startOffset. 
+   * @return a {@link List} of these ordered tokens.
    */
   public List makeIndex() {
     List result = new ArrayList();
@@ -82,9 +93,8 @@ public class SegGraph {
   }
 
   /**
-   * ??apä¸?????ä¸?okenï¼??äº?oken????¸å?startOffset?¾å????ä¸??è¡¨ä¸­ï¼?
-   * 
-   * @param token
+   * Add a {@link SegToken} to the mapping, creating a new mapping at the token's startOffset if one does not exist. 
+   * @param token {@link SegToken}
    */
   public void addToken(SegToken token) {
     int s = token.startOffset;
@@ -101,18 +111,18 @@ public class SegGraph {
   }
 
   /**
-   * ?·å?SegGraphä¸????µ·å§??Startï¼??ç½?okenç±»ç?ä¸??ï¼??ä¸??å§??ç½???½æ?å¤?¸ªTokenï¼??æ­¤ä?ç½??ä¸?oken?°å¹¶ä¸????
-   * 
-   * @return
+   * Get the number of startOffsets.
+   *
+   * @return number of startOffsets in the mapping
    */
   public int getStartCount() {
     return tokenListTable.size();
   }
 
   /**
-   * å°?apä¸???¨ç?????oken???èµ·å?ä½?½®ä»???°å¤§???å¼?????ä¸??è¡?
+   * Return a {@link List} of all tokens in the map, ordered by startOffset.
    * 
-   * @return
+   * @return {@link List} of all tokens in the map.
    */
   public List toTokenList() {
     List result = new ArrayList();
diff --git a/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/SegToken.java b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/SegToken.java
index 7ce7e9e..1fb463f 100644
--- a/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/SegToken.java
+++ b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/SegToken.java
@@ -17,6 +17,9 @@
 
 package org.apache.lucene.analysis.cn.smart.hhmm;
 
+/**
+ * SmartChineseAnalyzer internal token
+ */
 public class SegToken {
   public char[] charArray;
 
@@ -51,13 +54,6 @@ public class SegToken {
   // + endOffset + ")/w(" + weight + ")t(" + wordType + ")";
   // }
 
-  /**
-   * ?¤æ?ä¸¤ä¸ªToken?¸ç????è¦??ä»¶æ?ä»?»¬??µ·å§??ç½??ç­????¸ºè¿??ä»?»¬????¥ä¸­???å®¹ä??·ï?
-   * ??osä¸?eight?½å?ä»¥ä?è¯??ä¸???°å?ä¸????»¥?¨ä?å¯¹å????æ³?¡¨ç¤ºï???????è¦??ä¸?oken
-   * 
-   * @param t
-   * @return
-   */
   // public boolean equals(RawToken t) {
   // return this.startOffset == t.startOffset
   // && this.endOffset == t.endOffset;
diff --git a/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/SegTokenFilter.java b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/SegTokenFilter.java
index 5295740..b2c2f0d 100644
--- a/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/SegTokenFilter.java
+++ b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/SegTokenFilter.java
@@ -20,27 +20,43 @@ package org.apache.lucene.analysis.cn.smart.hhmm;
 import org.apache.lucene.analysis.cn.smart.Utility;
 import org.apache.lucene.analysis.cn.smart.WordType;
 
+/**
+ * <p>
+ * Filters a {@link SegToken} by converting full-width latin to half-width, then lowercasing latin.
+ * Additionally, all punctuation is converted into {@link Utility#COMMON_DELIMITER}
+ * </p>
+ */
 public class SegTokenFilter {
 
+  /**
+   * Filter an input {@link SegToken}
+   * <p>
+   * Full-width latin will be converted to half-width, then all latin will be lowercased.
+   * All punctuation is converted into {@link Utility#COMMON_DELIMITER}
+   * </p>
+   * 
+   * @param token input {@link SegToken}
+   * @return normalized {@link SegToken}
+   */
   public SegToken filter(SegToken token) {
     switch (token.wordType) {
       case WordType.FULLWIDTH_NUMBER:
-      case WordType.FULLWIDTH_STRING:
+      case WordType.FULLWIDTH_STRING: /* first convert full-width -> half-width */
         for (int i = 0; i < token.charArray.length; i++) {
           if (token.charArray[i] >= 0xFF10)
             token.charArray[i] -= 0xFEE0;
 
-          if (token.charArray[i] >= 0x0041 && token.charArray[i] <= 0x005A)
+          if (token.charArray[i] >= 0x0041 && token.charArray[i] <= 0x005A) /* lowercase latin */
             token.charArray[i] += 0x0020;
         }
         break;
       case WordType.STRING:
         for (int i = 0; i < token.charArray.length; i++) {
-          if (token.charArray[i] >= 0x0041 && token.charArray[i] <= 0x005A)
+          if (token.charArray[i] >= 0x0041 && token.charArray[i] <= 0x005A) /* lowercase latin */
             token.charArray[i] += 0x0020;
         }
         break;
-      case WordType.DELIMITER:
+      case WordType.DELIMITER: /* convert all punctuation to Utility.COMMON_DELIMITER */
         token.charArray = Utility.COMMON_DELIMITER;
         break;
       default:
diff --git a/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/SegTokenPair.java b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/SegTokenPair.java
index b41cb76..96fcef4 100644
--- a/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/SegTokenPair.java
+++ b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/SegTokenPair.java
@@ -17,15 +17,21 @@
 
 package org.apache.lucene.analysis.cn.smart.hhmm;
 
+/**
+ * A pair of tokens in {@link SegGraph}
+ */
 public class SegTokenPair {
 
   public char[] charArray;
 
   /**
-   * from??o??okenå¯¹ç?index?·ï?è¡¨ç¤º??okenPair??¸¤ä¸?oken??egGraghä¸??ä½?½®??
+   * index of the first token in {@link SegGraph}
    */
   public int from;
 
+  /**
+   * index of the second token in {@link SegGraph}
+   */
   public int to;
 
   public double weight;
diff --git a/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/WordDictionary.java b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/WordDictionary.java
index 5e6b061..bc7514f 100644
--- a/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/WordDictionary.java
+++ b/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/WordDictionary.java
@@ -33,6 +33,10 @@ import java.nio.ByteOrder;
 import org.apache.lucene.analysis.cn.smart.AnalyzerProfile;
 import org.apache.lucene.analysis.cn.smart.Utility;
 
+/**
+ * SmartChineseAnalyzer Word Dictionary
+ *
+ */
 public class WordDictionary extends AbstractDictionary {
 
   private WordDictionary() {
@@ -41,7 +45,7 @@ public class WordDictionary extends AbstractDictionary {
   private static WordDictionary singleInstance;
 
   /**
-   * ä¸?ä¸??å¤§ç?ç´??ï¼??è¯?ash?¥æ??½å?????????ç½?
+   * Large prime number for hash function
    */
   public static final int PRIME_INDEX_LENGTH = 12071;
 
@@ -66,6 +70,10 @@ public class WordDictionary extends AbstractDictionary {
 
   // static Logger log = Logger.getLogger(WordDictionary.class);
 
+  /**
+   * Get the singleton dictionary instance.
+   * @return singleton
+   */
   public synchronized static WordDictionary getInstance() {
     if (singleInstance == null) {
       singleInstance = new WordDictionary();
@@ -82,10 +90,9 @@ public class WordDictionary extends AbstractDictionary {
   }
 
   /**
-   * ä»???¨æ?ä»¶å¤¹dctFileRoot??½½è¯??åº??ä»¶ï?é¦??æµ???????oredict.mem??»¶ï¼? å¦??????´æ?ä½?¸ºåº?????è±¡å?è½½ï?
-   * å¦??æ²¡æ????è½½è??¸å?æº??ä»?oredict.dct
+   * Attempt to load dictionary from provided directory, first trying coredict.mem, failing back on coredict.dct
    * 
-   * @param dctFileName è¯??åº??ä»¶ç?è·??
+   * @param dctFileRoot path to dictionary directory
    */
   public void load(String dctFileRoot) {
     String dctFilePath = dctFileRoot + "/coredict.dct";
@@ -119,9 +126,8 @@ public class WordDictionary extends AbstractDictionary {
   }
 
   /**
-   * ä»?ar?????½½è¯??åº??ä»¶ï?è¦??ä¿??WordDictionaryç±»å???·¯å¾?¸­??oredict.mem??»¶ï¼?»¥å°??ä½?¸ºåº?????è±¡å?è½?
+   * Load coredict.mem internally from the jar file.
    * 
-   * @param dctFileName è¯??åº??ä»¶ç?è·??
    * @throws ClassNotFoundException
    * @throws IOException
    */
@@ -171,10 +177,10 @@ public class WordDictionary extends AbstractDictionary {
   }
 
   /**
-   * å°??åº??ä»¶å?è½½å?WordDictionary????³æ??????¸­ï¼?????è½½ï?æ²¡æ?è¿????¹¶??¿®?¹æ?ä½?
+   * Load the datafile into this WordDictionary
    * 
-   * @param dctFilePath
-   * @return
+   * @param dctFilePath path to word dictionary (coredict.mem)
+   * @return number of words read
    * @throws FileNotFoundException
    * @throws IOException
    * @throws UnsupportedEncodingException
@@ -188,13 +194,13 @@ public class WordDictionary extends AbstractDictionary {
     String tmpword;
     RandomAccessFile dctFile = new RandomAccessFile(dctFilePath, "r");
 
-    // å­????»¶ä¸??ä¸?ä¸??å­???°ç?ä½?½®??0ï¼?????ä¸??6768
+    // GB2312 characters 0 - 6768
     for (i = GB2312_FIRST_CHAR; i < GB2312_FIRST_CHAR + CHAR_NUM_IN_FILE; i++) {
       // if (i == 5231)
       // System.out.println(i);
 
-      dctFile.read(intBuffer);// ???åº??ä»¶å?cä¸???????ä»¥å??¥ç???»¶ä¸?ittle
-      // endianç¼??ï¼???avaä¸?ig endianï¼??é¡»è½¬?¢è???
+      dctFile.read(intBuffer);
+      // the dictionary was developed for C, and byte order must be converted to work with Java
       cnt = ByteBuffer.wrap(intBuffer).order(ByteOrder.LITTLE_ENDIAN).getInt();
       if (cnt <= 0) {
         wordItem_charArrayTable[i] = null;
@@ -287,8 +293,8 @@ public class WordDictionary extends AbstractDictionary {
     wordItem_frequencyTable[delimiterIndex] = null;
   }
 
-  /**
-   * ???åº??????§æ?æ³?????å°?????ä¸??è¯??§ç?é¢????¹¶?°å?ä¸?ä¸??ä¸??ä»¥å?å°???¨ç©º?´ï???¿«??´¢??º¦
+  /*
+   * since we aren't doing POS-tagging, merge the frequencies for entries of the same word (with different POS)
    */
   private void mergeSameWords() {
     int i;
@@ -350,12 +356,9 @@ public class WordDictionary extends AbstractDictionary {
     }
   }
 
-  /**
+  /*
    * è®¡ç?å­??c?¨å?å¸?¡¨ä¸??è¯¥å????ç½???¶å?å°??????¡¨ä¸??ä½?½®???¼å?å§??
    * 
-   * @param c
-   * @param j
-   * @return
    */
   private boolean setTableIndex(char c, int j) {
     int index = getAvaliableTableIndex(c);
@@ -390,10 +393,6 @@ public class WordDictionary extends AbstractDictionary {
       return -1;
   }
 
-  /**
-   * @param c
-   * @return
-   */
   private short getWordItemTableIndex(char c) {
     int hash1 = (int) (hash1(c) % PRIME_INDEX_LENGTH);
     int hash2 = hash2(c) % PRIME_INDEX_LENGTH;
@@ -465,32 +464,33 @@ public class WordDictionary extends AbstractDictionary {
   }
 
   /**
-   * charArrayè¿?¸ª???å¯¹å????ç»??ä¸??WordDictionaryä¸????
+   * Returns true if the input word appears in the dictionary
    * 
-   * @param charArray
-   * @return trueè¡¨ç¤ºå­??ï¼?alseè¡¨ç¤ºä¸????
+   * @param charArray input word
+   * @return true if the word exists
    */
   public boolean isExist(char[] charArray) {
     return findInTable(charArray) != -1;
   }
 
   /**
-   * @see{getPrefixMatch(char[] charArray, int knownStart)}
-   * @param charArray
-   * @return
+   * Find the first word in the dictionary that starts with the supplied prefix
+   * 
+   * @see #getPrefixMatch(char[], int)
+   * @param charArray input prefix
+   * @return index of word, or -1 if not found
    */
   public int getPrefixMatch(char[] charArray) {
     return getPrefixMatch(charArray, 0);
   }
 
   /**
-   * ä»???¸ä¸­?¥æ?ä»?harArrayå¯¹å????è¯?¸º???(prefix)???è¯??ä½?½®, å¹¶è????ä¸?ä¸?»¡è¶³æ?ä»¶ç?ä½?½®??¸ºäº??å°??ç´?»£ä»?,
-   * ??»¥?¹æ?å·²æ??¥è?è®¾ç½®èµ·å???´¢ä½?½®, å¦??ä¸????µ·å§??ç½??é»????0
+   * Find the nth word in the dictionary that starts with the supplied prefix
    * 
-   * @see{getPrefixMatch(char[] charArray)}
-   * @param charArray ??????
-   * @param knownStart å·²ç???µ·å§??ç½?
-   * @return æ»¡è¶³????¡ä»¶???ä¸?ä¸??è¯??ä½?½®
+   * @see #getPrefixMatch(char[])
+   * @param charArray input prefix
+   * @param knownStart relative position in the dictionary to start
+   * @return index of word, or -1 if not found
    */
   public int getPrefixMatch(char[] charArray, int knownStart) {
     short index = getWordItemTableIndex(charArray[0]);
@@ -521,11 +521,10 @@ public class WordDictionary extends AbstractDictionary {
   }
 
   /**
-   * ?·å?idArrayå¯¹å???????é¢????osä¸?-1??????????§ç?è¯??
+   * Get the frequency of a word from the dictionary
    * 
-   * @param charArray è¾?????è¯??åº??charArray
-   * @param pos è¯??§ï?-1è¡¨ç¤ºè¦??æ±???????è¯??§ç?è¯??
-   * @return idArrayå¯¹å????é¢?
+   * @param charArray input word
+   * @return word frequency, or zero if the word is not found
    */
   public int getFrequency(char[] charArray) {
     short hashIndex = getWordItemTableIndex(charArray[0]);
@@ -539,12 +538,11 @@ public class WordDictionary extends AbstractDictionary {
   }
 
   /**
-   * ?¤æ?charArrayå¯¹å????ç¬?¸²???è·???¸ä¸­charArray[0]å¯¹å???ordIndex??harArray?¸ç?,
-   * ä¹?°±???charArray???ç½???¾ç????ä¸??å°±æ?wordIndex
+   * Return true if the dictionary entry at itemIndex for table charArray[0] is charArray
    * 
-   * @param charArray è¾????harArrayè¯??ï¼??ä¸?ä¸??è¡¨ç¤ºè¯??ä¸??ç´¢å???
-   * @param itemIndex ä½?½®ç¼??
-   * @return ????¸ç?
+   * @param charArray input word
+   * @param itemIndex item index for table charArray[0]
+   * @return true if the entry exists
    */
   public boolean isEqual(char[] charArray, int itemIndex) {
     short hashIndex = getWordItemTableIndex(charArray[0]);
diff --git a/contrib/analyzers/src/resources/org/apache/lucene/analysis/cn/stopwords.txt b/contrib/analyzers/src/resources/org/apache/lucene/analysis/cn/stopwords.txt
index c35f9fa..547853a 100644
--- a/contrib/analyzers/src/resources/org/apache/lucene/analysis/cn/stopwords.txt
+++ b/contrib/analyzers/src/resources/org/apache/lucene/analysis/cn/stopwords.txt
@@ -1,4 +1,4 @@
-////////// å°???¹ç??·å??¨å??? ////////////////
+////////// Punctuation tokens to remove ////////////////
 ,
 .
 `
@@ -51,8 +51,8 @@ $
 ï¼?
 ï¼?
 ??
-??//ä¸??ç©ºæ?å­??
+??//IDEOGRAPHIC SPACE character (Used as a space in Chinese)
 
-//////////////// ?±æ????è¯? ////////////////
+//////////////// English Stop Words ////////////////
 
-//////////////// ä¸?????è¯? ////////////////
+//////////////// Chinese Stop Words ////////////////

