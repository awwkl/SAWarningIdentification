GitDiffStart: b3a77738fa1b8af7b78cbead89f79e0dda2f6744 | Tue Nov 26 20:14:22 2013 +0000
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter.java b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter.java
index 7aad386..0090774 100644
--- a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter.java
+++ b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/directory/DirectoryTaxonomyWriter.java
@@ -26,8 +26,8 @@ import org.apache.lucene.facet.taxonomy.FacetLabel;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.facet.taxonomy.TaxonomyWriter;
 import org.apache.lucene.facet.taxonomy.writercache.TaxonomyWriterCache;
-import org.apache.lucene.facet.taxonomy.writercache.cl2o.Cl2oTaxonomyWriterCache;
-import org.apache.lucene.facet.taxonomy.writercache.lru.LruTaxonomyWriterCache;
+import org.apache.lucene.facet.taxonomy.writercache.Cl2oTaxonomyWriterCache;
+import org.apache.lucene.facet.taxonomy.writercache.LruTaxonomyWriterCache;
 import org.apache.lucene.index.AtomicReader;
 import org.apache.lucene.index.AtomicReaderContext;
 import org.apache.lucene.index.CorruptIndexException; // javadocs
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/CategoryPathUtils.java b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/CategoryPathUtils.java
new file mode 100644
index 0000000..1ea5e97
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/CategoryPathUtils.java
@@ -0,0 +1,82 @@
+package org.apache.lucene.facet.taxonomy.writercache;
+
+import org.apache.lucene.facet.taxonomy.FacetLabel;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/** Utilities for use of {@link FacetLabel} by {@link CompactLabelToOrdinal}. */
+class CategoryPathUtils {
+  
+  /** Serializes the given {@link FacetLabel} to the {@link CharBlockArray}. */
+  public static void serialize(FacetLabel cp, CharBlockArray charBlockArray) {
+    charBlockArray.append((char) cp.length);
+    if (cp.length == 0) {
+      return;
+    }
+    for (int i = 0; i < cp.length; i++) {
+      charBlockArray.append((char) cp.components[i].length());
+      charBlockArray.append(cp.components[i]);
+    }
+  }
+
+  /**
+   * Calculates a hash function of a path that was serialized with
+   * {@link #serialize(FacetLabel, CharBlockArray)}.
+   */
+  public static int hashCodeOfSerialized(CharBlockArray charBlockArray, int offset) {
+    int length = charBlockArray.charAt(offset++);
+    if (length == 0) {
+      return 0;
+    }
+    
+    int hash = length;
+    for (int i = 0; i < length; i++) {
+      int len = charBlockArray.charAt(offset++);
+      hash = hash * 31 + charBlockArray.subSequence(offset, offset + len).hashCode();
+      offset += len;
+    }
+    return hash;
+  }
+
+  /**
+   * Check whether the {@link FacetLabel} is equal to the one serialized in
+   * {@link CharBlockArray}.
+   */
+  public static boolean equalsToSerialized(FacetLabel cp, CharBlockArray charBlockArray, int offset) {
+    int n = charBlockArray.charAt(offset++);
+    if (cp.length != n) {
+      return false;
+    }
+    if (cp.length == 0) {
+      return true;
+    }
+    
+    for (int i = 0; i < cp.length; i++) {
+      int len = charBlockArray.charAt(offset++);
+      if (len != cp.components[i].length()) {
+        return false;
+      }
+      if (!cp.components[i].equals(charBlockArray.subSequence(offset, offset + len))) {
+        return false;
+      }
+      offset += len;
+    }
+    return true;
+  }
+
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/CharBlockArray.java b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/CharBlockArray.java
new file mode 100644
index 0000000..5c7d46f
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/CharBlockArray.java
@@ -0,0 +1,212 @@
+package org.apache.lucene.facet.taxonomy.writercache;
+
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.ObjectInputStream;
+import java.io.ObjectOutputStream;
+import java.io.OutputStream;
+import java.io.Serializable;
+import java.util.ArrayList;
+import java.util.List;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Similar to {@link StringBuilder}, but with a more efficient growing strategy.
+ * This class uses char array blocks to grow.
+ * 
+ * @lucene.experimental
+ */
+class CharBlockArray implements Appendable, Serializable, CharSequence {
+
+  private static final long serialVersionUID = 1L;
+
+  private final static int DefaultBlockSize = 32 * 1024;  // 32 KB default size
+
+  final static class Block implements Serializable, Cloneable {
+    private static final long serialVersionUID = 1L;
+
+    final char[] chars;
+    int length;
+
+    Block(int size) {
+      this.chars = new char[size];
+      this.length = 0;
+    }
+  }
+
+  List<Block> blocks;
+  Block current;
+  int blockSize;
+  int length;
+
+  CharBlockArray() {
+    this(DefaultBlockSize);
+  }
+
+  CharBlockArray(int blockSize) {
+    this.blocks = new ArrayList<Block>();
+    this.blockSize = blockSize;
+    addBlock();
+  }
+
+  private void addBlock() {
+    this.current = new Block(this.blockSize);
+    this.blocks.add(this.current);
+  }
+
+  int blockIndex(int index) {
+    return index / blockSize;
+  }
+
+  int indexInBlock(int index) {
+    return index % blockSize;
+  }
+
+  @Override
+  public CharBlockArray append(CharSequence chars) {
+    return append(chars, 0, chars.length());
+  }
+
+  @Override
+  public CharBlockArray append(char c) {
+    if (this.current.length == this.blockSize) {
+      addBlock();
+    }
+    this.current.chars[this.current.length++] = c;
+    this.length++;
+
+    return this;
+  }
+
+  @Override
+  public CharBlockArray append(CharSequence chars, int start, int length) {
+    int end = start + length;
+    for (int i = start; i < end; i++) {
+      append(chars.charAt(i));
+    }
+    return this;
+  }
+
+  public CharBlockArray append(char[] chars, int start, int length) {
+    int offset = start;
+    int remain = length;
+    while (remain > 0) {
+      if (this.current.length == this.blockSize) {
+        addBlock();
+      }
+      int toCopy = remain;
+      int remainingInBlock = this.blockSize - this.current.length;
+      if (remainingInBlock < toCopy) {
+        toCopy = remainingInBlock;
+      }
+      System.arraycopy(chars, offset, this.current.chars, this.current.length, toCopy);
+      offset += toCopy;
+      remain -= toCopy;
+      this.current.length += toCopy;
+    }
+
+    this.length += length;
+    return this;
+  }
+
+  public CharBlockArray append(String s) {
+    int remain = s.length();
+    int offset = 0;
+    while (remain > 0) {
+      if (this.current.length == this.blockSize) {
+        addBlock();
+      }
+      int toCopy = remain;
+      int remainingInBlock = this.blockSize - this.current.length;
+      if (remainingInBlock < toCopy) {
+        toCopy = remainingInBlock;
+      }
+      s.getChars(offset, offset + toCopy, this.current.chars, this.current.length);
+      offset += toCopy;
+      remain -= toCopy;
+      this.current.length += toCopy;
+    }
+
+    this.length += s.length();
+    return this;
+  }
+
+  @Override
+  public char charAt(int index) {
+    Block b = blocks.get(blockIndex(index));
+    return b.chars[indexInBlock(index)];
+  }
+
+  @Override
+  public int length() {
+    return this.length;
+  }
+
+  @Override
+  public CharSequence subSequence(int start, int end) {
+    int remaining = end - start;
+    StringBuilder sb = new StringBuilder(remaining);
+    int blockIdx = blockIndex(start);
+    int indexInBlock = indexInBlock(start);
+    while (remaining > 0) {
+      Block b = blocks.get(blockIdx++);
+      int numToAppend = Math.min(remaining, b.length - indexInBlock);
+      sb.append(b.chars, indexInBlock, numToAppend);
+      remaining -= numToAppend;
+      indexInBlock = 0; // 2nd+ iterations read from start of the block 
+    }
+    return sb.toString();
+  }
+
+  @Override
+  public String toString() {
+    StringBuilder sb = new StringBuilder();
+    for (Block b : blocks) {
+      sb.append(b.chars, 0, b.length);
+    }
+    return sb.toString();
+  }
+
+  void flush(OutputStream out) throws IOException {
+    ObjectOutputStream oos = null;
+    try {
+      oos = new ObjectOutputStream(out);
+      oos.writeObject(this);
+      oos.flush();
+    } finally {
+      if (oos != null) {
+        oos.close();
+      }
+    }
+  }
+
+  public static CharBlockArray open(InputStream in) throws IOException, ClassNotFoundException {
+    ObjectInputStream ois = null;
+    try {
+      ois = new ObjectInputStream(in);
+      CharBlockArray a = (CharBlockArray) ois.readObject();
+      return a;
+    } finally {
+      if (ois != null) {
+        ois.close();
+      }
+    }
+  }
+
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/Cl2oTaxonomyWriterCache.java b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/Cl2oTaxonomyWriterCache.java
new file mode 100644
index 0000000..abbac2d
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/Cl2oTaxonomyWriterCache.java
@@ -0,0 +1,98 @@
+package org.apache.lucene.facet.taxonomy.writercache;
+
+import java.util.concurrent.locks.ReadWriteLock;
+import java.util.concurrent.locks.ReentrantReadWriteLock;
+
+import org.apache.lucene.facet.taxonomy.FacetLabel;
+import org.apache.lucene.facet.taxonomy.writercache.TaxonomyWriterCache;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * {@link TaxonomyWriterCache} using {@link CompactLabelToOrdinal}. Although
+ * called cache, it maintains in memory all the mappings from category to
+ * ordinal, relying on that {@link CompactLabelToOrdinal} is an efficient
+ * mapping for this purpose.
+ * 
+ * @lucene.experimental
+ */
+public class Cl2oTaxonomyWriterCache implements TaxonomyWriterCache {  
+
+  private final ReadWriteLock lock = new ReentrantReadWriteLock();
+  private final int initialCapcity, numHashArrays;
+  private final float loadFactor;
+  
+  private volatile CompactLabelToOrdinal cache;
+
+  public Cl2oTaxonomyWriterCache(int initialCapcity, float loadFactor, int numHashArrays) {
+    this.cache = new CompactLabelToOrdinal(initialCapcity, loadFactor, numHashArrays);
+    this.initialCapcity = initialCapcity;
+    this.numHashArrays = numHashArrays;
+    this.loadFactor = loadFactor;
+  }
+
+  @Override
+  public void clear() {
+    lock.writeLock().lock();
+    try {
+      cache = new CompactLabelToOrdinal(initialCapcity, loadFactor, numHashArrays);
+    } finally {
+      lock.writeLock().unlock();
+    }
+  }
+  
+  @Override
+  public synchronized void close() {
+    cache = null;
+  }
+
+  @Override
+  public boolean isFull() {
+    // This cache is never full
+    return false;
+  }
+
+  @Override
+  public int get(FacetLabel categoryPath) {
+    lock.readLock().lock();
+    try {
+      return cache.getOrdinal(categoryPath);
+    } finally {
+      lock.readLock().unlock();
+    }
+  }
+
+  @Override
+  public boolean put(FacetLabel categoryPath, int ordinal) {
+    lock.writeLock().lock();
+    try {
+      cache.addLabel(categoryPath, ordinal);
+      // Tell the caller we didn't clear part of the cache, so it doesn't
+      // have to flush its on-disk index now
+      return false;
+    } finally {
+      lock.writeLock().unlock();
+    }
+  }
+
+  /** Returns the number of bytes in memory used by this object. */
+  public int getMemoryUsage() {
+    return cache == null ? 0 : cache.getMemoryUsage();
+  }
+
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/CollisionMap.java b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/CollisionMap.java
new file mode 100644
index 0000000..bcd3b9a
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/CollisionMap.java
@@ -0,0 +1,230 @@
+package org.apache.lucene.facet.taxonomy.writercache;
+
+import java.util.Iterator;
+import java.util.NoSuchElementException;
+
+import org.apache.lucene.facet.taxonomy.FacetLabel;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * HashMap to store colliding labels. See {@link CompactLabelToOrdinal} for
+ * details.
+ * 
+ * @lucene.experimental
+ */
+public class CollisionMap {
+
+  private int capacity;
+  private float loadFactor;
+  private int size;
+  private int threshold;
+
+  static class Entry {
+    int offset;
+    int cid;
+    Entry next;
+    int hash;
+
+    Entry(int offset, int cid, int h, Entry e) {
+      this.offset = offset;
+      this.cid = cid;
+      this.next = e;
+      this.hash = h;
+    }
+  }
+
+  private CharBlockArray labelRepository;
+
+  private Entry[] entries;
+
+  CollisionMap(CharBlockArray labelRepository) {
+    this(16 * 1024, 0.75f, labelRepository);
+  }
+
+  CollisionMap(int initialCapacity, CharBlockArray labelRepository) {
+    this(initialCapacity, 0.75f, labelRepository);
+  }
+
+  private CollisionMap(int initialCapacity, float loadFactor, CharBlockArray labelRepository) {
+    this.labelRepository = labelRepository;
+    this.loadFactor = loadFactor;
+    this.capacity = CompactLabelToOrdinal.determineCapacity(2, initialCapacity);
+
+    this.entries = new Entry[this.capacity];
+    this.threshold = (int) (this.capacity * this.loadFactor);
+  }
+
+  public int size() {
+    return this.size;
+  }
+
+  public int capacity() {
+    return this.capacity;
+  }
+
+  private void grow() {
+    int newCapacity = this.capacity * 2;
+    Entry[] newEntries = new Entry[newCapacity];
+    Entry[] src = this.entries;
+
+    for (int j = 0; j < src.length; j++) {
+      Entry e = src[j];
+      if (e != null) {
+        src[j] = null;
+        do {
+          Entry next = e.next;
+          int hash = e.hash;
+          int i = indexFor(hash, newCapacity);  
+          e.next = newEntries[i];
+          newEntries[i] = e;
+          e = next;
+        } while (e != null);
+      }
+    }
+
+    this.capacity = newCapacity;
+    this.entries = newEntries;
+    this.threshold = (int) (this.capacity * this.loadFactor);
+  }
+
+  public int get(FacetLabel label, int hash) {
+    int bucketIndex = indexFor(hash, this.capacity);
+    Entry e = this.entries[bucketIndex];
+
+    while (e != null && !(hash == e.hash && CategoryPathUtils.equalsToSerialized(label, labelRepository, e.offset))) { 
+      e = e.next;
+    }
+    if (e == null) {
+      return LabelToOrdinal.INVALID_ORDINAL;
+    }
+
+    return e.cid;
+  }
+
+  public int addLabel(FacetLabel label, int hash, int cid) {
+    int bucketIndex = indexFor(hash, this.capacity);
+    for (Entry e = this.entries[bucketIndex]; e != null; e = e.next) {
+      if (e.hash == hash && CategoryPathUtils.equalsToSerialized(label, labelRepository, e.offset)) {
+        return e.cid;
+      }
+    }
+
+    // new string; add to label repository
+    int offset = labelRepository.length();
+    CategoryPathUtils.serialize(label, labelRepository);
+    addEntry(offset, cid, hash, bucketIndex);
+    return cid;
+  }
+
+  /**
+   * This method does not check if the same value is already in the map because
+   * we pass in an char-array offset, so so we now that we're in resize-mode
+   * here.
+   */
+  public void addLabelOffset(int hash, int offset, int cid) {
+    int bucketIndex = indexFor(hash, this.capacity);
+    addEntry(offset, cid, hash, bucketIndex);
+  }
+
+  private void addEntry(int offset, int cid, int hash, int bucketIndex) {
+    Entry e = this.entries[bucketIndex];
+    this.entries[bucketIndex] = new Entry(offset, cid, hash, e);
+    if (this.size++ >= this.threshold) {
+      grow();
+    }
+  }
+
+  Iterator<CollisionMap.Entry> entryIterator() {
+    return new EntryIterator(entries, size);
+  }
+
+  /**
+   * Returns index for hash code h. 
+   */
+  static int indexFor(int h, int length) {
+    return h & (length - 1);
+  }
+
+  /**
+   * Returns an estimate of the memory usage of this CollisionMap.
+   * @return The approximate number of bytes used by this structure.
+   */
+  int getMemoryUsage() {
+    int memoryUsage = 0;
+    if (this.entries != null) {
+      for (Entry e : this.entries) {
+        if (e != null) {
+          memoryUsage += (4 * 4);
+          for (Entry ee = e.next; ee != null; ee = ee.next) {
+            memoryUsage += (4 * 4);
+          }
+        }
+      }
+    }
+    return memoryUsage;
+  }
+
+  private class EntryIterator implements Iterator<Entry> {
+    Entry next;    // next entry to return
+    int index;        // current slot 
+    Entry[] ents;
+    
+    EntryIterator(Entry[] entries, int size) {
+      this.ents = entries;
+      Entry[] t = entries;
+      int i = t.length;
+      Entry n = null;
+      if (size != 0) { // advance to first entry
+        while (i > 0 && (n = t[--i]) == null) {
+          // advance
+        }
+      }
+      this.next = n;
+      this.index = i;
+    }
+
+    @Override
+    public boolean hasNext() {
+      return this.next != null;
+    }
+
+    @Override
+    public Entry next() { 
+      Entry e = this.next;
+      if (e == null) throw new NoSuchElementException();
+
+      Entry n = e.next;
+      Entry[] t = ents;
+      int i = this.index;
+      while (n == null && i > 0) {
+        n = t[--i];
+      }
+      this.index = i;
+      this.next = n;
+      return  e;
+    }
+
+    @Override
+    public void remove() {
+      throw new UnsupportedOperationException();
+    }
+
+  }
+
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/CompactLabelToOrdinal.java b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/CompactLabelToOrdinal.java
new file mode 100644
index 0000000..fed23ed
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/CompactLabelToOrdinal.java
@@ -0,0 +1,465 @@
+package org.apache.lucene.facet.taxonomy.writercache;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.BufferedInputStream;
+import java.io.BufferedOutputStream;
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.File;
+import java.io.FileInputStream;
+import java.io.FileOutputStream;
+import java.io.IOException;
+import java.util.Iterator;
+
+import org.apache.lucene.facet.taxonomy.FacetLabel;
+
+/**
+ * This is a very efficient LabelToOrdinal implementation that uses a
+ * CharBlockArray to store all labels and a configurable number of HashArrays to
+ * reference the labels.
+ * <p>
+ * Since the HashArrays don't handle collisions, a {@link CollisionMap} is used
+ * to store the colliding labels.
+ * <p>
+ * This data structure grows by adding a new HashArray whenever the number of
+ * collisions in the {@link CollisionMap} exceeds {@code loadFactor} * 
+ * {@link #getMaxOrdinal()}. Growing also includes reinserting all colliding
+ * labels into the HashArrays to possibly reduce the number of collisions.
+ * 
+ * For setting the {@code loadFactor} see 
+ * {@link #CompactLabelToOrdinal(int, float, int)}. 
+ * 
+ * <p>
+ * This data structure has a much lower memory footprint (~30%) compared to a
+ * Java HashMap&lt;String, Integer&gt;. It also only uses a small fraction of objects
+ * a HashMap would use, thus limiting the GC overhead. Ingestion speed was also
+ * ~50% faster compared to a HashMap for 3M unique labels.
+ * 
+ * @lucene.experimental
+ */
+public class CompactLabelToOrdinal extends LabelToOrdinal {
+
+  public static final float DefaultLoadFactor = 0.15f;
+
+  static final char TERMINATOR_CHAR = 0xffff;
+  private static final int COLLISION = -5;
+
+  private HashArray[] hashArrays;
+  private CollisionMap collisionMap;
+  private CharBlockArray labelRepository;
+
+  private int capacity;
+  private int threshold;
+  private float loadFactor;
+
+  public int sizeOfMap() {
+    return this.collisionMap.size();
+  }
+
+  private CompactLabelToOrdinal() {
+  }
+
+  public CompactLabelToOrdinal(int initialCapacity, float loadFactor,
+                                int numHashArrays) {
+
+    this.hashArrays = new HashArray[numHashArrays];
+
+    this.capacity = determineCapacity((int) Math.pow(2, numHashArrays),
+        initialCapacity);
+    init();
+    this.collisionMap = new CollisionMap(this.labelRepository);
+
+    this.counter = 0;
+    this.loadFactor = loadFactor;
+
+    this.threshold = (int) (this.loadFactor * this.capacity);
+  }
+
+  static int determineCapacity(int minCapacity, int initialCapacity) {
+    int capacity = minCapacity;
+    while (capacity < initialCapacity) {
+      capacity <<= 1;
+    }
+    return capacity;
+  }
+
+  private void init() {
+    labelRepository = new CharBlockArray();
+    CategoryPathUtils.serialize(FacetLabel.EMPTY, labelRepository);
+
+    int c = this.capacity;
+    for (int i = 0; i < this.hashArrays.length; i++) {
+      this.hashArrays[i] = new HashArray(c);
+      c /= 2;
+    }
+  }
+
+  @Override
+  public void addLabel(FacetLabel label, int ordinal) {
+    if (collisionMap.size() > threshold) {
+      grow();
+    }
+
+    int hash = CompactLabelToOrdinal.stringHashCode(label);
+    for (int i = 0; i < this.hashArrays.length; i++) {
+      if (addLabel(this.hashArrays[i], label, hash, ordinal)) {
+        return;
+      }
+    }
+
+    int prevVal = collisionMap.addLabel(label, hash, ordinal);
+    if (prevVal != ordinal) {
+      throw new IllegalArgumentException("Label already exists: " + label.toString('/') + " prev ordinal " + prevVal);
+    }
+  }
+
+  @Override
+  public int getOrdinal(FacetLabel label) {
+    if (label == null) {
+      return LabelToOrdinal.INVALID_ORDINAL;
+    }
+
+    int hash = CompactLabelToOrdinal.stringHashCode(label);
+    for (int i = 0; i < this.hashArrays.length; i++) {
+      int ord = getOrdinal(this.hashArrays[i], label, hash);
+      if (ord != COLLISION) {
+        return ord;
+      }
+    }
+
+    return this.collisionMap.get(label, hash);
+  }
+
+  private void grow() {
+    HashArray temp = this.hashArrays[this.hashArrays.length - 1];
+
+    for (int i = this.hashArrays.length - 1; i > 0; i--) {
+      this.hashArrays[i] = this.hashArrays[i - 1];
+    }
+
+    this.capacity *= 2;
+    this.hashArrays[0] = new HashArray(this.capacity);
+
+    for (int i = 1; i < this.hashArrays.length; i++) {
+      int[] sourceOffsetArray = this.hashArrays[i].offsets;
+      int[] sourceCidsArray = this.hashArrays[i].cids;
+
+      for (int k = 0; k < sourceOffsetArray.length; k++) {
+
+        for (int j = 0; j < i && sourceOffsetArray[k] != 0; j++) {
+          int[] targetOffsetArray = this.hashArrays[j].offsets;
+          int[] targetCidsArray = this.hashArrays[j].cids;
+
+          int newIndex = indexFor(stringHashCode(
+              this.labelRepository, sourceOffsetArray[k]),
+              targetOffsetArray.length);
+          if (targetOffsetArray[newIndex] == 0) {
+            targetOffsetArray[newIndex] = sourceOffsetArray[k];
+            targetCidsArray[newIndex] = sourceCidsArray[k];
+            sourceOffsetArray[k] = 0;
+          }
+        }
+      }
+    }
+
+    for (int i = 0; i < temp.offsets.length; i++) {
+      int offset = temp.offsets[i];
+      if (offset > 0) {
+        int hash = stringHashCode(this.labelRepository, offset);
+        addLabelOffset(hash, temp.cids[i], offset);
+      }
+    }
+
+    CollisionMap oldCollisionMap = this.collisionMap;
+    this.collisionMap = new CollisionMap(oldCollisionMap.capacity(),
+        this.labelRepository);
+    this.threshold = (int) (this.capacity * this.loadFactor);
+
+    Iterator<CollisionMap.Entry> it = oldCollisionMap.entryIterator();
+    while (it.hasNext()) {
+      CollisionMap.Entry e = it.next();
+      addLabelOffset(stringHashCode(this.labelRepository, e.offset),
+          e.cid, e.offset);
+    }
+  }
+
+  private boolean addLabel(HashArray a, FacetLabel label, int hash, int ordinal) {
+    int index = CompactLabelToOrdinal.indexFor(hash, a.offsets.length);
+    int offset = a.offsets[index];
+
+    if (offset == 0) {
+      a.offsets[index] = this.labelRepository.length();
+      CategoryPathUtils.serialize(label, labelRepository);
+      a.cids[index] = ordinal;
+      return true;
+    }
+
+    return false;
+  }
+
+  private void addLabelOffset(int hash, int cid, int knownOffset) {
+    for (int i = 0; i < this.hashArrays.length; i++) {
+      if (addLabelOffsetToHashArray(this.hashArrays[i], hash, cid,
+          knownOffset)) {
+        return;
+      }
+    }
+
+    this.collisionMap.addLabelOffset(hash, knownOffset, cid);
+
+    if (this.collisionMap.size() > this.threshold) {
+      grow();
+    }
+  }
+
+  private boolean addLabelOffsetToHashArray(HashArray a, int hash, int ordinal,
+                                            int knownOffset) {
+
+    int index = CompactLabelToOrdinal.indexFor(hash, a.offsets.length);
+    int offset = a.offsets[index];
+
+    if (offset == 0) {
+      a.offsets[index] = knownOffset;
+      a.cids[index] = ordinal;
+      return true;
+    }
+
+    return false;
+  }
+
+  private int getOrdinal(HashArray a, FacetLabel label, int hash) {
+    if (label == null) {
+      return LabelToOrdinal.INVALID_ORDINAL;
+    }
+
+    int index = indexFor(hash, a.offsets.length);
+    int offset = a.offsets[index];
+    if (offset == 0) {
+      return LabelToOrdinal.INVALID_ORDINAL;
+    }
+
+    if (CategoryPathUtils.equalsToSerialized(label, labelRepository, offset)) {
+      return a.cids[index];
+    }
+
+    return COLLISION;
+  }
+
+  /** Returns index for hash code h. */
+  static int indexFor(int h, int length) {
+    return h & (length - 1);
+  }
+
+  // static int stringHashCode(String label) {
+  // int len = label.length();
+  // int hash = 0;
+  // int i;
+  // for (i = 0; i < len; ++i)
+  // hash = 33 * hash + label.charAt(i);
+  //
+  // hash = hash ^ ((hash >>> 20) ^ (hash >>> 12));
+  // hash = hash ^ (hash >>> 7) ^ (hash >>> 4);
+  //
+  // return hash;
+  //
+  // }
+
+  static int stringHashCode(FacetLabel label) {
+    int hash = label.hashCode();
+
+    hash = hash ^ ((hash >>> 20) ^ (hash >>> 12));
+    hash = hash ^ (hash >>> 7) ^ (hash >>> 4);
+
+    return hash;
+
+  }
+
+  static int stringHashCode(CharBlockArray labelRepository, int offset) {
+    int hash = CategoryPathUtils.hashCodeOfSerialized(labelRepository, offset);
+    hash = hash ^ ((hash >>> 20) ^ (hash >>> 12));
+    hash = hash ^ (hash >>> 7) ^ (hash >>> 4);
+    return hash;
+  }
+
+  // public static boolean equals(CharSequence label, CharBlockArray array,
+  // int offset) {
+  // // CONTINUE HERE
+  // int len = label.length();
+  // int bi = array.blockIndex(offset);
+  // CharBlockArray.Block b = array.blocks.get(bi);
+  // int index = array.indexInBlock(offset);
+  //
+  // for (int i = 0; i < len; i++) {
+  // if (label.charAt(i) != b.chars[index]) {
+  // return false;
+  // }
+  // index++;
+  // if (index == b.length) {
+  // b = array.blocks.get(++bi);
+  // index = 0;
+  // }
+  // }
+  //
+  // return b.chars[index] == TerminatorChar;
+  // }
+
+  /**
+   * Returns an estimate of the amount of memory used by this table. Called only in
+   * this package. Memory is consumed mainly by three structures: the hash arrays,
+   * label repository and collision map.
+   */
+  int getMemoryUsage() {
+    int memoryUsage = 0;
+    if (this.hashArrays != null) {
+      // HashArray capacity is instance-specific.
+      for (HashArray ha : this.hashArrays) {
+        // Each has 2 capacity-length arrays of ints.
+        memoryUsage += ( ha.capacity * 2 * 4 ) + 4;
+      }
+    }
+    if (this.labelRepository != null) {
+      // All blocks are the same size.
+      int blockSize = this.labelRepository.blockSize;
+      // Each block has room for blockSize UTF-16 chars.
+      int actualBlockSize = ( blockSize * 2 ) + 4;
+      memoryUsage += this.labelRepository.blocks.size() * actualBlockSize; 
+      memoryUsage += 8;   // Two int values for array as a whole.
+    }
+    if (this.collisionMap != null) {
+      memoryUsage += this.collisionMap.getMemoryUsage();
+    }
+    return memoryUsage;
+  }
+
+  /**
+   * Opens the file and reloads the CompactLabelToOrdinal. The file it expects
+   * is generated from the {@link #flush(File)} command.
+   */
+  static CompactLabelToOrdinal open(File file, float loadFactor,
+                                    int numHashArrays) throws IOException {
+    /**
+     * Part of the file is the labelRepository, which needs to be rehashed
+     * and label offsets re-added to the object. I am unsure as to why we
+     * can't just store these off in the file as well, but in keeping with
+     * the spirit of the original code, I did it this way. (ssuppe)
+     */
+    CompactLabelToOrdinal l2o = new CompactLabelToOrdinal();
+    l2o.loadFactor = loadFactor;
+    l2o.hashArrays = new HashArray[numHashArrays];
+
+    DataInputStream dis = null;
+    try {
+      dis = new DataInputStream(new BufferedInputStream(
+          new FileInputStream(file)));
+
+      // TaxiReader needs to load the "counter" or occupancy (L2O) to know
+      // the next unique facet. we used to load the delimiter too, but
+      // never used it.
+      l2o.counter = dis.readInt();
+
+      l2o.capacity = determineCapacity((int) Math.pow(2,
+          l2o.hashArrays.length), l2o.counter);
+      l2o.init();
+
+      // now read the chars
+      l2o.labelRepository = CharBlockArray.open(dis);
+
+      l2o.collisionMap = new CollisionMap(l2o.labelRepository);
+
+      // Calculate hash on the fly based on how CategoryPath hashes
+      // itself. Maybe in the future we can call some static based methods
+      // in CategoryPath so that this doesn't break again? I don't like
+      // having code in two different places...
+      int cid = 0;
+      // Skip the initial offset, it's the CategoryPath(0,0), which isn't
+      // a hashed value.
+      int offset = 1;
+      int lastStartOffset = offset;
+      // This loop really relies on a well-formed input (assumes pretty blindly
+      // that array offsets will work).  Since the initial file is machine 
+      // generated, I think this should be OK.
+      while (offset < l2o.labelRepository.length()) {
+        // identical code to CategoryPath.hashFromSerialized. since we need to
+        // advance offset, we cannot call the method directly. perhaps if we
+        // could pass a mutable Integer or something...
+        int length = (short) l2o.labelRepository.charAt(offset++);
+        int hash = length;
+        if (length != 0) {
+          for (int i = 0; i < length; i++) {
+            int len = (short) l2o.labelRepository.charAt(offset++);
+            hash = hash * 31 + l2o.labelRepository.subSequence(offset, offset + len).hashCode();
+            offset += len;
+          }
+        }
+        // Now that we've hashed the components of the label, do the
+        // final part of the hash algorithm.
+        hash = hash ^ ((hash >>> 20) ^ (hash >>> 12));
+        hash = hash ^ (hash >>> 7) ^ (hash >>> 4);
+        // Add the label, and let's keep going
+        l2o.addLabelOffset(hash, cid, lastStartOffset);
+        cid++;
+        lastStartOffset = offset;
+      }
+
+    } catch (ClassNotFoundException cnfe) {
+      throw new IOException("Invalid file format. Cannot deserialize.");
+    } finally {
+      if (dis != null) {
+        dis.close();
+      }
+    }
+
+    l2o.threshold = (int) (l2o.loadFactor * l2o.capacity);
+    return l2o;
+
+  }
+
+  void flush(File file) throws IOException {
+    FileOutputStream fos = new FileOutputStream(file);
+
+    try {
+      BufferedOutputStream os = new BufferedOutputStream(fos);
+
+      DataOutputStream dos = new DataOutputStream(os);
+      dos.writeInt(this.counter);
+
+      // write the labelRepository
+      this.labelRepository.flush(dos);
+
+      // Closes the data output stream
+      dos.close();
+
+    } finally {
+      fos.close();
+    }
+  }
+
+  private static final class HashArray {
+    int[] offsets;
+    int[] cids;
+
+    int capacity;
+
+    HashArray(int c) {
+      this.capacity = c;
+      this.offsets = new int[this.capacity];
+      this.cids = new int[this.capacity];
+    }
+  }
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/LabelToOrdinal.java b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/LabelToOrdinal.java
new file mode 100644
index 0000000..33b60fe
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/LabelToOrdinal.java
@@ -0,0 +1,60 @@
+package org.apache.lucene.facet.taxonomy.writercache;
+
+import org.apache.lucene.facet.taxonomy.FacetLabel;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Abstract class for storing Label->Ordinal mappings in a taxonomy. 
+ * 
+ * @lucene.experimental
+ */
+public abstract class LabelToOrdinal {
+
+  protected int counter;
+  public static final int INVALID_ORDINAL = -2;
+
+  /**
+   * return the maximal Ordinal assigned so far
+   */
+  public int getMaxOrdinal() {
+    return this.counter;
+  }
+
+  /**
+   * Returns the next unassigned ordinal. The default behavior of this method
+   * is to simply increment a counter.
+   */
+  public int getNextOrdinal() {
+    return this.counter++;
+  }
+
+  /**
+   * Adds a new label if its not yet in the table.
+   * Throws an {@link IllegalArgumentException} if the same label with
+   * a different ordinal was previoulsy added to this table.
+   */
+  public abstract void addLabel(FacetLabel label, int ordinal);
+
+  /**
+   * @return the ordinal assigned to the given label, 
+   * or {@link #INVALID_ORDINAL} if the label cannot be found in this table.
+   */
+  public abstract int getOrdinal(FacetLabel label);
+
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/LruTaxonomyWriterCache.java b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/LruTaxonomyWriterCache.java
new file mode 100644
index 0000000..7bdbb2c
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/LruTaxonomyWriterCache.java
@@ -0,0 +1,105 @@
+ package org.apache.lucene.facet.taxonomy.writercache;
+
+import org.apache.lucene.facet.taxonomy.FacetLabel;
+import org.apache.lucene.facet.taxonomy.writercache.TaxonomyWriterCache;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * LRU {@link TaxonomyWriterCache} - good choice for huge taxonomies.
+ * 
+ * @lucene.experimental
+ */
+public class LruTaxonomyWriterCache implements TaxonomyWriterCache {
+
+  /**
+   * Determines cache type.
+   * For guaranteed correctness - not relying on no-collisions in the hash
+   * function, LRU_STRING should be used.
+   */
+  public enum LRUType { LRU_HASHED, LRU_STRING }
+
+  private NameIntCacheLRU cache;
+
+  public LruTaxonomyWriterCache(int cacheSize) {
+    // TODO (Facet): choose between NameHashIntCacheLRU and NameIntCacheLRU.
+    // For guaranteed correctness - not relying on no-collisions in the hash
+    // function, NameIntCacheLRU should be used:
+    // On the other hand, NameHashIntCacheLRU takes less RAM but if there
+    // are collisions (which we never found) two different paths would be
+    // mapped to the same ordinal...
+    this(cacheSize, LRUType.LRU_HASHED);
+  }
+
+  public LruTaxonomyWriterCache(int cacheSize, LRUType lruType) {
+    // TODO (Facet): choose between NameHashIntCacheLRU and NameIntCacheLRU.
+    // For guaranteed correctness - not relying on no-collisions in the hash
+    // function, NameIntCacheLRU should be used:
+    // On the other hand, NameHashIntCacheLRU takes less RAM but if there
+    // are collisions (which we never found) two different paths would be
+    // mapped to the same ordinal...
+    if (lruType == LRUType.LRU_HASHED) {
+      this.cache = new NameHashIntCacheLRU(cacheSize);
+    } else {
+      this.cache = new NameIntCacheLRU(cacheSize);
+    }
+  }
+
+  @Override
+  public synchronized boolean isFull() {
+    return cache.getSize() == cache.getMaxSize();
+  }
+
+  @Override
+  public synchronized void clear() {
+    cache.clear();
+  }
+  
+  @Override
+  public synchronized void close() {
+    cache.clear();
+    cache = null;
+  }
+
+  @Override
+  public synchronized int get(FacetLabel categoryPath) {
+    Integer res = cache.get(categoryPath);
+    if (res == null) {
+      return -1;
+    }
+
+    return res.intValue();
+  }
+
+  @Override
+  public synchronized boolean put(FacetLabel categoryPath, int ordinal) {
+    boolean ret = cache.put(categoryPath, new Integer(ordinal));
+    // If the cache is full, we need to clear one or more old entries
+    // from the cache. However, if we delete from the cache a recent
+    // addition that isn't yet in our reader, for this entry to be
+    // visible to us we need to make sure that the changes have been
+    // committed and we reopen the reader. Because this is a slow
+    // operation, we don't delete entries one-by-one but rather in bulk
+    // (put() removes the 2/3rd oldest entries).
+    if (ret) {
+      cache.makeRoomLRU();
+    }
+    return ret;
+  }
+
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/NameHashIntCacheLRU.java b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/NameHashIntCacheLRU.java
new file mode 100644
index 0000000..9743d4b
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/NameHashIntCacheLRU.java
@@ -0,0 +1,47 @@
+package org.apache.lucene.facet.taxonomy.writercache;
+
+import org.apache.lucene.facet.taxonomy.FacetLabel;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * An an LRU cache of mapping from name to int.
+ * Used to cache Ordinals of category paths.
+ * It uses as key, hash of the path instead of the path.
+ * This way the cache takes less RAM, but correctness depends on
+ * assuming no collisions. 
+ * 
+ * @lucene.experimental
+ */
+public class NameHashIntCacheLRU extends NameIntCacheLRU {
+
+  NameHashIntCacheLRU(int maxCacheSize) {
+    super(maxCacheSize);
+  }
+
+  @Override
+  Object key(FacetLabel name) {
+    return new Long(name.longHashCode());
+  }
+
+  @Override
+  Object key(FacetLabel name, int prefixLen) {
+    return new Long(name.subpath(prefixLen).longHashCode());
+  }
+  
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/NameIntCacheLRU.java b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/NameIntCacheLRU.java
new file mode 100644
index 0000000..f43b6b1
--- /dev/null
+++ b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/NameIntCacheLRU.java
@@ -0,0 +1,131 @@
+package org.apache.lucene.facet.taxonomy.writercache;
+
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.LinkedHashMap;
+import org.apache.lucene.facet.taxonomy.FacetLabel;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * An an LRU cache of mapping from name to int.
+ * Used to cache Ordinals of category paths.
+ * 
+ * @lucene.experimental
+ */
+// Note: Nothing in this class is synchronized. The caller is assumed to be
+// synchronized so that no two methods of this class are called concurrently.
+class NameIntCacheLRU {
+
+  private HashMap<Object, Integer> cache;
+  long nMisses = 0; // for debug
+  long nHits = 0;  // for debug
+  private int maxCacheSize;
+
+  NameIntCacheLRU(int maxCacheSize) {
+    this.maxCacheSize = maxCacheSize;
+    createCache(maxCacheSize);
+  }
+
+  public int getMaxSize() {
+    return maxCacheSize;
+  }
+  
+  public int getSize() {
+    return cache.size();
+  }
+
+  private void createCache (int maxSize) {
+    if (maxSize<Integer.MAX_VALUE) {
+      cache = new LinkedHashMap<Object, Integer>(1000,(float)0.7,true); //for LRU
+    } else {
+      cache = new HashMap<Object, Integer>(1000,(float)0.7); //no need for LRU
+    }
+  }
+
+  Integer get (FacetLabel name) {
+    Integer res = cache.get(key(name));
+    if (res==null) {
+      nMisses ++;
+    } else {
+      nHits ++;
+    }
+    return res;
+  }
+
+  /** Subclasses can override this to provide caching by e.g. hash of the string. */
+  Object key(FacetLabel name) {
+    return name;
+  }
+
+  Object key(FacetLabel name, int prefixLen) {
+    return name.subpath(prefixLen);
+  }
+
+  /**
+   * Add a new value to cache.
+   * Return true if cache became full and some room need to be made. 
+   */
+  boolean put (FacetLabel name, Integer val) {
+    cache.put(key(name), val);
+    return isCacheFull();
+  }
+
+  boolean put (FacetLabel name, int prefixLen, Integer val) {
+    cache.put(key(name, prefixLen), val);
+    return isCacheFull();
+  }
+
+  private boolean isCacheFull() {
+    return cache.size() > maxCacheSize;
+  }
+
+  void clear() {
+    cache.clear();
+  }
+
+  String stats() {
+    return "#miss="+nMisses+" #hit="+nHits;
+  }
+  
+  /**
+   * If cache is full remove least recently used entries from cache. Return true
+   * if anything was removed, false otherwise.
+   * 
+   * See comment in DirectoryTaxonomyWriter.addToCache(CategoryPath, int) for an
+   * explanation why we clean 2/3rds of the cache, and not just one entry.
+   */ 
+  boolean makeRoomLRU() {
+    if (!isCacheFull()) {
+      return false;
+    }
+    int n = cache.size() - (2*maxCacheSize)/3;
+    if (n<=0) {
+      return false;
+    }
+    Iterator<Object> it = cache.keySet().iterator();
+    int i = 0;
+    while (i<n && it.hasNext()) {
+      it.next();
+      it.remove();
+      i++;
+    }
+    return true;
+  }
+
+}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/cl2o/CategoryPathUtils.java b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/cl2o/CategoryPathUtils.java
deleted file mode 100644
index b9167b0..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/cl2o/CategoryPathUtils.java
+++ /dev/null
@@ -1,82 +0,0 @@
-package org.apache.lucene.facet.taxonomy.writercache.cl2o;
-
-import org.apache.lucene.facet.taxonomy.FacetLabel;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/** Utilities for use of {@link FacetLabel} by {@link CompactLabelToOrdinal}. */
-class CategoryPathUtils {
-  
-  /** Serializes the given {@link FacetLabel} to the {@link CharBlockArray}. */
-  public static void serialize(FacetLabel cp, CharBlockArray charBlockArray) {
-    charBlockArray.append((char) cp.length);
-    if (cp.length == 0) {
-      return;
-    }
-    for (int i = 0; i < cp.length; i++) {
-      charBlockArray.append((char) cp.components[i].length());
-      charBlockArray.append(cp.components[i]);
-    }
-  }
-
-  /**
-   * Calculates a hash function of a path that was serialized with
-   * {@link #serialize(FacetLabel, CharBlockArray)}.
-   */
-  public static int hashCodeOfSerialized(CharBlockArray charBlockArray, int offset) {
-    int length = charBlockArray.charAt(offset++);
-    if (length == 0) {
-      return 0;
-    }
-    
-    int hash = length;
-    for (int i = 0; i < length; i++) {
-      int len = charBlockArray.charAt(offset++);
-      hash = hash * 31 + charBlockArray.subSequence(offset, offset + len).hashCode();
-      offset += len;
-    }
-    return hash;
-  }
-
-  /**
-   * Check whether the {@link FacetLabel} is equal to the one serialized in
-   * {@link CharBlockArray}.
-   */
-  public static boolean equalsToSerialized(FacetLabel cp, CharBlockArray charBlockArray, int offset) {
-    int n = charBlockArray.charAt(offset++);
-    if (cp.length != n) {
-      return false;
-    }
-    if (cp.length == 0) {
-      return true;
-    }
-    
-    for (int i = 0; i < cp.length; i++) {
-      int len = charBlockArray.charAt(offset++);
-      if (len != cp.components[i].length()) {
-        return false;
-      }
-      if (!cp.components[i].equals(charBlockArray.subSequence(offset, offset + len))) {
-        return false;
-      }
-      offset += len;
-    }
-    return true;
-  }
-
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/cl2o/CharBlockArray.java b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/cl2o/CharBlockArray.java
deleted file mode 100644
index e0707de..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/cl2o/CharBlockArray.java
+++ /dev/null
@@ -1,212 +0,0 @@
-package org.apache.lucene.facet.taxonomy.writercache.cl2o;
-
-import java.io.IOException;
-import java.io.InputStream;
-import java.io.ObjectInputStream;
-import java.io.ObjectOutputStream;
-import java.io.OutputStream;
-import java.io.Serializable;
-import java.util.ArrayList;
-import java.util.List;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Similar to {@link StringBuilder}, but with a more efficient growing strategy.
- * This class uses char array blocks to grow.
- * 
- * @lucene.experimental
- */
-class CharBlockArray implements Appendable, Serializable, CharSequence {
-
-  private static final long serialVersionUID = 1L;
-
-  private final static int DefaultBlockSize = 32 * 1024;  // 32 KB default size
-
-  final static class Block implements Serializable, Cloneable {
-    private static final long serialVersionUID = 1L;
-
-    final char[] chars;
-    int length;
-
-    Block(int size) {
-      this.chars = new char[size];
-      this.length = 0;
-    }
-  }
-
-  List<Block> blocks;
-  Block current;
-  int blockSize;
-  int length;
-
-  CharBlockArray() {
-    this(DefaultBlockSize);
-  }
-
-  CharBlockArray(int blockSize) {
-    this.blocks = new ArrayList<Block>();
-    this.blockSize = blockSize;
-    addBlock();
-  }
-
-  private void addBlock() {
-    this.current = new Block(this.blockSize);
-    this.blocks.add(this.current);
-  }
-
-  int blockIndex(int index) {
-    return index / blockSize;
-  }
-
-  int indexInBlock(int index) {
-    return index % blockSize;
-  }
-
-  @Override
-  public CharBlockArray append(CharSequence chars) {
-    return append(chars, 0, chars.length());
-  }
-
-  @Override
-  public CharBlockArray append(char c) {
-    if (this.current.length == this.blockSize) {
-      addBlock();
-    }
-    this.current.chars[this.current.length++] = c;
-    this.length++;
-
-    return this;
-  }
-
-  @Override
-  public CharBlockArray append(CharSequence chars, int start, int length) {
-    int end = start + length;
-    for (int i = start; i < end; i++) {
-      append(chars.charAt(i));
-    }
-    return this;
-  }
-
-  public CharBlockArray append(char[] chars, int start, int length) {
-    int offset = start;
-    int remain = length;
-    while (remain > 0) {
-      if (this.current.length == this.blockSize) {
-        addBlock();
-      }
-      int toCopy = remain;
-      int remainingInBlock = this.blockSize - this.current.length;
-      if (remainingInBlock < toCopy) {
-        toCopy = remainingInBlock;
-      }
-      System.arraycopy(chars, offset, this.current.chars, this.current.length, toCopy);
-      offset += toCopy;
-      remain -= toCopy;
-      this.current.length += toCopy;
-    }
-
-    this.length += length;
-    return this;
-  }
-
-  public CharBlockArray append(String s) {
-    int remain = s.length();
-    int offset = 0;
-    while (remain > 0) {
-      if (this.current.length == this.blockSize) {
-        addBlock();
-      }
-      int toCopy = remain;
-      int remainingInBlock = this.blockSize - this.current.length;
-      if (remainingInBlock < toCopy) {
-        toCopy = remainingInBlock;
-      }
-      s.getChars(offset, offset + toCopy, this.current.chars, this.current.length);
-      offset += toCopy;
-      remain -= toCopy;
-      this.current.length += toCopy;
-    }
-
-    this.length += s.length();
-    return this;
-  }
-
-  @Override
-  public char charAt(int index) {
-    Block b = blocks.get(blockIndex(index));
-    return b.chars[indexInBlock(index)];
-  }
-
-  @Override
-  public int length() {
-    return this.length;
-  }
-
-  @Override
-  public CharSequence subSequence(int start, int end) {
-    int remaining = end - start;
-    StringBuilder sb = new StringBuilder(remaining);
-    int blockIdx = blockIndex(start);
-    int indexInBlock = indexInBlock(start);
-    while (remaining > 0) {
-      Block b = blocks.get(blockIdx++);
-      int numToAppend = Math.min(remaining, b.length - indexInBlock);
-      sb.append(b.chars, indexInBlock, numToAppend);
-      remaining -= numToAppend;
-      indexInBlock = 0; // 2nd+ iterations read from start of the block 
-    }
-    return sb.toString();
-  }
-
-  @Override
-  public String toString() {
-    StringBuilder sb = new StringBuilder();
-    for (Block b : blocks) {
-      sb.append(b.chars, 0, b.length);
-    }
-    return sb.toString();
-  }
-
-  void flush(OutputStream out) throws IOException {
-    ObjectOutputStream oos = null;
-    try {
-      oos = new ObjectOutputStream(out);
-      oos.writeObject(this);
-      oos.flush();
-    } finally {
-      if (oos != null) {
-        oos.close();
-      }
-    }
-  }
-
-  public static CharBlockArray open(InputStream in) throws IOException, ClassNotFoundException {
-    ObjectInputStream ois = null;
-    try {
-      ois = new ObjectInputStream(in);
-      CharBlockArray a = (CharBlockArray) ois.readObject();
-      return a;
-    } finally {
-      if (ois != null) {
-        ois.close();
-      }
-    }
-  }
-
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/cl2o/Cl2oTaxonomyWriterCache.java b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/cl2o/Cl2oTaxonomyWriterCache.java
deleted file mode 100644
index 7421338..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/cl2o/Cl2oTaxonomyWriterCache.java
+++ /dev/null
@@ -1,98 +0,0 @@
-package org.apache.lucene.facet.taxonomy.writercache.cl2o;
-
-import java.util.concurrent.locks.ReadWriteLock;
-import java.util.concurrent.locks.ReentrantReadWriteLock;
-
-import org.apache.lucene.facet.taxonomy.FacetLabel;
-import org.apache.lucene.facet.taxonomy.writercache.TaxonomyWriterCache;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * {@link TaxonomyWriterCache} using {@link CompactLabelToOrdinal}. Although
- * called cache, it maintains in memory all the mappings from category to
- * ordinal, relying on that {@link CompactLabelToOrdinal} is an efficient
- * mapping for this purpose.
- * 
- * @lucene.experimental
- */
-public class Cl2oTaxonomyWriterCache implements TaxonomyWriterCache {  
-
-  private final ReadWriteLock lock = new ReentrantReadWriteLock();
-  private final int initialCapcity, numHashArrays;
-  private final float loadFactor;
-  
-  private volatile CompactLabelToOrdinal cache;
-
-  public Cl2oTaxonomyWriterCache(int initialCapcity, float loadFactor, int numHashArrays) {
-    this.cache = new CompactLabelToOrdinal(initialCapcity, loadFactor, numHashArrays);
-    this.initialCapcity = initialCapcity;
-    this.numHashArrays = numHashArrays;
-    this.loadFactor = loadFactor;
-  }
-
-  @Override
-  public void clear() {
-    lock.writeLock().lock();
-    try {
-      cache = new CompactLabelToOrdinal(initialCapcity, loadFactor, numHashArrays);
-    } finally {
-      lock.writeLock().unlock();
-    }
-  }
-  
-  @Override
-  public synchronized void close() {
-    cache = null;
-  }
-
-  @Override
-  public boolean isFull() {
-    // This cache is never full
-    return false;
-  }
-
-  @Override
-  public int get(FacetLabel categoryPath) {
-    lock.readLock().lock();
-    try {
-      return cache.getOrdinal(categoryPath);
-    } finally {
-      lock.readLock().unlock();
-    }
-  }
-
-  @Override
-  public boolean put(FacetLabel categoryPath, int ordinal) {
-    lock.writeLock().lock();
-    try {
-      cache.addLabel(categoryPath, ordinal);
-      // Tell the caller we didn't clear part of the cache, so it doesn't
-      // have to flush its on-disk index now
-      return false;
-    } finally {
-      lock.writeLock().unlock();
-    }
-  }
-
-  /** Returns the number of bytes in memory used by this object. */
-  public int getMemoryUsage() {
-    return cache == null ? 0 : cache.getMemoryUsage();
-  }
-
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/cl2o/CollisionMap.java b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/cl2o/CollisionMap.java
deleted file mode 100644
index f59ed2e..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/cl2o/CollisionMap.java
+++ /dev/null
@@ -1,230 +0,0 @@
-package org.apache.lucene.facet.taxonomy.writercache.cl2o;
-
-import java.util.Iterator;
-import java.util.NoSuchElementException;
-
-import org.apache.lucene.facet.taxonomy.FacetLabel;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * HashMap to store colliding labels. See {@link CompactLabelToOrdinal} for
- * details.
- * 
- * @lucene.experimental
- */
-public class CollisionMap {
-
-  private int capacity;
-  private float loadFactor;
-  private int size;
-  private int threshold;
-
-  static class Entry {
-    int offset;
-    int cid;
-    Entry next;
-    int hash;
-
-    Entry(int offset, int cid, int h, Entry e) {
-      this.offset = offset;
-      this.cid = cid;
-      this.next = e;
-      this.hash = h;
-    }
-  }
-
-  private CharBlockArray labelRepository;
-
-  private Entry[] entries;
-
-  CollisionMap(CharBlockArray labelRepository) {
-    this(16 * 1024, 0.75f, labelRepository);
-  }
-
-  CollisionMap(int initialCapacity, CharBlockArray labelRepository) {
-    this(initialCapacity, 0.75f, labelRepository);
-  }
-
-  private CollisionMap(int initialCapacity, float loadFactor, CharBlockArray labelRepository) {
-    this.labelRepository = labelRepository;
-    this.loadFactor = loadFactor;
-    this.capacity = CompactLabelToOrdinal.determineCapacity(2, initialCapacity);
-
-    this.entries = new Entry[this.capacity];
-    this.threshold = (int) (this.capacity * this.loadFactor);
-  }
-
-  public int size() {
-    return this.size;
-  }
-
-  public int capacity() {
-    return this.capacity;
-  }
-
-  private void grow() {
-    int newCapacity = this.capacity * 2;
-    Entry[] newEntries = new Entry[newCapacity];
-    Entry[] src = this.entries;
-
-    for (int j = 0; j < src.length; j++) {
-      Entry e = src[j];
-      if (e != null) {
-        src[j] = null;
-        do {
-          Entry next = e.next;
-          int hash = e.hash;
-          int i = indexFor(hash, newCapacity);  
-          e.next = newEntries[i];
-          newEntries[i] = e;
-          e = next;
-        } while (e != null);
-      }
-    }
-
-    this.capacity = newCapacity;
-    this.entries = newEntries;
-    this.threshold = (int) (this.capacity * this.loadFactor);
-  }
-
-  public int get(FacetLabel label, int hash) {
-    int bucketIndex = indexFor(hash, this.capacity);
-    Entry e = this.entries[bucketIndex];
-
-    while (e != null && !(hash == e.hash && CategoryPathUtils.equalsToSerialized(label, labelRepository, e.offset))) { 
-      e = e.next;
-    }
-    if (e == null) {
-      return LabelToOrdinal.INVALID_ORDINAL;
-    }
-
-    return e.cid;
-  }
-
-  public int addLabel(FacetLabel label, int hash, int cid) {
-    int bucketIndex = indexFor(hash, this.capacity);
-    for (Entry e = this.entries[bucketIndex]; e != null; e = e.next) {
-      if (e.hash == hash && CategoryPathUtils.equalsToSerialized(label, labelRepository, e.offset)) {
-        return e.cid;
-      }
-    }
-
-    // new string; add to label repository
-    int offset = labelRepository.length();
-    CategoryPathUtils.serialize(label, labelRepository);
-    addEntry(offset, cid, hash, bucketIndex);
-    return cid;
-  }
-
-  /**
-   * This method does not check if the same value is already in the map because
-   * we pass in an char-array offset, so so we now that we're in resize-mode
-   * here.
-   */
-  public void addLabelOffset(int hash, int offset, int cid) {
-    int bucketIndex = indexFor(hash, this.capacity);
-    addEntry(offset, cid, hash, bucketIndex);
-  }
-
-  private void addEntry(int offset, int cid, int hash, int bucketIndex) {
-    Entry e = this.entries[bucketIndex];
-    this.entries[bucketIndex] = new Entry(offset, cid, hash, e);
-    if (this.size++ >= this.threshold) {
-      grow();
-    }
-  }
-
-  Iterator<CollisionMap.Entry> entryIterator() {
-    return new EntryIterator(entries, size);
-  }
-
-  /**
-   * Returns index for hash code h. 
-   */
-  static int indexFor(int h, int length) {
-    return h & (length - 1);
-  }
-
-  /**
-   * Returns an estimate of the memory usage of this CollisionMap.
-   * @return The approximate number of bytes used by this structure.
-   */
-  int getMemoryUsage() {
-    int memoryUsage = 0;
-    if (this.entries != null) {
-      for (Entry e : this.entries) {
-        if (e != null) {
-          memoryUsage += (4 * 4);
-          for (Entry ee = e.next; ee != null; ee = ee.next) {
-            memoryUsage += (4 * 4);
-          }
-        }
-      }
-    }
-    return memoryUsage;
-  }
-
-  private class EntryIterator implements Iterator<Entry> {
-    Entry next;    // next entry to return
-    int index;        // current slot 
-    Entry[] ents;
-    
-    EntryIterator(Entry[] entries, int size) {
-      this.ents = entries;
-      Entry[] t = entries;
-      int i = t.length;
-      Entry n = null;
-      if (size != 0) { // advance to first entry
-        while (i > 0 && (n = t[--i]) == null) {
-          // advance
-        }
-      }
-      this.next = n;
-      this.index = i;
-    }
-
-    @Override
-    public boolean hasNext() {
-      return this.next != null;
-    }
-
-    @Override
-    public Entry next() { 
-      Entry e = this.next;
-      if (e == null) throw new NoSuchElementException();
-
-      Entry n = e.next;
-      Entry[] t = ents;
-      int i = this.index;
-      while (n == null && i > 0) {
-        n = t[--i];
-      }
-      this.index = i;
-      this.next = n;
-      return  e;
-    }
-
-    @Override
-    public void remove() {
-      throw new UnsupportedOperationException();
-    }
-
-  }
-
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/cl2o/CompactLabelToOrdinal.java b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/cl2o/CompactLabelToOrdinal.java
deleted file mode 100644
index da2883d..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/cl2o/CompactLabelToOrdinal.java
+++ /dev/null
@@ -1,465 +0,0 @@
-package org.apache.lucene.facet.taxonomy.writercache.cl2o;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.BufferedInputStream;
-import java.io.BufferedOutputStream;
-import java.io.DataInputStream;
-import java.io.DataOutputStream;
-import java.io.File;
-import java.io.FileInputStream;
-import java.io.FileOutputStream;
-import java.io.IOException;
-import java.util.Iterator;
-
-import org.apache.lucene.facet.taxonomy.FacetLabel;
-
-/**
- * This is a very efficient LabelToOrdinal implementation that uses a
- * CharBlockArray to store all labels and a configurable number of HashArrays to
- * reference the labels.
- * <p>
- * Since the HashArrays don't handle collisions, a {@link CollisionMap} is used
- * to store the colliding labels.
- * <p>
- * This data structure grows by adding a new HashArray whenever the number of
- * collisions in the {@link CollisionMap} exceeds {@code loadFactor} * 
- * {@link #getMaxOrdinal()}. Growing also includes reinserting all colliding
- * labels into the HashArrays to possibly reduce the number of collisions.
- * 
- * For setting the {@code loadFactor} see 
- * {@link #CompactLabelToOrdinal(int, float, int)}. 
- * 
- * <p>
- * This data structure has a much lower memory footprint (~30%) compared to a
- * Java HashMap&lt;String, Integer&gt;. It also only uses a small fraction of objects
- * a HashMap would use, thus limiting the GC overhead. Ingestion speed was also
- * ~50% faster compared to a HashMap for 3M unique labels.
- * 
- * @lucene.experimental
- */
-public class CompactLabelToOrdinal extends LabelToOrdinal {
-
-  public static final float DefaultLoadFactor = 0.15f;
-
-  static final char TERMINATOR_CHAR = 0xffff;
-  private static final int COLLISION = -5;
-
-  private HashArray[] hashArrays;
-  private CollisionMap collisionMap;
-  private CharBlockArray labelRepository;
-
-  private int capacity;
-  private int threshold;
-  private float loadFactor;
-
-  public int sizeOfMap() {
-    return this.collisionMap.size();
-  }
-
-  private CompactLabelToOrdinal() {
-  }
-
-  public CompactLabelToOrdinal(int initialCapacity, float loadFactor,
-                                int numHashArrays) {
-
-    this.hashArrays = new HashArray[numHashArrays];
-
-    this.capacity = determineCapacity((int) Math.pow(2, numHashArrays),
-        initialCapacity);
-    init();
-    this.collisionMap = new CollisionMap(this.labelRepository);
-
-    this.counter = 0;
-    this.loadFactor = loadFactor;
-
-    this.threshold = (int) (this.loadFactor * this.capacity);
-  }
-
-  static int determineCapacity(int minCapacity, int initialCapacity) {
-    int capacity = minCapacity;
-    while (capacity < initialCapacity) {
-      capacity <<= 1;
-    }
-    return capacity;
-  }
-
-  private void init() {
-    labelRepository = new CharBlockArray();
-    CategoryPathUtils.serialize(FacetLabel.EMPTY, labelRepository);
-
-    int c = this.capacity;
-    for (int i = 0; i < this.hashArrays.length; i++) {
-      this.hashArrays[i] = new HashArray(c);
-      c /= 2;
-    }
-  }
-
-  @Override
-  public void addLabel(FacetLabel label, int ordinal) {
-    if (collisionMap.size() > threshold) {
-      grow();
-    }
-
-    int hash = CompactLabelToOrdinal.stringHashCode(label);
-    for (int i = 0; i < this.hashArrays.length; i++) {
-      if (addLabel(this.hashArrays[i], label, hash, ordinal)) {
-        return;
-      }
-    }
-
-    int prevVal = collisionMap.addLabel(label, hash, ordinal);
-    if (prevVal != ordinal) {
-      throw new IllegalArgumentException("Label already exists: " + label.toString('/') + " prev ordinal " + prevVal);
-    }
-  }
-
-  @Override
-  public int getOrdinal(FacetLabel label) {
-    if (label == null) {
-      return LabelToOrdinal.INVALID_ORDINAL;
-    }
-
-    int hash = CompactLabelToOrdinal.stringHashCode(label);
-    for (int i = 0; i < this.hashArrays.length; i++) {
-      int ord = getOrdinal(this.hashArrays[i], label, hash);
-      if (ord != COLLISION) {
-        return ord;
-      }
-    }
-
-    return this.collisionMap.get(label, hash);
-  }
-
-  private void grow() {
-    HashArray temp = this.hashArrays[this.hashArrays.length - 1];
-
-    for (int i = this.hashArrays.length - 1; i > 0; i--) {
-      this.hashArrays[i] = this.hashArrays[i - 1];
-    }
-
-    this.capacity *= 2;
-    this.hashArrays[0] = new HashArray(this.capacity);
-
-    for (int i = 1; i < this.hashArrays.length; i++) {
-      int[] sourceOffsetArray = this.hashArrays[i].offsets;
-      int[] sourceCidsArray = this.hashArrays[i].cids;
-
-      for (int k = 0; k < sourceOffsetArray.length; k++) {
-
-        for (int j = 0; j < i && sourceOffsetArray[k] != 0; j++) {
-          int[] targetOffsetArray = this.hashArrays[j].offsets;
-          int[] targetCidsArray = this.hashArrays[j].cids;
-
-          int newIndex = indexFor(stringHashCode(
-              this.labelRepository, sourceOffsetArray[k]),
-              targetOffsetArray.length);
-          if (targetOffsetArray[newIndex] == 0) {
-            targetOffsetArray[newIndex] = sourceOffsetArray[k];
-            targetCidsArray[newIndex] = sourceCidsArray[k];
-            sourceOffsetArray[k] = 0;
-          }
-        }
-      }
-    }
-
-    for (int i = 0; i < temp.offsets.length; i++) {
-      int offset = temp.offsets[i];
-      if (offset > 0) {
-        int hash = stringHashCode(this.labelRepository, offset);
-        addLabelOffset(hash, temp.cids[i], offset);
-      }
-    }
-
-    CollisionMap oldCollisionMap = this.collisionMap;
-    this.collisionMap = new CollisionMap(oldCollisionMap.capacity(),
-        this.labelRepository);
-    this.threshold = (int) (this.capacity * this.loadFactor);
-
-    Iterator<CollisionMap.Entry> it = oldCollisionMap.entryIterator();
-    while (it.hasNext()) {
-      CollisionMap.Entry e = it.next();
-      addLabelOffset(stringHashCode(this.labelRepository, e.offset),
-          e.cid, e.offset);
-    }
-  }
-
-  private boolean addLabel(HashArray a, FacetLabel label, int hash, int ordinal) {
-    int index = CompactLabelToOrdinal.indexFor(hash, a.offsets.length);
-    int offset = a.offsets[index];
-
-    if (offset == 0) {
-      a.offsets[index] = this.labelRepository.length();
-      CategoryPathUtils.serialize(label, labelRepository);
-      a.cids[index] = ordinal;
-      return true;
-    }
-
-    return false;
-  }
-
-  private void addLabelOffset(int hash, int cid, int knownOffset) {
-    for (int i = 0; i < this.hashArrays.length; i++) {
-      if (addLabelOffsetToHashArray(this.hashArrays[i], hash, cid,
-          knownOffset)) {
-        return;
-      }
-    }
-
-    this.collisionMap.addLabelOffset(hash, knownOffset, cid);
-
-    if (this.collisionMap.size() > this.threshold) {
-      grow();
-    }
-  }
-
-  private boolean addLabelOffsetToHashArray(HashArray a, int hash, int ordinal,
-                                            int knownOffset) {
-
-    int index = CompactLabelToOrdinal.indexFor(hash, a.offsets.length);
-    int offset = a.offsets[index];
-
-    if (offset == 0) {
-      a.offsets[index] = knownOffset;
-      a.cids[index] = ordinal;
-      return true;
-    }
-
-    return false;
-  }
-
-  private int getOrdinal(HashArray a, FacetLabel label, int hash) {
-    if (label == null) {
-      return LabelToOrdinal.INVALID_ORDINAL;
-    }
-
-    int index = indexFor(hash, a.offsets.length);
-    int offset = a.offsets[index];
-    if (offset == 0) {
-      return LabelToOrdinal.INVALID_ORDINAL;
-    }
-
-    if (CategoryPathUtils.equalsToSerialized(label, labelRepository, offset)) {
-      return a.cids[index];
-    }
-
-    return COLLISION;
-  }
-
-  /** Returns index for hash code h. */
-  static int indexFor(int h, int length) {
-    return h & (length - 1);
-  }
-
-  // static int stringHashCode(String label) {
-  // int len = label.length();
-  // int hash = 0;
-  // int i;
-  // for (i = 0; i < len; ++i)
-  // hash = 33 * hash + label.charAt(i);
-  //
-  // hash = hash ^ ((hash >>> 20) ^ (hash >>> 12));
-  // hash = hash ^ (hash >>> 7) ^ (hash >>> 4);
-  //
-  // return hash;
-  //
-  // }
-
-  static int stringHashCode(FacetLabel label) {
-    int hash = label.hashCode();
-
-    hash = hash ^ ((hash >>> 20) ^ (hash >>> 12));
-    hash = hash ^ (hash >>> 7) ^ (hash >>> 4);
-
-    return hash;
-
-  }
-
-  static int stringHashCode(CharBlockArray labelRepository, int offset) {
-    int hash = CategoryPathUtils.hashCodeOfSerialized(labelRepository, offset);
-    hash = hash ^ ((hash >>> 20) ^ (hash >>> 12));
-    hash = hash ^ (hash >>> 7) ^ (hash >>> 4);
-    return hash;
-  }
-
-  // public static boolean equals(CharSequence label, CharBlockArray array,
-  // int offset) {
-  // // CONTINUE HERE
-  // int len = label.length();
-  // int bi = array.blockIndex(offset);
-  // CharBlockArray.Block b = array.blocks.get(bi);
-  // int index = array.indexInBlock(offset);
-  //
-  // for (int i = 0; i < len; i++) {
-  // if (label.charAt(i) != b.chars[index]) {
-  // return false;
-  // }
-  // index++;
-  // if (index == b.length) {
-  // b = array.blocks.get(++bi);
-  // index = 0;
-  // }
-  // }
-  //
-  // return b.chars[index] == TerminatorChar;
-  // }
-
-  /**
-   * Returns an estimate of the amount of memory used by this table. Called only in
-   * this package. Memory is consumed mainly by three structures: the hash arrays,
-   * label repository and collision map.
-   */
-  int getMemoryUsage() {
-    int memoryUsage = 0;
-    if (this.hashArrays != null) {
-      // HashArray capacity is instance-specific.
-      for (HashArray ha : this.hashArrays) {
-        // Each has 2 capacity-length arrays of ints.
-        memoryUsage += ( ha.capacity * 2 * 4 ) + 4;
-      }
-    }
-    if (this.labelRepository != null) {
-      // All blocks are the same size.
-      int blockSize = this.labelRepository.blockSize;
-      // Each block has room for blockSize UTF-16 chars.
-      int actualBlockSize = ( blockSize * 2 ) + 4;
-      memoryUsage += this.labelRepository.blocks.size() * actualBlockSize; 
-      memoryUsage += 8;   // Two int values for array as a whole.
-    }
-    if (this.collisionMap != null) {
-      memoryUsage += this.collisionMap.getMemoryUsage();
-    }
-    return memoryUsage;
-  }
-
-  /**
-   * Opens the file and reloads the CompactLabelToOrdinal. The file it expects
-   * is generated from the {@link #flush(File)} command.
-   */
-  static CompactLabelToOrdinal open(File file, float loadFactor,
-                                    int numHashArrays) throws IOException {
-    /**
-     * Part of the file is the labelRepository, which needs to be rehashed
-     * and label offsets re-added to the object. I am unsure as to why we
-     * can't just store these off in the file as well, but in keeping with
-     * the spirit of the original code, I did it this way. (ssuppe)
-     */
-    CompactLabelToOrdinal l2o = new CompactLabelToOrdinal();
-    l2o.loadFactor = loadFactor;
-    l2o.hashArrays = new HashArray[numHashArrays];
-
-    DataInputStream dis = null;
-    try {
-      dis = new DataInputStream(new BufferedInputStream(
-          new FileInputStream(file)));
-
-      // TaxiReader needs to load the "counter" or occupancy (L2O) to know
-      // the next unique facet. we used to load the delimiter too, but
-      // never used it.
-      l2o.counter = dis.readInt();
-
-      l2o.capacity = determineCapacity((int) Math.pow(2,
-          l2o.hashArrays.length), l2o.counter);
-      l2o.init();
-
-      // now read the chars
-      l2o.labelRepository = CharBlockArray.open(dis);
-
-      l2o.collisionMap = new CollisionMap(l2o.labelRepository);
-
-      // Calculate hash on the fly based on how CategoryPath hashes
-      // itself. Maybe in the future we can call some static based methods
-      // in CategoryPath so that this doesn't break again? I don't like
-      // having code in two different places...
-      int cid = 0;
-      // Skip the initial offset, it's the CategoryPath(0,0), which isn't
-      // a hashed value.
-      int offset = 1;
-      int lastStartOffset = offset;
-      // This loop really relies on a well-formed input (assumes pretty blindly
-      // that array offsets will work).  Since the initial file is machine 
-      // generated, I think this should be OK.
-      while (offset < l2o.labelRepository.length()) {
-        // identical code to CategoryPath.hashFromSerialized. since we need to
-        // advance offset, we cannot call the method directly. perhaps if we
-        // could pass a mutable Integer or something...
-        int length = (short) l2o.labelRepository.charAt(offset++);
-        int hash = length;
-        if (length != 0) {
-          for (int i = 0; i < length; i++) {
-            int len = (short) l2o.labelRepository.charAt(offset++);
-            hash = hash * 31 + l2o.labelRepository.subSequence(offset, offset + len).hashCode();
-            offset += len;
-          }
-        }
-        // Now that we've hashed the components of the label, do the
-        // final part of the hash algorithm.
-        hash = hash ^ ((hash >>> 20) ^ (hash >>> 12));
-        hash = hash ^ (hash >>> 7) ^ (hash >>> 4);
-        // Add the label, and let's keep going
-        l2o.addLabelOffset(hash, cid, lastStartOffset);
-        cid++;
-        lastStartOffset = offset;
-      }
-
-    } catch (ClassNotFoundException cnfe) {
-      throw new IOException("Invalid file format. Cannot deserialize.");
-    } finally {
-      if (dis != null) {
-        dis.close();
-      }
-    }
-
-    l2o.threshold = (int) (l2o.loadFactor * l2o.capacity);
-    return l2o;
-
-  }
-
-  void flush(File file) throws IOException {
-    FileOutputStream fos = new FileOutputStream(file);
-
-    try {
-      BufferedOutputStream os = new BufferedOutputStream(fos);
-
-      DataOutputStream dos = new DataOutputStream(os);
-      dos.writeInt(this.counter);
-
-      // write the labelRepository
-      this.labelRepository.flush(dos);
-
-      // Closes the data output stream
-      dos.close();
-
-    } finally {
-      fos.close();
-    }
-  }
-
-  private static final class HashArray {
-    int[] offsets;
-    int[] cids;
-
-    int capacity;
-
-    HashArray(int c) {
-      this.capacity = c;
-      this.offsets = new int[this.capacity];
-      this.cids = new int[this.capacity];
-    }
-  }
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/cl2o/LabelToOrdinal.java b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/cl2o/LabelToOrdinal.java
deleted file mode 100644
index e1c8360..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/cl2o/LabelToOrdinal.java
+++ /dev/null
@@ -1,60 +0,0 @@
-package org.apache.lucene.facet.taxonomy.writercache.cl2o;
-
-import org.apache.lucene.facet.taxonomy.FacetLabel;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Abstract class for storing Label->Ordinal mappings in a taxonomy. 
- * 
- * @lucene.experimental
- */
-public abstract class LabelToOrdinal {
-
-  protected int counter;
-  public static final int INVALID_ORDINAL = -2;
-
-  /**
-   * return the maximal Ordinal assigned so far
-   */
-  public int getMaxOrdinal() {
-    return this.counter;
-  }
-
-  /**
-   * Returns the next unassigned ordinal. The default behavior of this method
-   * is to simply increment a counter.
-   */
-  public int getNextOrdinal() {
-    return this.counter++;
-  }
-
-  /**
-   * Adds a new label if its not yet in the table.
-   * Throws an {@link IllegalArgumentException} if the same label with
-   * a different ordinal was previoulsy added to this table.
-   */
-  public abstract void addLabel(FacetLabel label, int ordinal);
-
-  /**
-   * @return the ordinal assigned to the given label, 
-   * or {@link #INVALID_ORDINAL} if the label cannot be found in this table.
-   */
-  public abstract int getOrdinal(FacetLabel label);
-
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/cl2o/package.html b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/cl2o/package.html
deleted file mode 100644
index 9a495a1..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/cl2o/package.html
+++ /dev/null
@@ -1,27 +0,0 @@
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-<title>Category->Ordinal caching implementation using an optimized data-structures</title>
-</head>
-<body>
-	<h1>Category->Ordinal caching implementation using an optimized data-structures</h1>
-	
-	The internal map data structure consumes less memory (~30%) and is faster (~50%) compared to a
- 	Java HashMap&lt;String, Integer&gt;.
-</body>
-</html>
\ No newline at end of file
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/lru/LruTaxonomyWriterCache.java b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/lru/LruTaxonomyWriterCache.java
deleted file mode 100644
index 4aac2cd..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/lru/LruTaxonomyWriterCache.java
+++ /dev/null
@@ -1,105 +0,0 @@
-package org.apache.lucene.facet.taxonomy.writercache.lru;
-
-import org.apache.lucene.facet.taxonomy.FacetLabel;
-import org.apache.lucene.facet.taxonomy.writercache.TaxonomyWriterCache;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * LRU {@link TaxonomyWriterCache} - good choice for huge taxonomies.
- * 
- * @lucene.experimental
- */
-public class LruTaxonomyWriterCache implements TaxonomyWriterCache {
-
-  /**
-   * Determines cache type.
-   * For guaranteed correctness - not relying on no-collisions in the hash
-   * function, LRU_STRING should be used.
-   */
-  public enum LRUType { LRU_HASHED, LRU_STRING }
-
-  private NameIntCacheLRU cache;
-
-  public LruTaxonomyWriterCache(int cacheSize) {
-    // TODO (Facet): choose between NameHashIntCacheLRU and NameIntCacheLRU.
-    // For guaranteed correctness - not relying on no-collisions in the hash
-    // function, NameIntCacheLRU should be used:
-    // On the other hand, NameHashIntCacheLRU takes less RAM but if there
-    // are collisions (which we never found) two different paths would be
-    // mapped to the same ordinal...
-    this(cacheSize, LRUType.LRU_HASHED);
-  }
-
-  public LruTaxonomyWriterCache(int cacheSize, LRUType lruType) {
-    // TODO (Facet): choose between NameHashIntCacheLRU and NameIntCacheLRU.
-    // For guaranteed correctness - not relying on no-collisions in the hash
-    // function, NameIntCacheLRU should be used:
-    // On the other hand, NameHashIntCacheLRU takes less RAM but if there
-    // are collisions (which we never found) two different paths would be
-    // mapped to the same ordinal...
-    if (lruType == LRUType.LRU_HASHED) {
-      this.cache = new NameHashIntCacheLRU(cacheSize);
-    } else {
-      this.cache = new NameIntCacheLRU(cacheSize);
-    }
-  }
-
-  @Override
-  public synchronized boolean isFull() {
-    return cache.getSize() == cache.getMaxSize();
-  }
-
-  @Override
-  public synchronized void clear() {
-    cache.clear();
-  }
-  
-  @Override
-  public synchronized void close() {
-    cache.clear();
-    cache = null;
-  }
-
-  @Override
-  public synchronized int get(FacetLabel categoryPath) {
-    Integer res = cache.get(categoryPath);
-    if (res == null) {
-      return -1;
-    }
-
-    return res.intValue();
-  }
-
-  @Override
-  public synchronized boolean put(FacetLabel categoryPath, int ordinal) {
-    boolean ret = cache.put(categoryPath, new Integer(ordinal));
-    // If the cache is full, we need to clear one or more old entries
-    // from the cache. However, if we delete from the cache a recent
-    // addition that isn't yet in our reader, for this entry to be
-    // visible to us we need to make sure that the changes have been
-    // committed and we reopen the reader. Because this is a slow
-    // operation, we don't delete entries one-by-one but rather in bulk
-    // (put() removes the 2/3rd oldest entries).
-    if (ret) {
-      cache.makeRoomLRU();
-    }
-    return ret;
-  }
-
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/lru/NameHashIntCacheLRU.java b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/lru/NameHashIntCacheLRU.java
deleted file mode 100644
index ef062d7..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/lru/NameHashIntCacheLRU.java
+++ /dev/null
@@ -1,47 +0,0 @@
-package org.apache.lucene.facet.taxonomy.writercache.lru;
-
-import org.apache.lucene.facet.taxonomy.FacetLabel;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * An an LRU cache of mapping from name to int.
- * Used to cache Ordinals of category paths.
- * It uses as key, hash of the path instead of the path.
- * This way the cache takes less RAM, but correctness depends on
- * assuming no collisions. 
- * 
- * @lucene.experimental
- */
-public class NameHashIntCacheLRU extends NameIntCacheLRU {
-
-  NameHashIntCacheLRU(int maxCacheSize) {
-    super(maxCacheSize);
-  }
-
-  @Override
-  Object key(FacetLabel name) {
-    return new Long(name.longHashCode());
-  }
-
-  @Override
-  Object key(FacetLabel name, int prefixLen) {
-    return new Long(name.subpath(prefixLen).longHashCode());
-  }
-  
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/lru/NameIntCacheLRU.java b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/lru/NameIntCacheLRU.java
deleted file mode 100644
index 2f2c487..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/lru/NameIntCacheLRU.java
+++ /dev/null
@@ -1,131 +0,0 @@
-package org.apache.lucene.facet.taxonomy.writercache.lru;
-
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.LinkedHashMap;
-import org.apache.lucene.facet.taxonomy.FacetLabel;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * An an LRU cache of mapping from name to int.
- * Used to cache Ordinals of category paths.
- * 
- * @lucene.experimental
- */
-// Note: Nothing in this class is synchronized. The caller is assumed to be
-// synchronized so that no two methods of this class are called concurrently.
-class NameIntCacheLRU {
-
-  private HashMap<Object, Integer> cache;
-  long nMisses = 0; // for debug
-  long nHits = 0;  // for debug
-  private int maxCacheSize;
-
-  NameIntCacheLRU(int maxCacheSize) {
-    this.maxCacheSize = maxCacheSize;
-    createCache(maxCacheSize);
-  }
-
-  public int getMaxSize() {
-    return maxCacheSize;
-  }
-  
-  public int getSize() {
-    return cache.size();
-  }
-
-  private void createCache (int maxSize) {
-    if (maxSize<Integer.MAX_VALUE) {
-      cache = new LinkedHashMap<Object, Integer>(1000,(float)0.7,true); //for LRU
-    } else {
-      cache = new HashMap<Object, Integer>(1000,(float)0.7); //no need for LRU
-    }
-  }
-
-  Integer get (FacetLabel name) {
-    Integer res = cache.get(key(name));
-    if (res==null) {
-      nMisses ++;
-    } else {
-      nHits ++;
-    }
-    return res;
-  }
-
-  /** Subclasses can override this to provide caching by e.g. hash of the string. */
-  Object key(FacetLabel name) {
-    return name;
-  }
-
-  Object key(FacetLabel name, int prefixLen) {
-    return name.subpath(prefixLen);
-  }
-
-  /**
-   * Add a new value to cache.
-   * Return true if cache became full and some room need to be made. 
-   */
-  boolean put (FacetLabel name, Integer val) {
-    cache.put(key(name), val);
-    return isCacheFull();
-  }
-
-  boolean put (FacetLabel name, int prefixLen, Integer val) {
-    cache.put(key(name, prefixLen), val);
-    return isCacheFull();
-  }
-
-  private boolean isCacheFull() {
-    return cache.size() > maxCacheSize;
-  }
-
-  void clear() {
-    cache.clear();
-  }
-
-  String stats() {
-    return "#miss="+nMisses+" #hit="+nHits;
-  }
-  
-  /**
-   * If cache is full remove least recently used entries from cache. Return true
-   * if anything was removed, false otherwise.
-   * 
-   * See comment in DirectoryTaxonomyWriter.addToCache(CategoryPath, int) for an
-   * explanation why we clean 2/3rds of the cache, and not just one entry.
-   */ 
-  boolean makeRoomLRU() {
-    if (!isCacheFull()) {
-      return false;
-    }
-    int n = cache.size() - (2*maxCacheSize)/3;
-    if (n<=0) {
-      return false;
-    }
-    Iterator<Object> it = cache.keySet().iterator();
-    int i = 0;
-    while (i<n && it.hasNext()) {
-      it.next();
-      it.remove();
-      i++;
-    }
-    return true;
-  }
-
-}
diff --git a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/lru/package.html b/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/lru/package.html
deleted file mode 100644
index b430d6b..0000000
--- a/lucene/facet/src/java/org/apache/lucene/facet/taxonomy/writercache/lru/package.html
+++ /dev/null
@@ -1,25 +0,0 @@
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-<title>An LRU cache implementation for the CategoryPath to Ordinal map</title>
-</head>
-<body>
-	<h1>An LRU cache implementation for the CategoryPath to Ordinal map</h1>
-	
-</body>
-</html>
\ No newline at end of file
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestConcurrentFacetedIndexing.java b/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestConcurrentFacetedIndexing.java
index 70f572b..f0bc44c 100644
--- a/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestConcurrentFacetedIndexing.java
+++ b/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestConcurrentFacetedIndexing.java
@@ -13,8 +13,8 @@ import org.apache.lucene.facet.FacetTestCase;
 import org.apache.lucene.facet.FacetsConfig;
 import org.apache.lucene.facet.taxonomy.FacetLabel;
 import org.apache.lucene.facet.taxonomy.writercache.TaxonomyWriterCache;
-import org.apache.lucene.facet.taxonomy.writercache.cl2o.Cl2oTaxonomyWriterCache;
-import org.apache.lucene.facet.taxonomy.writercache.lru.LruTaxonomyWriterCache;
+import org.apache.lucene.facet.taxonomy.writercache.Cl2oTaxonomyWriterCache;
+import org.apache.lucene.facet.taxonomy.writercache.LruTaxonomyWriterCache;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexWriterConfig.OpenMode;
 import org.apache.lucene.store.Directory;
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestDirectoryTaxonomyWriter.java b/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestDirectoryTaxonomyWriter.java
index e5f8848..01800eb 100644
--- a/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestDirectoryTaxonomyWriter.java
+++ b/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/directory/TestDirectoryTaxonomyWriter.java
@@ -18,8 +18,8 @@ import org.apache.lucene.facet.taxonomy.FacetLabel;
 import org.apache.lucene.facet.taxonomy.TaxonomyReader;
 import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter.MemoryOrdinalMap;
 import org.apache.lucene.facet.taxonomy.writercache.TaxonomyWriterCache;
-import org.apache.lucene.facet.taxonomy.writercache.cl2o.Cl2oTaxonomyWriterCache;
-import org.apache.lucene.facet.taxonomy.writercache.lru.LruTaxonomyWriterCache;
+import org.apache.lucene.facet.taxonomy.writercache.Cl2oTaxonomyWriterCache;
+import org.apache.lucene.facet.taxonomy.writercache.LruTaxonomyWriterCache;
 import org.apache.lucene.index.DirectoryReader;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/writercache/TestCharBlockArray.java b/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/writercache/TestCharBlockArray.java
new file mode 100644
index 0000000..3f5ec18
--- /dev/null
+++ b/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/writercache/TestCharBlockArray.java
@@ -0,0 +1,108 @@
+package org.apache.lucene.facet.taxonomy.writercache;
+
+import java.io.BufferedInputStream;
+import java.io.BufferedOutputStream;
+import java.io.File;
+import java.io.FileInputStream;
+import java.io.FileOutputStream;
+import java.nio.ByteBuffer;
+import java.nio.charset.CharsetDecoder;
+import java.nio.charset.CodingErrorAction;
+
+import org.apache.lucene.facet.FacetTestCase;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util._TestUtil;
+import org.junit.Test;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+public class TestCharBlockArray extends FacetTestCase {
+
+  @Test public void testArray() throws Exception {
+    CharBlockArray array = new CharBlockArray();
+    StringBuilder builder = new StringBuilder();
+
+    final int n = 100 * 1000;
+
+    byte[] buffer = new byte[50];
+
+    for (int i = 0; i < n; i++) {
+      random().nextBytes(buffer);
+      int size = 1 + random().nextInt(50);
+      // This test is turning random bytes into a string,
+      // this is asking for trouble.
+      CharsetDecoder decoder = IOUtils.CHARSET_UTF_8.newDecoder()
+          .onUnmappableCharacter(CodingErrorAction.REPLACE)
+          .onMalformedInput(CodingErrorAction.REPLACE);
+      String s = decoder.decode(ByteBuffer.wrap(buffer, 0, size)).toString();
+      array.append(s);
+      builder.append(s);
+    }
+
+    for (int i = 0; i < n; i++) {
+      random().nextBytes(buffer);
+      int size = 1 + random().nextInt(50);
+      // This test is turning random bytes into a string,
+      // this is asking for trouble.
+      CharsetDecoder decoder = IOUtils.CHARSET_UTF_8.newDecoder()
+          .onUnmappableCharacter(CodingErrorAction.REPLACE)
+          .onMalformedInput(CodingErrorAction.REPLACE);
+      String s = decoder.decode(ByteBuffer.wrap(buffer, 0, size)).toString();
+      array.append((CharSequence)s);
+      builder.append(s);
+    }
+
+    for (int i = 0; i < n; i++) {
+      random().nextBytes(buffer);
+      int size = 1 + random().nextInt(50);
+      // This test is turning random bytes into a string,
+      // this is asking for trouble.
+      CharsetDecoder decoder = IOUtils.CHARSET_UTF_8.newDecoder()
+          .onUnmappableCharacter(CodingErrorAction.REPLACE)
+          .onMalformedInput(CodingErrorAction.REPLACE);
+      String s = decoder.decode(ByteBuffer.wrap(buffer, 0, size)).toString();
+      for (int j = 0; j < s.length(); j++) {
+        array.append(s.charAt(j));
+      }
+      builder.append(s);
+    }
+
+    assertEqualsInternal("GrowingCharArray<->StringBuilder mismatch.", builder, array);
+
+    File tempDir = _TestUtil.getTempDir("growingchararray");
+    File f = new File(tempDir, "GrowingCharArrayTest.tmp");
+    BufferedOutputStream out = new BufferedOutputStream(new FileOutputStream(f));
+    array.flush(out);
+    out.flush();
+    out.close();
+
+    BufferedInputStream in = new BufferedInputStream(new FileInputStream(f));
+    array = CharBlockArray.open(in);
+    assertEqualsInternal("GrowingCharArray<->StringBuilder mismatch after flush/load.", builder, array);
+    in.close();
+    f.delete();
+  }
+
+  private static void assertEqualsInternal(String msg, StringBuilder expected, CharBlockArray actual) {
+    assertEquals(msg, expected.length(), actual.length());
+    for (int i = 0; i < expected.length(); i++) {
+      assertEquals(msg, expected.charAt(i), actual.charAt(i));
+    }
+  }
+
+}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/writercache/TestCompactLabelToOrdinal.java b/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/writercache/TestCompactLabelToOrdinal.java
new file mode 100644
index 0000000..5b2f700
--- /dev/null
+++ b/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/writercache/TestCompactLabelToOrdinal.java
@@ -0,0 +1,125 @@
+package org.apache.lucene.facet.taxonomy.writercache;
+
+import java.io.File;
+import java.nio.ByteBuffer;
+import java.nio.charset.CharsetDecoder;
+import java.nio.charset.CodingErrorAction;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.Random;
+
+import org.apache.lucene.facet.FacetTestCase;
+import org.apache.lucene.facet.taxonomy.FacetLabel;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util._TestUtil;
+import org.junit.Test;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+public class TestCompactLabelToOrdinal extends FacetTestCase {
+
+  @Test
+  public void testL2O() throws Exception {
+    LabelToOrdinal map = new LabelToOrdinalMap();
+
+    CompactLabelToOrdinal compact = new CompactLabelToOrdinal(2000000, 0.15f, 3);
+
+    final int n = atLeast(10 * 1000);
+    final int numUniqueValues = 50 * 1000;
+
+    String[] uniqueValues = new String[numUniqueValues];
+    byte[] buffer = new byte[50];
+
+    Random random = random();
+    for (int i = 0; i < numUniqueValues;) {
+      random.nextBytes(buffer);
+      int size = 1 + random.nextInt(buffer.length);
+
+      // This test is turning random bytes into a string,
+      // this is asking for trouble.
+      CharsetDecoder decoder = IOUtils.CHARSET_UTF_8.newDecoder()
+          .onUnmappableCharacter(CodingErrorAction.REPLACE)
+          .onMalformedInput(CodingErrorAction.REPLACE);
+      uniqueValues[i] = decoder.decode(ByteBuffer.wrap(buffer, 0, size)).toString();
+      // we cannot have empty path components, so eliminate all prefix as well
+      // as middle consecuive delimiter chars.
+      uniqueValues[i] = uniqueValues[i].replaceAll("/+", "/");
+      if (uniqueValues[i].startsWith("/")) {
+        uniqueValues[i] = uniqueValues[i].substring(1);
+      }
+      if (uniqueValues[i].indexOf(CompactLabelToOrdinal.TERMINATOR_CHAR) == -1) {
+        i++;
+      }
+    }
+
+    File tmpDir = _TestUtil.getTempDir("testLableToOrdinal");
+    File f = new File(tmpDir, "CompactLabelToOrdinalTest.tmp");
+    int flushInterval = 10;
+
+    for (int i = 0; i < n; i++) {
+      if (i > 0 && i % flushInterval == 0) {
+        compact.flush(f);    
+        compact = CompactLabelToOrdinal.open(f, 0.15f, 3);
+        assertTrue(f.delete());
+        if (flushInterval < (n / 10)) {
+          flushInterval *= 10;
+        }
+      }
+
+      int index = random.nextInt(numUniqueValues);
+      FacetLabel label = new FacetLabel(uniqueValues[index], '/');
+
+      int ord1 = map.getOrdinal(label);
+      int ord2 = compact.getOrdinal(label);
+
+      assertEquals(ord1, ord2);
+
+      if (ord1 == LabelToOrdinal.INVALID_ORDINAL) {
+        ord1 = compact.getNextOrdinal();
+        map.addLabel(label, ord1);
+        compact.addLabel(label, ord1);
+      }
+    }
+
+    for (int i = 0; i < numUniqueValues; i++) {
+      FacetLabel label = new FacetLabel(uniqueValues[i], '/');
+      int ord1 = map.getOrdinal(label);
+      int ord2 = compact.getOrdinal(label);
+      assertEquals(ord1, ord2);
+    }
+  }
+
+  private static class LabelToOrdinalMap extends LabelToOrdinal {
+    private Map<FacetLabel, Integer> map = new HashMap<FacetLabel, Integer>();
+
+    LabelToOrdinalMap() { }
+    
+    @Override
+    public void addLabel(FacetLabel label, int ordinal) {
+      map.put(label, ordinal);
+    }
+
+    @Override
+    public int getOrdinal(FacetLabel label) {
+      Integer value = map.get(label);
+      return (value != null) ? value.intValue() : LabelToOrdinal.INVALID_ORDINAL;
+    }
+
+  }
+
+}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/writercache/cl2o/TestCharBlockArray.java b/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/writercache/cl2o/TestCharBlockArray.java
deleted file mode 100644
index 76f8c16..0000000
--- a/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/writercache/cl2o/TestCharBlockArray.java
+++ /dev/null
@@ -1,108 +0,0 @@
-package org.apache.lucene.facet.taxonomy.writercache.cl2o;
-
-import java.io.BufferedInputStream;
-import java.io.BufferedOutputStream;
-import java.io.File;
-import java.io.FileInputStream;
-import java.io.FileOutputStream;
-import java.nio.ByteBuffer;
-import java.nio.charset.CharsetDecoder;
-import java.nio.charset.CodingErrorAction;
-
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util._TestUtil;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class TestCharBlockArray extends FacetTestCase {
-
-  @Test public void testArray() throws Exception {
-    CharBlockArray array = new CharBlockArray();
-    StringBuilder builder = new StringBuilder();
-
-    final int n = 100 * 1000;
-
-    byte[] buffer = new byte[50];
-
-    for (int i = 0; i < n; i++) {
-      random().nextBytes(buffer);
-      int size = 1 + random().nextInt(50);
-      // This test is turning random bytes into a string,
-      // this is asking for trouble.
-      CharsetDecoder decoder = IOUtils.CHARSET_UTF_8.newDecoder()
-          .onUnmappableCharacter(CodingErrorAction.REPLACE)
-          .onMalformedInput(CodingErrorAction.REPLACE);
-      String s = decoder.decode(ByteBuffer.wrap(buffer, 0, size)).toString();
-      array.append(s);
-      builder.append(s);
-    }
-
-    for (int i = 0; i < n; i++) {
-      random().nextBytes(buffer);
-      int size = 1 + random().nextInt(50);
-      // This test is turning random bytes into a string,
-      // this is asking for trouble.
-      CharsetDecoder decoder = IOUtils.CHARSET_UTF_8.newDecoder()
-          .onUnmappableCharacter(CodingErrorAction.REPLACE)
-          .onMalformedInput(CodingErrorAction.REPLACE);
-      String s = decoder.decode(ByteBuffer.wrap(buffer, 0, size)).toString();
-      array.append((CharSequence)s);
-      builder.append(s);
-    }
-
-    for (int i = 0; i < n; i++) {
-      random().nextBytes(buffer);
-      int size = 1 + random().nextInt(50);
-      // This test is turning random bytes into a string,
-      // this is asking for trouble.
-      CharsetDecoder decoder = IOUtils.CHARSET_UTF_8.newDecoder()
-          .onUnmappableCharacter(CodingErrorAction.REPLACE)
-          .onMalformedInput(CodingErrorAction.REPLACE);
-      String s = decoder.decode(ByteBuffer.wrap(buffer, 0, size)).toString();
-      for (int j = 0; j < s.length(); j++) {
-        array.append(s.charAt(j));
-      }
-      builder.append(s);
-    }
-
-    assertEqualsInternal("GrowingCharArray<->StringBuilder mismatch.", builder, array);
-
-    File tempDir = _TestUtil.getTempDir("growingchararray");
-    File f = new File(tempDir, "GrowingCharArrayTest.tmp");
-    BufferedOutputStream out = new BufferedOutputStream(new FileOutputStream(f));
-    array.flush(out);
-    out.flush();
-    out.close();
-
-    BufferedInputStream in = new BufferedInputStream(new FileInputStream(f));
-    array = CharBlockArray.open(in);
-    assertEqualsInternal("GrowingCharArray<->StringBuilder mismatch after flush/load.", builder, array);
-    in.close();
-    f.delete();
-  }
-
-  private static void assertEqualsInternal(String msg, StringBuilder expected, CharBlockArray actual) {
-    assertEquals(msg, expected.length(), actual.length());
-    for (int i = 0; i < expected.length(); i++) {
-      assertEquals(msg, expected.charAt(i), actual.charAt(i));
-    }
-  }
-
-}
diff --git a/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/writercache/cl2o/TestCompactLabelToOrdinal.java b/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/writercache/cl2o/TestCompactLabelToOrdinal.java
deleted file mode 100644
index 51b154f..0000000
--- a/lucene/facet/src/test/org/apache/lucene/facet/taxonomy/writercache/cl2o/TestCompactLabelToOrdinal.java
+++ /dev/null
@@ -1,125 +0,0 @@
-package org.apache.lucene.facet.taxonomy.writercache.cl2o;
-
-import java.io.File;
-import java.nio.ByteBuffer;
-import java.nio.charset.CharsetDecoder;
-import java.nio.charset.CodingErrorAction;
-import java.util.HashMap;
-import java.util.Map;
-import java.util.Random;
-
-import org.apache.lucene.facet.FacetTestCase;
-import org.apache.lucene.facet.taxonomy.FacetLabel;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util._TestUtil;
-import org.junit.Test;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class TestCompactLabelToOrdinal extends FacetTestCase {
-
-  @Test
-  public void testL2O() throws Exception {
-    LabelToOrdinal map = new LabelToOrdinalMap();
-
-    CompactLabelToOrdinal compact = new CompactLabelToOrdinal(2000000, 0.15f, 3);
-
-    final int n = atLeast(10 * 1000);
-    final int numUniqueValues = 50 * 1000;
-
-    String[] uniqueValues = new String[numUniqueValues];
-    byte[] buffer = new byte[50];
-
-    Random random = random();
-    for (int i = 0; i < numUniqueValues;) {
-      random.nextBytes(buffer);
-      int size = 1 + random.nextInt(buffer.length);
-
-      // This test is turning random bytes into a string,
-      // this is asking for trouble.
-      CharsetDecoder decoder = IOUtils.CHARSET_UTF_8.newDecoder()
-          .onUnmappableCharacter(CodingErrorAction.REPLACE)
-          .onMalformedInput(CodingErrorAction.REPLACE);
-      uniqueValues[i] = decoder.decode(ByteBuffer.wrap(buffer, 0, size)).toString();
-      // we cannot have empty path components, so eliminate all prefix as well
-      // as middle consecuive delimiter chars.
-      uniqueValues[i] = uniqueValues[i].replaceAll("/+", "/");
-      if (uniqueValues[i].startsWith("/")) {
-        uniqueValues[i] = uniqueValues[i].substring(1);
-      }
-      if (uniqueValues[i].indexOf(CompactLabelToOrdinal.TERMINATOR_CHAR) == -1) {
-        i++;
-      }
-    }
-
-    File tmpDir = _TestUtil.getTempDir("testLableToOrdinal");
-    File f = new File(tmpDir, "CompactLabelToOrdinalTest.tmp");
-    int flushInterval = 10;
-
-    for (int i = 0; i < n; i++) {
-      if (i > 0 && i % flushInterval == 0) {
-        compact.flush(f);    
-        compact = CompactLabelToOrdinal.open(f, 0.15f, 3);
-        assertTrue(f.delete());
-        if (flushInterval < (n / 10)) {
-          flushInterval *= 10;
-        }
-      }
-
-      int index = random.nextInt(numUniqueValues);
-      FacetLabel label = new FacetLabel(uniqueValues[index], '/');
-
-      int ord1 = map.getOrdinal(label);
-      int ord2 = compact.getOrdinal(label);
-
-      assertEquals(ord1, ord2);
-
-      if (ord1 == LabelToOrdinal.INVALID_ORDINAL) {
-        ord1 = compact.getNextOrdinal();
-        map.addLabel(label, ord1);
-        compact.addLabel(label, ord1);
-      }
-    }
-
-    for (int i = 0; i < numUniqueValues; i++) {
-      FacetLabel label = new FacetLabel(uniqueValues[i], '/');
-      int ord1 = map.getOrdinal(label);
-      int ord2 = compact.getOrdinal(label);
-      assertEquals(ord1, ord2);
-    }
-  }
-
-  private static class LabelToOrdinalMap extends LabelToOrdinal {
-    private Map<FacetLabel, Integer> map = new HashMap<FacetLabel, Integer>();
-
-    LabelToOrdinalMap() { }
-    
-    @Override
-    public void addLabel(FacetLabel label, int ordinal) {
-      map.put(label, ordinal);
-    }
-
-    @Override
-    public int getOrdinal(FacetLabel label) {
-      Integer value = map.get(label);
-      return (value != null) ? value.intValue() : LabelToOrdinal.INVALID_ORDINAL;
-    }
-
-  }
-
-}

