GitDiffStart: 21b7c5bbee13b88fc2b28318314c8a38b9388d07 | Wed Nov 30 05:19:35 2011 +0000
diff --git a/lucene/contrib/misc/src/java/org/apache/lucene/misc/HighFreqTerms.java b/lucene/contrib/misc/src/java/org/apache/lucene/misc/HighFreqTerms.java
index 33a8709..79c4699 100644
--- a/lucene/contrib/misc/src/java/org/apache/lucene/misc/HighFreqTerms.java
+++ b/lucene/contrib/misc/src/java/org/apache/lucene/misc/HighFreqTerms.java
@@ -25,11 +25,15 @@ import org.apache.lucene.index.FieldsEnum;
 import org.apache.lucene.index.Terms;
 import org.apache.lucene.index.FieldReaderException;
 import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.store.FSDirectory;
 import org.apache.lucene.util.PriorityQueue;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.ReaderUtil;
+
 import java.io.File;
+import java.io.IOException;
 import java.util.Arrays;
 import java.util.Comparator;
 
@@ -178,45 +182,34 @@ public class HighFreqTerms {
     return ts;
   }
   
-  public static long getTotalTermFreq(IndexReader reader, String field, BytesRef termText) throws Exception {
-
-    long totalTF = 0;
+  public static long getTotalTermFreq(IndexReader reader, final String field, final BytesRef termText) throws Exception {   
+    final long totalTF[] = new long[1];
     
-    Terms terms = MultiFields.getTerms(reader, field);
-    if (terms == null) {
-      return 0;
-    }
-
-    TermsEnum termsEnum = terms.iterator(null);
-    if (termsEnum.seekCeil(termText) != TermsEnum.SeekStatus.FOUND) {
-      return 0;
-    }
+    new ReaderUtil.Gather(reader) {
 
-    Bits liveDocs = MultiFields.getLiveDocs(reader);
-    if (liveDocs == null) {
-      // TODO: we could do this up front, during the scan
-      // (next()), instead of after-the-fact here w/ seek,
-      // if the codec supports it and there are no del
-      // docs...
-      final long totTF = termsEnum.totalTermFreq();
-      if (totTF != -1) {
-        return totTF;
+      @Override
+      protected void add(int base, IndexReader r) throws IOException {
+        Bits liveDocs = r.getLiveDocs();
+        if (liveDocs == null) {
+          // TODO: we could do this up front, during the scan
+          // (next()), instead of after-the-fact here w/ seek,
+          // if the codec supports it and there are no del
+          // docs...
+          final long totTF = r.totalTermFreq(field, termText);
+          if (totTF != -1) {
+            totalTF[0] += totTF;
+            return;
+          }
+        }
+        DocsEnum de = r.termDocsEnum(liveDocs, field, termText);
+        if (de != null) {
+          while (de.nextDoc() != DocIdSetIterator.NO_MORE_DOCS)
+            totalTF[0] += de.freq();
+        }
       }
-    }
+    }.run();
     
-    DocsEnum de = termsEnum.docs(liveDocs, null);
-
-    // use DocsEnum.read() and BulkResult api
-    final DocsEnum.BulkReadResult bulkresult = de.getBulkResult();
-    int count;
-    while ((count = de.read()) != 0) {
-      final int[] freqs = bulkresult.freqs.ints;
-      final int limit = bulkresult.freqs.offset + count;
-      for(int i=bulkresult.freqs.offset;i<limit;i++) {
-        totalTF += freqs[i];
-      }
-    }
-    return totalTF;
+    return totalTF[0];
   }
   
   public static void fillQueue(TermsEnum termsEnum, TermStatsQueue tiq, String field) throws Exception {
diff --git a/lucene/src/java/org/apache/lucene/index/DocTermOrds.java b/lucene/src/java/org/apache/lucene/index/DocTermOrds.java
index 65ddf6a..41e6dc2 100644
--- a/lucene/src/java/org/apache/lucene/index/DocTermOrds.java
+++ b/lucene/src/java/org/apache/lucene/index/DocTermOrds.java
@@ -17,6 +17,7 @@
 
 package org.apache.lucene.index;
 
+import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.util.PagedBytes;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.Bits;
@@ -315,90 +316,85 @@ public class DocTermOrds {
 
         docsEnum = te.docs(liveDocs, docsEnum);
 
-        final DocsEnum.BulkReadResult bulkResult = docsEnum.getBulkResult();
-
         // dF, but takes deletions into account
         int actualDF = 0;
 
         for (;;) {
-          int chunk = docsEnum.read();
-          if (chunk <= 0) {
+          int doc = docsEnum.nextDoc();
+          if (doc == DocIdSetIterator.NO_MORE_DOCS) {
             break;
           }
           //System.out.println("  chunk=" + chunk + " docs");
 
-          actualDF += chunk;
+          actualDF ++;
+          termInstances++;
+          
+          //System.out.println("    docID=" + doc);
+          // add TNUM_OFFSET to the term number to make room for special reserved values:
+          // 0 (end term) and 1 (index into byte array follows)
+          int delta = termNum - lastTerm[doc] + TNUM_OFFSET;
+          lastTerm[doc] = termNum;
+          int val = index[doc];
+
+          if ((val & 0xff)==1) {
+            // index into byte array (actually the end of
+            // the doc-specific byte[] when building)
+            int pos = val >>> 8;
+            int ilen = vIntSize(delta);
+            byte[] arr = bytes[doc];
+            int newend = pos+ilen;
+            if (newend > arr.length) {
+              // We avoid a doubling strategy to lower memory usage.
+              // this faceting method isn't for docs with many terms.
+              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.
+              // TODO: figure out what array lengths we can round up to w/o actually using more memory
+              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?
+              // It should be safe to round up to the nearest 32 bits in any case.
+              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment
+              byte[] newarr = new byte[newLen];
+              System.arraycopy(arr, 0, newarr, 0, pos);
+              arr = newarr;
+              bytes[doc] = newarr;
+            }
+            pos = writeInt(delta, arr, pos);
+            index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]
+          } else {
+            // OK, this int has data in it... find the end (a zero starting byte - not
+            // part of another number, hence not following a byte with the high bit set).
+            int ipos;
+            if (val==0) {
+              ipos=0;
+            } else if ((val & 0x0000ff80)==0) {
+              ipos=1;
+            } else if ((val & 0x00ff8000)==0) {
+              ipos=2;
+            } else if ((val & 0xff800000)==0) {
+              ipos=3;
+            } else {
+              ipos=4;
+            }
 
-          for (int i=0; i<chunk; i++) {
-            termInstances++;
-            int doc = bulkResult.docs.ints[i];
-            //System.out.println("    docID=" + doc);
-            // add TNUM_OFFSET to the term number to make room for special reserved values:
-            // 0 (end term) and 1 (index into byte array follows)
-            int delta = termNum - lastTerm[doc] + TNUM_OFFSET;
-            lastTerm[doc] = termNum;
-            int val = index[doc];
+            //System.out.println("      ipos=" + ipos);
 
-            if ((val & 0xff)==1) {
-              // index into byte array (actually the end of
-              // the doc-specific byte[] when building)
-              int pos = val >>> 8;
-              int ilen = vIntSize(delta);
-              byte[] arr = bytes[doc];
-              int newend = pos+ilen;
-              if (newend > arr.length) {
-                // We avoid a doubling strategy to lower memory usage.
-                // this faceting method isn't for docs with many terms.
-                // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.
-                // TODO: figure out what array lengths we can round up to w/o actually using more memory
-                // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?
-                // It should be safe to round up to the nearest 32 bits in any case.
-                int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment
-                byte[] newarr = new byte[newLen];
-                System.arraycopy(arr, 0, newarr, 0, pos);
-                arr = newarr;
-                bytes[doc] = newarr;
+            int endPos = writeInt(delta, tempArr, ipos);
+            //System.out.println("      endpos=" + endPos);
+            if (endPos <= 4) {
+              //System.out.println("      fits!");
+              // value will fit in the integer... move bytes back
+              for (int j=ipos; j<endPos; j++) {
+                val |= (tempArr[j] & 0xff) << (j<<3);
               }
-              pos = writeInt(delta, arr, pos);
-              index[doc] = (pos<<8) | 1;  // update pointer to end index in byte[]
+              index[doc] = val;
             } else {
-              // OK, this int has data in it... find the end (a zero starting byte - not
-              // part of another number, hence not following a byte with the high bit set).
-              int ipos;
-              if (val==0) {
-                ipos=0;
-              } else if ((val & 0x0000ff80)==0) {
-                ipos=1;
-              } else if ((val & 0x00ff8000)==0) {
-                ipos=2;
-              } else if ((val & 0xff800000)==0) {
-                ipos=3;
-              } else {
-                ipos=4;
-              }
-
-              //System.out.println("      ipos=" + ipos);
-
-              int endPos = writeInt(delta, tempArr, ipos);
-              //System.out.println("      endpos=" + endPos);
-              if (endPos <= 4) {
-                //System.out.println("      fits!");
-                // value will fit in the integer... move bytes back
-                for (int j=ipos; j<endPos; j++) {
-                  val |= (tempArr[j] & 0xff) << (j<<3);
-                }
-                index[doc] = val;
-              } else {
-                // value won't fit... move integer into byte[]
-                for (int j=0; j<ipos; j++) {
-                  tempArr[j] = (byte)val;
-                  val >>>=8;
-                }
-                // point at the end index in the byte[]
-                index[doc] = (endPos<<8) | 1;
-                bytes[doc] = tempArr;
-                tempArr = new byte[12];
+              // value won't fit... move integer into byte[]
+              for (int j=0; j<ipos; j++) {
+                tempArr[j] = (byte)val;
+                val >>>=8;
               }
+              // point at the end index in the byte[]
+              index[doc] = (endPos<<8) | 1;
+              bytes[doc] = tempArr;
+              tempArr = new byte[12];
             }
           }
         }
diff --git a/lucene/src/java/org/apache/lucene/index/DocsAndPositionsEnum.java b/lucene/src/java/org/apache/lucene/index/DocsAndPositionsEnum.java
index 551216e..52a1d17 100644
--- a/lucene/src/java/org/apache/lucene/index/DocsAndPositionsEnum.java
+++ b/lucene/src/java/org/apache/lucene/index/DocsAndPositionsEnum.java
@@ -35,14 +35,4 @@ public abstract class DocsAndPositionsEnum extends DocsEnum {
   public abstract BytesRef getPayload() throws IOException;
 
   public abstract boolean hasPayload();
-
-  @Override
-  public final int read() {
-    throw new UnsupportedOperationException();
-  }
-
-  @Override
-  public BulkReadResult getBulkResult() {
-    throw new UnsupportedOperationException();
-  }
 }
diff --git a/lucene/src/java/org/apache/lucene/index/DocsEnum.java b/lucene/src/java/org/apache/lucene/index/DocsEnum.java
index 88b2de3..32b9831 100644
--- a/lucene/src/java/org/apache/lucene/index/DocsEnum.java
+++ b/lucene/src/java/org/apache/lucene/index/DocsEnum.java
@@ -17,18 +17,12 @@ package org.apache.lucene.index;
  * limitations under the License.
  */
 
-import java.io.IOException;
-
 import org.apache.lucene.search.DocIdSetIterator;
 import org.apache.lucene.util.AttributeSource;
-import org.apache.lucene.util.IntsRef;
 
 /** Iterates through the documents, term freq and positions.
  *  NOTE: you must first call {@link #nextDoc} before using
- *  any of the per-doc methods (this does not apply to the
- *  bulk read {@link #read} method).
- *
- *  @lucene.experimental */
+ *  any of the per-doc methods. */
 public abstract class DocsEnum extends DocIdSetIterator {
 
   private AttributeSource atts = null;
@@ -43,60 +37,4 @@ public abstract class DocsEnum extends DocIdSetIterator {
     if (atts == null) atts = new AttributeSource();
     return atts;
   }
-
-  // TODO: maybe add bulk read only docIDs (for eventual
-  // match-only scoring)
-
-  public static class BulkReadResult {
-    public final IntsRef docs = new IntsRef();
-    public final IntsRef freqs = new IntsRef();
-  }
-
-  protected BulkReadResult bulkResult;
-
-  protected final void initBulkResult() {
-    if (bulkResult == null) {
-      bulkResult = new BulkReadResult();
-      bulkResult.docs.ints = new int[64];
-      bulkResult.freqs.ints = new int[64];
-    }
-  }
-
-  /** Call this once, up front, and hold a reference to the
-   *  returned bulk result.  When you call {@link #read}, it
-   *  fills the docs and freqs of this pre-shared bulk
-   *  result. */
-  public BulkReadResult getBulkResult() {
-    initBulkResult();
-    return bulkResult;
-  }
-  
-  /** Bulk read (docs and freqs).  After this is called,
-   *  {@link #docID()} and {@link #freq} are undefined.
-   *  This returns the count read, or 0 if the end is
-   *  reached.  The resulting docs and freqs are placed into
-   *  the pre-shard {@link BulkReadResult} instance returned
-   *  by {@link #getBulkResult}.  Note that the {@link
-   *  IntsRef} for docs and freqs will not have their length
-   *  set.
-   * 
-   *  <p>NOTE: the default impl simply delegates to {@link
-   *  #nextDoc}, but subclasses may do this more
-   *  efficiently. */
-  public int read() throws IOException {
-    int count = 0;
-    final int[] docs = bulkResult.docs.ints;
-    final int[] freqs = bulkResult.freqs.ints;
-    while(count < docs.length) {
-      final int doc = nextDoc();
-      if (doc != NO_MORE_DOCS) {
-        docs[count] = doc;
-        freqs[count] = freq();
-        count++;
-      } else {
-        break;
-      }
-    }
-    return count;
-  }
 }
diff --git a/lucene/src/java/org/apache/lucene/index/FilterIndexReader.java b/lucene/src/java/org/apache/lucene/index/FilterIndexReader.java
index 8f01cdb..ef0bec8 100644
--- a/lucene/src/java/org/apache/lucene/index/FilterIndexReader.java
+++ b/lucene/src/java/org/apache/lucene/index/FilterIndexReader.java
@@ -227,16 +227,6 @@ public class FilterIndexReader extends IndexReader {
     public int advance(int target) throws IOException {
       return in.advance(target);
     }
-
-    @Override
-    public BulkReadResult getBulkResult() {
-      return in.getBulkResult();
-    }
-
-    @Override
-    public int read() throws IOException {
-      return in.read();
-    }
   }
 
   /** Base class for filtering {@link DocsAndPositionsEnum} implementations. */
diff --git a/lucene/src/java/org/apache/lucene/index/MultiDocsEnum.java b/lucene/src/java/org/apache/lucene/index/MultiDocsEnum.java
index 4387159..c90ab1d 100644
--- a/lucene/src/java/org/apache/lucene/index/MultiDocsEnum.java
+++ b/lucene/src/java/org/apache/lucene/index/MultiDocsEnum.java
@@ -19,6 +19,7 @@ package org.apache.lucene.index;
 
 import org.apache.lucene.util.ReaderUtil;
 import java.io.IOException;
+import java.util.Arrays;
 
 /**
  * Exposes flex API, merged from flex API of sub-segments.
@@ -123,6 +124,16 @@ public final class MultiDocsEnum extends DocsEnum {
   public final static class EnumWithSlice {
     public DocsEnum docsEnum;
     public ReaderUtil.Slice slice;
+    
+    @Override
+    public String toString() {
+      return slice.toString()+":"+docsEnum;
+    }
+  }
+
+  @Override
+  public String toString() {
+    return "MultiDocsEnum(" + Arrays.toString(getSubs()) + ")";
   }
 }
 
diff --git a/lucene/src/java/org/apache/lucene/index/MultiFields.java b/lucene/src/java/org/apache/lucene/index/MultiFields.java
index 2a24a69..2a077a5 100644
--- a/lucene/src/java/org/apache/lucene/index/MultiFields.java
+++ b/lucene/src/java/org/apache/lucene/index/MultiFields.java
@@ -250,7 +250,7 @@ public final class MultiFields extends Fields {
 
   @Override
   public int getUniqueFieldCount() {
-    return terms.size();
+    return -1;
   }
 }
 
diff --git a/lucene/src/java/org/apache/lucene/index/MultiNorms.java b/lucene/src/java/org/apache/lucene/index/MultiNorms.java
index 1026480..4fd7a08 100644
--- a/lucene/src/java/org/apache/lucene/index/MultiNorms.java
+++ b/lucene/src/java/org/apache/lucene/index/MultiNorms.java
@@ -39,6 +39,9 @@ import org.apache.lucene.util.ReaderUtil;
  * @lucene.experimental
  */
 public class MultiNorms {
+  // no need to instantiate this
+  private MultiNorms() { }
+  
   /**
    * Warning: this is heavy! Do not use in a loop, or implement norms()
    * in your own reader with this (you should likely cache the result).
diff --git a/lucene/src/java/org/apache/lucene/index/MultiTermsEnum.java b/lucene/src/java/org/apache/lucene/index/MultiTermsEnum.java
index f9093bf..489df81 100644
--- a/lucene/src/java/org/apache/lucene/index/MultiTermsEnum.java
+++ b/lucene/src/java/org/apache/lucene/index/MultiTermsEnum.java
@@ -25,6 +25,7 @@ import org.apache.lucene.util.MultiBits;
 import org.apache.lucene.util.ReaderUtil;
 
 import java.io.IOException;
+import java.util.Arrays;
 import java.util.Comparator;
 
 /**
@@ -506,6 +507,11 @@ public final class MultiTermsEnum extends TermsEnum {
       this.terms = terms;
       current = term;
     }
+
+    @Override
+    public String toString() {
+      return subSlice.toString()+":"+terms;
+    }
   }
 
   private final static class TermMergeQueue extends PriorityQueue<TermsEnumWithSlice> {
@@ -524,4 +530,9 @@ public final class MultiTermsEnum extends TermsEnum {
       }
     }
   }
+
+  @Override
+  public String toString() {
+    return "MultiTermsEnum(" + Arrays.toString(subs) + ")";
+  }
 }
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/lucene3x/Lucene3xFields.java b/lucene/src/java/org/apache/lucene/index/codecs/lucene3x/Lucene3xFields.java
index a26af61..fe142c9 100644
--- a/lucene/src/java/org/apache/lucene/index/codecs/lucene3x/Lucene3xFields.java
+++ b/lucene/src/java/org/apache/lucene/index/codecs/lucene3x/Lucene3xFields.java
@@ -1025,16 +1025,6 @@ public class Lucene3xFields extends FieldsProducer {
     public int docID() {
       return docID;
     }
-
-    @Override
-    public int read() throws IOException {
-      if (bulkResult == null) {
-        initBulkResult();
-        bulkResult.docs.ints = new int[32];
-        bulkResult.freqs.ints = new int[32];
-      }
-      return this.docs.read(bulkResult.docs.ints, bulkResult.freqs.ints);
-    }
   }
 
   private final class PreDocsAndPositionsEnum extends DocsAndPositionsEnum {
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40PostingsReader.java b/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40PostingsReader.java
index 102e733..8822ab4 100644
--- a/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40PostingsReader.java
+++ b/lucene/src/java/org/apache/lucene/index/codecs/lucene40/Lucene40PostingsReader.java
@@ -18,6 +18,7 @@ package org.apache.lucene.index.codecs.lucene40;
  */
 
 import java.io.IOException;
+import java.util.Arrays;
 import java.util.Collection;
 
 import org.apache.lucene.index.DocsAndPositionsEnum;
@@ -263,8 +264,16 @@ public class Lucene40PostingsReader extends PostingsReaderBase {
     }
   }
 
+  static final int BUFFERSIZE = 64;
+
   // Decodes only docs
   private class SegmentDocsEnum extends DocsEnum {
+    final int[] docs = new int[BUFFERSIZE];
+    final int[] freqs = new int[BUFFERSIZE];
+    
+    int start = -1;
+    int count = 0;
+    
     final IndexInput freqIn;
     final IndexInput startFreqIn;
 
@@ -294,7 +303,9 @@ public class Lucene40PostingsReader extends PostingsReaderBase {
       omitTF = fieldInfo.indexOptions == IndexOptions.DOCS_ONLY;
       if (omitTF) {
         freq = 1;
+        Arrays.fill(freqs, 1);
       }
+      
       storePayloads = fieldInfo.storePayloads;
       this.liveDocs = liveDocs;
       freqOffset = termState.freqOffset;
@@ -313,89 +324,126 @@ public class Lucene40PostingsReader extends PostingsReaderBase {
 
       skipped = false;
 
+      start = -1;
+      count = 0;
       return this;
     }
+    
+    @Override
+    public int freq() {
+      return freq;
+    }
 
     @Override
+    public int docID() {
+      return doc;
+    }
+    
+    @Override
     public int nextDoc() throws IOException {
-      //if (DEBUG) System.out.println("    stpr.nextDoc seg=" + segment + " fp=" + freqIn.getFilePointer());
-      while(true) {
-        if (ord == limit) {
-          //if (DEBUG) System.out.println("      return doc=" + NO_MORE_DOCS);
-          return doc = NO_MORE_DOCS;
+      while (++start < count) {
+        int d = docs[start];
+        if (liveDocs == null || liveDocs.get(d)) {
+          freq = freqs[start];
+          return doc = d;
         }
-
-        ord++;
-
-        // Decode next doc/freq pair
-        final int code = freqIn.readVInt();
-        // if (DEBUG) System.out.println("      code=" + code);
-        if (omitTF) {
-          accum += code;
+      }
+      return doc = refill();
+    }
+    
+    @Override
+    public int advance(int target) throws IOException {
+      // last doc in our buffer is >= target, binary search + next()
+      if (++start < count && docs[count-1] >= target) {
+        binarySearch(target);
+        return nextDoc();
+      }
+      
+      start = count; // buffer is consumed
+      
+      return doc = skipTo(target);
+    }
+    
+    private void binarySearch(int target) {
+      int hi = count - 1;
+      while (start <= hi) {
+        int mid = (hi + start) >>> 1;
+        int doc = docs[mid];
+        if (doc < target) {
+          start = mid + 1;
+        } else if (doc > target) {
+          hi = mid - 1;
         } else {
-          accum += code >>> 1;              // shift off low bit
-          if ((code & 1) != 0) {          // if low bit is set
-            freq = 1;                     // freq is one
-          } else {
-            freq = freqIn.readVInt();     // else read freq
-          }
-        }
-
-        if (liveDocs == null || liveDocs.get(accum)) {
+          start = mid;
           break;
         }
       }
-
-      //if (DEBUG) System.out.println("    stpr.nextDoc return doc=" + doc);
-      return (doc = accum);
+      start--;
     }
 
-    @Override
-    public int read() throws IOException {
-
-      final int[] docs = bulkResult.docs.ints;
-      final int[] freqs = bulkResult.freqs.ints;
-      int i = 0;
-      final int length = docs.length;
-      while (i < length && ord < limit) {
-        ord++;
-        // manually inlined call to next() for speed
-        final int code = freqIn.readVInt();
+    private int refill() throws IOException {
+      int doc = scanTo(0);
+      
+      int bufferSize = Math.min(docs.length, limit - ord);
+      start = -1;
+      count = bufferSize;
+      ord += bufferSize;
+      
+      if (omitTF)
+        fillDocs(bufferSize);
+      else
+        fillDocsAndFreqs(bufferSize);
+      
+      return doc;
+    }
+    
+    private int scanTo(int target) throws IOException {
+      while (ord++ < limit) {
+        int code = freqIn.readVInt();
         if (omitTF) {
           accum += code;
         } else {
-          accum += code >>> 1;              // shift off low bit
+          accum += code >>> 1;            // shift off low bit
           if ((code & 1) != 0) {          // if low bit is set
             freq = 1;                     // freq is one
           } else {
             freq = freqIn.readVInt();     // else read freq
           }
         }
-
-        if (liveDocs == null || liveDocs.get(accum)) {
-          docs[i] = doc = accum;
-          freqs[i] = freq;
-          ++i;
+        
+        if (accum >= target && (liveDocs == null || liveDocs.get(accum))) {
+          return accum;
         }
       }
       
-      return i;
+      return NO_MORE_DOCS;
     }
-
-    @Override
-    public int docID() {
-      return doc;
+    
+    private void fillDocs(int size) throws IOException {
+      int docs[] = this.docs;
+      for (int i = 0; i < size; i++) {
+        accum += freqIn.readVInt();
+        docs[i] = accum;
+      }
     }
-
-    @Override
-    public int freq() {
-      return freq;
+    
+    private void fillDocsAndFreqs(int size) throws IOException {
+      int docs[] = this.docs;
+      int freqs[] = this.freqs;
+      for (int i = 0; i < size; i++) {
+        int code = freqIn.readVInt();
+        accum += code >>> 1;                   // shift off low bit
+        docs[i] = accum;
+        if ((code & 1) != 0) {                 // if low bit is set
+          freqs[i] = 1;                        // freq is one
+        } else {
+          freqs[i] = freqIn.readVInt();        // else read freq
+        }
+      }
     }
 
-    @Override
-    public int advance(int target) throws IOException {
-
-      if ((target - skipInterval) >= doc && limit >= skipMinimum) {
+    private int skipTo(int target) throws IOException {
+      if ((target - skipInterval) >= accum && limit >= skipMinimum) {
 
         // There are enough docs in the posting to have
         // skip data, and it isn't too close.
@@ -424,17 +472,11 @@ public class Lucene40PostingsReader extends PostingsReaderBase {
           // Skipper moved
 
           ord = newOrd;
-          doc = accum = skipper.getDoc();
+          accum = skipper.getDoc();
           freqIn.seek(skipper.getFreqPointer());
         }
       }
-        
-      // scan for the rest:
-      do {
-        nextDoc();
-      } while (target > doc);
-
-      return doc;
+      return scanTo(target);
     }
   }
 
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/sep/SepPostingsReader.java b/lucene/src/java/org/apache/lucene/index/codecs/sep/SepPostingsReader.java
index 40acb60..6d7ec66 100644
--- a/lucene/src/java/org/apache/lucene/index/codecs/sep/SepPostingsReader.java
+++ b/lucene/src/java/org/apache/lucene/index/codecs/sep/SepPostingsReader.java
@@ -411,34 +411,6 @@ public class SepPostingsReader extends PostingsReaderBase {
     }
 
     @Override
-    public int read() throws IOException {
-      // TODO: -- switch to bulk read api in IntIndexInput
-      //System.out.println("sepdocs read");
-      final int[] docs = bulkResult.docs.ints;
-      final int[] freqs = bulkResult.freqs.ints;
-      int i = 0;
-      final int length = docs.length;
-      while (i < length && count < docFreq) {
-        count++;
-        // manually inlined call to next() for speed
-        //System.out.println("decode doc");
-        accum += docReader.next();
-        if (!omitTF) {
-          //System.out.println("decode freq");
-          freq = freqReader.next();
-        }
-
-        if (liveDocs == null || liveDocs.get(accum)) {
-          docs[i] = doc = accum;
-          freqs[i] = freq;
-          //System.out.println("  docs[" + i + "]=" + doc + " count=" + count + " dF=" + docFreq);
-          i++;
-        }
-      }
-      return i;
-    }
-
-    @Override
     public int freq() {
       return freq;
     }
diff --git a/lucene/src/java/org/apache/lucene/search/MultiTermQueryWrapperFilter.java b/lucene/src/java/org/apache/lucene/search/MultiTermQueryWrapperFilter.java
index 267b709..49c798b 100644
--- a/lucene/src/java/org/apache/lucene/search/MultiTermQueryWrapperFilter.java
+++ b/lucene/src/java/org/apache/lucene/search/MultiTermQueryWrapperFilter.java
@@ -101,24 +101,14 @@ public class MultiTermQueryWrapperFilter<Q extends MultiTermQuery> extends Filte
     if (termsEnum.next() != null) {
       // fill into a FixedBitSet
       final FixedBitSet bitSet = new FixedBitSet(context.reader.maxDoc());
-      int termCount = 0;
       DocsEnum docsEnum = null;
       do {
-        termCount++;
         // System.out.println("  iter termCount=" + termCount + " term=" +
         // enumerator.term().toBytesString());
         docsEnum = termsEnum.docs(acceptDocs, docsEnum);
-        final DocsEnum.BulkReadResult result = docsEnum.getBulkResult();
-        while (true) {
-          final int count = docsEnum.read();
-          if (count != 0) {
-            final int[] docs = result.docs.ints;
-            for (int i = 0; i < count; i++) {
-              bitSet.set(docs[i]);
-            }
-          } else {
-            break;
-          }
+        int docid;
+        while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {
+          bitSet.set(docid);
         }
       } while (termsEnum.next() != null);
       // System.out.println("  done termCount=" + termCount);
diff --git a/lucene/src/java/org/apache/lucene/search/TermScorer.java b/lucene/src/java/org/apache/lucene/search/TermScorer.java
index fd98b22..02f860f 100644
--- a/lucene/src/java/org/apache/lucene/search/TermScorer.java
+++ b/lucene/src/java/org/apache/lucene/search/TermScorer.java
@@ -26,15 +26,6 @@ import org.apache.lucene.search.similarities.Similarity;
  */
 final class TermScorer extends Scorer {
   private final DocsEnum docsEnum;
-  private int doc = -1;
-  private int freq;
-
-  private int pointer;
-  private int pointerMax;
-
-  private final int[] docs;
-  private final int[] freqs;
-  private final DocsEnum.BulkReadResult bulkResult;
   private final Similarity.ExactDocScorer docScorer;
   
   /**
@@ -52,76 +43,32 @@ final class TermScorer extends Scorer {
     super(weight);
     this.docScorer = docScorer;
     this.docsEnum = td;
-    bulkResult = td.getBulkResult();
-    docs = bulkResult.docs.ints;
-    freqs = bulkResult.freqs.ints;
-  }
-
-  @Override
-  public void score(Collector c) throws IOException {
-    score(c, Integer.MAX_VALUE, nextDoc());
-  }
-
-  // firstDocID is ignored since nextDoc() sets 'doc'
-  @Override
-  public boolean score(Collector c, int end, int firstDocID) throws IOException {
-    c.setScorer(this);
-    while (doc < end) {                           // for docs in window
-      //System.out.println("TS: collect doc=" + doc);
-      c.collect(doc);                      // collect score
-      if (++pointer >= pointerMax) {
-        pointerMax = docsEnum.read();  // refill
-        if (pointerMax != 0) {
-          pointer = 0;
-        } else {
-          doc = NO_MORE_DOCS;                // set to sentinel value
-          return false;
-        }
-      } 
-      doc = docs[pointer];
-      freq = freqs[pointer];
-    }
-    return true;
   }
 
   @Override
   public int docID() {
-    return doc;
+    return docsEnum.docID();
   }
 
   @Override
   public float freq() {
-    return freq;
+    return docsEnum.freq();
   }
 
   /**
    * Advances to the next document matching the query. <br>
-   * The iterator over the matching documents is buffered using
-   * {@link TermDocs#read(int[],int[])}.
    * 
    * @return the document matching the query or NO_MORE_DOCS if there are no more documents.
    */
   @Override
   public int nextDoc() throws IOException {
-    pointer++;
-    if (pointer >= pointerMax) {
-      pointerMax = docsEnum.read();  // refill
-      if (pointerMax != 0) {
-        pointer = 0;
-      } else {
-        return doc = NO_MORE_DOCS;
-      }
-    } 
-    doc = docs[pointer];
-    freq = freqs[pointer];
-    assert doc != NO_MORE_DOCS;
-    return doc;
+    return docsEnum.nextDoc();
   }
   
   @Override
   public float score() {
-    assert doc != NO_MORE_DOCS;
-    return docScorer.score(doc, freq);  
+    assert docID() != NO_MORE_DOCS;
+    return docScorer.score(docsEnum.docID(), docsEnum.freq());  
   }
 
   /**
@@ -135,24 +82,7 @@ final class TermScorer extends Scorer {
    */
   @Override
   public int advance(int target) throws IOException {
-    // first scan in cache
-    for (pointer++; pointer < pointerMax; pointer++) {
-      if (docs[pointer] >= target) {
-        freq = freqs[pointer];
-        return doc = docs[pointer];
-      }
-    }
-
-    // not found in readahead cache, seek underlying stream
-    int newDoc = docsEnum.advance(target);
-    //System.out.println("ts.advance docsEnum=" + docsEnum);
-    if (newDoc != NO_MORE_DOCS) {
-      doc = newDoc;
-      freq = docsEnum.freq();
-    } else {
-      doc = NO_MORE_DOCS;
-    }
-    return doc;
+    return docsEnum.advance(target);
   }
 
   /** Returns a string representation of this <code>TermScorer</code>. */
diff --git a/lucene/src/test/org/apache/lucene/index/TestDuelingCodecs.java b/lucene/src/test/org/apache/lucene/index/TestDuelingCodecs.java
new file mode 100644
index 0000000..14ad9d0
--- /dev/null
+++ b/lucene/src/test/org/apache/lucene/index/TestDuelingCodecs.java
@@ -0,0 +1,597 @@
+package org.apache.lucene.index;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Iterator;
+import java.util.Random;
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.index.codecs.Codec;
+import org.apache.lucene.index.codecs.PerDocValues;
+import org.apache.lucene.index.values.IndexDocValues;
+import org.apache.lucene.index.values.IndexDocValues.Source;
+import org.apache.lucene.index.values.ValueType;
+import org.apache.lucene.search.DocIdSetIterator;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.FixedBitSet;
+import org.apache.lucene.util.LineFileDocs;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.automaton.AutomatonTestUtil;
+import org.apache.lucene.util.automaton.CompiledAutomaton;
+import org.apache.lucene.util.automaton.RegExp;
+
+/**
+ * Compares one codec against another
+ */
+public class TestDuelingCodecs extends LuceneTestCase {
+  private Directory leftDir;
+  private IndexReader leftReader;
+  private Codec leftCodec;
+
+  private Directory rightDir;
+  private IndexReader rightReader;
+  private Codec rightCodec;
+  
+  private String info;  // for debugging
+
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+
+    // for now its SimpleText vs Lucene40(random postings format)
+    // as this gives the best overall coverage. when we have more
+    // codecs we should probably pick 2 from Codec.availableCodecs()
+    
+    // TODO: it would also be nice to support preflex, but it doesn't
+    // support a lot of the current feature set (docvalues, statistics)
+    // so this would make assertEquals complicated.
+
+    leftCodec = Codec.forName("SimpleText");
+    rightCodec = new RandomCodec(random, false);
+    leftDir = newDirectory();
+    rightDir = newDirectory();
+
+    long seed = random.nextLong();
+
+    // must use same seed because of random payloads, etc
+    Analyzer leftAnalyzer = new MockAnalyzer(new Random(seed));
+    Analyzer rightAnalyzer = new MockAnalyzer(new Random(seed));
+    
+    // but these can be different
+    // TODO: this turns this into a really big test of Multi*, is that what we want?
+    IndexWriterConfig leftConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, leftAnalyzer);
+    leftConfig.setCodec(leftCodec);
+    // preserve docids
+    leftConfig.setMergePolicy(newLogMergePolicy());
+
+    IndexWriterConfig rightConfig = newIndexWriterConfig(TEST_VERSION_CURRENT, rightAnalyzer);
+    rightConfig.setCodec(rightCodec);
+    // preserve docids
+    rightConfig.setMergePolicy(newLogMergePolicy());
+
+    // must use same seed because of random docvalues fields, etc
+    RandomIndexWriter leftWriter = new RandomIndexWriter(new Random(seed), leftDir, leftConfig);
+    RandomIndexWriter rightWriter = new RandomIndexWriter(new Random(seed), rightDir, rightConfig);
+    
+    int numdocs = atLeast(500);
+    createRandomIndex(numdocs, leftWriter, seed);
+    createRandomIndex(numdocs, rightWriter, seed);
+
+    leftReader = leftWriter.getReader();
+    leftWriter.close();
+    rightReader = rightWriter.getReader();
+    rightWriter.close();
+    
+    info = "left: " + leftCodec.toString() + " / right: " + rightCodec.toString();
+  }
+  
+  @Override
+  public void tearDown() throws Exception {
+    leftReader.close();
+    rightReader.close();   
+    leftDir.close();
+    rightDir.close();
+    
+    super.tearDown();
+  }
+  
+  /**
+   * populates a writer with random stuff. this must be fully reproducable with the seed!
+   */
+  public static void createRandomIndex(int numdocs, RandomIndexWriter writer, long seed) throws IOException {
+    Random random = new Random(seed);
+    // primary source for our data is from linefiledocs, its realistic.
+    LineFileDocs lineFileDocs = new LineFileDocs(random);
+
+    // TODO: we should add other fields that use things like docs&freqs but omit positions,
+    // because linefiledocs doesn't cover all the possibilities.
+    for (int i = 0; i < numdocs; i++) {
+      writer.addDocument(lineFileDocs.nextDoc());
+    }
+  }
+  
+  /**
+   * checks the two indexes are equivalent
+   */
+  public void testEquals() throws Exception {
+    assertReaderStatistics(leftReader, rightReader);
+    assertFields(MultiFields.getFields(leftReader), MultiFields.getFields(rightReader));
+    assertNorms(leftReader, rightReader);
+    assertStoredFields(leftReader, rightReader);
+    assertTermVectors(leftReader, rightReader);
+    assertDocValues(leftReader, rightReader);
+    assertDeletedDocs(leftReader, rightReader);
+  }
+  
+  /** 
+   * checks that reader-level statistics are the same 
+   */
+  public void assertReaderStatistics(IndexReader leftReader, IndexReader rightReader) throws Exception {
+    // Somewhat redundant: we never delete docs
+    assertEquals(info, leftReader.maxDoc(), rightReader.maxDoc());
+    assertEquals(info, leftReader.numDocs(), rightReader.numDocs());
+    assertEquals(info, leftReader.numDeletedDocs(), rightReader.numDeletedDocs());
+    assertEquals(info, leftReader.hasDeletions(), rightReader.hasDeletions());
+    
+    if (leftReader.getUniqueTermCount() != -1 && rightReader.getUniqueTermCount() != -1) {
+      assertEquals(info, leftReader.getUniqueTermCount(), rightReader.getUniqueTermCount());
+    }
+  }
+  
+  /** 
+   * Fields api equivalency 
+   */
+  public void assertFields(Fields leftFields, Fields rightFields) throws Exception {
+    // Fields could be null if there are no postings,
+    // but then it must be null for both
+    if (leftFields == null || rightFields == null) {
+      assertNull(info, leftFields);
+      assertNull(info, rightFields);
+      return;
+    }
+    assertFieldStatistics(leftFields, rightFields);
+    
+    FieldsEnum leftEnum = leftFields.iterator();
+    FieldsEnum rightEnum = rightFields.iterator();
+    
+    String field;
+    while ((field = leftEnum.next()) != null) {
+      assertEquals(info, field, rightEnum.next());
+      assertTerms(leftEnum.terms(), rightEnum.terms());
+    }
+    assertNull(rightEnum.next());
+  }
+  
+  /** 
+   * checks that top-level statistics on Fields are the same 
+   */
+  public void assertFieldStatistics(Fields leftFields, Fields rightFields) throws Exception {
+    if (leftFields.getUniqueFieldCount() != -1 && rightFields.getUniqueFieldCount() != -1) {
+      assertEquals(info, leftFields.getUniqueFieldCount(), rightFields.getUniqueFieldCount());
+    }
+    
+    if (leftFields.getUniqueTermCount() != -1 && rightFields.getUniqueTermCount() != -1) {
+      assertEquals(info, leftFields.getUniqueTermCount(), rightFields.getUniqueTermCount());
+    }
+  }
+  
+  /** 
+   * Terms api equivalency 
+   */
+  public void assertTerms(Terms leftTerms, Terms rightTerms) throws Exception {
+    if (leftTerms == null || rightTerms == null) {
+      assertNull(info, leftTerms);
+      assertNull(info, rightTerms);
+      return;
+    }
+    assertTermsStatistics(leftTerms, rightTerms);
+
+    TermsEnum leftTermsEnum = leftTerms.iterator(null);
+    TermsEnum rightTermsEnum = rightTerms.iterator(null);
+    assertTermsEnum(leftTermsEnum, rightTermsEnum, true);
+    // TODO: test seeking too
+    
+    int numIntersections = atLeast(3);
+    for (int i = 0; i < numIntersections; i++) {
+      String re = AutomatonTestUtil.randomRegexp(random);
+      CompiledAutomaton automaton = new CompiledAutomaton(new RegExp(re, RegExp.NONE).toAutomaton());
+      if (automaton.type == CompiledAutomaton.AUTOMATON_TYPE.NORMAL) {
+        // TODO: test start term too
+        TermsEnum leftIntersection = leftTerms.intersect(automaton, null);
+        TermsEnum rightIntersection = rightTerms.intersect(automaton, null);
+        assertTermsEnum(leftIntersection, rightIntersection, rarely());
+      }
+    }
+  }
+  
+  /** 
+   * checks collection-level statistics on Terms 
+   */
+  public void assertTermsStatistics(Terms leftTerms, Terms rightTerms) throws Exception {
+    assert leftTerms.getComparator() == rightTerms.getComparator();
+    if (leftTerms.getDocCount() != -1 && rightTerms.getDocCount() != -1) {
+      assertEquals(info, leftTerms.getDocCount(), rightTerms.getDocCount());
+    }
+    if (leftTerms.getSumDocFreq() != -1 && rightTerms.getSumDocFreq() != -1) {
+      assertEquals(info, leftTerms.getSumDocFreq(), rightTerms.getSumDocFreq());
+    }
+    if (leftTerms.getSumTotalTermFreq() != -1 && rightTerms.getSumTotalTermFreq() != -1) {
+      assertEquals(info, leftTerms.getSumTotalTermFreq(), rightTerms.getSumTotalTermFreq());
+    }
+    if (leftTerms.getUniqueTermCount() != -1 && rightTerms.getUniqueTermCount() != -1) {
+      assertEquals(info, leftTerms.getUniqueTermCount(), rightTerms.getUniqueTermCount());
+    }
+  }
+
+  /** 
+   * checks the terms enum sequentially
+   * if deep is false, it does a 'shallow' test that doesnt go down to the docsenums
+   */
+  public void assertTermsEnum(TermsEnum leftTermsEnum, TermsEnum rightTermsEnum, boolean deep) throws Exception {
+    BytesRef term;
+    Bits randomBits = new RandomBits(leftReader.maxDoc(), random.nextDouble(), random);
+    DocsAndPositionsEnum leftPositions = null;
+    DocsAndPositionsEnum rightPositions = null;
+    DocsEnum leftDocs = null;
+    DocsEnum rightDocs = null;
+    
+    while ((term = leftTermsEnum.next()) != null) {
+      assertEquals(info, term, rightTermsEnum.next());
+      assertTermStats(leftTermsEnum, rightTermsEnum);
+      if (deep) {
+        assertDocsAndPositionsEnum(leftPositions = leftTermsEnum.docsAndPositions(null, leftPositions),
+            rightPositions = rightTermsEnum.docsAndPositions(null, rightPositions));
+        assertDocsAndPositionsEnum(leftPositions = leftTermsEnum.docsAndPositions(randomBits, leftPositions),
+            rightPositions = rightTermsEnum.docsAndPositions(randomBits, rightPositions));
+
+        assertPositionsSkipping(leftTermsEnum.docFreq(), 
+            leftPositions = leftTermsEnum.docsAndPositions(null, leftPositions),
+            rightPositions = rightTermsEnum.docsAndPositions(null, rightPositions));
+        assertPositionsSkipping(leftTermsEnum.docFreq(), 
+            leftPositions = leftTermsEnum.docsAndPositions(randomBits, leftPositions),
+            rightPositions = rightTermsEnum.docsAndPositions(randomBits, rightPositions));
+        
+        assertDocsEnum(leftDocs = leftTermsEnum.docs(null, leftDocs),
+            rightDocs = rightTermsEnum.docs(null, rightDocs));
+        assertDocsEnum(leftDocs = leftTermsEnum.docs(randomBits, leftDocs),
+            rightDocs = rightTermsEnum.docs(randomBits, rightDocs));
+        
+        assertDocsSkipping(leftTermsEnum.docFreq(), 
+            leftDocs = leftTermsEnum.docs(null, leftDocs),
+            rightDocs = rightTermsEnum.docs(null, rightDocs));
+        assertDocsSkipping(leftTermsEnum.docFreq(), 
+            leftDocs = leftTermsEnum.docs(randomBits, leftDocs),
+            rightDocs = rightTermsEnum.docs(randomBits, rightDocs));
+      }
+    }
+    assertNull(info, rightTermsEnum.next());
+  }
+  
+  /**
+   * checks term-level statistics
+   */
+  public void assertTermStats(TermsEnum leftTermsEnum, TermsEnum rightTermsEnum) throws Exception {
+    assertEquals(info, leftTermsEnum.docFreq(), rightTermsEnum.docFreq());
+    if (leftTermsEnum.totalTermFreq() != -1 && rightTermsEnum.totalTermFreq() != -1) {
+      assertEquals(info, leftTermsEnum.totalTermFreq(), rightTermsEnum.totalTermFreq());
+    }
+  }
+  
+  /**
+   * checks docs + freqs + positions + payloads, sequentially
+   */
+  public void assertDocsAndPositionsEnum(DocsAndPositionsEnum leftDocs, DocsAndPositionsEnum rightDocs) throws Exception {
+    if (leftDocs == null || rightDocs == null) {
+      assertNull(leftDocs);
+      assertNull(rightDocs);
+      return;
+    }
+    assertTrue(info, leftDocs.docID() == -1 || leftDocs.docID() == DocIdSetIterator.NO_MORE_DOCS);
+    assertTrue(info, rightDocs.docID() == -1 || rightDocs.docID() == DocIdSetIterator.NO_MORE_DOCS);
+    int docid;
+    while ((docid = leftDocs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {
+      assertEquals(info, docid, rightDocs.nextDoc());
+      int freq = leftDocs.freq();
+      assertEquals(info, freq, rightDocs.freq());
+      for (int i = 0; i < freq; i++) {
+        assertEquals(info, leftDocs.nextPosition(), rightDocs.nextPosition());
+        assertEquals(info, leftDocs.hasPayload(), rightDocs.hasPayload());
+        if (leftDocs.hasPayload()) {
+          assertEquals(info, leftDocs.getPayload(), rightDocs.getPayload());
+        }
+      }
+    }
+    assertEquals(info, DocIdSetIterator.NO_MORE_DOCS, rightDocs.nextDoc());
+  }
+  
+  /**
+   * checks docs + freqs, sequentially
+   */
+  public void assertDocsEnum(DocsEnum leftDocs, DocsEnum rightDocs) throws Exception {
+    assertTrue(info, leftDocs.docID() == -1 || leftDocs.docID() == DocIdSetIterator.NO_MORE_DOCS);
+    assertTrue(info, rightDocs.docID() == -1 || rightDocs.docID() == DocIdSetIterator.NO_MORE_DOCS);
+    int docid;
+    while ((docid = leftDocs.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {
+      assertEquals(info, docid, rightDocs.nextDoc());
+      assertEquals(info, leftDocs.freq(), rightDocs.freq());
+    }
+    assertEquals(info, DocIdSetIterator.NO_MORE_DOCS, rightDocs.nextDoc());
+  }
+  
+  /**
+   * checks advancing docs
+   */
+  public void assertDocsSkipping(int docFreq, DocsEnum leftDocs, DocsEnum rightDocs) throws Exception {
+    int docid = -1;
+    int averageGap = leftReader.maxDoc() / (1+docFreq);
+    int skipInterval = 16;
+
+    while (true) {
+      if (random.nextBoolean()) {
+        // nextDoc()
+        docid = leftDocs.nextDoc();
+        assertEquals(info, docid, rightDocs.nextDoc());
+      } else {
+        // advance()
+        int skip = docid + (int) Math.ceil(Math.abs(skipInterval + random.nextGaussian() * averageGap));
+        docid = leftDocs.advance(skip);
+        assertEquals(info, docid, rightDocs.advance(skip));
+      }
+      
+      if (docid == DocIdSetIterator.NO_MORE_DOCS) {
+        return;
+      }
+      assertEquals(info, leftDocs.freq(), rightDocs.freq());
+    }
+  }
+  
+  /**
+   * checks advancing docs + positions
+   */
+  public void assertPositionsSkipping(int docFreq, DocsAndPositionsEnum leftDocs, DocsAndPositionsEnum rightDocs) throws Exception {
+    if (leftDocs == null || rightDocs == null) {
+      assertNull(leftDocs);
+      assertNull(rightDocs);
+      return;
+    }
+    
+    int docid = -1;
+    int averageGap = leftReader.maxDoc() / (1+docFreq);
+    int skipInterval = 16;
+
+    while (true) {
+      if (random.nextBoolean()) {
+        // nextDoc()
+        docid = leftDocs.nextDoc();
+        assertEquals(info, docid, rightDocs.nextDoc());
+      } else {
+        // advance()
+        int skip = docid + (int) Math.ceil(Math.abs(skipInterval + random.nextGaussian() * averageGap));
+        docid = leftDocs.advance(skip);
+        assertEquals(info, docid, rightDocs.advance(skip));
+      }
+      
+      if (docid == DocIdSetIterator.NO_MORE_DOCS) {
+        return;
+      }
+      int freq = leftDocs.freq();
+      assertEquals(info, freq, rightDocs.freq());
+      for (int i = 0; i < freq; i++) {
+        assertEquals(info, leftDocs.nextPosition(), rightDocs.nextPosition());
+        assertEquals(info, leftDocs.hasPayload(), rightDocs.hasPayload());
+        if (leftDocs.hasPayload()) {
+          assertEquals(info, leftDocs.getPayload(), rightDocs.getPayload());
+        }
+      }
+    }
+  }
+  
+  /** 
+   * checks that norms are the same across all fields 
+   */
+  public void assertNorms(IndexReader leftReader, IndexReader rightReader) throws Exception {
+    Fields leftFields = MultiFields.getFields(leftReader);
+    Fields rightFields = MultiFields.getFields(rightReader);
+    // Fields could be null if there are no postings,
+    // but then it must be null for both
+    if (leftFields == null || rightFields == null) {
+      assertNull(info, leftFields);
+      assertNull(info, rightFields);
+      return;
+    }
+    
+    FieldsEnum fieldsEnum = leftFields.iterator();
+    String field;
+    while ((field = fieldsEnum.next()) != null) {
+      assertEquals(info, leftReader.hasNorms(field), rightReader.hasNorms(field));
+      if (leftReader.hasNorms(field)) {
+        byte leftNorms[] = MultiNorms.norms(leftReader, field);
+        byte rightNorms[] = MultiNorms.norms(rightReader, field);
+        assertArrayEquals(info, leftNorms, rightNorms);
+      }
+    }
+  }
+  
+  /** 
+   * checks that stored fields of all documents are the same 
+   */
+  public void assertStoredFields(IndexReader leftReader, IndexReader rightReader) throws Exception {
+    assert leftReader.maxDoc() == rightReader.maxDoc();
+    for (int i = 0; i < leftReader.maxDoc(); i++) {
+      Document leftDoc = leftReader.document(i);
+      Document rightDoc = rightReader.document(i);
+      
+      // TODO: I think this is bogus because we don't document what the order should be
+      // from these iterators, etc. I think the codec should be free to order this stuff
+      // in whatever way it wants (e.g. maybe it packs related fields together or something)
+
+      Iterator<IndexableField> leftIterator = leftDoc.iterator();
+      Iterator<IndexableField> rightIterator = rightDoc.iterator();
+      while (leftIterator.hasNext()) {
+        assertTrue(info, rightIterator.hasNext());
+        assertStoredField(leftIterator.next(), rightIterator.next());
+      }
+      assertFalse(info, rightIterator.hasNext());
+    }
+  }
+  
+  /** 
+   * checks that two stored fields are equivalent 
+   */
+  public void assertStoredField(IndexableField leftField, IndexableField rightField) {
+    assertEquals(info, leftField.name(), rightField.name());
+    assertEquals(info, leftField.binaryValue(), rightField.binaryValue());
+    assertEquals(info, leftField.stringValue(), rightField.stringValue());
+    assertEquals(info, leftField.numericValue(), rightField.numericValue());
+    assertEquals(info, leftField.numeric(), rightField.numeric());
+    assertEquals(info, leftField.numericDataType(), rightField.numericDataType());
+    // TODO: should we check the FT at all?
+  }
+  
+  /** 
+   * checks that term vectors across all fields are equivalent 
+   */
+  public void assertTermVectors(IndexReader leftReader, IndexReader rightReader) throws Exception {
+    assert leftReader.maxDoc() == rightReader.maxDoc();
+    for (int i = 0; i < leftReader.maxDoc(); i++) {
+      Fields leftFields = leftReader.getTermVectors(i);
+      Fields rightFields = rightReader.getTermVectors(i);
+      assertFields(leftFields, rightFields);
+    }
+  }
+  
+  /**
+   * checks that docvalues across all fields are equivalent
+   */
+  public void assertDocValues(IndexReader leftReader, IndexReader rightReader) throws Exception {
+    PerDocValues leftPerDoc = MultiPerDocValues.getPerDocs(leftReader);
+    PerDocValues rightPerDoc = MultiPerDocValues.getPerDocs(rightReader);
+    
+    Fields leftFields = MultiFields.getFields(leftReader);
+    Fields rightFields = MultiFields.getFields(rightReader);
+    // Fields could be null if there are no postings,
+    // but then it must be null for both
+    if (leftFields == null || rightFields == null) {
+      assertNull(info, leftFields);
+      assertNull(info, rightFields);
+      return;
+    }
+    
+    FieldsEnum fieldsEnum = leftFields.iterator();
+    String field;
+    while ((field = fieldsEnum.next()) != null) {
+      IndexDocValues leftDocValues = leftPerDoc.docValues(field);
+      IndexDocValues rightDocValues = rightPerDoc.docValues(field);
+      if (leftDocValues == null || rightDocValues == null) {
+        assertNull(info, leftDocValues);
+        assertNull(info, rightDocValues);
+        continue;
+      }
+      assertDocValuesSource(leftDocValues.getDirectSource(), rightDocValues.getDirectSource());
+      assertDocValuesSource(leftDocValues.getSource(), rightDocValues.getSource());
+    }
+  }
+  
+  /**
+   * checks source API
+   */
+  public void assertDocValuesSource(Source left, Source right) throws Exception {
+    ValueType leftType = left.type();
+    assertEquals(info, leftType, right.type());
+    switch(leftType) {
+      case VAR_INTS:
+      case FIXED_INTS_8:
+      case FIXED_INTS_16:
+      case FIXED_INTS_32:
+      case FIXED_INTS_64:
+        for (int i = 0; i < leftReader.maxDoc(); i++) {
+          assertEquals(info, left.getInt(i), right.getInt(i));
+        }
+        break;
+      case FLOAT_32:
+      case FLOAT_64:
+        for (int i = 0; i < leftReader.maxDoc(); i++) {
+          assertEquals(info, left.getFloat(i), right.getFloat(i), 0F);
+        }
+        break;
+      case BYTES_FIXED_STRAIGHT:
+      case BYTES_FIXED_DEREF:
+      case BYTES_VAR_STRAIGHT:
+      case BYTES_VAR_DEREF:
+        BytesRef b1 = new BytesRef();
+        BytesRef b2 = new BytesRef();
+        for (int i = 0; i < leftReader.maxDoc(); i++) {
+          left.getBytes(i, b1);
+          right.getBytes(i, b2);
+          assertEquals(info, b1, b2);
+        }
+        break;
+      // TODO: can we test these?
+      case BYTES_VAR_SORTED:
+      case BYTES_FIXED_SORTED:
+    }
+  }
+  
+  // TODO: this is kinda stupid, we don't delete documents in the test.
+  public void assertDeletedDocs(IndexReader leftReader, IndexReader rightReader) throws Exception {
+    assert leftReader.numDeletedDocs() == rightReader.numDeletedDocs();
+    Bits leftBits = MultiFields.getLiveDocs(leftReader);
+    Bits rightBits = MultiFields.getLiveDocs(rightReader);
+    
+    if (leftBits == null || rightBits == null) {
+      assertNull(info, leftBits);
+      assertNull(info, rightBits);
+      return;
+    }
+    
+    assert leftReader.maxDoc() == rightReader.maxDoc();
+    assertEquals(info, leftBits.length(), rightBits.length());
+    for (int i = 0; i < leftReader.maxDoc(); i++) {
+      assertEquals(info, leftBits.get(i), rightBits.get(i));
+    }
+  }
+  
+  
+  private static class RandomBits implements Bits {
+    FixedBitSet bits;
+    
+    RandomBits(int maxDoc, double pctLive, Random random) {
+      bits = new FixedBitSet(maxDoc);
+      for (int i = 0; i < maxDoc; i++) {
+        if (random.nextDouble() <= pctLive) {        
+          bits.set(i);
+        }
+      }
+    }
+    
+    @Override
+    public boolean get(int index) {
+      return bits.get(index);
+    }
+
+    @Override
+    public int length() {
+      return bits.length();
+    }
+  }
+}
diff --git a/solr/core/src/java/org/apache/solr/request/SimpleFacets.java b/solr/core/src/java/org/apache/solr/request/SimpleFacets.java
index e9ccadc..c1e6c09 100644
--- a/solr/core/src/java/org/apache/solr/request/SimpleFacets.java
+++ b/solr/core/src/java/org/apache/solr/request/SimpleFacets.java
@@ -696,31 +696,16 @@ public class SimpleFacets {
               for (int subindex = 0; subindex<numSubs; subindex++) {
                 MultiDocsEnum.EnumWithSlice sub = subs[subindex];
                 if (sub.docsEnum == null) continue;
-                DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();
                 int base = sub.slice.start;
-                for (;;) {
-                  int nDocs = sub.docsEnum.read();
-                  if (nDocs == 0) break;
-                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.
-                  int end = bulk.docs.offset + nDocs;
-                  for (int i=bulk.docs.offset; i<end; i++) {
-                    if (fastForRandomSet.exists(docArr[i]+base)) c++;
-                  }
+                int docid;
+                while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {
+                  if (fastForRandomSet.exists(docid+base)) c++;
                 }
               }
             } else {
-
-              // this should be the same bulk result object if sharing of the docsEnum succeeded
-              DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();
-
-              for (;;) {
-                int nDocs = docsEnum.read();
-                if (nDocs == 0) break;
-                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.
-                int end = bulk.docs.offset + nDocs;
-                for (int i=bulk.docs.offset; i<end; i++) {
-                  if (fastForRandomSet.exists(docArr[i])) c++;
-                }
+              int docid;
+              while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {
+                if (fastForRandomSet.exists(docid)) c++;
               }
             }
             
diff --git a/solr/core/src/java/org/apache/solr/search/JoinQParserPlugin.java b/solr/core/src/java/org/apache/solr/search/JoinQParserPlugin.java
index 7852f52..43b01b7 100644
--- a/solr/core/src/java/org/apache/solr/search/JoinQParserPlugin.java
+++ b/solr/core/src/java/org/apache/solr/search/JoinQParserPlugin.java
@@ -323,35 +323,21 @@ class JoinQuery extends Query {
             outer: for (int subindex = 0; subindex<numSubs; subindex++) {
               MultiDocsEnum.EnumWithSlice sub = subs[subindex];
               if (sub.docsEnum == null) continue;
-              DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();
               int base = sub.slice.start;
-              for (;;) {
-                int nDocs = sub.docsEnum.read();
-                if (nDocs == 0) break;
-                int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.
-                int end = bulk.docs.offset + nDocs;
-                for (int i=bulk.docs.offset; i<end; i++) {
-                  if (fastForRandomSet.exists(docArr[i]+base)) {
-                    intersects = true;
-                    break outer;
-                  }
+              int docid;
+              while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {
+                if (fastForRandomSet.exists(docid+base)) {
+                  intersects = true;
+                  break outer;
                 }
               }
             }
           } else {
-            // this should be the same bulk result object if sharing of the docsEnum succeeded
-            DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();
-
-            outer: for (;;) {
-              int nDocs = docsEnum.read();
-              if (nDocs == 0) break;
-              int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.
-              int end = bulk.docs.offset + nDocs;
-              for (int i=bulk.docs.offset; i<end; i++) {
-                if (fastForRandomSet.exists(docArr[i])) {
-                  intersects = true;
-                  break outer;
-                }
+            int docid;
+            while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {
+              if (fastForRandomSet.exists(docid)) {
+                intersects = true;
+                break;
               }
             }
           }
@@ -402,32 +388,18 @@ class JoinQuery extends Query {
                 for (int subindex = 0; subindex<numSubs; subindex++) {
                   MultiDocsEnum.EnumWithSlice sub = subs[subindex];
                   if (sub.docsEnum == null) continue;
-                  DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();
                   int base = sub.slice.start;
-                  for (;;) {
-                    int nDocs = sub.docsEnum.read();
-                    if (nDocs == 0) break;
-                    resultListDocs += nDocs;
-                    int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.
-                    int end = bulk.docs.offset + nDocs;
-                    for (int i=bulk.docs.offset; i<end; i++) {
-                      resultBits.fastSet(docArr[i]+base);
-                    }
+                  int docid;
+                  while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {
+                    resultListDocs++;
+                    resultBits.fastSet(docid + base);
                   }
                 }
               } else {
-                // this should be the same bulk result object if sharing of the docsEnum succeeded
-                DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();
-
-                for (;;) {
-                  int nDocs = docsEnum.read();
-                  if (nDocs == 0) break;
-                  resultListDocs += nDocs;
-                  int[] docArr = bulk.docs.ints;  // this might be movable outside the loop, but perhaps not worth the risk.
-                  int end = bulk.docs.offset + nDocs;
-                  for (int i=bulk.docs.offset; i<end; i++) {
-                    resultBits.fastSet(docArr[i]);
-                  }
+                int docid;
+                while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {
+                  resultListDocs++;
+                  resultBits.fastSet(docid);
                 }
               }
             }
diff --git a/solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java b/solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java
index db986fd..9e2fa0f 100644
--- a/solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java
+++ b/solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java
@@ -872,47 +872,34 @@ public class SolrIndexSearcher extends IndexSearcher implements SolrInfoMBean {
       for (int subindex = 0; subindex<numSubs; subindex++) {
         MultiDocsEnum.EnumWithSlice sub = subs[subindex];
         if (sub.docsEnum == null) continue;
-        DocsEnum.BulkReadResult bulk = sub.docsEnum.getBulkResult();
         int base = sub.slice.start;
-
-        for (;;) {
-          int nDocs = sub.docsEnum.read();
-          if (nDocs == 0) break;
-          int[] docArr = bulk.docs.ints;
-          int end = bulk.docs.offset + nDocs;
-          if (upto + nDocs > docs.length) {
-            if (obs == null) obs = new OpenBitSet(maxDoc());
-            for (int i=bulk.docs.offset; i<end; i++) {
-              obs.fastSet(docArr[i]+base);
-            }
-            bitsSet += nDocs;
-          } else {
-            for (int i=bulk.docs.offset; i<end; i++) {
-              docs[upto++] = docArr[i]+base;
-            }
-          }
-        }
-      }
-    } else {
-      DocsEnum.BulkReadResult bulk = docsEnum.getBulkResult();
-      for (;;) {
-        int nDocs = docsEnum.read();
-        if (nDocs == 0) break;
-        int[] docArr = bulk.docs.ints;
-        int end = bulk.docs.offset + nDocs;
-
-        if (upto + nDocs > docs.length) {
+        int docid;
+        
+        if (largestPossible > docs.length) {
           if (obs == null) obs = new OpenBitSet(maxDoc());
-          for (int i=bulk.docs.offset; i<end; i++) {
-            obs.fastSet(docArr[i]);
+          while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {
+            obs.fastSet(docid + base);
+            bitsSet++;
           }
-          bitsSet += nDocs;
         } else {
-          for (int i=bulk.docs.offset; i<end; i++) {
-            docs[upto++] = docArr[i];
+          while ((docid = sub.docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {
+            docs[upto++] = docid + base;
           }
         }
       }
+    } else {
+      int docid;
+      if (largestPossible > docs.length) {
+        if (obs == null) obs = new OpenBitSet(maxDoc());
+        while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {
+          obs.fastSet(docid);
+          bitsSet++;
+        }
+      } else {
+        while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {
+          docs[upto++] = docid;
+        }
+      }
     }
 
     DocSet result;
@@ -960,15 +947,9 @@ public class SolrIndexSearcher extends IndexSearcher implements SolrInfoMBean {
           }
 
           if (docsEnum != null) {
-            DocsEnum.BulkReadResult readResult = docsEnum.getBulkResult();
-            for (;;) {
-              int n = docsEnum.read();
-              if (n==0) break;
-              int[] arr = readResult.docs.ints;
-              int end = readResult.docs.offset + n;
-              for (int j=readResult.docs.offset; j<end; j++) {
-                collector.collect(arr[j]);
-              }
+            int docid;
+            while ((docid = docsEnum.nextDoc()) != DocIdSetIterator.NO_MORE_DOCS) {
+              collector.collect(docid);
             }
           }
         }

