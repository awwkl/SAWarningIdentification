GitDiffStart: 99fd795df2dd4d2545ff20cdb4973187b7fa588a | Tue Oct 26 08:29:35 2010 +0000
diff --git a/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexReader.java b/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexReader.java
index 8fede64..96d9198 100644
--- a/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexReader.java
+++ b/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/InstantiatedIndexReader.java
@@ -31,6 +31,7 @@ import java.util.Comparator;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.FieldSelector;
 import org.apache.lucene.index.*;
+import org.apache.lucene.index.values.DocValues;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.BitVector;
 import org.apache.lucene.util.BytesRef;
@@ -394,6 +395,11 @@ public class InstantiatedIndexReader extends IndexReader {
           public TermsEnum terms() {
             return new InstantiatedTermsEnum(orderedTerms, upto, currentField);
           }
+
+          @Override
+          public DocValues docValues() throws IOException {
+            return null;
+          }
         };
       }
 
@@ -422,6 +428,11 @@ public class InstantiatedIndexReader extends IndexReader {
           }
         };
       }
+
+      @Override
+      public DocValues docValues(String field) throws IOException {
+        return null;
+      }
     };
   }
 
diff --git a/lucene/contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java b/lucene/contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java
index 11dd692..5072cf3 100644
--- a/lucene/contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java
+++ b/lucene/contrib/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java
@@ -48,6 +48,7 @@ import org.apache.lucene.index.TermFreqVector;
 import org.apache.lucene.index.TermPositionVector;
 import org.apache.lucene.index.TermVectorMapper;
 import org.apache.lucene.index.FieldInvertState;
+import org.apache.lucene.index.values.DocValues;
 import org.apache.lucene.search.Collector;
 import org.apache.lucene.search.IndexSearcher;
 import org.apache.lucene.search.Query;
@@ -789,6 +790,12 @@ public class MemoryIndex implements Serializable {
             public TermsEnum terms() {
               return new MemoryTermsEnum(sortedFields[upto].getValue());
             }
+
+            @Override
+            public DocValues docValues() throws IOException {
+              // TODO 
+              throw new UnsupportedOperationException("not implemented");
+            }
           };
         }
 
@@ -819,6 +826,12 @@ public class MemoryIndex implements Serializable {
             };
           }
         }
+
+        @Override
+        public DocValues docValues(String field) throws IOException {
+          // TODO 
+          throw new UnsupportedOperationException("not implemented");
+        }
       };
     }
 
diff --git a/lucene/src/java/org/apache/lucene/index/DirectoryReader.java b/lucene/src/java/org/apache/lucene/index/DirectoryReader.java
index 2d29a24..9863fb9 100644
--- a/lucene/src/java/org/apache/lucene/index/DirectoryReader.java
+++ b/lucene/src/java/org/apache/lucene/index/DirectoryReader.java
@@ -36,14 +36,7 @@ import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.Lock;
 import org.apache.lucene.store.LockObtainFailedException;
 import org.apache.lucene.index.codecs.CodecProvider;
-import org.apache.lucene.index.values.Reader;
-import org.apache.lucene.index.values.Values;
-import org.apache.lucene.index.values.ValuesEnum;
-import org.apache.lucene.index.values.Reader.Source;
-import org.apache.lucene.util.AttributeSource;
 import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.FloatsRef;
-import org.apache.lucene.util.LongsRef;
 import org.apache.lucene.util.ReaderUtil;
 import org.apache.lucene.util.BytesRef;
 
@@ -996,264 +989,7 @@ class DirectoryReader extends IndexReader implements Cloneable {
     }
 
     return commits;
-  }
-  
-  public Reader getIndexValues(String field) {
-    ensureOpen();
-    if (subReaders.length == 1) {
-      return subReaders[0].getIndexValues(field);
-    } 
-    return new MultiValueReader(field);
-  }
-  
-  private class MultiValueReader extends Reader {
-
-    private String id;
-    private Values value;
-
-    public MultiValueReader(String id) {
-      this.id = id;
-      for (SegmentReader reader : subReaders) {
-        FieldInfo fieldInfo = reader.fieldInfos().fieldInfo(id);
-        if(fieldInfo != null){
-          value = fieldInfo.getIndexValues();
-          break;
-        }
-      }
-    }
-
-    @Override
-    public ValuesEnum getEnum(AttributeSource source) throws IOException {
-      return new MultiValuesEnum(id, value);
-    }
-
-    @Override
-    public Source load() throws IOException {
-      return new MultiSource(id);
-    }
-
-    public void close() throws IOException {
-      //      
-    }
-    
-  }
-  
-  private class MultiValuesEnum extends ValuesEnum {
-    private int numDocs_ = 0;
-    private int pos = -1;
-    private int start = 0;
-    private final String id;
-    private final ValuesEnum[] enumCache;
-    private ValuesEnum current;
-
-    protected MultiValuesEnum(String id, Values enumType) {
-      super(enumType);
-      enumCache = new ValuesEnum[subReaders.length];
-      this.id = id;
-    }
-
-    @Override
-    public void close() throws IOException {
-      for (ValuesEnum valuesEnum : enumCache) {
-        if(valuesEnum != null)
-          valuesEnum.close();
-      }
-    }
-
-    @Override
-    public int advance( int target) throws IOException {
-      int n = target - start;
-      do {
-        if(target >= maxDoc)
-          return pos = NO_MORE_DOCS;
-        if (n >= numDocs_) {
-          int idx = readerIndex(target);
-          if (enumCache[idx] == null) {
-            try {
-              Reader indexValues = subReaders[idx].getIndexValues(id);
-              if (indexValues != null) // nocommit does that work with default
-                // values?
-                enumCache[idx] = indexValues.getEnum(this.attributes());
-              else
-                enumCache[idx] = new DummyEnum(this.attributes(),
-                    subReaders[idx].maxDoc(), attr.type());
-            } catch (IOException ex) {
-              // nocommit what to do here?
-              throw new RuntimeException(ex);
-            }
-          }
-          current = enumCache[idx];
-          start = starts[idx];
-          numDocs_ = subReaders[idx].maxDoc();
-          n = target - start;
-        }
-        target = start+numDocs_;
-      } while ((n = current.advance(n)) == NO_MORE_DOCS);
-      return pos = start+current.docID();
-    }
-
-
-    @Override
-    public int docID() {
-      return pos;
-    }
-
-    @Override
-    public int nextDoc() throws IOException {
-      return advance(pos+1);
-    }
-  }
-  
-  private class MultiSource extends Source {
-    private int numDocs_ = 0;
-    private int start = 0;
-    private Source current;
-    private final String id;
-    
-    MultiSource(String id) {
-      this.id = id;
-    }
-    
-    public long ints(int docID) {
-      int n = docID - start;
-      if(n >= numDocs_) {
-        int idx = readerIndex(docID);
-        try{
-        current = subReaders[idx].getIndexValuesCache().getInts(id);
-        if(current == null) //nocommit does that work with default values?
-          current = new DummySource();
-        }catch(IOException ex) {
-          // nocommit what to do here?
-          throw new RuntimeException(ex);
-        }
-        start = starts[idx];
-        numDocs_ = subReaders[idx].maxDoc();
-        n = docID - start;
-      }
-      return current.ints(n);
-    }
-
-    public double floats(int docID) {
-      int n = docID - start;
-      if(n >= numDocs_) {
-          int idx = readerIndex(docID);
-          try{
-          current = subReaders[idx].getIndexValuesCache().getFloats(id);
-          if(current == null) //nocommit does that work with default values?
-            current = new DummySource();
-          }catch(IOException ex) {
-            // nocommit what to do here?
-            throw new RuntimeException(ex);
-          }
-          numDocs_ = subReaders[idx].maxDoc();
-
-          start = starts[idx];
-          n = docID - start;
-      }
-      return current.floats(n);
-    }
-
-    public BytesRef bytes(int docID) {
-      int n = docID - start;
-      if(n >= numDocs_) {
-        int idx = readerIndex(docID);
-        try{
-        current = subReaders[idx].getIndexValuesCache().getBytes(id);
-        if(current == null) //nocommit does that work with default values?
-          current = new DummySource();
-        }catch(IOException ex) {
-          // nocommit what to do here?
-          throw new RuntimeException(ex);
-        }
-        numDocs_ = subReaders[idx].maxDoc();
-        start = starts[idx];
-        n = docID - start;
-      }
-      return current.bytes(n);
-    }
-    
-    public long ramBytesUsed() {
-      return current.ramBytesUsed();
-    }
-    
-  }
-
-  private static class DummySource extends Source {
-    private final BytesRef ref = new BytesRef();
-    @Override
-    public BytesRef bytes(int docID) {
-      return ref;
-    }
-
-    
-    @Override
-    public double floats(int docID) {
-      return 0.0d;
-    }
-
-    @Override
-    public long ints(int docID) {
-      return 0;
-    }
-
-    public long ramBytesUsed() {
-      return 0;
-    }
-  }
-  
-  private static class DummyEnum extends ValuesEnum {
-    private int pos = -1;
-    private final int maxDoc;
-    
-    public DummyEnum(AttributeSource source, int maxDoc, Values type) {
-      super(source, type);
-      this.maxDoc = maxDoc;
-      switch (type) {
-      case BYTES_VAR_STRAIGHT:
-      case BYTES_FIXED_STRAIGHT:
-      case BYTES_FIXED_DEREF:
-      case BYTES_FIXED_SORTED:
-      case BYTES_VAR_DEREF:
-      case BYTES_VAR_SORTED:
-        // nocommit - this is not correct for Fixed_straight
-        BytesRef bytes = attr.bytes();
-        bytes.length = 0;
-        bytes.offset = 0;
-        break;
-      case PACKED_INTS:
-      case PACKED_INTS_FIXED:
-        LongsRef ints = attr.ints();
-        ints.set(0);
-        break;
-
-      case SIMPLE_FLOAT_4BYTE:
-      case SIMPLE_FLOAT_8BYTE:
-        FloatsRef floats = attr.floats();
-        floats.set(0d);
-        break;
-      default:
-        throw new IllegalArgumentException("unknown Values type: " + type);
-      }
-    }
-    @Override
-    public void close() throws IOException {
-    }
-
-    @Override
-    public int advance(int target) throws IOException {
-      return pos = (pos < maxDoc ? target: NO_MORE_DOCS);
-    }
-    @Override
-    public int docID() {
-      return pos;
-    }
-    @Override
-    public int nextDoc() throws IOException {
-      return advance(pos+1);
-    }
-    
-  }
-  
+  }  
   
   private static final class ReaderCommit extends IndexCommit {
     private String segmentsFileName;
diff --git a/lucene/src/java/org/apache/lucene/index/DocFieldProcessor.java b/lucene/src/java/org/apache/lucene/index/DocFieldProcessor.java
index dee8168..4db1363 100644
--- a/lucene/src/java/org/apache/lucene/index/DocFieldProcessor.java
+++ b/lucene/src/java/org/apache/lucene/index/DocFieldProcessor.java
@@ -18,11 +18,13 @@ package org.apache.lucene.index;
  */
 
 import org.apache.lucene.store.Directory;
+import org.apache.lucene.index.codecs.FieldsConsumer;
 import org.apache.lucene.index.values.Ints;
 import org.apache.lucene.index.values.Floats;
 import org.apache.lucene.index.values.Bytes;
 import org.apache.lucene.index.values.ValuesAttribute;
 import org.apache.lucene.index.values.Writer;
+import org.apache.lucene.index.values.codec.DocValuesConsumer;
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.FloatsRef;
 import org.apache.lucene.util.LongsRef;
@@ -48,154 +50,33 @@ final class DocFieldProcessor extends DocConsumer {
   final FieldInfos fieldInfos = new FieldInfos();
   final DocFieldConsumer consumer;
   final StoredFieldsWriter fieldsWriter;
-  final private Map<String,IndexValuesProcessor> indexValues = new HashMap<String,IndexValuesProcessor>();
-
-  synchronized IndexValuesProcessor getProcessor(Directory dir, String segment, String name, ValuesAttribute attr, FieldInfo fieldInfo)
-    throws IOException {
-    if(attr == null)
-      return null;
-    IndexValuesProcessor p = indexValues.get(name);
-    if (p == null) {
-        org.apache.lucene.index.values.Values v = attr.type();
-        final String id = segment + "_" + fieldInfo.number;
-        switch(v) {
-        case PACKED_INTS:
-          p = new IntValuesProcessor(dir, id, false);
-          break;
-        case PACKED_INTS_FIXED:
-          p = new IntValuesProcessor(dir, id, true);
-          break;
-        case SIMPLE_FLOAT_4BYTE:
-          p = new FloatValuesProcessor(dir, id, 4);
-          break;
-        case SIMPLE_FLOAT_8BYTE:
-          p = new FloatValuesProcessor(dir, id, 8);
-          break;
-        case BYTES_FIXED_STRAIGHT:
-          p = new BytesValuesProcessor(dir, id, true, null, Bytes.Mode.STRAIGHT);
-          break;
-        case BYTES_FIXED_DEREF:
-          p = new BytesValuesProcessor(dir, id, true, null, Bytes.Mode.DEREF);
-          break;
-        case BYTES_FIXED_SORTED:
-          p = new BytesValuesProcessor(dir, id, true, attr.bytesComparator(), Bytes.Mode.SORTED);
-          break;
-        case BYTES_VAR_STRAIGHT:
-          p = new BytesValuesProcessor(dir, id, false, null, Bytes.Mode.STRAIGHT);
-          break;
-        case BYTES_VAR_DEREF:
-          p = new BytesValuesProcessor(dir, id, false, null, Bytes.Mode.DEREF);
-          break;
-        case BYTES_VAR_SORTED:
-          p = new BytesValuesProcessor(dir, id, false, attr.bytesComparator(), Bytes.Mode.SORTED);
-          break;
-        }
-        fieldInfo.setIndexValues(v);
-        indexValues.put(name, p);
-    }
-
-    return p;
-  }
-
-  static abstract class IndexValuesProcessor {
-    public abstract void add(int docID, String name, ValuesAttribute attr) throws IOException;
-    public abstract void finish(int docCount) throws IOException;
-    public abstract void files(Collection<String> files) throws IOException;
-  }
-
-  static class FloatValuesProcessor extends IndexValuesProcessor {
-    private final Writer writer;
-    private final String id;
-
-    public FloatValuesProcessor(Directory dir, String id, int precision) throws IOException {
-      this.id = id;
-      writer = Floats.getWriter(dir, id, precision);
-    }
-
-    @Override
-    public void add(int docID, String name, ValuesAttribute attr) throws IOException {
-        final FloatsRef floats = attr.floats();
-        if(floats != null) {
-          writer.add(docID, floats.get());
-          return;
-        }
-      throw new IllegalArgumentException("could not extract float/double from field " + name);
-    }
-
-    @Override
-    public void finish(int docCount) throws IOException {
-      writer.finish(docCount);
-    }
-
-    @Override
-    public void files(Collection<String> files) {
-      Floats.files(id, files);
-    }
-  }
-
-  static class IntValuesProcessor extends IndexValuesProcessor {
-    private final Writer writer;
-    private final String id;
-
-    public IntValuesProcessor(Directory dir, String id, boolean fixedArray) throws IOException {
-      this.id = id;
-      writer = Ints.getWriter(dir, id, fixedArray);
-    }
-
-    @Override
-      public void add(int docID, String name, ValuesAttribute attr) throws IOException {
-        final LongsRef ints = attr.ints();
-        if(ints != null) {
-          writer.add(docID, ints.get());
-          return;
-        }
-      throw new IllegalArgumentException("could not extract int/long from field " + name);
-    }
-
-    @Override
-    public void finish(int docCount) throws IOException {
-      writer.finish(docCount);
-    }
-
-    @Override
-    public void files(Collection<String> files) throws IOException {
-      Ints.files(id, files);
-    }
-  }
-
-  static class BytesValuesProcessor extends IndexValuesProcessor {
-    private final Writer writer;
-    private final String id;
-    private final Directory dir;
-
-    public BytesValuesProcessor(Directory dir, String id, boolean fixedSize, Comparator<BytesRef> comp, Bytes.Mode mode) throws IOException {
-      this.id = id;
-      writer = Bytes.getWriter(dir, id, mode,comp, fixedSize);
-      this.dir = dir;
-    }
-
-    // nocommit -- make this thread private and not sync'd
-    @Override
-    public synchronized void add(int docID, String name, ValuesAttribute attr) throws IOException {
-      final BytesRef bytes = attr.bytes();
-      if(bytes != null) {
-        writer.add(docID, bytes);
-        return;
+  final private Map<String,DocValuesConsumer> docValues = new HashMap<String,DocValuesConsumer>();
+  private FieldsConsumer fieldsConsumer; // TODO this should be encapsulated in DocumentsWriter
+
+  synchronized DocValuesConsumer docValuesConsumer(Directory dir,
+      String segment, String name, ValuesAttribute attr, FieldInfo fieldInfo)
+      throws IOException {
+    DocValuesConsumer valuesConsumer;
+    if ((valuesConsumer = docValues.get(name)) == null) {
+      fieldInfo.setIndexValues(attr.type());
+
+      if(fieldsConsumer == null) {
+        /* nocommit -- this is a hack and only works since DocValuesCodec supports initializing the FieldsConsumer twice.
+         * we need to find a way that allows us to obtain a FieldsConsumer per DocumentsWriter. Currently some codecs rely on 
+         * the SegmentsWriteState passed in right at the moment when the segment is flushed (doccount etc) but we need the consumer earlier 
+         * to support docvalues and later on stored fields too.  
+         */
+      SegmentWriteState state = docWriter.segWriteState();
+      fieldsConsumer = state.codec.fieldsConsumer(state);
       }
-      throw new IllegalArgumentException("could not extract byte[] from field " + name);
+      valuesConsumer = fieldsConsumer.addValuesField(fieldInfo);
+      docValues.put(name, valuesConsumer);
     }
+    return valuesConsumer;
 
-    @Override
-    public void finish(int docCount) throws IOException {
-      writer.finish(docCount);
-    }
-
-    @Override
-    public void files(Collection<String> files) throws IOException {
-      Bytes.files(dir, id, files);
-    }
   }
 
+ 
   public DocFieldProcessor(DocumentsWriter docWriter, DocFieldConsumer consumer) {
     this.docWriter = docWriter;
     this.consumer = consumer;
@@ -221,13 +102,17 @@ final class DocFieldProcessor extends DocConsumer {
     fieldsWriter.flush(state);
     consumer.flush(childThreadsAndFields, state);
 
-    for(IndexValuesProcessor p : indexValues.values()) {
+    for(DocValuesConsumer p : docValues.values()) {
       if (p != null) {
         p.finish(state.numDocs);
         p.files(state.flushedFiles);
       }
     }
-    indexValues.clear();
+    docValues.clear();
+    if(fieldsConsumer != null) {
+      fieldsConsumer.close(); // nocommit this should go away
+      fieldsConsumer = null;
+    }
 
     // Important to save after asking consumer to flush so
     // consumer can alter the FieldInfo* if necessary.  EG,
diff --git a/lucene/src/java/org/apache/lucene/index/DocFieldProcessorPerThread.java b/lucene/src/java/org/apache/lucene/index/DocFieldProcessorPerThread.java
index 56e7dea..0f2fed9 100644
--- a/lucene/src/java/org/apache/lucene/index/DocFieldProcessorPerThread.java
+++ b/lucene/src/java/org/apache/lucene/index/DocFieldProcessorPerThread.java
@@ -20,14 +20,12 @@ package org.apache.lucene.index;
 import java.util.Collection;
 import java.util.HashSet;
 import java.util.List;
-import java.util.Set;
-import java.util.Map.Entry;
 import java.io.IOException;
 
-import org.apache.lucene.analysis.TokenStream;
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.Fieldable;
 import org.apache.lucene.index.values.ValuesAttribute;
+import org.apache.lucene.index.values.codec.DocValuesConsumer;
 import org.apache.lucene.util.ArrayUtil;
 import org.apache.lucene.util.AttributeSource;
 import org.apache.lucene.util.RamUsageEstimator;
@@ -255,17 +253,16 @@ final class DocFieldProcessorPerThread extends DocConsumerPerThread {
       final DocFieldProcessorPerField perField = fields[i];
       final Fieldable fieldable = perField.fields[0];
       perField.consumer.processFields(perField.fields, perField.fieldCount);
+     
       if(!fieldable.hasFieldAttribute())
         continue;
       final AttributeSource attrSource = fieldable.getFieldAttributes();
       if(!attrSource.hasAttribute(ValuesAttribute.class))
         continue;
       final ValuesAttribute attribute = attrSource.getAttribute(ValuesAttribute.class);
-      final DocFieldProcessor.IndexValuesProcessor processor = docFieldProcessor
-          .getProcessor(docState.docWriter.directory,
+      final DocValuesConsumer consumer = docFieldProcessor.docValuesConsumer(docState.docWriter.directory,
               docState.docWriter.segment, fieldable.name(), attribute, perField.fieldInfo);
-      if (processor != null)
-        processor.add(docState.docID, fieldable.name(), attribute);
+      consumer.add(docState.docID, attribute);
     }
     if (docState.maxTermPrefix != null && docState.infoStream != null) {
       docState.infoStream.println("WARNING: document contains at least one immense term (whose UTF8 encoding is longer than the max length " + DocumentsWriter.MAX_TERM_LENGTH_UTF8 + "), all of which were skipped.  Please correct the analyzer to not produce such terms.  The prefix of the first immense term is: '" + docState.maxTermPrefix + "...'"); 
diff --git a/lucene/src/java/org/apache/lucene/index/DocumentsWriter.java b/lucene/src/java/org/apache/lucene/index/DocumentsWriter.java
index 27784c2..acc20d4 100644
--- a/lucene/src/java/org/apache/lucene/index/DocumentsWriter.java
+++ b/lucene/src/java/org/apache/lucene/index/DocumentsWriter.java
@@ -604,9 +604,13 @@ final class DocumentsWriter {
 
   synchronized private void initFlushState(boolean onlyDocStore) {
     initSegmentName(onlyDocStore);
-    flushState = new SegmentWriteState(infoStream, directory, segment, docFieldProcessor.fieldInfos,
-                                       docStoreSegment, numDocsInRAM, numDocsInStore, writer.getConfig().getTermIndexInterval(),
-                                       writer.codecs);
+    flushState = segWriteState();
+  }
+  
+  SegmentWriteState segWriteState() { 
+    return new SegmentWriteState(infoStream, directory, segment, docFieldProcessor.fieldInfos,
+        docStoreSegment, numDocsInRAM, numDocsInStore, writer.getConfig().getTermIndexInterval(),
+        writer.codecs);
   }
 
   /** Returns the codec used to flush the last segment */
diff --git a/lucene/src/java/org/apache/lucene/index/FieldInfo.java b/lucene/src/java/org/apache/lucene/index/FieldInfo.java
index d0bdd1c..d752987 100644
--- a/lucene/src/java/org/apache/lucene/index/FieldInfo.java
+++ b/lucene/src/java/org/apache/lucene/index/FieldInfo.java
@@ -103,7 +103,7 @@ public final class FieldInfo {
     }
   }
 
-  Values getIndexValues() {
+  public Values getIndexValues() {
     return indexValues;
   }
 }
diff --git a/lucene/src/java/org/apache/lucene/index/Fields.java b/lucene/src/java/org/apache/lucene/index/Fields.java
index a14ca1d..f3fe654 100644
--- a/lucene/src/java/org/apache/lucene/index/Fields.java
+++ b/lucene/src/java/org/apache/lucene/index/Fields.java
@@ -19,6 +19,8 @@ package org.apache.lucene.index;
 
 import java.io.IOException;
 
+import org.apache.lucene.index.values.DocValues;
+
 /** Flex API for access to fields and terms
  *  @lucene.experimental */
 
@@ -31,6 +33,16 @@ public abstract class Fields {
   /** Get the {@link Terms} for this field.  This may return
    *  null if the field does not exist. */
   public abstract Terms terms(String field) throws IOException;
+  
+  /**
+   * Returns {@link DocValues} for the current field.
+   * 
+   * @param field the field name
+   * @return the {@link DocValues} for this field or <code>null</code> if not
+   *         applicable.
+   * @throws IOException
+   */
+  public abstract DocValues docValues(String field) throws IOException;
 
   public final static Fields[] EMPTY_ARRAY = new Fields[0];
 }
diff --git a/lucene/src/java/org/apache/lucene/index/FieldsEnum.java b/lucene/src/java/org/apache/lucene/index/FieldsEnum.java
index 4a2d2dc..e3112ca 100644
--- a/lucene/src/java/org/apache/lucene/index/FieldsEnum.java
+++ b/lucene/src/java/org/apache/lucene/index/FieldsEnum.java
@@ -19,6 +19,7 @@ package org.apache.lucene.index;
 
 import java.io.IOException;
 
+import org.apache.lucene.index.values.DocValues;
 import org.apache.lucene.index.values.ValuesEnum;
 import org.apache.lucene.util.AttributeSource;
 
@@ -57,6 +58,16 @@ public abstract class FieldsEnum {
    *  will not return null. */
   public abstract TermsEnum terms() throws IOException;
   
+  /**
+   * Returns {@link DocValues} for the current field.
+   * 
+   * @return the {@link DocValues} for this field or <code>null</code> if not
+   *         applicable.
+   * @throws IOException
+   */
+  public abstract DocValues docValues() throws IOException;
+
+  
   public final static FieldsEnum[] EMPTY_ARRAY = new FieldsEnum[0];
 
   /** Provides zero fields */
@@ -71,5 +82,10 @@ public abstract class FieldsEnum {
     public TermsEnum terms() {
       throw new IllegalStateException("this method should never be called");
     }
+
+    @Override
+    public DocValues docValues() throws IOException {
+      throw new IllegalStateException("this method should never be called");
+    }
   };
 }
diff --git a/lucene/src/java/org/apache/lucene/index/FilterIndexReader.java b/lucene/src/java/org/apache/lucene/index/FilterIndexReader.java
index 0731a1c..838e939 100644
--- a/lucene/src/java/org/apache/lucene/index/FilterIndexReader.java
+++ b/lucene/src/java/org/apache/lucene/index/FilterIndexReader.java
@@ -19,6 +19,7 @@ package org.apache.lucene.index;
 
 import org.apache.lucene.document.Document;
 import org.apache.lucene.document.FieldSelector;
+import org.apache.lucene.index.values.DocValues;
 import org.apache.lucene.index.values.ValuesEnum;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.Bits;
@@ -59,6 +60,11 @@ public class FilterIndexReader extends IndexReader {
     public Terms terms(String field) throws IOException {
       return in.terms(field);
     }
+
+    @Override
+    public DocValues docValues(String field) throws IOException {
+      return in.docValues(field);
+    }
   }
 
   /** Base class for filtering {@link Terms}
@@ -117,6 +123,11 @@ public class FilterIndexReader extends IndexReader {
     public TermsEnum terms() throws IOException {
       return in.terms();
     }
+
+    @Override
+    public DocValues docValues() throws IOException {
+      return in.docValues();
+    }
   }
 
   /** Base class for filtering {@link TermsEnum} implementations. */
diff --git a/lucene/src/java/org/apache/lucene/index/IndexReader.java b/lucene/src/java/org/apache/lucene/index/IndexReader.java
index c28f138..2cb8d6d 100644
--- a/lucene/src/java/org/apache/lucene/index/IndexReader.java
+++ b/lucene/src/java/org/apache/lucene/index/IndexReader.java
@@ -22,7 +22,7 @@ import org.apache.lucene.document.FieldSelector;
 import org.apache.lucene.search.Similarity;
 import org.apache.lucene.index.codecs.CodecProvider;
 import org.apache.lucene.index.values.Cache;
-import org.apache.lucene.index.values.Reader;
+import org.apache.lucene.index.values.DocValues;
 import org.apache.lucene.store.*;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.BytesRef;
@@ -117,6 +117,9 @@ public abstract class IndexReader implements Cloneable,Closeable {
     public static final FieldOption TERMVECTOR_WITH_OFFSET = new FieldOption ("TERMVECTOR_WITH_OFFSET");
     /** All fields with termvectors with offset values and position values enabled */
     public static final FieldOption TERMVECTOR_WITH_POSITION_OFFSET = new FieldOption ("TERMVECTOR_WITH_POSITION_OFFSET");
+    /** All fields holding doc values */
+    public static final FieldOption DOC_VALUES = new FieldOption ("DOC_VALUES");
+
   }
 
   private boolean closed;
@@ -1374,10 +1377,13 @@ public abstract class IndexReader implements Cloneable,Closeable {
   public int getTermInfosIndexDivisor() {
     throw new UnsupportedOperationException("This reader does not support this method.");
   }
-
-  // nocommit -- should this expose the iterator API via Fields and access Source only via getIndexValuesCache?
-  public Reader getIndexValues(String field) {
-    throw new UnsupportedOperationException();
+  
+  public DocValues docValues(String field) throws IOException {
+    final Fields fields = fields();
+    if (fields == null) {
+      return null;
+    }
+    return fields.docValues(field);
   }
 
   private final Cache indexValuesCache = new Cache(this);
diff --git a/lucene/src/java/org/apache/lucene/index/MultiFields.java b/lucene/src/java/org/apache/lucene/index/MultiFields.java
index fc3beb7..f642383 100644
--- a/lucene/src/java/org/apache/lucene/index/MultiFields.java
+++ b/lucene/src/java/org/apache/lucene/index/MultiFields.java
@@ -22,6 +22,10 @@ import java.util.Map;
 import java.util.HashMap;
 import java.util.List;
 import java.util.ArrayList;
+
+import org.apache.lucene.index.values.DocValues;
+import org.apache.lucene.index.values.MultiDocValues;
+import org.apache.lucene.index.values.MultiDocValues.DocValuesIndex;
 import org.apache.lucene.util.ReaderUtil;
 import org.apache.lucene.util.ReaderUtil.Gather;  // for javadocs
 import org.apache.lucene.util.Bits;
@@ -46,6 +50,7 @@ public final class MultiFields extends Fields {
   private final Fields[] subs;
   private final ReaderUtil.Slice[] subSlices;
   private final Map<String,Terms> terms = new HashMap<String,Terms>();
+  private final Map<String,DocValues> docValues = new HashMap<String,DocValues>();
 
   /** Returns a single {@link Fields} instance for this
    *  reader, merging fields/terms/docs/positions on the
@@ -186,6 +191,12 @@ public final class MultiFields extends Fields {
       return fields.terms(field);
     }
   }
+  
+  /**  This method may return null if the field does not exist.*/
+  public static DocValues getDocValues(IndexReader r, String field) throws IOException {
+    final Fields fields = getFields(r);
+    return fields == null? null: fields.docValues(field);
+  }
 
   /** Returns {@link DocsEnum} for the specified field &
    *  term.  This may return null if the term does not
@@ -270,5 +281,35 @@ public final class MultiFields extends Fields {
 
     return result;
   }
+  
+  @Override
+  public DocValues docValues(String field) throws IOException {
+    final DocValues result;
+
+    if (!docValues.containsKey(field)) {
+
+      // Lazy init: first time this field is requested, we
+      // create & add to docValues:
+      final List<MultiDocValues.DocValuesIndex> subs2 = new ArrayList<MultiDocValues.DocValuesIndex>();
+      final List<ReaderUtil.Slice> slices2 = new ArrayList<ReaderUtil.Slice>();
+
+      // Gather all sub-readers that share this field
+      for(int i=0;i<subs.length;i++) {
+        final DocValues values = subs[i].docValues(field);
+        if (values != null) {
+          subs2.add(new MultiDocValues.DocValuesIndex(values, i));
+          slices2.add(subSlices[i]);
+        }
+      }
+      result = subs2.isEmpty()?null: new MultiDocValues(subs2.toArray(DocValuesIndex.EMPTY_ARRAY),
+                                slices2.toArray(ReaderUtil.Slice.EMPTY_ARRAY));
+      docValues.put(field, result);
+    } else {
+      result = docValues.get(field);
+    }
+
+    return result;  }
+  
+ 
 }
 
diff --git a/lucene/src/java/org/apache/lucene/index/MultiFieldsEnum.java b/lucene/src/java/org/apache/lucene/index/MultiFieldsEnum.java
index bd9856d..cf534ed 100644
--- a/lucene/src/java/org/apache/lucene/index/MultiFieldsEnum.java
+++ b/lucene/src/java/org/apache/lucene/index/MultiFieldsEnum.java
@@ -17,6 +17,8 @@ package org.apache.lucene.index;
  * limitations under the License.
  */
 
+import org.apache.lucene.index.values.DocValues;
+import org.apache.lucene.index.values.MultiDocValues;
 import org.apache.lucene.util.PriorityQueue;
 import org.apache.lucene.util.ReaderUtil;
 
@@ -42,6 +44,8 @@ public final  class MultiFieldsEnum extends FieldsEnum {
 
   // Re-used TermsEnum
   private final MultiTermsEnum terms;
+  private final MultiDocValues docValues;
+
 
   private String currentField;
 
@@ -50,6 +54,7 @@ public final  class MultiFieldsEnum extends FieldsEnum {
   public MultiFieldsEnum(FieldsEnum[] subs, ReaderUtil.Slice[] subSlices) throws IOException {
     terms = new MultiTermsEnum(subSlices);
     queue = new FieldMergeQueue(subs.length);
+    docValues = new MultiDocValues(subSlices);
     top = new FieldsEnumWithSlice[subs.length];
 
     // Init q
@@ -138,5 +143,19 @@ public final  class MultiFieldsEnum extends FieldsEnum {
       return fieldsA.current.compareTo(fieldsB.current) < 0;
     }
   }
+
+  @Override
+  public DocValues docValues() throws IOException {
+    final List<MultiDocValues.DocValuesIndex> values = new ArrayList<MultiDocValues.DocValuesIndex>();
+    for (int i = 0; i < numTop; i++) {
+      final DocValues docValues = top[i].fields.docValues();
+      if (docValues != null) {
+        values.add(new MultiDocValues.DocValuesIndex(docValues,
+            top[i].index));
+      }
+    }
+    // TODO return an empty docvalues instance if values are empty
+    return docValues.reset(values.toArray(MultiDocValues.DocValuesIndex.EMPTY_ARRAY));
+  }
 }
 
diff --git a/lucene/src/java/org/apache/lucene/index/ParallelReader.java b/lucene/src/java/org/apache/lucene/index/ParallelReader.java
index 0aa19ae..e553f29 100644
--- a/lucene/src/java/org/apache/lucene/index/ParallelReader.java
+++ b/lucene/src/java/org/apache/lucene/index/ParallelReader.java
@@ -21,7 +21,9 @@ import org.apache.lucene.document.Document;
 import org.apache.lucene.document.FieldSelector;
 import org.apache.lucene.document.FieldSelectorResult;
 import org.apache.lucene.document.Fieldable;
+import org.apache.lucene.index.values.DocValues;
 import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.Pair;
 import org.apache.lucene.search.FieldCache; // not great (circular); used only to purge FieldCache entry on close
 import org.apache.lucene.util.BytesRef;
 
@@ -174,14 +176,22 @@ public class ParallelReader extends IndexReader {
         return TermsEnum.EMPTY;
       }
     }
+
+    @Override
+    public DocValues docValues() throws IOException {
+      assert currentReader != null;
+      return MultiFields.getDocValues(currentReader, currentField);
+    }
   }
 
   // Single instance of this, per ParallelReader instance
   private class ParallelFields extends Fields {
-    final HashMap<String,Terms> fields = new HashMap<String,Terms>();
+    final HashMap<String,Pair<Terms, DocValues>> fields = new HashMap<String,Pair<Terms, DocValues>>();
 
     public void addField(String field, IndexReader r) throws IOException {
-      fields.put(field, MultiFields.getFields(r).terms(field));
+      Fields multiFields = MultiFields.getFields(r);
+      fields.put(field, new Pair<Terms, DocValues>( multiFields.terms(field),
+          multiFields.docValues(field)));
     }
 
     @Override
@@ -190,11 +200,16 @@ public class ParallelReader extends IndexReader {
     }
     @Override
     public Terms terms(String field) throws IOException {
-      return fields.get(field);
+      return fields.get(field).cur;
     }
-  }
 
-  @Override
+    @Override
+    public DocValues docValues(String field) throws IOException {
+      return fields.get(field).cud;
+    }
+  }
+  
+   @Override
   public Bits getDeletedDocs() {
     return MultiFields.getDeletedDocs(readers.get(0));
   }
diff --git a/lucene/src/java/org/apache/lucene/index/SegmentInfo.java b/lucene/src/java/org/apache/lucene/index/SegmentInfo.java
index e880465..830072e 100644
--- a/lucene/src/java/org/apache/lucene/index/SegmentInfo.java
+++ b/lucene/src/java/org/apache/lucene/index/SegmentInfo.java
@@ -478,12 +478,7 @@ public final class SegmentInfo {
     if (delFileName != null && (delGen >= YES || dir.fileExists(delFileName))) {
       fileSet.add(delFileName);
     }
-    //nocommit - is there a better way to get all the dat / idx files?
-    for(String file : dir.listAll()) {
-      if(file.startsWith(name) && (file.endsWith("dat") || file.endsWith("idx"))){
-        fileSet.add(file);
-      }
-    }
+   
     if (normGen != null) {
       for (int i = 0; i < normGen.length; i++) {
         long gen = normGen[i];
diff --git a/lucene/src/java/org/apache/lucene/index/SegmentMerger.java b/lucene/src/java/org/apache/lucene/index/SegmentMerger.java
index 4546b3d..9557795 100644
--- a/lucene/src/java/org/apache/lucene/index/SegmentMerger.java
+++ b/lucene/src/java/org/apache/lucene/index/SegmentMerger.java
@@ -33,7 +33,7 @@ import org.apache.lucene.index.codecs.MergeState;
 import org.apache.lucene.index.codecs.FieldsConsumer;
 import org.apache.lucene.index.values.Bytes;
 import org.apache.lucene.index.values.Ints;
-import org.apache.lucene.index.values.Reader;
+import org.apache.lucene.index.values.DocValues;
 import org.apache.lucene.index.values.Floats;
 import org.apache.lucene.index.values.Values;
 import org.apache.lucene.index.values.Writer;
@@ -162,9 +162,6 @@ final class SegmentMerger {
 
     if (mergeDocStores && fieldInfos.hasVectors())
       mergeVectors();
-
-    mergeIndexValues();
-
     return mergedDocs;
   }
 
@@ -178,12 +175,6 @@ final class SegmentMerger {
       reader.close();
     }
   }
-  
-  private void addIfExists(Set<String> files, String file, Directory dir) throws IOException{
-    if(dir.fileExists(file)){
-      files.add(file);
-    }
-  }
 
   final List<String> createCompoundFile(String fileName, final SegmentInfo info)
           throws IOException {
@@ -203,14 +194,6 @@ final class SegmentMerger {
     final int numFIs = fieldInfos.size();
     for (int i = 0; i < numFIs; i++) {
       final FieldInfo fi = fieldInfos.fieldInfo(i);
-      // Index Values aka. CSF
-      if (fi.indexValues != null) {
-        addIfExists(fileSet, IndexFileNames.segmentFileName(segment, Integer
-            .toString(fi.number), IndexFileNames.CSF_DATA_EXTENSION), directory);
-        addIfExists(fileSet, IndexFileNames.segmentFileName(segment, Integer
-            .toString(fi.number), IndexFileNames.CSF_INDEX_EXTENSION),
-            directory);
-      }
       if (fi.isIndexed && !fi.omitNorms) {
         fileSet.add(IndexFileNames.segmentFileName(segment, "", IndexFileNames.NORMS_EXTENSION));
         break;
@@ -318,7 +301,7 @@ final class SegmentMerger {
           if (mergedIndexValues == null) {
             merged.setIndexValues(fiIndexValues);
           } else if (mergedIndexValues != fiIndexValues) {
-            // nocommit -- what to do?
+            // TODO -- can we recover from this?
             throw new IllegalStateException("cannot merge field " + fi.name + " indexValues changed from " + mergedIndexValues + " to " + fiIndexValues);
           }
         }
@@ -331,8 +314,7 @@ final class SegmentMerger {
         addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.STORES_PAYLOADS), false, false, false, true, false);
         addIndexed(reader, fieldInfos, reader.getFieldNames(FieldOption.INDEXED), false, false, false, false, false);
         fieldInfos.add(reader.getFieldNames(FieldOption.UNINDEXED), false);
-
-        // nocommit -- how should we handle index values here?
+        fieldInfos.add(reader.getFieldNames(FieldOption.DOC_VALUES), false);
       }
     }
     fieldInfos.write(directory, segment + ".fnm");
@@ -393,77 +375,6 @@ final class SegmentMerger {
     return docCount;
   }
 
-  private void mergeIndexValues() throws IOException {
-    final int numFields = fieldInfos.size();
-    for (int i = 0; i < numFields; i++) {
-      final FieldInfo fieldInfo = fieldInfos.fieldInfo(i);
-      final Values v = fieldInfo.indexValues;
-      // nocommit we need some kind of compatibility notation for values such
-      // that two slighly different segments can be merged eg. fixed vs.
-      // variable byte len or float32 vs. float64
-
-      if (v != null) {
-        int docBase = 0;
-        final List<Writer.MergeState> mergeStates = new ArrayList<Writer.MergeState>();
-        for (IndexReader reader : readers) {
-          Reader r = reader.getIndexValues(fieldInfo.name);
-          if (r != null) {
-            mergeStates.add(new Writer.MergeState(r, docBase, reader
-                .maxDoc(), reader.getDeletedDocs()));
-          }
-          docBase += reader.numDocs();
-        }
-        if (mergeStates.isEmpty()) {
-          continue;
-        }
-        final String id = segment + "_" + fieldInfo.number;
-        final Writer writer;
-        switch (v) {
-        case PACKED_INTS:
-        case PACKED_INTS_FIXED:
-          writer = Ints.getWriter(directory, id, true);
-          break;
-        case SIMPLE_FLOAT_4BYTE:
-          writer = Floats.getWriter(directory, id, 4);
-          break;
-        case SIMPLE_FLOAT_8BYTE:
-          writer = Floats.getWriter(directory, id, 8);
-          break;
-        case BYTES_FIXED_STRAIGHT:
-          writer = Bytes.getWriter(directory, id,
-              Bytes.Mode.STRAIGHT, null, true);
-          break;
-        case BYTES_FIXED_DEREF:
-          writer = Bytes.getWriter(directory, id,
-              Bytes.Mode.DEREF, null, true);
-          break;
-        case BYTES_FIXED_SORTED:
-          // nocommit -- enable setting Comparator
-          writer = Bytes.getWriter(directory, id,
-              Bytes.Mode.SORTED, null, true);
-          break;
-        case BYTES_VAR_STRAIGHT:
-          writer = Bytes.getWriter(directory, id,
-              Bytes.Mode.STRAIGHT, null, false);
-          break;
-        case BYTES_VAR_DEREF:
-          writer = Bytes.getWriter(directory, id,
-              Bytes.Mode.DEREF, null, false);
-          break;
-        case BYTES_VAR_SORTED:
-          // nocommit -- enable setting Comparator
-          writer = Bytes.getWriter(directory, id,
-              Bytes.Mode.SORTED, null, false);
-          break;
-        default:
-          continue;
-        }
-        writer.add(mergeStates);
-        writer.finish(mergedDocs);
-      }
-    }
-  }
-
   private int copyFieldsWithDeletions(final FieldsWriter fieldsWriter, final IndexReader reader,
                                       final FieldsReader matchingFieldsReader)
     throws IOException, MergeAbortedException, CorruptIndexException {
diff --git a/lucene/src/java/org/apache/lucene/index/SegmentReader.java b/lucene/src/java/org/apache/lucene/index/SegmentReader.java
index bbbd90e..9c85466 100644
--- a/lucene/src/java/org/apache/lucene/index/SegmentReader.java
+++ b/lucene/src/java/org/apache/lucene/index/SegmentReader.java
@@ -44,7 +44,7 @@ import org.apache.lucene.index.codecs.CodecProvider;
 import org.apache.lucene.index.codecs.FieldsProducer;
 import org.apache.lucene.index.values.Bytes;
 import org.apache.lucene.index.values.Ints;
-import org.apache.lucene.index.values.Reader;
+import org.apache.lucene.index.values.DocValues;
 import org.apache.lucene.index.values.Floats;
 import org.apache.lucene.index.values.Values;
 import org.apache.lucene.search.FieldCache; // not great (circular); used only to purge FieldCache entry on close
@@ -141,7 +141,6 @@ public class SegmentReader extends IndexReader implements Cloneable {
         // Ask codec for its Fields
         fields = si.getCodec().fieldsProducer(new SegmentReadState(cfsDir, si, fieldInfos, readBufferSize, termsIndexDivisor));
         assert fields != null;
-        openIndexValuesReaders(cfsDir, si);
         success = true;
       } finally {
         if (!success) {
@@ -155,57 +154,8 @@ public class SegmentReader extends IndexReader implements Cloneable {
       // not assigned yet).
       this.origInstance = origInstance;
     }
-
-    final Map<String,Reader> indexValues = new HashMap<String,Reader>();
-
-    // Only opens files... doesn't actually load any values
-    private void openIndexValuesReaders(Directory dir, SegmentInfo si) throws IOException {
-      final int numFields = fieldInfos.size();
-      for(int i=0;i<numFields;i++) {
-        final FieldInfo fieldInfo = fieldInfos.fieldInfo(i);
-        final Values v = fieldInfo.getIndexValues();
-        final String field = fieldInfo.name;
-        final String id = IndexFileNames.segmentFileName(segment, Integer
-            .toString(fieldInfo.number), "");
-        // nocommit - externalize the filenames 
-        if (v != null && dir.fileExists(id+".dat")) {
-          switch(v) {
-          case PACKED_INTS:
-            indexValues.put(field, Ints.getReader(dir, id, false));
-            break;
-          case PACKED_INTS_FIXED:
-            indexValues.put(field, Ints.getReader(dir, id, true));
-            break;
-          case SIMPLE_FLOAT_4BYTE:
-            indexValues.put(field, Floats.getReader(dir, id, si.docCount));
-            break;
-          case SIMPLE_FLOAT_8BYTE:
-            indexValues.put(field, Floats.getReader(dir, id, si.docCount));
-            break;
-          case BYTES_FIXED_STRAIGHT:
-            indexValues.put(field, Bytes.getReader(dir, id, Bytes.Mode.STRAIGHT, true, si.docCount));
-            break;
-          case BYTES_FIXED_DEREF:
-            indexValues.put(field, Bytes.getReader(dir, id, Bytes.Mode.DEREF, true, si.docCount));
-            break;
-          case BYTES_FIXED_SORTED:
-            indexValues.put(field, Bytes.getReader(dir, id, Bytes.Mode.SORTED, true, si.docCount));
-            break;
-          case BYTES_VAR_STRAIGHT:
-            indexValues.put(field, Bytes.getReader(dir, id, Bytes.Mode.STRAIGHT, false, si.docCount));
-            break;
-          case BYTES_VAR_DEREF:
-            indexValues.put(field, Bytes.getReader(dir, id, Bytes.Mode.DEREF, false, si.docCount));
-            break;
-          case BYTES_VAR_SORTED:
-            indexValues.put(field, Bytes.getReader(dir, id, Bytes.Mode.SORTED, false, si.docCount));
-            break;
-          default:
-            throw new IllegalStateException("unrecognized index values mode " + v);
-          }
-        }
-      }
-    }
+    
+    
 
     synchronized TermVectorsReader getTermVectorsReaderOrig() {
       return termVectorsReaderOrig;
@@ -253,17 +203,9 @@ public class SegmentReader extends IndexReader implements Cloneable {
         if (origInstance != null) {
           FieldCache.DEFAULT.purge(origInstance);
         }
-        closeIndexValuesReaders();
       }
     }
 
-    private void closeIndexValuesReaders() throws IOException {
-      for (Reader reader : indexValues.values()) {
-        reader.close();
-      }
-    }
-
-
     synchronized void openDocStores(SegmentInfo si) throws IOException {
 
       assert si.name.equals(segment);
@@ -1026,6 +968,9 @@ public class SegmentReader extends IndexReader implements Cloneable {
                 fieldOption == IndexReader.FieldOption.TERMVECTOR_WITH_POSITION_OFFSET) {
         fieldSet.add(fi.name);
       }
+      else if (fi.indexValues != null && fieldOption == IndexReader.FieldOption.DOC_VALUES) {
+        fieldSet.add(fi.name);
+      }
     }
     return fieldSet;
   }
@@ -1344,9 +1289,9 @@ public class SegmentReader extends IndexReader implements Cloneable {
   public int getTermInfosIndexDivisor() {
     return core.termsIndexDivisor;
   }
-
+  
   @Override
-  public Reader getIndexValues(String field) {
-    return core.indexValues.get(field);
+  public DocValues docValues(String field) throws IOException {
+    return core.fields.docValues(field);
   }
 }
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/FieldsConsumer.java b/lucene/src/java/org/apache/lucene/index/codecs/FieldsConsumer.java
index 8389df0..5bc0b48 100644
--- a/lucene/src/java/org/apache/lucene/index/codecs/FieldsConsumer.java
+++ b/lucene/src/java/org/apache/lucene/index/codecs/FieldsConsumer.java
@@ -20,9 +20,17 @@ package org.apache.lucene.index.codecs;
 import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.Fields;
 import org.apache.lucene.index.FieldsEnum;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.index.values.DocValues;
+import org.apache.lucene.index.values.Writer;
+import org.apache.lucene.index.values.Values;
+import org.apache.lucene.index.values.codec.DocValuesConsumer;
 
 import java.io.IOException;
 import java.io.Closeable;
+import java.util.ArrayList;
+import java.util.List;
 
 /** Abstract API that consumes terms, doc, freq, prox and
  *  payloads postings.  Concrete implementations of this
@@ -35,6 +43,13 @@ public abstract class FieldsConsumer implements Closeable {
 
   /** Add a new field */
   public abstract TermsConsumer addField(FieldInfo field) throws IOException;
+  
+  /** Adds a new DocValuesField */
+  public /*abstract*/ DocValuesConsumer addValuesField(FieldInfo field) throws IOException {
+    throw new UnsupportedOperationException("docvalues are not supported");
+  }
+  
+
 
   /** Called when we are done adding everything. */
   public abstract void close() throws IOException;
@@ -45,8 +60,39 @@ public abstract class FieldsConsumer implements Closeable {
     String field;
     while((field = fieldsEnum.next()) != null) {
       mergeState.fieldInfo = mergeState.fieldInfos.fieldInfo(field);
-      final TermsConsumer termsConsumer = addField(mergeState.fieldInfo);
-      termsConsumer.merge(mergeState, fieldsEnum.terms());
+      assert mergeState.fieldInfo != null : "FieldInfo for field is null: "+ field;
+      TermsEnum terms = fieldsEnum.terms();
+      if(terms != null) {
+        final TermsConsumer termsConsumer = addField(mergeState.fieldInfo);
+        termsConsumer.merge(mergeState, terms);
+      }
+      
+      DocValues docValues = fieldsEnum.docValues();   // fix this - does not work due to multi fields
+      if(docValues != null) {
+      // TODO we need some kind of compatibility notation for values such
+      // that two slighly different segments can be merged eg. fixed vs.
+      // variable byte len or float32 vs. float64
+        int docBase = 0;
+        final List<Writer.MergeState> mergeStates = new ArrayList<Writer.MergeState>();
+        for (IndexReader reader : mergeState.readers) {
+          DocValues r = reader.docValues(mergeState.fieldInfo.name);
+          if (r != null) {
+            mergeStates.add(new Writer.MergeState(r, docBase, reader
+                .maxDoc(), reader.getDeletedDocs()));
+          }
+          docBase += reader.numDocs();
+        }
+        if (mergeStates.isEmpty()) {
+          continue;
+        }
+        final DocValuesConsumer docValuesConsumer = addValuesField(mergeState.fieldInfo);
+        docValuesConsumer.merge(mergeStates);
+        docValuesConsumer.finish(mergeState.mergedDocCount);
+      }
+      
+      // merge doc values
+//   
     }
   }
+ 
 }
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/FieldsProducer.java b/lucene/src/java/org/apache/lucene/index/codecs/FieldsProducer.java
index a378680..a4ce963 100644
--- a/lucene/src/java/org/apache/lucene/index/codecs/FieldsProducer.java
+++ b/lucene/src/java/org/apache/lucene/index/codecs/FieldsProducer.java
@@ -17,10 +17,13 @@ package org.apache.lucene.index.codecs;
  * limitations under the License.
  */
 
-import org.apache.lucene.index.Fields;
-
-import java.io.IOException;
 import java.io.Closeable;
+import java.io.IOException;
+
+import org.apache.lucene.index.Fields;
+import org.apache.lucene.index.FieldsEnum;
+import org.apache.lucene.index.Terms;
+import org.apache.lucene.index.values.DocValues;
 
 /** Abstract API that consumes terms, doc, freq, prox and
  *  payloads postings.  Concrete implementations of this
@@ -33,4 +36,33 @@ import java.io.Closeable;
 public abstract class FieldsProducer extends Fields implements Closeable {
   public abstract void close() throws IOException;
   public abstract void loadTermsIndex(int indexDivisor) throws IOException;
+
+  @Override
+  public DocValues docValues(String field) throws IOException {
+    return null;
+  }
+  
+  public static final FieldsProducer EMPTY = new FieldsProducer() {
+    
+    @Override
+    public Terms terms(String field) throws IOException {
+      return null;
+    }
+    
+    @Override
+    public FieldsEnum iterator() throws IOException {
+      return FieldsEnum.EMPTY;
+    }
+    
+    @Override
+    public void loadTermsIndex(int indexDivisor) throws IOException {
+      
+    }
+    
+    @Override
+    public void close() throws IOException {
+      
+    }
+  };
+  
 }
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/PerFieldCodecWrapper.java b/lucene/src/java/org/apache/lucene/index/codecs/PerFieldCodecWrapper.java
index 8839e8f..cf21d6c 100644
--- a/lucene/src/java/org/apache/lucene/index/codecs/PerFieldCodecWrapper.java
+++ b/lucene/src/java/org/apache/lucene/index/codecs/PerFieldCodecWrapper.java
@@ -35,6 +35,8 @@ import org.apache.lucene.index.FieldInfos;
 import org.apache.lucene.index.SegmentInfo;
 import org.apache.lucene.index.SegmentWriteState;
 import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.values.DocValues;
+import org.apache.lucene.index.values.codec.DocValuesConsumer;
 import org.apache.lucene.store.Directory;
 
 
@@ -112,6 +114,18 @@ public class PerFieldCodecWrapper extends Codec {
         throw err;
       }
     }
+
+    @Override
+    public DocValuesConsumer addValuesField(FieldInfo field) throws IOException {
+      fieldsSeen.add(field.name);
+      Codec codec = getCodec(field.name);
+      FieldsConsumer fields = codecs.get(codec);
+      if (fields == null) {
+        fields = codec.fieldsConsumer(state);
+        codecs.put(codec, fields);
+      }
+      return fields.addValuesField(field);
+    }
   }
 
   private class FieldsReader extends FieldsProducer {
@@ -164,6 +178,11 @@ public class PerFieldCodecWrapper extends Codec {
           return null;
         }
       }
+
+      @Override
+      public DocValues docValues() throws IOException {
+        return codecs.get(getCodec(current)).docValues(current);
+      }
     }
       
     @Override
@@ -207,6 +226,14 @@ public class PerFieldCodecWrapper extends Codec {
         it.next().loadTermsIndex(indexDivisor);
       }
     }
+
+    @Override
+    public DocValues docValues(String field) throws IOException {
+      final Codec codec = getCodec(field);
+      FieldsProducer fields = codecs.get(codec);
+      assert fields != null;
+      return fields.docValues(field);
+    }
   }
 
   public FieldsProducer fieldsProducer(SegmentReadState state)
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/PrefixCodedTermsReader.java b/lucene/src/java/org/apache/lucene/index/codecs/PrefixCodedTermsReader.java
index d3cd2ce..5f5f607 100644
--- a/lucene/src/java/org/apache/lucene/index/codecs/PrefixCodedTermsReader.java
+++ b/lucene/src/java/org/apache/lucene/index/codecs/PrefixCodedTermsReader.java
@@ -41,6 +41,7 @@ import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.util.CodecUtil;
 
 import org.apache.lucene.index.codecs.standard.StandardPostingsReader; // javadocs
+import org.apache.lucene.index.values.DocValues;
 
 /** Handles a terms dict, but decouples all details of
  *  doc/freqs/positions reading to an instance of {@link
@@ -245,6 +246,12 @@ public class PrefixCodedTermsReader extends FieldsProducer {
     public TermsEnum terms() throws IOException {
       return current.iterator();
     }
+
+    @Override
+    public DocValues docValues() throws IOException {
+      // TODO Auto-generated method stub
+      return null;
+    }
   }
 
   private class FieldReader extends Terms implements Closeable {
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/PrefixCodedTermsWriter.java b/lucene/src/java/org/apache/lucene/index/codecs/PrefixCodedTermsWriter.java
index 198ed7d..c992990 100644
--- a/lucene/src/java/org/apache/lucene/index/codecs/PrefixCodedTermsWriter.java
+++ b/lucene/src/java/org/apache/lucene/index/codecs/PrefixCodedTermsWriter.java
@@ -94,7 +94,7 @@ public class PrefixCodedTermsWriter extends FieldsConsumer {
 
   @Override
   public TermsConsumer addField(FieldInfo field) {
-    assert currentField == null || currentField.name.compareTo(field.name) < 0;
+    assert currentField == null || currentField.name.compareTo(field.name) < 0 : "current field name " + (currentField == null? null: currentField.name) + " given: " +field.name;
     currentField = field;
     TermsIndexWriterBase.FieldWriter fieldIndexWriter = termsIndexWriter.addField(field);
     TermsConsumer terms = new TermsWriter(fieldIndexWriter, field, postingsWriter);
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/preflex/PreFlexFields.java b/lucene/src/java/org/apache/lucene/index/codecs/preflex/PreFlexFields.java
index 57072463..61781ae 100644
--- a/lucene/src/java/org/apache/lucene/index/codecs/preflex/PreFlexFields.java
+++ b/lucene/src/java/org/apache/lucene/index/codecs/preflex/PreFlexFields.java
@@ -37,6 +37,7 @@ import org.apache.lucene.index.Terms;
 import org.apache.lucene.index.TermsEnum;
 import org.apache.lucene.index.CompoundFileReader;
 import org.apache.lucene.index.codecs.FieldsProducer;
+import org.apache.lucene.index.values.DocValues;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IndexInput;
 import org.apache.lucene.util.Bits;
@@ -222,6 +223,12 @@ public class PreFlexFields extends FieldsProducer {
       termsEnum.reset(current);
       return termsEnum;
     }
+
+    @Override
+    public DocValues docValues() throws IOException {
+      //DocValues are not available on PreFlex indices
+      return null;
+    }
   }
   
   private class PreTerms extends Terms {
diff --git a/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextFieldsReader.java b/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextFieldsReader.java
index 14c72b8..cc9f7de 100644
--- a/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextFieldsReader.java
+++ b/lucene/src/java/org/apache/lucene/index/codecs/simpletext/SimpleTextFieldsReader.java
@@ -19,6 +19,7 @@ package org.apache.lucene.index.codecs.simpletext;
 
 import org.apache.lucene.util.BytesRef;
 import org.apache.lucene.index.codecs.FieldsProducer;
+import org.apache.lucene.index.values.DocValues;
 import org.apache.lucene.index.SegmentReadState;
 import org.apache.lucene.index.FieldsEnum;
 import org.apache.lucene.index.Terms;
@@ -103,6 +104,12 @@ class SimpleTextFieldsReader extends FieldsProducer {
     public TermsEnum terms() throws IOException {
       return new SimpleTextTermsEnum(in.getFilePointer(), omitTF);
     }
+
+    @Override
+    public DocValues docValues() throws IOException {
+      // TODO Auto-generated method stub
+      return null;
+    }
   }
 
   private class SimpleTextTermsEnum extends TermsEnum {
diff --git a/lucene/src/java/org/apache/lucene/index/values/Bytes.java b/lucene/src/java/org/apache/lucene/index/values/Bytes.java
index 34e7975..bd9fd45 100644
--- a/lucene/src/java/org/apache/lucene/index/values/Bytes.java
+++ b/lucene/src/java/org/apache/lucene/index/values/Bytes.java
@@ -25,8 +25,8 @@ import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.concurrent.atomic.AtomicLong;
 
 import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.values.Reader.SortedSource;
-import org.apache.lucene.index.values.Reader.Source;
+import org.apache.lucene.index.values.DocValues.SortedSource;
+import org.apache.lucene.index.values.DocValues.Source;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.IndexInput;
 import org.apache.lucene.store.IndexOutput;
@@ -47,8 +47,7 @@ import org.apache.lucene.util.CodecUtil;
  * NOTE: Each byte[] must be <= 32768 bytes in length
  * </p>
  */
-//nocommit - add mmap version 
-//nocommti - add bulk copy where possible
+//TODO - add bulk copy where possible
 public final class Bytes {
 
   // don't instantiate!
@@ -59,17 +58,7 @@ public final class Bytes {
     STRAIGHT, DEREF, SORTED
   };
 
-  public static void files(Directory dir, String id, Collection<String> files)
-      throws IOException {
-    files.add(IndexFileNames.segmentFileName(id, "",
-        IndexFileNames.CSF_DATA_EXTENSION));
-    final String idxFile = IndexFileNames.segmentFileName(id, "",
-        IndexFileNames.CSF_INDEX_EXTENSION);
-    if (dir.fileExists(idxFile)) {
-      files.add(idxFile);
-    }
-  }
-
+  
   // nocommit -- i shouldn't have to specify fixed? can
   // track itself & do the write thing at write time?
   public static Writer getWriter(Directory dir, String id, Mode mode,
@@ -101,7 +90,7 @@ public final class Bytes {
   }
 
   // nocommit -- I can peek @ header to determing fixed/mode?
-  public static Reader getReader(Directory dir, String id, Mode mode,
+  public static DocValues getValues(Directory dir, String id, Mode mode,
       boolean fixedSize, int maxDoc) throws IOException {
     if (fixedSize) {
       if (mode == Mode.STRAIGHT) {
@@ -172,6 +161,7 @@ public final class Bytes {
 
   static abstract class BytesWriterBase extends Writer {
 
+
     private final Directory dir;
     private final String id;
     protected IndexOutput idxOut;
@@ -239,13 +229,32 @@ public final class Bytes {
       bytesRef = attr.bytes();
       assert bytesRef != null;
     }
+    
+    @Override
+    public void add(int docID, ValuesAttribute attr) throws IOException {
+      final BytesRef ref;
+      if((ref = attr.bytes()) != null) {
+        add(docID, ref);
+      }
+    }
+
+    @Override
+    public void files(Collection<String> files) throws IOException {
+      files.add(IndexFileNames.segmentFileName(id, "",
+          IndexFileNames.CSF_DATA_EXTENSION));
+      final String idxFile = IndexFileNames.segmentFileName(id, "",
+          IndexFileNames.CSF_INDEX_EXTENSION);
+      if (dir.fileExists(idxFile)) { // TODO is this correct? could be initialized lazy
+        files.add(idxFile);
+      }
+    }
   }
 
   /**
    * Opens all necessary files, but does not read any data in until you call
    * {@link #load}.
    */
-   static abstract class BytesReaderBase extends Reader {
+   static abstract class BytesReaderBase extends DocValues {
     protected final IndexInput idxIn;
     protected final IndexInput datIn;
     protected final int version;
@@ -270,20 +279,15 @@ public final class Bytes {
     }
 
     protected final IndexInput cloneData() {
-      assert !isClosed.get():printEx();
       // is never NULL
       return (IndexInput) datIn.clone();
     }
 
     protected final IndexInput cloneIndex() {
-      assert !isClosed.get():printEx();
       return idxIn == null ? null : (IndexInput) idxIn.clone();
     }
-    private final AtomicBoolean isClosed = new AtomicBoolean(false);
-    Exception ex;
+
     public void close() throws IOException {
-      assert !isClosed.getAndSet(true);
-      ex =new Exception();
       if (datIn != null) {
         datIn.close();
       }
@@ -291,11 +295,6 @@ public final class Bytes {
         idxIn.close();
       }
     }
-    
-    private String printEx() {
-      ex.printStackTrace();
-      return ex.getMessage();
-    }
   }
 
 }
\ No newline at end of file
diff --git a/lucene/src/java/org/apache/lucene/index/values/Cache.java b/lucene/src/java/org/apache/lucene/index/values/Cache.java
index 3f3b9dc..711e11c 100644
--- a/lucene/src/java/org/apache/lucene/index/values/Cache.java
+++ b/lucene/src/java/org/apache/lucene/index/values/Cache.java
@@ -23,8 +23,8 @@ import java.util.HashMap;
 import java.util.Map;
 
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.values.Reader.SortedSource;
-import org.apache.lucene.index.values.Reader.Source;
+import org.apache.lucene.index.values.DocValues.SortedSource;
+import org.apache.lucene.index.values.DocValues.Source;
 import org.apache.lucene.util.BytesRef;
 
 public class Cache {
@@ -42,7 +42,7 @@ public class Cache {
   synchronized public Source getInts(String id) throws IOException {
     Source s = ints.get(id);
     if (s == null) {
-      final Reader indexValues = r.getIndexValues(id);
+      final DocValues indexValues = r.docValues(id);
       if (indexValues == null) {
         return null;
       }
@@ -56,7 +56,7 @@ public class Cache {
   synchronized public Source getFloats(String id) throws IOException {
     Source s = floats.get(id);
     if (s == null) {
-      final Reader indexValues = r.getIndexValues(id);
+      final DocValues indexValues = r.docValues(id);
       if (indexValues == null) {
         return null;
       }
@@ -71,7 +71,7 @@ public class Cache {
       Comparator<BytesRef> comp) throws IOException {
     SortedSource s = sortedBytes.get(id);
     if (s == null) {
-      final Reader indexValues = r.getIndexValues(id);
+      final DocValues indexValues = r.docValues(id);
       if (indexValues == null) {
         return null;
       }
@@ -87,7 +87,7 @@ public class Cache {
   synchronized public Source getBytes(String id) throws IOException {
     Source s = bytes.get(id);
     if (s == null) {
-      final Reader indexValues = r.getIndexValues(id);
+      final DocValues indexValues = r.docValues(id);
       if (indexValues == null) {
         return null;
       }
diff --git a/lucene/src/java/org/apache/lucene/index/values/DocValues.java b/lucene/src/java/org/apache/lucene/index/values/DocValues.java
new file mode 100644
index 0000000..501a2c9
--- /dev/null
+++ b/lucene/src/java/org/apache/lucene/index/values/DocValues.java
@@ -0,0 +1,113 @@
+package org.apache.lucene.index.values;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+import java.io.Closeable;
+import java.io.IOException;
+import java.util.Comparator;
+
+import org.apache.lucene.util.AttributeSource;
+import org.apache.lucene.util.BytesRef;
+
+public abstract class DocValues implements Closeable {
+  
+  
+  public static final DocValues[] EMPTY_ARRAY = new DocValues[0];
+
+  public ValuesEnum getEnum() throws IOException{
+    return getEnum(null);
+  }
+
+  public abstract ValuesEnum getEnum(AttributeSource attrSource) throws IOException;
+
+  public abstract Source load() throws IOException;
+
+  public SortedSource loadSorted(Comparator<BytesRef> comparator) throws IOException {
+    throw new UnsupportedOperationException();
+  }
+  
+  public abstract Values type();
+  
+
+  /**
+   * Source of integer (returned as java long), per document. The underlying
+   * implementation may use different numbers of bits per value; long is only
+   * used since it can handle all precisions.
+   */
+  public static abstract class Source {
+
+    public long ints(int docID) {
+      throw new UnsupportedOperationException("ints are not supported");
+    }
+
+    public double floats(int docID) {
+      throw new UnsupportedOperationException("floats are not supported");
+    }
+
+    public BytesRef bytes(int docID) {
+      throw new UnsupportedOperationException("bytes are not supported");
+    }
+    
+    /** Returns number of unique values.  Some impls may
+     * throw UnsupportedOperationException. */
+    public int getValueCount() {
+      throw new UnsupportedOperationException();
+    }
+    
+    public ValuesEnum getEnum() throws IOException{
+      return getEnum(null);
+    }
+    
+    // nocommit - enable obtaining enum from source since this is already in memory
+    public /*abstract*/ ValuesEnum getEnum(AttributeSource attrSource) throws IOException {
+      throw new UnsupportedOperationException();
+    }
+
+    public abstract long ramBytesUsed();
+  }
+
+  public static abstract class SortedSource extends Source {
+
+    @Override
+    public BytesRef bytes(int docID) {
+      return getByOrd(ord(docID));
+    }
+
+    /**
+     * Returns ord for specified docID. If this docID had not been added to the
+     * Writer, the ord is 0. Ord is dense, ie, starts at 0, then increments by 1
+     * for the next (as defined by {@link Comparator} value.
+     */
+    public abstract int ord(int docID);
+
+    /** Returns value for specified ord. */
+    public abstract BytesRef getByOrd(int ord);
+
+    public static class LookupResult {
+      public boolean found;
+      public int ord;
+    }
+
+    /**
+     * Finds the largest ord whose value is <= the requested value. If
+     * {@link LookupResult#found} is true, then ord is an exact match. The
+     * returned {@link LookupResult} may be reused across calls.
+     */
+    public abstract LookupResult getByValue(BytesRef value);
+  }
+  
+}
diff --git a/lucene/src/java/org/apache/lucene/index/values/FixedDerefBytesImpl.java b/lucene/src/java/org/apache/lucene/index/values/FixedDerefBytesImpl.java
index 3cac5b2..7e30711 100644
--- a/lucene/src/java/org/apache/lucene/index/values/FixedDerefBytesImpl.java
+++ b/lucene/src/java/org/apache/lucene/index/values/FixedDerefBytesImpl.java
@@ -257,6 +257,11 @@ class FixedDerefBytesImpl {
       }
 
     }
+
+    @Override
+    public Values type() {
+      return Values.BYTES_FIXED_DEREF;
+    }
   }
 
 }
diff --git a/lucene/src/java/org/apache/lucene/index/values/FixedSortedBytesImpl.java b/lucene/src/java/org/apache/lucene/index/values/FixedSortedBytesImpl.java
index 75e26eb..810c6a0 100644
--- a/lucene/src/java/org/apache/lucene/index/values/FixedSortedBytesImpl.java
+++ b/lucene/src/java/org/apache/lucene/index/values/FixedSortedBytesImpl.java
@@ -156,7 +156,7 @@ class FixedSortedBytesImpl {
     }
 
     @Override
-    public org.apache.lucene.index.values.Reader.Source load() throws IOException {
+    public org.apache.lucene.index.values.DocValues.Source load() throws IOException {
       return loadSorted(null);
     }
 
@@ -254,5 +254,10 @@ class FixedSortedBytesImpl {
         // do unsorted
         return new DerefBytesEnum(source, cloneData(), cloneIndex(), CODEC_NAME, size);
     }
+    
+    @Override
+    public Values type() {
+      return Values.BYTES_FIXED_SORTED;
+    }
   }
 }
diff --git a/lucene/src/java/org/apache/lucene/index/values/FixedStraightBytesImpl.java b/lucene/src/java/org/apache/lucene/index/values/FixedStraightBytesImpl.java
index 0056426..3566e33 100644
--- a/lucene/src/java/org/apache/lucene/index/values/FixedStraightBytesImpl.java
+++ b/lucene/src/java/org/apache/lucene/index/values/FixedStraightBytesImpl.java
@@ -217,5 +217,10 @@ class FixedStraightBytesImpl {
         return advance(pos+1);
       }
     }
+    
+    @Override
+    public Values type() {
+      return Values.BYTES_FIXED_STRAIGHT;
+    }
   }
 }
diff --git a/lucene/src/java/org/apache/lucene/index/values/Floats.java b/lucene/src/java/org/apache/lucene/index/values/Floats.java
index 3caccdb..e343565 100644
--- a/lucene/src/java/org/apache/lucene/index/values/Floats.java
+++ b/lucene/src/java/org/apache/lucene/index/values/Floats.java
@@ -28,10 +28,6 @@ public class Floats {
   private static final int INT_ZERO = Float.floatToRawIntBits(0.0f);
   private static final long LONG_ZERO = Double.doubleToRawLongBits(0.0);
 
-  public static void files(String id, Collection<String> files) {
-    files.add(id + "." + IndexFileNames.CSF_DATA_EXTENSION);
-  }
-
   public static Writer getWriter(Directory dir, String id, int precisionBytes)
       throws IOException {
     if (precisionBytes != 4 && precisionBytes != 8) {
@@ -45,12 +41,14 @@ public class Floats {
     }
   }
 
-  public static Reader getReader(Directory dir, String id, int maxDoc)
+  public static DocValues getValues(Directory dir, String id, int maxDoc)
       throws IOException {
     return new FloatsReader(dir, id, maxDoc);
   }
 
   abstract static class FloatsWriter extends Writer {
+
+
     private final Directory dir;
     private final String id;
     private FloatsRef floatsRef;
@@ -81,6 +79,13 @@ public class Floats {
     protected void add(int docID) throws IOException {
       add(docID, floatsRef.get());
     }
+    
+    @Override
+    public void add(int docID, ValuesAttribute attr) throws IOException {
+      final FloatsRef ref;
+      if((ref = attr.floats()) != null)
+      add(docID, ref.get());
+    }
 
     @Override
     protected void setNextAttribute(ValuesAttribute attr) {
@@ -109,6 +114,13 @@ public class Floats {
       } else
         super.merge(state);
     }
+    
+    @Override
+    public void files(Collection<String> files) throws IOException {
+      files.add(IndexFileNames.segmentFileName(id, "",
+          IndexFileNames.CSF_DATA_EXTENSION));
+    }
+
 
   }
 
@@ -203,7 +215,7 @@ public class Floats {
    * Opens all necessary files, but does not read any data in until you call
    * {@link #load}.
    */
-  static class FloatsReader extends Reader {
+  static class FloatsReader extends DocValues {
 
     private final IndexInput datIn;
     private final int precisionBytes;
@@ -303,6 +315,12 @@ public class Floats {
       return precisionBytes == 4 ? new Floats4Enum(source, indexInput, maxDoc)
           : new Floats8EnumImpl(source, indexInput, maxDoc);
     }
+    
+    @Override
+    public Values type() {
+      return precisionBytes == 4 ? Values.SIMPLE_FLOAT_4BYTE
+          : Values.SIMPLE_FLOAT_8BYTE;
+    }
   }
 
   static final class Floats4Enum extends FloatsEnumImpl {
diff --git a/lucene/src/java/org/apache/lucene/index/values/Ints.java b/lucene/src/java/org/apache/lucene/index/values/Ints.java
index a5ea552..9b4e585 100644
--- a/lucene/src/java/org/apache/lucene/index/values/Ints.java
+++ b/lucene/src/java/org/apache/lucene/index/values/Ints.java
@@ -1,32 +1,24 @@
 package org.apache.lucene.index.values;
 
 import java.io.IOException;
-import java.util.Collection;
 
-import org.apache.lucene.index.IndexFileNames;
 import org.apache.lucene.index.values.PackedIntsImpl.IntsReader;
 import org.apache.lucene.index.values.PackedIntsImpl.IntsWriter;
 import org.apache.lucene.store.Directory;
-//nocommit - add mmap version 
-//nocommti - add bulk copy where possible
+//TODO - add bulk copy where possible
 public class Ints {
 
   private Ints() {
   }
   
-  public static void files(String id, Collection<String> files)
-      throws IOException {
-    files.add(IndexFileNames.segmentFileName(id, "",
-        IndexFileNames.CSF_DATA_EXTENSION));
-  }
 
   public static Writer getWriter(Directory dir, String id, boolean useFixedArray)
       throws IOException {
-     //nocommit - implement fixed?!
+     //TODO - implement fixed?!
     return new IntsWriter(dir, id);
   }
 
-  public static Reader getReader(Directory dir, String id, boolean useFixedArray) throws IOException {
+  public static DocValues getValues(Directory dir, String id, boolean useFixedArray) throws IOException {
     return new IntsReader(dir, id);
   }
 }
diff --git a/lucene/src/java/org/apache/lucene/index/values/PackedIntsImpl.java b/lucene/src/java/org/apache/lucene/index/values/PackedIntsImpl.java
index ac84385..64735a6 100644
--- a/lucene/src/java/org/apache/lucene/index/values/PackedIntsImpl.java
+++ b/lucene/src/java/org/apache/lucene/index/values/PackedIntsImpl.java
@@ -17,6 +17,7 @@ package org.apache.lucene.index.values;
  * limitations under the License.
  */
 import java.io.IOException;
+import java.util.Collection;
 
 import org.apache.lucene.index.IndexFileNames;
 import org.apache.lucene.store.Directory;
@@ -38,6 +39,8 @@ class PackedIntsImpl {
   static final int VERSION_CURRENT = VERSION_START;
 
   static class IntsWriter extends Writer {
+   
+
     // nocommit - can we bulkcopy this on a merge?
     private LongsRef intsRef;
     private long[] docToValue;
@@ -125,13 +128,27 @@ class PackedIntsImpl {
     protected void setNextAttribute(ValuesAttribute attr) {
       intsRef = attr.ints();
     }
+    
+    @Override
+    public void add(int docID, ValuesAttribute attr) throws IOException {
+      final LongsRef ref;
+      if((ref = attr.ints()) != null) {
+        add(docID, ref.get());
+      }
+    }
+
+    @Override
+    public void files(Collection<String> files) throws IOException {
+      files.add(IndexFileNames.segmentFileName(id, "",
+          IndexFileNames.CSF_DATA_EXTENSION));      
+    }
   }
 
   /**
    * Opens all necessary files, but does not read any data in until you call
    * {@link #load}.
    */
-  static class IntsReader extends Reader {
+  static class IntsReader extends DocValues {
     private final IndexInput datIn;
 
     protected IntsReader(Directory dir, String id) throws IOException {
@@ -186,6 +203,11 @@ class PackedIntsImpl {
     public ValuesEnum getEnum(AttributeSource source) throws IOException {
       return new IntsEnumImpl(source, (IndexInput) datIn.clone());
     }
+    
+    @Override
+    public Values type() {
+      return Values.PACKED_INTS;
+    }
 
   }
 
diff --git a/lucene/src/java/org/apache/lucene/index/values/Reader.java b/lucene/src/java/org/apache/lucene/index/values/Reader.java
deleted file mode 100644
index 0bbd90f..0000000
--- a/lucene/src/java/org/apache/lucene/index/values/Reader.java
+++ /dev/null
@@ -1,109 +0,0 @@
-package org.apache.lucene.index.values;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-import java.io.Closeable;
-import java.io.IOException;
-import java.util.Comparator;
-
-import org.apache.lucene.util.AttributeSource;
-import org.apache.lucene.util.BytesRef;
-
-public abstract class Reader implements Closeable {
-  
-  
-  public ValuesEnum getEnum() throws IOException{
-    return getEnum(null);
-  }
-
-  public abstract ValuesEnum getEnum(AttributeSource attrSource) throws IOException;
-
-  public abstract Source load() throws IOException;
-
-  public SortedSource loadSorted(Comparator<BytesRef> comparator) throws IOException {
-    throw new UnsupportedOperationException();
-  }
-  
-
-  /**
-   * Source of integer (returned as java long), per document. The underlying
-   * implementation may use different numbers of bits per value; long is only
-   * used since it can handle all precisions.
-   */
-  public static abstract class Source {
-
-    public long ints(int docID) {
-      throw new UnsupportedOperationException("ints are not supported");
-    }
-
-    public double floats(int docID) {
-      throw new UnsupportedOperationException("floats are not supported");
-    }
-
-    public BytesRef bytes(int docID) {
-      throw new UnsupportedOperationException("bytes are not supported");
-    }
-    
-    /** Returns number of unique values.  Some impls may
-     * throw UnsupportedOperationException. */
-    public int getValueCount() {
-      throw new UnsupportedOperationException();
-    }
-    
-    public ValuesEnum getEnum() throws IOException{
-      return getEnum(null);
-    }
-    
-    // nocommit - enable obtaining enum from source since this is already in memory
-    public /*abstract*/ ValuesEnum getEnum(AttributeSource attrSource) throws IOException {
-      throw new UnsupportedOperationException();
-    }
-
-    public abstract long ramBytesUsed();
-  }
-
-  public static abstract class SortedSource extends Source {
-
-    @Override
-    public BytesRef bytes(int docID) {
-      return getByOrd(ord(docID));
-    }
-
-    /**
-     * Returns ord for specified docID. If this docID had not been added to the
-     * Writer, the ord is 0. Ord is dense, ie, starts at 0, then increments by 1
-     * for the next (as defined by {@link Comparator} value.
-     */
-    public abstract int ord(int docID);
-
-    /** Returns value for specified ord. */
-    public abstract BytesRef getByOrd(int ord);
-
-    public static class LookupResult {
-      public boolean found;
-      public int ord;
-    }
-
-    /**
-     * Finds the largest ord whose value is <= the requested value. If
-     * {@link LookupResult#found} is true, then ord is an exact match. The
-     * returned {@link LookupResult} may be reused across calls.
-     */
-    public abstract LookupResult getByValue(BytesRef value);
-  }
-  
-}
diff --git a/lucene/src/java/org/apache/lucene/index/values/VarDerefBytesImpl.java b/lucene/src/java/org/apache/lucene/index/values/VarDerefBytesImpl.java
index 5a9f9d6..dccbd3b 100644
--- a/lucene/src/java/org/apache/lucene/index/values/VarDerefBytesImpl.java
+++ b/lucene/src/java/org/apache/lucene/index/values/VarDerefBytesImpl.java
@@ -251,5 +251,10 @@ class VarDerefBytesImpl {
         datIn.readBytes(ref.bytes, 0, size);
       }
     }
+    
+    @Override
+    public Values type() {
+      return Values.BYTES_VAR_DEREF;
+    }
   }
 }
diff --git a/lucene/src/java/org/apache/lucene/index/values/VarSortedBytesImpl.java b/lucene/src/java/org/apache/lucene/index/values/VarSortedBytesImpl.java
index 9987343..c8536d8 100644
--- a/lucene/src/java/org/apache/lucene/index/values/VarSortedBytesImpl.java
+++ b/lucene/src/java/org/apache/lucene/index/values/VarSortedBytesImpl.java
@@ -157,7 +157,7 @@ class VarSortedBytesImpl {
     }
 
     @Override
-    public org.apache.lucene.index.values.Reader.Source load()
+    public org.apache.lucene.index.values.DocValues.Source load()
         throws IOException {
       return loadSorted(null);
     }
@@ -340,5 +340,10 @@ class VarSortedBytesImpl {
         return advance(pos + 1);
       }
     }
+    
+    @Override
+    public Values type() {
+      return Values.BYTES_VAR_SORTED;
+    }
   }
 }
diff --git a/lucene/src/java/org/apache/lucene/index/values/VarStraightBytesImpl.java b/lucene/src/java/org/apache/lucene/index/values/VarStraightBytesImpl.java
index 83b9747..436a979 100644
--- a/lucene/src/java/org/apache/lucene/index/values/VarStraightBytesImpl.java
+++ b/lucene/src/java/org/apache/lucene/index/values/VarStraightBytesImpl.java
@@ -228,5 +228,10 @@ class VarStraightBytesImpl {
         return advance(pos+1);
       }
     }
+    
+    @Override
+    public Values type() {
+      return Values.BYTES_VAR_STRAIGHT;
+    }
   }
 }
diff --git a/lucene/src/java/org/apache/lucene/index/values/Writer.java b/lucene/src/java/org/apache/lucene/index/values/Writer.java
index 13bf094..ae08177 100644
--- a/lucene/src/java/org/apache/lucene/index/values/Writer.java
+++ b/lucene/src/java/org/apache/lucene/index/values/Writer.java
@@ -17,12 +17,17 @@ package org.apache.lucene.index.values;
  * limitations under the License.
  */
 import java.io.IOException;
-import java.util.List;
+import java.util.Comparator;
 
+import org.apache.lucene.index.values.codec.DocValuesConsumer;
+import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.Bits;
 import org.apache.lucene.util.BytesRef;
 
-public abstract class Writer {
+public abstract class Writer extends DocValuesConsumer {
+  
+  public static final String INDEX_EXTENSION = "idx";
+  public static final String DATA_EXTENSION = "dat";
 
   /** Records the specfied value for the docID */
   public void add(int docID, long value) throws IOException {
@@ -47,28 +52,8 @@ public abstract class Writer {
   /** Finish writing, close any files */
   public abstract void finish(int docCount) throws IOException;
 
-  public static class MergeState {
-    public final Reader reader;
-    public final int docBase;
-    public final int docCount;
-    public final Bits bits;
-
-    public MergeState(Reader reader, int docBase, int docCount, Bits bits) {
-      assert reader != null;
-      this.reader = reader;
-      this.docBase = docBase;
-      this.docCount = docCount;
-      this.bits = bits;
-    }
-  }
-
-  public void add(List<MergeState> states) throws IOException {
-    for (MergeState state : states) {
-      merge(state);
-    }
-  }
-
   // enables bulk copies in subclasses per MergeState
+  @Override
   protected void merge(MergeState state) throws IOException {
     final ValuesEnum valEnum = state.reader.getEnum();
     assert valEnum != null;
@@ -89,4 +74,31 @@ public abstract class Writer {
       valEnum.close();
     }
   }
+  
+  public static Writer create(Values v, String id,
+      Directory directory, Comparator<BytesRef> comp) throws IOException {
+    switch (v) {
+    case PACKED_INTS:
+    case PACKED_INTS_FIXED:
+      return Ints.getWriter(directory, id, true);
+    case SIMPLE_FLOAT_4BYTE:
+      return Floats.getWriter(directory, id, 4);
+    case SIMPLE_FLOAT_8BYTE:
+      return Floats.getWriter(directory, id, 8);
+    case BYTES_FIXED_STRAIGHT:
+      return Bytes.getWriter(directory, id, Bytes.Mode.STRAIGHT, comp, true);
+    case BYTES_FIXED_DEREF:
+      return Bytes.getWriter(directory, id, Bytes.Mode.DEREF, comp, true);
+    case BYTES_FIXED_SORTED:
+      return Bytes.getWriter(directory, id, Bytes.Mode.SORTED, comp, true);
+    case BYTES_VAR_STRAIGHT:
+      return Bytes.getWriter(directory, id, Bytes.Mode.STRAIGHT, comp, false);
+    case BYTES_VAR_DEREF:
+      return Bytes.getWriter(directory, id, Bytes.Mode.DEREF, comp, false);
+    case BYTES_VAR_SORTED:
+      return Bytes.getWriter(directory, id, Bytes.Mode.SORTED, comp, false);
+    default:
+      throw new IllegalArgumentException("Unknown Values: " + v);
+    }
+  }
 }
diff --git a/lucene/src/java/org/apache/lucene/search/FieldComparator.java b/lucene/src/java/org/apache/lucene/search/FieldComparator.java
index d71b89f..2b322d6 100644
--- a/lucene/src/java/org/apache/lucene/search/FieldComparator.java
+++ b/lucene/src/java/org/apache/lucene/search/FieldComparator.java
@@ -22,7 +22,7 @@ import java.text.Collator;
 import java.util.Locale;
 
 import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.values.Reader.Source;
+import org.apache.lucene.index.values.DocValues.Source;
 import org.apache.lucene.search.FieldCache.DocTerms;
 import org.apache.lucene.search.FieldCache.DocTermsIndex;
 import org.apache.lucene.search.cache.ByteValuesCreator;
diff --git a/lucene/src/test/org/apache/lucene/TestExternalCodecs.java b/lucene/src/test/org/apache/lucene/TestExternalCodecs.java
index 2d421b0..cf552bf 100644
--- a/lucene/src/test/org/apache/lucene/TestExternalCodecs.java
+++ b/lucene/src/test/org/apache/lucene/TestExternalCodecs.java
@@ -25,6 +25,8 @@ import org.apache.lucene.analysis.*;
 import org.apache.lucene.index.codecs.*;
 import org.apache.lucene.index.codecs.standard.*;
 import org.apache.lucene.index.codecs.pulsing.*;
+import org.apache.lucene.index.values.DocValues;
+import org.apache.lucene.index.values.codec.DocValuesConsumer;
 import org.apache.lucene.store.*;
 import java.util.*;
 import java.io.*;
@@ -159,6 +161,13 @@ public class TestExternalCodecs extends LuceneTestCase {
       public void close() {
         // TODO: finalize stuff
       }
+
+      @Override
+      public DocValuesConsumer addValuesField(FieldInfo field)
+          throws IOException {
+      //TODO(simonw): can we fix this easily?
+        throw new UnsupportedOperationException("no implemented");
+      }
     }
 
     private static class RAMTermsConsumer extends TermsConsumer {
@@ -257,6 +266,11 @@ public class TestExternalCodecs extends LuceneTestCase {
       public TermsEnum terms() {
         return new RAMTermsEnum(postings.fieldToTerms.get(current));
       }
+
+      @Override
+      public DocValues docValues() throws IOException {
+        throw new UnsupportedOperationException("not implemented");
+       }
     }
 
     static class RAMTermsEnum extends TermsEnum {
diff --git a/lucene/src/test/org/apache/lucene/index/codecs/preflexrw/PreFlexFieldsWriter.java b/lucene/src/test/org/apache/lucene/index/codecs/preflexrw/PreFlexFieldsWriter.java
index 2b0a416..deb23f0 100644
--- a/lucene/src/test/org/apache/lucene/index/codecs/preflexrw/PreFlexFieldsWriter.java
+++ b/lucene/src/test/org/apache/lucene/index/codecs/preflexrw/PreFlexFieldsWriter.java
@@ -28,6 +28,7 @@ import org.apache.lucene.index.IndexFileNames;
 import org.apache.lucene.index.SegmentWriteState;
 import org.apache.lucene.index.FieldInfo;
 import org.apache.lucene.index.codecs.preflex.TermInfo;
+import org.apache.lucene.index.values.codec.DocValuesConsumer;
 import org.apache.lucene.store.IndexOutput;
 
 import java.io.IOException;
@@ -209,4 +210,10 @@ class PreFlexFieldsWriter extends FieldsConsumer {
       return BytesRef.getUTF8SortedAsUTF16Comparator();
     }
   }
+
+  @Override
+  public DocValuesConsumer addValuesField(FieldInfo field) throws IOException {
+    //TODO(simonw): can we fix this easily?
+    throw new UnsupportedOperationException("no implemented");
+  }
 }
\ No newline at end of file
diff --git a/lucene/src/test/org/apache/lucene/index/values/TestIndexValues.java b/lucene/src/test/org/apache/lucene/index/values/TestIndexValues.java
index 87efd06..5bc0649 100644
--- a/lucene/src/test/org/apache/lucene/index/values/TestIndexValues.java
+++ b/lucene/src/test/org/apache/lucene/index/values/TestIndexValues.java
@@ -33,15 +33,20 @@ import org.apache.lucene.document.ValuesField;
 import org.apache.lucene.document.Field.Index;
 import org.apache.lucene.document.Field.Store;
 import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.Fields;
+import org.apache.lucene.index.FieldsEnum;
 import org.apache.lucene.index.IndexReader;
 import org.apache.lucene.index.IndexWriter;
 import org.apache.lucene.index.IndexWriterConfig;
 import org.apache.lucene.index.LogDocMergePolicy;
 import org.apache.lucene.index.LogMergePolicy;
 import org.apache.lucene.index.MergePolicy;
+import org.apache.lucene.index.MultiFields;
 import org.apache.lucene.index.Term;
-import org.apache.lucene.index.values.Reader.SortedSource;
-import org.apache.lucene.index.values.Reader.Source;
+import org.apache.lucene.index.codecs.CodecProvider;
+import org.apache.lucene.index.values.DocValues.SortedSource;
+import org.apache.lucene.index.values.DocValues.Source;
+import org.apache.lucene.index.values.codec.DocValuesCodec;
 import org.apache.lucene.store.Directory;
 import org.apache.lucene.store.LockObtainFailedException;
 import org.apache.lucene.util.BytesRef;
@@ -51,9 +56,33 @@ import org.apache.lucene.util.LuceneTestCase;
 import org.apache.lucene.util.OpenBitSet;
 import org.apache.lucene.util.UnicodeUtil;
 import org.apache.lucene.util._TestUtil;
+import org.junit.After;
+import org.junit.AfterClass;
+import org.junit.Before;
+import org.junit.BeforeClass;
 
 public class TestIndexValues extends LuceneTestCase {
 
+  // TODO test addIndexes
+  private static DocValuesCodec docValuesCodec;
+
+  @BeforeClass
+  public static void beforeClassLuceneTestCaseJ4() {
+    LuceneTestCase.beforeClassLuceneTestCaseJ4();
+    final CodecProvider cp = CodecProvider.getDefault();
+    docValuesCodec = new DocValuesCodec(cp.lookup(CodecProvider.getDefaultCodec()));
+    cp.register(docValuesCodec);
+    CodecProvider.setDefaultCodec(docValuesCodec.name);
+  }
+  
+  @AfterClass
+  public static void afterClassLuceneTestCaseJ4() {
+    final CodecProvider cp = CodecProvider.getDefault();
+    cp.unregister(docValuesCodec);
+    LuceneTestCase.afterClassLuceneTestCaseJ4();    
+  }
+  
+  
   public void testBytesStraight() throws IOException {
     runTestBytes(Bytes.Mode.STRAIGHT, true);
     runTestBytes(Bytes.Mode.STRAIGHT, false);
@@ -71,18 +100,16 @@ public class TestIndexValues extends LuceneTestCase {
 
   // nocommit -- for sorted test, do our own Sort of the
   // values and verify it's identical
-  public void runTestBytes(final Bytes.Mode mode,
-      final boolean fixedSize) throws IOException {
+  public void runTestBytes(final Bytes.Mode mode, final boolean fixedSize)
+      throws IOException {
 
     final BytesRef bytesRef = new BytesRef();
 
     final Comparator<BytesRef> comp = mode == Bytes.Mode.SORTED ? BytesRef
-        .getUTF8SortedAsUnicodeComparator()
-        : null;
+        .getUTF8SortedAsUnicodeComparator() : null;
 
     Directory dir = newDirectory();
-    Writer w = Bytes
-        .getWriter(dir, "test", mode, comp, fixedSize);
+    Writer w = Bytes.getWriter(dir, "test", mode, comp, fixedSize);
     int maxDoc = 220;
     final String[] values = new String[maxDoc];
     final int lenMin, lenMax;
@@ -107,32 +134,33 @@ public class TestIndexValues extends LuceneTestCase {
     }
     w.finish(maxDoc);
 
-    Reader r = Bytes.getReader(dir, "test", mode, fixedSize, maxDoc);
+    DocValues r = Bytes.getValues(dir, "test", mode, fixedSize, maxDoc);
     for (int iter = 0; iter < 2; iter++) {
       ValuesEnum bytesEnum = r.getEnum();
       assertNotNull("enum is null", bytesEnum);
       ValuesAttribute attr = bytesEnum.addAttribute(ValuesAttribute.class);
       assertNotNull("attribute is null", attr);
       BytesRef ref = attr.bytes();
-      assertNotNull("BytesRef is null - enum not initialized to use bytes", attr);
+      assertNotNull("BytesRef is null - enum not initialized to use bytes",
+          attr);
 
       for (int i = 0; i < 2; i++) {
         final int idx = 2 * i;
         assertEquals("doc: " + idx, idx, bytesEnum.advance(idx));
         String utf8String = ref.utf8ToString();
-        assertEquals("doc: " + idx + " lenLeft: " + values[idx].length() + " lenRight: " + utf8String.length() , values[idx],  utf8String);
+        assertEquals("doc: " + idx + " lenLeft: " + values[idx].length()
+            + " lenRight: " + utf8String.length(), values[idx], utf8String);
       }
       assertEquals(ValuesEnum.NO_MORE_DOCS, bytesEnum.advance(maxDoc));
-      assertEquals(ValuesEnum.NO_MORE_DOCS, bytesEnum.advance(maxDoc+1));
+      assertEquals(ValuesEnum.NO_MORE_DOCS, bytesEnum.advance(maxDoc + 1));
 
       bytesEnum.close();
     }
-    
-    
+
     // Verify we can load source twice:
     for (int iter = 0; iter < 2; iter++) {
       Source s;
-      Reader.SortedSource ss;
+      DocValues.SortedSource ss;
       if (mode == Bytes.Mode.SORTED) {
         s = ss = r.loadSorted(comp);
       } else {
@@ -147,8 +175,8 @@ public class TestIndexValues extends LuceneTestCase {
         if (ss != null) {
           assertEquals("doc " + idx, values[idx], ss.getByOrd(ss.ord(idx))
               .utf8ToString());
-          Reader.SortedSource.LookupResult result = ss.getByValue(new BytesRef(
-              values[idx]));
+          DocValues.SortedSource.LookupResult result = ss
+              .getByValue(new BytesRef(values[idx]));
           assertTrue(result.found);
           assertEquals(ss.ord(idx), result.ord);
         }
@@ -217,7 +245,7 @@ public class TestIndexValues extends LuceneTestCase {
         final int additionalDocs = 1 + random.nextInt(9);
         w.finish(NUM_VALUES + additionalDocs);
 
-        Reader r = Ints.getReader(dir, "test", useFixedArrays);
+        DocValues r = Ints.getValues(dir, "test", useFixedArrays);
         for (int iter = 0; iter < 2; iter++) {
           Source s = r.load();
           for (int i = 0; i < NUM_VALUES; i++) {
@@ -254,7 +282,7 @@ public class TestIndexValues extends LuceneTestCase {
             assertEquals(i, iEnum.advance(i));
             assertEquals("" + i, 0, ints.get());
           }
-          
+
           iEnum.close();
         }
         r.close();
@@ -267,22 +295,21 @@ public class TestIndexValues extends LuceneTestCase {
     runTestFloats(4, 0.00001);
   }
 
-  private void runTestFloats(int precision, double delta)
-      throws IOException {
+  private void runTestFloats(int precision, double delta) throws IOException {
     Directory dir = newDirectory();
     Writer w = Floats.getWriter(dir, "test", precision);
     final int NUM_VALUES = 1000;
     final double[] values = new double[NUM_VALUES];
     for (int i = 0; i < NUM_VALUES; i++) {
-      final double v = precision == 4 ? random.nextFloat() : random.nextDouble();
+      final double v = precision == 4 ? random.nextFloat() : random
+          .nextDouble();
       values[i] = v;
       w.add(i, v);
     }
     final int additionalValues = 1 + random.nextInt(10);
     w.finish(NUM_VALUES + additionalValues);
 
-    Reader r = Floats.getReader(dir, "test", NUM_VALUES
-        + additionalValues);
+    DocValues r = Floats.getValues(dir, "test", NUM_VALUES + additionalValues);
     for (int iter = 0; iter < 2; iter++) {
       Source s = r.load();
       for (int i = 0; i < NUM_VALUES; i++) {
@@ -298,7 +325,7 @@ public class TestIndexValues extends LuceneTestCase {
         assertEquals(i, fEnum.nextDoc());
         assertEquals(values[i], floats.get(), delta);
       }
-      for(int i = NUM_VALUES; i < NUM_VALUES + additionalValues; i++) {
+      for (int i = NUM_VALUES; i < NUM_VALUES + additionalValues; i++) {
         assertEquals(i, fEnum.nextDoc());
         assertEquals(0.0, floats.get(), delta);
       }
@@ -312,7 +339,7 @@ public class TestIndexValues extends LuceneTestCase {
         assertEquals(i, fEnum.advance(i));
         assertEquals(values[i], floats.get(), delta);
       }
-      for(int i = NUM_VALUES; i < NUM_VALUES + additionalValues; i++) {
+      for (int i = NUM_VALUES; i < NUM_VALUES + additionalValues; i++) {
         assertEquals(i, fEnum.advance(i));
         assertEquals(0.0, floats.get(), delta);
       }
@@ -335,7 +362,7 @@ public class TestIndexValues extends LuceneTestCase {
     // without deletions
     IndexWriterConfig cfg = writerConfig(true);
     // primitives - no deletes
-    runTestNumerics(cfg,false);
+    runTestNumerics(cfg, false);
 
     cfg = writerConfig(true);
     // bytes - no deletes
@@ -377,12 +404,12 @@ public class TestIndexValues extends LuceneTestCase {
   }
 
   private IndexWriterConfig writerConfig(boolean useCompoundFile) {
-    final IndexWriterConfig cfg = newIndexWriterConfig(
-        TEST_VERSION_CURRENT, new MockAnalyzer());
+    final IndexWriterConfig cfg = newIndexWriterConfig(TEST_VERSION_CURRENT,
+        new MockAnalyzer());
     MergePolicy mergePolicy = cfg.getMergePolicy();
-    if(mergePolicy instanceof LogMergePolicy) {
-      ((LogMergePolicy)mergePolicy).setUseCompoundFile(useCompoundFile);
-    } else if(useCompoundFile) {
+    if (mergePolicy instanceof LogMergePolicy) {
+      ((LogMergePolicy) mergePolicy).setUseCompoundFile(useCompoundFile);
+    } else if (useCompoundFile) {
       LogMergePolicy policy = new LogDocMergePolicy();
       policy.setUseCompoundFile(useCompoundFile);
       cfg.setMergePolicy(policy);
@@ -390,8 +417,8 @@ public class TestIndexValues extends LuceneTestCase {
     return cfg;
   }
 
-  public void runTestNumerics(IndexWriterConfig cfg,
-      boolean withDeletions) throws IOException {
+  public void runTestNumerics(IndexWriterConfig cfg, boolean withDeletions)
+      throws IOException {
     Directory d = newDirectory();
     IndexWriter w = new IndexWriter(d, cfg);
     final int numValues = 350;
@@ -409,14 +436,15 @@ public class TestIndexValues extends LuceneTestCase {
       switch (val) {
       case PACKED_INTS:
       case PACKED_INTS_FIXED: {
-        Reader intsReader = r.getIndexValues(val.name());
+        DocValues intsReader = getDocValues(r, val.name());
         Source ints = intsReader.load();
         ValuesEnum intsEnum = intsReader.getEnum();
         assertNotNull(intsEnum);
         LongsRef enumRef = intsEnum.addAttribute(ValuesAttribute.class).ints();
         for (int i = 0; i < base; i++) {
           assertEquals(0, ints.ints(i));
-          assertEquals(val.name() + " base: " + base + " index: " + i, i, random.nextBoolean()?intsEnum.advance(i): intsEnum.nextDoc());
+          assertEquals(val.name() + " base: " + base + " index: " + i, i,
+              random.nextBoolean() ? intsEnum.advance(i) : intsEnum.nextDoc());
           assertEquals(0, enumRef.get());
         }
         int expected = 0;
@@ -424,7 +452,8 @@ public class TestIndexValues extends LuceneTestCase {
           while (deleted.get(expected)) {
             expected++;
           }
-          assertEquals("advance failed at index: " + i + " of " + r.numDocs() + " docs", i, intsEnum.advance(i));
+          assertEquals("advance failed at index: " + i + " of " + r.numDocs()
+              + " docs", i, intsEnum.advance(i));
           assertEquals(expected, ints.ints(i));
           assertEquals(expected, enumRef.get());
 
@@ -433,24 +462,27 @@ public class TestIndexValues extends LuceneTestCase {
         break;
       case SIMPLE_FLOAT_4BYTE:
       case SIMPLE_FLOAT_8BYTE: {
-        Reader floatReader = r.getIndexValues(val.name());
+        DocValues floatReader = getDocValues(r, val.name());
         Source floats = floatReader.load();
         ValuesEnum floatEnum = floatReader.getEnum();
         assertNotNull(floatEnum);
-        FloatsRef enumRef = floatEnum.addAttribute(ValuesAttribute.class).floats();
+        FloatsRef enumRef = floatEnum.addAttribute(ValuesAttribute.class)
+            .floats();
 
         for (int i = 0; i < base; i++) {
           assertEquals(0.0d, floats.floats(i), 0.0d);
-          assertEquals(i, random.nextBoolean()?floatEnum.advance(i): floatEnum.nextDoc());
-          assertEquals("index " + i, 0.0 ,enumRef.get(), 0.0);
+          assertEquals(i, random.nextBoolean() ? floatEnum.advance(i)
+              : floatEnum.nextDoc());
+          assertEquals("index " + i, 0.0, enumRef.get(), 0.0);
         }
         int expected = 0;
         for (int i = base; i < r.numDocs(); i++, expected++) {
           while (deleted.get(expected)) {
             expected++;
           }
-          assertEquals("advance failed at index: " + i + " of " + r.numDocs() + " docs base:" + base, i, floatEnum.advance(i));
-          assertEquals("index " + i, 2.0 * expected ,enumRef.get() , 0.00001);
+          assertEquals("advance failed at index: " + i + " of " + r.numDocs()
+              + " docs base:" + base, i, floatEnum.advance(i));
+          assertEquals("index " + i, 2.0 * expected, enumRef.get(), 0.00001);
           assertEquals("index " + i, 2.0 * expected, floats.floats(i), 0.00001);
         }
       }
@@ -468,30 +500,30 @@ public class TestIndexValues extends LuceneTestCase {
     d.close();
   }
 
-  private static EnumSet<Values> BYTES = EnumSet.of(
-      Values.BYTES_FIXED_DEREF,
-      Values.BYTES_FIXED_SORTED, 
-      Values.BYTES_FIXED_STRAIGHT,
-      Values.BYTES_VAR_DEREF ,
-      Values.BYTES_VAR_SORTED,
-      Values.BYTES_VAR_STRAIGHT
-      );
-  
+  private static EnumSet<Values> BYTES = EnumSet.of(Values.BYTES_FIXED_DEREF,
+      Values.BYTES_FIXED_SORTED, Values.BYTES_FIXED_STRAIGHT,
+      Values.BYTES_VAR_DEREF, Values.BYTES_VAR_SORTED,
+      Values.BYTES_VAR_STRAIGHT);
+
   private static EnumSet<Values> STRAIGHT_BYTES = EnumSet.of(
-      Values.BYTES_FIXED_STRAIGHT,
-      Values.BYTES_VAR_STRAIGHT
-      );
+      Values.BYTES_FIXED_STRAIGHT, Values.BYTES_VAR_STRAIGHT);
 
-  private static EnumSet<Values> NUMERICS = EnumSet.of(Values.PACKED_INTS, Values.PACKED_INTS_FIXED, Values.SIMPLE_FLOAT_4BYTE, Values.SIMPLE_FLOAT_8BYTE);
-  
-  private static Index[] IDX_VALUES = new Index[] { Index.ANALYZED, Index.ANALYZED_NO_NORMS, Index.NOT_ANALYZED, Index.NOT_ANALYZED_NO_NORMS};
-  private OpenBitSet indexValues(IndexWriter w, int numValues,
-      Values value, List<Values> valueVarList, boolean withDeletions,
-      int multOfSeven) throws CorruptIndexException, IOException {
+  private static EnumSet<Values> NUMERICS = EnumSet.of(Values.PACKED_INTS,
+      Values.PACKED_INTS_FIXED, Values.SIMPLE_FLOAT_4BYTE,
+      Values.SIMPLE_FLOAT_8BYTE);
+
+  private static Index[] IDX_VALUES = new Index[] { Index.ANALYZED,
+      Index.ANALYZED_NO_NORMS, Index.NOT_ANALYZED, Index.NOT_ANALYZED_NO_NORMS };
+
+  private OpenBitSet indexValues(IndexWriter w, int numValues, Values value,
+      List<Values> valueVarList, boolean withDeletions, int multOfSeven)
+      throws CorruptIndexException, IOException {
     final boolean isNumeric = NUMERICS.contains(value);
     OpenBitSet deleted = new OpenBitSet(numValues);
     Document doc = new Document();
-    Fieldable field = random.nextBoolean()? new ValuesField(value.name()):newField(value.name(), _TestUtil.randomRealisticUnicodeString(random, 10), IDX_VALUES[random.nextInt(IDX_VALUES.length)]);
+    Fieldable field = random.nextBoolean() ? new ValuesField(value.name())
+        : newField(value.name(), _TestUtil.randomRealisticUnicodeString(random,
+            10), IDX_VALUES[random.nextInt(IDX_VALUES.length)]);
     doc.add(field);
 
     ValuesAttribute valuesAttribute = ValuesField.values(field);
@@ -549,16 +581,15 @@ public class TestIndexValues extends LuceneTestCase {
       }
     }
     w.commit();
-    
+
     // nocommit test unoptimized with deletions
-    if(withDeletions || random.nextBoolean())
+    if (true || withDeletions || random.nextBoolean())
       w.optimize();
     return deleted;
   }
 
-  public void runTestIndexBytes(IndexWriterConfig cfg,
-      boolean withDeletions) throws CorruptIndexException,
-      LockObtainFailedException, IOException {
+  public void runTestIndexBytes(IndexWriterConfig cfg, boolean withDeletions)
+      throws CorruptIndexException, LockObtainFailedException, IOException {
     Directory d = newDirectory();
     IndexWriter w = new IndexWriter(d, cfg);
     final List<Values> byteVariantList = new ArrayList<Values>(BYTES);
@@ -577,14 +608,14 @@ public class TestIndexValues extends LuceneTestCase {
       final int numRemainingValues = (int) (numValues - deleted.cardinality());
       final int base = r.numDocs() - numRemainingValues;
 
-      Reader bytesReader = r.getIndexValues(byteIndexValue.name());
-//      closeables.add(bytesReader);
+      DocValues bytesReader = getDocValues(r, byteIndexValue.name());
       assertNotNull("field " + byteIndexValue.name()
           + " returned null reader - maybe merged failed", bytesReader);
       Source bytes = bytesReader.load();
       ValuesEnum bytesEnum = bytesReader.getEnum();
       assertNotNull(bytesEnum);
-      final ValuesAttribute attr = bytesEnum.addAttribute(ValuesAttribute.class);
+      final ValuesAttribute attr = bytesEnum
+          .addAttribute(ValuesAttribute.class);
       byte upto = 0;
       // test the filled up slots for correctness
       for (int i = 0; i < base; i++) {
@@ -598,7 +629,7 @@ public class TestIndexValues extends LuceneTestCase {
           // fixed straight returns bytesref with zero bytes all of fixed
           // length
           assertNotNull("expected none null - " + msg, br);
-          if(br.length != 0) {
+          if (br.length != 0) {
             assertEquals("expected zero bytes of length " + bytesSize + " - "
                 + msg, bytesSize, br.length);
             for (int j = 0; j < br.length; j++) {
@@ -613,35 +644,38 @@ public class TestIndexValues extends LuceneTestCase {
         case BYTES_FIXED_DEREF:
         default:
           assertNotNull("expected none null - " + msg, br);
-          if(br.length != 0){
+          if (br.length != 0) {
             bytes.bytes(i);
           }
-          assertEquals("expected empty bytes - " + br.utf8ToString() + msg, 0, br.length);
+          assertEquals("expected empty bytes - " + br.utf8ToString() + msg, 0,
+              br.length);
         }
       }
       final BytesRef enumRef = attr.bytes();
 
-     
       // test the actual doc values added in this iteration
       assertEquals(base + numRemainingValues, r.numDocs());
       int v = 0;
       for (int i = base; i < r.numDocs(); i++) {
-        
+
         String msg = " field: " + byteIndexValue.name() + " at index: " + i
-            + " base: " + base + " numDocs:" + r.numDocs() + " bytesSize: " + bytesSize;
+            + " base: " + base + " numDocs:" + r.numDocs() + " bytesSize: "
+            + bytesSize;
         while (withDeletions && deleted.get(v++)) {
           upto += bytesSize;
         }
-        
+
         BytesRef br = bytes.bytes(i);
-        if(bytesEnum.docID() != i)
-          assertEquals("seek failed for index " + i + " " + msg, i, bytesEnum.advance(i));
+        if (bytesEnum.docID() != i)
+          assertEquals("seek failed for index " + i + " " + msg, i, bytesEnum
+              .advance(i));
         for (int j = 0; j < br.length; j++, upto++) {
-          assertEquals("EnumRef Byte at index " + j + " doesn't match - " + msg,
-              upto, enumRef.bytes[enumRef.offset + j]);      
-          assertEquals("SourceRef Byte at index " + j + " doesn't match - " + msg,
-              upto, br.bytes[br.offset + j]);
-           }
+          assertEquals(
+              "EnumRef Byte at index " + j + " doesn't match - " + msg, upto,
+              enumRef.bytes[enumRef.offset + j]);
+          assertEquals("SourceRef Byte at index " + j + " doesn't match - "
+              + msg, upto, br.bytes[br.offset + j]);
+        }
       }
 
       // clean up
@@ -650,9 +684,32 @@ public class TestIndexValues extends LuceneTestCase {
         toClose.close();
       }
     }
-    
+
     w.close();
     d.close();
   }
-  
+
+  private DocValues getDocValues(IndexReader reader, String field)
+      throws IOException {
+    boolean optimized = reader.isOptimized();
+    Fields fields = optimized ? reader.getSequentialSubReaders()[0].fields() : MultiFields
+        .getFields(reader);
+//    return fields.docValues(field);
+    switch (random.nextInt(optimized ? 3 : 2)) {
+    case 0:
+      return fields.docValues(field);
+    case 1:
+      FieldsEnum iterator = fields.iterator();
+      String name;
+      while ((name = iterator.next()) != null) {
+        if (name.equals(field))
+          return iterator.docValues();
+      }
+      throw new RuntimeException("no such field " + field);
+    case 2:
+      return reader.getSequentialSubReaders()[0].docValues(field);
+    }
+throw new RuntimeException();
+}
+
 }

