GitDiffStart: 8ab762aef2e7081566a8f570df487b92e334ca5f | Mon Nov 27 20:25:32 2006 +0000
diff --git a/contrib/memory/src/java/org/apache/lucene/index/memory/AnalyzerUtil.java b/contrib/memory/src/java/org/apache/lucene/index/memory/AnalyzerUtil.java
index 9fe0afe..1b3a01a 100644
--- a/contrib/memory/src/java/org/apache/lucene/index/memory/AnalyzerUtil.java
+++ b/contrib/memory/src/java/org/apache/lucene/index/memory/AnalyzerUtil.java
@@ -203,10 +203,10 @@ public class AnalyzerUtil {
 
   
   /**
-   * Analyzer wrapper that caches all tokens generated by the underlying child analyzer's
+   * Returns an analyzer wrapper that caches all tokens generated by the underlying child analyzer's
    * token streams, and delivers those cached tokens on subsequent calls to 
-   * <code>tokenStream(String fieldName, Reader reader)</code>, 
-   * if the fieldName has been seen before, altogether ignoring the Reader parameter.
+   * <code>tokenStream(String fieldName, Reader reader)</code> 
+   * if the fieldName has been seen before, altogether ignoring the Reader parameter on cache lookup.
    * <p>
    * If Analyzer / TokenFilter chains are expensive in terms of I/O or CPU, such caching can 
    * help improve performance if the same document is added to multiple Lucene indexes, 
@@ -216,61 +216,49 @@ public class AnalyzerUtil {
    * <ul>
    * <li>Caching the tokens of large Lucene documents can lead to out of memory exceptions.</li> 
    * <li>The Token instances delivered by the underlying child analyzer must be immutable.</li>
-   * <li>A caching analyzer instance must not be used for more than one document, unless 
-   * <code>clear()</code> is called before each new document.</li>
+   * <li>A caching analyzer instance must not be used for more than one document
+   * because the cache is not keyed on the Reader parameter.</li>
    * </ul>
+   * 
+   * @param child
+   *            the underlying child analyzer
+   * @return a new analyzer
    */
-  public static class TokenCachingAnalyzer extends Analyzer {
-    
-    private final Analyzer child;
-    private final HashMap cache = new HashMap();
-      
-    /**
-     * Creates and returns a new caching analyzer that wraps the given underlying child analyzer.
-     * 
-     * @param child
-     *            the underlying child analyzer
-     * @return a new caching analyzer
-     */
-    public TokenCachingAnalyzer(Analyzer child) {
-      if (child == null)
-        throw new IllegalArgumentException("child analyzer must not be null");
+  public static Analyzer getTokenCachingAnalyzer(final Analyzer child) {
 
-      this.child = child;
-    }
-    
-    /**
-     * Removes all cached data.
-     */
-    public void clear() {
-      cache.clear();
-    }
+    if (child == null)
+      throw new IllegalArgumentException("child analyzer must not be null");
 
-    public TokenStream tokenStream(String fieldName, Reader reader) {
-      final ArrayList tokens = (ArrayList) cache.get(fieldName);
-      if (tokens == null) { // not yet cached
-        final ArrayList tokens2 = new ArrayList();
-        cache.put(fieldName, tokens2);
-        return new TokenFilter(child.tokenStream(fieldName, reader)) {
+    return new Analyzer() {
 
-          public Token next() throws IOException {
-            Token token = input.next(); // from filter super class
-            if (token != null) tokens2.add(token);
-            return token;
-          }
-        };
-      } else { // already cached
-        return new TokenStream() {
+      private final HashMap cache = new HashMap();
 
-          private Iterator iter = tokens.iterator();
+      public TokenStream tokenStream(String fieldName, Reader reader) {
+        final ArrayList tokens = (ArrayList) cache.get(fieldName);
+        if (tokens == null) { // not yet cached
+          final ArrayList tokens2 = new ArrayList();
+          cache.put(fieldName, tokens2);
+          return new TokenFilter(child.tokenStream(fieldName, reader)) {
 
-          public Token next() {
-            if (!iter.hasNext()) return null;
-            return (Token) iter.next();
-          }
-        };
+            public Token next() throws IOException {
+              Token token = input.next(); // from filter super class
+              if (token != null) tokens2.add(token);
+              return token;
+            }
+          };
+        } else { // already cached
+          return new TokenStream() {
+
+            private Iterator iter = tokens.iterator();
+
+            public Token next() {
+              if (!iter.hasNext()) return null;
+              return (Token) iter.next();
+            }
+          };
+        }
       }
-    }
+    };
   }
       
   

