GitDiffStart: da6d1ed228c383ff5d71dfc85c6feef2f1f71391 | Tue Nov 13 14:54:09 2012 +0000
diff --git a/lucene/CHANGES.txt b/lucene/CHANGES.txt
index 311b2b2..195ce7f 100644
--- a/lucene/CHANGES.txt
+++ b/lucene/CHANGES.txt
@@ -36,9 +36,8 @@ Changes in backwards compatibility policy
 
 New Features
 
-* LUCENE-4226: New experimental StoredFieldsFormat (in lucene/codecs) that
-  compresses chunks of documents together in order to improve the compression
-  ratio. (Adrien Grand)
+* LUCENE-4226: New experimental StoredFieldsFormat that compresses chunks of
+  documents together in order to improve the compression ratio. (Adrien Grand)
 
 * LUCENE-4426: New ValueSource implementations (in lucene/queries) for
   DocValues fields. (Adrien Grand)
@@ -150,6 +149,9 @@ Bug Fixes
 
 Optimizations
 
+* LUCENE-4509: Enable stored fields compression by default in the Lucene 4.1
+  default codec. (Adrien Grand)
+
 * LUCENE-4536: PackedInts on-disk format is now byte-aligned (it used to be
   long-aligned), saving up to 7 bytes per array of values.
   (Adrien Grand, Mike McCandless)
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsFormat.java b/lucene/codecs/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsFormat.java
deleted file mode 100644
index ae4d279..0000000
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsFormat.java
+++ /dev/null
@@ -1,111 +0,0 @@
-package org.apache.lucene.codecs.compressing;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.StoredFieldsFormat;
-import org.apache.lucene.codecs.StoredFieldsReader;
-import org.apache.lucene.codecs.StoredFieldsWriter;
-import org.apache.lucene.codecs.lucene40.Lucene40StoredFieldsFormat;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.MergePolicy;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-
-/**
- * A {@link StoredFieldsFormat} that is very similar to
- * {@link Lucene40StoredFieldsFormat} but compresses documents in chunks in
- * order to improve the compression ratio.
- * <p>
- * For a chunk size of <tt>chunkSize</tt> bytes, this {@link StoredFieldsFormat}
- * does not support documents larger than (<tt>2<sup>31</sup> - chunkSize</tt>)
- * bytes. In case this is a problem, you should use another format, such as
- * {@link Lucene40StoredFieldsFormat}.
- * <p>
- * For optimal performance, you should use a {@link MergePolicy} that returns
- * segments that have the biggest byte size first.
- * @lucene.experimental
- */
-public class CompressingStoredFieldsFormat extends StoredFieldsFormat {
-
-  private final CompressionMode compressionMode;
-  private final int chunkSize;
-
-  /**
-   * Create a new {@link CompressingStoredFieldsFormat}.
-   * <p>
-   * The <code>compressionMode</code> parameter allows you to choose between
-   * compression algorithms that have various compression and decompression
-   * speeds so that you can pick the one that best fits your indexing and
-   * searching throughput.
-   * <p>
-   * <code>chunkSize</code> is the minimum byte size of a chunk of documents.
-   * A value of <code>1</code> can make sense if there is redundancy across
-   * fields. In that case, both performance and compression ratio should be
-   * better than with {@link Lucene40StoredFieldsFormat} with compressed
-   * fields.
-   * <p>
-   * Higher values of <code>chunkSize</code> should improve the compression
-   * ratio but will require more memory at indexing time and might make document
-   * loading a little slower (depending on the size of your OS cache compared
-   * to the size of your index).
-   *
-   * @param compressionMode the {@link CompressionMode} to use
-   * @param chunkSize the minimum number of bytes of a single chunk of stored documents
-   * @see CompressionMode
-   */
-  public CompressingStoredFieldsFormat(CompressionMode compressionMode, int chunkSize) {
-    this.compressionMode = compressionMode;
-    if (chunkSize < 1) {
-      throw new IllegalArgumentException("chunkSize must be >= 1");
-    }
-    this.chunkSize = chunkSize;
-  }
-
-  /**
-   * Create a new {@link CompressingStoredFieldsFormat} with
-   * {@link CompressionMode#FAST} compression and chunks of <tt>16 KB</tt>.
-   *
-   * @see CompressingStoredFieldsFormat#CompressingStoredFieldsFormat(CompressionMode, int)
-   */
-  public CompressingStoredFieldsFormat() {
-    this(CompressionMode.FAST, 1 << 14);
-  }
-
-  @Override
-  public StoredFieldsReader fieldsReader(Directory directory, SegmentInfo si,
-      FieldInfos fn, IOContext context) throws IOException {
-    return new CompressingStoredFieldsReader(directory, si, fn, context);
-  }
-
-  @Override
-  public StoredFieldsWriter fieldsWriter(Directory directory, SegmentInfo si,
-      IOContext context) throws IOException {
-    return new CompressingStoredFieldsWriter(directory, si, context,
-        compressionMode, chunkSize);
-  }
-
-  @Override
-  public String toString() {
-    return getClass().getSimpleName() + "(compressionMode=" + compressionMode
-        + ", chunkSize=" + chunkSize + ")";
-  }
-
-}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsIndexReader.java b/lucene/codecs/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsIndexReader.java
deleted file mode 100644
index 04989ba..0000000
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsIndexReader.java
+++ /dev/null
@@ -1,183 +0,0 @@
-package org.apache.lucene.codecs.compressing;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.Closeable;
-import java.io.IOException;
-import java.util.Arrays;
-
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.packed.PackedInts;
-
-class CompressingStoredFieldsIndexReader implements Closeable, Cloneable {
-
-  final IndexInput fieldsIndexIn;
-
-  static long moveLowOrderBitToSign(long n) {
-    return ((n >>> 1) ^ -(n & 1));
-  }
-
-  final int maxDoc;
-  final int[] docBases;
-  final long[] startPointers;
-  final int[] avgChunkDocs;
-  final long[] avgChunkSizes;
-  final PackedInts.Reader[] docBasesDeltas; // delta from the avg
-  final PackedInts.Reader[] startPointersDeltas; // delta from the avg
-
-  CompressingStoredFieldsIndexReader(IndexInput fieldsIndexIn, SegmentInfo si) throws IOException {
-    this.fieldsIndexIn = fieldsIndexIn;
-    maxDoc = si.getDocCount();
-    int[] docBases = new int[16];
-    long[] startPointers = new long[16];
-    int[] avgChunkDocs = new int[16];
-    long[] avgChunkSizes = new long[16];
-    PackedInts.Reader[] docBasesDeltas = new PackedInts.Reader[16];
-    PackedInts.Reader[] startPointersDeltas = new PackedInts.Reader[16];
-
-    final int packedIntsVersion = fieldsIndexIn.readVInt();
-
-    int blockCount = 0;
-
-    for (;;) {
-      final int numChunks = fieldsIndexIn.readVInt();
-      if (numChunks == 0) {
-        break;
-      }
-      if (blockCount == docBases.length) {
-        final int newSize = ArrayUtil.oversize(blockCount + 1, 8);
-        docBases = Arrays.copyOf(docBases, newSize);
-        startPointers = Arrays.copyOf(startPointers, newSize);
-        avgChunkDocs = Arrays.copyOf(avgChunkDocs, newSize);
-        avgChunkSizes = Arrays.copyOf(avgChunkSizes, newSize);
-        docBasesDeltas = Arrays.copyOf(docBasesDeltas, newSize);
-        startPointersDeltas = Arrays.copyOf(startPointersDeltas, newSize);
-      }
-
-      // doc bases
-      docBases[blockCount] = fieldsIndexIn.readVInt();
-      avgChunkDocs[blockCount] = fieldsIndexIn.readVInt();
-      final int bitsPerDocBase = fieldsIndexIn.readVInt();
-      if (bitsPerDocBase > 32) {
-        throw new CorruptIndexException("Corrupted");
-      }
-      docBasesDeltas[blockCount] = PackedInts.getReaderNoHeader(fieldsIndexIn, PackedInts.Format.PACKED, packedIntsVersion, numChunks, bitsPerDocBase);
-
-      // start pointers
-      startPointers[blockCount] = fieldsIndexIn.readVLong();
-      avgChunkSizes[blockCount] = fieldsIndexIn.readVLong();
-      final int bitsPerStartPointer = fieldsIndexIn.readVInt();
-      if (bitsPerStartPointer > 64) {
-        throw new CorruptIndexException("Corrupted");
-      }
-      startPointersDeltas[blockCount] = PackedInts.getReaderNoHeader(fieldsIndexIn, PackedInts.Format.PACKED, packedIntsVersion, numChunks, bitsPerStartPointer);
-
-      ++blockCount;
-    }
-
-    this.docBases = Arrays.copyOf(docBases, blockCount);
-    this.startPointers = Arrays.copyOf(startPointers, blockCount);
-    this.avgChunkDocs = Arrays.copyOf(avgChunkDocs, blockCount);
-    this.avgChunkSizes = Arrays.copyOf(avgChunkSizes, blockCount);
-    this.docBasesDeltas = Arrays.copyOf(docBasesDeltas, blockCount);
-    this.startPointersDeltas = Arrays.copyOf(startPointersDeltas, blockCount);
-  }
-
-  private CompressingStoredFieldsIndexReader(CompressingStoredFieldsIndexReader other) {
-    this.fieldsIndexIn = null;
-    this.maxDoc = other.maxDoc;
-    this.docBases = other.docBases;
-    this.startPointers = other.startPointers;
-    this.avgChunkDocs = other.avgChunkDocs;
-    this.avgChunkSizes = other.avgChunkSizes;
-    this.docBasesDeltas = other.docBasesDeltas;
-    this.startPointersDeltas = other.startPointersDeltas;
-  }
-
-  private int block(int docID) {
-    int lo = 0, hi = docBases.length - 1;
-    while (lo <= hi) {
-      final int mid = (lo + hi) >>> 1;
-      final int midValue = docBases[mid];
-      if (midValue == docID) {
-        return mid;
-      } else if (midValue < docID) {
-        lo = mid + 1;
-      } else {
-        hi = mid - 1;
-      }
-    }
-    return hi;
-  }
-
-  private int relativeDocBase(int block, int relativeChunk) {
-    final int expected = avgChunkDocs[block] * relativeChunk;
-    final long delta = moveLowOrderBitToSign(docBasesDeltas[block].get(relativeChunk));
-    return expected + (int) delta;
-  }
-
-  private long relativeStartPointer(int block, int relativeChunk) {
-    final long expected = avgChunkSizes[block] * relativeChunk;
-    final long delta = moveLowOrderBitToSign(startPointersDeltas[block].get(relativeChunk));
-    return expected + delta;
-  }
-
-  private int relativeChunk(int block, int relativeDoc) {
-    int lo = 0, hi = docBasesDeltas[block].size() - 1;
-    while (lo <= hi) {
-      final int mid = (lo + hi) >>> 1;
-      final int midValue = relativeDocBase(block, mid);
-      if (midValue == relativeDoc) {
-        return mid;
-      } else if (midValue < relativeDoc) {
-        lo = mid + 1;
-      } else {
-        hi = mid - 1;
-      }
-    }
-    return hi;
-  }
-
-  long getStartPointer(int docID) {
-    if (docID < 0 || docID >= maxDoc) {
-      throw new IllegalArgumentException("docID out of range [0-" + maxDoc + "]: " + docID);
-    }
-    final int block = block(docID);
-    final int relativeChunk = relativeChunk(block, docID - docBases[block]);
-    return startPointers[block] + relativeStartPointer(block, relativeChunk);
-  }
-
-  @Override
-  public CompressingStoredFieldsIndexReader clone() {
-    if (fieldsIndexIn == null) {
-      return this;
-    } else {
-      return new CompressingStoredFieldsIndexReader(this);
-    }
-  }
-
-  @Override
-  public void close() throws IOException {
-    IOUtils.close(fieldsIndexIn);
-  }
-
-}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsIndexWriter.java b/lucene/codecs/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsIndexWriter.java
deleted file mode 100644
index 2fcfbe2..0000000
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsIndexWriter.java
+++ /dev/null
@@ -1,165 +0,0 @@
-package org.apache.lucene.codecs.compressing;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.Closeable;
-import java.io.IOException;
-
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.packed.PackedInts;
-
-class CompressingStoredFieldsIndexWriter implements Closeable {
-  
-  static final int BLOCK_SIZE = 1024; // number of chunks to serialize at once
-
-  static long moveSignToLowOrderBit(long n) {
-    return (n >> 63) ^ (n << 1);
-  }
-
-  final IndexOutput fieldsIndexOut;
-  int totalDocs;
-  int blockDocs;
-  int blockChunks;
-  long firstStartPointer;
-  long maxStartPointer;
-  final int[] docBaseDeltas;
-  final long[] startPointerDeltas;
-
-  CompressingStoredFieldsIndexWriter(IndexOutput indexOutput) throws IOException {
-    this.fieldsIndexOut = indexOutput;
-    reset();
-    totalDocs = 0;
-    docBaseDeltas = new int[BLOCK_SIZE];
-    startPointerDeltas = new long[BLOCK_SIZE];
-    fieldsIndexOut.writeVInt(PackedInts.VERSION_CURRENT);
-  }
-
-  private void reset() {
-    blockChunks = 0;
-    blockDocs = 0;
-    firstStartPointer = -1; // means unset
-  }
-
-  private void writeBlock() throws IOException {
-    assert blockChunks > 0;
-    fieldsIndexOut.writeVInt(blockChunks);
-
-    // The trick here is that we only store the difference from the average start
-    // pointer or doc base, this helps save bits per value.
-    // And in order to prevent a few chunks that would be far from the average to
-    // raise the number of bits per value for all of them, we only encode blocks
-    // of 1024 chunks at once
-    // See LUCENE-4512
-
-    // doc bases
-    final int avgChunkDocs;
-    if (blockChunks == 1) {
-      avgChunkDocs = 0;
-    } else {
-      avgChunkDocs = Math.round((float) (blockDocs - docBaseDeltas[blockChunks - 1]) / (blockChunks - 1));
-    }
-    fieldsIndexOut.writeVInt(totalDocs - blockDocs); // docBase
-    fieldsIndexOut.writeVInt(avgChunkDocs);
-    int docBase = 0;
-    long maxDelta = 0;
-    for (int i = 0; i < blockChunks; ++i) {
-      final int delta = docBase - avgChunkDocs * i;
-      maxDelta |= moveSignToLowOrderBit(delta);
-      docBase += docBaseDeltas[i];
-    }
-
-    final int bitsPerDocBase = PackedInts.bitsRequired(maxDelta);
-    fieldsIndexOut.writeVInt(bitsPerDocBase);
-    PackedInts.Writer writer = PackedInts.getWriterNoHeader(fieldsIndexOut,
-        PackedInts.Format.PACKED, blockChunks, bitsPerDocBase, 1);
-    docBase = 0;
-    for (int i = 0; i < blockChunks; ++i) {
-      final long delta = docBase - avgChunkDocs * i;
-      assert PackedInts.bitsRequired(moveSignToLowOrderBit(delta)) <= writer.bitsPerValue();
-      writer.add(moveSignToLowOrderBit(delta));
-      docBase += docBaseDeltas[i];
-    }
-    writer.finish();
-
-    // start pointers
-    fieldsIndexOut.writeVLong(firstStartPointer);
-    final long avgChunkSize;
-    if (blockChunks == 1) {
-      avgChunkSize = 0;
-    } else {
-      avgChunkSize = (maxStartPointer - firstStartPointer) / (blockChunks - 1);
-    }
-    fieldsIndexOut.writeVLong(avgChunkSize);
-    long startPointer = 0;
-    maxDelta = 0;
-    for (int i = 0; i < blockChunks; ++i) {
-      startPointer += startPointerDeltas[i];
-      final long delta = startPointer - avgChunkSize * i;
-      maxDelta |= moveSignToLowOrderBit(delta);
-    }
-
-    final int bitsPerStartPointer = PackedInts.bitsRequired(maxDelta);
-    fieldsIndexOut.writeVInt(bitsPerStartPointer);
-    writer = PackedInts.getWriterNoHeader(fieldsIndexOut, PackedInts.Format.PACKED,
-        blockChunks, bitsPerStartPointer, 1);
-    startPointer = 0;
-    for (int i = 0; i < blockChunks; ++i) {
-      startPointer += startPointerDeltas[i];
-      final long delta = startPointer - avgChunkSize * i;
-      assert PackedInts.bitsRequired(moveSignToLowOrderBit(delta)) <= writer.bitsPerValue();
-      writer.add(moveSignToLowOrderBit(delta));
-    }
-    writer.finish();
-  }
-
-  void writeIndex(int numDocs, long startPointer) throws IOException {
-    if (blockChunks == BLOCK_SIZE) {
-      writeBlock();
-      reset();
-    }
-
-    if (firstStartPointer == -1) {
-      firstStartPointer = maxStartPointer = startPointer;
-    }
-    assert firstStartPointer > 0 && startPointer >= firstStartPointer;
-
-    docBaseDeltas[blockChunks] = numDocs;
-    startPointerDeltas[blockChunks] = startPointer - maxStartPointer;
-
-    ++blockChunks;
-    blockDocs += numDocs;
-    totalDocs += numDocs;
-    maxStartPointer = startPointer;
-  }
-
-  void finish(int numDocs) throws IOException {
-    if (numDocs != totalDocs) {
-      throw new IllegalStateException("Expected " + numDocs + " docs, but got " + totalDocs);
-    }
-    if (blockChunks > 0) {
-      writeBlock();
-    }
-    fieldsIndexOut.writeVInt(0); // end marker
-  }
-
-  @Override
-  public void close() throws IOException {
-    fieldsIndexOut.close();
-  }
-
-}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsReader.java b/lucene/codecs/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsReader.java
deleted file mode 100644
index afb5e86..0000000
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsReader.java
+++ /dev/null
@@ -1,406 +0,0 @@
-package org.apache.lucene.codecs.compressing;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import static org.apache.lucene.codecs.compressing.CompressingStoredFieldsWriter.BYTE_ARR;
-import static org.apache.lucene.codecs.compressing.CompressingStoredFieldsWriter.CODEC_NAME_DAT;
-import static org.apache.lucene.codecs.compressing.CompressingStoredFieldsWriter.CODEC_NAME_IDX;
-import static org.apache.lucene.codecs.compressing.CompressingStoredFieldsWriter.HEADER_LENGTH_DAT;
-import static org.apache.lucene.codecs.compressing.CompressingStoredFieldsWriter.HEADER_LENGTH_IDX;
-import static org.apache.lucene.codecs.compressing.CompressingStoredFieldsWriter.NUMERIC_DOUBLE;
-import static org.apache.lucene.codecs.compressing.CompressingStoredFieldsWriter.NUMERIC_FLOAT;
-import static org.apache.lucene.codecs.compressing.CompressingStoredFieldsWriter.NUMERIC_INT;
-import static org.apache.lucene.codecs.compressing.CompressingStoredFieldsWriter.NUMERIC_LONG;
-import static org.apache.lucene.codecs.compressing.CompressingStoredFieldsWriter.STRING;
-import static org.apache.lucene.codecs.compressing.CompressingStoredFieldsWriter.TYPE_BITS;
-import static org.apache.lucene.codecs.compressing.CompressingStoredFieldsWriter.TYPE_MASK;
-import static org.apache.lucene.codecs.compressing.CompressingStoredFieldsWriter.VERSION_CURRENT;
-import static org.apache.lucene.codecs.compressing.CompressingStoredFieldsWriter.VERSION_START;
-import static org.apache.lucene.codecs.lucene40.Lucene40StoredFieldsWriter.FIELDS_EXTENSION;
-import static org.apache.lucene.codecs.lucene40.Lucene40StoredFieldsWriter.FIELDS_INDEX_EXTENSION;
-
-import java.io.IOException;
-import java.util.Arrays;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.StoredFieldsReader;
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.StoredFieldVisitor;
-import org.apache.lucene.store.AlreadyClosedException;
-import org.apache.lucene.store.ByteArrayDataInput;
-import org.apache.lucene.store.DataOutput;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.packed.PackedInts;
-
-final class CompressingStoredFieldsReader extends StoredFieldsReader {
-
-  private final FieldInfos fieldInfos;
-  private final CompressingStoredFieldsIndexReader indexReader;
-  private final IndexInput fieldsStream;
-  private final int packedIntsVersion;
-  private final CompressionMode compressionMode;
-  private final Decompressor decompressor;
-  private final BytesRef bytes;
-  private final int numDocs;
-  private boolean closed;
-
-  // used by clone
-  private CompressingStoredFieldsReader(CompressingStoredFieldsReader reader) {
-    this.fieldInfos = reader.fieldInfos;
-    this.fieldsStream = reader.fieldsStream.clone();
-    this.indexReader = reader.indexReader.clone();
-    this.packedIntsVersion = reader.packedIntsVersion;
-    this.compressionMode = reader.compressionMode;
-    this.decompressor = reader.decompressor.clone();
-    this.numDocs = reader.numDocs;
-    this.bytes = new BytesRef(reader.bytes.bytes.length);
-    this.closed = false;
-  }
-
-  public CompressingStoredFieldsReader(Directory d, SegmentInfo si, FieldInfos fn, IOContext context) throws IOException {
-    final String segment = si.name;
-    boolean success = false;
-    fieldInfos = fn;
-    numDocs = si.getDocCount();
-    IndexInput indexStream = null;
-    try {
-      fieldsStream = d.openInput(IndexFileNames.segmentFileName(segment, "", FIELDS_EXTENSION), context);
-      final String indexStreamFN = IndexFileNames.segmentFileName(segment, "", FIELDS_INDEX_EXTENSION);
-      indexStream = d.openInput(indexStreamFN, context);
-
-      CodecUtil.checkHeader(indexStream, CODEC_NAME_IDX, VERSION_START, VERSION_CURRENT);
-      CodecUtil.checkHeader(fieldsStream, CODEC_NAME_DAT, VERSION_START, VERSION_CURRENT);
-      assert HEADER_LENGTH_DAT == fieldsStream.getFilePointer();
-      assert HEADER_LENGTH_IDX == indexStream.getFilePointer();
-
-      indexReader = new CompressingStoredFieldsIndexReader(indexStream, si);
-      indexStream = null;
-
-      packedIntsVersion = fieldsStream.readVInt();
-      final int compressionModeId = fieldsStream.readVInt();
-      compressionMode = CompressionMode.byId(compressionModeId);
-      decompressor = compressionMode.newDecompressor();
-      this.bytes = new BytesRef();
-
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(this, indexStream);
-      }
-    }
-  }
-
-  /**
-   * @throws AlreadyClosedException if this FieldsReader is closed
-   */
-  private void ensureOpen() throws AlreadyClosedException {
-    if (closed) {
-      throw new AlreadyClosedException("this FieldsReader is closed");
-    }
-  }
-
-  @Override
-  public void close() throws IOException {
-    if (!closed) {
-      IOUtils.close(fieldsStream, indexReader);
-      closed = true;
-    }
-  }
-
-  private static void readField(ByteArrayDataInput in, StoredFieldVisitor visitor, FieldInfo info, int bits) throws IOException {
-    switch (bits & TYPE_MASK) {
-      case BYTE_ARR:
-        int length = in.readVInt();
-        byte[] data = new byte[length];
-        in.readBytes(data, 0, length);
-        visitor.binaryField(info, data);
-        break;
-      case STRING:
-        length = in.readVInt();
-        data = new byte[length];
-        in.readBytes(data, 0, length);
-        visitor.stringField(info, new String(data, IOUtils.CHARSET_UTF_8));
-        break;
-      case NUMERIC_INT:
-        visitor.intField(info, in.readInt());
-        break;
-      case NUMERIC_FLOAT:
-        visitor.floatField(info, Float.intBitsToFloat(in.readInt()));
-        break;
-      case NUMERIC_LONG:
-        visitor.longField(info, in.readLong());
-        break;
-      case NUMERIC_DOUBLE:
-        visitor.doubleField(info, Double.longBitsToDouble(in.readLong()));
-        break;
-      default:
-        throw new AssertionError("Unknown type flag: " + Integer.toHexString(bits));
-    }
-  }
-
-  private static void skipField(ByteArrayDataInput in, int bits) throws IOException {
-    switch (bits & TYPE_MASK) {
-      case BYTE_ARR:
-      case STRING:
-        final int length = in.readVInt();
-        in.skipBytes(length);
-        break;
-      case NUMERIC_INT:
-      case NUMERIC_FLOAT:
-        in.readInt();
-        break;
-      case NUMERIC_LONG:
-      case NUMERIC_DOUBLE:
-        in.readLong();
-        break;
-      default:
-        throw new AssertionError("Unknown type flag: " + Integer.toHexString(bits));
-    }
-  }
-
-  @Override
-  public void visitDocument(int docID, StoredFieldVisitor visitor)
-      throws IOException {
-    fieldsStream.seek(indexReader.getStartPointer(docID));
-
-    final int docBase = fieldsStream.readVInt();
-    final int chunkDocs = fieldsStream.readVInt();
-    if (docID < docBase
-        || docID >= docBase + chunkDocs
-        || docBase + chunkDocs > numDocs) {
-      throw new CorruptIndexException("Corrupted: docID=" + docID
-          + ", docBase=" + docBase + ", chunkDocs=" + chunkDocs
-          + ", numDocs=" + numDocs);
-    }
-
-    final int numStoredFields, offset, length, totalLength;
-    if (chunkDocs == 1) {
-      numStoredFields = fieldsStream.readVInt();
-      offset = 0;
-      length = fieldsStream.readVInt();
-      totalLength = length;
-    } else {
-      final int bitsPerStoredFields = fieldsStream.readVInt();
-      if (bitsPerStoredFields == 0) {
-        numStoredFields = fieldsStream.readVInt();
-      } else if (bitsPerStoredFields > 31) {
-        throw new CorruptIndexException("bitsPerStoredFields=" + bitsPerStoredFields);
-      } else {
-        final long filePointer = fieldsStream.getFilePointer();
-        final PackedInts.Reader reader = PackedInts.getDirectReaderNoHeader(fieldsStream, PackedInts.Format.PACKED, packedIntsVersion, chunkDocs, bitsPerStoredFields);
-        numStoredFields = (int) (reader.get(docID - docBase));
-        fieldsStream.seek(filePointer + PackedInts.Format.PACKED.byteCount(packedIntsVersion, chunkDocs, bitsPerStoredFields));
-      }
-
-      final int bitsPerLength = fieldsStream.readVInt();
-      if (bitsPerLength == 0) {
-        length = fieldsStream.readVInt();
-        offset = (docID - docBase) * length;
-        totalLength = chunkDocs * length;
-      } else if (bitsPerStoredFields > 31) {
-        throw new CorruptIndexException("bitsPerLength=" + bitsPerLength);
-      } else {
-        final PackedInts.ReaderIterator it = PackedInts.getReaderIteratorNoHeader(fieldsStream, PackedInts.Format.PACKED, packedIntsVersion, chunkDocs, bitsPerLength, 1);
-        int off = 0;
-        for (int i = 0; i < docID - docBase; ++i) {
-          off += it.next();
-        }
-        offset = off;
-        length = (int) it.next();
-        off += length;
-        for (int i = docID - docBase + 1; i < chunkDocs; ++i) {
-          off += it.next();
-        }
-        totalLength = off;
-      }
-    }
-
-    if ((length == 0) != (numStoredFields == 0)) {
-      throw new CorruptIndexException("length=" + length + ", numStoredFields=" + numStoredFields);
-    }
-    if (numStoredFields == 0) {
-      // nothing to do
-      return;
-    }
-
-    decompressor.decompress(fieldsStream, totalLength, offset, length, bytes);
-    assert bytes.length == length;
-
-    final ByteArrayDataInput documentInput = new ByteArrayDataInput(bytes.bytes, bytes.offset, bytes.length);
-    for (int fieldIDX = 0; fieldIDX < numStoredFields; fieldIDX++) {
-      final long infoAndBits = documentInput.readVLong();
-      final int fieldNumber = (int) (infoAndBits >>> TYPE_BITS);
-      final FieldInfo fieldInfo = fieldInfos.fieldInfo(fieldNumber);
-
-      final int bits = (int) (infoAndBits & TYPE_MASK);
-      assert bits <= NUMERIC_DOUBLE: "bits=" + Integer.toHexString(bits);
-
-      switch(visitor.needsField(fieldInfo)) {
-        case YES:
-          readField(documentInput, visitor, fieldInfo, bits);
-          assert documentInput.getPosition() <= bytes.offset + bytes.length : documentInput.getPosition() + " " + bytes.offset + bytes.length;
-          break;
-        case NO:
-          skipField(documentInput, bits);
-          assert documentInput.getPosition() <= bytes.offset + bytes.length : documentInput.getPosition() + " " + bytes.offset + bytes.length;
-          break;
-        case STOP:
-          return;
-      }
-    }
-    assert documentInput.getPosition() == bytes.offset + bytes.length : documentInput.getPosition() + " " + bytes.offset + " " + bytes.length;
-  }
-
-  @Override
-  public StoredFieldsReader clone() {
-    ensureOpen();
-    return new CompressingStoredFieldsReader(this);
-  }
-
-  CompressionMode getCompressionMode() {
-    return compressionMode;
-  }
-
-  ChunkIterator chunkIterator(int startDocID) throws IOException {
-    ensureOpen();
-    fieldsStream.seek(indexReader.getStartPointer(startDocID));
-    return new ChunkIterator();
-  }
-
-  final class ChunkIterator {
-
-    BytesRef bytes;
-    int docBase;
-    int chunkDocs;
-    int[] numStoredFields;
-    int[] lengths;
-
-    private ChunkIterator() {
-      this.docBase = -1;
-      bytes = new BytesRef();
-      numStoredFields = new int[1];
-      lengths = new int[1];
-    }
-
-    /**
-     * Return the decompressed size of the chunk
-     */
-    int chunkSize() {
-      int sum = 0;
-      for (int i = 0; i < chunkDocs; ++i) {
-        sum += lengths[i];
-      }
-      return sum;
-    }
-
-    /**
-     * Go to the chunk containing the provided doc ID.
-     */
-    void next(int doc) throws IOException {
-      assert doc >= docBase + chunkDocs : doc + " " + docBase + " " + chunkDocs;
-      fieldsStream.seek(indexReader.getStartPointer(doc));
-
-      final int docBase = fieldsStream.readVInt();
-      final int chunkDocs = fieldsStream.readVInt();
-      if (docBase < this.docBase + this.chunkDocs
-          || docBase + chunkDocs > numDocs) {
-        throw new CorruptIndexException("Corrupted: current docBase=" + this.docBase
-            + ", current numDocs=" + this.chunkDocs + ", new docBase=" + docBase
-            + ", new numDocs=" + chunkDocs);
-      }
-      this.docBase = docBase;
-      this.chunkDocs = chunkDocs;
-
-      if (chunkDocs > numStoredFields.length) {
-        final int newLength = ArrayUtil.oversize(chunkDocs, 4);
-        numStoredFields = new int[newLength];
-        lengths = new int[newLength];
-      }
-
-      if (chunkDocs == 1) {
-        numStoredFields[0] = fieldsStream.readVInt();
-        lengths[0] = fieldsStream.readVInt();
-      } else {
-        final int bitsPerStoredFields = fieldsStream.readVInt();
-        if (bitsPerStoredFields == 0) {
-          Arrays.fill(numStoredFields, 0, chunkDocs, fieldsStream.readVInt());
-        } else if (bitsPerStoredFields > 31) {
-          throw new CorruptIndexException("bitsPerStoredFields=" + bitsPerStoredFields);
-        } else {
-          final PackedInts.ReaderIterator it = PackedInts.getReaderIteratorNoHeader(fieldsStream, PackedInts.Format.PACKED, packedIntsVersion, chunkDocs, bitsPerStoredFields, 1);
-          for (int i = 0; i < chunkDocs; ++i) {
-            numStoredFields[i] = (int) it.next();
-          }
-        }
-
-        final int bitsPerLength = fieldsStream.readVInt();
-        if (bitsPerLength == 0) {
-          Arrays.fill(lengths, 0, chunkDocs, fieldsStream.readVInt());
-        } else if (bitsPerLength > 31) {
-          throw new CorruptIndexException("bitsPerLength=" + bitsPerLength);
-        } else {
-          final PackedInts.ReaderIterator it = PackedInts.getReaderIteratorNoHeader(fieldsStream, PackedInts.Format.PACKED, packedIntsVersion, chunkDocs, bitsPerLength, 1);
-          for (int i = 0; i < chunkDocs; ++i) {
-            lengths[i] = (int) it.next();
-          }
-        }
-      }
-    }
-
-    // total length of the chunk
-    private int totalLength() {
-      int totalLength = 0;
-      for (int i = 0; i < chunkDocs; ++i) {
-        totalLength += lengths[i];
-      }
-      return totalLength;
-    }
-
-    /**
-     * Decompress the chunk.
-     */
-    void decompress() throws IOException {
-      // decompress data
-      final int totalLength = totalLength();
-      decompressor.decompress(fieldsStream, totalLength, 0, totalLength, bytes);
-      assert bytes.length == totalLength;
-      if (bytes.length != chunkSize()) {
-        throw new CorruptIndexException("Corrupted: expected chunk size = " + chunkSize() + ", got " + bytes.length);
-      }
-    }
-
-    /**
-     * Copy compressed data.
-     */
-    void copyCompressedData(DataOutput out) throws IOException {
-      final int totalLength = totalLength();
-      decompressor.copyCompressedData(fieldsStream, totalLength, out);
-    }
-
-  }
-
-}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter.java b/lucene/codecs/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter.java
deleted file mode 100644
index fb4b1dd..0000000
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter.java
+++ /dev/null
@@ -1,409 +0,0 @@
-package org.apache.lucene.codecs.compressing;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import static org.apache.lucene.codecs.lucene40.Lucene40StoredFieldsWriter.FIELDS_EXTENSION;
-import static org.apache.lucene.codecs.lucene40.Lucene40StoredFieldsWriter.FIELDS_INDEX_EXTENSION;
-
-import java.io.IOException;
-import java.util.Arrays;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.StoredFieldsReader;
-import org.apache.lucene.codecs.StoredFieldsWriter;
-import org.apache.lucene.codecs.compressing.CompressingStoredFieldsReader.ChunkIterator;
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.MergeState;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.SegmentReader;
-import org.apache.lucene.index.StorableField;
-import org.apache.lucene.index.StoredDocument;
-import org.apache.lucene.store.DataOutput;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.packed.PackedInts;
-
-final class CompressingStoredFieldsWriter extends StoredFieldsWriter {
-
-  static final int         STRING = 0x00;
-  static final int       BYTE_ARR = 0x01;
-  static final int    NUMERIC_INT = 0x02;
-  static final int  NUMERIC_FLOAT = 0x03;
-  static final int   NUMERIC_LONG = 0x04;
-  static final int NUMERIC_DOUBLE = 0x05;
-
-  static final int TYPE_BITS = PackedInts.bitsRequired(NUMERIC_DOUBLE);
-  static final int TYPE_MASK = (int) PackedInts.maxValue(TYPE_BITS);
-
-  static final String CODEC_NAME_IDX = "CompressingStoredFieldsIndex";
-  static final String CODEC_NAME_DAT = "CompressingStoredFieldsData";
-  static final int VERSION_START = 0;
-  static final int VERSION_CURRENT = VERSION_START;
-  static final long HEADER_LENGTH_IDX = CodecUtil.headerLength(CODEC_NAME_IDX);
-  static final long HEADER_LENGTH_DAT = CodecUtil.headerLength(CODEC_NAME_DAT);
-
-  private final Directory directory;
-  private final String segment;
-  private CompressingStoredFieldsIndexWriter indexWriter;
-  private IndexOutput fieldsStream;
-
-  private final CompressionMode compressionMode;
-  private final Compressor compressor;
-  private final int chunkSize;
-
-  private final GrowableByteArrayDataOutput bufferedDocs;
-  private int[] numStoredFields; // number of stored fields
-  private int[] endOffsets; // end offsets in bufferedDocs
-  private int docBase; // doc ID at the beginning of the chunk
-  private int numBufferedDocs; // docBase + numBufferedDocs == current doc ID
-
-  public CompressingStoredFieldsWriter(Directory directory, SegmentInfo si,
-      IOContext context, CompressionMode compressionMode, int chunkSize) throws IOException {
-    assert directory != null;
-    this.directory = directory;
-    this.segment = si.name;
-    this.compressionMode = compressionMode;
-    this.compressor = compressionMode.newCompressor();
-    this.chunkSize = chunkSize;
-    this.docBase = 0;
-    this.bufferedDocs = new GrowableByteArrayDataOutput(chunkSize);
-    this.numStoredFields = new int[16];
-    this.endOffsets = new int[16];
-    this.numBufferedDocs = 0;
-
-    boolean success = false;
-    IndexOutput indexStream = directory.createOutput(IndexFileNames.segmentFileName(segment, "", FIELDS_INDEX_EXTENSION), context);
-    try {
-      fieldsStream = directory.createOutput(IndexFileNames.segmentFileName(segment, "", FIELDS_EXTENSION), context);
-
-      CodecUtil.writeHeader(indexStream, CODEC_NAME_IDX, VERSION_CURRENT);
-      CodecUtil.writeHeader(fieldsStream, CODEC_NAME_DAT, VERSION_CURRENT);
-      assert HEADER_LENGTH_IDX == indexStream.getFilePointer();
-      assert HEADER_LENGTH_DAT == fieldsStream.getFilePointer();
-
-      indexWriter = new CompressingStoredFieldsIndexWriter(indexStream);
-      indexStream = null;
-
-      fieldsStream.writeVInt(PackedInts.VERSION_CURRENT);
-      fieldsStream.writeVInt(compressionMode.getId());
-
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(indexStream);
-        abort();
-      }
-    }
-  }
-
-  @Override
-  public void close() throws IOException {
-    try {
-      IOUtils.close(fieldsStream, indexWriter);
-    } finally {
-      fieldsStream = null;
-      indexWriter = null;
-    }
-  }
-
-  private void endWithPreviousDocument() throws IOException {
-    if (numBufferedDocs > 0) {
-      endOffsets[numBufferedDocs - 1] = bufferedDocs.length;
-    }
-  }
-
-  @Override
-  public void startDocument(int numStoredFields) throws IOException {
-    endWithPreviousDocument();
-    if (triggerFlush()) {
-      flush();
-    }
-
-    if (numBufferedDocs == this.numStoredFields.length) {
-      final int newLength = ArrayUtil.oversize(numBufferedDocs + 1, 4);
-      this.numStoredFields = Arrays.copyOf(this.numStoredFields, newLength);
-      endOffsets = Arrays.copyOf(endOffsets, newLength);
-    }
-    this.numStoredFields[numBufferedDocs] = numStoredFields;
-    ++numBufferedDocs;
-  }
-
-  private static void saveInts(int[] values, int length, DataOutput out) throws IOException {
-    assert length > 0;
-    if (length == 1) {
-      out.writeVInt(values[0]);
-    } else {
-      boolean allEqual = true;
-      for (int i = 1; i < length; ++i) {
-        if (values[i] != values[0]) {
-          allEqual = false;
-          break;
-        }
-      }
-      if (allEqual) {
-        out.writeVInt(0);
-        out.writeVInt(values[0]);
-      } else {
-        long max = 0;
-        for (int i = 0; i < length; ++i) {
-          max |= values[i];
-        }
-        final int bitsRequired = PackedInts.bitsRequired(max);
-        out.writeVInt(bitsRequired);
-        final PackedInts.Writer w = PackedInts.getWriterNoHeader(out, PackedInts.Format.PACKED, length, bitsRequired, 1);
-        for (int i = 0; i < length; ++i) {
-          w.add(values[i]);
-        }
-        w.finish();
-      }
-    }
-  }
-
-  private void writeHeader(int docBase, int numBufferedDocs, int[] numStoredFields, int[] lengths) throws IOException {
-    // save docBase and numBufferedDocs
-    fieldsStream.writeVInt(docBase);
-    fieldsStream.writeVInt(numBufferedDocs);
-
-    // save numStoredFields
-    saveInts(numStoredFields, numBufferedDocs, fieldsStream);
-
-    // save lengths
-    saveInts(lengths, numBufferedDocs, fieldsStream);
-  }
-
-  private boolean triggerFlush() {
-    return bufferedDocs.length >= chunkSize || // chunks of at least chunkSize bytes
-        numBufferedDocs >= chunkSize; // can be necessary if most docs are empty
-  }
-
-  private void flush() throws IOException {
-    indexWriter.writeIndex(numBufferedDocs, fieldsStream.getFilePointer());
-
-    // transform end offsets into lengths
-    final int[] lengths = endOffsets;
-    for (int i = numBufferedDocs - 1; i > 0; --i) {
-      lengths[i] = endOffsets[i] - endOffsets[i - 1];
-      assert lengths[i] >= 0;
-    }
-    writeHeader(docBase, numBufferedDocs, numStoredFields, lengths);
-
-    // compress stored fields to fieldsStream
-    compressor.compress(bufferedDocs.bytes, 0, bufferedDocs.length, fieldsStream);
-
-    // reset
-    docBase += numBufferedDocs;
-    numBufferedDocs = 0;
-    bufferedDocs.length = 0;
-  }
-
-  @Override
-  public void writeField(FieldInfo info, StorableField field)
-      throws IOException {
-    int bits = 0;
-    final BytesRef bytes;
-    final String string;
-
-    Number number = field.numericValue();
-    if (number != null) {
-      if (number instanceof Byte || number instanceof Short || number instanceof Integer) {
-        bits = NUMERIC_INT;
-      } else if (number instanceof Long) {
-        bits = NUMERIC_LONG;
-      } else if (number instanceof Float) {
-        bits = NUMERIC_FLOAT;
-      } else if (number instanceof Double) {
-        bits = NUMERIC_DOUBLE;
-      } else {
-        throw new IllegalArgumentException("cannot store numeric type " + number.getClass());
-      }
-      string = null;
-      bytes = null;
-    } else {
-      bytes = field.binaryValue();
-      if (bytes != null) {
-        bits = BYTE_ARR;
-        string = null;
-      } else {
-        bits = STRING;
-        string = field.stringValue();
-        if (string == null) {
-          throw new IllegalArgumentException("field " + field.name() + " is stored but does not have binaryValue, stringValue nor numericValue");
-        }
-      }
-    }
-
-    final long infoAndBits = (((long) info.number) << TYPE_BITS) | bits;
-    bufferedDocs.writeVLong(infoAndBits);
-
-    if (bytes != null) {
-      bufferedDocs.writeVInt(bytes.length);
-      bufferedDocs.writeBytes(bytes.bytes, bytes.offset, bytes.length);
-    } else if (string != null) {
-      bufferedDocs.writeString(field.stringValue());
-    } else {
-      if (number instanceof Byte || number instanceof Short || number instanceof Integer) {
-        bufferedDocs.writeInt(number.intValue());
-      } else if (number instanceof Long) {
-        bufferedDocs.writeLong(number.longValue());
-      } else if (number instanceof Float) {
-        bufferedDocs.writeInt(Float.floatToIntBits(number.floatValue()));
-      } else if (number instanceof Double) {
-        bufferedDocs.writeLong(Double.doubleToLongBits(number.doubleValue()));
-      } else {
-        throw new AssertionError("Cannot get here");
-      }
-    }
-  }
-
-  @Override
-  public void abort() {
-    IOUtils.closeWhileHandlingException(this);
-    IOUtils.deleteFilesIgnoringExceptions(directory,
-        IndexFileNames.segmentFileName(segment, "", FIELDS_EXTENSION),
-        IndexFileNames.segmentFileName(segment, "", FIELDS_INDEX_EXTENSION));
-  }
-
-  @Override
-  public void finish(FieldInfos fis, int numDocs) throws IOException {
-    endWithPreviousDocument();
-    if (numBufferedDocs > 0) {
-      flush();
-    }
-    if (docBase != numDocs) {
-      throw new RuntimeException("Wrote " + docBase + " docs, finish called with numDocs=" + numDocs);
-    }
-    indexWriter.finish(numDocs);
-    assert bufferedDocs.length == 0;
-  }
-
-  @Override
-  public int merge(MergeState mergeState) throws IOException {
-    int docCount = 0;
-    int idx = 0;
-
-    for (AtomicReader reader : mergeState.readers) {
-      final SegmentReader matchingSegmentReader = mergeState.matchingSegmentReaders[idx++];
-      CompressingStoredFieldsReader matchingFieldsReader = null;
-      if (matchingSegmentReader != null) {
-        final StoredFieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();
-        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader
-        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {
-          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;
-        }
-      }
-
-      final int maxDoc = reader.maxDoc();
-      final Bits liveDocs = reader.getLiveDocs();
-
-      if (matchingFieldsReader == null) {
-        // naive merge...
-        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {
-          StoredDocument doc = reader.document(i);
-          addDocument(doc, mergeState.fieldInfos);
-          ++docCount;
-          mergeState.checkAbort.work(300);
-        }
-      } else {
-        int docID = nextLiveDoc(0, liveDocs, maxDoc);
-        if (docID < maxDoc) {
-          // not all docs were deleted
-          final ChunkIterator it = matchingFieldsReader.chunkIterator(docID);
-          int[] startOffsets = new int[0];
-          do {
-            // go to the next chunk that contains docID
-            it.next(docID);
-            // transform lengths into offsets
-            if (startOffsets.length < it.chunkDocs) {
-              startOffsets = new int[ArrayUtil.oversize(it.chunkDocs, 4)];
-            }
-            for (int i = 1; i < it.chunkDocs; ++i) {
-              startOffsets[i] = startOffsets[i - 1] + it.lengths[i - 1];
-            }
-
-            if (compressionMode == matchingFieldsReader.getCompressionMode() // same compression mode
-                && (numBufferedDocs == 0 || triggerFlush()) // starting a new chunk
-                && startOffsets[it.chunkDocs - 1] < chunkSize // chunk is small enough
-                && startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] >= chunkSize // chunk is large enough
-                && nextDeletedDoc(it.docBase, liveDocs, it.docBase + it.chunkDocs) == it.docBase + it.chunkDocs) { // no deletion in the chunk
-              assert docID == it.docBase;
-
-              // no need to decompress, just copy data
-              endWithPreviousDocument();
-              if (triggerFlush()) {
-                flush();
-              }
-              indexWriter.writeIndex(it.chunkDocs, fieldsStream.getFilePointer());
-              writeHeader(this.docBase, it.chunkDocs, it.numStoredFields, it.lengths);
-              it.copyCompressedData(fieldsStream);
-              this.docBase += it.chunkDocs;
-              docID = nextLiveDoc(it.docBase + it.chunkDocs, liveDocs, maxDoc);
-              docCount += it.chunkDocs;
-              mergeState.checkAbort.work(300 * it.chunkDocs);
-            } else {
-              // decompress
-              it.decompress();
-              if (startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] != it.bytes.length) {
-                throw new CorruptIndexException("Corrupted: expected chunk size=" + startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] + ", got " + it.bytes.length);
-              }
-              // copy non-deleted docs
-              for (; docID < it.docBase + it.chunkDocs; docID = nextLiveDoc(docID + 1, liveDocs, maxDoc)) {
-                final int diff = docID - it.docBase;
-                startDocument(it.numStoredFields[diff]);
-                bufferedDocs.writeBytes(it.bytes.bytes, it.bytes.offset + startOffsets[diff], it.lengths[diff]);
-                ++docCount;
-                mergeState.checkAbort.work(300);
-              }
-            }
-          } while (docID < maxDoc);
-        }
-      }
-    }
-    finish(mergeState.fieldInfos, docCount);
-    return docCount;
-  }
-
-  private static int nextLiveDoc(int doc, Bits liveDocs, int maxDoc) {
-    if (liveDocs == null) {
-      return doc;
-    }
-    while (doc < maxDoc && !liveDocs.get(doc)) {
-      ++doc;
-    }
-    return doc;
-  }
-
-  private static int nextDeletedDoc(int doc, Bits liveDocs, int maxDoc) {
-    if (liveDocs == null) {
-      return maxDoc;
-    }
-    while (doc < maxDoc && liveDocs.get(doc)) {
-      ++doc;
-    }
-    return doc;
-  }
-
-}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/compressing/CompressionMode.java b/lucene/codecs/src/java/org/apache/lucene/codecs/compressing/CompressionMode.java
deleted file mode 100644
index b8237e9..0000000
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/compressing/CompressionMode.java
+++ /dev/null
@@ -1,289 +0,0 @@
-package org.apache.lucene.codecs.compressing;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.zip.DataFormatException;
-import java.util.zip.Deflater;
-import java.util.zip.Inflater;
-
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.store.DataInput;
-import org.apache.lucene.store.DataOutput;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.BytesRef;
-
-/**
- * A compression mode. Tells how much effort should be spent on compression and
- * decompression of stored fields.
- * @lucene.experimental
- */
-public enum CompressionMode {
-
-  /**
-   * A compression mode that trades compression ratio for speed. Although the
-   * compression ratio might remain high, compression and decompression are
-   * very fast. Use this mode with indices that have a high update rate but
-   * should be able to load documents from disk quickly.
-   */
-  FAST(0) {
-
-    @Override
-    Compressor newCompressor() {
-      return LZ4_FAST_COMPRESSOR;
-    }
-
-    @Override
-    Decompressor newDecompressor() {
-      return LZ4_DECOMPRESSOR;
-    }
-
-  },
-
-  /**
-   * A compression mode that trades speed for compression ratio. Although
-   * compression and decompression might be slow, this compression mode should
-   * provide a good compression ratio. This mode might be interesting if/when
-   * your index size is much bigger than your OS cache.
-   */
-  HIGH_COMPRESSION(1) {
-
-    @Override
-    Compressor newCompressor() {
-      return new DeflateCompressor(Deflater.BEST_COMPRESSION);
-    }
-
-    @Override
-    Decompressor newDecompressor() {
-      return new DeflateDecompressor();
-    }
-
-  },
-
-  /**
-   * This compression mode is similar to {@link #FAST} but it spends more time
-   * compressing in order to improve the compression ratio. This compression
-   * mode is best used with indices that have a low update rate but should be
-   * able to load documents from disk quickly.
-   */
-  FAST_DECOMPRESSION(2) {
-
-    @Override
-    Compressor newCompressor() {
-      return LZ4_HIGH_COMPRESSOR;
-    }
-
-    @Override
-    Decompressor newDecompressor() {
-      return LZ4_DECOMPRESSOR;
-    }
-
-  };
-
-  /** Get a {@link CompressionMode} according to its id. */
-  public static CompressionMode byId(int id) {
-    for (CompressionMode mode : CompressionMode.values()) {
-      if (mode.getId() == id) {
-        return mode;
-      }
-    }
-    throw new IllegalArgumentException("Unknown id: " + id);
-  }
-
-  private final int id;
-
-  private CompressionMode(int id) {
-    this.id = id;
-  }
-
-  /**
-   * Returns an ID for this compression mode. Should be unique across
-   * {@link CompressionMode}s as it is used for serialization and
-   * unserialization.
-   */
-  public final int getId() {
-    return id;
-  }
-
-  /**
-   * Create a new {@link Compressor} instance.
-   */
-  abstract Compressor newCompressor();
-
-  /**
-   * Create a new {@link Decompressor} instance.
-   */
-  abstract Decompressor newDecompressor();
-
-
-  private static final Decompressor LZ4_DECOMPRESSOR = new Decompressor() {
-
-    @Override
-    public void decompress(DataInput in, int originalLength, int offset, int length, BytesRef bytes) throws IOException {
-      assert offset + length <= originalLength;
-      // add 7 padding bytes, this is not necessary but can help decompression run faster
-      if (bytes.bytes.length < originalLength + 7) {
-        bytes.bytes = new byte[ArrayUtil.oversize(originalLength + 7, 1)];
-      }
-      final int decompressedLength = LZ4.decompress(in, offset + length, bytes.bytes, 0);
-      if (decompressedLength > originalLength) {
-        throw new CorruptIndexException("Corrupted: lengths mismatch: " + decompressedLength + " > " + originalLength);
-      }
-      bytes.offset = offset;
-      bytes.length = length;
-    }
-
-    @Override
-    public void copyCompressedData(DataInput in, int originalLength, DataOutput out) throws IOException {
-      final int copied = LZ4.copyCompressedData(in, originalLength, out);
-      if (copied != originalLength) {
-        throw new CorruptIndexException("Currupted compressed stream: expected " + originalLength + " bytes, but got at least" + copied);
-      }
-    }
-
-    @Override
-    public Decompressor clone() {
-      return this;
-    }
-
-  };
-
-  private static final Compressor LZ4_FAST_COMPRESSOR = new Compressor() {
-
-    @Override
-    public void compress(byte[] bytes, int off, int len, DataOutput out)
-        throws IOException {
-      LZ4.compress(bytes, off, len, out);
-    }
-
-  };
-
-  private static final Compressor LZ4_HIGH_COMPRESSOR = new Compressor() {
-
-    @Override
-    public void compress(byte[] bytes, int off, int len, DataOutput out)
-        throws IOException {
-      LZ4.compressHC(bytes, off, len, out);
-    }
-
-  };
-
-  private static final class DeflateDecompressor extends Decompressor {
-
-    final Inflater decompressor;
-    byte[] compressed;
-
-    DeflateDecompressor() {
-      decompressor = new Inflater();
-      compressed = new byte[0];
-    }
-
-    @Override
-    public void decompress(DataInput in, int originalLength, int offset, int length, BytesRef bytes) throws IOException {
-      assert offset + length <= originalLength;
-      if (length == 0) {
-        bytes.length = 0;
-        return;
-      }
-      final int compressedLength = in.readVInt();
-      if (compressedLength > compressed.length) {
-        compressed = new byte[ArrayUtil.oversize(compressedLength, 1)];
-      }
-      in.readBytes(compressed, 0, compressedLength);
-
-      decompressor.reset();
-      decompressor.setInput(compressed, 0, compressedLength);
-
-      bytes.offset = bytes.length = 0;
-      while (true) {
-        final int count;
-        try {
-          final int remaining = bytes.bytes.length - bytes.length;
-          count = decompressor.inflate(bytes.bytes, bytes.length, remaining);
-        } catch (DataFormatException e) {
-          throw new IOException(e);
-        }
-        bytes.length += count;
-        if (decompressor.finished()) {
-          break;
-        } else {
-          bytes.bytes = ArrayUtil.grow(bytes.bytes);
-        }
-      }
-      if (bytes.length != originalLength) {
-        throw new CorruptIndexException("Lengths mismatch: " + bytes.length + " != " + originalLength);
-      }
-      bytes.offset = offset;
-      bytes.length = length;
-    }
-
-    @Override
-    public void copyCompressedData(DataInput in, int originalLength, DataOutput out) throws IOException {
-      final int compressedLength = in.readVInt();
-      out.writeVInt(compressedLength);
-      out.copyBytes(in, compressedLength);
-    }
-
-    @Override
-    public Decompressor clone() {
-      return new DeflateDecompressor();
-    }
-
-  }
-
-  private static class DeflateCompressor extends Compressor {
-
-    final Deflater compressor;
-    byte[] compressed;
-
-    DeflateCompressor(int level) {
-      compressor = new Deflater(level);
-      compressed = new byte[64];
-    }
-
-    @Override
-    public void compress(byte[] bytes, int off, int len, DataOutput out) throws IOException {
-      compressor.reset();
-      compressor.setInput(bytes, off, len);
-      compressor.finish();
-
-      if (compressor.needsInput()) {
-        // no output
-        out.writeVInt(0);
-        return;
-      }
-
-      int totalCount = 0;
-      for (;;) {
-        final int count = compressor.deflate(compressed, totalCount, compressed.length - totalCount);
-        totalCount += count;
-        assert totalCount <= compressed.length;
-        if (compressor.finished()) {
-          break;
-        } else {
-          compressed = ArrayUtil.grow(compressed);
-        }
-      }
-
-      out.writeVInt(totalCount);
-      out.writeBytes(compressed, totalCount);
-    }
-
-  }
-
-}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/compressing/Compressor.java b/lucene/codecs/src/java/org/apache/lucene/codecs/compressing/Compressor.java
deleted file mode 100644
index a652999..0000000
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/compressing/Compressor.java
+++ /dev/null
@@ -1,36 +0,0 @@
-package org.apache.lucene.codecs.compressing;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.store.DataOutput;
-
-/**
- * A data compressor.
- */
-abstract class Compressor {
-
-  /**
-   * Compress bytes into <code>out</code>. It it the responsibility of the
-   * compressor to add all necessary information so that a {@link Decompressor}
-   * will know when to stop decompressing bytes from the stream.
-   */
-  public abstract void compress(byte[] bytes, int off, int len, DataOutput out) throws IOException;
-
-}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/compressing/Decompressor.java b/lucene/codecs/src/java/org/apache/lucene/codecs/compressing/Decompressor.java
deleted file mode 100644
index 43f30c2..0000000
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/compressing/Decompressor.java
+++ /dev/null
@@ -1,54 +0,0 @@
-package org.apache.lucene.codecs.compressing;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.store.DataInput;
-import org.apache.lucene.store.DataOutput;
-import org.apache.lucene.util.BytesRef;
-
-/**
- * An decompressor.
- */
-abstract class Decompressor implements Cloneable {
-
-  /**
-   * Decompress bytes that were stored between offsets <code>offset</code> and
-   * <code>offset+length</code> in the original stream from the compressed
-   * stream <code>in</code> to <code>bytes</code>. After returning, the length
-   * of <code>bytes</code> (<code>bytes.length</code>) must be equal to
-   * <code>length</code>. Implementations of this method are free to resize
-   * <code>bytes</code> depending on their needs.
-   *
-   * @param in the input that stores the compressed stream
-   * @param originalLength the length of the original data (before compression)
-   * @param offset bytes before this offset do not need to be decompressed
-   * @param length bytes after <code>offset+length</code> do not need to be decompressed
-   * @param bytes a {@link BytesRef} where to store the decompressed data
-   */
-  public abstract void decompress(DataInput in, int originalLength, int offset, int length, BytesRef bytes) throws IOException;
-
-  /** Copy a compressed stream whose original length is
-   * <code>originalLength</code> from <code>in</code> to <code>out</code>. */
-  public abstract void copyCompressedData(DataInput in, int originalLength, DataOutput out) throws IOException;
-
-  @Override
-  public abstract Decompressor clone();
-
-}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/compressing/GrowableByteArrayDataOutput.java b/lucene/codecs/src/java/org/apache/lucene/codecs/compressing/GrowableByteArrayDataOutput.java
deleted file mode 100644
index f5aeefb..0000000
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/compressing/GrowableByteArrayDataOutput.java
+++ /dev/null
@@ -1,56 +0,0 @@
-package org.apache.lucene.codecs.compressing;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.store.DataOutput;
-import org.apache.lucene.util.ArrayUtil;
-
-/**
- * A {@link DataOutput} that can be used to build a byte[].
- */
-final class GrowableByteArrayDataOutput extends DataOutput {
-
-  byte[] bytes;
-  int length;
-
-  GrowableByteArrayDataOutput(int cp) {
-    this.bytes = new byte[ArrayUtil.oversize(cp, 1)];
-    this.length = 0;
-  }
-
-  @Override
-  public void writeByte(byte b) throws IOException {
-    if (length >= bytes.length) {
-      bytes = ArrayUtil.grow(bytes);
-    }
-    bytes[length++] = b;
-  }
-
-  @Override
-  public void writeBytes(byte[] b, int off, int len) throws IOException {
-    final int newLength = length + len;
-    if (newLength > bytes.length) {
-      bytes = ArrayUtil.grow(bytes, newLength);
-    }
-    System.arraycopy(b, off, bytes, length, len);
-    length = newLength;
-  }
-
-}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/compressing/LZ4.java b/lucene/codecs/src/java/org/apache/lucene/codecs/compressing/LZ4.java
deleted file mode 100644
index 359ecb5..0000000
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/compressing/LZ4.java
+++ /dev/null
@@ -1,556 +0,0 @@
-package org.apache.lucene.codecs.compressing;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Arrays;
-
-import org.apache.lucene.store.DataInput;
-import org.apache.lucene.store.DataOutput;
-import org.apache.lucene.util.packed.PackedInts;
-
-/**
- * LZ4 compression and decompression routines.
- *
- * http://code.google.com/p/lz4/
- * http://fastcompression.blogspot.fr/p/lz4.html
- */
-class LZ4 {
-
-  private LZ4() {}
-
-  static final int MEMORY_USAGE = 14;
-  static final int MIN_MATCH = 4; // minimum length of a match
-  static final int MAX_DISTANCE = 1 << 16; // maximum distance of a reference
-  static final int LAST_LITERALS = 5; // the last 5 bytes must be encoded as literals
-  static final int HASH_LOG_HC = 15; // log size of the dictionary for compressHC
-  static final int HASH_TABLE_SIZE_HC = 1 << HASH_LOG_HC;
-  static final int OPTIMAL_ML = 0x0F + 4 - 1; // match length that doesn't require an additional byte
-
-
-  private static int hash(int i, int hashBits) {
-    return (i * -1640531535) >>> (32 - hashBits);
-  }
-
-  private static int hashHC(int i) {
-    return hash(i, HASH_LOG_HC);
-  }
-
-  private static int readInt(byte[] buf, int i) {
-    return ((buf[i] & 0xFF) << 24) | ((buf[i+1] & 0xFF) << 16) | ((buf[i+2] & 0xFF) << 8) | (buf[i+3] & 0xFF);
-  }
-
-  private static boolean readIntEquals(byte[] buf, int i, int j) {
-    return readInt(buf, i) == readInt(buf, j);
-  }
-
-  private static int commonBytes(byte[] b, int o1, int o2, int limit) {
-    assert o1 < o2;
-    int count = 0;
-    while (o2 < limit && b[o1++] == b[o2++]) {
-      ++count;
-    }
-    return count;
-  }
-
-  private static int commonBytesBackward(byte[] b, int o1, int o2, int l1, int l2) {
-    int count = 0;
-    while (o1 > l1 && o2 > l2 && b[--o1] == b[--o2]) {
-      ++count;
-    }
-    return count;
-  }
-
-  /**
-   * Decompress at least <code>decompressedLen</code> bytes into
-   * <code>dest[dOff:]</code>. Please note that <code>dest</code> must be large
-   * enough to be able to hold <b>all</b> decompressed data (meaning that you
-   * need to know the total decompressed length).
-   */
-  public static int decompress(DataInput compressed, int decompressedLen, byte[] dest, int dOff) throws IOException {
-    final int destEnd = dest.length;
-
-    do {
-      // literals
-      final int token = compressed.readByte() & 0xFF;
-      int literalLen = token >>> 4;
-
-      if (literalLen != 0) {
-        if (literalLen == 0x0F) {
-          byte len;
-          while ((len = compressed.readByte()) == (byte) 0xFF) {
-            literalLen += 0xFF;
-          }
-          literalLen += len & 0xFF;
-        }
-        compressed.readBytes(dest, dOff, literalLen);
-        dOff += literalLen;
-      }
-
-      if (dOff >= decompressedLen) {
-        break;
-      }
-
-      // matchs
-      final int matchDec = (compressed.readByte() & 0xFF) | ((compressed.readByte() & 0xFF) << 8);
-      assert matchDec > 0;
-
-      int matchLen = token & 0x0F;
-      if (matchLen == 0x0F) {
-        int len;
-        while ((len = compressed.readByte()) == (byte) 0xFF) {
-          matchLen += 0xFF;
-        }
-        matchLen += len & 0xFF;
-      }
-      matchLen += MIN_MATCH;
-
-      // copying a multiple of 8 bytes can make decompression from 5% to 10% faster
-      final int fastLen = ((matchLen - 1) & 0xFFFFFFF8) + 8;
-      if (matchDec < matchLen || dOff + fastLen > destEnd) {
-        // overlap -> naive incremental copy
-        for (int ref = dOff - matchDec, end = dOff + matchLen; dOff < end; ++ref, ++dOff) {
-          dest[dOff] = dest[ref];
-        }
-      } else {
-        // no overlap -> arraycopy
-        System.arraycopy(dest, dOff - matchDec, dest, dOff, fastLen);
-        dOff += matchLen;
-      }
-    } while (dOff < decompressedLen);
-
-    return dOff;
-  }
-
-  private static void encodeLen(int l, DataOutput out) throws IOException {
-    while (l >= 0xFF) {
-      out.writeByte((byte) 0xFF);
-      l -= 0xFF;
-    }
-    out.writeByte((byte) l);
-  }
-
-  private static void encodeLiterals(byte[] bytes, int token, int anchor, int literalLen, DataOutput out) throws IOException {
-    out.writeByte((byte) token);
-
-    // encode literal length
-    if (literalLen >= 0x0F) {
-      encodeLen(literalLen - 0x0F, out);
-    }
-
-    // encode literals
-    out.writeBytes(bytes, anchor, literalLen);
-  }
-
-  private static void encodeLastLiterals(byte[] bytes, int anchor, int literalLen, DataOutput out) throws IOException {
-    final int token = Math.min(literalLen, 0x0F) << 4;
-    encodeLiterals(bytes, token, anchor, literalLen, out);
-  }
-
-  private static void encodeSequence(byte[] bytes, int anchor, int matchRef, int matchOff, int matchLen, DataOutput out) throws IOException {
-    final int literalLen = matchOff - anchor;
-    assert matchLen >= 4;
-    // encode token
-    final int token = (Math.min(literalLen, 0x0F) << 4) | Math.min(matchLen - 4, 0x0F);
-    encodeLiterals(bytes, token, anchor, literalLen, out);
-
-    // encode match dec
-    final int matchDec = matchOff - matchRef;
-    assert matchDec > 0 && matchDec < 1 << 16;
-    out.writeByte((byte) matchDec);
-    out.writeByte((byte) (matchDec >>> 8));
-
-    // encode match len
-    if (matchLen >= MIN_MATCH + 0x0F) {
-      encodeLen(matchLen - 0x0F - MIN_MATCH, out);
-    }
-  }
-
-  /**
-   * Compress <code>bytes[off:off+len]</code> into <code>out</code> using
-   * at most 16KB of memory.
-   */
-  public static void compress(byte[] bytes, int off, int len, DataOutput out) throws IOException {
-
-    final int base = off;
-    final int end = off + len;
-
-    int anchor = off++;
-
-    if (len > LAST_LITERALS + MIN_MATCH) {
-
-      final int limit = end - LAST_LITERALS;
-      final int matchLimit = limit - MIN_MATCH;
-
-      final int bitsPerOffset = PackedInts.bitsRequired(len - LAST_LITERALS);
-      final int bitsPerOffsetLog = 32 - Integer.numberOfLeadingZeros(bitsPerOffset - 1);
-      final int hashLog = MEMORY_USAGE + 3 - bitsPerOffsetLog;
-      final PackedInts.Mutable hashTable = PackedInts.getMutable(1 << hashLog, bitsPerOffset, PackedInts.DEFAULT);
-
-      main:
-      while (off < limit) {
-        // find a match
-        int ref;
-        while (true) {
-          if (off >= matchLimit) {
-            break main;
-          }
-          final int v = readInt(bytes, off);
-          final int h = hash(v, hashLog);
-          ref = base + (int) hashTable.get(h);
-          assert PackedInts.bitsRequired(off - base) <= hashTable.getBitsPerValue();
-          hashTable.set(h, off - base);
-          if (off - ref < MAX_DISTANCE && readInt(bytes, ref) == v) {
-            break;
-          }
-          ++off;
-        }
-
-        // compute match length
-        final int matchLen = MIN_MATCH + commonBytes(bytes, ref + 4, off + 4, limit);
-
-        encodeSequence(bytes, anchor, ref, off, matchLen, out);
-        off += matchLen;
-        anchor = off;
-      }
-    }
-
-    // last literals
-    final int literalLen = end - anchor;
-    assert literalLen >= LAST_LITERALS || literalLen == len;
-    encodeLastLiterals(bytes, anchor, end - anchor, out);
-  }
-
-  private static class Match {
-    int start, ref, len;
-
-    void fix(int correction) {
-      start += correction;
-      ref += correction;
-      len -= correction;
-    }
-
-    int end() {
-      return start + len;
-    }
-  }
-
-  private static void copyTo(Match m1, Match m2) {
-    m2.len = m1.len;
-    m2.start = m1.start;
-    m2.ref = m1.ref;
-  }
-
-  private static class HashTable {
-    static final int MAX_ATTEMPTS = 256;
-    static final int MASK = MAX_DISTANCE - 1;
-    int nextToUpdate;
-    private final int base;
-    private final int[] hashTable;
-    private final short[] chainTable;
-
-    HashTable(int base) {
-      this.base = base;
-      nextToUpdate = base;
-      hashTable = new int[HASH_TABLE_SIZE_HC];
-      Arrays.fill(hashTable, -1);
-      chainTable = new short[MAX_DISTANCE];
-    }
-
-    private int hashPointer(byte[] bytes, int off) {
-      final int v = readInt(bytes, off);
-      final int h = hashHC(v);
-      return base + hashTable[h];
-    }
-
-    private int next(int off) {
-      return base + off - (chainTable[off & MASK] & 0xFFFF);
-    }
-
-    private void addHash(byte[] bytes, int off) {
-      final int v = readInt(bytes, off);
-      final int h = hashHC(v);
-      int delta = off - hashTable[h];
-      if (delta >= MAX_DISTANCE) {
-        delta = MAX_DISTANCE - 1;
-      }
-      chainTable[off & MASK] = (short) delta;
-      hashTable[h] = off - base;
-    }
-
-    void insert(int off, byte[] bytes) {
-      for (; nextToUpdate < off; ++nextToUpdate) {
-        addHash(bytes, nextToUpdate);
-      }
-    }
-
-    boolean insertAndFindBestMatch(byte[] buf, int off, int matchLimit, Match match) {
-      match.start = off;
-      match.len = 0;
-
-      insert(off, buf);
-
-      int ref = hashPointer(buf, off);
-      for (int i = 0; i < MAX_ATTEMPTS; ++i) {
-        if (ref < Math.max(base, off - MAX_DISTANCE + 1)) {
-          break;
-        }
-        if (buf[ref + match.len] == buf[off + match.len] && readIntEquals(buf, ref, off)) {
-          final int matchLen = MIN_MATCH + commonBytes(buf, ref + MIN_MATCH, off + MIN_MATCH, matchLimit);
-          if (matchLen > match.len) {
-            match.ref = ref;
-            match.len = matchLen;
-          }
-        }
-        ref = next(ref);
-      }
-
-      return match.len != 0;
-    }
-
-    boolean insertAndFindWiderMatch(byte[] buf, int off, int startLimit, int matchLimit, int minLen, Match match) {
-      match.len = minLen;
-
-      insert(off, buf);
-
-      final int delta = off - startLimit;
-      int ref = hashPointer(buf, off);
-      for (int i = 0; i < MAX_ATTEMPTS; ++i) {
-        if (ref < Math.max(base, off - MAX_DISTANCE + 1)) {
-          break;
-        }
-        if (buf[ref - delta + match.len] == buf[startLimit + match.len]
-            && readIntEquals(buf, ref, off)) {
-          final int matchLenForward = MIN_MATCH + commonBytes(buf, ref + MIN_MATCH, off + MIN_MATCH, matchLimit);
-          final int matchLenBackward = commonBytesBackward(buf, ref, off, base, startLimit);
-          final int matchLen = matchLenBackward + matchLenForward;
-          if (matchLen > match.len) {
-            match.len = matchLen;
-            match.ref = ref - matchLenBackward;
-            match.start = off - matchLenBackward;
-          }
-        }
-        ref = next(ref);
-      }
-
-      return match.len > minLen;
-    }
-
-  }
-
-  /**
-   * Compress <code>bytes[off:off+len]</code> into <code>out</code>. Compared to
-   * {@link LZ4#compress(byte[], int, int, DataOutput)}, this method is slower,
-   * uses more memory (~ 256KB), but should provide better compression ratios
-   * (especially on large inputs) because it chooses the best match among up to
-   * 256 candidates and then performs trade-offs to fix overlapping matches.
-   */
-  public static void compressHC(byte[] src, int srcOff, int srcLen, DataOutput out) throws IOException {
-
-    final int srcEnd = srcOff + srcLen;
-    final int matchLimit = srcEnd - LAST_LITERALS;
-
-    int sOff = srcOff;
-    int anchor = sOff++;
-
-    final HashTable ht = new HashTable(srcOff);
-    final Match match0 = new Match();
-    final Match match1 = new Match();
-    final Match match2 = new Match();
-    final Match match3 = new Match();
-
-    main:
-    while (sOff < matchLimit) {
-      if (!ht.insertAndFindBestMatch(src, sOff, matchLimit, match1)) {
-        ++sOff;
-        continue;
-      }
-
-      // saved, in case we would skip too much
-      copyTo(match1, match0);
-
-      search2:
-      while (true) {
-        assert match1.start >= anchor;
-        if (match1.end() >= matchLimit
-            || !ht.insertAndFindWiderMatch(src, match1.end() - 2, match1.start + 1, matchLimit, match1.len, match2)) {
-          // no better match
-          encodeSequence(src, anchor, match1.ref, match1.start, match1.len, out);
-          anchor = sOff = match1.end();
-          continue main;
-        }
-
-        if (match0.start < match1.start) {
-          if (match2.start < match1.start + match0.len) { // empirical
-            copyTo(match0, match1);
-          }
-        }
-        assert match2.start > match1.start;
-
-        if (match2.start - match1.start < 3) { // First Match too small : removed
-          copyTo(match2, match1);
-          continue search2;
-        }
-
-        search3:
-        while (true) {
-          if (match2.start - match1.start < OPTIMAL_ML) {
-            int newMatchLen = match1.len;
-            if (newMatchLen > OPTIMAL_ML) {
-              newMatchLen = OPTIMAL_ML;
-            }
-            if (match1.start + newMatchLen > match2.end() - MIN_MATCH) {
-              newMatchLen = match2.start - match1.start + match2.len - MIN_MATCH;
-            }
-            final int correction = newMatchLen - (match2.start - match1.start);
-            if (correction > 0) {
-              match2.fix(correction);
-            }
-          }
-
-          if (match2.start + match2.len >= matchLimit
-              || !ht.insertAndFindWiderMatch(src, match2.end() - 3, match2.start, matchLimit, match2.len, match3)) {
-            // no better match -> 2 sequences to encode
-            if (match2.start < match1.end()) {
-              if (match2.start - match1.start < OPTIMAL_ML) {
-                if (match1.len > OPTIMAL_ML) {
-                  match1.len = OPTIMAL_ML;
-                }
-                if (match1.end() > match2.end() - MIN_MATCH) {
-                  match1.len = match2.end() - match1.start - MIN_MATCH;
-                }
-                final int correction = match1.len - (match2.start - match1.start);
-                if (correction > 0) {
-                  match2.fix(correction);
-                }
-              } else {
-                match1.len = match2.start - match1.start;
-              }
-            }
-            // encode seq 1
-            encodeSequence(src, anchor, match1.ref, match1.start, match1.len, out);
-            anchor = sOff = match1.end();
-            // encode seq 2
-            encodeSequence(src, anchor, match2.ref, match2.start, match2.len, out);
-            anchor = sOff = match2.end();
-            continue main;
-          }
-
-          if (match3.start < match1.end() + 3) { // Not enough space for match 2 : remove it
-            if (match3.start >= match1.end()) { // // can write Seq1 immediately ==> Seq2 is removed, so Seq3 becomes Seq1
-              if (match2.start < match1.end()) {
-                final int correction = match1.end() - match2.start;
-                match2.fix(correction);
-                if (match2.len < MIN_MATCH) {
-                  copyTo(match3, match2);
-                }
-              }
-
-              encodeSequence(src, anchor, match1.ref, match1.start, match1.len, out);
-              anchor = sOff = match1.end();
-
-              copyTo(match3, match1);
-              copyTo(match2, match0);
-
-              continue search2;
-            }
-
-            copyTo(match3, match2);
-            continue search3;
-          }
-
-          // OK, now we have 3 ascending matches; let's write at least the first one
-          if (match2.start < match1.end()) {
-            if (match2.start - match1.start < 0x0F) {
-              if (match1.len > OPTIMAL_ML) {
-                match1.len = OPTIMAL_ML;
-              }
-              if (match1.end() > match2.end() - MIN_MATCH) {
-                match1.len = match2.end() - match1.start - MIN_MATCH;
-              }
-              final int correction = match1.end() - match2.start;
-              match2.fix(correction);
-            } else {
-              match1.len = match2.start - match1.start;
-            }
-          }
-
-          encodeSequence(src, anchor, match1.ref, match1.start, match1.len, out);
-          anchor = sOff = match1.end();
-
-          copyTo(match2, match1);
-          copyTo(match3, match2);
-
-          continue search3;
-        }
-
-      }
-
-    }
-
-    encodeLastLiterals(src, anchor, srcEnd - anchor, out);
-  }
-
-  /** Copy bytes from <code>in</code> to <code>out</code> where
-   *  <code>in</code> is a LZ4-encoded stream. This method copies enough bytes
-   *  so that <code>out</code> can be used later on to restore the first
-   *  <code>length</code> bytes of the stream. This method always reads at
-   *  least one byte from <code>in</code> so make sure not to call this method
-   *  if <code>in</code> reached the end of the stream, even if
-   *  <code>length=0</code>. */
-  public static int copyCompressedData(DataInput in, int length, DataOutput out) throws IOException {
-    int n = 0;
-    do {
-      // literals
-      final byte token = in.readByte();
-      out.writeByte(token);
-      int literalLen = (token & 0xFF) >>> 4;
-      if (literalLen == 0x0F) {
-        byte len;
-        while ((len = in.readByte()) == (byte) 0xFF) {
-          literalLen += 0xFF;
-          out.writeByte(len);
-        }
-        literalLen += len & 0xFF;
-        out.writeByte(len);
-      }
-      out.copyBytes(in, literalLen);
-      n += literalLen;
-      if (n >= length) {
-        break;
-      }
-
-      // matchs
-      out.copyBytes(in, 2); // match dec
-      int matchLen = token & 0x0F;
-      if (matchLen == 0x0F) {
-        byte len;
-        while ((len = in.readByte()) == (byte) 0xFF) {
-          matchLen += 0xFF;
-          out.writeByte(len);
-        }
-        matchLen += len & 0xFF;
-        out.writeByte(len);
-      }
-      matchLen += MIN_MATCH;
-      n += matchLen;
-    } while (n < length);
-    return n;
-  }
-
-}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/compressing/package.html b/lucene/codecs/src/java/org/apache/lucene/codecs/compressing/package.html
deleted file mode 100644
index 4d899f7..0000000
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/compressing/package.html
+++ /dev/null
@@ -1,25 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
-</head>
-<body>
-StoredFieldsFormat that allows cross-document and cross-field compression of stored fields.
-</body>
-</html>
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsFormat.java
new file mode 100644
index 0000000..ae4d279
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsFormat.java
@@ -0,0 +1,111 @@
+package org.apache.lucene.codecs.compressing;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.StoredFieldsFormat;
+import org.apache.lucene.codecs.StoredFieldsReader;
+import org.apache.lucene.codecs.StoredFieldsWriter;
+import org.apache.lucene.codecs.lucene40.Lucene40StoredFieldsFormat;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.MergePolicy;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+
+/**
+ * A {@link StoredFieldsFormat} that is very similar to
+ * {@link Lucene40StoredFieldsFormat} but compresses documents in chunks in
+ * order to improve the compression ratio.
+ * <p>
+ * For a chunk size of <tt>chunkSize</tt> bytes, this {@link StoredFieldsFormat}
+ * does not support documents larger than (<tt>2<sup>31</sup> - chunkSize</tt>)
+ * bytes. In case this is a problem, you should use another format, such as
+ * {@link Lucene40StoredFieldsFormat}.
+ * <p>
+ * For optimal performance, you should use a {@link MergePolicy} that returns
+ * segments that have the biggest byte size first.
+ * @lucene.experimental
+ */
+public class CompressingStoredFieldsFormat extends StoredFieldsFormat {
+
+  private final CompressionMode compressionMode;
+  private final int chunkSize;
+
+  /**
+   * Create a new {@link CompressingStoredFieldsFormat}.
+   * <p>
+   * The <code>compressionMode</code> parameter allows you to choose between
+   * compression algorithms that have various compression and decompression
+   * speeds so that you can pick the one that best fits your indexing and
+   * searching throughput.
+   * <p>
+   * <code>chunkSize</code> is the minimum byte size of a chunk of documents.
+   * A value of <code>1</code> can make sense if there is redundancy across
+   * fields. In that case, both performance and compression ratio should be
+   * better than with {@link Lucene40StoredFieldsFormat} with compressed
+   * fields.
+   * <p>
+   * Higher values of <code>chunkSize</code> should improve the compression
+   * ratio but will require more memory at indexing time and might make document
+   * loading a little slower (depending on the size of your OS cache compared
+   * to the size of your index).
+   *
+   * @param compressionMode the {@link CompressionMode} to use
+   * @param chunkSize the minimum number of bytes of a single chunk of stored documents
+   * @see CompressionMode
+   */
+  public CompressingStoredFieldsFormat(CompressionMode compressionMode, int chunkSize) {
+    this.compressionMode = compressionMode;
+    if (chunkSize < 1) {
+      throw new IllegalArgumentException("chunkSize must be >= 1");
+    }
+    this.chunkSize = chunkSize;
+  }
+
+  /**
+   * Create a new {@link CompressingStoredFieldsFormat} with
+   * {@link CompressionMode#FAST} compression and chunks of <tt>16 KB</tt>.
+   *
+   * @see CompressingStoredFieldsFormat#CompressingStoredFieldsFormat(CompressionMode, int)
+   */
+  public CompressingStoredFieldsFormat() {
+    this(CompressionMode.FAST, 1 << 14);
+  }
+
+  @Override
+  public StoredFieldsReader fieldsReader(Directory directory, SegmentInfo si,
+      FieldInfos fn, IOContext context) throws IOException {
+    return new CompressingStoredFieldsReader(directory, si, fn, context);
+  }
+
+  @Override
+  public StoredFieldsWriter fieldsWriter(Directory directory, SegmentInfo si,
+      IOContext context) throws IOException {
+    return new CompressingStoredFieldsWriter(directory, si, context,
+        compressionMode, chunkSize);
+  }
+
+  @Override
+  public String toString() {
+    return getClass().getSimpleName() + "(compressionMode=" + compressionMode
+        + ", chunkSize=" + chunkSize + ")";
+  }
+
+}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsIndexReader.java b/lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsIndexReader.java
new file mode 100644
index 0000000..04989ba
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsIndexReader.java
@@ -0,0 +1,183 @@
+package org.apache.lucene.codecs.compressing;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.Closeable;
+import java.io.IOException;
+import java.util.Arrays;
+
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.packed.PackedInts;
+
+class CompressingStoredFieldsIndexReader implements Closeable, Cloneable {
+
+  final IndexInput fieldsIndexIn;
+
+  static long moveLowOrderBitToSign(long n) {
+    return ((n >>> 1) ^ -(n & 1));
+  }
+
+  final int maxDoc;
+  final int[] docBases;
+  final long[] startPointers;
+  final int[] avgChunkDocs;
+  final long[] avgChunkSizes;
+  final PackedInts.Reader[] docBasesDeltas; // delta from the avg
+  final PackedInts.Reader[] startPointersDeltas; // delta from the avg
+
+  CompressingStoredFieldsIndexReader(IndexInput fieldsIndexIn, SegmentInfo si) throws IOException {
+    this.fieldsIndexIn = fieldsIndexIn;
+    maxDoc = si.getDocCount();
+    int[] docBases = new int[16];
+    long[] startPointers = new long[16];
+    int[] avgChunkDocs = new int[16];
+    long[] avgChunkSizes = new long[16];
+    PackedInts.Reader[] docBasesDeltas = new PackedInts.Reader[16];
+    PackedInts.Reader[] startPointersDeltas = new PackedInts.Reader[16];
+
+    final int packedIntsVersion = fieldsIndexIn.readVInt();
+
+    int blockCount = 0;
+
+    for (;;) {
+      final int numChunks = fieldsIndexIn.readVInt();
+      if (numChunks == 0) {
+        break;
+      }
+      if (blockCount == docBases.length) {
+        final int newSize = ArrayUtil.oversize(blockCount + 1, 8);
+        docBases = Arrays.copyOf(docBases, newSize);
+        startPointers = Arrays.copyOf(startPointers, newSize);
+        avgChunkDocs = Arrays.copyOf(avgChunkDocs, newSize);
+        avgChunkSizes = Arrays.copyOf(avgChunkSizes, newSize);
+        docBasesDeltas = Arrays.copyOf(docBasesDeltas, newSize);
+        startPointersDeltas = Arrays.copyOf(startPointersDeltas, newSize);
+      }
+
+      // doc bases
+      docBases[blockCount] = fieldsIndexIn.readVInt();
+      avgChunkDocs[blockCount] = fieldsIndexIn.readVInt();
+      final int bitsPerDocBase = fieldsIndexIn.readVInt();
+      if (bitsPerDocBase > 32) {
+        throw new CorruptIndexException("Corrupted");
+      }
+      docBasesDeltas[blockCount] = PackedInts.getReaderNoHeader(fieldsIndexIn, PackedInts.Format.PACKED, packedIntsVersion, numChunks, bitsPerDocBase);
+
+      // start pointers
+      startPointers[blockCount] = fieldsIndexIn.readVLong();
+      avgChunkSizes[blockCount] = fieldsIndexIn.readVLong();
+      final int bitsPerStartPointer = fieldsIndexIn.readVInt();
+      if (bitsPerStartPointer > 64) {
+        throw new CorruptIndexException("Corrupted");
+      }
+      startPointersDeltas[blockCount] = PackedInts.getReaderNoHeader(fieldsIndexIn, PackedInts.Format.PACKED, packedIntsVersion, numChunks, bitsPerStartPointer);
+
+      ++blockCount;
+    }
+
+    this.docBases = Arrays.copyOf(docBases, blockCount);
+    this.startPointers = Arrays.copyOf(startPointers, blockCount);
+    this.avgChunkDocs = Arrays.copyOf(avgChunkDocs, blockCount);
+    this.avgChunkSizes = Arrays.copyOf(avgChunkSizes, blockCount);
+    this.docBasesDeltas = Arrays.copyOf(docBasesDeltas, blockCount);
+    this.startPointersDeltas = Arrays.copyOf(startPointersDeltas, blockCount);
+  }
+
+  private CompressingStoredFieldsIndexReader(CompressingStoredFieldsIndexReader other) {
+    this.fieldsIndexIn = null;
+    this.maxDoc = other.maxDoc;
+    this.docBases = other.docBases;
+    this.startPointers = other.startPointers;
+    this.avgChunkDocs = other.avgChunkDocs;
+    this.avgChunkSizes = other.avgChunkSizes;
+    this.docBasesDeltas = other.docBasesDeltas;
+    this.startPointersDeltas = other.startPointersDeltas;
+  }
+
+  private int block(int docID) {
+    int lo = 0, hi = docBases.length - 1;
+    while (lo <= hi) {
+      final int mid = (lo + hi) >>> 1;
+      final int midValue = docBases[mid];
+      if (midValue == docID) {
+        return mid;
+      } else if (midValue < docID) {
+        lo = mid + 1;
+      } else {
+        hi = mid - 1;
+      }
+    }
+    return hi;
+  }
+
+  private int relativeDocBase(int block, int relativeChunk) {
+    final int expected = avgChunkDocs[block] * relativeChunk;
+    final long delta = moveLowOrderBitToSign(docBasesDeltas[block].get(relativeChunk));
+    return expected + (int) delta;
+  }
+
+  private long relativeStartPointer(int block, int relativeChunk) {
+    final long expected = avgChunkSizes[block] * relativeChunk;
+    final long delta = moveLowOrderBitToSign(startPointersDeltas[block].get(relativeChunk));
+    return expected + delta;
+  }
+
+  private int relativeChunk(int block, int relativeDoc) {
+    int lo = 0, hi = docBasesDeltas[block].size() - 1;
+    while (lo <= hi) {
+      final int mid = (lo + hi) >>> 1;
+      final int midValue = relativeDocBase(block, mid);
+      if (midValue == relativeDoc) {
+        return mid;
+      } else if (midValue < relativeDoc) {
+        lo = mid + 1;
+      } else {
+        hi = mid - 1;
+      }
+    }
+    return hi;
+  }
+
+  long getStartPointer(int docID) {
+    if (docID < 0 || docID >= maxDoc) {
+      throw new IllegalArgumentException("docID out of range [0-" + maxDoc + "]: " + docID);
+    }
+    final int block = block(docID);
+    final int relativeChunk = relativeChunk(block, docID - docBases[block]);
+    return startPointers[block] + relativeStartPointer(block, relativeChunk);
+  }
+
+  @Override
+  public CompressingStoredFieldsIndexReader clone() {
+    if (fieldsIndexIn == null) {
+      return this;
+    } else {
+      return new CompressingStoredFieldsIndexReader(this);
+    }
+  }
+
+  @Override
+  public void close() throws IOException {
+    IOUtils.close(fieldsIndexIn);
+  }
+
+}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsIndexWriter.java b/lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsIndexWriter.java
new file mode 100644
index 0000000..2fcfbe2
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsIndexWriter.java
@@ -0,0 +1,165 @@
+package org.apache.lucene.codecs.compressing;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.Closeable;
+import java.io.IOException;
+
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.packed.PackedInts;
+
+class CompressingStoredFieldsIndexWriter implements Closeable {
+  
+  static final int BLOCK_SIZE = 1024; // number of chunks to serialize at once
+
+  static long moveSignToLowOrderBit(long n) {
+    return (n >> 63) ^ (n << 1);
+  }
+
+  final IndexOutput fieldsIndexOut;
+  int totalDocs;
+  int blockDocs;
+  int blockChunks;
+  long firstStartPointer;
+  long maxStartPointer;
+  final int[] docBaseDeltas;
+  final long[] startPointerDeltas;
+
+  CompressingStoredFieldsIndexWriter(IndexOutput indexOutput) throws IOException {
+    this.fieldsIndexOut = indexOutput;
+    reset();
+    totalDocs = 0;
+    docBaseDeltas = new int[BLOCK_SIZE];
+    startPointerDeltas = new long[BLOCK_SIZE];
+    fieldsIndexOut.writeVInt(PackedInts.VERSION_CURRENT);
+  }
+
+  private void reset() {
+    blockChunks = 0;
+    blockDocs = 0;
+    firstStartPointer = -1; // means unset
+  }
+
+  private void writeBlock() throws IOException {
+    assert blockChunks > 0;
+    fieldsIndexOut.writeVInt(blockChunks);
+
+    // The trick here is that we only store the difference from the average start
+    // pointer or doc base, this helps save bits per value.
+    // And in order to prevent a few chunks that would be far from the average to
+    // raise the number of bits per value for all of them, we only encode blocks
+    // of 1024 chunks at once
+    // See LUCENE-4512
+
+    // doc bases
+    final int avgChunkDocs;
+    if (blockChunks == 1) {
+      avgChunkDocs = 0;
+    } else {
+      avgChunkDocs = Math.round((float) (blockDocs - docBaseDeltas[blockChunks - 1]) / (blockChunks - 1));
+    }
+    fieldsIndexOut.writeVInt(totalDocs - blockDocs); // docBase
+    fieldsIndexOut.writeVInt(avgChunkDocs);
+    int docBase = 0;
+    long maxDelta = 0;
+    for (int i = 0; i < blockChunks; ++i) {
+      final int delta = docBase - avgChunkDocs * i;
+      maxDelta |= moveSignToLowOrderBit(delta);
+      docBase += docBaseDeltas[i];
+    }
+
+    final int bitsPerDocBase = PackedInts.bitsRequired(maxDelta);
+    fieldsIndexOut.writeVInt(bitsPerDocBase);
+    PackedInts.Writer writer = PackedInts.getWriterNoHeader(fieldsIndexOut,
+        PackedInts.Format.PACKED, blockChunks, bitsPerDocBase, 1);
+    docBase = 0;
+    for (int i = 0; i < blockChunks; ++i) {
+      final long delta = docBase - avgChunkDocs * i;
+      assert PackedInts.bitsRequired(moveSignToLowOrderBit(delta)) <= writer.bitsPerValue();
+      writer.add(moveSignToLowOrderBit(delta));
+      docBase += docBaseDeltas[i];
+    }
+    writer.finish();
+
+    // start pointers
+    fieldsIndexOut.writeVLong(firstStartPointer);
+    final long avgChunkSize;
+    if (blockChunks == 1) {
+      avgChunkSize = 0;
+    } else {
+      avgChunkSize = (maxStartPointer - firstStartPointer) / (blockChunks - 1);
+    }
+    fieldsIndexOut.writeVLong(avgChunkSize);
+    long startPointer = 0;
+    maxDelta = 0;
+    for (int i = 0; i < blockChunks; ++i) {
+      startPointer += startPointerDeltas[i];
+      final long delta = startPointer - avgChunkSize * i;
+      maxDelta |= moveSignToLowOrderBit(delta);
+    }
+
+    final int bitsPerStartPointer = PackedInts.bitsRequired(maxDelta);
+    fieldsIndexOut.writeVInt(bitsPerStartPointer);
+    writer = PackedInts.getWriterNoHeader(fieldsIndexOut, PackedInts.Format.PACKED,
+        blockChunks, bitsPerStartPointer, 1);
+    startPointer = 0;
+    for (int i = 0; i < blockChunks; ++i) {
+      startPointer += startPointerDeltas[i];
+      final long delta = startPointer - avgChunkSize * i;
+      assert PackedInts.bitsRequired(moveSignToLowOrderBit(delta)) <= writer.bitsPerValue();
+      writer.add(moveSignToLowOrderBit(delta));
+    }
+    writer.finish();
+  }
+
+  void writeIndex(int numDocs, long startPointer) throws IOException {
+    if (blockChunks == BLOCK_SIZE) {
+      writeBlock();
+      reset();
+    }
+
+    if (firstStartPointer == -1) {
+      firstStartPointer = maxStartPointer = startPointer;
+    }
+    assert firstStartPointer > 0 && startPointer >= firstStartPointer;
+
+    docBaseDeltas[blockChunks] = numDocs;
+    startPointerDeltas[blockChunks] = startPointer - maxStartPointer;
+
+    ++blockChunks;
+    blockDocs += numDocs;
+    totalDocs += numDocs;
+    maxStartPointer = startPointer;
+  }
+
+  void finish(int numDocs) throws IOException {
+    if (numDocs != totalDocs) {
+      throw new IllegalStateException("Expected " + numDocs + " docs, but got " + totalDocs);
+    }
+    if (blockChunks > 0) {
+      writeBlock();
+    }
+    fieldsIndexOut.writeVInt(0); // end marker
+  }
+
+  @Override
+  public void close() throws IOException {
+    fieldsIndexOut.close();
+  }
+
+}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsReader.java b/lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsReader.java
new file mode 100644
index 0000000..afb5e86
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsReader.java
@@ -0,0 +1,406 @@
+package org.apache.lucene.codecs.compressing;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import static org.apache.lucene.codecs.compressing.CompressingStoredFieldsWriter.BYTE_ARR;
+import static org.apache.lucene.codecs.compressing.CompressingStoredFieldsWriter.CODEC_NAME_DAT;
+import static org.apache.lucene.codecs.compressing.CompressingStoredFieldsWriter.CODEC_NAME_IDX;
+import static org.apache.lucene.codecs.compressing.CompressingStoredFieldsWriter.HEADER_LENGTH_DAT;
+import static org.apache.lucene.codecs.compressing.CompressingStoredFieldsWriter.HEADER_LENGTH_IDX;
+import static org.apache.lucene.codecs.compressing.CompressingStoredFieldsWriter.NUMERIC_DOUBLE;
+import static org.apache.lucene.codecs.compressing.CompressingStoredFieldsWriter.NUMERIC_FLOAT;
+import static org.apache.lucene.codecs.compressing.CompressingStoredFieldsWriter.NUMERIC_INT;
+import static org.apache.lucene.codecs.compressing.CompressingStoredFieldsWriter.NUMERIC_LONG;
+import static org.apache.lucene.codecs.compressing.CompressingStoredFieldsWriter.STRING;
+import static org.apache.lucene.codecs.compressing.CompressingStoredFieldsWriter.TYPE_BITS;
+import static org.apache.lucene.codecs.compressing.CompressingStoredFieldsWriter.TYPE_MASK;
+import static org.apache.lucene.codecs.compressing.CompressingStoredFieldsWriter.VERSION_CURRENT;
+import static org.apache.lucene.codecs.compressing.CompressingStoredFieldsWriter.VERSION_START;
+import static org.apache.lucene.codecs.lucene40.Lucene40StoredFieldsWriter.FIELDS_EXTENSION;
+import static org.apache.lucene.codecs.lucene40.Lucene40StoredFieldsWriter.FIELDS_INDEX_EXTENSION;
+
+import java.io.IOException;
+import java.util.Arrays;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.StoredFieldsReader;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.StoredFieldVisitor;
+import org.apache.lucene.store.AlreadyClosedException;
+import org.apache.lucene.store.ByteArrayDataInput;
+import org.apache.lucene.store.DataOutput;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.packed.PackedInts;
+
+final class CompressingStoredFieldsReader extends StoredFieldsReader {
+
+  private final FieldInfos fieldInfos;
+  private final CompressingStoredFieldsIndexReader indexReader;
+  private final IndexInput fieldsStream;
+  private final int packedIntsVersion;
+  private final CompressionMode compressionMode;
+  private final Decompressor decompressor;
+  private final BytesRef bytes;
+  private final int numDocs;
+  private boolean closed;
+
+  // used by clone
+  private CompressingStoredFieldsReader(CompressingStoredFieldsReader reader) {
+    this.fieldInfos = reader.fieldInfos;
+    this.fieldsStream = reader.fieldsStream.clone();
+    this.indexReader = reader.indexReader.clone();
+    this.packedIntsVersion = reader.packedIntsVersion;
+    this.compressionMode = reader.compressionMode;
+    this.decompressor = reader.decompressor.clone();
+    this.numDocs = reader.numDocs;
+    this.bytes = new BytesRef(reader.bytes.bytes.length);
+    this.closed = false;
+  }
+
+  public CompressingStoredFieldsReader(Directory d, SegmentInfo si, FieldInfos fn, IOContext context) throws IOException {
+    final String segment = si.name;
+    boolean success = false;
+    fieldInfos = fn;
+    numDocs = si.getDocCount();
+    IndexInput indexStream = null;
+    try {
+      fieldsStream = d.openInput(IndexFileNames.segmentFileName(segment, "", FIELDS_EXTENSION), context);
+      final String indexStreamFN = IndexFileNames.segmentFileName(segment, "", FIELDS_INDEX_EXTENSION);
+      indexStream = d.openInput(indexStreamFN, context);
+
+      CodecUtil.checkHeader(indexStream, CODEC_NAME_IDX, VERSION_START, VERSION_CURRENT);
+      CodecUtil.checkHeader(fieldsStream, CODEC_NAME_DAT, VERSION_START, VERSION_CURRENT);
+      assert HEADER_LENGTH_DAT == fieldsStream.getFilePointer();
+      assert HEADER_LENGTH_IDX == indexStream.getFilePointer();
+
+      indexReader = new CompressingStoredFieldsIndexReader(indexStream, si);
+      indexStream = null;
+
+      packedIntsVersion = fieldsStream.readVInt();
+      final int compressionModeId = fieldsStream.readVInt();
+      compressionMode = CompressionMode.byId(compressionModeId);
+      decompressor = compressionMode.newDecompressor();
+      this.bytes = new BytesRef();
+
+      success = true;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(this, indexStream);
+      }
+    }
+  }
+
+  /**
+   * @throws AlreadyClosedException if this FieldsReader is closed
+   */
+  private void ensureOpen() throws AlreadyClosedException {
+    if (closed) {
+      throw new AlreadyClosedException("this FieldsReader is closed");
+    }
+  }
+
+  @Override
+  public void close() throws IOException {
+    if (!closed) {
+      IOUtils.close(fieldsStream, indexReader);
+      closed = true;
+    }
+  }
+
+  private static void readField(ByteArrayDataInput in, StoredFieldVisitor visitor, FieldInfo info, int bits) throws IOException {
+    switch (bits & TYPE_MASK) {
+      case BYTE_ARR:
+        int length = in.readVInt();
+        byte[] data = new byte[length];
+        in.readBytes(data, 0, length);
+        visitor.binaryField(info, data);
+        break;
+      case STRING:
+        length = in.readVInt();
+        data = new byte[length];
+        in.readBytes(data, 0, length);
+        visitor.stringField(info, new String(data, IOUtils.CHARSET_UTF_8));
+        break;
+      case NUMERIC_INT:
+        visitor.intField(info, in.readInt());
+        break;
+      case NUMERIC_FLOAT:
+        visitor.floatField(info, Float.intBitsToFloat(in.readInt()));
+        break;
+      case NUMERIC_LONG:
+        visitor.longField(info, in.readLong());
+        break;
+      case NUMERIC_DOUBLE:
+        visitor.doubleField(info, Double.longBitsToDouble(in.readLong()));
+        break;
+      default:
+        throw new AssertionError("Unknown type flag: " + Integer.toHexString(bits));
+    }
+  }
+
+  private static void skipField(ByteArrayDataInput in, int bits) throws IOException {
+    switch (bits & TYPE_MASK) {
+      case BYTE_ARR:
+      case STRING:
+        final int length = in.readVInt();
+        in.skipBytes(length);
+        break;
+      case NUMERIC_INT:
+      case NUMERIC_FLOAT:
+        in.readInt();
+        break;
+      case NUMERIC_LONG:
+      case NUMERIC_DOUBLE:
+        in.readLong();
+        break;
+      default:
+        throw new AssertionError("Unknown type flag: " + Integer.toHexString(bits));
+    }
+  }
+
+  @Override
+  public void visitDocument(int docID, StoredFieldVisitor visitor)
+      throws IOException {
+    fieldsStream.seek(indexReader.getStartPointer(docID));
+
+    final int docBase = fieldsStream.readVInt();
+    final int chunkDocs = fieldsStream.readVInt();
+    if (docID < docBase
+        || docID >= docBase + chunkDocs
+        || docBase + chunkDocs > numDocs) {
+      throw new CorruptIndexException("Corrupted: docID=" + docID
+          + ", docBase=" + docBase + ", chunkDocs=" + chunkDocs
+          + ", numDocs=" + numDocs);
+    }
+
+    final int numStoredFields, offset, length, totalLength;
+    if (chunkDocs == 1) {
+      numStoredFields = fieldsStream.readVInt();
+      offset = 0;
+      length = fieldsStream.readVInt();
+      totalLength = length;
+    } else {
+      final int bitsPerStoredFields = fieldsStream.readVInt();
+      if (bitsPerStoredFields == 0) {
+        numStoredFields = fieldsStream.readVInt();
+      } else if (bitsPerStoredFields > 31) {
+        throw new CorruptIndexException("bitsPerStoredFields=" + bitsPerStoredFields);
+      } else {
+        final long filePointer = fieldsStream.getFilePointer();
+        final PackedInts.Reader reader = PackedInts.getDirectReaderNoHeader(fieldsStream, PackedInts.Format.PACKED, packedIntsVersion, chunkDocs, bitsPerStoredFields);
+        numStoredFields = (int) (reader.get(docID - docBase));
+        fieldsStream.seek(filePointer + PackedInts.Format.PACKED.byteCount(packedIntsVersion, chunkDocs, bitsPerStoredFields));
+      }
+
+      final int bitsPerLength = fieldsStream.readVInt();
+      if (bitsPerLength == 0) {
+        length = fieldsStream.readVInt();
+        offset = (docID - docBase) * length;
+        totalLength = chunkDocs * length;
+      } else if (bitsPerStoredFields > 31) {
+        throw new CorruptIndexException("bitsPerLength=" + bitsPerLength);
+      } else {
+        final PackedInts.ReaderIterator it = PackedInts.getReaderIteratorNoHeader(fieldsStream, PackedInts.Format.PACKED, packedIntsVersion, chunkDocs, bitsPerLength, 1);
+        int off = 0;
+        for (int i = 0; i < docID - docBase; ++i) {
+          off += it.next();
+        }
+        offset = off;
+        length = (int) it.next();
+        off += length;
+        for (int i = docID - docBase + 1; i < chunkDocs; ++i) {
+          off += it.next();
+        }
+        totalLength = off;
+      }
+    }
+
+    if ((length == 0) != (numStoredFields == 0)) {
+      throw new CorruptIndexException("length=" + length + ", numStoredFields=" + numStoredFields);
+    }
+    if (numStoredFields == 0) {
+      // nothing to do
+      return;
+    }
+
+    decompressor.decompress(fieldsStream, totalLength, offset, length, bytes);
+    assert bytes.length == length;
+
+    final ByteArrayDataInput documentInput = new ByteArrayDataInput(bytes.bytes, bytes.offset, bytes.length);
+    for (int fieldIDX = 0; fieldIDX < numStoredFields; fieldIDX++) {
+      final long infoAndBits = documentInput.readVLong();
+      final int fieldNumber = (int) (infoAndBits >>> TYPE_BITS);
+      final FieldInfo fieldInfo = fieldInfos.fieldInfo(fieldNumber);
+
+      final int bits = (int) (infoAndBits & TYPE_MASK);
+      assert bits <= NUMERIC_DOUBLE: "bits=" + Integer.toHexString(bits);
+
+      switch(visitor.needsField(fieldInfo)) {
+        case YES:
+          readField(documentInput, visitor, fieldInfo, bits);
+          assert documentInput.getPosition() <= bytes.offset + bytes.length : documentInput.getPosition() + " " + bytes.offset + bytes.length;
+          break;
+        case NO:
+          skipField(documentInput, bits);
+          assert documentInput.getPosition() <= bytes.offset + bytes.length : documentInput.getPosition() + " " + bytes.offset + bytes.length;
+          break;
+        case STOP:
+          return;
+      }
+    }
+    assert documentInput.getPosition() == bytes.offset + bytes.length : documentInput.getPosition() + " " + bytes.offset + " " + bytes.length;
+  }
+
+  @Override
+  public StoredFieldsReader clone() {
+    ensureOpen();
+    return new CompressingStoredFieldsReader(this);
+  }
+
+  CompressionMode getCompressionMode() {
+    return compressionMode;
+  }
+
+  ChunkIterator chunkIterator(int startDocID) throws IOException {
+    ensureOpen();
+    fieldsStream.seek(indexReader.getStartPointer(startDocID));
+    return new ChunkIterator();
+  }
+
+  final class ChunkIterator {
+
+    BytesRef bytes;
+    int docBase;
+    int chunkDocs;
+    int[] numStoredFields;
+    int[] lengths;
+
+    private ChunkIterator() {
+      this.docBase = -1;
+      bytes = new BytesRef();
+      numStoredFields = new int[1];
+      lengths = new int[1];
+    }
+
+    /**
+     * Return the decompressed size of the chunk
+     */
+    int chunkSize() {
+      int sum = 0;
+      for (int i = 0; i < chunkDocs; ++i) {
+        sum += lengths[i];
+      }
+      return sum;
+    }
+
+    /**
+     * Go to the chunk containing the provided doc ID.
+     */
+    void next(int doc) throws IOException {
+      assert doc >= docBase + chunkDocs : doc + " " + docBase + " " + chunkDocs;
+      fieldsStream.seek(indexReader.getStartPointer(doc));
+
+      final int docBase = fieldsStream.readVInt();
+      final int chunkDocs = fieldsStream.readVInt();
+      if (docBase < this.docBase + this.chunkDocs
+          || docBase + chunkDocs > numDocs) {
+        throw new CorruptIndexException("Corrupted: current docBase=" + this.docBase
+            + ", current numDocs=" + this.chunkDocs + ", new docBase=" + docBase
+            + ", new numDocs=" + chunkDocs);
+      }
+      this.docBase = docBase;
+      this.chunkDocs = chunkDocs;
+
+      if (chunkDocs > numStoredFields.length) {
+        final int newLength = ArrayUtil.oversize(chunkDocs, 4);
+        numStoredFields = new int[newLength];
+        lengths = new int[newLength];
+      }
+
+      if (chunkDocs == 1) {
+        numStoredFields[0] = fieldsStream.readVInt();
+        lengths[0] = fieldsStream.readVInt();
+      } else {
+        final int bitsPerStoredFields = fieldsStream.readVInt();
+        if (bitsPerStoredFields == 0) {
+          Arrays.fill(numStoredFields, 0, chunkDocs, fieldsStream.readVInt());
+        } else if (bitsPerStoredFields > 31) {
+          throw new CorruptIndexException("bitsPerStoredFields=" + bitsPerStoredFields);
+        } else {
+          final PackedInts.ReaderIterator it = PackedInts.getReaderIteratorNoHeader(fieldsStream, PackedInts.Format.PACKED, packedIntsVersion, chunkDocs, bitsPerStoredFields, 1);
+          for (int i = 0; i < chunkDocs; ++i) {
+            numStoredFields[i] = (int) it.next();
+          }
+        }
+
+        final int bitsPerLength = fieldsStream.readVInt();
+        if (bitsPerLength == 0) {
+          Arrays.fill(lengths, 0, chunkDocs, fieldsStream.readVInt());
+        } else if (bitsPerLength > 31) {
+          throw new CorruptIndexException("bitsPerLength=" + bitsPerLength);
+        } else {
+          final PackedInts.ReaderIterator it = PackedInts.getReaderIteratorNoHeader(fieldsStream, PackedInts.Format.PACKED, packedIntsVersion, chunkDocs, bitsPerLength, 1);
+          for (int i = 0; i < chunkDocs; ++i) {
+            lengths[i] = (int) it.next();
+          }
+        }
+      }
+    }
+
+    // total length of the chunk
+    private int totalLength() {
+      int totalLength = 0;
+      for (int i = 0; i < chunkDocs; ++i) {
+        totalLength += lengths[i];
+      }
+      return totalLength;
+    }
+
+    /**
+     * Decompress the chunk.
+     */
+    void decompress() throws IOException {
+      // decompress data
+      final int totalLength = totalLength();
+      decompressor.decompress(fieldsStream, totalLength, 0, totalLength, bytes);
+      assert bytes.length == totalLength;
+      if (bytes.length != chunkSize()) {
+        throw new CorruptIndexException("Corrupted: expected chunk size = " + chunkSize() + ", got " + bytes.length);
+      }
+    }
+
+    /**
+     * Copy compressed data.
+     */
+    void copyCompressedData(DataOutput out) throws IOException {
+      final int totalLength = totalLength();
+      decompressor.copyCompressedData(fieldsStream, totalLength, out);
+    }
+
+  }
+
+}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter.java b/lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter.java
new file mode 100644
index 0000000..fb4b1dd
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter.java
@@ -0,0 +1,409 @@
+package org.apache.lucene.codecs.compressing;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import static org.apache.lucene.codecs.lucene40.Lucene40StoredFieldsWriter.FIELDS_EXTENSION;
+import static org.apache.lucene.codecs.lucene40.Lucene40StoredFieldsWriter.FIELDS_INDEX_EXTENSION;
+
+import java.io.IOException;
+import java.util.Arrays;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.StoredFieldsReader;
+import org.apache.lucene.codecs.StoredFieldsWriter;
+import org.apache.lucene.codecs.compressing.CompressingStoredFieldsReader.ChunkIterator;
+import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.MergeState;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.SegmentReader;
+import org.apache.lucene.index.StorableField;
+import org.apache.lucene.index.StoredDocument;
+import org.apache.lucene.store.DataOutput;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.packed.PackedInts;
+
+final class CompressingStoredFieldsWriter extends StoredFieldsWriter {
+
+  static final int         STRING = 0x00;
+  static final int       BYTE_ARR = 0x01;
+  static final int    NUMERIC_INT = 0x02;
+  static final int  NUMERIC_FLOAT = 0x03;
+  static final int   NUMERIC_LONG = 0x04;
+  static final int NUMERIC_DOUBLE = 0x05;
+
+  static final int TYPE_BITS = PackedInts.bitsRequired(NUMERIC_DOUBLE);
+  static final int TYPE_MASK = (int) PackedInts.maxValue(TYPE_BITS);
+
+  static final String CODEC_NAME_IDX = "CompressingStoredFieldsIndex";
+  static final String CODEC_NAME_DAT = "CompressingStoredFieldsData";
+  static final int VERSION_START = 0;
+  static final int VERSION_CURRENT = VERSION_START;
+  static final long HEADER_LENGTH_IDX = CodecUtil.headerLength(CODEC_NAME_IDX);
+  static final long HEADER_LENGTH_DAT = CodecUtil.headerLength(CODEC_NAME_DAT);
+
+  private final Directory directory;
+  private final String segment;
+  private CompressingStoredFieldsIndexWriter indexWriter;
+  private IndexOutput fieldsStream;
+
+  private final CompressionMode compressionMode;
+  private final Compressor compressor;
+  private final int chunkSize;
+
+  private final GrowableByteArrayDataOutput bufferedDocs;
+  private int[] numStoredFields; // number of stored fields
+  private int[] endOffsets; // end offsets in bufferedDocs
+  private int docBase; // doc ID at the beginning of the chunk
+  private int numBufferedDocs; // docBase + numBufferedDocs == current doc ID
+
+  public CompressingStoredFieldsWriter(Directory directory, SegmentInfo si,
+      IOContext context, CompressionMode compressionMode, int chunkSize) throws IOException {
+    assert directory != null;
+    this.directory = directory;
+    this.segment = si.name;
+    this.compressionMode = compressionMode;
+    this.compressor = compressionMode.newCompressor();
+    this.chunkSize = chunkSize;
+    this.docBase = 0;
+    this.bufferedDocs = new GrowableByteArrayDataOutput(chunkSize);
+    this.numStoredFields = new int[16];
+    this.endOffsets = new int[16];
+    this.numBufferedDocs = 0;
+
+    boolean success = false;
+    IndexOutput indexStream = directory.createOutput(IndexFileNames.segmentFileName(segment, "", FIELDS_INDEX_EXTENSION), context);
+    try {
+      fieldsStream = directory.createOutput(IndexFileNames.segmentFileName(segment, "", FIELDS_EXTENSION), context);
+
+      CodecUtil.writeHeader(indexStream, CODEC_NAME_IDX, VERSION_CURRENT);
+      CodecUtil.writeHeader(fieldsStream, CODEC_NAME_DAT, VERSION_CURRENT);
+      assert HEADER_LENGTH_IDX == indexStream.getFilePointer();
+      assert HEADER_LENGTH_DAT == fieldsStream.getFilePointer();
+
+      indexWriter = new CompressingStoredFieldsIndexWriter(indexStream);
+      indexStream = null;
+
+      fieldsStream.writeVInt(PackedInts.VERSION_CURRENT);
+      fieldsStream.writeVInt(compressionMode.getId());
+
+      success = true;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(indexStream);
+        abort();
+      }
+    }
+  }
+
+  @Override
+  public void close() throws IOException {
+    try {
+      IOUtils.close(fieldsStream, indexWriter);
+    } finally {
+      fieldsStream = null;
+      indexWriter = null;
+    }
+  }
+
+  private void endWithPreviousDocument() throws IOException {
+    if (numBufferedDocs > 0) {
+      endOffsets[numBufferedDocs - 1] = bufferedDocs.length;
+    }
+  }
+
+  @Override
+  public void startDocument(int numStoredFields) throws IOException {
+    endWithPreviousDocument();
+    if (triggerFlush()) {
+      flush();
+    }
+
+    if (numBufferedDocs == this.numStoredFields.length) {
+      final int newLength = ArrayUtil.oversize(numBufferedDocs + 1, 4);
+      this.numStoredFields = Arrays.copyOf(this.numStoredFields, newLength);
+      endOffsets = Arrays.copyOf(endOffsets, newLength);
+    }
+    this.numStoredFields[numBufferedDocs] = numStoredFields;
+    ++numBufferedDocs;
+  }
+
+  private static void saveInts(int[] values, int length, DataOutput out) throws IOException {
+    assert length > 0;
+    if (length == 1) {
+      out.writeVInt(values[0]);
+    } else {
+      boolean allEqual = true;
+      for (int i = 1; i < length; ++i) {
+        if (values[i] != values[0]) {
+          allEqual = false;
+          break;
+        }
+      }
+      if (allEqual) {
+        out.writeVInt(0);
+        out.writeVInt(values[0]);
+      } else {
+        long max = 0;
+        for (int i = 0; i < length; ++i) {
+          max |= values[i];
+        }
+        final int bitsRequired = PackedInts.bitsRequired(max);
+        out.writeVInt(bitsRequired);
+        final PackedInts.Writer w = PackedInts.getWriterNoHeader(out, PackedInts.Format.PACKED, length, bitsRequired, 1);
+        for (int i = 0; i < length; ++i) {
+          w.add(values[i]);
+        }
+        w.finish();
+      }
+    }
+  }
+
+  private void writeHeader(int docBase, int numBufferedDocs, int[] numStoredFields, int[] lengths) throws IOException {
+    // save docBase and numBufferedDocs
+    fieldsStream.writeVInt(docBase);
+    fieldsStream.writeVInt(numBufferedDocs);
+
+    // save numStoredFields
+    saveInts(numStoredFields, numBufferedDocs, fieldsStream);
+
+    // save lengths
+    saveInts(lengths, numBufferedDocs, fieldsStream);
+  }
+
+  private boolean triggerFlush() {
+    return bufferedDocs.length >= chunkSize || // chunks of at least chunkSize bytes
+        numBufferedDocs >= chunkSize; // can be necessary if most docs are empty
+  }
+
+  private void flush() throws IOException {
+    indexWriter.writeIndex(numBufferedDocs, fieldsStream.getFilePointer());
+
+    // transform end offsets into lengths
+    final int[] lengths = endOffsets;
+    for (int i = numBufferedDocs - 1; i > 0; --i) {
+      lengths[i] = endOffsets[i] - endOffsets[i - 1];
+      assert lengths[i] >= 0;
+    }
+    writeHeader(docBase, numBufferedDocs, numStoredFields, lengths);
+
+    // compress stored fields to fieldsStream
+    compressor.compress(bufferedDocs.bytes, 0, bufferedDocs.length, fieldsStream);
+
+    // reset
+    docBase += numBufferedDocs;
+    numBufferedDocs = 0;
+    bufferedDocs.length = 0;
+  }
+
+  @Override
+  public void writeField(FieldInfo info, StorableField field)
+      throws IOException {
+    int bits = 0;
+    final BytesRef bytes;
+    final String string;
+
+    Number number = field.numericValue();
+    if (number != null) {
+      if (number instanceof Byte || number instanceof Short || number instanceof Integer) {
+        bits = NUMERIC_INT;
+      } else if (number instanceof Long) {
+        bits = NUMERIC_LONG;
+      } else if (number instanceof Float) {
+        bits = NUMERIC_FLOAT;
+      } else if (number instanceof Double) {
+        bits = NUMERIC_DOUBLE;
+      } else {
+        throw new IllegalArgumentException("cannot store numeric type " + number.getClass());
+      }
+      string = null;
+      bytes = null;
+    } else {
+      bytes = field.binaryValue();
+      if (bytes != null) {
+        bits = BYTE_ARR;
+        string = null;
+      } else {
+        bits = STRING;
+        string = field.stringValue();
+        if (string == null) {
+          throw new IllegalArgumentException("field " + field.name() + " is stored but does not have binaryValue, stringValue nor numericValue");
+        }
+      }
+    }
+
+    final long infoAndBits = (((long) info.number) << TYPE_BITS) | bits;
+    bufferedDocs.writeVLong(infoAndBits);
+
+    if (bytes != null) {
+      bufferedDocs.writeVInt(bytes.length);
+      bufferedDocs.writeBytes(bytes.bytes, bytes.offset, bytes.length);
+    } else if (string != null) {
+      bufferedDocs.writeString(field.stringValue());
+    } else {
+      if (number instanceof Byte || number instanceof Short || number instanceof Integer) {
+        bufferedDocs.writeInt(number.intValue());
+      } else if (number instanceof Long) {
+        bufferedDocs.writeLong(number.longValue());
+      } else if (number instanceof Float) {
+        bufferedDocs.writeInt(Float.floatToIntBits(number.floatValue()));
+      } else if (number instanceof Double) {
+        bufferedDocs.writeLong(Double.doubleToLongBits(number.doubleValue()));
+      } else {
+        throw new AssertionError("Cannot get here");
+      }
+    }
+  }
+
+  @Override
+  public void abort() {
+    IOUtils.closeWhileHandlingException(this);
+    IOUtils.deleteFilesIgnoringExceptions(directory,
+        IndexFileNames.segmentFileName(segment, "", FIELDS_EXTENSION),
+        IndexFileNames.segmentFileName(segment, "", FIELDS_INDEX_EXTENSION));
+  }
+
+  @Override
+  public void finish(FieldInfos fis, int numDocs) throws IOException {
+    endWithPreviousDocument();
+    if (numBufferedDocs > 0) {
+      flush();
+    }
+    if (docBase != numDocs) {
+      throw new RuntimeException("Wrote " + docBase + " docs, finish called with numDocs=" + numDocs);
+    }
+    indexWriter.finish(numDocs);
+    assert bufferedDocs.length == 0;
+  }
+
+  @Override
+  public int merge(MergeState mergeState) throws IOException {
+    int docCount = 0;
+    int idx = 0;
+
+    for (AtomicReader reader : mergeState.readers) {
+      final SegmentReader matchingSegmentReader = mergeState.matchingSegmentReaders[idx++];
+      CompressingStoredFieldsReader matchingFieldsReader = null;
+      if (matchingSegmentReader != null) {
+        final StoredFieldsReader fieldsReader = matchingSegmentReader.getFieldsReader();
+        // we can only bulk-copy if the matching reader is also a CompressingStoredFieldsReader
+        if (fieldsReader != null && fieldsReader instanceof CompressingStoredFieldsReader) {
+          matchingFieldsReader = (CompressingStoredFieldsReader) fieldsReader;
+        }
+      }
+
+      final int maxDoc = reader.maxDoc();
+      final Bits liveDocs = reader.getLiveDocs();
+
+      if (matchingFieldsReader == null) {
+        // naive merge...
+        for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {
+          StoredDocument doc = reader.document(i);
+          addDocument(doc, mergeState.fieldInfos);
+          ++docCount;
+          mergeState.checkAbort.work(300);
+        }
+      } else {
+        int docID = nextLiveDoc(0, liveDocs, maxDoc);
+        if (docID < maxDoc) {
+          // not all docs were deleted
+          final ChunkIterator it = matchingFieldsReader.chunkIterator(docID);
+          int[] startOffsets = new int[0];
+          do {
+            // go to the next chunk that contains docID
+            it.next(docID);
+            // transform lengths into offsets
+            if (startOffsets.length < it.chunkDocs) {
+              startOffsets = new int[ArrayUtil.oversize(it.chunkDocs, 4)];
+            }
+            for (int i = 1; i < it.chunkDocs; ++i) {
+              startOffsets[i] = startOffsets[i - 1] + it.lengths[i - 1];
+            }
+
+            if (compressionMode == matchingFieldsReader.getCompressionMode() // same compression mode
+                && (numBufferedDocs == 0 || triggerFlush()) // starting a new chunk
+                && startOffsets[it.chunkDocs - 1] < chunkSize // chunk is small enough
+                && startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] >= chunkSize // chunk is large enough
+                && nextDeletedDoc(it.docBase, liveDocs, it.docBase + it.chunkDocs) == it.docBase + it.chunkDocs) { // no deletion in the chunk
+              assert docID == it.docBase;
+
+              // no need to decompress, just copy data
+              endWithPreviousDocument();
+              if (triggerFlush()) {
+                flush();
+              }
+              indexWriter.writeIndex(it.chunkDocs, fieldsStream.getFilePointer());
+              writeHeader(this.docBase, it.chunkDocs, it.numStoredFields, it.lengths);
+              it.copyCompressedData(fieldsStream);
+              this.docBase += it.chunkDocs;
+              docID = nextLiveDoc(it.docBase + it.chunkDocs, liveDocs, maxDoc);
+              docCount += it.chunkDocs;
+              mergeState.checkAbort.work(300 * it.chunkDocs);
+            } else {
+              // decompress
+              it.decompress();
+              if (startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] != it.bytes.length) {
+                throw new CorruptIndexException("Corrupted: expected chunk size=" + startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] + ", got " + it.bytes.length);
+              }
+              // copy non-deleted docs
+              for (; docID < it.docBase + it.chunkDocs; docID = nextLiveDoc(docID + 1, liveDocs, maxDoc)) {
+                final int diff = docID - it.docBase;
+                startDocument(it.numStoredFields[diff]);
+                bufferedDocs.writeBytes(it.bytes.bytes, it.bytes.offset + startOffsets[diff], it.lengths[diff]);
+                ++docCount;
+                mergeState.checkAbort.work(300);
+              }
+            }
+          } while (docID < maxDoc);
+        }
+      }
+    }
+    finish(mergeState.fieldInfos, docCount);
+    return docCount;
+  }
+
+  private static int nextLiveDoc(int doc, Bits liveDocs, int maxDoc) {
+    if (liveDocs == null) {
+      return doc;
+    }
+    while (doc < maxDoc && !liveDocs.get(doc)) {
+      ++doc;
+    }
+    return doc;
+  }
+
+  private static int nextDeletedDoc(int doc, Bits liveDocs, int maxDoc) {
+    if (liveDocs == null) {
+      return maxDoc;
+    }
+    while (doc < maxDoc && liveDocs.get(doc)) {
+      ++doc;
+    }
+    return doc;
+  }
+
+}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressionMode.java b/lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressionMode.java
new file mode 100644
index 0000000..b8237e9
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressionMode.java
@@ -0,0 +1,289 @@
+package org.apache.lucene.codecs.compressing;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.zip.DataFormatException;
+import java.util.zip.Deflater;
+import java.util.zip.Inflater;
+
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.store.DataInput;
+import org.apache.lucene.store.DataOutput;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.BytesRef;
+
+/**
+ * A compression mode. Tells how much effort should be spent on compression and
+ * decompression of stored fields.
+ * @lucene.experimental
+ */
+public enum CompressionMode {
+
+  /**
+   * A compression mode that trades compression ratio for speed. Although the
+   * compression ratio might remain high, compression and decompression are
+   * very fast. Use this mode with indices that have a high update rate but
+   * should be able to load documents from disk quickly.
+   */
+  FAST(0) {
+
+    @Override
+    Compressor newCompressor() {
+      return LZ4_FAST_COMPRESSOR;
+    }
+
+    @Override
+    Decompressor newDecompressor() {
+      return LZ4_DECOMPRESSOR;
+    }
+
+  },
+
+  /**
+   * A compression mode that trades speed for compression ratio. Although
+   * compression and decompression might be slow, this compression mode should
+   * provide a good compression ratio. This mode might be interesting if/when
+   * your index size is much bigger than your OS cache.
+   */
+  HIGH_COMPRESSION(1) {
+
+    @Override
+    Compressor newCompressor() {
+      return new DeflateCompressor(Deflater.BEST_COMPRESSION);
+    }
+
+    @Override
+    Decompressor newDecompressor() {
+      return new DeflateDecompressor();
+    }
+
+  },
+
+  /**
+   * This compression mode is similar to {@link #FAST} but it spends more time
+   * compressing in order to improve the compression ratio. This compression
+   * mode is best used with indices that have a low update rate but should be
+   * able to load documents from disk quickly.
+   */
+  FAST_DECOMPRESSION(2) {
+
+    @Override
+    Compressor newCompressor() {
+      return LZ4_HIGH_COMPRESSOR;
+    }
+
+    @Override
+    Decompressor newDecompressor() {
+      return LZ4_DECOMPRESSOR;
+    }
+
+  };
+
+  /** Get a {@link CompressionMode} according to its id. */
+  public static CompressionMode byId(int id) {
+    for (CompressionMode mode : CompressionMode.values()) {
+      if (mode.getId() == id) {
+        return mode;
+      }
+    }
+    throw new IllegalArgumentException("Unknown id: " + id);
+  }
+
+  private final int id;
+
+  private CompressionMode(int id) {
+    this.id = id;
+  }
+
+  /**
+   * Returns an ID for this compression mode. Should be unique across
+   * {@link CompressionMode}s as it is used for serialization and
+   * unserialization.
+   */
+  public final int getId() {
+    return id;
+  }
+
+  /**
+   * Create a new {@link Compressor} instance.
+   */
+  abstract Compressor newCompressor();
+
+  /**
+   * Create a new {@link Decompressor} instance.
+   */
+  abstract Decompressor newDecompressor();
+
+
+  private static final Decompressor LZ4_DECOMPRESSOR = new Decompressor() {
+
+    @Override
+    public void decompress(DataInput in, int originalLength, int offset, int length, BytesRef bytes) throws IOException {
+      assert offset + length <= originalLength;
+      // add 7 padding bytes, this is not necessary but can help decompression run faster
+      if (bytes.bytes.length < originalLength + 7) {
+        bytes.bytes = new byte[ArrayUtil.oversize(originalLength + 7, 1)];
+      }
+      final int decompressedLength = LZ4.decompress(in, offset + length, bytes.bytes, 0);
+      if (decompressedLength > originalLength) {
+        throw new CorruptIndexException("Corrupted: lengths mismatch: " + decompressedLength + " > " + originalLength);
+      }
+      bytes.offset = offset;
+      bytes.length = length;
+    }
+
+    @Override
+    public void copyCompressedData(DataInput in, int originalLength, DataOutput out) throws IOException {
+      final int copied = LZ4.copyCompressedData(in, originalLength, out);
+      if (copied != originalLength) {
+        throw new CorruptIndexException("Currupted compressed stream: expected " + originalLength + " bytes, but got at least" + copied);
+      }
+    }
+
+    @Override
+    public Decompressor clone() {
+      return this;
+    }
+
+  };
+
+  private static final Compressor LZ4_FAST_COMPRESSOR = new Compressor() {
+
+    @Override
+    public void compress(byte[] bytes, int off, int len, DataOutput out)
+        throws IOException {
+      LZ4.compress(bytes, off, len, out);
+    }
+
+  };
+
+  private static final Compressor LZ4_HIGH_COMPRESSOR = new Compressor() {
+
+    @Override
+    public void compress(byte[] bytes, int off, int len, DataOutput out)
+        throws IOException {
+      LZ4.compressHC(bytes, off, len, out);
+    }
+
+  };
+
+  private static final class DeflateDecompressor extends Decompressor {
+
+    final Inflater decompressor;
+    byte[] compressed;
+
+    DeflateDecompressor() {
+      decompressor = new Inflater();
+      compressed = new byte[0];
+    }
+
+    @Override
+    public void decompress(DataInput in, int originalLength, int offset, int length, BytesRef bytes) throws IOException {
+      assert offset + length <= originalLength;
+      if (length == 0) {
+        bytes.length = 0;
+        return;
+      }
+      final int compressedLength = in.readVInt();
+      if (compressedLength > compressed.length) {
+        compressed = new byte[ArrayUtil.oversize(compressedLength, 1)];
+      }
+      in.readBytes(compressed, 0, compressedLength);
+
+      decompressor.reset();
+      decompressor.setInput(compressed, 0, compressedLength);
+
+      bytes.offset = bytes.length = 0;
+      while (true) {
+        final int count;
+        try {
+          final int remaining = bytes.bytes.length - bytes.length;
+          count = decompressor.inflate(bytes.bytes, bytes.length, remaining);
+        } catch (DataFormatException e) {
+          throw new IOException(e);
+        }
+        bytes.length += count;
+        if (decompressor.finished()) {
+          break;
+        } else {
+          bytes.bytes = ArrayUtil.grow(bytes.bytes);
+        }
+      }
+      if (bytes.length != originalLength) {
+        throw new CorruptIndexException("Lengths mismatch: " + bytes.length + " != " + originalLength);
+      }
+      bytes.offset = offset;
+      bytes.length = length;
+    }
+
+    @Override
+    public void copyCompressedData(DataInput in, int originalLength, DataOutput out) throws IOException {
+      final int compressedLength = in.readVInt();
+      out.writeVInt(compressedLength);
+      out.copyBytes(in, compressedLength);
+    }
+
+    @Override
+    public Decompressor clone() {
+      return new DeflateDecompressor();
+    }
+
+  }
+
+  private static class DeflateCompressor extends Compressor {
+
+    final Deflater compressor;
+    byte[] compressed;
+
+    DeflateCompressor(int level) {
+      compressor = new Deflater(level);
+      compressed = new byte[64];
+    }
+
+    @Override
+    public void compress(byte[] bytes, int off, int len, DataOutput out) throws IOException {
+      compressor.reset();
+      compressor.setInput(bytes, off, len);
+      compressor.finish();
+
+      if (compressor.needsInput()) {
+        // no output
+        out.writeVInt(0);
+        return;
+      }
+
+      int totalCount = 0;
+      for (;;) {
+        final int count = compressor.deflate(compressed, totalCount, compressed.length - totalCount);
+        totalCount += count;
+        assert totalCount <= compressed.length;
+        if (compressor.finished()) {
+          break;
+        } else {
+          compressed = ArrayUtil.grow(compressed);
+        }
+      }
+
+      out.writeVInt(totalCount);
+      out.writeBytes(compressed, totalCount);
+    }
+
+  }
+
+}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/compressing/Compressor.java b/lucene/core/src/java/org/apache/lucene/codecs/compressing/Compressor.java
new file mode 100644
index 0000000..a652999
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/codecs/compressing/Compressor.java
@@ -0,0 +1,36 @@
+package org.apache.lucene.codecs.compressing;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.store.DataOutput;
+
+/**
+ * A data compressor.
+ */
+abstract class Compressor {
+
+  /**
+   * Compress bytes into <code>out</code>. It it the responsibility of the
+   * compressor to add all necessary information so that a {@link Decompressor}
+   * will know when to stop decompressing bytes from the stream.
+   */
+  public abstract void compress(byte[] bytes, int off, int len, DataOutput out) throws IOException;
+
+}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/compressing/Decompressor.java b/lucene/core/src/java/org/apache/lucene/codecs/compressing/Decompressor.java
new file mode 100644
index 0000000..43f30c2
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/codecs/compressing/Decompressor.java
@@ -0,0 +1,54 @@
+package org.apache.lucene.codecs.compressing;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.store.DataInput;
+import org.apache.lucene.store.DataOutput;
+import org.apache.lucene.util.BytesRef;
+
+/**
+ * An decompressor.
+ */
+abstract class Decompressor implements Cloneable {
+
+  /**
+   * Decompress bytes that were stored between offsets <code>offset</code> and
+   * <code>offset+length</code> in the original stream from the compressed
+   * stream <code>in</code> to <code>bytes</code>. After returning, the length
+   * of <code>bytes</code> (<code>bytes.length</code>) must be equal to
+   * <code>length</code>. Implementations of this method are free to resize
+   * <code>bytes</code> depending on their needs.
+   *
+   * @param in the input that stores the compressed stream
+   * @param originalLength the length of the original data (before compression)
+   * @param offset bytes before this offset do not need to be decompressed
+   * @param length bytes after <code>offset+length</code> do not need to be decompressed
+   * @param bytes a {@link BytesRef} where to store the decompressed data
+   */
+  public abstract void decompress(DataInput in, int originalLength, int offset, int length, BytesRef bytes) throws IOException;
+
+  /** Copy a compressed stream whose original length is
+   * <code>originalLength</code> from <code>in</code> to <code>out</code>. */
+  public abstract void copyCompressedData(DataInput in, int originalLength, DataOutput out) throws IOException;
+
+  @Override
+  public abstract Decompressor clone();
+
+}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/compressing/GrowableByteArrayDataOutput.java b/lucene/core/src/java/org/apache/lucene/codecs/compressing/GrowableByteArrayDataOutput.java
new file mode 100644
index 0000000..f5aeefb
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/codecs/compressing/GrowableByteArrayDataOutput.java
@@ -0,0 +1,56 @@
+package org.apache.lucene.codecs.compressing;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.store.DataOutput;
+import org.apache.lucene.util.ArrayUtil;
+
+/**
+ * A {@link DataOutput} that can be used to build a byte[].
+ */
+final class GrowableByteArrayDataOutput extends DataOutput {
+
+  byte[] bytes;
+  int length;
+
+  GrowableByteArrayDataOutput(int cp) {
+    this.bytes = new byte[ArrayUtil.oversize(cp, 1)];
+    this.length = 0;
+  }
+
+  @Override
+  public void writeByte(byte b) throws IOException {
+    if (length >= bytes.length) {
+      bytes = ArrayUtil.grow(bytes);
+    }
+    bytes[length++] = b;
+  }
+
+  @Override
+  public void writeBytes(byte[] b, int off, int len) throws IOException {
+    final int newLength = length + len;
+    if (newLength > bytes.length) {
+      bytes = ArrayUtil.grow(bytes, newLength);
+    }
+    System.arraycopy(b, off, bytes, length, len);
+    length = newLength;
+  }
+
+}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/compressing/LZ4.java b/lucene/core/src/java/org/apache/lucene/codecs/compressing/LZ4.java
new file mode 100644
index 0000000..359ecb5
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/codecs/compressing/LZ4.java
@@ -0,0 +1,556 @@
+package org.apache.lucene.codecs.compressing;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Arrays;
+
+import org.apache.lucene.store.DataInput;
+import org.apache.lucene.store.DataOutput;
+import org.apache.lucene.util.packed.PackedInts;
+
+/**
+ * LZ4 compression and decompression routines.
+ *
+ * http://code.google.com/p/lz4/
+ * http://fastcompression.blogspot.fr/p/lz4.html
+ */
+class LZ4 {
+
+  private LZ4() {}
+
+  static final int MEMORY_USAGE = 14;
+  static final int MIN_MATCH = 4; // minimum length of a match
+  static final int MAX_DISTANCE = 1 << 16; // maximum distance of a reference
+  static final int LAST_LITERALS = 5; // the last 5 bytes must be encoded as literals
+  static final int HASH_LOG_HC = 15; // log size of the dictionary for compressHC
+  static final int HASH_TABLE_SIZE_HC = 1 << HASH_LOG_HC;
+  static final int OPTIMAL_ML = 0x0F + 4 - 1; // match length that doesn't require an additional byte
+
+
+  private static int hash(int i, int hashBits) {
+    return (i * -1640531535) >>> (32 - hashBits);
+  }
+
+  private static int hashHC(int i) {
+    return hash(i, HASH_LOG_HC);
+  }
+
+  private static int readInt(byte[] buf, int i) {
+    return ((buf[i] & 0xFF) << 24) | ((buf[i+1] & 0xFF) << 16) | ((buf[i+2] & 0xFF) << 8) | (buf[i+3] & 0xFF);
+  }
+
+  private static boolean readIntEquals(byte[] buf, int i, int j) {
+    return readInt(buf, i) == readInt(buf, j);
+  }
+
+  private static int commonBytes(byte[] b, int o1, int o2, int limit) {
+    assert o1 < o2;
+    int count = 0;
+    while (o2 < limit && b[o1++] == b[o2++]) {
+      ++count;
+    }
+    return count;
+  }
+
+  private static int commonBytesBackward(byte[] b, int o1, int o2, int l1, int l2) {
+    int count = 0;
+    while (o1 > l1 && o2 > l2 && b[--o1] == b[--o2]) {
+      ++count;
+    }
+    return count;
+  }
+
+  /**
+   * Decompress at least <code>decompressedLen</code> bytes into
+   * <code>dest[dOff:]</code>. Please note that <code>dest</code> must be large
+   * enough to be able to hold <b>all</b> decompressed data (meaning that you
+   * need to know the total decompressed length).
+   */
+  public static int decompress(DataInput compressed, int decompressedLen, byte[] dest, int dOff) throws IOException {
+    final int destEnd = dest.length;
+
+    do {
+      // literals
+      final int token = compressed.readByte() & 0xFF;
+      int literalLen = token >>> 4;
+
+      if (literalLen != 0) {
+        if (literalLen == 0x0F) {
+          byte len;
+          while ((len = compressed.readByte()) == (byte) 0xFF) {
+            literalLen += 0xFF;
+          }
+          literalLen += len & 0xFF;
+        }
+        compressed.readBytes(dest, dOff, literalLen);
+        dOff += literalLen;
+      }
+
+      if (dOff >= decompressedLen) {
+        break;
+      }
+
+      // matchs
+      final int matchDec = (compressed.readByte() & 0xFF) | ((compressed.readByte() & 0xFF) << 8);
+      assert matchDec > 0;
+
+      int matchLen = token & 0x0F;
+      if (matchLen == 0x0F) {
+        int len;
+        while ((len = compressed.readByte()) == (byte) 0xFF) {
+          matchLen += 0xFF;
+        }
+        matchLen += len & 0xFF;
+      }
+      matchLen += MIN_MATCH;
+
+      // copying a multiple of 8 bytes can make decompression from 5% to 10% faster
+      final int fastLen = ((matchLen - 1) & 0xFFFFFFF8) + 8;
+      if (matchDec < matchLen || dOff + fastLen > destEnd) {
+        // overlap -> naive incremental copy
+        for (int ref = dOff - matchDec, end = dOff + matchLen; dOff < end; ++ref, ++dOff) {
+          dest[dOff] = dest[ref];
+        }
+      } else {
+        // no overlap -> arraycopy
+        System.arraycopy(dest, dOff - matchDec, dest, dOff, fastLen);
+        dOff += matchLen;
+      }
+    } while (dOff < decompressedLen);
+
+    return dOff;
+  }
+
+  private static void encodeLen(int l, DataOutput out) throws IOException {
+    while (l >= 0xFF) {
+      out.writeByte((byte) 0xFF);
+      l -= 0xFF;
+    }
+    out.writeByte((byte) l);
+  }
+
+  private static void encodeLiterals(byte[] bytes, int token, int anchor, int literalLen, DataOutput out) throws IOException {
+    out.writeByte((byte) token);
+
+    // encode literal length
+    if (literalLen >= 0x0F) {
+      encodeLen(literalLen - 0x0F, out);
+    }
+
+    // encode literals
+    out.writeBytes(bytes, anchor, literalLen);
+  }
+
+  private static void encodeLastLiterals(byte[] bytes, int anchor, int literalLen, DataOutput out) throws IOException {
+    final int token = Math.min(literalLen, 0x0F) << 4;
+    encodeLiterals(bytes, token, anchor, literalLen, out);
+  }
+
+  private static void encodeSequence(byte[] bytes, int anchor, int matchRef, int matchOff, int matchLen, DataOutput out) throws IOException {
+    final int literalLen = matchOff - anchor;
+    assert matchLen >= 4;
+    // encode token
+    final int token = (Math.min(literalLen, 0x0F) << 4) | Math.min(matchLen - 4, 0x0F);
+    encodeLiterals(bytes, token, anchor, literalLen, out);
+
+    // encode match dec
+    final int matchDec = matchOff - matchRef;
+    assert matchDec > 0 && matchDec < 1 << 16;
+    out.writeByte((byte) matchDec);
+    out.writeByte((byte) (matchDec >>> 8));
+
+    // encode match len
+    if (matchLen >= MIN_MATCH + 0x0F) {
+      encodeLen(matchLen - 0x0F - MIN_MATCH, out);
+    }
+  }
+
+  /**
+   * Compress <code>bytes[off:off+len]</code> into <code>out</code> using
+   * at most 16KB of memory.
+   */
+  public static void compress(byte[] bytes, int off, int len, DataOutput out) throws IOException {
+
+    final int base = off;
+    final int end = off + len;
+
+    int anchor = off++;
+
+    if (len > LAST_LITERALS + MIN_MATCH) {
+
+      final int limit = end - LAST_LITERALS;
+      final int matchLimit = limit - MIN_MATCH;
+
+      final int bitsPerOffset = PackedInts.bitsRequired(len - LAST_LITERALS);
+      final int bitsPerOffsetLog = 32 - Integer.numberOfLeadingZeros(bitsPerOffset - 1);
+      final int hashLog = MEMORY_USAGE + 3 - bitsPerOffsetLog;
+      final PackedInts.Mutable hashTable = PackedInts.getMutable(1 << hashLog, bitsPerOffset, PackedInts.DEFAULT);
+
+      main:
+      while (off < limit) {
+        // find a match
+        int ref;
+        while (true) {
+          if (off >= matchLimit) {
+            break main;
+          }
+          final int v = readInt(bytes, off);
+          final int h = hash(v, hashLog);
+          ref = base + (int) hashTable.get(h);
+          assert PackedInts.bitsRequired(off - base) <= hashTable.getBitsPerValue();
+          hashTable.set(h, off - base);
+          if (off - ref < MAX_DISTANCE && readInt(bytes, ref) == v) {
+            break;
+          }
+          ++off;
+        }
+
+        // compute match length
+        final int matchLen = MIN_MATCH + commonBytes(bytes, ref + 4, off + 4, limit);
+
+        encodeSequence(bytes, anchor, ref, off, matchLen, out);
+        off += matchLen;
+        anchor = off;
+      }
+    }
+
+    // last literals
+    final int literalLen = end - anchor;
+    assert literalLen >= LAST_LITERALS || literalLen == len;
+    encodeLastLiterals(bytes, anchor, end - anchor, out);
+  }
+
+  private static class Match {
+    int start, ref, len;
+
+    void fix(int correction) {
+      start += correction;
+      ref += correction;
+      len -= correction;
+    }
+
+    int end() {
+      return start + len;
+    }
+  }
+
+  private static void copyTo(Match m1, Match m2) {
+    m2.len = m1.len;
+    m2.start = m1.start;
+    m2.ref = m1.ref;
+  }
+
+  private static class HashTable {
+    static final int MAX_ATTEMPTS = 256;
+    static final int MASK = MAX_DISTANCE - 1;
+    int nextToUpdate;
+    private final int base;
+    private final int[] hashTable;
+    private final short[] chainTable;
+
+    HashTable(int base) {
+      this.base = base;
+      nextToUpdate = base;
+      hashTable = new int[HASH_TABLE_SIZE_HC];
+      Arrays.fill(hashTable, -1);
+      chainTable = new short[MAX_DISTANCE];
+    }
+
+    private int hashPointer(byte[] bytes, int off) {
+      final int v = readInt(bytes, off);
+      final int h = hashHC(v);
+      return base + hashTable[h];
+    }
+
+    private int next(int off) {
+      return base + off - (chainTable[off & MASK] & 0xFFFF);
+    }
+
+    private void addHash(byte[] bytes, int off) {
+      final int v = readInt(bytes, off);
+      final int h = hashHC(v);
+      int delta = off - hashTable[h];
+      if (delta >= MAX_DISTANCE) {
+        delta = MAX_DISTANCE - 1;
+      }
+      chainTable[off & MASK] = (short) delta;
+      hashTable[h] = off - base;
+    }
+
+    void insert(int off, byte[] bytes) {
+      for (; nextToUpdate < off; ++nextToUpdate) {
+        addHash(bytes, nextToUpdate);
+      }
+    }
+
+    boolean insertAndFindBestMatch(byte[] buf, int off, int matchLimit, Match match) {
+      match.start = off;
+      match.len = 0;
+
+      insert(off, buf);
+
+      int ref = hashPointer(buf, off);
+      for (int i = 0; i < MAX_ATTEMPTS; ++i) {
+        if (ref < Math.max(base, off - MAX_DISTANCE + 1)) {
+          break;
+        }
+        if (buf[ref + match.len] == buf[off + match.len] && readIntEquals(buf, ref, off)) {
+          final int matchLen = MIN_MATCH + commonBytes(buf, ref + MIN_MATCH, off + MIN_MATCH, matchLimit);
+          if (matchLen > match.len) {
+            match.ref = ref;
+            match.len = matchLen;
+          }
+        }
+        ref = next(ref);
+      }
+
+      return match.len != 0;
+    }
+
+    boolean insertAndFindWiderMatch(byte[] buf, int off, int startLimit, int matchLimit, int minLen, Match match) {
+      match.len = minLen;
+
+      insert(off, buf);
+
+      final int delta = off - startLimit;
+      int ref = hashPointer(buf, off);
+      for (int i = 0; i < MAX_ATTEMPTS; ++i) {
+        if (ref < Math.max(base, off - MAX_DISTANCE + 1)) {
+          break;
+        }
+        if (buf[ref - delta + match.len] == buf[startLimit + match.len]
+            && readIntEquals(buf, ref, off)) {
+          final int matchLenForward = MIN_MATCH + commonBytes(buf, ref + MIN_MATCH, off + MIN_MATCH, matchLimit);
+          final int matchLenBackward = commonBytesBackward(buf, ref, off, base, startLimit);
+          final int matchLen = matchLenBackward + matchLenForward;
+          if (matchLen > match.len) {
+            match.len = matchLen;
+            match.ref = ref - matchLenBackward;
+            match.start = off - matchLenBackward;
+          }
+        }
+        ref = next(ref);
+      }
+
+      return match.len > minLen;
+    }
+
+  }
+
+  /**
+   * Compress <code>bytes[off:off+len]</code> into <code>out</code>. Compared to
+   * {@link LZ4#compress(byte[], int, int, DataOutput)}, this method is slower,
+   * uses more memory (~ 256KB), but should provide better compression ratios
+   * (especially on large inputs) because it chooses the best match among up to
+   * 256 candidates and then performs trade-offs to fix overlapping matches.
+   */
+  public static void compressHC(byte[] src, int srcOff, int srcLen, DataOutput out) throws IOException {
+
+    final int srcEnd = srcOff + srcLen;
+    final int matchLimit = srcEnd - LAST_LITERALS;
+
+    int sOff = srcOff;
+    int anchor = sOff++;
+
+    final HashTable ht = new HashTable(srcOff);
+    final Match match0 = new Match();
+    final Match match1 = new Match();
+    final Match match2 = new Match();
+    final Match match3 = new Match();
+
+    main:
+    while (sOff < matchLimit) {
+      if (!ht.insertAndFindBestMatch(src, sOff, matchLimit, match1)) {
+        ++sOff;
+        continue;
+      }
+
+      // saved, in case we would skip too much
+      copyTo(match1, match0);
+
+      search2:
+      while (true) {
+        assert match1.start >= anchor;
+        if (match1.end() >= matchLimit
+            || !ht.insertAndFindWiderMatch(src, match1.end() - 2, match1.start + 1, matchLimit, match1.len, match2)) {
+          // no better match
+          encodeSequence(src, anchor, match1.ref, match1.start, match1.len, out);
+          anchor = sOff = match1.end();
+          continue main;
+        }
+
+        if (match0.start < match1.start) {
+          if (match2.start < match1.start + match0.len) { // empirical
+            copyTo(match0, match1);
+          }
+        }
+        assert match2.start > match1.start;
+
+        if (match2.start - match1.start < 3) { // First Match too small : removed
+          copyTo(match2, match1);
+          continue search2;
+        }
+
+        search3:
+        while (true) {
+          if (match2.start - match1.start < OPTIMAL_ML) {
+            int newMatchLen = match1.len;
+            if (newMatchLen > OPTIMAL_ML) {
+              newMatchLen = OPTIMAL_ML;
+            }
+            if (match1.start + newMatchLen > match2.end() - MIN_MATCH) {
+              newMatchLen = match2.start - match1.start + match2.len - MIN_MATCH;
+            }
+            final int correction = newMatchLen - (match2.start - match1.start);
+            if (correction > 0) {
+              match2.fix(correction);
+            }
+          }
+
+          if (match2.start + match2.len >= matchLimit
+              || !ht.insertAndFindWiderMatch(src, match2.end() - 3, match2.start, matchLimit, match2.len, match3)) {
+            // no better match -> 2 sequences to encode
+            if (match2.start < match1.end()) {
+              if (match2.start - match1.start < OPTIMAL_ML) {
+                if (match1.len > OPTIMAL_ML) {
+                  match1.len = OPTIMAL_ML;
+                }
+                if (match1.end() > match2.end() - MIN_MATCH) {
+                  match1.len = match2.end() - match1.start - MIN_MATCH;
+                }
+                final int correction = match1.len - (match2.start - match1.start);
+                if (correction > 0) {
+                  match2.fix(correction);
+                }
+              } else {
+                match1.len = match2.start - match1.start;
+              }
+            }
+            // encode seq 1
+            encodeSequence(src, anchor, match1.ref, match1.start, match1.len, out);
+            anchor = sOff = match1.end();
+            // encode seq 2
+            encodeSequence(src, anchor, match2.ref, match2.start, match2.len, out);
+            anchor = sOff = match2.end();
+            continue main;
+          }
+
+          if (match3.start < match1.end() + 3) { // Not enough space for match 2 : remove it
+            if (match3.start >= match1.end()) { // // can write Seq1 immediately ==> Seq2 is removed, so Seq3 becomes Seq1
+              if (match2.start < match1.end()) {
+                final int correction = match1.end() - match2.start;
+                match2.fix(correction);
+                if (match2.len < MIN_MATCH) {
+                  copyTo(match3, match2);
+                }
+              }
+
+              encodeSequence(src, anchor, match1.ref, match1.start, match1.len, out);
+              anchor = sOff = match1.end();
+
+              copyTo(match3, match1);
+              copyTo(match2, match0);
+
+              continue search2;
+            }
+
+            copyTo(match3, match2);
+            continue search3;
+          }
+
+          // OK, now we have 3 ascending matches; let's write at least the first one
+          if (match2.start < match1.end()) {
+            if (match2.start - match1.start < 0x0F) {
+              if (match1.len > OPTIMAL_ML) {
+                match1.len = OPTIMAL_ML;
+              }
+              if (match1.end() > match2.end() - MIN_MATCH) {
+                match1.len = match2.end() - match1.start - MIN_MATCH;
+              }
+              final int correction = match1.end() - match2.start;
+              match2.fix(correction);
+            } else {
+              match1.len = match2.start - match1.start;
+            }
+          }
+
+          encodeSequence(src, anchor, match1.ref, match1.start, match1.len, out);
+          anchor = sOff = match1.end();
+
+          copyTo(match2, match1);
+          copyTo(match3, match2);
+
+          continue search3;
+        }
+
+      }
+
+    }
+
+    encodeLastLiterals(src, anchor, srcEnd - anchor, out);
+  }
+
+  /** Copy bytes from <code>in</code> to <code>out</code> where
+   *  <code>in</code> is a LZ4-encoded stream. This method copies enough bytes
+   *  so that <code>out</code> can be used later on to restore the first
+   *  <code>length</code> bytes of the stream. This method always reads at
+   *  least one byte from <code>in</code> so make sure not to call this method
+   *  if <code>in</code> reached the end of the stream, even if
+   *  <code>length=0</code>. */
+  public static int copyCompressedData(DataInput in, int length, DataOutput out) throws IOException {
+    int n = 0;
+    do {
+      // literals
+      final byte token = in.readByte();
+      out.writeByte(token);
+      int literalLen = (token & 0xFF) >>> 4;
+      if (literalLen == 0x0F) {
+        byte len;
+        while ((len = in.readByte()) == (byte) 0xFF) {
+          literalLen += 0xFF;
+          out.writeByte(len);
+        }
+        literalLen += len & 0xFF;
+        out.writeByte(len);
+      }
+      out.copyBytes(in, literalLen);
+      n += literalLen;
+      if (n >= length) {
+        break;
+      }
+
+      // matchs
+      out.copyBytes(in, 2); // match dec
+      int matchLen = token & 0x0F;
+      if (matchLen == 0x0F) {
+        byte len;
+        while ((len = in.readByte()) == (byte) 0xFF) {
+          matchLen += 0xFF;
+          out.writeByte(len);
+        }
+        matchLen += len & 0xFF;
+        out.writeByte(len);
+      }
+      matchLen += MIN_MATCH;
+      n += matchLen;
+    } while (n < length);
+    return n;
+  }
+
+}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/compressing/package.html b/lucene/core/src/java/org/apache/lucene/codecs/compressing/package.html
new file mode 100644
index 0000000..4d899f7
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/codecs/compressing/package.html
@@ -0,0 +1,25 @@
+<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+<html>
+<head>
+   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
+</head>
+<body>
+StoredFieldsFormat that allows cross-document and cross-field compression of stored fields.
+</body>
+</html>
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41Codec.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41Codec.java
index 4821958..e5c430f 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41Codec.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41Codec.java
@@ -32,7 +32,6 @@ import org.apache.lucene.codecs.lucene40.Lucene40FieldInfosFormat;
 import org.apache.lucene.codecs.lucene40.Lucene40LiveDocsFormat;
 import org.apache.lucene.codecs.lucene40.Lucene40NormsFormat;
 import org.apache.lucene.codecs.lucene40.Lucene40SegmentInfoFormat;
-import org.apache.lucene.codecs.lucene40.Lucene40StoredFieldsFormat;
 import org.apache.lucene.codecs.lucene40.Lucene40TermVectorsFormat;
 import org.apache.lucene.codecs.perfield.PerFieldPostingsFormat;
 
@@ -49,7 +48,7 @@ import org.apache.lucene.codecs.perfield.PerFieldPostingsFormat;
 // if they are backwards compatible or smallish we can probably do the backwards in the postingsreader
 // (it writes a minor version, etc).
 public class Lucene41Codec extends Codec {
-  private final StoredFieldsFormat fieldsFormat = new Lucene40StoredFieldsFormat();
+  private final StoredFieldsFormat fieldsFormat = new Lucene41StoredFieldsFormat();
   private final TermVectorsFormat vectorsFormat = new Lucene40TermVectorsFormat();
   private final FieldInfosFormat fieldInfosFormat = new Lucene40FieldInfosFormat();
   private final DocValuesFormat docValuesFormat = new Lucene40DocValuesFormat();
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41StoredFieldsFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41StoredFieldsFormat.java
new file mode 100644
index 0000000..ab6a9e3
--- /dev/null
+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene41/Lucene41StoredFieldsFormat.java
@@ -0,0 +1,152 @@
+package org.apache.lucene.codecs.lucene41;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.StoredFieldsFormat;
+import org.apache.lucene.codecs.compressing.CompressingStoredFieldsFormat;
+import org.apache.lucene.codecs.compressing.CompressionMode;
+import org.apache.lucene.codecs.lucene40.Lucene40StoredFieldsFormat;
+import org.apache.lucene.store.DataOutput;
+import org.apache.lucene.util.packed.PackedInts;
+
+/**
+ * Lucene 4.1 stored fields format.
+ *
+ * <p><b>Principle</b></p>
+ * <p>This {@link StoredFieldsFormat} compresses blocks of 16KB of documents in
+ * order to improve the compression ratio compared to document-level
+ * compression. It uses the <a href="http://code.google.com/p/lz4/">LZ4</a>
+ * compression algorithm, which is fast to compress and very fast to decompress
+ * data. Although the compression method that is used focuses more on speed
+ * than on compression ratio, it should provide interesting compression ratios
+ * for redundant inputs (such as log files, HTML or plain text).</p>
+ * <p><b>File formats</b></p>
+ * <p>Stored fields are represented by two files:</p>
+ * <ol>
+ * <li><a name="field_data" id="field_data"></a>
+ * <p>A fields data file (extension <tt>.fdt</tt>). This file stores a compact
+ * representation of documents in compressed blocks of 16KB or more. When
+ * writing a segment, documents are appended to an in-memory <tt>byte[]</tt>
+ * buffer. When its size reaches 16KB or more, some metadata about the documents
+ * is flushed to disk, immediately followed by a compressed representation of
+ * the buffer using the
+ * <a href="http://code.google.com/p/lz4/">LZ4</a>
+ * <a href="http://fastcompression.blogspot.fr/2011/05/lz4-explained.html">compression format</a>.</p>
+ * <p>Here is a more detailed description of the field data file format:</p>
+ * <ul>
+ * <li>FieldData (.fdt) --&gt; &lt;Header&gt;, PackedIntsVersion, CompressionFormat, &lt;Chunk&gt;<sup>ChunkCount</sup></li>
+ * <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
+ * <li>PackedIntsVersion --&gt; {@link PackedInts#VERSION_CURRENT} as a {@link DataOutput#writeVInt VInt}</li>
+ * <li>CompressionFormat --&gt; always <tt>0</tt> as a {@link DataOutput#writeVInt VInt}, this may allow for different compression formats in the future</li>
+ * <li>ChunkCount is not known in advance and is the number of chunks necessary to store all document of the segment</li>
+ * <li>Chunk --&gt; DocBase, ChunkDocs, DocFieldCounts, DocLengths, &lt;CompressedDocs&gt;</li>
+ * <li>DocBase --&gt; the ID of the first document of the chunk as a {@link DataOutput#writeVInt VInt}</li>
+ * <li>ChunkDocs --&gt; the number of documents in the chunk as a {@link DataOutput#writeVInt VInt}</li>
+ * <li>DocFieldCounts --&gt; the number of stored fields of every document in the chunk, encoded as followed:<ul>
+ *   <li>if chunkDocs=1, the unique value is encoded as a {@link DataOutput#writeVInt VInt}</li>
+ *   <li>else read a {@link DataOutput#writeVInt VInt} (let's call it <tt>bitsRequired</tt>)<ul>
+ *     <li>if <tt>bitsRequired</tt> is <tt>0</tt> then all values are equal, and the common value is the following {@link DataOutput#writeVInt VInt}</li>
+ *     <li>else <tt>bitsRequired</tt> is the number of bits required to store any value, and values are stored in a {@link PackedInts packed} array where every value is stored on exactly <tt>bitsRequired</tt> bits</li>
+ *   </ul></li>
+ * </ul></li>
+ * <li>DocLengths --&gt; the lengths of all documents in the chunk, encoded with the same method as DocFieldCounts</li>
+ * <li>CompressedDocs --&gt; a compressed representation of &lt;Docs&gt; using the LZ4 compression format</li>
+ * <li>Docs --&gt; &lt;Doc&gt;<sup>ChunkDocs</sup></li>
+ * <li>Doc --&gt; &lt;FieldNumAndType, Value&gt;<sup>DocFieldCount</sup></li>
+ * <li>FieldNumAndType --&gt; a {@link DataOutput#writeVLong VLong}, whose 3 last bits are Type and other bits are FieldNum</li>
+ * <li>Type --&gt;<ul>
+ *   <li>0: Value is String</li>
+ *   <li>1: Value is BinaryValue</li>
+ *   <li>2: Value is Int</li>
+ *   <li>3: Value is Float</li>
+ *   <li>4: Value is Long</li>
+ *   <li>5: Value is Double</li>
+ *   <li>6, 7: unused</li>
+ * </ul></li>
+ * <li>FieldNum --&gt; an ID of the field</li>
+ * <li>Value --&gt; {@link DataOutput#writeString(String) String} | BinaryValue | Int | Float | Long | Double depending on Type</li>
+ * <li>BinaryValue --&gt; ValueLength &lt;Byte&gt;<sup>ValueLength</sup></li>
+ * </ul>
+ * <p>Notes</p>
+ * <ul>
+ * <li>If documents are larger than 16KB then chunks will likely contain only
+ * one document. However, documents can never spread across several chunks (all
+ * fields of a single document are in the same chunk).</li>
+ * <li>Given that the original lengths are written in the metadata of the chunk,
+ * the decompressor can leverage this information to stop decoding as soon as
+ * enough data has been decompressed.</li>
+ * <li>In case documents are incompressible, CompressedDocs will be less than
+ * 0.5% larger than Docs.</li>
+ * </ul>
+ * </li>
+ * <li><a name="field_index" id="field_index"></a>
+ * <p>A fields index file (extension <tt>.fdx</tt>). The data stored in this
+ * file is read to load an in-memory data-structure that can be used to locate
+ * the start offset of a block containing any document in the fields data file.</p>
+ * <p>In order to have a compact in-memory representation, for every block of
+ * 1024 chunks, this stored fields index computes the average number of bytes per
+ * chunk and for every chunk, only stores the difference between<ul>
+ * <li>${chunk number} * ${average length of a chunk}</li>
+ * <li>and the actual start offset of the chunk</li></ul></p>
+ * <p>Data is written as follows:</p>
+ * <ul>
+ * <li>FieldsIndex (.fdx) --&gt; &lt;Header&gt;, FieldsIndex, PackedIntsVersion, &lt;Block&gt;<sup>BlockCount</sup>, BlocksEndMarker</li>
+ * <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
+ * <li>PackedIntsVersion --&gt; {@link PackedInts#VERSION_CURRENT} as a {@link DataOutput#writeVInt VInt}</li>
+ * <li>BlocksEndMarker --&gt; <tt>0</tt> as a {@link DataOutput#writeVInt VInt}, this marks the end of blocks since blocks are not allowed to start with <tt>0</tt></li>
+ * <li>Block --&gt; BlockChunks, &lt;DocBases&gt;, &lt;StartPointers&gt;</li>
+ * <li>BlockChunks --&gt; a {@link DataOutput#writeVInt VInt} which is the number of chunks encoded in the block</li>
+ * <li>DocBases --&gt; DocBase, AvgChunkDocs, BitsPerDocBaseDelta, DocBaseDeltas</li>
+ * <li>DocBase --&gt; first document ID of the block of chunks, as a {@link DataOutput#writeVInt VInt}</li>
+ * <li>AvgChunkDocs --&gt; average number of documents in a single chunk, as a {@link DataOutput#writeVInt VInt}</li>
+ * <li>BitsPerDocBaseDelta --&gt; number of bits required to represent a delta from the average using <a href="https://developers.google.com/protocol-buffers/docs/encoding#types">ZigZag encoding</a></li>
+ * <li>DocBaseDeltas --&gt; {@link PackedInts packed} array of BlockChunks elements of BitsPerDocBaseDelta bits each, representing the deltas from the average doc base using <a href="https://developers.google.com/protocol-buffers/docs/encoding#types">ZigZag encoding</a>.</li>
+ * <li>StartPointers --&gt; StartPointerBase, AvgChunkSize, BitsPerStartPointerDelta, StartPointerDeltas</li>
+ * <li>StartPointerBase --&gt; the first start pointer of the block, as a {@link DataOutput#writeVLong VLong}</li>
+ * <li>AvgChunkSize --&gt; the average size of a chunk of compressed documents, as a {@link DataOutput#writeVLong VLong}</li>
+ * <li>BitsPerStartPointerDelta --&gt; number of bits required to represent a delta from the average using <a href="https://developers.google.com/protocol-buffers/docs/encoding#types">ZigZag encoding</a></li>
+ * <li>StartPointerDeltas --&gt; {@link PackedInts packed} array of BlockChunks elements of BitsPerStartPointerDelta bits each, representing the deltas from the average start pointer using <a href="https://developers.google.com/protocol-buffers/docs/encoding#types">ZigZag encoding</a></li>
+ * </ul>
+ * <p>Notes</p>
+ * <ul>
+ * <li>For any block, the doc base of the n-th chunk can be restored with
+ * <code>DocBase + AvgChunkDocs * n + DocBaseDeltas[n]</code>.</li>
+ * <li>For any block, the start pointer of the n-th chunk can be restored with
+ * <code>StartPointerBase + AvgChunkSize * n + StartPointerDeltas[n]</code>.</li>
+ * <li>Once data is loaded into memory, you can lookup the start pointer of any
+ * document by performing two binary searches: a first one based on the values
+ * of DocBase in order to find the right block, and then inside the block based
+ * on DocBaseDeltas (by reconstructing the doc bases for every chunk).</li>
+ * </ul>
+ * </li>
+ * </ol>
+ * <p><b>Known limitations</b></p>
+ * <p>This {@link StoredFieldsFormat} does not support individual documents
+ * larger than (<tt>2<sup>31</sup> - 2<sup>14</sup></tt>) bytes. In case this
+ * is a problem, you should use another format, such as
+ * {@link Lucene40StoredFieldsFormat}.</p>
+ * @lucene.experimental
+ */
+public final class Lucene41StoredFieldsFormat extends CompressingStoredFieldsFormat {
+
+  public Lucene41StoredFieldsFormat() {
+    super(CompressionMode.FAST, 1 << 14);
+  }
+
+}

