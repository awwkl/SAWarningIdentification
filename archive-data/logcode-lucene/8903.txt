GitDiffStart: 8d8c559a5c325fbdb9bd52ea2a305f36ae635d33 | Fri Apr 12 08:42:34 2013 +0000
diff --git a/lucene/CHANGES.txt b/lucene/CHANGES.txt
index f266aff..1248dec 100644
--- a/lucene/CHANGES.txt
+++ b/lucene/CHANGES.txt
@@ -275,6 +275,13 @@ Bug Fixes
   now succeed even if the directory contains a corrupted index (Billow
   Gao, Robert Muir, Mike McCandless)
 
+* LUCENE-4928: Stored fields and term vectors could become super slow in case
+  of tiny documents (a few bytes). This is especially problematic when switching
+  codecs since bulk-merge strategies can't be applied and the same chunk of
+  documents can end up being decompressed thousands of times. A hard limit on
+  the number of documents per chunk has been added to fix this issue.
+  (Robert Muir, Adrien Grand)
+
 Documentation
 
 * LUCENE-4841: Added example SimpleSortedSetFacetsExample to show how
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter.java b/lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter.java
index e0498ed..865267a 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter.java
@@ -53,6 +53,9 @@ import org.apache.lucene.util.packed.PackedInts;
  */
 public final class CompressingStoredFieldsWriter extends StoredFieldsWriter {
 
+  // hard limit on the maximum number of documents per chunk
+  static final int MAX_DOCUMENTS_PER_CHUNK = 128;
+
   static final int         STRING = 0x00;
   static final int       BYTE_ARR = 0x01;
   static final int    NUMERIC_INT = 0x02;
@@ -200,7 +203,7 @@ public final class CompressingStoredFieldsWriter extends StoredFieldsWriter {
 
   private boolean triggerFlush() {
     return bufferedDocs.length >= chunkSize || // chunks of at least chunkSize bytes
-        numBufferedDocs >= chunkSize; // can be necessary if most docs are empty
+        numBufferedDocs >= MAX_DOCUMENTS_PER_CHUNK;
   }
 
   private void flush() throws IOException {
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter.java b/lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter.java
index 272de40..56d7519 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingTermVectorsWriter.java
@@ -56,6 +56,9 @@ import org.apache.lucene.util.packed.PackedInts;
  */
 public final class CompressingTermVectorsWriter extends TermVectorsWriter {
 
+  // hard limit on the maximum number of documents per chunk
+  static final int MAX_DOCUMENTS_PER_CHUNK = 128;
+
   static final String VECTORS_EXTENSION = "tvd";
   static final String VECTORS_INDEX_EXTENSION = "tvx";
 
@@ -322,7 +325,8 @@ public final class CompressingTermVectorsWriter extends TermVectorsWriter {
   }
 
   private boolean triggerFlush() {
-    return termSuffixes.length >= chunkSize || pendingDocs.size() >= chunkSize;
+    return termSuffixes.length >= chunkSize
+        || pendingDocs.size() >= MAX_DOCUMENTS_PER_CHUNK;
   }
 
   private void flush() throws IOException {

