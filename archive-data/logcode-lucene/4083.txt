GitDiffStart: de381d2816f6a8baceaf09fe16bc0d225bf30164 | Wed Mar 4 19:45:09 2015 +0000
diff --git a/solr/core/src/java/org/apache/solr/core/SolrCore.java b/solr/core/src/java/org/apache/solr/core/SolrCore.java
index 1169ed7..1ffbcc2 100644
--- a/solr/core/src/java/org/apache/solr/core/SolrCore.java
+++ b/solr/core/src/java/org/apache/solr/core/SolrCore.java
@@ -83,9 +83,9 @@ import org.apache.solr.common.util.IOUtils;
 import org.apache.solr.common.util.NamedList;
 import org.apache.solr.common.util.SimpleOrderedMap;
 import org.apache.solr.core.DirectoryFactory.DirContext;
+import org.apache.solr.handler.IndexFetcher;
 import org.apache.solr.handler.ReplicationHandler;
 import org.apache.solr.handler.RequestHandlerBase;
-import org.apache.solr.handler.SnapPuller;
 import org.apache.solr.handler.admin.ShowFileRequestHandler;
 import org.apache.solr.handler.component.DebugComponent;
 import org.apache.solr.handler.component.ExpandComponent;
@@ -291,7 +291,7 @@ public final class SolrCore implements SolrInfoMBean, Closeable {
       dir = getDirectoryFactory().get(getDataDir(), DirContext.META_DATA, getSolrConfig().indexConfig.lockType);
       IndexInput input;
       try {
-        input = dir.openInput(SnapPuller.INDEX_PROPERTIES, IOContext.DEFAULT);
+        input = dir.openInput(IndexFetcher.INDEX_PROPERTIES, IOContext.DEFAULT);
       } catch (FileNotFoundException | NoSuchFileException e) {
         input = null;
       }
@@ -307,7 +307,7 @@ public final class SolrCore implements SolrInfoMBean, Closeable {
           }
           
         } catch (Exception e) {
-          log.error("Unable to load " + SnapPuller.INDEX_PROPERTIES, e);
+          log.error("Unable to load " + IndexFetcher.INDEX_PROPERTIES, e);
         } finally {
           IOUtils.closeQuietly(is);
         }
diff --git a/solr/core/src/java/org/apache/solr/handler/IndexFetcher.java b/solr/core/src/java/org/apache/solr/handler/IndexFetcher.java
new file mode 100644
index 0000000..3cda5ef
--- /dev/null
+++ b/solr/core/src/java/org/apache/solr/handler/IndexFetcher.java
@@ -0,0 +1,1557 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.solr.handler;
+
+import static org.apache.solr.handler.ReplicationHandler.ALIAS;
+import static org.apache.solr.handler.ReplicationHandler.CHECKSUM;
+import static org.apache.solr.handler.ReplicationHandler.CMD_DETAILS;
+import static org.apache.solr.handler.ReplicationHandler.CMD_GET_FILE;
+import static org.apache.solr.handler.ReplicationHandler.CMD_GET_FILE_LIST;
+import static org.apache.solr.handler.ReplicationHandler.CMD_INDEX_VERSION;
+import static org.apache.solr.handler.ReplicationHandler.COMMAND;
+import static org.apache.solr.handler.ReplicationHandler.COMPRESSION;
+import static org.apache.solr.handler.ReplicationHandler.CONF_FILES;
+import static org.apache.solr.handler.ReplicationHandler.CONF_FILE_SHORT;
+import static org.apache.solr.handler.ReplicationHandler.EXTERNAL;
+import static org.apache.solr.handler.ReplicationHandler.FILE;
+import static org.apache.solr.handler.ReplicationHandler.FILE_STREAM;
+import static org.apache.solr.handler.ReplicationHandler.GENERATION;
+import static org.apache.solr.handler.ReplicationHandler.INTERNAL;
+import static org.apache.solr.handler.ReplicationHandler.MASTER_URL;
+import static org.apache.solr.handler.ReplicationHandler.NAME;
+import static org.apache.solr.handler.ReplicationHandler.OFFSET;
+import static org.apache.solr.handler.ReplicationHandler.SIZE;
+
+import java.io.File;
+import java.io.FileNotFoundException;
+import java.io.FileOutputStream;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.InputStreamReader;
+import java.io.OutputStreamWriter;
+import java.io.Writer;
+import java.nio.ByteBuffer;
+import java.nio.channels.FileChannel;
+import java.nio.charset.StandardCharsets;
+import java.nio.file.Files;
+import java.nio.file.NoSuchFileException;
+import java.text.SimpleDateFormat;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collection;
+import java.util.Collections;
+import java.util.Date;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Locale;
+import java.util.Map;
+import java.util.Properties;
+import java.util.Set;
+import java.util.concurrent.CountDownLatch;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Executors;
+import java.util.concurrent.Future;
+import java.util.concurrent.TimeUnit;
+import java.util.zip.Adler32;
+import java.util.zip.Checksum;
+import java.util.zip.InflaterInputStream;
+
+import org.apache.commons.io.IOUtils;
+import org.apache.http.client.HttpClient;
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.index.IndexCommit;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.SegmentInfos;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.solr.client.solrj.SolrServerException;
+import org.apache.solr.client.solrj.impl.HttpClientUtil;
+import org.apache.solr.client.solrj.impl.HttpSolrClient;
+import org.apache.solr.client.solrj.request.QueryRequest;
+import org.apache.solr.common.SolrException;
+import org.apache.solr.common.SolrException.ErrorCode;
+import org.apache.solr.common.params.CommonParams;
+import org.apache.solr.common.params.ModifiableSolrParams;
+import org.apache.solr.common.util.FastInputStream;
+import org.apache.solr.common.util.NamedList;
+import org.apache.solr.core.DirectoryFactory;
+import org.apache.solr.core.DirectoryFactory.DirContext;
+import org.apache.solr.core.IndexDeletionPolicyWrapper;
+import org.apache.solr.core.SolrCore;
+import org.apache.solr.handler.ReplicationHandler.FileInfo;
+import org.apache.solr.request.LocalSolrQueryRequest;
+import org.apache.solr.request.SolrQueryRequest;
+import org.apache.solr.search.SolrIndexSearcher;
+import org.apache.solr.update.CommitUpdateCommand;
+import org.apache.solr.util.DefaultSolrThreadFactory;
+import org.apache.solr.util.FileUtils;
+import org.apache.solr.util.PropertiesInputStream;
+import org.apache.solr.util.PropertiesOutputStream;
+import org.apache.solr.util.RefCounted;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+/**
+ * <p> Provides functionality of downloading changed index files as well as config files and a timer for scheduling fetches from the
+ * master. </p>
+ *
+ *
+ * @since solr 1.4
+ */
+public class IndexFetcher {
+  private static final int _100K = 100000;
+
+  public static final String INDEX_PROPERTIES = "index.properties";
+
+  private static final Logger LOG = LoggerFactory.getLogger(IndexFetcher.class.getName());
+
+  private final String masterUrl;
+
+  final ReplicationHandler replicationHandler;
+
+  private volatile long replicationStartTime;
+
+  private final SolrCore solrCore;
+
+  private volatile List<Map<String, Object>> filesToDownload;
+
+  private volatile List<Map<String, Object>> confFilesToDownload;
+
+  private volatile List<Map<String, Object>> filesDownloaded;
+
+  private volatile List<Map<String, Object>> confFilesDownloaded;
+
+  private volatile Map<String, Object> currentFile;
+
+  private volatile DirectoryFileFetcher dirFileFetcher;
+  
+  private volatile LocalFsFileFetcher localFileFetcher;
+
+  private volatile ExecutorService fsyncService;
+
+  private volatile boolean stop = false;
+
+  private boolean useInternal = false;
+
+  private boolean useExternal = false;
+
+  private final HttpClient myHttpClient;
+
+  private static HttpClient createHttpClient(SolrCore core, String connTimeout, String readTimeout, String httpBasicAuthUser, String httpBasicAuthPassword, boolean useCompression) {
+    final ModifiableSolrParams httpClientParams = new ModifiableSolrParams();
+    httpClientParams.set(HttpClientUtil.PROP_CONNECTION_TIMEOUT, connTimeout != null ? connTimeout : "5000");
+    httpClientParams.set(HttpClientUtil.PROP_SO_TIMEOUT, readTimeout != null ? readTimeout : "20000");
+    httpClientParams.set(HttpClientUtil.PROP_BASIC_AUTH_USER, httpBasicAuthUser);
+    httpClientParams.set(HttpClientUtil.PROP_BASIC_AUTH_PASS, httpBasicAuthPassword);
+    httpClientParams.set(HttpClientUtil.PROP_ALLOW_COMPRESSION, useCompression);
+
+    return HttpClientUtil.createClient(httpClientParams, core.getCoreDescriptor().getCoreContainer().getUpdateShardHandler().getConnectionManager());
+  }
+
+  public IndexFetcher(final NamedList initArgs, final ReplicationHandler handler, final SolrCore sc) {
+    solrCore = sc;
+    String masterUrl = (String) initArgs.get(MASTER_URL);
+    if (masterUrl == null)
+      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,
+              "'masterUrl' is required for a slave");
+    if (masterUrl.endsWith("/replication")) {
+      masterUrl = masterUrl.substring(0, masterUrl.length()-12);
+      LOG.warn("'masterUrl' must be specified without the /replication suffix");
+    }
+    this.masterUrl = masterUrl;
+    
+    this.replicationHandler = handler;
+    String compress = (String) initArgs.get(COMPRESSION);
+    useInternal = INTERNAL.equals(compress);
+    useExternal = EXTERNAL.equals(compress);
+    String connTimeout = (String) initArgs.get(HttpClientUtil.PROP_CONNECTION_TIMEOUT);
+    String readTimeout = (String) initArgs.get(HttpClientUtil.PROP_SO_TIMEOUT);
+    String httpBasicAuthUser = (String) initArgs.get(HttpClientUtil.PROP_BASIC_AUTH_USER);
+    String httpBasicAuthPassword = (String) initArgs.get(HttpClientUtil.PROP_BASIC_AUTH_PASS);
+    myHttpClient = createHttpClient(solrCore, connTimeout, readTimeout, httpBasicAuthUser, httpBasicAuthPassword, useExternal);
+  }
+
+  /**
+   * Gets the latest commit version and generation from the master
+   */
+  @SuppressWarnings("unchecked")
+  NamedList getLatestVersion() throws IOException {
+    ModifiableSolrParams params = new ModifiableSolrParams();
+    params.set(COMMAND, CMD_INDEX_VERSION);
+    params.set(CommonParams.WT, "javabin");
+    params.set(CommonParams.QT, "/replication");
+    QueryRequest req = new QueryRequest(params);
+
+    // TODO modify to use shardhandler
+    try (HttpSolrClient client = new HttpSolrClient(masterUrl, myHttpClient)) {
+      client.setSoTimeout(60000);
+      client.setConnectionTimeout(15000);
+      
+      return client.request(req);
+    } catch (SolrServerException e) {
+      throw new SolrException(ErrorCode.SERVER_ERROR, e.getMessage(), e);
+    }
+  }
+
+  /**
+   * Fetches the list of files in a given index commit point and updates internal list of files to download.
+   */
+  private void fetchFileList(long gen) throws IOException {
+    ModifiableSolrParams params = new ModifiableSolrParams();
+    params.set(COMMAND,  CMD_GET_FILE_LIST);
+    params.set(GENERATION, String.valueOf(gen));
+    params.set(CommonParams.WT, "javabin");
+    params.set(CommonParams.QT, "/replication");
+    QueryRequest req = new QueryRequest(params);
+
+    // TODO modify to use shardhandler
+    try (HttpSolrClient client = new HttpSolrClient(masterUrl, myHttpClient)) {
+      client.setSoTimeout(60000);
+      client.setConnectionTimeout(15000);
+      NamedList response = client.request(req);
+
+      List<Map<String, Object>> files = (List<Map<String,Object>>) response.get(CMD_GET_FILE_LIST);
+      if (files != null)
+        filesToDownload = Collections.synchronizedList(files);
+      else {
+        filesToDownload = Collections.emptyList();
+        LOG.error("No files to download for index generation: "+ gen);
+      }
+
+      files = (List<Map<String,Object>>) response.get(CONF_FILES);
+      if (files != null)
+        confFilesToDownload = Collections.synchronizedList(files);
+
+    } catch (SolrServerException e) {
+      throw new IOException(e);
+    }
+  }
+
+  private boolean successfulInstall = false;
+
+  /**
+   * This command downloads all the necessary files from master to install a index commit point. Only changed files are
+   * downloaded. It also downloads the conf files (if they are modified).
+   *
+   * @param core the SolrCore
+   * @param forceReplication force a replication in all cases 
+   * @return true on success, false if slave is already in sync
+   * @throws IOException if an exception occurs
+   */
+  boolean fetchLatestIndex(final SolrCore core, boolean forceReplication) throws IOException, InterruptedException {
+    successfulInstall = false;
+    replicationStartTime = System.currentTimeMillis();
+    Directory tmpIndexDir = null;
+    String tmpIndex = null;
+    Directory indexDir = null;
+    String indexDirPath = null;
+    boolean deleteTmpIdxDir = true;
+    try {
+      //get the current 'replicateable' index version in the master
+      NamedList response = null;
+      try {
+        response = getLatestVersion();
+      } catch (Exception e) {
+        LOG.error("Master at: " + masterUrl + " is not available. Index fetch failed. Exception: " + e.getMessage());
+        return false;
+      }
+      long latestVersion = (Long) response.get(CMD_INDEX_VERSION);
+      long latestGeneration = (Long) response.get(GENERATION);
+
+      // TODO: make sure that getLatestCommit only returns commit points for the main index (i.e. no side-car indexes)
+      IndexCommit commit = core.getDeletionPolicy().getLatestCommit();
+      if (commit == null) {
+        // Presumably the IndexWriter hasn't been opened yet, and hence the deletion policy hasn't been updated with commit points
+        RefCounted<SolrIndexSearcher> searcherRefCounted = null;
+        try {
+          searcherRefCounted = core.getNewestSearcher(false);
+          if (searcherRefCounted == null) {
+            LOG.warn("No open searcher found - fetch aborted");
+            return false;
+          }
+          commit = searcherRefCounted.get().getIndexReader().getIndexCommit();
+        } finally {
+          if (searcherRefCounted != null)
+            searcherRefCounted.decref();
+        }
+      }
+
+
+      if (latestVersion == 0L) {
+        if (forceReplication && commit.getGeneration() != 0) {
+          // since we won't get the files for an empty index,
+          // we just clear ours and commit
+          RefCounted<IndexWriter> iw = core.getUpdateHandler().getSolrCoreState().getIndexWriter(core);
+          try {
+            iw.get().deleteAll();
+          } finally {
+            iw.decref();
+          }
+          SolrQueryRequest req = new LocalSolrQueryRequest(core,
+              new ModifiableSolrParams());
+          core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));
+        }
+        
+        //there is nothing to be replicated
+        successfulInstall = true;
+        return true;
+      }
+      
+      if (!forceReplication && IndexDeletionPolicyWrapper.getCommitTimestamp(commit) == latestVersion) {
+        //master and slave are already in sync just return
+        LOG.info("Slave in sync with master.");
+        successfulInstall = true;
+        return true;
+      }
+      LOG.info("Master's generation: " + latestGeneration);
+      LOG.info("Slave's generation: " + commit.getGeneration());
+      LOG.info("Starting replication process");
+      // get the list of files first
+      fetchFileList(latestGeneration);
+      // this can happen if the commit point is deleted before we fetch the file list.
+      if(filesToDownload.isEmpty()) return false;
+      LOG.info("Number of files in latest index in master: " + filesToDownload.size());
+
+      // Create the sync service
+      fsyncService = Executors.newSingleThreadExecutor(new DefaultSolrThreadFactory("fsyncService"));
+      // use a synchronized list because the list is read by other threads (to show details)
+      filesDownloaded = Collections.synchronizedList(new ArrayList<Map<String, Object>>());
+      // if the generation of master is older than that of the slave , it means they are not compatible to be copied
+      // then a new index directory to be created and all the files need to be copied
+      boolean isFullCopyNeeded = IndexDeletionPolicyWrapper
+          .getCommitTimestamp(commit) >= latestVersion
+          || commit.getGeneration() >= latestGeneration || forceReplication;
+
+      String tmpIdxDirName = "index." + new SimpleDateFormat(SnapShooter.DATE_FMT, Locale.ROOT).format(new Date());
+      tmpIndex = createTempindexDir(core, tmpIdxDirName);
+
+      tmpIndexDir = core.getDirectoryFactory().get(tmpIndex, DirContext.DEFAULT, core.getSolrConfig().indexConfig.lockType);
+      
+      // cindex dir...
+      indexDirPath = core.getIndexDir();
+      indexDir = core.getDirectoryFactory().get(indexDirPath, DirContext.DEFAULT, core.getSolrConfig().indexConfig.lockType);
+
+      try {
+        
+        if (isIndexStale(indexDir)) {
+          isFullCopyNeeded = true;
+        }
+        
+        if (!isFullCopyNeeded) {
+          // a searcher might be using some flushed but not committed segments
+          // because of soft commits (which open a searcher on IW's data)
+          // so we need to close the existing searcher on the last commit
+          // and wait until we are able to clean up all unused lucene files
+          if (solrCore.getCoreDescriptor().getCoreContainer().isZooKeeperAware()) {
+            solrCore.closeSearcher();
+          }
+
+          // rollback and reopen index writer and wait until all unused files
+          // are successfully deleted
+          solrCore.getUpdateHandler().newIndexWriter(true);
+          RefCounted<IndexWriter> writer = solrCore.getUpdateHandler().getSolrCoreState().getIndexWriter(null);
+          try {
+            IndexWriter indexWriter = writer.get();
+            int c = 0;
+            indexWriter.deleteUnusedFiles();
+            while (hasUnusedFiles(indexDir, commit)) {
+              indexWriter.deleteUnusedFiles();
+              LOG.info("Sleeping for 1000ms to wait for unused lucene index files to be delete-able");
+              Thread.sleep(1000);
+              c++;
+              if (c >= 30)  {
+                LOG.warn("IndexFetcher unable to cleanup unused lucene index files so we must do a full copy instead");
+                isFullCopyNeeded = true;
+                break;
+              }
+            }
+            if (c > 0)  {
+              LOG.info("IndexFetcher slept for " + (c * 1000) + "ms for unused lucene index files to be delete-able");
+            }
+          } finally {
+            writer.decref();
+          }
+          solrCore.getUpdateHandler().getSolrCoreState().closeIndexWriter(core, true);
+        }
+        boolean reloadCore = false;
+        
+        try {
+          LOG.info("Starting download to " + tmpIndexDir + " fullCopy="
+              + isFullCopyNeeded);
+          successfulInstall = false;
+          
+          downloadIndexFiles(isFullCopyNeeded, indexDir, tmpIndexDir, latestGeneration);
+          LOG.info("Total time taken for download : "
+              + ((System.currentTimeMillis() - replicationStartTime) / 1000)
+              + " secs");
+          Collection<Map<String,Object>> modifiedConfFiles = getModifiedConfFiles(confFilesToDownload);
+          if (!modifiedConfFiles.isEmpty()) {
+            downloadConfFiles(confFilesToDownload, latestGeneration);
+            if (isFullCopyNeeded) {
+              successfulInstall = modifyIndexProps(tmpIdxDirName);
+              deleteTmpIdxDir = false;
+            } else {
+              successfulInstall = moveIndexFiles(tmpIndexDir, indexDir);
+            }
+            if (successfulInstall) {
+              if (isFullCopyNeeded) {
+                // let the system know we are changing dir's and the old one
+                // may be closed
+                if (indexDir != null) {
+                  LOG.info("removing old index directory " + indexDir);
+                  core.getDirectoryFactory().doneWithDirectory(indexDir);
+                  core.getDirectoryFactory().remove(indexDir);
+                }
+              }
+              
+              LOG.info("Configuration files are modified, core will be reloaded");
+              logReplicationTimeAndConfFiles(modifiedConfFiles,
+                  successfulInstall);// write to a file time of replication and
+                                     // conf files.
+              reloadCore = true;
+            }
+          } else {
+            terminateAndWaitFsyncService();
+            if (isFullCopyNeeded) {
+              successfulInstall = modifyIndexProps(tmpIdxDirName);
+              deleteTmpIdxDir = false;
+            } else {
+              successfulInstall = moveIndexFiles(tmpIndexDir, indexDir);
+            }
+            if (successfulInstall) {
+              logReplicationTimeAndConfFiles(modifiedConfFiles,
+                  successfulInstall);
+            }
+          }
+        } finally {
+          if (!isFullCopyNeeded) {
+            solrCore.getUpdateHandler().getSolrCoreState().openIndexWriter(core);
+          }
+        }
+        
+        // we must reload the core after we open the IW back up
+        if (reloadCore) {
+          reloadCore();
+        }
+
+        if (successfulInstall) {
+          if (isFullCopyNeeded) {
+            // let the system know we are changing dir's and the old one
+            // may be closed
+            if (indexDir != null) {
+              LOG.info("removing old index directory " + indexDir);
+              core.getDirectoryFactory().doneWithDirectory(indexDir);
+              core.getDirectoryFactory().remove(indexDir);
+            }
+          }
+          if (isFullCopyNeeded) {
+            solrCore.getUpdateHandler().newIndexWriter(isFullCopyNeeded);
+          }
+          
+          openNewSearcherAndUpdateCommitPoint();
+        }
+        
+        replicationStartTime = 0;
+        return successfulInstall;
+      } catch (ReplicationHandlerException e) {
+        LOG.error("User aborted Replication");
+        return false;
+      } catch (SolrException e) {
+        throw e;
+      } catch (InterruptedException e) {
+        throw new InterruptedException("Index fetch interrupted");
+      } catch (Exception e) {
+        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, "Index fetch failed : ", e);
+      }
+    } finally {
+      try {
+        if (!successfulInstall) {
+          try {
+            logReplicationTimeAndConfFiles(null, successfulInstall);
+          } catch(Exception e) {
+            LOG.error("caught", e);
+          }
+        }
+        filesToDownload = filesDownloaded = confFilesDownloaded = confFilesToDownload = null;
+        replicationStartTime = 0;
+        dirFileFetcher = null;
+        localFileFetcher = null;
+        if (fsyncService != null && !fsyncService.isShutdown()) fsyncService
+            .shutdownNow();
+        fsyncService = null;
+        stop = false;
+        fsyncException = null;
+      } finally {
+        if (deleteTmpIdxDir && tmpIndexDir != null) {
+          try {
+            core.getDirectoryFactory().doneWithDirectory(tmpIndexDir);
+            core.getDirectoryFactory().remove(tmpIndexDir);
+          } catch (IOException e) {
+            SolrException.log(LOG, "Error removing directory " + tmpIndexDir, e);
+          }
+        }
+        
+        if (tmpIndexDir != null) {
+          core.getDirectoryFactory().release(tmpIndexDir);
+        }
+        
+        if (indexDir != null) {
+          core.getDirectoryFactory().release(indexDir);
+        }
+      }
+    }
+  }
+
+  private boolean hasUnusedFiles(Directory indexDir, IndexCommit commit) throws IOException {
+    String segmentsFileName = commit.getSegmentsFileName();
+    SegmentInfos infos = SegmentInfos.readCommit(indexDir, segmentsFileName);
+    Set<String> currentFiles = new HashSet<>(infos.files(true));
+    String[] allFiles = indexDir.listAll();
+    for (String file : allFiles) {
+      if (!file.equals(segmentsFileName) && !currentFiles.contains(file) && !file.endsWith(".lock")) {
+        LOG.info("Found unused file: " + file);
+        return true;
+      }
+    }
+    return false;
+  }
+
+  private volatile Exception fsyncException;
+
+  /**
+   * terminate the fsync service and wait for all the tasks to complete. If it is already terminated
+   */
+  private void terminateAndWaitFsyncService() throws Exception {
+    if (fsyncService.isTerminated()) return;
+    fsyncService.shutdown();
+     // give a long wait say 1 hr
+    fsyncService.awaitTermination(3600, TimeUnit.SECONDS);
+    // if any fsync failed, throw that exception back
+    Exception fsyncExceptionCopy = fsyncException;
+    if (fsyncExceptionCopy != null) throw fsyncExceptionCopy;
+  }
+
+  /**
+   * Helper method to record the last replication's details so that we can show them on the statistics page across
+   * restarts.
+   * @throws IOException on IO error
+   */
+  private void logReplicationTimeAndConfFiles(Collection<Map<String, Object>> modifiedConfFiles, boolean successfulInstall) throws IOException {
+    List<String> confFiles = new ArrayList<>();
+    if (modifiedConfFiles != null && !modifiedConfFiles.isEmpty())
+      for (Map<String, Object> map1 : modifiedConfFiles)
+        confFiles.add((String) map1.get(NAME));
+
+    Properties props = replicationHandler.loadReplicationProperties();
+    long replicationTime = System.currentTimeMillis();
+    long replicationTimeTaken = (replicationTime - getReplicationStartTime()) / 1000;
+    Directory dir = null;
+    try {
+      dir = solrCore.getDirectoryFactory().get(solrCore.getDataDir(), DirContext.META_DATA, solrCore.getSolrConfig().indexConfig.lockType);
+      
+      int indexCount = 1, confFilesCount = 1;
+      if (props.containsKey(TIMES_INDEX_REPLICATED)) {
+        indexCount = Integer.valueOf(props.getProperty(TIMES_INDEX_REPLICATED)) + 1;
+      }
+      StringBuilder sb = readToStringBuilder(replicationTime, props.getProperty(INDEX_REPLICATED_AT_LIST));
+      props.setProperty(INDEX_REPLICATED_AT_LIST, sb.toString());
+      props.setProperty(INDEX_REPLICATED_AT, String.valueOf(replicationTime));
+      props.setProperty(PREVIOUS_CYCLE_TIME_TAKEN, String.valueOf(replicationTimeTaken));
+      props.setProperty(TIMES_INDEX_REPLICATED, String.valueOf(indexCount));
+      if (modifiedConfFiles != null && !modifiedConfFiles.isEmpty()) {
+        props.setProperty(CONF_FILES_REPLICATED, confFiles.toString());
+        props.setProperty(CONF_FILES_REPLICATED_AT, String.valueOf(replicationTime));
+        if (props.containsKey(TIMES_CONFIG_REPLICATED)) {
+          confFilesCount = Integer.valueOf(props.getProperty(TIMES_CONFIG_REPLICATED)) + 1;
+        }
+        props.setProperty(TIMES_CONFIG_REPLICATED, String.valueOf(confFilesCount));
+      }
+
+      props.setProperty(LAST_CYCLE_BYTES_DOWNLOADED, String.valueOf(getTotalBytesDownloaded()));
+      if (!successfulInstall) {
+        int numFailures = 1;
+        if (props.containsKey(TIMES_FAILED)) {
+          numFailures = Integer.valueOf(props.getProperty(TIMES_FAILED)) + 1;
+        }
+        props.setProperty(TIMES_FAILED, String.valueOf(numFailures));
+        props.setProperty(REPLICATION_FAILED_AT, String.valueOf(replicationTime));
+        sb = readToStringBuilder(replicationTime, props.getProperty(REPLICATION_FAILED_AT_LIST));
+        props.setProperty(REPLICATION_FAILED_AT_LIST, sb.toString());
+      }
+
+      final IndexOutput out = dir.createOutput(REPLICATION_PROPERTIES, DirectoryFactory.IOCONTEXT_NO_CACHE);
+      Writer outFile = new OutputStreamWriter(new PropertiesOutputStream(out), StandardCharsets.UTF_8);
+      try {
+        props.store(outFile, "Replication details");
+        dir.sync(Collections.singleton(REPLICATION_PROPERTIES));
+      } finally {
+        IOUtils.closeQuietly(outFile);
+      }
+    } catch (Exception e) {
+      LOG.warn("Exception while updating statistics", e);
+    } finally {
+      if (dir != null) {
+        solrCore.getDirectoryFactory().release(dir);
+      }
+    }
+  }
+
+  long getTotalBytesDownloaded() {
+    long bytesDownloaded = 0;
+    //get size from list of files to download
+    for (Map<String, Object> file : getFilesDownloaded()) {
+      bytesDownloaded += (Long) file.get(SIZE);
+    }
+
+    //get size from list of conf files to download
+    for (Map<String, Object> file : getConfFilesDownloaded()) {
+      bytesDownloaded += (Long) file.get(SIZE);
+    }
+
+    //get size from current file being downloaded
+    Map<String, Object> currentFile = getCurrentFile();
+    if (currentFile != null) {
+      if (currentFile.containsKey("bytesDownloaded")) {
+        bytesDownloaded += (Long) currentFile.get("bytesDownloaded");
+      }
+    }
+    return bytesDownloaded;
+  }
+
+  private StringBuilder readToStringBuilder(long replicationTime, String str) {
+    StringBuilder sb = new StringBuilder();
+    List<String> l = new ArrayList<>();
+    if (str != null && str.length() != 0) {
+      String[] ss = str.split(",");
+      Collections.addAll(l, ss);
+    }
+    sb.append(replicationTime);
+    if (!l.isEmpty()) {
+      for (int i = 0; i < l.size() || i < 9; i++) {
+        if (i == l.size() || i == 9) break;
+        String s = l.get(i);
+        sb.append(",").append(s);
+      }
+    }
+    return sb;
+  }
+
+  private void openNewSearcherAndUpdateCommitPoint() throws IOException {
+    SolrQueryRequest req = new LocalSolrQueryRequest(solrCore,
+        new ModifiableSolrParams());
+    
+    RefCounted<SolrIndexSearcher> searcher = null;
+    IndexCommit commitPoint;
+    try {
+      Future[] waitSearcher = new Future[1];
+      searcher = solrCore.getSearcher(true, true, waitSearcher, true);
+      if (waitSearcher[0] != null) {
+        try {
+          waitSearcher[0].get();
+        } catch (InterruptedException | ExecutionException e) {
+          SolrException.log(LOG, e);
+        }
+      }
+      commitPoint = searcher.get().getIndexReader().getIndexCommit();
+    } finally {
+      req.close();
+      if (searcher != null) {
+        searcher.decref();
+      }
+    }
+
+    // update the commit point in replication handler
+    replicationHandler.indexCommitPoint = commitPoint;
+    
+  }
+
+  /**
+   * All the files are copied to a temp dir first
+   */
+  private String createTempindexDir(SolrCore core, String tmpIdxDirName) {
+    // TODO: there should probably be a DirectoryFactory#concatPath(parent, name)
+    // or something
+    return core.getDataDir() + tmpIdxDirName;
+  }
+
+  private void reloadCore() {
+    final CountDownLatch latch = new CountDownLatch(1);
+    new Thread() {
+      @Override
+      public void run() {
+        try {
+          solrCore.getCoreDescriptor().getCoreContainer().reload(solrCore.getName());
+        } catch (Exception e) {
+          LOG.error("Could not reload core ", e);
+        } finally {
+          latch.countDown();
+        }
+      }
+    }.start();
+    try {
+      latch.await();
+    } catch (InterruptedException e) {
+      Thread.currentThread().interrupt();
+      throw new RuntimeException("Interrupted while waiting for core reload to finish", e);
+    }
+  }
+
+  private void downloadConfFiles(List<Map<String, Object>> confFilesToDownload, long latestGeneration) throws Exception {
+    LOG.info("Starting download of configuration files from master: " + confFilesToDownload);
+    confFilesDownloaded = Collections.synchronizedList(new ArrayList<>());
+    File tmpconfDir = new File(solrCore.getResourceLoader().getConfigDir(), "conf." + getDateAsStr(new Date()));
+    try {
+      boolean status = tmpconfDir.mkdirs();
+      if (!status) {
+        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,
+                "Failed to create temporary config folder: " + tmpconfDir.getName());
+      }
+      for (Map<String, Object> file : confFilesToDownload) {
+        String saveAs = (String) (file.get(ALIAS) == null ? file.get(NAME) : file.get(ALIAS));
+        localFileFetcher = new LocalFsFileFetcher(tmpconfDir, file, saveAs, true, latestGeneration);
+        currentFile = file;
+        localFileFetcher.fetchFile();
+        confFilesDownloaded.add(new HashMap<>(file));
+      }
+      // this is called before copying the files to the original conf dir
+      // so that if there is an exception avoid corrupting the original files.
+      terminateAndWaitFsyncService();
+      copyTmpConfFiles2Conf(tmpconfDir);
+    } finally {
+      delTree(tmpconfDir);
+    }
+  }
+
+  /**
+   * Download the index files. If a new index is needed, download all the files.
+   *
+   * @param downloadCompleteIndex is it a fresh index copy
+   * @param tmpIndexDir              the directory to which files need to be downloadeed to
+   * @param indexDir                 the indexDir to be merged to
+   * @param latestGeneration         the version number
+   */
+  private void downloadIndexFiles(boolean downloadCompleteIndex, Directory indexDir, Directory tmpIndexDir, long latestGeneration)
+      throws Exception {
+    if (LOG.isDebugEnabled()) {
+      LOG.debug("Download files to dir: " + Arrays.asList(indexDir.listAll()));
+    }
+    for (Map<String,Object> file : filesToDownload) {
+      String filename = (String) file.get(NAME);
+      long size = (Long) file.get(SIZE);
+      CompareResult compareResult = compareFile(indexDir, filename, size, (Long) file.get(CHECKSUM));
+      if (!compareResult.equal || downloadCompleteIndex
+          || filesToAlwaysDownloadIfNoChecksums(filename, size, compareResult)) {
+        dirFileFetcher = new DirectoryFileFetcher(tmpIndexDir, file,
+            (String) file.get(NAME), false, latestGeneration);
+        currentFile = file;
+        dirFileFetcher.fetchFile();
+        filesDownloaded.add(new HashMap<>(file));
+      } else {
+        LOG.info("Skipping download for " + file.get(NAME)
+            + " because it already exists");
+      }
+    }
+  }
+  
+  private boolean filesToAlwaysDownloadIfNoChecksums(String filename,
+      long size, CompareResult compareResult) {
+    // without checksums to compare, we always download .si, .liv, segments_N,
+    // and any very small files
+    return !compareResult.checkSummed && (filename.endsWith(".si") || filename.endsWith(".liv")
+    || filename.startsWith("segments_") || size < _100K);
+  }
+
+  static class CompareResult {
+    boolean equal = false;
+    boolean checkSummed = false;
+  }
+  
+  private CompareResult compareFile(Directory indexDir, String filename, Long backupIndexFileLen, Long backupIndexFileChecksum) {
+    CompareResult compareResult = new CompareResult();
+    try {
+      try (final IndexInput indexInput = indexDir.openInput(filename, IOContext.READONCE)) {
+        long indexFileLen = indexInput.length();
+        long indexFileChecksum = 0;
+        
+        if (backupIndexFileChecksum != null) {
+          try {
+            indexFileChecksum = CodecUtil.retrieveChecksum(indexInput);
+            compareResult.checkSummed = true;
+          } catch (Exception e) {
+            LOG.warn("Could not retrieve checksum from file.", e);
+          }
+        }
+        
+        if (!compareResult.checkSummed) {
+          // we don't have checksums to compare
+          
+          if (indexFileLen == backupIndexFileLen) {
+            compareResult.equal = true;
+            return compareResult;
+          } else {
+            LOG.warn(
+                "File {} did not match. expected length is {} and actual length is {}", filename, backupIndexFileLen, indexFileLen);
+            compareResult.equal = false;
+            return compareResult;
+          }
+        }
+        
+        // we have checksums to compare
+        
+        if (indexFileLen == backupIndexFileLen && indexFileChecksum == backupIndexFileChecksum) {
+          compareResult.equal = true;
+          return compareResult;
+        } else {
+          LOG.warn("File {} did not match. expected checksum is {} and actual is checksum {}. " +
+              "expected length is {} and actual length is {}", filename, backupIndexFileChecksum, indexFileChecksum,
+              backupIndexFileLen, indexFileLen);
+          compareResult.equal = false;
+          return compareResult;
+        }
+      }
+    } catch (NoSuchFileException | FileNotFoundException e) {
+      compareResult.equal = false;
+      return compareResult;
+    } catch (IOException e) {
+      LOG.error("Could not read file " + filename + ". Downloading it again", e);
+      compareResult.equal = false;
+      return compareResult;
+    }
+  }
+
+  /** Returns true if the file exists (can be opened), false
+   *  if it cannot be opened, and (unlike Java's
+   *  File.exists) throws IOException if there's some
+   *  unexpected error. */
+  private static boolean slowFileExists(Directory dir, String fileName) throws IOException {
+    try {
+      dir.openInput(fileName, IOContext.DEFAULT).close();
+      return true;
+    } catch (NoSuchFileException | FileNotFoundException e) {
+      return false;
+    }
+  }  
+
+  /**
+   * All the files which are common between master and slave must have same size else we assume they are
+   * not compatible (stale).
+   *
+   * @return true if the index stale and we need to download a fresh copy, false otherwise.
+   * @throws IOException  if low level io error
+   */
+  private boolean isIndexStale(Directory dir) throws IOException {
+    for (Map<String, Object> file : filesToDownload) {
+      String filename = (String) file.get(NAME);
+      Long length = (Long) file.get(SIZE);
+      Long checksum = (Long) file.get(CHECKSUM);
+      if (slowFileExists(dir, filename)) {
+        if (checksum != null) {
+          if (!(compareFile(dir, filename, length, checksum).equal)) {
+            // file exists and size or checksum is different, therefore we must download it again
+            return true;
+          }
+        } else {
+          if (length != dir.fileLength(filename)) {
+            LOG.warn("File {} did not match. expected length is {} and actual length is {}",
+                filename, length, dir.fileLength(filename));
+            return true;
+          }
+        }
+      }
+    }
+    return false;
+  }
+
+  /**
+   * Copy a file by the File#renameTo() method. If it fails, it is considered a failure
+   * <p/>
+   */
+  private boolean moveAFile(Directory tmpIdxDir, Directory indexDir, String fname) {
+    LOG.debug("Moving file: {}", fname);
+    boolean success = false;
+    try {
+      if (slowFileExists(indexDir, fname)) {
+        LOG.info("Skipping move file - it already exists:" + fname);
+        return true;
+      }
+    } catch (IOException e) {
+      SolrException.log(LOG, "could not check if a file exists", e);
+      return false;
+    }
+    try {
+      solrCore.getDirectoryFactory().move(tmpIdxDir, indexDir, fname, DirectoryFactory.IOCONTEXT_NO_CACHE);
+      success = true;
+    } catch (IOException e) {
+      SolrException.log(LOG, "Could not move file", e);
+    }
+    return success;
+  }
+
+  /**
+   * Copy all index files from the temp index dir to the actual index. The segments_N file is copied last.
+   */
+  private boolean moveIndexFiles(Directory tmpIdxDir, Directory indexDir) {
+    if (LOG.isDebugEnabled()) {
+      try {
+        LOG.info("From dir files:" + Arrays.asList(tmpIdxDir.listAll()));
+        LOG.info("To dir files:" + Arrays.asList(indexDir.listAll()));
+      } catch (IOException e) {
+        throw new RuntimeException(e);
+      }
+    }
+    String segmentsFile = null;
+    for (Map<String, Object> f : filesDownloaded) {
+      String fname = (String) f.get(NAME);
+      // the segments file must be copied last
+      // or else if there is a failure in between the
+      // index will be corrupted
+      if (fname.startsWith("segments_")) {
+        //The segments file must be copied in the end
+        //Otherwise , if the copy fails index ends up corrupted
+        segmentsFile = fname;
+        continue;
+      }
+      if (!moveAFile(tmpIdxDir, indexDir, fname)) return false;
+    }
+    //copy the segments file last
+    if (segmentsFile != null) {
+      if (!moveAFile(tmpIdxDir, indexDir, segmentsFile)) return false;
+    }
+    return true;
+  }
+
+  /**
+   * Make file list 
+   */
+  private List<File> makeTmpConfDirFileList(File dir, List<File> fileList) {
+    File[] files = dir.listFiles();
+    for (File file : files) {
+      if (file.isFile()) {
+        fileList.add(file);
+      } else if (file.isDirectory()) {
+        fileList = makeTmpConfDirFileList(file, fileList);
+      }
+    }
+    return fileList;
+  }
+  
+  /**
+   * The conf files are copied to the tmp dir to the conf dir. A backup of the old file is maintained
+   */
+  private void copyTmpConfFiles2Conf(File tmpconfDir) {
+    boolean status = false;
+    File confDir = new File(solrCore.getResourceLoader().getConfigDir());
+    for (File file : makeTmpConfDirFileList(tmpconfDir, new ArrayList<>())) {
+      File oldFile = new File(confDir, file.getPath().substring(tmpconfDir.getPath().length(), file.getPath().length()));
+      if (!oldFile.getParentFile().exists()) {
+        status = oldFile.getParentFile().mkdirs();
+        if (!status) {
+          throw new SolrException(ErrorCode.SERVER_ERROR,
+                  "Unable to mkdirs: " + oldFile.getParentFile());
+        }
+      }
+      if (oldFile.exists()) {
+        File backupFile = new File(oldFile.getPath() + "." + getDateAsStr(new Date(oldFile.lastModified())));
+        if (!backupFile.getParentFile().exists()) {
+          status = backupFile.getParentFile().mkdirs();
+          if (!status) {
+            throw new SolrException(ErrorCode.SERVER_ERROR,
+                    "Unable to mkdirs: " + backupFile.getParentFile());
+          }
+        }
+        status = oldFile.renameTo(backupFile);
+        if (!status) {
+          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,
+                  "Unable to rename: " + oldFile + " to: " + backupFile);
+        }
+      }
+      status = file.renameTo(oldFile);
+      if (!status) {
+        throw new SolrException(ErrorCode.SERVER_ERROR,
+                "Unable to rename: " + file + " to: " + oldFile);
+      }
+    }
+  }
+
+  private String getDateAsStr(Date d) {
+    return new SimpleDateFormat(SnapShooter.DATE_FMT, Locale.ROOT).format(d);
+  }
+
+  /**
+   * If the index is stale by any chance, load index from a different dir in the data dir.
+   */
+  private boolean modifyIndexProps(String tmpIdxDirName) {
+    LOG.info("New index installed. Updating index properties... index="+tmpIdxDirName);
+    Properties p = new Properties();
+    Directory dir = null;
+    try {
+      dir = solrCore.getDirectoryFactory().get(solrCore.getDataDir(), DirContext.META_DATA, solrCore.getSolrConfig().indexConfig.lockType);
+      if (slowFileExists(dir, IndexFetcher.INDEX_PROPERTIES)){
+        final IndexInput input = dir.openInput(IndexFetcher.INDEX_PROPERTIES, DirectoryFactory.IOCONTEXT_NO_CACHE);
+  
+        final InputStream is = new PropertiesInputStream(input);
+        try {
+          p.load(new InputStreamReader(is, StandardCharsets.UTF_8));
+        } catch (Exception e) {
+          LOG.error("Unable to load " + IndexFetcher.INDEX_PROPERTIES, e);
+        } finally {
+          IOUtils.closeQuietly(is);
+        }
+      }
+      try {
+        dir.deleteFile(IndexFetcher.INDEX_PROPERTIES);
+      } catch (IOException e) {
+        // no problem
+      }
+      final IndexOutput out = dir.createOutput(IndexFetcher.INDEX_PROPERTIES, DirectoryFactory.IOCONTEXT_NO_CACHE);
+      p.put("index", tmpIdxDirName);
+      Writer os = null;
+      try {
+        os = new OutputStreamWriter(new PropertiesOutputStream(out), StandardCharsets.UTF_8);
+        p.store(os, IndexFetcher.INDEX_PROPERTIES);
+        dir.sync(Collections.singleton(INDEX_PROPERTIES));
+      } catch (Exception e) {
+        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,
+            "Unable to write " + IndexFetcher.INDEX_PROPERTIES, e);
+      } finally {
+        IOUtils.closeQuietly(os);
+      }
+      return true;
+
+    } catch (IOException e1) {
+      throw new RuntimeException(e1);
+    } finally {
+      if (dir != null) {
+        try {
+          solrCore.getDirectoryFactory().release(dir);
+        } catch (IOException e) {
+          SolrException.log(LOG, "", e);
+        }
+      }
+    }
+    
+  }
+
+  private final Map<String, FileInfo> confFileInfoCache = new HashMap<>();
+
+  /**
+   * The local conf files are compared with the conf files in the master. If they are same (by checksum) do not copy.
+   *
+   * @param confFilesToDownload The list of files obtained from master
+   *
+   * @return a list of configuration files which have changed on the master and need to be downloaded.
+   */
+  private Collection<Map<String, Object>> getModifiedConfFiles(List<Map<String, Object>> confFilesToDownload) {
+    if (confFilesToDownload == null || confFilesToDownload.isEmpty())
+      return Collections.EMPTY_LIST;
+    //build a map with alias/name as the key
+    Map<String, Map<String, Object>> nameVsFile = new HashMap<>();
+    NamedList names = new NamedList();
+    for (Map<String, Object> map : confFilesToDownload) {
+      //if alias is present that is the name the file may have in the slave
+      String name = (String) (map.get(ALIAS) == null ? map.get(NAME) : map.get(ALIAS));
+      nameVsFile.put(name, map);
+      names.add(name, null);
+    }
+    //get the details of the local conf files with the same alias/name
+    List<Map<String, Object>> localFilesInfo = replicationHandler.getConfFileInfoFromCache(names, confFileInfoCache);
+    //compare their size/checksum to see if
+    for (Map<String, Object> fileInfo : localFilesInfo) {
+      String name = (String) fileInfo.get(NAME);
+      Map<String, Object> m = nameVsFile.get(name);
+      if (m == null) continue; // the file is not even present locally (so must be downloaded)
+      if (m.get(CHECKSUM).equals(fileInfo.get(CHECKSUM))) {
+        nameVsFile.remove(name); //checksums are same so the file need not be downloaded
+      }
+    }
+    return nameVsFile.isEmpty() ? Collections.EMPTY_LIST : nameVsFile.values();
+  }
+  
+  /** 
+   * This simulates File.delete exception-wise, since this class has some strange behavior with it.
+   * The only difference is it returns null on success, throws SecurityException on SecurityException, 
+   * otherwise returns Throwable preventing deletion (instead of false), for additional information.
+   */
+  static Throwable delete(File file) {
+    try {
+      Files.delete(file.toPath());
+      return null;
+    } catch (SecurityException e) {
+      throw e;
+    } catch (Throwable other) {
+      return other;
+    }
+  }
+  
+  static boolean delTree(File dir) {
+    try {
+      org.apache.lucene.util.IOUtils.rm(dir.toPath());
+      return true;
+    } catch (IOException e) {
+      LOG.warn("Unable to delete directory : " + dir, e);
+      return false;
+    }
+  }
+
+  /**
+   * Stops the ongoing fetch
+   */
+  void abortFetch() {
+    stop = true;
+  }
+
+  long getReplicationStartTime() {
+    return replicationStartTime;
+  }
+
+  long getReplicationTimeElapsed() {
+    long timeElapsed = 0;
+    if (getReplicationStartTime() > 0)
+      timeElapsed = TimeUnit.SECONDS.convert(System.currentTimeMillis() - getReplicationStartTime(), TimeUnit.MILLISECONDS);
+    return timeElapsed;
+  }
+
+  List<Map<String, Object>> getConfFilesToDownload() {
+    //make a copy first because it can be null later
+    List<Map<String, Object>> tmp = confFilesToDownload;
+    //create a new instance. or else iterator may fail
+    return tmp == null ? Collections.EMPTY_LIST : new ArrayList<>(tmp);
+  }
+
+  List<Map<String, Object>> getConfFilesDownloaded() {
+    //make a copy first because it can be null later
+    List<Map<String, Object>> tmp = confFilesDownloaded;
+    // NOTE: it's safe to make a copy of a SynchronizedCollection(ArrayList)
+    return tmp == null ? Collections.EMPTY_LIST : new ArrayList<>(tmp);
+  }
+
+  List<Map<String, Object>> getFilesToDownload() {
+    //make a copy first because it can be null later
+    List<Map<String, Object>> tmp = filesToDownload;
+    return tmp == null ? Collections.EMPTY_LIST : new ArrayList<>(tmp);
+  }
+
+  List<Map<String, Object>> getFilesDownloaded() {
+    List<Map<String, Object>> tmp = filesDownloaded;
+    return tmp == null ? Collections.EMPTY_LIST : new ArrayList<>(tmp);
+  }
+
+  // TODO: currently does not reflect conf files
+  Map<String, Object> getCurrentFile() {
+    Map<String, Object> tmp = currentFile;
+    DirectoryFileFetcher tmpFileFetcher = dirFileFetcher;
+    if (tmp == null)
+      return null;
+    tmp = new HashMap<>(tmp);
+    if (tmpFileFetcher != null)
+      tmp.put("bytesDownloaded", tmpFileFetcher.getBytesDownloaded());
+    return tmp;
+  }
+
+  private static class ReplicationHandlerException extends InterruptedException {
+    public ReplicationHandlerException(String message) {
+      super(message);
+    }
+  }
+
+  private interface FileInterface {
+    public void sync() throws IOException;
+    public void write(byte[] buf, int packetSize) throws IOException;
+    public void close() throws Exception;
+    public void delete() throws Exception;
+  }
+
+  /**
+   * The class acts as a client for ReplicationHandler.FileStream. It understands the protocol of wt=filestream
+   *
+   * @see org.apache.solr.handler.ReplicationHandler.DirectoryFileStream
+   */
+  private class FileFetcher {
+    private final FileInterface file;
+    private boolean includeChecksum = true;
+    private String fileName;
+    private String saveAs;
+    private boolean isConf;
+    private Long indexGen;
+
+    private long size;
+    private long bytesDownloaded = 0;
+    private byte[] buf = new byte[1024 * 1024];
+    private Checksum checksum;
+    private int errorCount = 0;
+    private boolean aborted = false;
+
+    FileFetcher(FileInterface file, Map<String, Object> fileDetails, String saveAs,
+                boolean isConf, long latestGen) throws IOException {
+      this.file = file;
+      this.fileName = (String) fileDetails.get(NAME);
+      this.size = (Long) fileDetails.get(SIZE);
+      this.isConf = isConf;
+      this.saveAs = saveAs;
+      indexGen = latestGen;
+      if (includeChecksum)
+        checksum = new Adler32();
+    }
+
+    public long getBytesDownloaded() {
+      return bytesDownloaded;
+    }
+
+    /**
+     * The main method which downloads file
+     */
+    public void fetchFile() throws Exception {
+      try {
+        while (true) {
+          final FastInputStream is = getStream();
+          int result;
+          try {
+            //fetch packets one by one in a single request
+            result = fetchPackets(is);
+            if (result == 0 || result == NO_CONTENT) {
+
+              return;
+            }
+            //if there is an error continue. But continue from the point where it got broken
+          } finally {
+            IOUtils.closeQuietly(is);
+          }
+        }
+      } finally {
+        cleanup();
+        //if cleanup succeeds . The file is downloaded fully. do an fsync
+        fsyncService.submit(new Runnable(){
+          @Override
+          public void run() {
+            try {
+              file.sync();
+            } catch (IOException e) {
+              fsyncException = e;
+            }
+          }
+        });
+      }
+    }
+
+    private int fetchPackets(FastInputStream fis) throws Exception {
+      byte[] intbytes = new byte[4];
+      byte[] longbytes = new byte[8];
+      try {
+        while (true) {
+          if (stop) {
+            stop = false;
+            aborted = true;
+            throw new ReplicationHandlerException("User aborted replication");
+          }
+          long checkSumServer = -1;
+          fis.readFully(intbytes);
+          //read the size of the packet
+          int packetSize = readInt(intbytes);
+          if (packetSize <= 0) {
+            LOG.warn("No content received for file: {}", fileName);
+            return NO_CONTENT;
+          }
+          if (buf.length < packetSize)
+            buf = new byte[packetSize];
+          if (checksum != null) {
+            //read the checksum
+            fis.readFully(longbytes);
+            checkSumServer = readLong(longbytes);
+          }
+          //then read the packet of bytes
+          fis.readFully(buf, 0, packetSize);
+          //compare the checksum as sent from the master
+          if (includeChecksum) {
+            checksum.reset();
+            checksum.update(buf, 0, packetSize);
+            long checkSumClient = checksum.getValue();
+            if (checkSumClient != checkSumServer) {
+              LOG.error("Checksum not matched between client and server for file: {}", fileName);
+              //if checksum is wrong it is a problem return for retry
+              return 1;
+            }
+          }
+          //if everything is fine, write down the packet to the file
+          file.write(buf, packetSize);
+          bytesDownloaded += packetSize;
+          LOG.debug("Fetched and wrote {} bytes of file: {}", bytesDownloaded, fileName);
+          if (bytesDownloaded >= size)
+            return 0;
+          //errorCount is always set to zero after a successful packet
+          errorCount = 0;
+        }
+      } catch (ReplicationHandlerException e) {
+        throw e;
+      } catch (Exception e) {
+        LOG.warn("Error in fetching file: {} (downloaded {} of {} bytes)",
+            fileName, bytesDownloaded, size, e);
+        //for any failure, increment the error count
+        errorCount++;
+        //if it fails for the same packet for MAX_RETRIES fail and come out
+        if (errorCount > MAX_RETRIES) {
+          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,
+              "Failed to fetch file: " + fileName +
+                  " (downloaded " + bytesDownloaded + " of " + size + " bytes" +
+                  ", error count: " + errorCount + " > " + MAX_RETRIES + ")", e);
+        }
+        return ERR;
+      }
+    }
+
+    /**
+     * The webcontainer flushes the data only after it fills the buffer size. So, all data has to be read as readFully()
+     * other wise it fails. So read everything as bytes and then extract an integer out of it
+     */
+    private int readInt(byte[] b) {
+      return (((b[0] & 0xff) << 24) | ((b[1] & 0xff) << 16)
+          | ((b[2] & 0xff) << 8) | (b[3] & 0xff));
+
+    }
+
+    /**
+     * Same as above but to read longs from a byte array
+     */
+    private long readLong(byte[] b) {
+      return (((long) (b[0] & 0xff)) << 56) | (((long) (b[1] & 0xff)) << 48)
+          | (((long) (b[2] & 0xff)) << 40) | (((long) (b[3] & 0xff)) << 32)
+          | (((long) (b[4] & 0xff)) << 24) | ((b[5] & 0xff) << 16)
+          | ((b[6] & 0xff) << 8) | ((b[7] & 0xff));
+
+    }
+
+    /**
+     * cleanup everything
+     */
+    private void cleanup() {
+      try {
+        file.close();
+      } catch (Exception e) {/* no-op */
+        LOG.error("Error closing file: {}", this.saveAs, e);
+      }
+      if (bytesDownloaded != size) {
+        //if the download is not complete then
+        //delete the file being downloaded
+        try {
+          file.delete();
+        } catch (Exception e) {
+          LOG.error("Error deleting file: {}", this.saveAs, e);
+        }
+        //if the failure is due to a user abort it is returned normally else an exception is thrown
+        if (!aborted)
+          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,
+              "Unable to download " + fileName + " completely. Downloaded "
+                  + bytesDownloaded + "!=" + size);
+      }
+    }
+
+    /**
+     * Open a new stream using HttpClient
+     */
+    private FastInputStream getStream() throws IOException {
+
+      ModifiableSolrParams params = new ModifiableSolrParams();
+
+//    //the method is command=filecontent
+      params.set(COMMAND, CMD_GET_FILE);
+      params.set(GENERATION, Long.toString(indexGen));
+      params.set(CommonParams.QT, "/replication");
+      //add the version to download. This is used to reserve the download
+      if (isConf) {
+        //set cf instead of file for config file
+        params.set(CONF_FILE_SHORT, fileName);
+      } else {
+        params.set(FILE, fileName);
+      }
+      if (useInternal) {
+        params.set(COMPRESSION, "true");
+      }
+      //use checksum
+      if (this.includeChecksum) {
+        params.set(CHECKSUM, true);
+      }
+      //wt=filestream this is a custom protocol
+      params.set(CommonParams.WT, FILE_STREAM);
+      // This happen if there is a failure there is a retry. the offset=<sizedownloaded> ensures that
+      // the server starts from the offset
+      if (bytesDownloaded > 0) {
+        params.set(OFFSET, Long.toString(bytesDownloaded));
+      }
+
+
+      NamedList response;
+      InputStream is = null;
+
+      // TODO use shardhandler
+      try (HttpSolrClient client = new HttpSolrClient(masterUrl, myHttpClient, null)) {
+        client.setSoTimeout(60000);
+        client.setConnectionTimeout(15000);
+        QueryRequest req = new QueryRequest(params);
+        response = client.request(req);
+        is = (InputStream) response.get("stream");
+        if(useInternal) {
+          is = new InflaterInputStream(is);
+        }
+        return new FastInputStream(is);
+      } catch (Exception e) {
+        //close stream on error
+        IOUtils.closeQuietly(is);
+        throw new IOException("Could not download file '" + fileName + "'", e);
+      }
+    }
+  }
+
+  private class DirectoryFile implements FileInterface {
+    private final String saveAs;
+    private Directory copy2Dir;
+    private IndexOutput outStream;
+
+    DirectoryFile(Directory tmpIndexDir, String saveAs) throws IOException {
+      this.saveAs = saveAs;
+      this.copy2Dir = tmpIndexDir;
+      outStream = copy2Dir.createOutput(this.saveAs, DirectoryFactory.IOCONTEXT_NO_CACHE);
+    }
+
+    public void sync() throws IOException {
+      copy2Dir.sync(Collections.singleton(saveAs));
+    }
+
+    public void write(byte[] buf, int packetSize) throws IOException {
+      outStream.writeBytes(buf, 0, packetSize);
+    }
+
+    public void close() throws Exception {
+      outStream.close();
+    }
+
+    public void delete() throws Exception {
+      copy2Dir.deleteFile(saveAs);
+    }
+  }
+
+  private class DirectoryFileFetcher extends FileFetcher {
+    DirectoryFileFetcher(Directory tmpIndexDir, Map<String, Object> fileDetails, String saveAs,
+                boolean isConf, long latestGen) throws IOException {
+      super(new DirectoryFile(tmpIndexDir, saveAs), fileDetails, saveAs, isConf, latestGen);
+    }
+  }
+
+  private class LocalFsFile implements FileInterface {
+    private File copy2Dir;
+
+    FileChannel fileChannel;
+    private FileOutputStream fileOutputStream;
+    File file;
+
+    LocalFsFile(File dir, String saveAs) throws IOException {
+      this.copy2Dir = dir;
+
+      this.file = new File(copy2Dir, saveAs);
+
+      File parentDir = this.file.getParentFile();
+      if( ! parentDir.exists() ){
+        if ( ! parentDir.mkdirs() ) {
+          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,
+              "Failed to create (sub)directory for file: " + saveAs);
+        }
+      }
+
+      this.fileOutputStream = new FileOutputStream(file);
+      this.fileChannel = this.fileOutputStream.getChannel();
+    }
+
+    public void sync() throws IOException {
+      FileUtils.sync(file);
+    }
+
+    public void write(byte[] buf, int packetSize) throws IOException {
+      fileChannel.write(ByteBuffer.wrap(buf, 0, packetSize));
+    }
+
+    public void close() throws Exception {
+      //close the FileOutputStream (which also closes the Channel)
+      fileOutputStream.close();
+    }
+
+    public void delete() throws Exception {
+      Files.delete(file.toPath());
+    }
+  }
+
+  private class LocalFsFileFetcher extends FileFetcher {
+    LocalFsFileFetcher(File dir, Map<String, Object> fileDetails, String saveAs,
+                boolean isConf, long latestGen) throws IOException {
+      super(new LocalFsFile(dir, saveAs), fileDetails, saveAs, isConf, latestGen);
+    }
+  }
+
+  NamedList getDetails() throws IOException, SolrServerException {
+    ModifiableSolrParams params = new ModifiableSolrParams();
+    params.set(COMMAND, CMD_DETAILS);
+    params.set("slave", false);
+    params.set(CommonParams.QT, "/replication");
+
+    // TODO use shardhandler
+    try (HttpSolrClient client = new HttpSolrClient(masterUrl, myHttpClient)) {
+      client.setSoTimeout(60000);
+      client.setConnectionTimeout(15000);
+      QueryRequest request = new QueryRequest(params);
+      return client.request(request);
+    }
+  }
+
+  public void destroy() {
+    abortFetch();
+  }
+
+  String getMasterUrl() {
+    return masterUrl;
+  }
+
+  private static final int MAX_RETRIES = 5;
+
+  private static final int NO_CONTENT = 1;
+
+  private static final int ERR = 2;
+
+  public static final String REPLICATION_PROPERTIES = "replication.properties";
+
+  static final String INDEX_REPLICATED_AT = "indexReplicatedAt";
+
+  static final String TIMES_INDEX_REPLICATED = "timesIndexReplicated";
+
+  static final String CONF_FILES_REPLICATED = "confFilesReplicated";
+
+  static final String CONF_FILES_REPLICATED_AT = "confFilesReplicatedAt";
+
+  static final String TIMES_CONFIG_REPLICATED = "timesConfigReplicated";
+
+  static final String LAST_CYCLE_BYTES_DOWNLOADED = "lastCycleBytesDownloaded";
+
+  static final String TIMES_FAILED = "timesFailed";
+
+  static final String REPLICATION_FAILED_AT = "replicationFailedAt";
+
+  static final String PREVIOUS_CYCLE_TIME_TAKEN = "previousCycleTimeInSeconds";
+
+  static final String INDEX_REPLICATED_AT_LIST = "indexReplicatedAtList";
+
+  static final String REPLICATION_FAILED_AT_LIST = "replicationFailedAtList";
+}
diff --git a/solr/core/src/java/org/apache/solr/handler/ReplicationHandler.java b/solr/core/src/java/org/apache/solr/handler/ReplicationHandler.java
index f4ab68d..54630c9 100644
--- a/solr/core/src/java/org/apache/solr/handler/ReplicationHandler.java
+++ b/solr/core/src/java/org/apache/solr/handler/ReplicationHandler.java
@@ -36,9 +36,13 @@ import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 import java.util.Properties;
+import java.util.concurrent.Executors;
+import java.util.concurrent.ScheduledExecutorService;
 import java.util.concurrent.TimeUnit;
 import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.concurrent.locks.ReentrantLock;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
 import java.util.zip.Adler32;
 import java.util.zip.Checksum;
 import java.util.zip.DeflaterOutputStream;
@@ -60,6 +64,7 @@ import org.apache.solr.common.SolrException.ErrorCode;
 import org.apache.solr.common.params.CommonParams;
 import org.apache.solr.common.params.ModifiableSolrParams;
 import org.apache.solr.common.params.SolrParams;
+import org.apache.solr.common.util.ExecutorUtil;
 import org.apache.solr.common.util.FastOutputStream;
 import org.apache.solr.common.util.NamedList;
 import org.apache.solr.common.util.SimpleOrderedMap;
@@ -75,6 +80,7 @@ import org.apache.solr.request.SolrQueryRequest;
 import org.apache.solr.response.SolrQueryResponse;
 import org.apache.solr.search.SolrIndexSearcher;
 import org.apache.solr.update.SolrIndexWriter;
+import org.apache.solr.util.DefaultSolrThreadFactory;
 import org.apache.solr.util.NumberUtils;
 import org.apache.solr.util.PropertiesInputStream;
 import org.apache.solr.util.RefCounted;
@@ -90,8 +96,8 @@ import org.slf4j.LoggerFactory;
  * file (command=filecontent&amp;file=&lt;FILE_NAME&gt;) You can optionally specify an offset and length to get that
  * chunk of the file. You can request a configuration file by using "cf" parameter instead of the "file" parameter.</li>
  * <li>Get status/statistics (command=details)</li> </ol> <p>When running on the slave, it provides the following
- * commands <ol> <li>Perform a snap pull now (command=snappull)</li> <li>Get status/statistics (command=details)</li>
- * <li>Abort a snap pull (command=abort)</li> <li>Enable/Disable polling the master for new versions (command=enablepoll
+ * commands <ol> <li>Perform an index fetch now (command=snappull)</li> <li>Get status/statistics (command=details)</li>
+ * <li>Abort an index fetch (command=abort)</li> <li>Enable/Disable polling the master for new versions (command=enablepoll
  * or command=disablepoll)</li> </ol>
  *
  *
@@ -134,9 +140,9 @@ public class ReplicationHandler extends RequestHandlerBase implements SolrCoreAw
     }
   }
 
-  private SnapPuller snapPuller;
+  private IndexFetcher pollingIndexFetcher;
 
-  private ReentrantLock snapPullLock = new ReentrantLock();
+  private ReentrantLock indexFetchLock = new ReentrantLock();
 
   private String includeConfFiles;
 
@@ -151,14 +157,18 @@ public class ReplicationHandler extends RequestHandlerBase implements SolrCoreAw
   private boolean replicateOnCommit = false;
 
   private boolean replicateOnStart = false;
-  
+
+  private ScheduledExecutorService executorService;
+
+  private volatile long executorStartTime;
+
   private int numberBackupsToKeep = 0; //zero: do not delete old backups
 
   private int numTimesReplicated = 0;
 
   private final Map<String, FileInfo> confFileInfoCache = new HashMap<>();
 
-  private Integer reserveCommitDuration = SnapPuller.readInterval("00:00:10");
+  private Integer reserveCommitDuration = readInterval("00:00:10");
 
   volatile IndexCommit indexCommitPoint;
 
@@ -166,6 +176,19 @@ public class ReplicationHandler extends RequestHandlerBase implements SolrCoreAw
 
   private AtomicBoolean replicationEnabled = new AtomicBoolean(true);
 
+  private Integer pollInterval;
+
+  private String pollIntervalStr;
+
+  /**
+   * Disable the timer task for polling
+   */
+  private AtomicBoolean pollDisabled = new AtomicBoolean(false);
+
+  String getPollInterval() {
+    return pollIntervalStr;
+  }
+
   @Override
   public void handleRequestBody(SolrQueryRequest req, SolrQueryResponse rsp) throws Exception {
     rsp.setHttpCaching(false);
@@ -221,38 +244,38 @@ public class ReplicationHandler extends RequestHandlerBase implements SolrCoreAw
         return;
       }
       final SolrParams paramsCopy = new ModifiableSolrParams(solrParams);
-      Thread puller = new Thread("explicit-fetchindex-cmd") {
+      Thread fetchThread = new Thread("explicit-fetchindex-cmd") {
         @Override
         public void run() {
           doFetch(paramsCopy, false);
         }
       };
-      puller.setDaemon(false);
-      puller.start();
+      fetchThread.setDaemon(false);
+      fetchThread.start();
       if (solrParams.getBool(WAIT, false)) {
-        puller.join();
+        fetchThread.join();
       }
       rsp.add(STATUS, OK_STATUS);
     } else if (command.equalsIgnoreCase(CMD_DISABLE_POLL)) {
-      if (snapPuller != null){
-        snapPuller.disablePoll();
+      if (pollingIndexFetcher != null){
+        disablePoll();
         rsp.add(STATUS, OK_STATUS);
       } else {
         rsp.add(STATUS, ERR_STATUS);
         rsp.add("message","No slave configured");
       }
     } else if (command.equalsIgnoreCase(CMD_ENABLE_POLL)) {
-      if (snapPuller != null){
-        snapPuller.enablePoll();
+      if (pollingIndexFetcher != null){
+        enablePoll();
         rsp.add(STATUS, OK_STATUS);
       }else {
         rsp.add(STATUS,ERR_STATUS);
         rsp.add("message","No slave configured");
       }
     } else if (command.equalsIgnoreCase(CMD_ABORT_FETCH)) {
-      SnapPuller temp = tempSnapPuller;
-      if (temp != null){
-        temp.abortPull();
+      IndexFetcher fetcher = currentIndexFetcher;
+      if (fetcher != null){
+        fetcher.abortFetch();
         rsp.add(STATUS, OK_STATUS);
       } else {
         rsp.add(STATUS,ERR_STATUS);
@@ -321,38 +344,35 @@ public class ReplicationHandler extends RequestHandlerBase implements SolrCoreAw
     return null;
   }
 
-  private volatile SnapPuller tempSnapPuller;
+  private volatile IndexFetcher currentIndexFetcher;
 
   public boolean doFetch(SolrParams solrParams, boolean forceReplication) {
     String masterUrl = solrParams == null ? null : solrParams.get(MASTER_URL);
-    if (!snapPullLock.tryLock())
+    if (!indexFetchLock.tryLock())
       return false;
     try {
       if (masterUrl != null) {
-        if (tempSnapPuller != null && tempSnapPuller != snapPuller) {
-          tempSnapPuller.destroy();
+        if (currentIndexFetcher != null && currentIndexFetcher != pollingIndexFetcher) {
+          currentIndexFetcher.destroy();
         }
-        
-        NamedList<Object> nl = solrParams.toNamedList();
-        nl.remove(SnapPuller.POLL_INTERVAL);
-        tempSnapPuller = new SnapPuller(nl, this, core);
+        currentIndexFetcher = new IndexFetcher(solrParams.toNamedList(), this, core);
       } else {
-        tempSnapPuller = snapPuller;
+        currentIndexFetcher = pollingIndexFetcher;
       }
-      return tempSnapPuller.fetchLatestIndex(core, forceReplication);
+      return currentIndexFetcher.fetchLatestIndex(core, forceReplication);
     } catch (Exception e) {
-      SolrException.log(LOG, "SnapPull failed ", e);
+      SolrException.log(LOG, "Index fetch failed ", e);
     } finally {
-      if (snapPuller != null) {
-        tempSnapPuller = snapPuller;
+      if (pollingIndexFetcher != null) {
+        currentIndexFetcher = pollingIndexFetcher;
       }
-      snapPullLock.unlock();
+      indexFetchLock.unlock();
     }
     return false;
   }
 
   boolean isReplicating() {
-    return snapPullLock.isLocked();
+    return indexFetchLock.isLocked();
   }
 
   private void doSnapShoot(SolrParams params, SolrQueryResponse rsp,
@@ -390,10 +410,10 @@ public class ReplicationHandler extends RequestHandlerBase implements SolrCoreAw
 
   /**
    * This method adds an Object of FileStream to the response . The FileStream implements a custom protocol which is
-   * understood by SnapPuller.FileFetcher
+   * understood by IndexFetcher.FileFetcher
    *
-   * @see org.apache.solr.handler.SnapPuller.LocalFsFileFetcher
-   * @see org.apache.solr.handler.SnapPuller.DirectoryFileFetcher
+   * @see IndexFetcher.LocalFsFileFetcher
+   * @see IndexFetcher.DirectoryFileFetcher
    */
   private void getFileStream(SolrParams solrParams, SolrQueryResponse rsp) {
     ModifiableSolrParams rawParams = new ModifiableSolrParams(solrParams);
@@ -538,18 +558,28 @@ public class ReplicationHandler extends RequestHandlerBase implements SolrCoreAw
   }
 
   void disablePoll() {
-    if (isSlave)
-      snapPuller.disablePoll();
+    if (isSlave) {
+      pollDisabled.set(true);
+      LOG.info("inside disable poll, value of pollDisabled = " + pollDisabled);
+    }
   }
 
   void enablePoll() {
-    if (isSlave)
-      snapPuller.enablePoll();
+    if (isSlave) {
+      pollDisabled.set(false);
+      LOG.info("inside enable poll, value of pollDisabled = " + pollDisabled);
+    }
   }
 
   boolean isPollingDisabled() {
-    if (snapPuller == null) return true;
-    return snapPuller.isPollingDisabled();
+    return pollDisabled.get();
+  }
+
+  Long getNextScheduledExecTime() {
+    Long nextTime = null;
+    if (executorStartTime > 0)
+      nextTime = executorStartTime + pollInterval;
+    return nextTime;
   }
 
   int getTimesReplicatedSinceStartup() {
@@ -611,31 +641,31 @@ public class ReplicationHandler extends RequestHandlerBase implements SolrCoreAw
       list.add("isMaster", String.valueOf(isMaster));
       list.add("isSlave", String.valueOf(isSlave));
 
-      SnapPuller snapPuller = tempSnapPuller;
-      if (snapPuller != null) {
-        list.add(MASTER_URL, snapPuller.getMasterUrl());
-        if (snapPuller.getPollInterval() != null) {
-          list.add(SnapPuller.POLL_INTERVAL, snapPuller.getPollInterval());
+      IndexFetcher fetcher = currentIndexFetcher;
+      if (fetcher != null) {
+        list.add(MASTER_URL, fetcher.getMasterUrl());
+        if (getPollInterval() != null) {
+          list.add(POLL_INTERVAL, getPollInterval());
         }
         list.add("isPollingDisabled", String.valueOf(isPollingDisabled()));
         list.add("isReplicating", String.valueOf(isReplicating()));
-        long elapsed = getTimeElapsed(snapPuller);
-        long val = SnapPuller.getTotalBytesDownloaded(snapPuller);
+        long elapsed = fetcher.getReplicationTimeElapsed();
+        long val = fetcher.getTotalBytesDownloaded();
         if (elapsed > 0) {
           list.add("timeElapsed", elapsed);
           list.add("bytesDownloaded", val);
           list.add("downloadSpeed", val / elapsed);
         }
         Properties props = loadReplicationProperties();
-        addVal(list, SnapPuller.PREVIOUS_CYCLE_TIME_TAKEN, props, Long.class);
-        addVal(list, SnapPuller.INDEX_REPLICATED_AT, props, Date.class);
-        addVal(list, SnapPuller.CONF_FILES_REPLICATED_AT, props, Date.class);
-        addVal(list, SnapPuller.REPLICATION_FAILED_AT, props, Date.class);
-        addVal(list, SnapPuller.TIMES_FAILED, props, Integer.class);
-        addVal(list, SnapPuller.TIMES_INDEX_REPLICATED, props, Integer.class);
-        addVal(list, SnapPuller.LAST_CYCLE_BYTES_DOWNLOADED, props, Long.class);
-        addVal(list, SnapPuller.TIMES_CONFIG_REPLICATED, props, Integer.class);
-        addVal(list, SnapPuller.CONF_FILES_REPLICATED, props, String.class);
+        addVal(list, IndexFetcher.PREVIOUS_CYCLE_TIME_TAKEN, props, Long.class);
+        addVal(list, IndexFetcher.INDEX_REPLICATED_AT, props, Date.class);
+        addVal(list, IndexFetcher.CONF_FILES_REPLICATED_AT, props, Date.class);
+        addVal(list, IndexFetcher.REPLICATION_FAILED_AT, props, Date.class);
+        addVal(list, IndexFetcher.TIMES_FAILED, props, Integer.class);
+        addVal(list, IndexFetcher.TIMES_INDEX_REPLICATED, props, Integer.class);
+        addVal(list, IndexFetcher.LAST_CYCLE_BYTES_DOWNLOADED, props, Long.class);
+        addVal(list, IndexFetcher.TIMES_CONFIG_REPLICATED, props, Integer.class);
+        addVal(list, IndexFetcher.CONF_FILES_REPLICATED, props, String.class);
       }
       if (isMaster) {
         if (includeConfFiles != null) list.add("confFilesToReplicate", includeConfFiles);
@@ -677,12 +707,12 @@ public class ReplicationHandler extends RequestHandlerBase implements SolrCoreAw
       master.add("replicableGeneration", repCommitInfo.generation);
     }
 
-    SnapPuller snapPuller = tempSnapPuller;
-    if (snapPuller != null) {
+    IndexFetcher fetcher = currentIndexFetcher;
+    if (fetcher != null) {
       Properties props = loadReplicationProperties();
       if (showSlaveDetails) {
         try {
-          NamedList nl = snapPuller.getDetails();
+          NamedList nl = fetcher.getDetails();
           slave.add("masterDetails", nl.get(CMD_DETAILS));
         } catch (Exception e) {
           LOG.warn(
@@ -691,26 +721,26 @@ public class ReplicationHandler extends RequestHandlerBase implements SolrCoreAw
           slave.add(ERR_STATUS, "invalid_master");
         }
       }
-      slave.add(MASTER_URL, snapPuller.getMasterUrl());
-      if (snapPuller.getPollInterval() != null) {
-        slave.add(SnapPuller.POLL_INTERVAL, snapPuller.getPollInterval());
+      slave.add(MASTER_URL, fetcher.getMasterUrl());
+      if (getPollInterval() != null) {
+        slave.add(POLL_INTERVAL, getPollInterval());
       }
-      if (snapPuller.getNextScheduledExecTime() != null && !isPollingDisabled()) {
-        slave.add(NEXT_EXECUTION_AT, new Date(snapPuller.getNextScheduledExecTime()).toString());
+      if (getNextScheduledExecTime() != null && !isPollingDisabled()) {
+        slave.add(NEXT_EXECUTION_AT, new Date(getNextScheduledExecTime()).toString());
       } else if (isPollingDisabled()) {
         slave.add(NEXT_EXECUTION_AT, "Polling disabled");
       }
-      addVal(slave, SnapPuller.INDEX_REPLICATED_AT, props, Date.class);
-      addVal(slave, SnapPuller.INDEX_REPLICATED_AT_LIST, props, List.class);
-      addVal(slave, SnapPuller.REPLICATION_FAILED_AT_LIST, props, List.class);
-      addVal(slave, SnapPuller.TIMES_INDEX_REPLICATED, props, Integer.class);
-      addVal(slave, SnapPuller.CONF_FILES_REPLICATED, props, Integer.class);
-      addVal(slave, SnapPuller.TIMES_CONFIG_REPLICATED, props, Integer.class);
-      addVal(slave, SnapPuller.CONF_FILES_REPLICATED_AT, props, Integer.class);
-      addVal(slave, SnapPuller.LAST_CYCLE_BYTES_DOWNLOADED, props, Long.class);
-      addVal(slave, SnapPuller.TIMES_FAILED, props, Integer.class);
-      addVal(slave, SnapPuller.REPLICATION_FAILED_AT, props, Date.class);
-      addVal(slave, SnapPuller.PREVIOUS_CYCLE_TIME_TAKEN, props, Long.class);
+      addVal(slave, IndexFetcher.INDEX_REPLICATED_AT, props, Date.class);
+      addVal(slave, IndexFetcher.INDEX_REPLICATED_AT_LIST, props, List.class);
+      addVal(slave, IndexFetcher.REPLICATION_FAILED_AT_LIST, props, List.class);
+      addVal(slave, IndexFetcher.TIMES_INDEX_REPLICATED, props, Integer.class);
+      addVal(slave, IndexFetcher.CONF_FILES_REPLICATED, props, Integer.class);
+      addVal(slave, IndexFetcher.TIMES_CONFIG_REPLICATED, props, Integer.class);
+      addVal(slave, IndexFetcher.CONF_FILES_REPLICATED_AT, props, Integer.class);
+      addVal(slave, IndexFetcher.LAST_CYCLE_BYTES_DOWNLOADED, props, Long.class);
+      addVal(slave, IndexFetcher.TIMES_FAILED, props, Integer.class);
+      addVal(slave, IndexFetcher.REPLICATION_FAILED_AT, props, Date.class);
+      addVal(slave, IndexFetcher.PREVIOUS_CYCLE_TIME_TAKEN, props, Long.class);
 
       slave.add("currentDate", new Date().toString());
       slave.add("isPollingDisabled", String.valueOf(isPollingDisabled()));
@@ -720,13 +750,13 @@ public class ReplicationHandler extends RequestHandlerBase implements SolrCoreAw
         try {
           long bytesToDownload = 0;
           List<String> filesToDownload = new ArrayList<>();
-          for (Map<String, Object> file : snapPuller.getFilesToDownload()) {
+          for (Map<String, Object> file : fetcher.getFilesToDownload()) {
             filesToDownload.add((String) file.get(NAME));
             bytesToDownload += (Long) file.get(SIZE);
           }
 
           //get list of conf files to download
-          for (Map<String, Object> file : snapPuller.getConfFilesToDownload()) {
+          for (Map<String, Object> file : fetcher.getConfFilesToDownload()) {
             filesToDownload.add((String) file.get(NAME));
             bytesToDownload += (Long) file.get(SIZE);
           }
@@ -737,18 +767,18 @@ public class ReplicationHandler extends RequestHandlerBase implements SolrCoreAw
 
           long bytesDownloaded = 0;
           List<String> filesDownloaded = new ArrayList<>();
-          for (Map<String, Object> file : snapPuller.getFilesDownloaded()) {
+          for (Map<String, Object> file : fetcher.getFilesDownloaded()) {
             filesDownloaded.add((String) file.get(NAME));
             bytesDownloaded += (Long) file.get(SIZE);
           }
 
           //get list of conf files downloaded
-          for (Map<String, Object> file : snapPuller.getConfFilesDownloaded()) {
+          for (Map<String, Object> file : fetcher.getConfFilesDownloaded()) {
             filesDownloaded.add((String) file.get(NAME));
             bytesDownloaded += (Long) file.get(SIZE);
           }
 
-          Map<String, Object> currentFile = snapPuller.getCurrentFile();
+          Map<String, Object> currentFile = fetcher.getCurrentFile();
           String currFile = null;
           long currFileSize = 0, currFileSizeDownloaded = 0;
           float percentDownloaded = 0;
@@ -767,10 +797,10 @@ public class ReplicationHandler extends RequestHandlerBase implements SolrCoreAw
 
           long estimatedTimeRemaining = 0;
 
-          if (snapPuller.getReplicationStartTime() > 0) {
-            slave.add("replicationStartTime", new Date(snapPuller.getReplicationStartTime()).toString());
+          if (fetcher.getReplicationStartTime() > 0) {
+            slave.add("replicationStartTime", new Date(fetcher.getReplicationStartTime()).toString());
           }
-          long elapsed = getTimeElapsed(snapPuller);
+          long elapsed = fetcher.getReplicationTimeElapsed();
           slave.add("timeElapsed", String.valueOf(elapsed) + "s");
 
           if (bytesDownloaded > 0)
@@ -840,13 +870,6 @@ public class ReplicationHandler extends RequestHandlerBase implements SolrCoreAw
     return replicateAfter;
   }
 
-  private long getTimeElapsed(SnapPuller snapPuller) {
-    long timeElapsed = 0;
-    if (snapPuller.getReplicationStartTime() > 0)
-      timeElapsed = TimeUnit.SECONDS.convert(System.currentTimeMillis() - snapPuller.getReplicationStartTime(), TimeUnit.MILLISECONDS);
-    return timeElapsed;
-  }
-
   Properties loadReplicationProperties() {
     Directory dir = null;
     try {
@@ -856,7 +879,7 @@ public class ReplicationHandler extends RequestHandlerBase implements SolrCoreAw
         IndexInput input;
         try {
           input = dir.openInput(
-            SnapPuller.REPLICATION_PROPERTIES, IOContext.DEFAULT);
+            IndexFetcher.REPLICATION_PROPERTIES, IOContext.DEFAULT);
         } catch (FileNotFoundException | NoSuchFileException e) {
           return new Properties();
         }
@@ -887,6 +910,37 @@ public class ReplicationHandler extends RequestHandlerBase implements SolrCoreAw
 //    }
 //  }
 
+  private void setupPolling(String intervalStr) {
+    pollIntervalStr = intervalStr;
+    pollInterval = readInterval(pollIntervalStr);
+    if (pollInterval == null || pollInterval <= 0) {
+      LOG.info(" No value set for 'pollInterval'. Timer Task not started.");
+      return;
+    }
+
+    Runnable task = new Runnable() {
+      @Override
+      public void run() {
+        if (pollDisabled.get()) {
+          LOG.info("Poll disabled");
+          return;
+        }
+        try {
+          LOG.debug("Polling for index modifications");
+          executorStartTime = System.currentTimeMillis();
+          doFetch(null, false);
+        } catch (Exception e) {
+          LOG.error("Exception in fetching index", e);
+        }
+      }
+    };
+    executorService = Executors.newSingleThreadScheduledExecutor(
+        new DefaultSolrThreadFactory("indexFetcher"));
+    long initialDelay = pollInterval - (System.currentTimeMillis() % pollInterval);
+    executorService.scheduleAtFixedRate(task, initialDelay, pollInterval, TimeUnit.MILLISECONDS);
+    LOG.info("Poll Scheduled at an interval of " + pollInterval + "ms");
+  }
+
   @Override
   @SuppressWarnings("unchecked")
   public void inform(SolrCore core) {
@@ -901,7 +955,8 @@ public class ReplicationHandler extends RequestHandlerBase implements SolrCoreAw
     NamedList slave = (NamedList) initArgs.get("slave");
     boolean enableSlave = isEnabled( slave );
     if (enableSlave) {
-      tempSnapPuller = snapPuller = new SnapPuller(slave, this, core);
+      currentIndexFetcher = pollingIndexFetcher = new IndexFetcher(slave, this, core);
+      setupPolling((String) slave.get(POLL_INTERVAL));
       isSlave = true;
     }
     NamedList master = (NamedList) initArgs.get("master");
@@ -1005,7 +1060,7 @@ public class ReplicationHandler extends RequestHandlerBase implements SolrCoreAw
       }
       String reserve = (String) master.get(RESERVE);
       if (reserve != null && !reserve.trim().equals("")) {
-        reserveCommitDuration = SnapPuller.readInterval(reserve);
+        reserveCommitDuration = readInterval(reserve);
       }
       LOG.info("Commits will be reserved for  " + reserveCommitDuration);
       isMaster = true;
@@ -1029,11 +1084,20 @@ public class ReplicationHandler extends RequestHandlerBase implements SolrCoreAw
     core.addCloseHook(new CloseHook() {
       @Override
       public void preClose(SolrCore core) {
-        if (snapPuller != null) {
-          snapPuller.destroy();
+        try {
+          if (executorService != null) executorService.shutdown();
+        } finally {
+          try {
+            if (pollingIndexFetcher != null) {
+              pollingIndexFetcher.destroy();
+            }
+          } finally {
+            if (executorService != null) ExecutorUtil
+                .shutdownNowAndAwaitTermination(executorService);
+          }
         }
-        if (tempSnapPuller != null && tempSnapPuller != snapPuller) {
-          tempSnapPuller.destroy();
+        if (currentIndexFetcher != null && currentIndexFetcher != pollingIndexFetcher) {
+          currentIndexFetcher.destroy();
         }
       }
 
@@ -1307,8 +1371,40 @@ public class ReplicationHandler extends RequestHandlerBase implements SolrCoreAw
         releaseCommitPointAndExtendReserve();
       }
     }
-  } 
-  
+  }
+
+  static Integer readInterval(String interval) {
+    if (interval == null)
+      return null;
+    int result = 0;
+    if (interval != null) {
+      Matcher m = INTERVAL_PATTERN.matcher(interval.trim());
+      if (m.find()) {
+        String hr = m.group(1);
+        String min = m.group(2);
+        String sec = m.group(3);
+        result = 0;
+        try {
+          if (sec != null && sec.length() > 0)
+            result += Integer.parseInt(sec);
+          if (min != null && min.length() > 0)
+            result += (60 * Integer.parseInt(min));
+          if (hr != null && hr.length() > 0)
+            result += (60 * 60 * Integer.parseInt(hr));
+          result *= 1000;
+        } catch (NumberFormatException e) {
+          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,
+              INTERVAL_ERR_MSG);
+        }
+      } else {
+        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,
+            INTERVAL_ERR_MSG);
+      }
+
+    }
+    return result;
+  }
+
   public static final String MASTER_URL = "masterUrl";
 
   public static final String STATUS = "status";
@@ -1369,6 +1465,12 @@ public class ReplicationHandler extends RequestHandlerBase implements SolrCoreAw
 
   public static final String FILE_STREAM = "filestream";
 
+  public static final String POLL_INTERVAL = "pollInterval";
+
+  public static final String INTERVAL_ERR_MSG = "The " + POLL_INTERVAL + " must be in this format 'HH:mm:ss'";
+
+  private static final Pattern INTERVAL_PATTERN = Pattern.compile("(\\d*?):(\\d*?):(\\d*)");
+
   public static final int PACKET_SZ = 1024 * 1024; // 1MB
 
   public static final String RESERVE = "commitReserveDuration";
diff --git a/solr/core/src/java/org/apache/solr/handler/SnapPuller.java b/solr/core/src/java/org/apache/solr/handler/SnapPuller.java
deleted file mode 100644
index aba696b..0000000
--- a/solr/core/src/java/org/apache/solr/handler/SnapPuller.java
+++ /dev/null
@@ -1,1673 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.solr.handler;
-
-import static org.apache.solr.handler.ReplicationHandler.ALIAS;
-import static org.apache.solr.handler.ReplicationHandler.CHECKSUM;
-import static org.apache.solr.handler.ReplicationHandler.CMD_DETAILS;
-import static org.apache.solr.handler.ReplicationHandler.CMD_GET_FILE;
-import static org.apache.solr.handler.ReplicationHandler.CMD_GET_FILE_LIST;
-import static org.apache.solr.handler.ReplicationHandler.CMD_INDEX_VERSION;
-import static org.apache.solr.handler.ReplicationHandler.COMMAND;
-import static org.apache.solr.handler.ReplicationHandler.COMPRESSION;
-import static org.apache.solr.handler.ReplicationHandler.CONF_FILES;
-import static org.apache.solr.handler.ReplicationHandler.CONF_FILE_SHORT;
-import static org.apache.solr.handler.ReplicationHandler.EXTERNAL;
-import static org.apache.solr.handler.ReplicationHandler.FILE;
-import static org.apache.solr.handler.ReplicationHandler.FILE_STREAM;
-import static org.apache.solr.handler.ReplicationHandler.GENERATION;
-import static org.apache.solr.handler.ReplicationHandler.INTERNAL;
-import static org.apache.solr.handler.ReplicationHandler.MASTER_URL;
-import static org.apache.solr.handler.ReplicationHandler.NAME;
-import static org.apache.solr.handler.ReplicationHandler.OFFSET;
-import static org.apache.solr.handler.ReplicationHandler.SIZE;
-
-import java.io.File;
-import java.io.FileNotFoundException;
-import java.io.FileOutputStream;
-import java.io.IOException;
-import java.io.InputStream;
-import java.io.InputStreamReader;
-import java.io.OutputStreamWriter;
-import java.io.Writer;
-import java.nio.ByteBuffer;
-import java.nio.channels.FileChannel;
-import java.nio.charset.StandardCharsets;
-import java.nio.file.Files;
-import java.nio.file.NoSuchFileException;
-import java.text.SimpleDateFormat;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Collection;
-import java.util.Collections;
-import java.util.Date;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Locale;
-import java.util.Map;
-import java.util.Properties;
-import java.util.Set;
-import java.util.concurrent.CountDownLatch;
-import java.util.concurrent.ExecutionException;
-import java.util.concurrent.ExecutorService;
-import java.util.concurrent.Executors;
-import java.util.concurrent.Future;
-import java.util.concurrent.ScheduledExecutorService;
-import java.util.concurrent.TimeUnit;
-import java.util.concurrent.atomic.AtomicBoolean;
-import java.util.regex.Matcher;
-import java.util.regex.Pattern;
-import java.util.zip.Adler32;
-import java.util.zip.Checksum;
-import java.util.zip.InflaterInputStream;
-
-import org.apache.commons.io.IOUtils;
-import org.apache.http.client.HttpClient;
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.index.IndexCommit;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.SegmentInfos;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.solr.client.solrj.SolrServerException;
-import org.apache.solr.client.solrj.impl.HttpClientUtil;
-import org.apache.solr.client.solrj.impl.HttpSolrClient;
-import org.apache.solr.client.solrj.request.QueryRequest;
-import org.apache.solr.common.SolrException;
-import org.apache.solr.common.SolrException.ErrorCode;
-import org.apache.solr.common.params.CommonParams;
-import org.apache.solr.common.params.ModifiableSolrParams;
-import org.apache.solr.common.util.ExecutorUtil;
-import org.apache.solr.common.util.FastInputStream;
-import org.apache.solr.common.util.NamedList;
-import org.apache.solr.core.DirectoryFactory;
-import org.apache.solr.core.DirectoryFactory.DirContext;
-import org.apache.solr.core.IndexDeletionPolicyWrapper;
-import org.apache.solr.core.SolrCore;
-import org.apache.solr.handler.ReplicationHandler.FileInfo;
-import org.apache.solr.request.LocalSolrQueryRequest;
-import org.apache.solr.request.SolrQueryRequest;
-import org.apache.solr.search.SolrIndexSearcher;
-import org.apache.solr.update.CommitUpdateCommand;
-import org.apache.solr.util.DefaultSolrThreadFactory;
-import org.apache.solr.util.FileUtils;
-import org.apache.solr.util.PropertiesInputStream;
-import org.apache.solr.util.PropertiesOutputStream;
-import org.apache.solr.util.RefCounted;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-/**
- * <p> Provides functionality of downloading changed index files as well as config files and a timer for scheduling fetches from the
- * master. </p>
- *
- *
- * @since solr 1.4
- */
-public class SnapPuller {
-  private static final int _100K = 100000;
-
-  public static final String INDEX_PROPERTIES = "index.properties";
-
-  private static final Logger LOG = LoggerFactory.getLogger(SnapPuller.class.getName());
-
-  private final String masterUrl;
-
-  private final ReplicationHandler replicationHandler;
-
-  private final Integer pollInterval;
-
-  private String pollIntervalStr;
-
-  private ScheduledExecutorService executorService;
-
-  private volatile long executorStartTime;
-
-  private volatile long replicationStartTime;
-
-  private final SolrCore solrCore;
-
-  private volatile List<Map<String, Object>> filesToDownload;
-
-  private volatile List<Map<String, Object>> confFilesToDownload;
-
-  private volatile List<Map<String, Object>> filesDownloaded;
-
-  private volatile List<Map<String, Object>> confFilesDownloaded;
-
-  private volatile Map<String, Object> currentFile;
-
-  private volatile DirectoryFileFetcher dirFileFetcher;
-  
-  private volatile LocalFsFileFetcher localFileFetcher;
-
-  private volatile ExecutorService fsyncService;
-
-  private volatile boolean stop = false;
-
-  private boolean useInternal = false;
-
-  private boolean useExternal = false;
-
-  /**
-   * Disable the timer task for polling
-   */
-  private AtomicBoolean pollDisabled = new AtomicBoolean(false);
-
-  private final HttpClient myHttpClient;
-
-  private static HttpClient createHttpClient(SolrCore core, String connTimeout, String readTimeout, String httpBasicAuthUser, String httpBasicAuthPassword, boolean useCompression) {
-    final ModifiableSolrParams httpClientParams = new ModifiableSolrParams();
-    httpClientParams.set(HttpClientUtil.PROP_CONNECTION_TIMEOUT, connTimeout != null ? connTimeout : "5000");
-    httpClientParams.set(HttpClientUtil.PROP_SO_TIMEOUT, readTimeout != null ? readTimeout : "20000");
-    httpClientParams.set(HttpClientUtil.PROP_BASIC_AUTH_USER, httpBasicAuthUser);
-    httpClientParams.set(HttpClientUtil.PROP_BASIC_AUTH_PASS, httpBasicAuthPassword);
-    httpClientParams.set(HttpClientUtil.PROP_ALLOW_COMPRESSION, useCompression);
-
-    return HttpClientUtil.createClient(httpClientParams, core.getCoreDescriptor().getCoreContainer().getUpdateShardHandler().getConnectionManager());
-  }
-
-  public SnapPuller(final NamedList initArgs, final ReplicationHandler handler, final SolrCore sc) {
-    solrCore = sc;
-    String masterUrl = (String) initArgs.get(MASTER_URL);
-    if (masterUrl == null)
-      throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,
-              "'masterUrl' is required for a slave");
-    if (masterUrl.endsWith("/replication")) {
-      masterUrl = masterUrl.substring(0, masterUrl.length()-12);
-      LOG.warn("'masterUrl' must be specified without the /replication suffix");
-    }
-    this.masterUrl = masterUrl;
-    
-    this.replicationHandler = handler;
-    pollIntervalStr = (String) initArgs.get(POLL_INTERVAL);
-    pollInterval = readInterval(pollIntervalStr);
-    String compress = (String) initArgs.get(COMPRESSION);
-    useInternal = INTERNAL.equals(compress);
-    useExternal = EXTERNAL.equals(compress);
-    String connTimeout = (String) initArgs.get(HttpClientUtil.PROP_CONNECTION_TIMEOUT);
-    String readTimeout = (String) initArgs.get(HttpClientUtil.PROP_SO_TIMEOUT);
-    String httpBasicAuthUser = (String) initArgs.get(HttpClientUtil.PROP_BASIC_AUTH_USER);
-    String httpBasicAuthPassword = (String) initArgs.get(HttpClientUtil.PROP_BASIC_AUTH_PASS);
-    myHttpClient = createHttpClient(solrCore, connTimeout, readTimeout, httpBasicAuthUser, httpBasicAuthPassword, useExternal);
-    if (pollInterval != null && pollInterval > 0) {
-      startExecutorService();
-    } else {
-      LOG.info(" No value set for 'pollInterval'. Timer Task not started.");
-    }
-  }
-
-  private void startExecutorService() {
-    Runnable task = new Runnable() {
-      @Override
-      public void run() {
-        if (pollDisabled.get()) {
-          LOG.info("Poll disabled");
-          return;
-        }
-        try {
-          LOG.debug("Polling for index modifications");
-          executorStartTime = System.currentTimeMillis();
-          replicationHandler.doFetch(null, false);
-        } catch (Exception e) {
-          LOG.error("Exception in fetching index", e);
-        }
-      }
-    };
-    executorService = Executors.newSingleThreadScheduledExecutor(
-        new DefaultSolrThreadFactory("snapPuller"));
-    long initialDelay = pollInterval - (System.currentTimeMillis() % pollInterval);
-    executorService.scheduleAtFixedRate(task, initialDelay, pollInterval, TimeUnit.MILLISECONDS);
-    LOG.info("Poll Scheduled at an interval of " + pollInterval + "ms");
-  }
-
-  /**
-   * Gets the latest commit version and generation from the master
-   */
-  @SuppressWarnings("unchecked")
-  NamedList getLatestVersion() throws IOException {
-    ModifiableSolrParams params = new ModifiableSolrParams();
-    params.set(COMMAND, CMD_INDEX_VERSION);
-    params.set(CommonParams.WT, "javabin");
-    params.set(CommonParams.QT, "/replication");
-    QueryRequest req = new QueryRequest(params);
-
-    // TODO modify to use shardhandler
-    try (HttpSolrClient client = new HttpSolrClient(masterUrl, myHttpClient)) {
-      client.setSoTimeout(60000);
-      client.setConnectionTimeout(15000);
-      
-      return client.request(req);
-    } catch (SolrServerException e) {
-      throw new SolrException(ErrorCode.SERVER_ERROR, e.getMessage(), e);
-    }
-  }
-
-  /**
-   * Fetches the list of files in a given index commit point and updates internal list of files to download.
-   */
-  private void fetchFileList(long gen) throws IOException {
-    ModifiableSolrParams params = new ModifiableSolrParams();
-    params.set(COMMAND,  CMD_GET_FILE_LIST);
-    params.set(GENERATION, String.valueOf(gen));
-    params.set(CommonParams.WT, "javabin");
-    params.set(CommonParams.QT, "/replication");
-    QueryRequest req = new QueryRequest(params);
-
-    // TODO modify to use shardhandler
-    try (HttpSolrClient client = new HttpSolrClient(masterUrl, myHttpClient)) {
-      client.setSoTimeout(60000);
-      client.setConnectionTimeout(15000);
-      NamedList response = client.request(req);
-
-      List<Map<String, Object>> files = (List<Map<String,Object>>) response.get(CMD_GET_FILE_LIST);
-      if (files != null)
-        filesToDownload = Collections.synchronizedList(files);
-      else {
-        filesToDownload = Collections.emptyList();
-        LOG.error("No files to download for index generation: "+ gen);
-      }
-
-      files = (List<Map<String,Object>>) response.get(CONF_FILES);
-      if (files != null)
-        confFilesToDownload = Collections.synchronizedList(files);
-
-    } catch (SolrServerException e) {
-      throw new IOException(e);
-    }
-  }
-
-  private boolean successfulInstall = false;
-
-  /**
-   * This command downloads all the necessary files from master to install a index commit point. Only changed files are
-   * downloaded. It also downloads the conf files (if they are modified).
-   *
-   * @param core the SolrCore
-   * @param forceReplication force a replication in all cases 
-   * @return true on success, false if slave is already in sync
-   * @throws IOException if an exception occurs
-   */
-  boolean fetchLatestIndex(final SolrCore core, boolean forceReplication) throws IOException, InterruptedException {
-    successfulInstall = false;
-    replicationStartTime = System.currentTimeMillis();
-    Directory tmpIndexDir = null;
-    String tmpIndex = null;
-    Directory indexDir = null;
-    String indexDirPath = null;
-    boolean deleteTmpIdxDir = true;
-    try {
-      //get the current 'replicateable' index version in the master
-      NamedList response = null;
-      try {
-        response = getLatestVersion();
-      } catch (Exception e) {
-        LOG.error("Master at: " + masterUrl + " is not available. Index fetch failed. Exception: " + e.getMessage());
-        return false;
-      }
-      long latestVersion = (Long) response.get(CMD_INDEX_VERSION);
-      long latestGeneration = (Long) response.get(GENERATION);
-
-      // TODO: make sure that getLatestCommit only returns commit points for the main index (i.e. no side-car indexes)
-      IndexCommit commit = core.getDeletionPolicy().getLatestCommit();
-      if (commit == null) {
-        // Presumably the IndexWriter hasn't been opened yet, and hence the deletion policy hasn't been updated with commit points
-        RefCounted<SolrIndexSearcher> searcherRefCounted = null;
-        try {
-          searcherRefCounted = core.getNewestSearcher(false);
-          if (searcherRefCounted == null) {
-            LOG.warn("No open searcher found - fetch aborted");
-            return false;
-          }
-          commit = searcherRefCounted.get().getIndexReader().getIndexCommit();
-        } finally {
-          if (searcherRefCounted != null)
-            searcherRefCounted.decref();
-        }
-      }
-
-
-      if (latestVersion == 0L) {
-        if (forceReplication && commit.getGeneration() != 0) {
-          // since we won't get the files for an empty index,
-          // we just clear ours and commit
-          RefCounted<IndexWriter> iw = core.getUpdateHandler().getSolrCoreState().getIndexWriter(core);
-          try {
-            iw.get().deleteAll();
-          } finally {
-            iw.decref();
-          }
-          SolrQueryRequest req = new LocalSolrQueryRequest(core,
-              new ModifiableSolrParams());
-          core.getUpdateHandler().commit(new CommitUpdateCommand(req, false));
-        }
-        
-        //there is nothing to be replicated
-        successfulInstall = true;
-        return true;
-      }
-      
-      if (!forceReplication && IndexDeletionPolicyWrapper.getCommitTimestamp(commit) == latestVersion) {
-        //master and slave are already in sync just return
-        LOG.info("Slave in sync with master.");
-        successfulInstall = true;
-        return true;
-      }
-      LOG.info("Master's generation: " + latestGeneration);
-      LOG.info("Slave's generation: " + commit.getGeneration());
-      LOG.info("Starting replication process");
-      // get the list of files first
-      fetchFileList(latestGeneration);
-      // this can happen if the commit point is deleted before we fetch the file list.
-      if(filesToDownload.isEmpty()) return false;
-      LOG.info("Number of files in latest index in master: " + filesToDownload.size());
-
-      // Create the sync service
-      fsyncService = Executors.newSingleThreadExecutor(new DefaultSolrThreadFactory("fsyncService"));
-      // use a synchronized list because the list is read by other threads (to show details)
-      filesDownloaded = Collections.synchronizedList(new ArrayList<Map<String, Object>>());
-      // if the generation of master is older than that of the slave , it means they are not compatible to be copied
-      // then a new index directory to be created and all the files need to be copied
-      boolean isFullCopyNeeded = IndexDeletionPolicyWrapper
-          .getCommitTimestamp(commit) >= latestVersion
-          || commit.getGeneration() >= latestGeneration || forceReplication;
-
-      String tmpIdxDirName = "index." + new SimpleDateFormat(SnapShooter.DATE_FMT, Locale.ROOT).format(new Date());
-      tmpIndex = createTempindexDir(core, tmpIdxDirName);
-
-      tmpIndexDir = core.getDirectoryFactory().get(tmpIndex, DirContext.DEFAULT, core.getSolrConfig().indexConfig.lockType);
-      
-      // cindex dir...
-      indexDirPath = core.getIndexDir();
-      indexDir = core.getDirectoryFactory().get(indexDirPath, DirContext.DEFAULT, core.getSolrConfig().indexConfig.lockType);
-
-      try {
-        
-        if (isIndexStale(indexDir)) {
-          isFullCopyNeeded = true;
-        }
-        
-        if (!isFullCopyNeeded) {
-          // a searcher might be using some flushed but not committed segments
-          // because of soft commits (which open a searcher on IW's data)
-          // so we need to close the existing searcher on the last commit
-          // and wait until we are able to clean up all unused lucene files
-          if (solrCore.getCoreDescriptor().getCoreContainer().isZooKeeperAware()) {
-            solrCore.closeSearcher();
-          }
-
-          // rollback and reopen index writer and wait until all unused files
-          // are successfully deleted
-          solrCore.getUpdateHandler().newIndexWriter(true);
-          RefCounted<IndexWriter> writer = solrCore.getUpdateHandler().getSolrCoreState().getIndexWriter(null);
-          try {
-            IndexWriter indexWriter = writer.get();
-            int c = 0;
-            indexWriter.deleteUnusedFiles();
-            while (hasUnusedFiles(indexDir, commit)) {
-              indexWriter.deleteUnusedFiles();
-              LOG.info("Sleeping for 1000ms to wait for unused lucene index files to be delete-able");
-              Thread.sleep(1000);
-              c++;
-              if (c >= 30)  {
-                LOG.warn("SnapPuller unable to cleanup unused lucene index files so we must do a full copy instead");
-                isFullCopyNeeded = true;
-                break;
-              }
-            }
-            if (c > 0)  {
-              LOG.info("SnapPuller slept for " + (c * 1000) + "ms for unused lucene index files to be delete-able");
-            }
-          } finally {
-            writer.decref();
-          }
-          solrCore.getUpdateHandler().getSolrCoreState().closeIndexWriter(core, true);
-        }
-        boolean reloadCore = false;
-        
-        try {
-          LOG.info("Starting download to " + tmpIndexDir + " fullCopy="
-              + isFullCopyNeeded);
-          successfulInstall = false;
-          
-          downloadIndexFiles(isFullCopyNeeded, indexDir, tmpIndexDir, latestGeneration);
-          LOG.info("Total time taken for download : "
-              + ((System.currentTimeMillis() - replicationStartTime) / 1000)
-              + " secs");
-          Collection<Map<String,Object>> modifiedConfFiles = getModifiedConfFiles(confFilesToDownload);
-          if (!modifiedConfFiles.isEmpty()) {
-            downloadConfFiles(confFilesToDownload, latestGeneration);
-            if (isFullCopyNeeded) {
-              successfulInstall = modifyIndexProps(tmpIdxDirName);
-              deleteTmpIdxDir = false;
-            } else {
-              successfulInstall = moveIndexFiles(tmpIndexDir, indexDir);
-            }
-            if (successfulInstall) {
-              if (isFullCopyNeeded) {
-                // let the system know we are changing dir's and the old one
-                // may be closed
-                if (indexDir != null) {
-                  LOG.info("removing old index directory " + indexDir);
-                  core.getDirectoryFactory().doneWithDirectory(indexDir);
-                  core.getDirectoryFactory().remove(indexDir);
-                }
-              }
-              
-              LOG.info("Configuration files are modified, core will be reloaded");
-              logReplicationTimeAndConfFiles(modifiedConfFiles,
-                  successfulInstall);// write to a file time of replication and
-                                     // conf files.
-              reloadCore = true;
-            }
-          } else {
-            terminateAndWaitFsyncService();
-            if (isFullCopyNeeded) {
-              successfulInstall = modifyIndexProps(tmpIdxDirName);
-              deleteTmpIdxDir = false;
-            } else {
-              successfulInstall = moveIndexFiles(tmpIndexDir, indexDir);
-            }
-            if (successfulInstall) {
-              logReplicationTimeAndConfFiles(modifiedConfFiles,
-                  successfulInstall);
-            }
-          }
-        } finally {
-          if (!isFullCopyNeeded) {
-            solrCore.getUpdateHandler().getSolrCoreState().openIndexWriter(core);
-          }
-        }
-        
-        // we must reload the core after we open the IW back up
-        if (reloadCore) {
-          reloadCore();
-        }
-
-        if (successfulInstall) {
-          if (isFullCopyNeeded) {
-            // let the system know we are changing dir's and the old one
-            // may be closed
-            if (indexDir != null) {
-              LOG.info("removing old index directory " + indexDir);
-              core.getDirectoryFactory().doneWithDirectory(indexDir);
-              core.getDirectoryFactory().remove(indexDir);
-            }
-          }
-          if (isFullCopyNeeded) {
-            solrCore.getUpdateHandler().newIndexWriter(isFullCopyNeeded);
-          }
-          
-          openNewSearcherAndUpdateCommitPoint();
-        }
-        
-        replicationStartTime = 0;
-        return successfulInstall;
-      } catch (ReplicationHandlerException e) {
-        LOG.error("User aborted Replication");
-        return false;
-      } catch (SolrException e) {
-        throw e;
-      } catch (InterruptedException e) {
-        throw new InterruptedException("Index fetch interrupted");
-      } catch (Exception e) {
-        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR, "Index fetch failed : ", e);
-      }
-    } finally {
-      try {
-        if (!successfulInstall) {
-          try {
-            logReplicationTimeAndConfFiles(null, successfulInstall);
-          } catch(Exception e) {
-            LOG.error("caught", e);
-          }
-        }
-        filesToDownload = filesDownloaded = confFilesDownloaded = confFilesToDownload = null;
-        replicationStartTime = 0;
-        dirFileFetcher = null;
-        localFileFetcher = null;
-        if (fsyncService != null && !fsyncService.isShutdown()) fsyncService
-            .shutdownNow();
-        fsyncService = null;
-        stop = false;
-        fsyncException = null;
-      } finally {
-        if (deleteTmpIdxDir && tmpIndexDir != null) {
-          try {
-            core.getDirectoryFactory().doneWithDirectory(tmpIndexDir);
-            core.getDirectoryFactory().remove(tmpIndexDir);
-          } catch (IOException e) {
-            SolrException.log(LOG, "Error removing directory " + tmpIndexDir, e);
-          }
-        }
-        
-        if (tmpIndexDir != null) {
-          core.getDirectoryFactory().release(tmpIndexDir);
-        }
-        
-        if (indexDir != null) {
-          core.getDirectoryFactory().release(indexDir);
-        }
-      }
-    }
-  }
-
-  private boolean hasUnusedFiles(Directory indexDir, IndexCommit commit) throws IOException {
-    String segmentsFileName = commit.getSegmentsFileName();
-    SegmentInfos infos = SegmentInfos.readCommit(indexDir, segmentsFileName);
-    Set<String> currentFiles = new HashSet<>(infos.files(true));
-    String[] allFiles = indexDir.listAll();
-    for (String file : allFiles) {
-      if (!file.equals(segmentsFileName) && !currentFiles.contains(file) && !file.endsWith(".lock")) {
-        LOG.info("Found unused file: " + file);
-        return true;
-      }
-    }
-    return false;
-  }
-
-  private volatile Exception fsyncException;
-
-  /**
-   * terminate the fsync service and wait for all the tasks to complete. If it is already terminated
-   */
-  private void terminateAndWaitFsyncService() throws Exception {
-    if (fsyncService.isTerminated()) return;
-    fsyncService.shutdown();
-     // give a long wait say 1 hr
-    fsyncService.awaitTermination(3600, TimeUnit.SECONDS);
-    // if any fsync failed, throw that exception back
-    Exception fsyncExceptionCopy = fsyncException;
-    if (fsyncExceptionCopy != null) throw fsyncExceptionCopy;
-  }
-
-  /**
-   * Helper method to record the last replication's details so that we can show them on the statistics page across
-   * restarts.
-   * @throws IOException on IO error
-   */
-  private void logReplicationTimeAndConfFiles(Collection<Map<String, Object>> modifiedConfFiles, boolean successfulInstall) throws IOException {
-    List<String> confFiles = new ArrayList<>();
-    if (modifiedConfFiles != null && !modifiedConfFiles.isEmpty())
-      for (Map<String, Object> map1 : modifiedConfFiles)
-        confFiles.add((String) map1.get(NAME));
-
-    Properties props = replicationHandler.loadReplicationProperties();
-    long replicationTime = System.currentTimeMillis();
-    long replicationTimeTaken = (replicationTime - getReplicationStartTime()) / 1000;
-    Directory dir = null;
-    try {
-      dir = solrCore.getDirectoryFactory().get(solrCore.getDataDir(), DirContext.META_DATA, solrCore.getSolrConfig().indexConfig.lockType);
-      
-      int indexCount = 1, confFilesCount = 1;
-      if (props.containsKey(TIMES_INDEX_REPLICATED)) {
-        indexCount = Integer.valueOf(props.getProperty(TIMES_INDEX_REPLICATED)) + 1;
-      }
-      StringBuilder sb = readToStringBuilder(replicationTime, props.getProperty(INDEX_REPLICATED_AT_LIST));
-      props.setProperty(INDEX_REPLICATED_AT_LIST, sb.toString());
-      props.setProperty(INDEX_REPLICATED_AT, String.valueOf(replicationTime));
-      props.setProperty(PREVIOUS_CYCLE_TIME_TAKEN, String.valueOf(replicationTimeTaken));
-      props.setProperty(TIMES_INDEX_REPLICATED, String.valueOf(indexCount));
-      if (modifiedConfFiles != null && !modifiedConfFiles.isEmpty()) {
-        props.setProperty(CONF_FILES_REPLICATED, confFiles.toString());
-        props.setProperty(CONF_FILES_REPLICATED_AT, String.valueOf(replicationTime));
-        if (props.containsKey(TIMES_CONFIG_REPLICATED)) {
-          confFilesCount = Integer.valueOf(props.getProperty(TIMES_CONFIG_REPLICATED)) + 1;
-        }
-        props.setProperty(TIMES_CONFIG_REPLICATED, String.valueOf(confFilesCount));
-      }
-
-      props.setProperty(LAST_CYCLE_BYTES_DOWNLOADED, String.valueOf(getTotalBytesDownloaded(this)));
-      if (!successfulInstall) {
-        int numFailures = 1;
-        if (props.containsKey(TIMES_FAILED)) {
-          numFailures = Integer.valueOf(props.getProperty(TIMES_FAILED)) + 1;
-        }
-        props.setProperty(TIMES_FAILED, String.valueOf(numFailures));
-        props.setProperty(REPLICATION_FAILED_AT, String.valueOf(replicationTime));
-        sb = readToStringBuilder(replicationTime, props.getProperty(REPLICATION_FAILED_AT_LIST));
-        props.setProperty(REPLICATION_FAILED_AT_LIST, sb.toString());
-      }
-
-      final IndexOutput out = dir.createOutput(REPLICATION_PROPERTIES, DirectoryFactory.IOCONTEXT_NO_CACHE);
-      Writer outFile = new OutputStreamWriter(new PropertiesOutputStream(out), StandardCharsets.UTF_8);
-      try {
-        props.store(outFile, "Replication details");
-        dir.sync(Collections.singleton(REPLICATION_PROPERTIES));
-      } finally {
-        IOUtils.closeQuietly(outFile);
-      }
-    } catch (Exception e) {
-      LOG.warn("Exception while updating statistics", e);
-    } finally {
-      if (dir != null) {
-        solrCore.getDirectoryFactory().release(dir);
-      }
-    }
-  }
-
-  static long getTotalBytesDownloaded(SnapPuller snappuller) {
-    long bytesDownloaded = 0;
-    //get size from list of files to download
-    for (Map<String, Object> file : snappuller.getFilesDownloaded()) {
-      bytesDownloaded += (Long) file.get(SIZE);
-    }
-
-    //get size from list of conf files to download
-    for (Map<String, Object> file : snappuller.getConfFilesDownloaded()) {
-      bytesDownloaded += (Long) file.get(SIZE);
-    }
-
-    //get size from current file being downloaded
-    Map<String, Object> currentFile = snappuller.getCurrentFile();
-    if (currentFile != null) {
-      if (currentFile.containsKey("bytesDownloaded")) {
-        bytesDownloaded += (Long) currentFile.get("bytesDownloaded");
-      }
-    }
-    return bytesDownloaded;
-  }
-
-  private StringBuilder readToStringBuilder(long replicationTime, String str) {
-    StringBuilder sb = new StringBuilder();
-    List<String> l = new ArrayList<>();
-    if (str != null && str.length() != 0) {
-      String[] ss = str.split(",");
-      Collections.addAll(l, ss);
-    }
-    sb.append(replicationTime);
-    if (!l.isEmpty()) {
-      for (int i = 0; i < l.size() || i < 9; i++) {
-        if (i == l.size() || i == 9) break;
-        String s = l.get(i);
-        sb.append(",").append(s);
-      }
-    }
-    return sb;
-  }
-
-  private void openNewSearcherAndUpdateCommitPoint() throws IOException {
-    SolrQueryRequest req = new LocalSolrQueryRequest(solrCore,
-        new ModifiableSolrParams());
-    
-    RefCounted<SolrIndexSearcher> searcher = null;
-    IndexCommit commitPoint;
-    try {
-      Future[] waitSearcher = new Future[1];
-      searcher = solrCore.getSearcher(true, true, waitSearcher, true);
-      if (waitSearcher[0] != null) {
-        try {
-          waitSearcher[0].get();
-        } catch (InterruptedException | ExecutionException e) {
-          SolrException.log(LOG, e);
-        }
-      }
-      commitPoint = searcher.get().getIndexReader().getIndexCommit();
-    } finally {
-      req.close();
-      if (searcher != null) {
-        searcher.decref();
-      }
-    }
-
-    // update the commit point in replication handler
-    replicationHandler.indexCommitPoint = commitPoint;
-    
-  }
-
-  /**
-   * All the files are copied to a temp dir first
-   */
-  private String createTempindexDir(SolrCore core, String tmpIdxDirName) {
-    // TODO: there should probably be a DirectoryFactory#concatPath(parent, name)
-    // or something
-    return core.getDataDir() + tmpIdxDirName;
-  }
-
-  private void reloadCore() {
-    final CountDownLatch latch = new CountDownLatch(1);
-    new Thread() {
-      @Override
-      public void run() {
-        try {
-          solrCore.getCoreDescriptor().getCoreContainer().reload(solrCore.getName());
-        } catch (Exception e) {
-          LOG.error("Could not reload core ", e);
-        } finally {
-          latch.countDown();
-        }
-      }
-    }.start();
-    try {
-      latch.await();
-    } catch (InterruptedException e) {
-      Thread.currentThread().interrupt();
-      throw new RuntimeException("Interrupted while waiting for core reload to finish", e);
-    }
-  }
-
-  private void downloadConfFiles(List<Map<String, Object>> confFilesToDownload, long latestGeneration) throws Exception {
-    LOG.info("Starting download of configuration files from master: " + confFilesToDownload);
-    confFilesDownloaded = Collections.synchronizedList(new ArrayList<>());
-    File tmpconfDir = new File(solrCore.getResourceLoader().getConfigDir(), "conf." + getDateAsStr(new Date()));
-    try {
-      boolean status = tmpconfDir.mkdirs();
-      if (!status) {
-        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,
-                "Failed to create temporary config folder: " + tmpconfDir.getName());
-      }
-      for (Map<String, Object> file : confFilesToDownload) {
-        String saveAs = (String) (file.get(ALIAS) == null ? file.get(NAME) : file.get(ALIAS));
-        localFileFetcher = new LocalFsFileFetcher(tmpconfDir, file, saveAs, true, latestGeneration);
-        currentFile = file;
-        localFileFetcher.fetchFile();
-        confFilesDownloaded.add(new HashMap<>(file));
-      }
-      // this is called before copying the files to the original conf dir
-      // so that if there is an exception avoid corrupting the original files.
-      terminateAndWaitFsyncService();
-      copyTmpConfFiles2Conf(tmpconfDir);
-    } finally {
-      delTree(tmpconfDir);
-    }
-  }
-
-  /**
-   * Download the index files. If a new index is needed, download all the files.
-   *
-   * @param downloadCompleteIndex is it a fresh index copy
-   * @param tmpIndexDir              the directory to which files need to be downloadeed to
-   * @param indexDir                 the indexDir to be merged to
-   * @param latestGeneration         the version number
-   */
-  private void downloadIndexFiles(boolean downloadCompleteIndex, Directory indexDir, Directory tmpIndexDir, long latestGeneration)
-      throws Exception {
-    if (LOG.isDebugEnabled()) {
-      LOG.debug("Download files to dir: " + Arrays.asList(indexDir.listAll()));
-    }
-    for (Map<String,Object> file : filesToDownload) {
-      String filename = (String) file.get(NAME);
-      long size = (Long) file.get(SIZE);
-      CompareResult compareResult = compareFile(indexDir, filename, size, (Long) file.get(CHECKSUM));
-      if (!compareResult.equal || downloadCompleteIndex
-          || filesToAlwaysDownloadIfNoChecksums(filename, size, compareResult)) {
-        dirFileFetcher = new DirectoryFileFetcher(tmpIndexDir, file,
-            (String) file.get(NAME), false, latestGeneration);
-        currentFile = file;
-        dirFileFetcher.fetchFile();
-        filesDownloaded.add(new HashMap<>(file));
-      } else {
-        LOG.info("Skipping download for " + file.get(NAME)
-            + " because it already exists");
-      }
-    }
-  }
-  
-  private boolean filesToAlwaysDownloadIfNoChecksums(String filename,
-      long size, CompareResult compareResult) {
-    // without checksums to compare, we always download .si, .liv, segments_N,
-    // and any very small files
-    return !compareResult.checkSummed && (filename.endsWith(".si") || filename.endsWith(".liv")
-    || filename.startsWith("segments_") || size < _100K);
-  }
-
-  static class CompareResult {
-    boolean equal = false;
-    boolean checkSummed = false;
-  }
-  
-  private CompareResult compareFile(Directory indexDir, String filename, Long backupIndexFileLen, Long backupIndexFileChecksum) {
-    CompareResult compareResult = new CompareResult();
-    try {
-      try (final IndexInput indexInput = indexDir.openInput(filename, IOContext.READONCE)) {
-        long indexFileLen = indexInput.length();
-        long indexFileChecksum = 0;
-        
-        if (backupIndexFileChecksum != null) {
-          try {
-            indexFileChecksum = CodecUtil.retrieveChecksum(indexInput);
-            compareResult.checkSummed = true;
-          } catch (Exception e) {
-            LOG.warn("Could not retrieve checksum from file.", e);
-          }
-        }
-        
-        if (!compareResult.checkSummed) {
-          // we don't have checksums to compare
-          
-          if (indexFileLen == backupIndexFileLen) {
-            compareResult.equal = true;
-            return compareResult;
-          } else {
-            LOG.warn(
-                "File {} did not match. expected length is {} and actual length is {}", filename, backupIndexFileLen, indexFileLen);
-            compareResult.equal = false;
-            return compareResult;
-          }
-        }
-        
-        // we have checksums to compare
-        
-        if (indexFileLen == backupIndexFileLen && indexFileChecksum == backupIndexFileChecksum) {
-          compareResult.equal = true;
-          return compareResult;
-        } else {
-          LOG.warn("File {} did not match. expected checksum is {} and actual is checksum {}. " +
-              "expected length is {} and actual length is {}", filename, backupIndexFileChecksum, indexFileChecksum,
-              backupIndexFileLen, indexFileLen);
-          compareResult.equal = false;
-          return compareResult;
-        }
-      }
-    } catch (NoSuchFileException | FileNotFoundException e) {
-      compareResult.equal = false;
-      return compareResult;
-    } catch (IOException e) {
-      LOG.error("Could not read file " + filename + ". Downloading it again", e);
-      compareResult.equal = false;
-      return compareResult;
-    }
-  }
-
-  /** Returns true if the file exists (can be opened), false
-   *  if it cannot be opened, and (unlike Java's
-   *  File.exists) throws IOException if there's some
-   *  unexpected error. */
-  private static boolean slowFileExists(Directory dir, String fileName) throws IOException {
-    try {
-      dir.openInput(fileName, IOContext.DEFAULT).close();
-      return true;
-    } catch (NoSuchFileException | FileNotFoundException e) {
-      return false;
-    }
-  }  
-
-  /**
-   * All the files which are common between master and slave must have same size else we assume they are
-   * not compatible (stale).
-   *
-   * @return true if the index stale and we need to download a fresh copy, false otherwise.
-   * @throws IOException  if low level io error
-   */
-  private boolean isIndexStale(Directory dir) throws IOException {
-    for (Map<String, Object> file : filesToDownload) {
-      String filename = (String) file.get(NAME);
-      Long length = (Long) file.get(SIZE);
-      Long checksum = (Long) file.get(CHECKSUM);
-      if (slowFileExists(dir, filename)) {
-        if (checksum != null) {
-          if (!(compareFile(dir, filename, length, checksum).equal)) {
-            // file exists and size or checksum is different, therefore we must download it again
-            return true;
-          }
-        } else {
-          if (length != dir.fileLength(filename)) {
-            LOG.warn("File {} did not match. expected length is {} and actual length is {}",
-                filename, length, dir.fileLength(filename));
-            return true;
-          }
-        }
-      }
-    }
-    return false;
-  }
-
-  /**
-   * Copy a file by the File#renameTo() method. If it fails, it is considered a failure
-   * <p/>
-   */
-  private boolean moveAFile(Directory tmpIdxDir, Directory indexDir, String fname) {
-    LOG.debug("Moving file: {}", fname);
-    boolean success = false;
-    try {
-      if (slowFileExists(indexDir, fname)) {
-        LOG.info("Skipping move file - it already exists:" + fname);
-        return true;
-      }
-    } catch (IOException e) {
-      SolrException.log(LOG, "could not check if a file exists", e);
-      return false;
-    }
-    try {
-      solrCore.getDirectoryFactory().move(tmpIdxDir, indexDir, fname, DirectoryFactory.IOCONTEXT_NO_CACHE);
-      success = true;
-    } catch (IOException e) {
-      SolrException.log(LOG, "Could not move file", e);
-    }
-    return success;
-  }
-
-  /**
-   * Copy all index files from the temp index dir to the actual index. The segments_N file is copied last.
-   */
-  private boolean moveIndexFiles(Directory tmpIdxDir, Directory indexDir) {
-    if (LOG.isDebugEnabled()) {
-      try {
-        LOG.info("From dir files:" + Arrays.asList(tmpIdxDir.listAll()));
-        LOG.info("To dir files:" + Arrays.asList(indexDir.listAll()));
-      } catch (IOException e) {
-        throw new RuntimeException(e);
-      }
-    }
-    String segmentsFile = null;
-    for (Map<String, Object> f : filesDownloaded) {
-      String fname = (String) f.get(NAME);
-      // the segments file must be copied last
-      // or else if there is a failure in between the
-      // index will be corrupted
-      if (fname.startsWith("segments_")) {
-        //The segments file must be copied in the end
-        //Otherwise , if the copy fails index ends up corrupted
-        segmentsFile = fname;
-        continue;
-      }
-      if (!moveAFile(tmpIdxDir, indexDir, fname)) return false;
-    }
-    //copy the segments file last
-    if (segmentsFile != null) {
-      if (!moveAFile(tmpIdxDir, indexDir, segmentsFile)) return false;
-    }
-    return true;
-  }
-
-  /**
-   * Make file list 
-   */
-  private List<File> makeTmpConfDirFileList(File dir, List<File> fileList) {
-    File[] files = dir.listFiles();
-    for (File file : files) {
-      if (file.isFile()) {
-        fileList.add(file);
-      } else if (file.isDirectory()) {
-        fileList = makeTmpConfDirFileList(file, fileList);
-      }
-    }
-    return fileList;
-  }
-  
-  /**
-   * The conf files are copied to the tmp dir to the conf dir. A backup of the old file is maintained
-   */
-  private void copyTmpConfFiles2Conf(File tmpconfDir) {
-    boolean status = false;
-    File confDir = new File(solrCore.getResourceLoader().getConfigDir());
-    for (File file : makeTmpConfDirFileList(tmpconfDir, new ArrayList<>())) {
-      File oldFile = new File(confDir, file.getPath().substring(tmpconfDir.getPath().length(), file.getPath().length()));
-      if (!oldFile.getParentFile().exists()) {
-        status = oldFile.getParentFile().mkdirs();
-        if (!status) {
-          throw new SolrException(ErrorCode.SERVER_ERROR,
-                  "Unable to mkdirs: " + oldFile.getParentFile());
-        }
-      }
-      if (oldFile.exists()) {
-        File backupFile = new File(oldFile.getPath() + "." + getDateAsStr(new Date(oldFile.lastModified())));
-        if (!backupFile.getParentFile().exists()) {
-          status = backupFile.getParentFile().mkdirs();
-          if (!status) {
-            throw new SolrException(ErrorCode.SERVER_ERROR,
-                    "Unable to mkdirs: " + backupFile.getParentFile());
-          }
-        }
-        status = oldFile.renameTo(backupFile);
-        if (!status) {
-          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,
-                  "Unable to rename: " + oldFile + " to: " + backupFile);
-        }
-      }
-      status = file.renameTo(oldFile);
-      if (!status) {
-        throw new SolrException(ErrorCode.SERVER_ERROR,
-                "Unable to rename: " + file + " to: " + oldFile);
-      }
-    }
-  }
-
-  private String getDateAsStr(Date d) {
-    return new SimpleDateFormat(SnapShooter.DATE_FMT, Locale.ROOT).format(d);
-  }
-
-  /**
-   * If the index is stale by any chance, load index from a different dir in the data dir.
-   */
-  private boolean modifyIndexProps(String tmpIdxDirName) {
-    LOG.info("New index installed. Updating index properties... index="+tmpIdxDirName);
-    Properties p = new Properties();
-    Directory dir = null;
-    try {
-      dir = solrCore.getDirectoryFactory().get(solrCore.getDataDir(), DirContext.META_DATA, solrCore.getSolrConfig().indexConfig.lockType);
-      if (slowFileExists(dir, SnapPuller.INDEX_PROPERTIES)){
-        final IndexInput input = dir.openInput(SnapPuller.INDEX_PROPERTIES, DirectoryFactory.IOCONTEXT_NO_CACHE);
-  
-        final InputStream is = new PropertiesInputStream(input);
-        try {
-          p.load(new InputStreamReader(is, StandardCharsets.UTF_8));
-        } catch (Exception e) {
-          LOG.error("Unable to load " + SnapPuller.INDEX_PROPERTIES, e);
-        } finally {
-          IOUtils.closeQuietly(is);
-        }
-      }
-      try {
-        dir.deleteFile(SnapPuller.INDEX_PROPERTIES);
-      } catch (IOException e) {
-        // no problem
-      }
-      final IndexOutput out = dir.createOutput(SnapPuller.INDEX_PROPERTIES, DirectoryFactory.IOCONTEXT_NO_CACHE);
-      p.put("index", tmpIdxDirName);
-      Writer os = null;
-      try {
-        os = new OutputStreamWriter(new PropertiesOutputStream(out), StandardCharsets.UTF_8);
-        p.store(os, SnapPuller.INDEX_PROPERTIES);
-        dir.sync(Collections.singleton(INDEX_PROPERTIES));
-      } catch (Exception e) {
-        throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,
-            "Unable to write " + SnapPuller.INDEX_PROPERTIES, e);
-      } finally {
-        IOUtils.closeQuietly(os);
-      }
-      return true;
-
-    } catch (IOException e1) {
-      throw new RuntimeException(e1);
-    } finally {
-      if (dir != null) {
-        try {
-          solrCore.getDirectoryFactory().release(dir);
-        } catch (IOException e) {
-          SolrException.log(LOG, "", e);
-        }
-      }
-    }
-    
-  }
-
-  private final Map<String, FileInfo> confFileInfoCache = new HashMap<>();
-
-  /**
-   * The local conf files are compared with the conf files in the master. If they are same (by checksum) do not copy.
-   *
-   * @param confFilesToDownload The list of files obtained from master
-   *
-   * @return a list of configuration files which have changed on the master and need to be downloaded.
-   */
-  private Collection<Map<String, Object>> getModifiedConfFiles(List<Map<String, Object>> confFilesToDownload) {
-    if (confFilesToDownload == null || confFilesToDownload.isEmpty())
-      return Collections.EMPTY_LIST;
-    //build a map with alias/name as the key
-    Map<String, Map<String, Object>> nameVsFile = new HashMap<>();
-    NamedList names = new NamedList();
-    for (Map<String, Object> map : confFilesToDownload) {
-      //if alias is present that is the name the file may have in the slave
-      String name = (String) (map.get(ALIAS) == null ? map.get(NAME) : map.get(ALIAS));
-      nameVsFile.put(name, map);
-      names.add(name, null);
-    }
-    //get the details of the local conf files with the same alias/name
-    List<Map<String, Object>> localFilesInfo = replicationHandler.getConfFileInfoFromCache(names, confFileInfoCache);
-    //compare their size/checksum to see if
-    for (Map<String, Object> fileInfo : localFilesInfo) {
-      String name = (String) fileInfo.get(NAME);
-      Map<String, Object> m = nameVsFile.get(name);
-      if (m == null) continue; // the file is not even present locally (so must be downloaded)
-      if (m.get(CHECKSUM).equals(fileInfo.get(CHECKSUM))) {
-        nameVsFile.remove(name); //checksums are same so the file need not be downloaded
-      }
-    }
-    return nameVsFile.isEmpty() ? Collections.EMPTY_LIST : nameVsFile.values();
-  }
-  
-  /** 
-   * This simulates File.delete exception-wise, since this class has some strange behavior with it.
-   * The only difference is it returns null on success, throws SecurityException on SecurityException, 
-   * otherwise returns Throwable preventing deletion (instead of false), for additional information.
-   */
-  static Throwable delete(File file) {
-    try {
-      Files.delete(file.toPath());
-      return null;
-    } catch (SecurityException e) {
-      throw e;
-    } catch (Throwable other) {
-      return other;
-    }
-  }
-  
-  static boolean delTree(File dir) {
-    try {
-      org.apache.lucene.util.IOUtils.rm(dir.toPath());
-      return true;
-    } catch (IOException e) {
-      LOG.warn("Unable to delete directory : " + dir, e);
-      return false;
-    }
-  }
-
-  /**
-   * Disable periodic polling
-   */
-  void disablePoll() {
-    pollDisabled.set(true);
-    LOG.info("inside disable poll, value of pollDisabled = " + pollDisabled);
-  }
-
-  /**
-   * Enable periodic polling
-   */
-  void enablePoll() {
-    pollDisabled.set(false);
-    LOG.info("inside enable poll, value of pollDisabled = " + pollDisabled);
-  }
-
-  /**
-   * Stops the ongoing pull
-   */
-  void abortPull() {
-    stop = true;
-  }
-
-  long getReplicationStartTime() {
-    return replicationStartTime;
-  }
-
-  List<Map<String, Object>> getConfFilesToDownload() {
-    //make a copy first because it can be null later
-    List<Map<String, Object>> tmp = confFilesToDownload;
-    //create a new instance. or else iterator may fail
-    return tmp == null ? Collections.EMPTY_LIST : new ArrayList<>(tmp);
-  }
-
-  List<Map<String, Object>> getConfFilesDownloaded() {
-    //make a copy first because it can be null later
-    List<Map<String, Object>> tmp = confFilesDownloaded;
-    // NOTE: it's safe to make a copy of a SynchronizedCollection(ArrayList)
-    return tmp == null ? Collections.EMPTY_LIST : new ArrayList<>(tmp);
-  }
-
-  List<Map<String, Object>> getFilesToDownload() {
-    //make a copy first because it can be null later
-    List<Map<String, Object>> tmp = filesToDownload;
-    return tmp == null ? Collections.EMPTY_LIST : new ArrayList<>(tmp);
-  }
-
-  List<Map<String, Object>> getFilesDownloaded() {
-    List<Map<String, Object>> tmp = filesDownloaded;
-    return tmp == null ? Collections.EMPTY_LIST : new ArrayList<>(tmp);
-  }
-
-  // TODO: currently does not reflect conf files
-  Map<String, Object> getCurrentFile() {
-    Map<String, Object> tmp = currentFile;
-    DirectoryFileFetcher tmpFileFetcher = dirFileFetcher;
-    if (tmp == null)
-      return null;
-    tmp = new HashMap<>(tmp);
-    if (tmpFileFetcher != null)
-      tmp.put("bytesDownloaded", tmpFileFetcher.getBytesDownloaded());
-    return tmp;
-  }
-
-  boolean isPollingDisabled() {
-    return pollDisabled.get();
-  }
-
-  Long getNextScheduledExecTime() {
-    Long nextTime = null;
-    if (executorStartTime > 0)
-      nextTime = executorStartTime + pollInterval;
-    return nextTime;
-  }
-
-  private static class ReplicationHandlerException extends InterruptedException {
-    public ReplicationHandlerException(String message) {
-      super(message);
-    }
-  }
-
-  private interface FileInterface {
-    public void sync() throws IOException;
-    public void write(byte[] buf, int packetSize) throws IOException;
-    public void close() throws Exception;
-    public void delete() throws Exception;
-  }
-
-  /**
-   * The class acts as a client for ReplicationHandler.FileStream. It understands the protocol of wt=filestream
-   *
-   * @see org.apache.solr.handler.ReplicationHandler.DirectoryFileStream
-   */
-  private class FileFetcher {
-    private final FileInterface file;
-    private boolean includeChecksum = true;
-    private String fileName;
-    private String saveAs;
-    private boolean isConf;
-    private Long indexGen;
-
-    private long size;
-    private long bytesDownloaded = 0;
-    private byte[] buf = new byte[1024 * 1024];
-    private Checksum checksum;
-    private int errorCount = 0;
-    private boolean aborted = false;
-
-    FileFetcher(FileInterface file, Map<String, Object> fileDetails, String saveAs,
-                boolean isConf, long latestGen) throws IOException {
-      this.file = file;
-      this.fileName = (String) fileDetails.get(NAME);
-      this.size = (Long) fileDetails.get(SIZE);
-      this.isConf = isConf;
-      this.saveAs = saveAs;
-      indexGen = latestGen;
-      if (includeChecksum)
-        checksum = new Adler32();
-    }
-
-    public long getBytesDownloaded() {
-      return bytesDownloaded;
-    }
-
-    /**
-     * The main method which downloads file
-     */
-    public void fetchFile() throws Exception {
-      try {
-        while (true) {
-          final FastInputStream is = getStream();
-          int result;
-          try {
-            //fetch packets one by one in a single request
-            result = fetchPackets(is);
-            if (result == 0 || result == NO_CONTENT) {
-
-              return;
-            }
-            //if there is an error continue. But continue from the point where it got broken
-          } finally {
-            IOUtils.closeQuietly(is);
-          }
-        }
-      } finally {
-        cleanup();
-        //if cleanup succeeds . The file is downloaded fully. do an fsync
-        fsyncService.submit(new Runnable(){
-          @Override
-          public void run() {
-            try {
-              file.sync();
-            } catch (IOException e) {
-              fsyncException = e;
-            }
-          }
-        });
-      }
-    }
-
-    private int fetchPackets(FastInputStream fis) throws Exception {
-      byte[] intbytes = new byte[4];
-      byte[] longbytes = new byte[8];
-      try {
-        while (true) {
-          if (stop) {
-            stop = false;
-            aborted = true;
-            throw new ReplicationHandlerException("User aborted replication");
-          }
-          long checkSumServer = -1;
-          fis.readFully(intbytes);
-          //read the size of the packet
-          int packetSize = readInt(intbytes);
-          if (packetSize <= 0) {
-            LOG.warn("No content received for file: {}", fileName);
-            return NO_CONTENT;
-          }
-          if (buf.length < packetSize)
-            buf = new byte[packetSize];
-          if (checksum != null) {
-            //read the checksum
-            fis.readFully(longbytes);
-            checkSumServer = readLong(longbytes);
-          }
-          //then read the packet of bytes
-          fis.readFully(buf, 0, packetSize);
-          //compare the checksum as sent from the master
-          if (includeChecksum) {
-            checksum.reset();
-            checksum.update(buf, 0, packetSize);
-            long checkSumClient = checksum.getValue();
-            if (checkSumClient != checkSumServer) {
-              LOG.error("Checksum not matched between client and server for file: {}", fileName);
-              //if checksum is wrong it is a problem return for retry
-              return 1;
-            }
-          }
-          //if everything is fine, write down the packet to the file
-          file.write(buf, packetSize);
-          bytesDownloaded += packetSize;
-          LOG.debug("Fetched and wrote {} bytes of file: {}", bytesDownloaded, fileName);
-          if (bytesDownloaded >= size)
-            return 0;
-          //errorCount is always set to zero after a successful packet
-          errorCount = 0;
-        }
-      } catch (ReplicationHandlerException e) {
-        throw e;
-      } catch (Exception e) {
-        LOG.warn("Error in fetching file: {} (downloaded {} of {} bytes)",
-            fileName, bytesDownloaded, size, e);
-        //for any failure, increment the error count
-        errorCount++;
-        //if it fails for the same packet for MAX_RETRIES fail and come out
-        if (errorCount > MAX_RETRIES) {
-          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,
-              "Failed to fetch file: " + fileName +
-                  " (downloaded " + bytesDownloaded + " of " + size + " bytes" +
-                  ", error count: " + errorCount + " > " + MAX_RETRIES + ")", e);
-        }
-        return ERR;
-      }
-    }
-
-    /**
-     * The webcontainer flushes the data only after it fills the buffer size. So, all data has to be read as readFully()
-     * other wise it fails. So read everything as bytes and then extract an integer out of it
-     */
-    private int readInt(byte[] b) {
-      return (((b[0] & 0xff) << 24) | ((b[1] & 0xff) << 16)
-          | ((b[2] & 0xff) << 8) | (b[3] & 0xff));
-
-    }
-
-    /**
-     * Same as above but to read longs from a byte array
-     */
-    private long readLong(byte[] b) {
-      return (((long) (b[0] & 0xff)) << 56) | (((long) (b[1] & 0xff)) << 48)
-          | (((long) (b[2] & 0xff)) << 40) | (((long) (b[3] & 0xff)) << 32)
-          | (((long) (b[4] & 0xff)) << 24) | ((b[5] & 0xff) << 16)
-          | ((b[6] & 0xff) << 8) | ((b[7] & 0xff));
-
-    }
-
-    /**
-     * cleanup everything
-     */
-    private void cleanup() {
-      try {
-        file.close();
-      } catch (Exception e) {/* no-op */
-        LOG.error("Error closing file: {}", this.saveAs, e);
-      }
-      if (bytesDownloaded != size) {
-        //if the download is not complete then
-        //delete the file being downloaded
-        try {
-          file.delete();
-        } catch (Exception e) {
-          LOG.error("Error deleting file: {}", this.saveAs, e);
-        }
-        //if the failure is due to a user abort it is returned normally else an exception is thrown
-        if (!aborted)
-          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,
-              "Unable to download " + fileName + " completely. Downloaded "
-                  + bytesDownloaded + "!=" + size);
-      }
-    }
-
-    /**
-     * Open a new stream using HttpClient
-     */
-    private FastInputStream getStream() throws IOException {
-
-      ModifiableSolrParams params = new ModifiableSolrParams();
-
-//    //the method is command=filecontent
-      params.set(COMMAND, CMD_GET_FILE);
-      params.set(GENERATION, Long.toString(indexGen));
-      params.set(CommonParams.QT, "/replication");
-      //add the version to download. This is used to reserve the download
-      if (isConf) {
-        //set cf instead of file for config file
-        params.set(CONF_FILE_SHORT, fileName);
-      } else {
-        params.set(FILE, fileName);
-      }
-      if (useInternal) {
-        params.set(COMPRESSION, "true");
-      }
-      //use checksum
-      if (this.includeChecksum) {
-        params.set(CHECKSUM, true);
-      }
-      //wt=filestream this is a custom protocol
-      params.set(CommonParams.WT, FILE_STREAM);
-      // This happen if there is a failure there is a retry. the offset=<sizedownloaded> ensures that
-      // the server starts from the offset
-      if (bytesDownloaded > 0) {
-        params.set(OFFSET, Long.toString(bytesDownloaded));
-      }
-
-
-      NamedList response;
-      InputStream is = null;
-
-      // TODO use shardhandler
-      try (HttpSolrClient client = new HttpSolrClient(masterUrl, myHttpClient, null)) {
-        client.setSoTimeout(60000);
-        client.setConnectionTimeout(15000);
-        QueryRequest req = new QueryRequest(params);
-        response = client.request(req);
-        is = (InputStream) response.get("stream");
-        if(useInternal) {
-          is = new InflaterInputStream(is);
-        }
-        return new FastInputStream(is);
-      } catch (Exception e) {
-        //close stream on error
-        IOUtils.closeQuietly(is);
-        throw new IOException("Could not download file '" + fileName + "'", e);
-      }
-    }
-  }
-
-  private class DirectoryFile implements FileInterface {
-    private final String saveAs;
-    private Directory copy2Dir;
-    private IndexOutput outStream;
-
-    DirectoryFile(Directory tmpIndexDir, String saveAs) throws IOException {
-      this.saveAs = saveAs;
-      this.copy2Dir = tmpIndexDir;
-      outStream = copy2Dir.createOutput(this.saveAs, DirectoryFactory.IOCONTEXT_NO_CACHE);
-    }
-
-    public void sync() throws IOException {
-      copy2Dir.sync(Collections.singleton(saveAs));
-    }
-
-    public void write(byte[] buf, int packetSize) throws IOException {
-      outStream.writeBytes(buf, 0, packetSize);
-    }
-
-    public void close() throws Exception {
-      outStream.close();
-    }
-
-    public void delete() throws Exception {
-      copy2Dir.deleteFile(saveAs);
-    }
-  }
-
-  private class DirectoryFileFetcher extends FileFetcher {
-    DirectoryFileFetcher(Directory tmpIndexDir, Map<String, Object> fileDetails, String saveAs,
-                boolean isConf, long latestGen) throws IOException {
-      super(new DirectoryFile(tmpIndexDir, saveAs), fileDetails, saveAs, isConf, latestGen);
-    }
-  }
-
-  private class LocalFsFile implements FileInterface {
-    private File copy2Dir;
-
-    FileChannel fileChannel;
-    private FileOutputStream fileOutputStream;
-    File file;
-
-    LocalFsFile(File dir, String saveAs) throws IOException {
-      this.copy2Dir = dir;
-
-      this.file = new File(copy2Dir, saveAs);
-
-      File parentDir = this.file.getParentFile();
-      if( ! parentDir.exists() ){
-        if ( ! parentDir.mkdirs() ) {
-          throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,
-              "Failed to create (sub)directory for file: " + saveAs);
-        }
-      }
-
-      this.fileOutputStream = new FileOutputStream(file);
-      this.fileChannel = this.fileOutputStream.getChannel();
-    }
-
-    public void sync() throws IOException {
-      FileUtils.sync(file);
-    }
-
-    public void write(byte[] buf, int packetSize) throws IOException {
-      fileChannel.write(ByteBuffer.wrap(buf, 0, packetSize));
-    }
-
-    public void close() throws Exception {
-      //close the FileOutputStream (which also closes the Channel)
-      fileOutputStream.close();
-    }
-
-    public void delete() throws Exception {
-      Files.delete(file.toPath());
-    }
-  }
-
-  private class LocalFsFileFetcher extends FileFetcher {
-    LocalFsFileFetcher(File dir, Map<String, Object> fileDetails, String saveAs,
-                boolean isConf, long latestGen) throws IOException {
-      super(new LocalFsFile(dir, saveAs), fileDetails, saveAs, isConf, latestGen);
-    }
-  }
-
-  NamedList getDetails() throws IOException, SolrServerException {
-    ModifiableSolrParams params = new ModifiableSolrParams();
-    params.set(COMMAND, CMD_DETAILS);
-    params.set("slave", false);
-    params.set(CommonParams.QT, "/replication");
-
-    // TODO use shardhandler
-    try (HttpSolrClient client = new HttpSolrClient(masterUrl, myHttpClient)) {
-      client.setSoTimeout(60000);
-      client.setConnectionTimeout(15000);
-      QueryRequest request = new QueryRequest(params);
-      return client.request(request);
-    }
-  }
-
-  static Integer readInterval(String interval) {
-    if (interval == null)
-      return null;
-    int result = 0;
-    Matcher m = INTERVAL_PATTERN.matcher(interval.trim());
-    if (m.find()) {
-      String hr = m.group(1);
-      String min = m.group(2);
-      String sec = m.group(3);
-      result = 0;
-      try {
-        if (sec != null && sec.length() > 0)
-          result += Integer.parseInt(sec);
-        if (min != null && min.length() > 0)
-          result += (60 * Integer.parseInt(min));
-        if (hr != null && hr.length() > 0)
-          result += (60 * 60 * Integer.parseInt(hr));
-        result *= 1000;
-      } catch (NumberFormatException e) {
-        throw new SolrException(ErrorCode.SERVER_ERROR, INTERVAL_ERR_MSG);
-      }
-    } else {
-      throw new SolrException(ErrorCode.SERVER_ERROR, INTERVAL_ERR_MSG);
-    }
-
-    return result;
-  }
-
-  public void destroy() {
-    try {
-      if (executorService != null) executorService.shutdown();
-    } finally {
-      try {
-        abortPull();
-      } finally {
-        if (executorService != null) ExecutorUtil
-            .shutdownNowAndAwaitTermination(executorService);
-      }
-    }
-  }
-
-  String getMasterUrl() {
-    return masterUrl;
-  }
-
-  String getPollInterval() {
-    return pollIntervalStr;
-  }
-
-  private static final int MAX_RETRIES = 5;
-
-  private static final int NO_CONTENT = 1;
-
-  private static final int ERR = 2;
-
-  public static final String REPLICATION_PROPERTIES = "replication.properties";
-
-  public static final String POLL_INTERVAL = "pollInterval";
-
-  public static final String INTERVAL_ERR_MSG = "The " + POLL_INTERVAL + " must be in this format 'HH:mm:ss'";
-
-  private static final Pattern INTERVAL_PATTERN = Pattern.compile("(\\d*?):(\\d*?):(\\d*)");
-
-  static final String INDEX_REPLICATED_AT = "indexReplicatedAt";
-
-  static final String TIMES_INDEX_REPLICATED = "timesIndexReplicated";
-
-  static final String CONF_FILES_REPLICATED = "confFilesReplicated";
-
-  static final String CONF_FILES_REPLICATED_AT = "confFilesReplicatedAt";
-
-  static final String TIMES_CONFIG_REPLICATED = "timesConfigReplicated";
-
-  static final String LAST_CYCLE_BYTES_DOWNLOADED = "lastCycleBytesDownloaded";
-
-  static final String TIMES_FAILED = "timesFailed";
-
-  static final String REPLICATION_FAILED_AT = "replicationFailedAt";
-
-  static final String PREVIOUS_CYCLE_TIME_TAKEN = "previousCycleTimeInSeconds";
-
-  static final String INDEX_REPLICATED_AT_LIST = "indexReplicatedAtList";
-
-  static final String REPLICATION_FAILED_AT_LIST = "replicationFailedAtList";
-}
diff --git a/solr/core/src/java/org/apache/solr/handler/SnapShooter.java b/solr/core/src/java/org/apache/solr/handler/SnapShooter.java
index 3918019..60bf3c8 100644
--- a/solr/core/src/java/org/apache/solr/handler/SnapShooter.java
+++ b/solr/core/src/java/org/apache/solr/handler/SnapShooter.java
@@ -144,7 +144,7 @@ public class SnapShooter {
       details.add("snapshotName", snapshotName);
       LOG.info("Done creating backup snapshot: " + (snapshotName == null ? "<not named>" : snapshotName));
     } catch (Exception e) {
-      SnapPuller.delTree(snapShotDir);
+      IndexFetcher.delTree(snapShotDir);
       LOG.error("Exception while creating snapshot", e);
       details.add("snapShootException", e.getMessage());
     } finally {
@@ -170,7 +170,7 @@ public class SnapShooter {
     int i=1;
     for (OldBackupDirectory dir : dirs) {
       if (i++ > numberToKeep) {
-        SnapPuller.delTree(dir.dir);
+        IndexFetcher.delTree(dir.dir);
       }
     }
   }
@@ -181,7 +181,7 @@ public class SnapShooter {
     NamedList<Object> details = new NamedList<>();
     boolean isSuccess;
     File f = new File(snapDir, "snapshot." + snapshotName);
-    isSuccess = SnapPuller.delTree(f);
+    isSuccess = IndexFetcher.delTree(f);
 
     if(isSuccess) {
       details.add("status", "success");
diff --git a/solr/core/src/test-files/log4j.properties b/solr/core/src/test-files/log4j.properties
index 4a3a20a..659b430 100644
--- a/solr/core/src/test-files/log4j.properties
+++ b/solr/core/src/test-files/log4j.properties
@@ -25,7 +25,7 @@ log4j.logger.org.apache.solr.hadoop=INFO
 #log4j.logger.org.apache.solr.cloud.ChaosMonkey=DEBUG
 #log4j.logger.org.apache.solr.update.TransactionLog=DEBUG
 #log4j.logger.org.apache.solr.handler.ReplicationHandler=DEBUG
-#log4j.logger.org.apache.solr.handler.SnapPuller=DEBUG
+#log4j.logger.org.apache.solr.handler.IndexFetcher=DEBUG
 
 #log4j.logger.org.apache.solr.common.cloud.ClusterStateUtil=DEBUG
 #log4j.logger.org.apache.solr.cloud.OverseerAutoReplicaFailoverThread=DEBUG
\ No newline at end of file
diff --git a/solr/core/src/test/org/apache/solr/core/TestArbitraryIndexDir.java b/solr/core/src/test/org/apache/solr/core/TestArbitraryIndexDir.java
index 2ffb235..8b005b6 100644
--- a/solr/core/src/test/org/apache/solr/core/TestArbitraryIndexDir.java
+++ b/solr/core/src/test/org/apache/solr/core/TestArbitraryIndexDir.java
@@ -36,7 +36,7 @@ import org.apache.lucene.store.Directory;
 import org.apache.lucene.util.IOUtils;
 import org.apache.solr.common.SolrException;
 import org.apache.solr.common.params.CommonParams;
-import org.apache.solr.handler.SnapPuller;
+import org.apache.solr.handler.IndexFetcher;
 import org.apache.solr.util.AbstractSolrTestCase;
 import org.apache.solr.util.TestHarness;
 import org.junit.AfterClass;
@@ -93,7 +93,7 @@ public class TestArbitraryIndexDir extends AbstractSolrTestCase{
     assertU(adoc("id", String.valueOf(1),
         "name", "name"+String.valueOf(1)));
     //create a new index dir and index.properties file
-    File idxprops = new File(h.getCore().getDataDir() + SnapPuller.INDEX_PROPERTIES);
+    File idxprops = new File(h.getCore().getDataDir() + IndexFetcher.INDEX_PROPERTIES);
     Properties p = new Properties();
     File newDir = new File(h.getCore().getDataDir() + "index_temp");
     newDir.mkdirs();
@@ -104,7 +104,7 @@ public class TestArbitraryIndexDir extends AbstractSolrTestCase{
       p.store(os, "index properties");
     } catch (Exception e) {
       throw new SolrException(SolrException.ErrorCode.SERVER_ERROR,
-          "Unable to write " + SnapPuller.INDEX_PROPERTIES, e);
+          "Unable to write " + IndexFetcher.INDEX_PROPERTIES, e);
     } finally {
       IOUtils.closeWhileHandlingException(os);
     }
diff --git a/solr/core/src/test/org/apache/solr/handler/TestReplicationHandler.java b/solr/core/src/test/org/apache/solr/handler/TestReplicationHandler.java
index 228ddcb..5550397 100644
--- a/solr/core/src/test/org/apache/solr/handler/TestReplicationHandler.java
+++ b/solr/core/src/test/org/apache/solr/handler/TestReplicationHandler.java
@@ -172,17 +172,13 @@ public class TestReplicationHandler extends SolrTestCaseJ4 {
   }
 
   NamedList query(String query, SolrClient s) throws SolrServerException, IOException {
-    NamedList res = new SimpleOrderedMap();
     ModifiableSolrParams params = new ModifiableSolrParams();
 
     params.add("q", query);
     params.add("sort","id desc");
 
     QueryResponse qres = s.query(params);
-
-    res = qres.getResponse();
-
-    return res;
+    return qres.getResponse();
   }
 
   /** will sleep up to 30 seconds, looking for expectedDocCount */
@@ -304,7 +300,7 @@ public class TestReplicationHandler extends SolrTestCaseJ4 {
       assertNotNull("slave has slave section", 
                     details.get("slave"));
       // SOLR-2677: assert not false negatives
-      Object timesFailed = ((NamedList)details.get("slave")).get(SnapPuller.TIMES_FAILED);
+      Object timesFailed = ((NamedList)details.get("slave")).get(IndexFetcher.TIMES_FAILED);
       assertEquals("slave has fetch error count",
                    null, timesFailed);
 
@@ -513,7 +509,7 @@ public class TestReplicationHandler extends SolrTestCaseJ4 {
     slaveClient.close();
     slaveClient = createNewSolrClient(slaveJetty.getLocalPort());
 
-    //add a doc with new field and commit on master to trigger snappull from slave.
+    //add a doc with new field and commit on master to trigger index fetch from slave.
     index(masterClient, "id", "2000", "name", "name = " + 2000, "newname", "newname = " + 2000);
     masterClient.commit();
 
@@ -581,7 +577,7 @@ public class TestReplicationHandler extends SolrTestCaseJ4 {
   }
 
   @Test
-  public void doTestSnapPullWithMasterUrl() throws Exception {
+  public void doTestIndexFetchWithMasterUrl() throws Exception {
     //change solrconfig on slave
     //this has no entry for pollinginterval
     slave.copyConfigFile(CONF_DIR + "solrconfig-slave1.xml", "solrconfig.xml");
@@ -608,7 +604,7 @@ public class TestReplicationHandler extends SolrTestCaseJ4 {
     SolrDocumentList masterQueryResult = (SolrDocumentList) masterQueryRsp.get("response");
     assertEquals(nDocs, masterQueryResult.getNumFound());
 
-    // snappull
+    // index fetch
     String masterUrl = buildUrl(slaveJetty.getLocalPort()) + "/" + DEFAULT_TEST_CORENAME + "/replication?command=fetchindex&masterUrl=";
     masterUrl += buildUrl(masterJetty.getLocalPort()) + "/" + DEFAULT_TEST_CORENAME + "/replication";
     URL url = new URL(masterUrl);
@@ -623,7 +619,7 @@ public class TestReplicationHandler extends SolrTestCaseJ4 {
     String cmp = BaseDistributedSearchTestCase.compare(masterQueryResult, slaveQueryResult, 0, null);
     assertEquals(null, cmp);
 
-    // snappull from the slave to the master
+    // index fetch from the slave to the master
     
     for (int i = nDocs; i < nDocs + 3; i++)
       index(slaveClient, "id", i, "name", "name = " + i);
@@ -765,7 +761,7 @@ public class TestReplicationHandler extends SolrTestCaseJ4 {
             .get("response");
         assertEquals(totalDocs, masterQueryResult.getNumFound());
         
-        // snappull
+        // index fetch
         Date slaveCoreStart = watchCoreStartAt(slaveClient, 30*1000, null);
         pullFromMasterToSlave();
         if (confCoreReload) {
@@ -1219,7 +1215,7 @@ public class TestReplicationHandler extends SolrTestCaseJ4 {
     // record collection1's start time on slave
     final Date slaveStartTime = watchCoreStartAt(slaveClient, 30*1000, null);
 
-    //add a doc with new field and commit on master to trigger snappull from slave.
+    //add a doc with new field and commit on master to trigger index fetch from slave.
     index(masterClient, "id", "2000", "name", "name = " + 2000, "newname", "n2000");
     masterClient.commit();
     rQuery(1, "newname:n2000", masterClient);  // sanity check
diff --git a/solr/test-framework/src/java/org/apache/solr/core/MockDirectoryFactory.java b/solr/test-framework/src/java/org/apache/solr/core/MockDirectoryFactory.java
index d76b709..7f07f85 100644
--- a/solr/test-framework/src/java/org/apache/solr/core/MockDirectoryFactory.java
+++ b/solr/test-framework/src/java/org/apache/solr/core/MockDirectoryFactory.java
@@ -69,7 +69,7 @@ public class MockDirectoryFactory extends EphemeralDirectoryFactory {
       // already been created.
       mockDirWrapper.setPreventDoubleWrite(false);
       
-      // snappuller & co don't seem ready for this:
+      // IndexFetcher & co don't seem ready for this:
       mockDirWrapper.setEnableVirusScanner(false);
       
       if (allowReadingFilesStillOpenForWrite) {

