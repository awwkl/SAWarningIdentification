GitDiffStart: 257db54ab01f8edcee958187e64aa002d07589c7 | Wed Sep 5 08:44:11 2012 +0000
diff --git a/dev-tools/eclipse/dot.classpath b/dev-tools/eclipse/dot.classpath
index 0a37146..0c61c3f 100644
--- a/dev-tools/eclipse/dot.classpath
+++ b/dev-tools/eclipse/dot.classpath
@@ -3,6 +3,9 @@
   <classpathentry kind="src" path="lucene/core/src/java"/>
   <classpathentry kind="src" path="lucene/core/src/resources"/>
   <classpathentry kind="src" path="lucene/core/src/test"/>
+  <classpathentry kind="src" path="lucene/codecs/src/java"/>
+  <classpathentry kind="src" output="bin/codecs" path="lucene/codecs/src/resources"/>
+  <classpathentry kind="src" path="lucene/codecs/src/test"/>
   <classpathentry kind="src" path="lucene/demo/src/java"/>
   <classpathentry kind="src" path="lucene/demo/src/resources"/>
   <classpathentry kind="src" path="lucene/demo/src/test"/>
diff --git a/dev-tools/idea/.idea/ant.xml b/dev-tools/idea/.idea/ant.xml
index 9cfa45c..5372745 100644
--- a/dev-tools/idea/.idea/ant.xml
+++ b/dev-tools/idea/.idea/ant.xml
@@ -8,6 +8,7 @@
     <buildFile url="file://$PROJECT_DIR$/lucene/memory/build.xml" />
     <buildFile url="file://$PROJECT_DIR$/lucene/misc/build.xml" />
     <buildFile url="file://$PROJECT_DIR$/lucene/sandbox/build.xml" />
+    <buildFile url="file://$PROJECT_DIR$/lucene/codecs/build.xml" />
     <buildFile url="file://$PROJECT_DIR$/lucene/core/build.xml" />
     <buildFile url="file://$PROJECT_DIR$/lucene/tools/build.xml" />
     <buildFile url="file://$PROJECT_DIR$/lucene/test-framework/build.xml" />
diff --git a/dev-tools/idea/.idea/modules.xml b/dev-tools/idea/.idea/modules.xml
index 89cb477..14e7a0b 100644
--- a/dev-tools/idea/.idea/modules.xml
+++ b/dev-tools/idea/.idea/modules.xml
@@ -4,6 +4,7 @@
     <modules>
       <module filepath="$PROJECT_DIR$/parent.iml" />
       <module filepath="$PROJECT_DIR$/lucene/lucene.iml" />
+      <module filepath="$PROJECT_DIR$/lucene/codecs/codecs.iml" />
       <module filepath="$PROJECT_DIR$/lucene/demo/demo.iml" />
       <module filepath="$PROJECT_DIR$/lucene/highlighter/highlighter.iml" />
       <module filepath="$PROJECT_DIR$/lucene/memory/memory.iml" />
diff --git a/dev-tools/idea/.idea/workspace.xml b/dev-tools/idea/.idea/workspace.xml
index cb40bf4..d2be079 100644
--- a/dev-tools/idea/.idea/workspace.xml
+++ b/dev-tools/idea/.idea/workspace.xml
@@ -78,6 +78,13 @@
                 antfile="file://$PROJECT_DIR$/lucene/benchmark/build.xml" />
       </method>
     </configuration>
+    <configuration default="false" name="Module codecs" type="JUnit" factoryName="JUnit">
+      <module name="facet" />
+      <option name="TEST_OBJECT" value="package" />
+      <option name="WORKING_DIRECTORY" value="file://$PROJECT_DIR$/lucene/build/codecs" />
+      <option name="VM_PARAMETERS" value="-ea -DtempDir=temp" />
+      <option name="TEST_SEARCH_SCOPE"><value defaultName="singleModule" /></option>
+    </configuration>
     <configuration default="false" name="Module facet" type="JUnit" factoryName="JUnit">
       <module name="facet" />
       <option name="TEST_OBJECT" value="package" />
diff --git a/dev-tools/idea/lucene/codecs/codecs.iml b/dev-tools/idea/lucene/codecs/codecs.iml
new file mode 100644
index 0000000..1ace440
--- /dev/null
+++ b/dev-tools/idea/lucene/codecs/codecs.iml
@@ -0,0 +1,16 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<module type="JAVA_MODULE" version="4">
+  <component name="NewModuleRootManager" inherit-compiler-output="false">
+    <output url="file://$MODULE_DIR$/../build/codecs/classes/java" />
+    <output-test url="file://$MODULE_DIR$/../build/codecs/classes/test" />
+    <exclude-output />
+    <content url="file://$MODULE_DIR$">
+      <sourceFolder url="file://$MODULE_DIR$/src/java" isTestSource="false" />
+      <sourceFolder url="file://$MODULE_DIR$/src/test" isTestSource="true" />
+    </content>
+    <orderEntry type="inheritedJdk" />
+    <orderEntry type="sourceFolder" forTests="false" />
+    <orderEntry type="library" scope="TEST" name="JUnit" level="project" />
+    <orderEntry type="module" module-name="lucene" />
+  </component>
+</module>
diff --git a/dev-tools/maven/lucene/codecs/pom.xml.template b/dev-tools/maven/lucene/codecs/pom.xml.template
new file mode 100644
index 0000000..4638415
--- /dev/null
+++ b/dev-tools/maven/lucene/codecs/pom.xml.template
@@ -0,0 +1,72 @@
+<project xmlns="http://maven.apache.org/POM/4.0.0"
+         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">
+  <!--
+    Licensed to the Apache Software Foundation (ASF) under one
+    or more contributor license agreements.  See the NOTICE file
+    distributed with this work for additional information
+    regarding copyright ownership.  The ASF licenses this file
+    to you under the Apache License, Version 2.0 (the
+    "License"); you may not use this file except in compliance
+    with the License.  You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+    Unless required by applicable law or agreed to in writing,
+    software distributed under the License is distributed on an
+    "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+    KIND, either express or implied.  See the License for the
+    specific language governing permissions and limitations
+    under the License.
+  -->
+  <modelVersion>4.0.0</modelVersion>
+  <parent>
+    <groupId>org.apache.lucene</groupId>
+    <artifactId>lucene-parent</artifactId>
+    <version>@version@</version>
+    <relativePath>../pom.xml</relativePath>
+  </parent>
+  <groupId>org.apache.lucene</groupId>
+  <artifactId>lucene-codecs</artifactId>
+  <packaging>jar</packaging>
+  <name>Lucene codecs</name>
+  <description>
+    Codecs and postings formats for Apache Lucene.
+  </description>
+  <properties>
+    <module-directory>lucene/codecs</module-directory>
+    <top-level>../../..</top-level>
+    <module-path>${top-level}/${module-directory}</module-path>
+  </properties>
+  <scm>
+    <connection>scm:svn:${vc-anonymous-base-url}/${module-directory}</connection>
+    <developerConnection>scm:svn:${vc-dev-base-url}/${module-directory}</developerConnection>
+    <url>${vc-browse-base-url}/${module-directory}</url>
+  </scm>
+  <dependencies>
+    <dependency> 
+      <!-- lucene-test-framework dependency must be declared before lucene-core -->
+      <groupId>${project.groupId}</groupId>
+      <artifactId>lucene-test-framework</artifactId>
+      <version>${project.version}</version>
+      <scope>test</scope>
+    </dependency>
+    <dependency>
+      <groupId>${project.groupId}</groupId>
+      <artifactId>lucene-core</artifactId>
+      <version>${project.version}</version>
+    </dependency>
+  </dependencies>
+  <build>
+    <sourceDirectory>${module-path}/src/java</sourceDirectory>
+    <testSourceDirectory>${module-path}/src/test</testSourceDirectory>
+    <testResources>
+      <testResource>
+        <directory>${project.build.testSourceDirectory}</directory>
+        <excludes>
+          <exclude>**/*.java</exclude>
+        </excludes>
+      </testResource>
+    </testResources>
+  </build>
+</project>
diff --git a/dev-tools/maven/lucene/pom.xml.template b/dev-tools/maven/lucene/pom.xml.template
index 9883aa2..8e372a9 100644
--- a/dev-tools/maven/lucene/pom.xml.template
+++ b/dev-tools/maven/lucene/pom.xml.template
@@ -41,6 +41,7 @@
   </scm>
   <modules>
     <module>core</module>
+    <module>codecs</module>
     <module>test-framework</module>
     <module>analysis</module>
     <module>benchmark</module>
diff --git a/dev-tools/scripts/smokeTestRelease.py b/dev-tools/scripts/smokeTestRelease.py
index 54f1ef3..744b29f 100644
--- a/dev-tools/scripts/smokeTestRelease.py
+++ b/dev-tools/scripts/smokeTestRelease.py
@@ -441,7 +441,7 @@ def verifyUnpacked(project, artifact, unpackPath, version, tmpDir):
 
   if project == 'lucene':
     # TODO: clean this up to not be a list of modules that we must maintain
-    extras = ('analysis', 'benchmark', 'core', 'demo', 'docs', 'facet', 'grouping', 'highlighter', 'join', 'memory', 'misc', 'queries', 'queryparser', 'sandbox', 'spatial', 'suggest', 'test-framework', 'licenses')
+    extras = ('analysis', 'benchmark', 'codecs', 'core', 'demo', 'docs', 'facet', 'grouping', 'highlighter', 'join', 'memory', 'misc', 'queries', 'queryparser', 'sandbox', 'spatial', 'suggest', 'test-framework', 'licenses')
     if isSrc:
       extras += ('build.xml', 'common-build.xml', 'module-build.xml', 'ivy-settings.xml', 'backwards', 'tools', 'site')
   else:
diff --git a/lucene/CHANGES.txt b/lucene/CHANGES.txt
index 1292007..444a4f5 100644
--- a/lucene/CHANGES.txt
+++ b/lucene/CHANGES.txt
@@ -167,6 +167,9 @@ Build
   the licenses/ folder and the binary release. Some had different
   versions or additional unnecessary dependencies. (selckin via Robert Muir)
 
+* LUCENE-4340: Move all non-default codec, postings format and terms
+  dictionary implementations to lucene/codecs. (Adrien Grand)
+
 Documentation
 
 * LUCENE-4302: Fix facet userguide to have HTML loose doctype like
diff --git a/lucene/codecs/build.xml b/lucene/codecs/build.xml
new file mode 100644
index 0000000..c7f05cf
--- /dev/null
+++ b/lucene/codecs/build.xml
@@ -0,0 +1,24 @@
+<!--
+  Licensed to the Apache Software Foundation (ASF) under one or more
+  contributor license agreements.  See the NOTICE file distributed with
+  this work for additional information regarding copyright ownership.
+  The ASF licenses this file to You under the Apache License, Version 2.0
+  (the "License"); you may not use this file except in compliance with
+  the License.  You may obtain a copy of the License at
+
+      http://www.apache.org/licenses/LICENSE-2.0
+
+  Unless required by applicable law or agreed to in writing, software
+  distributed under the License is distributed on an "AS IS" BASIS,
+  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+  See the License for the specific language governing permissions and
+  limitations under the License.
+  -->
+
+<project name="codecs" default="default">
+  <description>
+    Lucene codecs and postings formats.
+  </description>
+
+  <import file="../module-build.xml"/>
+</project>
diff --git a/lucene/codecs/ivy.xml b/lucene/codecs/ivy.xml
new file mode 100644
index 0000000..388d3e3
--- /dev/null
+++ b/lucene/codecs/ivy.xml
@@ -0,0 +1,21 @@
+<!--
+   Licensed to the Apache Software Foundation (ASF) under one
+   or more contributor license agreements.  See the NOTICE file
+   distributed with this work for additional information
+   regarding copyright ownership.  The ASF licenses this file
+   to you under the Apache License, Version 2.0 (the
+   "License"); you may not use this file except in compliance
+   with the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing,
+   software distributed under the License is distributed on an
+   "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+   KIND, either express or implied.  See the License for the
+   specific language governing permissions and limitations
+   under the License.    
+-->
+<ivy-module version="2.0">
+    <info organisation="org.apache.lucene" module="codecs"/>
+</ivy-module>
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/BlockTermsReader.java b/lucene/codecs/src/java/org/apache/lucene/codecs/BlockTermsReader.java
new file mode 100644
index 0000000..7d1eece
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/BlockTermsReader.java
@@ -0,0 +1,870 @@
+package org.apache.lucene.codecs;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Collections;
+import java.util.Comparator;
+import java.util.Iterator;
+import java.util.TreeMap;
+
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.DocsAndPositionsEnum;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.TermState;
+import org.apache.lucene.index.Terms;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.store.ByteArrayDataInput;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.DoubleBarrelLRUCache;
+
+/** Handles a terms dict, but decouples all details of
+ *  doc/freqs/positions reading to an instance of {@link
+ *  PostingsReaderBase}.  This class is reusable for
+ *  codecs that use a different format for
+ *  docs/freqs/positions (though codecs are also free to
+ *  make their own terms dict impl).
+ *
+ * <p>This class also interacts with an instance of {@link
+ * TermsIndexReaderBase}, to abstract away the specific
+ * implementation of the terms dict index. 
+ * @lucene.experimental */
+
+public class BlockTermsReader extends FieldsProducer {
+  // Open input to the main terms dict file (_X.tis)
+  private final IndexInput in;
+
+  // Reads the terms dict entries, to gather state to
+  // produce DocsEnum on demand
+  private final PostingsReaderBase postingsReader;
+
+  private final TreeMap<String,FieldReader> fields = new TreeMap<String,FieldReader>();
+
+  // Caches the most recently looked-up field + terms:
+  private final DoubleBarrelLRUCache<FieldAndTerm,BlockTermState> termsCache;
+
+  // Reads the terms index
+  private TermsIndexReaderBase indexReader;
+
+  // keeps the dirStart offset
+  protected long dirOffset;
+
+  // Used as key for the terms cache
+  private static class FieldAndTerm extends DoubleBarrelLRUCache.CloneableKey {
+    String field;
+    BytesRef term;
+
+    public FieldAndTerm() {
+    }
+
+    public FieldAndTerm(FieldAndTerm other) {
+      field = other.field;
+      term = BytesRef.deepCopyOf(other.term);
+    }
+
+    @Override
+    public boolean equals(Object _other) {
+      FieldAndTerm other = (FieldAndTerm) _other;
+      return other.field.equals(field) && term.bytesEquals(other.term);
+    }
+
+    @Override
+    public FieldAndTerm clone() {
+      return new FieldAndTerm(this);
+    }
+
+    @Override
+    public int hashCode() {
+      return field.hashCode() * 31 + term.hashCode();
+    }
+  }
+  
+  // private String segment;
+  
+  public BlockTermsReader(TermsIndexReaderBase indexReader, Directory dir, FieldInfos fieldInfos, SegmentInfo info, PostingsReaderBase postingsReader, IOContext context,
+                          int termsCacheSize, String segmentSuffix)
+    throws IOException {
+    
+    this.postingsReader = postingsReader;
+    termsCache = new DoubleBarrelLRUCache<FieldAndTerm,BlockTermState>(termsCacheSize);
+
+    // this.segment = segment;
+    in = dir.openInput(IndexFileNames.segmentFileName(info.name, segmentSuffix, BlockTermsWriter.TERMS_EXTENSION),
+                       context);
+
+    boolean success = false;
+    try {
+      readHeader(in);
+
+      // Have PostingsReader init itself
+      postingsReader.init(in);
+
+      // Read per-field details
+      seekDir(in, dirOffset);
+
+      final int numFields = in.readVInt();
+      if (numFields < 0) {
+        throw new CorruptIndexException("invalid number of fields: " + numFields + " (resource=" + in + ")");
+      }
+      for(int i=0;i<numFields;i++) {
+        final int field = in.readVInt();
+        final long numTerms = in.readVLong();
+        assert numTerms >= 0;
+        final long termsStartPointer = in.readVLong();
+        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);
+        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();
+        final long sumDocFreq = in.readVLong();
+        final int docCount = in.readVInt();
+        if (docCount < 0 || docCount > info.getDocCount()) { // #docs with field must be <= #docs
+          throw new CorruptIndexException("invalid docCount: " + docCount + " maxDoc: " + info.getDocCount() + " (resource=" + in + ")");
+        }
+        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field
+          throw new CorruptIndexException("invalid sumDocFreq: " + sumDocFreq + " docCount: " + docCount + " (resource=" + in + ")");
+        }
+        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings
+          throw new CorruptIndexException("invalid sumTotalTermFreq: " + sumTotalTermFreq + " sumDocFreq: " + sumDocFreq + " (resource=" + in + ")");
+        }
+        FieldReader previous = fields.put(fieldInfo.name, new FieldReader(fieldInfo, numTerms, termsStartPointer, sumTotalTermFreq, sumDocFreq, docCount));
+        if (previous != null) {
+          throw new CorruptIndexException("duplicate fields: " + fieldInfo.name + " (resource=" + in + ")");
+        }
+      }
+      success = true;
+    } finally {
+      if (!success) {
+        in.close();
+      }
+    }
+
+    this.indexReader = indexReader;
+  }
+
+  protected void readHeader(IndexInput input) throws IOException {
+    CodecUtil.checkHeader(input, BlockTermsWriter.CODEC_NAME,
+                          BlockTermsWriter.VERSION_START,
+                          BlockTermsWriter.VERSION_CURRENT);
+    dirOffset = input.readLong();
+  }
+  
+  protected void seekDir(IndexInput input, long dirOffset)
+      throws IOException {
+    input.seek(dirOffset);
+  }
+  
+  @Override
+  public void close() throws IOException {
+    try {
+      try {
+        if (indexReader != null) {
+          indexReader.close();
+        }
+      } finally {
+        // null so if an app hangs on to us (ie, we are not
+        // GCable, despite being closed) we still free most
+        // ram
+        indexReader = null;
+        if (in != null) {
+          in.close();
+        }
+      }
+    } finally {
+      if (postingsReader != null) {
+        postingsReader.close();
+      }
+    }
+  }
+
+  @Override
+  public Iterator<String> iterator() {
+    return Collections.unmodifiableSet(fields.keySet()).iterator();
+  }
+
+  @Override
+  public Terms terms(String field) throws IOException {
+    assert field != null;
+    return fields.get(field);
+  }
+
+  @Override
+  public int size() {
+    return fields.size();
+  }
+
+  private class FieldReader extends Terms {
+    final long numTerms;
+    final FieldInfo fieldInfo;
+    final long termsStartPointer;
+    final long sumTotalTermFreq;
+    final long sumDocFreq;
+    final int docCount;
+
+    FieldReader(FieldInfo fieldInfo, long numTerms, long termsStartPointer, long sumTotalTermFreq, long sumDocFreq, int docCount) {
+      assert numTerms > 0;
+      this.fieldInfo = fieldInfo;
+      this.numTerms = numTerms;
+      this.termsStartPointer = termsStartPointer;
+      this.sumTotalTermFreq = sumTotalTermFreq;
+      this.sumDocFreq = sumDocFreq;
+      this.docCount = docCount;
+    }
+
+    @Override
+    public Comparator<BytesRef> getComparator() {
+      return BytesRef.getUTF8SortedAsUnicodeComparator();
+    }
+
+    @Override
+    public TermsEnum iterator(TermsEnum reuse) throws IOException {
+      return new SegmentTermsEnum();
+    }
+
+    @Override
+    public boolean hasOffsets() {
+      return fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
+    }
+
+    @Override
+    public boolean hasPositions() {
+      return fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;
+    }
+    
+    @Override
+    public boolean hasPayloads() {
+      return fieldInfo.hasPayloads();
+    }
+
+    @Override
+    public long size() {
+      return numTerms;
+    }
+
+    @Override
+    public long getSumTotalTermFreq() {
+      return sumTotalTermFreq;
+    }
+
+    @Override
+    public long getSumDocFreq() throws IOException {
+      return sumDocFreq;
+    }
+
+    @Override
+    public int getDocCount() throws IOException {
+      return docCount;
+    }
+
+    // Iterates through terms in this field
+    private final class SegmentTermsEnum extends TermsEnum {
+      private final IndexInput in;
+      private final BlockTermState state;
+      private final boolean doOrd;
+      private final FieldAndTerm fieldTerm = new FieldAndTerm();
+      private final TermsIndexReaderBase.FieldIndexEnum indexEnum;
+      private final BytesRef term = new BytesRef();
+
+      /* This is true if indexEnum is "still" seek'd to the index term
+         for the current term. We set it to true on seeking, and then it
+         remains valid until next() is called enough times to load another
+         terms block: */
+      private boolean indexIsCurrent;
+
+      /* True if we've already called .next() on the indexEnum, to "bracket"
+         the current block of terms: */
+      private boolean didIndexNext;
+
+      /* Next index term, bracketing the current block of terms; this is
+         only valid if didIndexNext is true: */
+      private BytesRef nextIndexTerm;
+
+      /* True after seekExact(TermState), do defer seeking.  If the app then
+         calls next() (which is not "typical"), then we'll do the real seek */
+      private boolean seekPending;
+
+      /* How many blocks we've read since last seek.  Once this
+         is >= indexEnum.getDivisor() we set indexIsCurrent to false (since
+         the index can no long bracket seek-within-block). */
+      private int blocksSinceSeek;
+
+      private byte[] termSuffixes;
+      private ByteArrayDataInput termSuffixesReader = new ByteArrayDataInput();
+
+      /* Common prefix used for all terms in this block. */
+      private int termBlockPrefix;
+
+      /* How many terms in current block */
+      private int blockTermCount;
+
+      private byte[] docFreqBytes;
+      private final ByteArrayDataInput freqReader = new ByteArrayDataInput();
+      private int metaDataUpto;
+
+      public SegmentTermsEnum() throws IOException {
+        in = BlockTermsReader.this.in.clone();
+        in.seek(termsStartPointer);
+        indexEnum = indexReader.getFieldEnum(fieldInfo);
+        doOrd = indexReader.supportsOrd();
+        fieldTerm.field = fieldInfo.name;
+        state = postingsReader.newTermState();
+        state.totalTermFreq = -1;
+        state.ord = -1;
+
+        termSuffixes = new byte[128];
+        docFreqBytes = new byte[64];
+        //System.out.println("BTR.enum init this=" + this + " postingsReader=" + postingsReader);
+      }
+
+      @Override
+      public Comparator<BytesRef> getComparator() {
+        return BytesRef.getUTF8SortedAsUnicodeComparator();
+      }
+
+      // TODO: we may want an alternate mode here which is
+      // "if you are about to return NOT_FOUND I won't use
+      // the terms data from that"; eg FuzzyTermsEnum will
+      // (usually) just immediately call seek again if we
+      // return NOT_FOUND so it's a waste for us to fill in
+      // the term that was actually NOT_FOUND
+      @Override
+      public SeekStatus seekCeil(final BytesRef target, final boolean useCache) throws IOException {
+
+        if (indexEnum == null) {
+          throw new IllegalStateException("terms index was not loaded");
+        }
+   
+        //System.out.println("BTR.seek seg=" + segment + " target=" + fieldInfo.name + ":" + target.utf8ToString() + " " + target + " current=" + term().utf8ToString() + " " + term() + " useCache=" + useCache + " indexIsCurrent=" + indexIsCurrent + " didIndexNext=" + didIndexNext + " seekPending=" + seekPending + " divisor=" + indexReader.getDivisor() + " this="  + this);
+        if (didIndexNext) {
+          if (nextIndexTerm == null) {
+            //System.out.println("  nextIndexTerm=null");
+          } else {
+            //System.out.println("  nextIndexTerm=" + nextIndexTerm.utf8ToString());
+          }
+        }
+
+        // Check cache
+        if (useCache) {
+          fieldTerm.term = target;
+          // TODO: should we differentiate "frozen"
+          // TermState (ie one that was cloned and
+          // cached/returned by termState()) from the
+          // malleable (primary) one?
+          final TermState cachedState = termsCache.get(fieldTerm);
+          if (cachedState != null) {
+            seekPending = true;
+            //System.out.println("  cached!");
+            seekExact(target, cachedState);
+            //System.out.println("  term=" + term.utf8ToString());
+            return SeekStatus.FOUND;
+          }
+        }
+
+        boolean doSeek = true;
+
+        // See if we can avoid seeking, because target term
+        // is after current term but before next index term:
+        if (indexIsCurrent) {
+
+          final int cmp = BytesRef.getUTF8SortedAsUnicodeComparator().compare(term, target);
+
+          if (cmp == 0) {
+            // Already at the requested term
+            return SeekStatus.FOUND;
+          } else if (cmp < 0) {
+
+            // Target term is after current term
+            if (!didIndexNext) {
+              if (indexEnum.next() == -1) {
+                nextIndexTerm = null;
+              } else {
+                nextIndexTerm = indexEnum.term();
+              }
+              //System.out.println("  now do index next() nextIndexTerm=" + (nextIndexTerm == null ? "null" : nextIndexTerm.utf8ToString()));
+              didIndexNext = true;
+            }
+
+            if (nextIndexTerm == null || BytesRef.getUTF8SortedAsUnicodeComparator().compare(target, nextIndexTerm) < 0) {
+              // Optimization: requested term is within the
+              // same term block we are now in; skip seeking
+              // (but do scanning):
+              doSeek = false;
+              //System.out.println("  skip seek: nextIndexTerm=" + (nextIndexTerm == null ? "null" : nextIndexTerm.utf8ToString()));
+            }
+          }
+        }
+
+        if (doSeek) {
+          //System.out.println("  seek");
+
+          // Ask terms index to find biggest indexed term (=
+          // first term in a block) that's <= our text:
+          in.seek(indexEnum.seek(target));
+          boolean result = nextBlock();
+
+          // Block must exist since, at least, the indexed term
+          // is in the block:
+          assert result;
+
+          indexIsCurrent = true;
+          didIndexNext = false;
+          blocksSinceSeek = 0;          
+
+          if (doOrd) {
+            state.ord = indexEnum.ord()-1;
+          }
+
+          term.copyBytes(indexEnum.term());
+          //System.out.println("  seek: term=" + term.utf8ToString());
+        } else {
+          //System.out.println("  skip seek");
+          if (state.termBlockOrd == blockTermCount && !nextBlock()) {
+            indexIsCurrent = false;
+            return SeekStatus.END;
+          }
+        }
+
+        seekPending = false;
+
+        int common = 0;
+
+        // Scan within block.  We could do this by calling
+        // _next() and testing the resulting term, but this
+        // is wasteful.  Instead, we first confirm the
+        // target matches the common prefix of this block,
+        // and then we scan the term bytes directly from the
+        // termSuffixesreader's byte[], saving a copy into
+        // the BytesRef term per term.  Only when we return
+        // do we then copy the bytes into the term.
+
+        while(true) {
+
+          // First, see if target term matches common prefix
+          // in this block:
+          if (common < termBlockPrefix) {
+            final int cmp = (term.bytes[common]&0xFF) - (target.bytes[target.offset + common]&0xFF);
+            if (cmp < 0) {
+
+              // TODO: maybe we should store common prefix
+              // in block header?  (instead of relying on
+              // last term of previous block)
+
+              // Target's prefix is after the common block
+              // prefix, so term cannot be in this block
+              // but it could be in next block.  We
+              // must scan to end-of-block to set common
+              // prefix for next block:
+              if (state.termBlockOrd < blockTermCount) {
+                while(state.termBlockOrd < blockTermCount-1) {
+                  state.termBlockOrd++;
+                  state.ord++;
+                  termSuffixesReader.skipBytes(termSuffixesReader.readVInt());
+                }
+                final int suffix = termSuffixesReader.readVInt();
+                term.length = termBlockPrefix + suffix;
+                if (term.bytes.length < term.length) {
+                  term.grow(term.length);
+                }
+                termSuffixesReader.readBytes(term.bytes, termBlockPrefix, suffix);
+              }
+              state.ord++;
+              
+              if (!nextBlock()) {
+                indexIsCurrent = false;
+                return SeekStatus.END;
+              }
+              common = 0;
+
+            } else if (cmp > 0) {
+              // Target's prefix is before the common prefix
+              // of this block, so we position to start of
+              // block and return NOT_FOUND:
+              assert state.termBlockOrd == 0;
+
+              final int suffix = termSuffixesReader.readVInt();
+              term.length = termBlockPrefix + suffix;
+              if (term.bytes.length < term.length) {
+                term.grow(term.length);
+              }
+              termSuffixesReader.readBytes(term.bytes, termBlockPrefix, suffix);
+              return SeekStatus.NOT_FOUND;
+            } else {
+              common++;
+            }
+
+            continue;
+          }
+
+          // Test every term in this block
+          while (true) {
+            state.termBlockOrd++;
+            state.ord++;
+
+            final int suffix = termSuffixesReader.readVInt();
+            
+            // We know the prefix matches, so just compare the new suffix:
+            final int termLen = termBlockPrefix + suffix;
+            int bytePos = termSuffixesReader.getPosition();
+
+            boolean next = false;
+            final int limit = target.offset + (termLen < target.length ? termLen : target.length);
+            int targetPos = target.offset + termBlockPrefix;
+            while(targetPos < limit) {
+              final int cmp = (termSuffixes[bytePos++]&0xFF) - (target.bytes[targetPos++]&0xFF);
+              if (cmp < 0) {
+                // Current term is still before the target;
+                // keep scanning
+                next = true;
+                break;
+              } else if (cmp > 0) {
+                // Done!  Current term is after target. Stop
+                // here, fill in real term, return NOT_FOUND.
+                term.length = termBlockPrefix + suffix;
+                if (term.bytes.length < term.length) {
+                  term.grow(term.length);
+                }
+                termSuffixesReader.readBytes(term.bytes, termBlockPrefix, suffix);
+                //System.out.println("  NOT_FOUND");
+                return SeekStatus.NOT_FOUND;
+              }
+            }
+
+            if (!next && target.length <= termLen) {
+              term.length = termBlockPrefix + suffix;
+              if (term.bytes.length < term.length) {
+                term.grow(term.length);
+              }
+              termSuffixesReader.readBytes(term.bytes, termBlockPrefix, suffix);
+
+              if (target.length == termLen) {
+                // Done!  Exact match.  Stop here, fill in
+                // real term, return FOUND.
+                //System.out.println("  FOUND");
+
+                if (useCache) {
+                  // Store in cache
+                  decodeMetaData();
+                  //System.out.println("  cache! state=" + state);
+                  termsCache.put(new FieldAndTerm(fieldTerm), (BlockTermState) state.clone());
+                }
+
+                return SeekStatus.FOUND;
+              } else {
+                //System.out.println("  NOT_FOUND");
+                return SeekStatus.NOT_FOUND;
+              }
+            }
+
+            if (state.termBlockOrd == blockTermCount) {
+              // Must pre-fill term for next block's common prefix
+              term.length = termBlockPrefix + suffix;
+              if (term.bytes.length < term.length) {
+                term.grow(term.length);
+              }
+              termSuffixesReader.readBytes(term.bytes, termBlockPrefix, suffix);
+              break;
+            } else {
+              termSuffixesReader.skipBytes(suffix);
+            }
+          }
+
+          // The purpose of the terms dict index is to seek
+          // the enum to the closest index term before the
+          // term we are looking for.  So, we should never
+          // cross another index term (besides the first
+          // one) while we are scanning:
+
+          assert indexIsCurrent;
+
+          if (!nextBlock()) {
+            //System.out.println("  END");
+            indexIsCurrent = false;
+            return SeekStatus.END;
+          }
+          common = 0;
+        }
+      }
+
+      @Override
+      public BytesRef next() throws IOException {
+        //System.out.println("BTR.next() seekPending=" + seekPending + " pendingSeekCount=" + state.termBlockOrd);
+
+        // If seek was previously called and the term was cached,
+        // usually caller is just going to pull a D/&PEnum or get
+        // docFreq, etc.  But, if they then call next(),
+        // this method catches up all internal state so next()
+        // works properly:
+        if (seekPending) {
+          assert !indexIsCurrent;
+          in.seek(state.blockFilePointer);
+          final int pendingSeekCount = state.termBlockOrd;
+          boolean result = nextBlock();
+
+          final long savOrd = state.ord;
+
+          // Block must exist since seek(TermState) was called w/ a
+          // TermState previously returned by this enum when positioned
+          // on a real term:
+          assert result;
+
+          while(state.termBlockOrd < pendingSeekCount) {
+            BytesRef nextResult = _next();
+            assert nextResult != null;
+          }
+          seekPending = false;
+          state.ord = savOrd;
+        }
+        return _next();
+      }
+
+      /* Decodes only the term bytes of the next term.  If caller then asks for
+         metadata, ie docFreq, totalTermFreq or pulls a D/&PEnum, we then (lazily)
+         decode all metadata up to the current term. */
+      private BytesRef _next() throws IOException {
+        //System.out.println("BTR._next seg=" + segment + " this=" + this + " termCount=" + state.termBlockOrd + " (vs " + blockTermCount + ")");
+        if (state.termBlockOrd == blockTermCount && !nextBlock()) {
+          //System.out.println("  eof");
+          indexIsCurrent = false;
+          return null;
+        }
+
+        // TODO: cutover to something better for these ints!  simple64?
+        final int suffix = termSuffixesReader.readVInt();
+        //System.out.println("  suffix=" + suffix);
+
+        term.length = termBlockPrefix + suffix;
+        if (term.bytes.length < term.length) {
+          term.grow(term.length);
+        }
+        termSuffixesReader.readBytes(term.bytes, termBlockPrefix, suffix);
+        state.termBlockOrd++;
+
+        // NOTE: meaningless in the non-ord case
+        state.ord++;
+
+        //System.out.println("  return term=" + fieldInfo.name + ":" + term.utf8ToString() + " " + term + " tbOrd=" + state.termBlockOrd);
+        return term;
+      }
+
+      @Override
+      public BytesRef term() {
+        return term;
+      }
+
+      @Override
+      public int docFreq() throws IOException {
+        //System.out.println("BTR.docFreq");
+        decodeMetaData();
+        //System.out.println("  return " + state.docFreq);
+        return state.docFreq;
+      }
+
+      @Override
+      public long totalTermFreq() throws IOException {
+        decodeMetaData();
+        return state.totalTermFreq;
+      }
+
+      @Override
+      public DocsEnum docs(Bits liveDocs, DocsEnum reuse, int flags) throws IOException {
+        //System.out.println("BTR.docs this=" + this);
+        decodeMetaData();
+        //System.out.println("BTR.docs:  state.docFreq=" + state.docFreq);
+        return postingsReader.docs(fieldInfo, state, liveDocs, reuse, flags);
+      }
+
+      @Override
+      public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum reuse, int flags) throws IOException {
+        if (fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) < 0) {
+          // Positions were not indexed:
+          return null;
+        }
+
+        decodeMetaData();
+        return postingsReader.docsAndPositions(fieldInfo, state, liveDocs, reuse, flags);
+      }
+
+      @Override
+      public void seekExact(BytesRef target, TermState otherState) {
+        //System.out.println("BTR.seekExact termState target=" + target.utf8ToString() + " " + target + " this=" + this);
+        assert otherState != null && otherState instanceof BlockTermState;
+        assert !doOrd || ((BlockTermState) otherState).ord < numTerms;
+        state.copyFrom(otherState);
+        seekPending = true;
+        indexIsCurrent = false;
+        term.copyBytes(target);
+      }
+      
+      @Override
+      public TermState termState() throws IOException {
+        //System.out.println("BTR.termState this=" + this);
+        decodeMetaData();
+        TermState ts = state.clone();
+        //System.out.println("  return ts=" + ts);
+        return ts;
+      }
+
+      @Override
+      public void seekExact(long ord) throws IOException {
+        //System.out.println("BTR.seek by ord ord=" + ord);
+        if (indexEnum == null) {
+          throw new IllegalStateException("terms index was not loaded");
+        }
+
+        assert ord < numTerms;
+
+        // TODO: if ord is in same terms block and
+        // after current ord, we should avoid this seek just
+        // like we do in the seek(BytesRef) case
+        in.seek(indexEnum.seek(ord));
+        boolean result = nextBlock();
+
+        // Block must exist since ord < numTerms:
+        assert result;
+
+        indexIsCurrent = true;
+        didIndexNext = false;
+        blocksSinceSeek = 0;
+        seekPending = false;
+
+        state.ord = indexEnum.ord()-1;
+        assert state.ord >= -1: "ord=" + state.ord;
+        term.copyBytes(indexEnum.term());
+
+        // Now, scan:
+        int left = (int) (ord - state.ord);
+        while(left > 0) {
+          final BytesRef term = _next();
+          assert term != null;
+          left--;
+          assert indexIsCurrent;
+        }
+      }
+
+      @Override
+      public long ord() {
+        if (!doOrd) {
+          throw new UnsupportedOperationException();
+        }
+        return state.ord;
+      }
+
+      /* Does initial decode of next block of terms; this
+         doesn't actually decode the docFreq, totalTermFreq,
+         postings details (frq/prx offset, etc.) metadata;
+         it just loads them as byte[] blobs which are then      
+         decoded on-demand if the metadata is ever requested
+         for any term in this block.  This enables terms-only
+         intensive consumes (eg certain MTQs, respelling) to
+         not pay the price of decoding metadata they won't
+         use. */
+      private boolean nextBlock() throws IOException {
+
+        // TODO: we still lazy-decode the byte[] for each
+        // term (the suffix), but, if we decoded
+        // all N terms up front then seeking could do a fast
+        // bsearch w/in the block...
+
+        //System.out.println("BTR.nextBlock() fp=" + in.getFilePointer() + " this=" + this);
+        state.blockFilePointer = in.getFilePointer();
+        blockTermCount = in.readVInt();
+        //System.out.println("  blockTermCount=" + blockTermCount);
+        if (blockTermCount == 0) {
+          return false;
+        }
+        termBlockPrefix = in.readVInt();
+
+        // term suffixes:
+        int len = in.readVInt();
+        if (termSuffixes.length < len) {
+          termSuffixes = new byte[ArrayUtil.oversize(len, 1)];
+        }
+        //System.out.println("  termSuffixes len=" + len);
+        in.readBytes(termSuffixes, 0, len);
+        termSuffixesReader.reset(termSuffixes, 0, len);
+
+        // docFreq, totalTermFreq
+        len = in.readVInt();
+        if (docFreqBytes.length < len) {
+          docFreqBytes = new byte[ArrayUtil.oversize(len, 1)];
+        }
+        //System.out.println("  freq bytes len=" + len);
+        in.readBytes(docFreqBytes, 0, len);
+        freqReader.reset(docFreqBytes, 0, len);
+        metaDataUpto = 0;
+
+        state.termBlockOrd = 0;
+
+        postingsReader.readTermsBlock(in, fieldInfo, state);
+
+        blocksSinceSeek++;
+        indexIsCurrent = indexIsCurrent && (blocksSinceSeek < indexReader.getDivisor());
+        //System.out.println("  indexIsCurrent=" + indexIsCurrent);
+
+        return true;
+      }
+     
+      private void decodeMetaData() throws IOException {
+        //System.out.println("BTR.decodeMetadata mdUpto=" + metaDataUpto + " vs termCount=" + state.termBlockOrd + " state=" + state);
+        if (!seekPending) {
+          // TODO: cutover to random-access API
+          // here.... really stupid that we have to decode N
+          // wasted term metadata just to get to the N+1th
+          // that we really need...
+
+          // lazily catch up on metadata decode:
+          final int limit = state.termBlockOrd;
+          // We must set/incr state.termCount because
+          // postings impl can look at this
+          state.termBlockOrd = metaDataUpto;
+          // TODO: better API would be "jump straight to term=N"???
+          while (metaDataUpto < limit) {
+            //System.out.println("  decode mdUpto=" + metaDataUpto);
+            // TODO: we could make "tiers" of metadata, ie,
+            // decode docFreq/totalTF but don't decode postings
+            // metadata; this way caller could get
+            // docFreq/totalTF w/o paying decode cost for
+            // postings
+
+            // TODO: if docFreq were bulk decoded we could
+            // just skipN here:
+            state.docFreq = freqReader.readVInt();
+            //System.out.println("    dF=" + state.docFreq);
+            if (fieldInfo.getIndexOptions() != IndexOptions.DOCS_ONLY) {
+              state.totalTermFreq = state.docFreq + freqReader.readVLong();
+              //System.out.println("    totTF=" + state.totalTermFreq);
+            }
+
+            postingsReader.nextTerm(fieldInfo, state);
+            metaDataUpto++;
+            state.termBlockOrd++;
+          }
+        } else {
+          //System.out.println("  skip! seekPending");
+        }
+      }
+    }
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/BlockTermsWriter.java b/lucene/codecs/src/java/org/apache/lucene/codecs/BlockTermsWriter.java
new file mode 100644
index 0000000..daf49d4
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/BlockTermsWriter.java
@@ -0,0 +1,318 @@
+package org.apache.lucene.codecs;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Comparator;
+import java.util.List;
+
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.store.RAMOutputStream;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.RamUsageEstimator;
+
+// TODO: currently we encode all terms between two indexed
+// terms as a block; but, we could decouple the two, ie
+// allow several blocks in between two indexed terms
+
+/**
+ * Writes terms dict, block-encoding (column stride) each
+ * term's metadata for each set of terms between two
+ * index terms.
+ *
+ * @lucene.experimental
+ */
+
+public class BlockTermsWriter extends FieldsConsumer {
+
+  final static String CODEC_NAME = "BLOCK_TERMS_DICT";
+
+  // Initial format
+  public static final int VERSION_START = 0;
+
+  public static final int VERSION_CURRENT = VERSION_START;
+
+  /** Extension of terms file */
+  static final String TERMS_EXTENSION = "tib";
+
+  protected final IndexOutput out;
+  final PostingsWriterBase postingsWriter;
+  final FieldInfos fieldInfos;
+  FieldInfo currentField;
+  private final TermsIndexWriterBase termsIndexWriter;
+  private final List<TermsWriter> fields = new ArrayList<TermsWriter>();
+
+  // private final String segment;
+
+  public BlockTermsWriter(TermsIndexWriterBase termsIndexWriter,
+      SegmentWriteState state, PostingsWriterBase postingsWriter)
+      throws IOException {
+    final String termsFileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, TERMS_EXTENSION);
+    this.termsIndexWriter = termsIndexWriter;
+    out = state.directory.createOutput(termsFileName, state.context);
+    boolean success = false;
+    try {
+      fieldInfos = state.fieldInfos;
+      writeHeader(out);
+      currentField = null;
+      this.postingsWriter = postingsWriter;
+      // segment = state.segmentName;
+      
+      //System.out.println("BTW.init seg=" + state.segmentName);
+      
+      postingsWriter.start(out); // have consumer write its format/header
+      success = true;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(out);
+      }
+    }
+  }
+  
+  protected void writeHeader(IndexOutput out) throws IOException {
+    CodecUtil.writeHeader(out, CODEC_NAME, VERSION_CURRENT); 
+
+    out.writeLong(0);                             // leave space for end index pointer    
+  }
+
+  @Override
+  public TermsConsumer addField(FieldInfo field) throws IOException {
+    //System.out.println("\nBTW.addField seg=" + segment + " field=" + field.name);
+    assert currentField == null || currentField.name.compareTo(field.name) < 0;
+    currentField = field;
+    TermsIndexWriterBase.FieldWriter fieldIndexWriter = termsIndexWriter.addField(field, out.getFilePointer());
+    final TermsWriter terms = new TermsWriter(fieldIndexWriter, field, postingsWriter);
+    fields.add(terms);
+    return terms;
+  }
+
+  @Override
+  public void close() throws IOException {
+
+    try {
+      
+      int nonZeroCount = 0;
+      for(TermsWriter field : fields) {
+        if (field.numTerms > 0) {
+          nonZeroCount++;
+        }
+      }
+
+      final long dirStart = out.getFilePointer();
+
+      out.writeVInt(nonZeroCount);
+      for(TermsWriter field : fields) {
+        if (field.numTerms > 0) {
+          out.writeVInt(field.fieldInfo.number);
+          out.writeVLong(field.numTerms);
+          out.writeVLong(field.termsStartPointer);
+          if (field.fieldInfo.getIndexOptions() != IndexOptions.DOCS_ONLY) {
+            out.writeVLong(field.sumTotalTermFreq);
+          }
+          out.writeVLong(field.sumDocFreq);
+          out.writeVInt(field.docCount);
+        }
+      }
+      writeTrailer(dirStart);
+    } finally {
+      IOUtils.close(out, postingsWriter, termsIndexWriter);
+    }
+  }
+
+  protected void writeTrailer(long dirStart) throws IOException {
+    out.seek(CodecUtil.headerLength(CODEC_NAME));
+    out.writeLong(dirStart);    
+  }
+  
+  private static class TermEntry {
+    public final BytesRef term = new BytesRef();
+    public TermStats stats;
+  }
+
+  class TermsWriter extends TermsConsumer {
+    private final FieldInfo fieldInfo;
+    private final PostingsWriterBase postingsWriter;
+    private final long termsStartPointer;
+    private long numTerms;
+    private final TermsIndexWriterBase.FieldWriter fieldIndexWriter;
+    long sumTotalTermFreq;
+    long sumDocFreq;
+    int docCount;
+
+    private TermEntry[] pendingTerms;
+
+    private int pendingCount;
+
+    TermsWriter(
+        TermsIndexWriterBase.FieldWriter fieldIndexWriter,
+        FieldInfo fieldInfo,
+        PostingsWriterBase postingsWriter) 
+    {
+      this.fieldInfo = fieldInfo;
+      this.fieldIndexWriter = fieldIndexWriter;
+      pendingTerms = new TermEntry[32];
+      for(int i=0;i<pendingTerms.length;i++) {
+        pendingTerms[i] = new TermEntry();
+      }
+      termsStartPointer = out.getFilePointer();
+      postingsWriter.setField(fieldInfo);
+      this.postingsWriter = postingsWriter;
+    }
+    
+    @Override
+    public Comparator<BytesRef> getComparator() {
+      return BytesRef.getUTF8SortedAsUnicodeComparator();
+    }
+
+    @Override
+    public PostingsConsumer startTerm(BytesRef text) throws IOException {
+      //System.out.println("BTW: startTerm term=" + fieldInfo.name + ":" + text.utf8ToString() + " " + text + " seg=" + segment);
+      postingsWriter.startTerm();
+      return postingsWriter;
+    }
+
+    private final BytesRef lastPrevTerm = new BytesRef();
+
+    @Override
+    public void finishTerm(BytesRef text, TermStats stats) throws IOException {
+
+      assert stats.docFreq > 0;
+      //System.out.println("BTW: finishTerm term=" + fieldInfo.name + ":" + text.utf8ToString() + " " + text + " seg=" + segment + " df=" + stats.docFreq);
+
+      final boolean isIndexTerm = fieldIndexWriter.checkIndexTerm(text, stats);
+
+      if (isIndexTerm) {
+        if (pendingCount > 0) {
+          // Instead of writing each term, live, we gather terms
+          // in RAM in a pending buffer, and then write the
+          // entire block in between index terms:
+          flushBlock();
+        }
+        fieldIndexWriter.add(text, stats, out.getFilePointer());
+        //System.out.println("  index term!");
+      }
+
+      if (pendingTerms.length == pendingCount) {
+        final TermEntry[] newArray = new TermEntry[ArrayUtil.oversize(pendingCount+1, RamUsageEstimator.NUM_BYTES_OBJECT_REF)];
+        System.arraycopy(pendingTerms, 0, newArray, 0, pendingCount);
+        for(int i=pendingCount;i<newArray.length;i++) {
+          newArray[i] = new TermEntry();
+        }
+        pendingTerms = newArray;
+      }
+      final TermEntry te = pendingTerms[pendingCount];
+      te.term.copyBytes(text);
+      te.stats = stats;
+
+      pendingCount++;
+
+      postingsWriter.finishTerm(stats);
+      numTerms++;
+    }
+
+    // Finishes all terms in this field
+    @Override
+    public void finish(long sumTotalTermFreq, long sumDocFreq, int docCount) throws IOException {
+      if (pendingCount > 0) {
+        flushBlock();
+      }
+      // EOF marker:
+      out.writeVInt(0);
+
+      this.sumTotalTermFreq = sumTotalTermFreq;
+      this.sumDocFreq = sumDocFreq;
+      this.docCount = docCount;
+      fieldIndexWriter.finish(out.getFilePointer());
+    }
+
+    private int sharedPrefix(BytesRef term1, BytesRef term2) {
+      assert term1.offset == 0;
+      assert term2.offset == 0;
+      int pos1 = 0;
+      int pos1End = pos1 + Math.min(term1.length, term2.length);
+      int pos2 = 0;
+      while(pos1 < pos1End) {
+        if (term1.bytes[pos1] != term2.bytes[pos2]) {
+          return pos1;
+        }
+        pos1++;
+        pos2++;
+      }
+      return pos1;
+    }
+
+    private final RAMOutputStream bytesWriter = new RAMOutputStream();
+
+    private void flushBlock() throws IOException {
+      //System.out.println("BTW.flushBlock seg=" + segment + " pendingCount=" + pendingCount + " fp=" + out.getFilePointer());
+
+      // First pass: compute common prefix for all terms
+      // in the block, against term before first term in
+      // this block:
+      int commonPrefix = sharedPrefix(lastPrevTerm, pendingTerms[0].term);
+      for(int termCount=1;termCount<pendingCount;termCount++) {
+        commonPrefix = Math.min(commonPrefix,
+                                sharedPrefix(lastPrevTerm,
+                                             pendingTerms[termCount].term));
+      }        
+
+      out.writeVInt(pendingCount);
+      out.writeVInt(commonPrefix);
+
+      // 2nd pass: write suffixes, as separate byte[] blob
+      for(int termCount=0;termCount<pendingCount;termCount++) {
+        final int suffix = pendingTerms[termCount].term.length - commonPrefix;
+        // TODO: cutover to better intblock codec, instead
+        // of interleaving here:
+        bytesWriter.writeVInt(suffix);
+        bytesWriter.writeBytes(pendingTerms[termCount].term.bytes, commonPrefix, suffix);
+      }
+      out.writeVInt((int) bytesWriter.getFilePointer());
+      bytesWriter.writeTo(out);
+      bytesWriter.reset();
+
+      // 3rd pass: write the freqs as byte[] blob
+      // TODO: cutover to better intblock codec.  simple64?
+      // write prefix, suffix first:
+      for(int termCount=0;termCount<pendingCount;termCount++) {
+        final TermStats stats = pendingTerms[termCount].stats;
+        assert stats != null;
+        bytesWriter.writeVInt(stats.docFreq);
+        if (fieldInfo.getIndexOptions() != IndexOptions.DOCS_ONLY) {
+          bytesWriter.writeVLong(stats.totalTermFreq-stats.docFreq);
+        }
+      }
+
+      out.writeVInt((int) bytesWriter.getFilePointer());
+      bytesWriter.writeTo(out);
+      bytesWriter.reset();
+
+      postingsWriter.flushTermsBlock(pendingCount, pendingCount);
+      lastPrevTerm.copyBytes(pendingTerms[pendingCount-1].term);
+      pendingCount = 0;
+    }
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/FixedGapTermsIndexReader.java b/lucene/codecs/src/java/org/apache/lucene/codecs/FixedGapTermsIndexReader.java
new file mode 100644
index 0000000..c655b93
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/FixedGapTermsIndexReader.java
@@ -0,0 +1,414 @@
+package org.apache.lucene.codecs;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.PagedBytes;
+import org.apache.lucene.util.packed.PackedInts;
+
+import java.util.HashMap;
+import java.util.Comparator;
+import java.io.IOException;
+
+import org.apache.lucene.index.IndexFileNames;
+
+/** 
+ * TermsIndexReader for simple every Nth terms indexes.
+ *
+ * @see FixedGapTermsIndexWriter
+ * @lucene.experimental 
+ */
+public class FixedGapTermsIndexReader extends TermsIndexReaderBase {
+
+  // NOTE: long is overkill here, since this number is 128
+  // by default and only indexDivisor * 128 if you change
+  // the indexDivisor at search time.  But, we use this in a
+  // number of places to multiply out the actual ord, and we
+  // will overflow int during those multiplies.  So to avoid
+  // having to upgrade each multiple to long in multiple
+  // places (error prone), we use long here:
+  private long totalIndexInterval;
+
+  private int indexDivisor;
+  final private int indexInterval;
+
+  // Closed if indexLoaded is true:
+  private IndexInput in;
+  private volatile boolean indexLoaded;
+
+  private final Comparator<BytesRef> termComp;
+
+  private final static int PAGED_BYTES_BITS = 15;
+
+  // all fields share this single logical byte[]
+  private final PagedBytes termBytes = new PagedBytes(PAGED_BYTES_BITS);
+  private PagedBytes.Reader termBytesReader;
+
+  final HashMap<FieldInfo,FieldIndexData> fields = new HashMap<FieldInfo,FieldIndexData>();
+  
+  // start of the field info data
+  protected long dirOffset;
+
+  public FixedGapTermsIndexReader(Directory dir, FieldInfos fieldInfos, String segment, int indexDivisor, Comparator<BytesRef> termComp, String segmentSuffix, IOContext context)
+    throws IOException {
+
+    this.termComp = termComp;
+
+    assert indexDivisor == -1 || indexDivisor > 0;
+
+    in = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, FixedGapTermsIndexWriter.TERMS_INDEX_EXTENSION), context);
+    
+    boolean success = false;
+
+    try {
+      
+      readHeader(in);
+      indexInterval = in.readInt();
+      if (indexInterval < 1) {
+        throw new CorruptIndexException("invalid indexInterval: " + indexInterval + " (resource=" + in + ")");
+      }
+      this.indexDivisor = indexDivisor;
+
+      if (indexDivisor < 0) {
+        totalIndexInterval = indexInterval;
+      } else {
+        // In case terms index gets loaded, later, on demand
+        totalIndexInterval = indexInterval * indexDivisor;
+      }
+      assert totalIndexInterval > 0;
+      
+      seekDir(in, dirOffset);
+
+      // Read directory
+      final int numFields = in.readVInt();     
+      if (numFields < 0) {
+        throw new CorruptIndexException("invalid numFields: " + numFields + " (resource=" + in + ")");
+      }
+      //System.out.println("FGR: init seg=" + segment + " div=" + indexDivisor + " nF=" + numFields);
+      for(int i=0;i<numFields;i++) {
+        final int field = in.readVInt();
+        final int numIndexTerms = in.readVInt();
+        if (numIndexTerms < 0) {
+          throw new CorruptIndexException("invalid numIndexTerms: " + numIndexTerms + " (resource=" + in + ")");
+        }
+        final long termsStart = in.readVLong();
+        final long indexStart = in.readVLong();
+        final long packedIndexStart = in.readVLong();
+        final long packedOffsetsStart = in.readVLong();
+        if (packedIndexStart < indexStart) {
+          throw new CorruptIndexException("invalid packedIndexStart: " + packedIndexStart + " indexStart: " + indexStart + "numIndexTerms: " + numIndexTerms + " (resource=" + in + ")");
+        }
+        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);
+        FieldIndexData previous = fields.put(fieldInfo, new FieldIndexData(fieldInfo, numIndexTerms, indexStart, termsStart, packedIndexStart, packedOffsetsStart));
+        if (previous != null) {
+          throw new CorruptIndexException("duplicate field: " + fieldInfo.name + " (resource=" + in + ")");
+        }
+      }
+      success = true;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(in);
+      }
+      if (indexDivisor > 0) {
+        in.close();
+        in = null;
+        if (success) {
+          indexLoaded = true;
+        }
+        termBytesReader = termBytes.freeze(true);
+      }
+    }
+  }
+  
+  @Override
+  public int getDivisor() {
+    return indexDivisor;
+  }
+
+  protected void readHeader(IndexInput input) throws IOException {
+    CodecUtil.checkHeader(input, FixedGapTermsIndexWriter.CODEC_NAME,
+      FixedGapTermsIndexWriter.VERSION_START, FixedGapTermsIndexWriter.VERSION_START);
+    dirOffset = input.readLong();
+  }
+
+  private class IndexEnum extends FieldIndexEnum {
+    private final FieldIndexData.CoreFieldIndex fieldIndex;
+    private final BytesRef term = new BytesRef();
+    private long ord;
+
+    public IndexEnum(FieldIndexData.CoreFieldIndex fieldIndex) {
+      this.fieldIndex = fieldIndex;
+    }
+
+    @Override
+    public BytesRef term() {
+      return term;
+    }
+
+    @Override
+    public long seek(BytesRef target) {
+      int lo = 0;				  // binary search
+      int hi = fieldIndex.numIndexTerms - 1;
+      assert totalIndexInterval > 0 : "totalIndexInterval=" + totalIndexInterval;
+
+      while (hi >= lo) {
+        int mid = (lo + hi) >>> 1;
+
+        final long offset = fieldIndex.termOffsets.get(mid);
+        final int length = (int) (fieldIndex.termOffsets.get(1+mid) - offset);
+        termBytesReader.fillSlice(term, fieldIndex.termBytesStart + offset, length);
+
+        int delta = termComp.compare(target, term);
+        if (delta < 0) {
+          hi = mid - 1;
+        } else if (delta > 0) {
+          lo = mid + 1;
+        } else {
+          assert mid >= 0;
+          ord = mid*totalIndexInterval;
+          return fieldIndex.termsStart + fieldIndex.termsDictOffsets.get(mid);
+        }
+      }
+
+      if (hi < 0) {
+        assert hi == -1;
+        hi = 0;
+      }
+
+      final long offset = fieldIndex.termOffsets.get(hi);
+      final int length = (int) (fieldIndex.termOffsets.get(1+hi) - offset);
+      termBytesReader.fillSlice(term, fieldIndex.termBytesStart + offset, length);
+
+      ord = hi*totalIndexInterval;
+      return fieldIndex.termsStart + fieldIndex.termsDictOffsets.get(hi);
+    }
+
+    @Override
+    public long next() {
+      final int idx = 1 + (int) (ord / totalIndexInterval);
+      if (idx >= fieldIndex.numIndexTerms) {
+        return -1;
+      }
+      ord += totalIndexInterval;
+
+      final long offset = fieldIndex.termOffsets.get(idx);
+      final int length = (int) (fieldIndex.termOffsets.get(1+idx) - offset);
+      termBytesReader.fillSlice(term, fieldIndex.termBytesStart + offset, length);
+      return fieldIndex.termsStart + fieldIndex.termsDictOffsets.get(idx);
+    }
+
+    @Override
+    public long ord() {
+      return ord;
+    }
+
+    @Override
+    public long seek(long ord) {
+      int idx = (int) (ord / totalIndexInterval);
+      // caller must ensure ord is in bounds
+      assert idx < fieldIndex.numIndexTerms;
+      final long offset = fieldIndex.termOffsets.get(idx);
+      final int length = (int) (fieldIndex.termOffsets.get(1+idx) - offset);
+      termBytesReader.fillSlice(term, fieldIndex.termBytesStart + offset, length);
+      this.ord = idx * totalIndexInterval;
+      return fieldIndex.termsStart + fieldIndex.termsDictOffsets.get(idx);
+    }
+  }
+
+  @Override
+  public boolean supportsOrd() {
+    return true;
+  }
+
+  private final class FieldIndexData {
+
+    volatile CoreFieldIndex coreIndex;
+
+    private final long indexStart;
+    private final long termsStart;
+    private final long packedIndexStart;
+    private final long packedOffsetsStart;
+
+    private final int numIndexTerms;
+
+    public FieldIndexData(FieldInfo fieldInfo, int numIndexTerms, long indexStart, long termsStart, long packedIndexStart,
+                          long packedOffsetsStart) throws IOException {
+
+      this.termsStart = termsStart;
+      this.indexStart = indexStart;
+      this.packedIndexStart = packedIndexStart;
+      this.packedOffsetsStart = packedOffsetsStart;
+      this.numIndexTerms = numIndexTerms;
+
+      if (indexDivisor > 0) {
+        loadTermsIndex();
+      }
+    }
+
+    private void loadTermsIndex() throws IOException {
+      if (coreIndex == null) {
+        coreIndex = new CoreFieldIndex(indexStart, termsStart, packedIndexStart, packedOffsetsStart, numIndexTerms);
+      }
+    }
+
+    private final class CoreFieldIndex {
+
+      // where this field's terms begin in the packed byte[]
+      // data
+      final long termBytesStart;
+
+      // offset into index termBytes
+      final PackedInts.Reader termOffsets;
+
+      // index pointers into main terms dict
+      final PackedInts.Reader termsDictOffsets;
+
+      final int numIndexTerms;
+      final long termsStart;
+
+      public CoreFieldIndex(long indexStart, long termsStart, long packedIndexStart, long packedOffsetsStart, int numIndexTerms) throws IOException {
+
+        this.termsStart = termsStart;
+        termBytesStart = termBytes.getPointer();
+
+        IndexInput clone = in.clone();
+        clone.seek(indexStart);
+
+        // -1 is passed to mean "don't load term index", but
+        // if we are then later loaded it's overwritten with
+        // a real value
+        assert indexDivisor > 0;
+
+        this.numIndexTerms = 1+(numIndexTerms-1) / indexDivisor;
+
+        assert this.numIndexTerms  > 0: "numIndexTerms=" + numIndexTerms + " indexDivisor=" + indexDivisor;
+
+        if (indexDivisor == 1) {
+          // Default (load all index terms) is fast -- slurp in the images from disk:
+          
+          try {
+            final long numTermBytes = packedIndexStart - indexStart;
+            termBytes.copy(clone, numTermBytes);
+
+            // records offsets into main terms dict file
+            termsDictOffsets = PackedInts.getReader(clone);
+            assert termsDictOffsets.size() == numIndexTerms;
+
+            // records offsets into byte[] term data
+            termOffsets = PackedInts.getReader(clone);
+            assert termOffsets.size() == 1+numIndexTerms;
+          } finally {
+            clone.close();
+          }
+        } else {
+          // Get packed iterators
+          final IndexInput clone1 = in.clone();
+          final IndexInput clone2 = in.clone();
+
+          try {
+            // Subsample the index terms
+            clone1.seek(packedIndexStart);
+            final PackedInts.ReaderIterator termsDictOffsetsIter = PackedInts.getReaderIterator(clone1, PackedInts.DEFAULT_BUFFER_SIZE);
+
+            clone2.seek(packedOffsetsStart);
+            final PackedInts.ReaderIterator termOffsetsIter = PackedInts.getReaderIterator(clone2,  PackedInts.DEFAULT_BUFFER_SIZE);
+
+            // TODO: often we can get by w/ fewer bits per
+            // value, below.. .but this'd be more complex:
+            // we'd have to try @ fewer bits and then grow
+            // if we overflowed it.
+
+            PackedInts.Mutable termsDictOffsetsM = PackedInts.getMutable(this.numIndexTerms, termsDictOffsetsIter.getBitsPerValue(), PackedInts.DEFAULT);
+            PackedInts.Mutable termOffsetsM = PackedInts.getMutable(this.numIndexTerms+1, termOffsetsIter.getBitsPerValue(), PackedInts.DEFAULT);
+
+            termsDictOffsets = termsDictOffsetsM;
+            termOffsets = termOffsetsM;
+
+            int upto = 0;
+
+            long termOffsetUpto = 0;
+
+            while(upto < this.numIndexTerms) {
+              // main file offset copies straight over
+              termsDictOffsetsM.set(upto, termsDictOffsetsIter.next());
+
+              termOffsetsM.set(upto, termOffsetUpto);
+
+              long termOffset = termOffsetsIter.next();
+              long nextTermOffset = termOffsetsIter.next();
+              final int numTermBytes = (int) (nextTermOffset - termOffset);
+
+              clone.seek(indexStart + termOffset);
+              assert indexStart + termOffset < clone.length() : "indexStart=" + indexStart + " termOffset=" + termOffset + " len=" + clone.length();
+              assert indexStart + termOffset + numTermBytes < clone.length();
+
+              termBytes.copy(clone, numTermBytes);
+              termOffsetUpto += numTermBytes;
+
+              upto++;
+              if (upto == this.numIndexTerms) {
+                break;
+              }
+
+              // skip terms:
+              termsDictOffsetsIter.next();
+              for(int i=0;i<indexDivisor-2;i++) {
+                termOffsetsIter.next();
+                termsDictOffsetsIter.next();
+              }
+            }
+            termOffsetsM.set(upto, termOffsetUpto);
+
+          } finally {
+            clone1.close();
+            clone2.close();
+            clone.close();
+          }
+        }
+      }
+    }
+  }
+
+  @Override
+  public FieldIndexEnum getFieldEnum(FieldInfo fieldInfo) {
+    final FieldIndexData fieldData = fields.get(fieldInfo);
+    if (fieldData.coreIndex == null) {
+      return null;
+    } else {
+      return new IndexEnum(fieldData.coreIndex);
+    }
+  }
+
+  @Override
+  public void close() throws IOException {
+    if (in != null && !indexLoaded) {
+      in.close();
+    }
+  }
+
+  protected void seekDir(IndexInput input, long dirOffset) throws IOException {
+    input.seek(dirOffset);
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/FixedGapTermsIndexWriter.java b/lucene/codecs/src/java/org/apache/lucene/codecs/FixedGapTermsIndexWriter.java
new file mode 100644
index 0000000..fa82964
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/FixedGapTermsIndexWriter.java
@@ -0,0 +1,255 @@
+package org.apache.lucene.codecs;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.packed.PackedInts;
+
+import java.util.List;
+import java.util.ArrayList;
+import java.io.IOException;
+
+/**
+ * Selects every Nth term as and index term, and hold term
+ * bytes (mostly) fully expanded in memory.  This terms index
+ * supports seeking by ord.  See {@link
+ * VariableGapTermsIndexWriter} for a more memory efficient
+ * terms index that does not support seeking by ord.
+ *
+ * @lucene.experimental */
+public class FixedGapTermsIndexWriter extends TermsIndexWriterBase {
+  protected final IndexOutput out;
+
+  /** Extension of terms index file */
+  static final String TERMS_INDEX_EXTENSION = "tii";
+
+  final static String CODEC_NAME = "SIMPLE_STANDARD_TERMS_INDEX";
+  final static int VERSION_START = 0;
+  final static int VERSION_CURRENT = VERSION_START;
+
+  final private int termIndexInterval;
+
+  private final List<SimpleFieldWriter> fields = new ArrayList<SimpleFieldWriter>();
+  
+  @SuppressWarnings("unused") private final FieldInfos fieldInfos; // unread
+
+  public FixedGapTermsIndexWriter(SegmentWriteState state) throws IOException {
+    final String indexFileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, TERMS_INDEX_EXTENSION);
+    termIndexInterval = state.termIndexInterval;
+    out = state.directory.createOutput(indexFileName, state.context);
+    boolean success = false;
+    try {
+      fieldInfos = state.fieldInfos;
+      writeHeader(out);
+      out.writeInt(termIndexInterval);
+      success = true;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(out);
+      }
+    }
+  }
+  
+  protected void writeHeader(IndexOutput out) throws IOException {
+    CodecUtil.writeHeader(out, CODEC_NAME, VERSION_CURRENT);
+    // Placeholder for dir offset
+    out.writeLong(0);
+  }
+
+  @Override
+  public FieldWriter addField(FieldInfo field, long termsFilePointer) {
+    //System.out.println("FGW: addFfield=" + field.name);
+    SimpleFieldWriter writer = new SimpleFieldWriter(field, termsFilePointer);
+    fields.add(writer);
+    return writer;
+  }
+
+  /** NOTE: if your codec does not sort in unicode code
+   *  point order, you must override this method, to simply
+   *  return indexedTerm.length. */
+  protected int indexedTermPrefixLength(final BytesRef priorTerm, final BytesRef indexedTerm) {
+    // As long as codec sorts terms in unicode codepoint
+    // order, we can safely strip off the non-distinguishing
+    // suffix to save RAM in the loaded terms index.
+    final int idxTermOffset = indexedTerm.offset;
+    final int priorTermOffset = priorTerm.offset;
+    final int limit = Math.min(priorTerm.length, indexedTerm.length);
+    for(int byteIdx=0;byteIdx<limit;byteIdx++) {
+      if (priorTerm.bytes[priorTermOffset+byteIdx] != indexedTerm.bytes[idxTermOffset+byteIdx]) {
+        return byteIdx+1;
+      }
+    }
+    return Math.min(1+priorTerm.length, indexedTerm.length);
+  }
+
+  private class SimpleFieldWriter extends FieldWriter {
+    final FieldInfo fieldInfo;
+    int numIndexTerms;
+    final long indexStart;
+    final long termsStart;
+    long packedIndexStart;
+    long packedOffsetsStart;
+    private long numTerms;
+
+    // TODO: we could conceivably make a PackedInts wrapper
+    // that auto-grows... then we wouldn't force 6 bytes RAM
+    // per index term:
+    private short[] termLengths;
+    private int[] termsPointerDeltas;
+    private long lastTermsPointer;
+    private long totTermLength;
+
+    private final BytesRef lastTerm = new BytesRef();
+
+    SimpleFieldWriter(FieldInfo fieldInfo, long termsFilePointer) {
+      this.fieldInfo = fieldInfo;
+      indexStart = out.getFilePointer();
+      termsStart = lastTermsPointer = termsFilePointer;
+      termLengths = new short[0];
+      termsPointerDeltas = new int[0];
+    }
+
+    @Override
+    public boolean checkIndexTerm(BytesRef text, TermStats stats) throws IOException {
+      // First term is first indexed term:
+      //System.out.println("FGW: checkIndexTerm text=" + text.utf8ToString());
+      if (0 == (numTerms++ % termIndexInterval)) {
+        return true;
+      } else {
+        if (0 == numTerms % termIndexInterval) {
+          // save last term just before next index term so we
+          // can compute wasted suffix
+          lastTerm.copyBytes(text);
+        }
+        return false;
+      }
+    }
+
+    @Override
+    public void add(BytesRef text, TermStats stats, long termsFilePointer) throws IOException {
+      final int indexedTermLength = indexedTermPrefixLength(lastTerm, text);
+      //System.out.println("FGW: add text=" + text.utf8ToString() + " " + text + " fp=" + termsFilePointer);
+
+      // write only the min prefix that shows the diff
+      // against prior term
+      out.writeBytes(text.bytes, text.offset, indexedTermLength);
+
+      if (termLengths.length == numIndexTerms) {
+        termLengths = ArrayUtil.grow(termLengths);
+      }
+      if (termsPointerDeltas.length == numIndexTerms) {
+        termsPointerDeltas = ArrayUtil.grow(termsPointerDeltas);
+      }
+
+      // save delta terms pointer
+      termsPointerDeltas[numIndexTerms] = (int) (termsFilePointer - lastTermsPointer);
+      lastTermsPointer = termsFilePointer;
+
+      // save term length (in bytes)
+      assert indexedTermLength <= Short.MAX_VALUE;
+      termLengths[numIndexTerms] = (short) indexedTermLength;
+      totTermLength += indexedTermLength;
+
+      lastTerm.copyBytes(text);
+      numIndexTerms++;
+    }
+
+    @Override
+    public void finish(long termsFilePointer) throws IOException {
+
+      // write primary terms dict offsets
+      packedIndexStart = out.getFilePointer();
+
+      PackedInts.Writer w = PackedInts.getWriter(out, numIndexTerms, PackedInts.bitsRequired(termsFilePointer), PackedInts.DEFAULT);
+
+      // relative to our indexStart
+      long upto = 0;
+      for(int i=0;i<numIndexTerms;i++) {
+        upto += termsPointerDeltas[i];
+        w.add(upto);
+      }
+      w.finish();
+
+      packedOffsetsStart = out.getFilePointer();
+
+      // write offsets into the byte[] terms
+      w = PackedInts.getWriter(out, 1+numIndexTerms, PackedInts.bitsRequired(totTermLength), PackedInts.DEFAULT);
+      upto = 0;
+      for(int i=0;i<numIndexTerms;i++) {
+        w.add(upto);
+        upto += termLengths[i];
+      }
+      w.add(upto);
+      w.finish();
+
+      // our referrer holds onto us, while other fields are
+      // being written, so don't tie up this RAM:
+      termLengths = null;
+      termsPointerDeltas = null;
+    }
+  }
+
+  public void close() throws IOException {
+    boolean success = false;
+    try {
+      final long dirStart = out.getFilePointer();
+      final int fieldCount = fields.size();
+      
+      int nonNullFieldCount = 0;
+      for(int i=0;i<fieldCount;i++) {
+        SimpleFieldWriter field = fields.get(i);
+        if (field.numIndexTerms > 0) {
+          nonNullFieldCount++;
+        }
+      }
+      
+      out.writeVInt(nonNullFieldCount);
+      for(int i=0;i<fieldCount;i++) {
+        SimpleFieldWriter field = fields.get(i);
+        if (field.numIndexTerms > 0) {
+          out.writeVInt(field.fieldInfo.number);
+          out.writeVInt(field.numIndexTerms);
+          out.writeVLong(field.termsStart);
+          out.writeVLong(field.indexStart);
+          out.writeVLong(field.packedIndexStart);
+          out.writeVLong(field.packedOffsetsStart);
+        }
+      }
+      writeTrailer(dirStart);
+      success = true;
+    } finally {
+      if (success) {
+        IOUtils.close(out);
+      } else {
+        IOUtils.closeWhileHandlingException(out);
+      }
+    }
+  }
+
+  protected void writeTrailer(long dirStart) throws IOException {
+    out.seek(CodecUtil.headerLength(CODEC_NAME));
+    out.writeLong(dirStart);
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/TermsIndexReaderBase.java b/lucene/codecs/src/java/org/apache/lucene/codecs/TermsIndexReaderBase.java
new file mode 100644
index 0000000..3845305
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/TermsIndexReaderBase.java
@@ -0,0 +1,74 @@
+package org.apache.lucene.codecs;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.util.BytesRef;
+
+import java.io.IOException;
+import java.io.Closeable;
+
+
+// TODO
+//   - allow for non-regular index intervals?  eg with a
+//     long string of rare terms, you don't need such
+//     frequent indexing
+
+/**
+ * {@link BlockTermsReader} interacts with an instance of this class
+ * to manage its terms index.  The writer must accept
+ * indexed terms (many pairs of BytesRef text + long
+ * fileOffset), and then this reader must be able to
+ * retrieve the nearest index term to a provided term
+ * text. 
+ * @lucene.experimental */
+
+public abstract class TermsIndexReaderBase implements Closeable {
+
+  public abstract FieldIndexEnum getFieldEnum(FieldInfo fieldInfo);
+
+  public abstract void close() throws IOException;
+
+  public abstract boolean supportsOrd();
+
+  public abstract int getDivisor();
+
+  /** 
+   * Similar to TermsEnum, except, the only "metadata" it
+   * reports for a given indexed term is the long fileOffset
+   * into the main terms dictionary file.
+   */
+  public static abstract class FieldIndexEnum {
+
+    /** Seeks to "largest" indexed term that's <=
+     *  term; returns file pointer index (into the main
+     *  terms index file) for that term */
+    public abstract long seek(BytesRef term) throws IOException;
+
+    /** Returns -1 at end */
+    public abstract long next() throws IOException;
+
+    public abstract BytesRef term();
+
+    /** Only implemented if {@link TermsIndexReaderBase#supportsOrd()} returns true. */
+    public abstract long seek(long ord) throws IOException;
+    
+    /** Only implemented if {@link TermsIndexReaderBase#supportsOrd()} returns true. */
+    public abstract long ord();
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/TermsIndexWriterBase.java b/lucene/codecs/src/java/org/apache/lucene/codecs/TermsIndexWriterBase.java
new file mode 100644
index 0000000..477dfde
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/TermsIndexWriterBase.java
@@ -0,0 +1,45 @@
+package org.apache.lucene.codecs;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.util.BytesRef;
+
+import java.io.Closeable;
+import java.io.IOException;
+
+/** 
+ * Base class for terms index implementations to plug
+ * into {@link BlockTermsWriter}.
+ * 
+ * @see TermsIndexReaderBase
+ * @lucene.experimental 
+ */
+public abstract class TermsIndexWriterBase implements Closeable {
+
+  /**
+   * Terms index API for a single field.
+   */
+  public abstract class FieldWriter {
+    public abstract boolean checkIndexTerm(BytesRef text, TermStats stats) throws IOException;
+    public abstract void add(BytesRef text, TermStats stats, long termsFilePointer) throws IOException;
+    public abstract void finish(long termsFilePointer) throws IOException;
+  }
+
+  public abstract FieldWriter addField(FieldInfo fieldInfo, long termsFilePointer) throws IOException;
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/VariableGapTermsIndexReader.java b/lucene/codecs/src/java/org/apache/lucene/codecs/VariableGapTermsIndexReader.java
new file mode 100644
index 0000000..6a78349
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/VariableGapTermsIndexReader.java
@@ -0,0 +1,234 @@
+package org.apache.lucene.codecs;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.io.FileOutputStream;   // for toDot
+import java.io.OutputStreamWriter; // for toDot
+import java.io.Writer;             // for toDot
+import java.util.HashMap;
+
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IntsRef;
+import org.apache.lucene.util.fst.Builder;
+import org.apache.lucene.util.fst.BytesRefFSTEnum;
+import org.apache.lucene.util.fst.FST;
+import org.apache.lucene.util.fst.PositiveIntOutputs;
+import org.apache.lucene.util.fst.Util; // for toDot
+
+/** See {@link VariableGapTermsIndexWriter}
+ * 
+ * @lucene.experimental */
+public class VariableGapTermsIndexReader extends TermsIndexReaderBase {
+
+  private final PositiveIntOutputs fstOutputs = PositiveIntOutputs.getSingleton(true);
+  private int indexDivisor;
+
+  // Closed if indexLoaded is true:
+  private IndexInput in;
+  private volatile boolean indexLoaded;
+
+  final HashMap<FieldInfo,FieldIndexData> fields = new HashMap<FieldInfo,FieldIndexData>();
+  
+  // start of the field info data
+  protected long dirOffset;
+
+  final String segment;
+  public VariableGapTermsIndexReader(Directory dir, FieldInfos fieldInfos, String segment, int indexDivisor, String segmentSuffix, IOContext context)
+    throws IOException {
+    in = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, VariableGapTermsIndexWriter.TERMS_INDEX_EXTENSION), new IOContext(context, true));
+    this.segment = segment;
+    boolean success = false;
+    assert indexDivisor == -1 || indexDivisor > 0;
+
+    try {
+      
+      readHeader(in);
+      this.indexDivisor = indexDivisor;
+
+      seekDir(in, dirOffset);
+
+      // Read directory
+      final int numFields = in.readVInt();
+      if (numFields < 0) {
+        throw new CorruptIndexException("invalid numFields: " + numFields + " (resource=" + in + ")");
+      }
+
+      for(int i=0;i<numFields;i++) {
+        final int field = in.readVInt();
+        final long indexStart = in.readVLong();
+        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);
+        FieldIndexData previous = fields.put(fieldInfo, new FieldIndexData(fieldInfo, indexStart));
+        if (previous != null) {
+          throw new CorruptIndexException("duplicate field: " + fieldInfo.name + " (resource=" + in + ")");
+        }
+      }
+      success = true;
+    } finally {
+      if (indexDivisor > 0) {
+        in.close();
+        in = null;
+        if (success) {
+          indexLoaded = true;
+        }
+      }
+    }
+  }
+
+  @Override
+  public int getDivisor() {
+    return indexDivisor;
+  }
+  
+  protected void readHeader(IndexInput input) throws IOException {
+    CodecUtil.checkHeader(input, VariableGapTermsIndexWriter.CODEC_NAME,
+      VariableGapTermsIndexWriter.VERSION_START, VariableGapTermsIndexWriter.VERSION_START);
+    dirOffset = input.readLong();
+  }
+
+  private static class IndexEnum extends FieldIndexEnum {
+    private final BytesRefFSTEnum<Long> fstEnum;
+    private BytesRefFSTEnum.InputOutput<Long> current;
+
+    public IndexEnum(FST<Long> fst) {
+      fstEnum = new BytesRefFSTEnum<Long>(fst);
+    }
+
+    @Override
+    public BytesRef term() {
+      if (current == null) {
+        return null;
+      } else {
+        return current.input;
+      }
+    }
+
+    @Override
+    public long seek(BytesRef target) throws IOException {
+      //System.out.println("VGR: seek field=" + fieldInfo.name + " target=" + target);
+      current = fstEnum.seekFloor(target);
+      //System.out.println("  got input=" + current.input + " output=" + current.output);
+      return current.output;
+    }
+
+    @Override
+    public long next() throws IOException {
+      //System.out.println("VGR: next field=" + fieldInfo.name);
+      current = fstEnum.next();
+      if (current == null) {
+        //System.out.println("  eof");
+        return -1;
+      } else {
+        return current.output;
+      }
+    }
+
+    @Override
+    public long ord() {
+      throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public long seek(long ord) {
+      throw new UnsupportedOperationException();
+    }
+  }
+
+  @Override
+  public boolean supportsOrd() {
+    return false;
+  }
+
+  private final class FieldIndexData {
+
+    private final long indexStart;
+    // Set only if terms index is loaded:
+    private volatile FST<Long> fst;
+
+    public FieldIndexData(FieldInfo fieldInfo, long indexStart) throws IOException {
+      this.indexStart = indexStart;
+
+      if (indexDivisor > 0) {
+        loadTermsIndex();
+      }
+    }
+
+    private void loadTermsIndex() throws IOException {
+      if (fst == null) {
+        IndexInput clone = in.clone();
+        clone.seek(indexStart);
+        fst = new FST<Long>(clone, fstOutputs);
+        clone.close();
+
+        /*
+        final String dotFileName = segment + "_" + fieldInfo.name + ".dot";
+        Writer w = new OutputStreamWriter(new FileOutputStream(dotFileName));
+        Util.toDot(fst, w, false, false);
+        System.out.println("FST INDEX: SAVED to " + dotFileName);
+        w.close();
+        */
+
+        if (indexDivisor > 1) {
+          // subsample
+          final IntsRef scratchIntsRef = new IntsRef();
+          final PositiveIntOutputs outputs = PositiveIntOutputs.getSingleton(true);
+          final Builder<Long> builder = new Builder<Long>(FST.INPUT_TYPE.BYTE1, outputs);
+          final BytesRefFSTEnum<Long> fstEnum = new BytesRefFSTEnum<Long>(fst);
+          BytesRefFSTEnum.InputOutput<Long> result;
+          int count = indexDivisor;
+          while((result = fstEnum.next()) != null) {
+            if (count == indexDivisor) {
+              builder.add(Util.toIntsRef(result.input, scratchIntsRef), result.output);
+              count = 0;
+            }
+            count++;
+          }
+          fst = builder.finish();
+        }
+      }
+    }
+  }
+
+  @Override
+  public FieldIndexEnum getFieldEnum(FieldInfo fieldInfo) {
+    final FieldIndexData fieldData = fields.get(fieldInfo);
+    if (fieldData.fst == null) {
+      return null;
+    } else {
+      return new IndexEnum(fieldData.fst);
+    }
+  }
+
+  @Override
+  public void close() throws IOException {
+    if (in != null && !indexLoaded) {
+      in.close();
+    }
+  }
+
+  protected void seekDir(IndexInput input, long dirOffset) throws IOException {
+    input.seek(dirOffset);
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/VariableGapTermsIndexWriter.java b/lucene/codecs/src/java/org/apache/lucene/codecs/VariableGapTermsIndexWriter.java
new file mode 100644
index 0000000..2156fbc
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/VariableGapTermsIndexWriter.java
@@ -0,0 +1,321 @@
+package org.apache.lucene.codecs;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.IntsRef;
+import org.apache.lucene.util.fst.Builder;
+import org.apache.lucene.util.fst.FST;
+import org.apache.lucene.util.fst.PositiveIntOutputs;
+import org.apache.lucene.util.fst.Util;
+
+/**
+ * Selects index terms according to provided pluggable
+ * {@link IndexTermSelector}, and stores them in a prefix trie that's
+ * loaded entirely in RAM stored as an FST.  This terms
+ * index only supports unsigned byte term sort order
+ * (unicode codepoint order when the bytes are UTF8).
+ *
+ * @lucene.experimental */
+public class VariableGapTermsIndexWriter extends TermsIndexWriterBase {
+  protected final IndexOutput out;
+
+  /** Extension of terms index file */
+  static final String TERMS_INDEX_EXTENSION = "tiv";
+
+  final static String CODEC_NAME = "VARIABLE_GAP_TERMS_INDEX";
+  final static int VERSION_START = 0;
+  final static int VERSION_CURRENT = VERSION_START;
+
+  private final List<FSTFieldWriter> fields = new ArrayList<FSTFieldWriter>();
+  
+  @SuppressWarnings("unused") private final FieldInfos fieldInfos; // unread
+  private final IndexTermSelector policy;
+
+  /** 
+   * Hook for selecting which terms should be placed in the terms index.
+   * <p>
+   * {@link #newField} is called at the start of each new field, and
+   * {@link #isIndexTerm} for each term in that field.
+   * 
+   * @lucene.experimental 
+   */
+  public static abstract class IndexTermSelector {
+    /** 
+     * Called sequentially on every term being written,
+     * returning true if this term should be indexed
+     */
+    public abstract boolean isIndexTerm(BytesRef term, TermStats stats);
+    /**
+     * Called when a new field is started.
+     */
+    public abstract void newField(FieldInfo fieldInfo);
+  }
+
+  /** Same policy as {@link FixedGapTermsIndexWriter} */
+  public static final class EveryNTermSelector extends IndexTermSelector {
+    private int count;
+    private final int interval;
+
+    public EveryNTermSelector(int interval) {
+      this.interval = interval;
+      // First term is first indexed term:
+      count = interval;
+    }
+
+    @Override
+    public boolean isIndexTerm(BytesRef term, TermStats stats) {
+      if (count >= interval) {
+        count = 1;
+        return true;
+      } else {
+        count++;
+        return false;
+      }
+    }
+
+    @Override
+    public void newField(FieldInfo fieldInfo) {
+      count = interval;
+    }
+  }
+
+  /** Sets an index term when docFreq >= docFreqThresh, or
+   *  every interval terms.  This should reduce seek time
+   *  to high docFreq terms.  */
+  public static final class EveryNOrDocFreqTermSelector extends IndexTermSelector {
+    private int count;
+    private final int docFreqThresh;
+    private final int interval;
+
+    public EveryNOrDocFreqTermSelector(int docFreqThresh, int interval) {
+      this.interval = interval;
+      this.docFreqThresh = docFreqThresh;
+
+      // First term is first indexed term:
+      count = interval;
+    }
+
+    @Override
+    public boolean isIndexTerm(BytesRef term, TermStats stats) {
+      if (stats.docFreq >= docFreqThresh || count >= interval) {
+        count = 1;
+        return true;
+      } else {
+        count++;
+        return false;
+      }
+    }
+
+    @Override
+    public void newField(FieldInfo fieldInfo) {
+      count = interval;
+    }
+  }
+
+  // TODO: it'd be nice to let the FST builder prune based
+  // on term count of each node (the prune1/prune2 that it
+  // accepts), and build the index based on that.  This
+  // should result in a more compact terms index, more like
+  // a prefix trie than the other selectors, because it
+  // only stores enough leading bytes to get down to N
+  // terms that may complete that prefix.  It becomes
+  // "deeper" when terms are dense, and "shallow" when they
+  // are less dense.
+  //
+  // However, it's not easy to make that work this this
+  // API, because that pruning doesn't immediately know on
+  // seeing each term whether that term will be a seek point
+  // or not.  It requires some non-causality in the API, ie
+  // only on seeing some number of future terms will the
+  // builder decide which past terms are seek points.
+  // Somehow the API'd need to be able to return a "I don't
+  // know" value, eg like a Future, which only later on is
+  // flipped (frozen) to true or false.
+  //
+  // We could solve this with a 2-pass approach, where the
+  // first pass would build an FSA (no outputs) solely to
+  // determine which prefixes are the 'leaves' in the
+  // pruning. The 2nd pass would then look at this prefix
+  // trie to mark the seek points and build the FST mapping
+  // to the true output.
+  //
+  // But, one downside to this approach is that it'd result
+  // in uneven index term selection.  EG with prune1=10, the
+  // resulting index terms could be as frequent as every 10
+  // terms or as rare as every <maxArcCount> * 10 (eg 2560),
+  // in the extremes.
+
+  public VariableGapTermsIndexWriter(SegmentWriteState state, IndexTermSelector policy) throws IOException {
+    final String indexFileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, TERMS_INDEX_EXTENSION);
+    out = state.directory.createOutput(indexFileName, state.context);
+    boolean success = false;
+    try {
+      fieldInfos = state.fieldInfos;
+      this.policy = policy;
+      writeHeader(out);
+      success = true;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(out);
+      }
+    }
+  }
+  
+  protected void writeHeader(IndexOutput out) throws IOException {
+    CodecUtil.writeHeader(out, CODEC_NAME, VERSION_CURRENT);
+    // Placeholder for dir offset
+    out.writeLong(0);
+  }
+
+  @Override
+  public FieldWriter addField(FieldInfo field, long termsFilePointer) throws IOException {
+    ////System.out.println("VGW: field=" + field.name);
+    policy.newField(field);
+    FSTFieldWriter writer = new FSTFieldWriter(field, termsFilePointer);
+    fields.add(writer);
+    return writer;
+  }
+
+  /** NOTE: if your codec does not sort in unicode code
+   *  point order, you must override this method, to simply
+   *  return indexedTerm.length. */
+  protected int indexedTermPrefixLength(final BytesRef priorTerm, final BytesRef indexedTerm) {
+    // As long as codec sorts terms in unicode codepoint
+    // order, we can safely strip off the non-distinguishing
+    // suffix to save RAM in the loaded terms index.
+    final int idxTermOffset = indexedTerm.offset;
+    final int priorTermOffset = priorTerm.offset;
+    final int limit = Math.min(priorTerm.length, indexedTerm.length);
+    for(int byteIdx=0;byteIdx<limit;byteIdx++) {
+      if (priorTerm.bytes[priorTermOffset+byteIdx] != indexedTerm.bytes[idxTermOffset+byteIdx]) {
+        return byteIdx+1;
+      }
+    }
+    return Math.min(1+priorTerm.length, indexedTerm.length);
+  }
+
+  private class FSTFieldWriter extends FieldWriter {
+    private final Builder<Long> fstBuilder;
+    private final PositiveIntOutputs fstOutputs;
+    private final long startTermsFilePointer;
+
+    final FieldInfo fieldInfo;
+    FST<Long> fst;
+    final long indexStart;
+
+    private final BytesRef lastTerm = new BytesRef();
+    private boolean first = true;
+
+    public FSTFieldWriter(FieldInfo fieldInfo, long termsFilePointer) throws IOException {
+      this.fieldInfo = fieldInfo;
+      fstOutputs = PositiveIntOutputs.getSingleton(true);
+      fstBuilder = new Builder<Long>(FST.INPUT_TYPE.BYTE1, fstOutputs);
+      indexStart = out.getFilePointer();
+      ////System.out.println("VGW: field=" + fieldInfo.name);
+
+      // Always put empty string in
+      fstBuilder.add(new IntsRef(), termsFilePointer);
+      startTermsFilePointer = termsFilePointer;
+    }
+
+    @Override
+    public boolean checkIndexTerm(BytesRef text, TermStats stats) throws IOException {
+      //System.out.println("VGW: index term=" + text.utf8ToString());
+      // NOTE: we must force the first term per field to be
+      // indexed, in case policy doesn't:
+      if (policy.isIndexTerm(text, stats) || first) {
+        first = false;
+        //System.out.println("  YES");
+        return true;
+      } else {
+        lastTerm.copyBytes(text);
+        return false;
+      }
+    }
+
+    private final IntsRef scratchIntsRef = new IntsRef();
+
+    @Override
+    public void add(BytesRef text, TermStats stats, long termsFilePointer) throws IOException {
+      if (text.length == 0) {
+        // We already added empty string in ctor
+        assert termsFilePointer == startTermsFilePointer;
+        return;
+      }
+      final int lengthSave = text.length;
+      text.length = indexedTermPrefixLength(lastTerm, text);
+      try {
+        fstBuilder.add(Util.toIntsRef(text, scratchIntsRef), termsFilePointer);
+      } finally {
+        text.length = lengthSave;
+      }
+      lastTerm.copyBytes(text);
+    }
+
+    @Override
+    public void finish(long termsFilePointer) throws IOException {
+      fst = fstBuilder.finish();
+      if (fst != null) {
+        fst.save(out);
+      }
+    }
+  }
+
+  public void close() throws IOException {
+    try {
+    final long dirStart = out.getFilePointer();
+    final int fieldCount = fields.size();
+
+    int nonNullFieldCount = 0;
+    for(int i=0;i<fieldCount;i++) {
+      FSTFieldWriter field = fields.get(i);
+      if (field.fst != null) {
+        nonNullFieldCount++;
+      }
+    }
+
+    out.writeVInt(nonNullFieldCount);
+    for(int i=0;i<fieldCount;i++) {
+      FSTFieldWriter field = fields.get(i);
+      if (field.fst != null) {
+        out.writeVInt(field.fieldInfo.number);
+        out.writeVLong(field.indexStart);
+      }
+    }
+    writeTrailer(dirStart);
+    } finally {
+    out.close();
+  }
+  }
+
+  protected void writeTrailer(long dirStart) throws IOException {
+    out.seek(CodecUtil.headerLength(CODEC_NAME));
+    out.writeLong(dirStart);
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/appending/AppendingCodec.java b/lucene/codecs/src/java/org/apache/lucene/codecs/appending/AppendingCodec.java
new file mode 100644
index 0000000..c811057
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/appending/AppendingCodec.java
@@ -0,0 +1,97 @@
+package org.apache.lucene.codecs.appending;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.DocValuesFormat;
+import org.apache.lucene.codecs.FieldInfosFormat;
+import org.apache.lucene.codecs.LiveDocsFormat;
+import org.apache.lucene.codecs.NormsFormat;
+import org.apache.lucene.codecs.PostingsFormat;
+import org.apache.lucene.codecs.SegmentInfoFormat;
+import org.apache.lucene.codecs.StoredFieldsFormat;
+import org.apache.lucene.codecs.TermVectorsFormat;
+import org.apache.lucene.codecs.lucene40.Lucene40Codec;
+import org.apache.lucene.codecs.lucene40.Lucene40DocValuesFormat;
+import org.apache.lucene.codecs.lucene40.Lucene40FieldInfosFormat;
+import org.apache.lucene.codecs.lucene40.Lucene40LiveDocsFormat;
+import org.apache.lucene.codecs.lucene40.Lucene40NormsFormat;
+import org.apache.lucene.codecs.lucene40.Lucene40SegmentInfoFormat;
+import org.apache.lucene.codecs.lucene40.Lucene40StoredFieldsFormat;
+import org.apache.lucene.codecs.lucene40.Lucene40TermVectorsFormat;
+
+/**
+ * This codec extends {@link Lucene40Codec} to work on append-only outputs, such
+ * as plain output streams and append-only filesystems.
+ * 
+ * @lucene.experimental
+ */
+public class AppendingCodec extends Codec {
+  public AppendingCodec() {
+    super("Appending");
+  }
+
+  private final PostingsFormat postings = new AppendingPostingsFormat();
+  private final SegmentInfoFormat infos = new Lucene40SegmentInfoFormat();
+  private final StoredFieldsFormat fields = new Lucene40StoredFieldsFormat();
+  private final FieldInfosFormat fieldInfos = new Lucene40FieldInfosFormat();
+  private final TermVectorsFormat vectors = new Lucene40TermVectorsFormat();
+  private final DocValuesFormat docValues = new Lucene40DocValuesFormat();
+  private final NormsFormat norms = new Lucene40NormsFormat();
+  private final LiveDocsFormat liveDocs = new Lucene40LiveDocsFormat();
+  
+  @Override
+  public PostingsFormat postingsFormat() {
+    return postings;
+  }
+
+  @Override
+  public StoredFieldsFormat storedFieldsFormat() {
+    return fields;
+  }
+  
+  @Override
+  public TermVectorsFormat termVectorsFormat() {
+    return vectors;
+  }
+
+  @Override
+  public DocValuesFormat docValuesFormat() {
+    return docValues;
+  }
+
+  @Override
+  public SegmentInfoFormat segmentInfoFormat() {
+    return infos;
+  }
+  
+  @Override
+  public FieldInfosFormat fieldInfosFormat() {
+    return fieldInfos;
+  }
+  
+  @Override
+  public NormsFormat normsFormat() {
+    return norms;
+  }
+  
+  @Override
+  public LiveDocsFormat liveDocsFormat() {
+    return liveDocs;
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/appending/AppendingPostingsFormat.java b/lucene/codecs/src/java/org/apache/lucene/codecs/appending/AppendingPostingsFormat.java
new file mode 100644
index 0000000..2f72178
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/appending/AppendingPostingsFormat.java
@@ -0,0 +1,80 @@
+package org.apache.lucene.codecs.appending;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.BlockTreeTermsWriter;
+import org.apache.lucene.codecs.FieldsConsumer;
+import org.apache.lucene.codecs.FieldsProducer;
+import org.apache.lucene.codecs.PostingsFormat;
+import org.apache.lucene.codecs.PostingsReaderBase;
+import org.apache.lucene.codecs.PostingsWriterBase;
+import org.apache.lucene.codecs.lucene40.Lucene40PostingsReader;
+import org.apache.lucene.codecs.lucene40.Lucene40PostingsWriter;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SegmentWriteState;
+
+/**
+ * Appending postings impl.
+ */
+class AppendingPostingsFormat extends PostingsFormat {
+  public static String CODEC_NAME = "Appending";
+  
+  public AppendingPostingsFormat() {
+    super(CODEC_NAME);
+  }
+
+  @Override
+  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
+    PostingsWriterBase docsWriter = new Lucene40PostingsWriter(state);
+    boolean success = false;
+    try {
+      FieldsConsumer ret = new AppendingTermsWriter(state, docsWriter, BlockTreeTermsWriter.DEFAULT_MIN_BLOCK_SIZE, BlockTreeTermsWriter.DEFAULT_MAX_BLOCK_SIZE);
+      success = true;
+      return ret;
+    } finally {
+      if (!success) {
+        docsWriter.close();
+      }
+    }
+  }
+
+  @Override
+  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
+    PostingsReaderBase postings = new Lucene40PostingsReader(state.dir, state.fieldInfos, state.segmentInfo, state.context, state.segmentSuffix);
+    
+    boolean success = false;
+    try {
+      FieldsProducer ret = new AppendingTermsReader(
+                                                    state.dir,
+                                                    state.fieldInfos,
+                                                    state.segmentInfo,
+                                                    postings,
+                                                    state.context,
+                                                    state.segmentSuffix,
+                                                    state.termsIndexDivisor);
+      success = true;
+      return ret;
+    } finally {
+      if (!success) {
+        postings.close();
+      }
+    }
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/appending/AppendingTermsReader.java b/lucene/codecs/src/java/org/apache/lucene/codecs/appending/AppendingTermsReader.java
new file mode 100644
index 0000000..f89eae1
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/appending/AppendingTermsReader.java
@@ -0,0 +1,62 @@
+package org.apache.lucene.codecs.appending;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.BlockTreeTermsReader;
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.PostingsReaderBase;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+
+/**
+ * Reads append-only terms from {@link AppendingTermsWriter}
+ * @lucene.experimental
+ */
+public class AppendingTermsReader extends BlockTreeTermsReader {
+
+  public AppendingTermsReader(Directory dir, FieldInfos fieldInfos, SegmentInfo info, PostingsReaderBase postingsReader, 
+      IOContext ioContext, String segmentSuffix, int indexDivisor) throws IOException {
+    super(dir, fieldInfos, info, postingsReader, ioContext, segmentSuffix, indexDivisor);
+  }
+
+  @Override
+  protected void readHeader(IndexInput input) throws IOException {
+    CodecUtil.checkHeader(input, AppendingTermsWriter.TERMS_CODEC_NAME,
+        AppendingTermsWriter.TERMS_VERSION_START,
+        AppendingTermsWriter.TERMS_VERSION_CURRENT);  
+  }
+
+  @Override
+  protected void readIndexHeader(IndexInput input) throws IOException {
+    CodecUtil.checkHeader(input, AppendingTermsWriter.TERMS_INDEX_CODEC_NAME,
+        AppendingTermsWriter.TERMS_INDEX_VERSION_START,
+        AppendingTermsWriter.TERMS_INDEX_VERSION_CURRENT);
+  }
+  
+  @Override
+  protected void seekDir(IndexInput input, long dirOffset) throws IOException {
+    input.seek(input.length() - Long.SIZE / 8);
+    long offset = input.readLong();
+    input.seek(offset);
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/appending/AppendingTermsWriter.java b/lucene/codecs/src/java/org/apache/lucene/codecs/appending/AppendingTermsWriter.java
new file mode 100644
index 0000000..8025a21
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/appending/AppendingTermsWriter.java
@@ -0,0 +1,64 @@
+package org.apache.lucene.codecs.appending;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.BlockTreeTermsWriter;
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.PostingsWriterBase;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.store.IndexOutput;
+
+/**
+ * Append-only version of {@link BlockTreeTermsWriter}
+ * @lucene.experimental
+ */
+public class AppendingTermsWriter extends BlockTreeTermsWriter {
+  final static String TERMS_CODEC_NAME = "APPENDING_TERMS_DICT";
+  final static int TERMS_VERSION_START = 0;
+  final static int TERMS_VERSION_CURRENT = TERMS_VERSION_START;
+  
+  final static String TERMS_INDEX_CODEC_NAME = "APPENDING_TERMS_INDEX";
+  final static int TERMS_INDEX_VERSION_START = 0;
+  final static int TERMS_INDEX_VERSION_CURRENT = TERMS_INDEX_VERSION_START;
+  
+  public AppendingTermsWriter(SegmentWriteState state, PostingsWriterBase postingsWriter, int minItemsInBlock, int maxItemsInBlock) throws IOException {
+    super(state, postingsWriter, minItemsInBlock, maxItemsInBlock);
+  }
+
+  @Override
+  protected void writeHeader(IndexOutput out) throws IOException {
+    CodecUtil.writeHeader(out, TERMS_CODEC_NAME, TERMS_VERSION_CURRENT);
+  }
+
+  @Override
+  protected void writeIndexHeader(IndexOutput out) throws IOException {
+    CodecUtil.writeHeader(out, TERMS_INDEX_CODEC_NAME, TERMS_INDEX_VERSION_CURRENT);
+  }
+
+  @Override
+  protected void writeTrailer(IndexOutput out, long dirStart) throws IOException {
+    out.writeLong(dirStart);
+  }
+
+  @Override
+  protected void writeIndexTrailer(IndexOutput indexOut, long dirStart) throws IOException {
+    indexOut.writeLong(dirStart);
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/appending/package.html b/lucene/codecs/src/java/org/apache/lucene/codecs/appending/package.html
new file mode 100644
index 0000000..940808a
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/appending/package.html
@@ -0,0 +1,25 @@
+<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+<html>
+<head>
+   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
+</head>
+<body>
+Codec for on append-only outputs, such as plain output streams and append-only filesystems.
+</body>
+</html>
\ No newline at end of file
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/block/BlockPostingsFormat.java b/lucene/codecs/src/java/org/apache/lucene/codecs/block/BlockPostingsFormat.java
new file mode 100644
index 0000000..6fafbd6
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/block/BlockPostingsFormat.java
@@ -0,0 +1,422 @@
+package org.apache.lucene.codecs.block;
+
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.BlockTreeTermsReader;
+import org.apache.lucene.codecs.BlockTreeTermsWriter;
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.FieldsConsumer;
+import org.apache.lucene.codecs.FieldsProducer;
+import org.apache.lucene.codecs.MultiLevelSkipListWriter;
+import org.apache.lucene.codecs.PostingsFormat;
+import org.apache.lucene.codecs.PostingsReaderBase;
+import org.apache.lucene.codecs.PostingsWriterBase;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.store.DataOutput;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.packed.PackedInts;
+
+/**
+ * Block postings format, which encodes postings in packed int blocks 
+ * for faster decode.
+ *
+ * <p><b>NOTE</b>: this format is still experimental and
+ * subject to change without backwards compatibility.
+ *
+ * <p>
+ * Basic idea:
+ * <ul>
+ *   <li>
+ *   <b>Packed Block and VInt Block</b>: 
+ *   <p>In packed block, integers are encoded with the same bit width ({@link PackedInts packed format}), 
+ *      the block size (i.e. number of integers inside block) is fixed. </p>
+ *   <p>In VInt block, integers are encoded as {@link DataOutput#writeVInt VInt}, 
+ *      the block size is variable.</p>
+ *   </li>
+ *
+ *   <li> 
+ *   <b>Block structure</b>: 
+ *   <p>When the postings is long enough, BlockPostingsFormat will try to encode most integer data 
+ *      as packed block.</p> 
+ *   <p>Take a term with 259 documents as example, the first 256 document ids are encoded as two packed 
+ *      blocks, while the remaining 3 as one VInt block. </p>
+ *   <p>Different kinds of data are always encoded separately into different packed blocks, but may 
+ *      possible be encoded into a same VInt block. </p>
+ *   <p>This strategy is applied to pairs: 
+ *      &lt;document number, frequency&gt;,
+ *      &lt;position, payload length&gt;, 
+ *      &lt;position, offset start, offset length&gt;, and
+ *      &lt;position, payload length, offsetstart, offset length&gt;.</p>
+ *   </li>
+ *
+ *   <li>
+ *   <b>Skipper setting</b>: 
+ *   <p>The structure of skip table is quite similar to Lucene40PostingsFormat. Skip interval is the 
+ *      same as block size, and each skip entry points to the beginning of each block. However, for 
+ *      the first block, skip data is omitted.</p>
+ *   </li>
+ *
+ *   <li>
+ *   <b>Positions, Payloads, and Offsets</b>: 
+ *   <p>A position is an integer indicating where the term occurs at within one document. 
+ *      A payload is a blob of metadata associated with current position. 
+ *      An offset is a pair of integers indicating the tokenized start/end offsets for given term 
+ *      in current position. </p>
+ *   <p>When payloads and offsets are not omitted, numPositions==numPayloads==numOffsets (assuming a 
+ *      null payload contributes one count). As mentioned in block structure, it is possible to encode 
+ *      these three either combined or separately. 
+ *   <p>For all the cases, payloads and offsets are stored together. When encoded as packed block, 
+ *      position data is separated out as .pos, while payloads and offsets are encoded in .pay (payload 
+ *      metadata will also be stored directly in .pay). When encoded as VInt block, all these three are 
+ *      stored in .pos (so as payload metadata).</p>
+ *   <p>With this strategy, the majority of payload and offset data will be outside .pos file. 
+ *      So for queries that require only position data, running on a full index with payloads and offsets, 
+ *      this reduces disk pre-fetches.</p>
+ *   </li>
+ * </ul>
+ * </p>
+ *
+ * <p>
+ * Files and detailed format:
+ * <ul>
+ *   <li><tt>.tim</tt>: <a href="#Termdictionary">Term Dictionary</a></li>
+ *   <li><tt>.tip</tt>: <a href="#Termindex">Term Index</a></li>
+ *   <li><tt>.doc</tt>: <a href="#Frequencies">Frequencies and Skip Data</a></li>
+ *   <li><tt>.pos</tt>: <a href="#Positions">Positions</a></li>
+ *   <li><tt>.pay</tt>: <a href="#Payloads">Payloads and Offsets</a></li>
+ * </ul>
+ * </p>
+ *
+ * <a name="Termdictionary" id="Termdictionary"></a>
+ * <dl>
+ * <dd>
+ * <b>Term Dictionary</b>
+ *
+ * <p>The .tim file format is quite similar to Lucene40PostingsFormat, 
+ *  with minor difference in MetadataBlock</p>
+ *
+ * <ul>
+ * <!-- TODO: expand on this, its not really correct and doesnt explain sub-blocks etc -->
+ *   <li>TermDictionary(.tim) --&gt; Header, DirOffset, PostingsHeader, PackedBlockSize, 
+ *                                   &lt;Block&gt;<sup>NumBlocks</sup>, FieldSummary</li>
+ *   <li>Block --&gt; SuffixBlock, StatsBlock, MetadataBlock</li>
+ *   <li>SuffixBlock --&gt; EntryCount, SuffixLength, {@link DataOutput#writeByte byte}<sup>SuffixLength</sup></li>
+ *   <li>StatsBlock --&gt; StatsLength, &lt;DocFreq, TotalTermFreq&gt;<sup>EntryCount</sup></li>
+ *   <li>MetadataBlock --&gt; MetaLength, &lt;DocFPDelta, 
+ *                            &lt;PosFPDelta, PosVIntBlockFPDelta?, PayFPDelta?&gt;?, 
+ *                            SkipFPDelta?&gt;<sup>EntryCount</sup></li>
+ *   <li>FieldSummary --&gt; NumFields, &lt;FieldNumber, NumTerms, RootCodeLength, 
+ *                           {@link DataOutput#writeByte byte}<sup>RootCodeLength</sup>, SumDocFreq, DocCount&gt;
+ *                           <sup>NumFields</sup></li>
+ *   <li>Header, PostingsHeader --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
+ *   <li>DirOffset --&gt; {@link DataOutput#writeLong Uint64}</li>
+ *   <li>PackedBlockSize, EntryCount, SuffixLength, StatsLength, DocFreq, MetaLength, 
+ *       PosVIntBlockFPDelta, SkipFPDelta, NumFields, FieldNumber, RootCodeLength, DocCount --&gt; 
+ *       {@link DataOutput#writeVInt VInt}</li>
+ *   <li>TotalTermFreq, DocFPDelta, PosFPDelta, PayFPDelta, NumTerms, SumTotalTermFreq, SumDocFreq --&gt; 
+ *       {@link DataOutput#writeVLong VLong}</li>
+ * </ul>
+ * <p>Notes:</p>
+ * <ul>
+ *    <li>Here explains MetadataBlock only, other fields are mentioned in 
+ *   <a href="../lucene40/Lucene40PostingsFormat.html#Termdictionary">Lucene40PostingsFormat:TermDictionary</a>
+ *    </li>
+ *    <li>PackedBlockSize is the fixed block size for packed blocks. In packed block, bit width is 
+ *        determined by the largest integer. Smaller block size result in smaller variance among width 
+ *        of integers hence smaller indexes. Larger block size result in more efficient bulk i/o hence
+ *        better acceleration. This value should always be a multiple of 64, currently fixed as 128 as 
+ *        a tradeoff. It is also the skip interval used to accelerate {@link DocsEnum#advance(int)}.
+ *    <li>DocFPDelta determines the position of this term's TermFreqs within the .doc file. 
+ *        In particular, it is the difference of file offset between this term's
+ *        data and previous term's data (or zero, for the first term in the block).On disk it is 
+ *        stored as the difference from previous value in sequence. </li>
+ *    <li>PosFPDelta determines the position of this term's TermPositions within the .pos file.
+ *        While PayFPDelta determines the position of this term's &lt;TermPayloads, TermOffsets?&gt; within 
+ *        the .pay file. Similar to DocFPDelta, it is the difference between two file positions (or 
+ *        neglected, for fields that omit payloads and offsets).</li>
+ *    <li>PosVIntBlockFPDelta determines the position of this term's last TermPosition in last pos packed
+ *        block within the .pos file. It is synonym for PayVIntBlockFPDelta or OffsetVIntBlockFPDelta. 
+ *        This is actually used to indicate whether it is necessary to load following
+ *        payloads and offsets from .pos instead of .pay. Every time a new block of positions are to be 
+ *        loaded, the PostingsReader will use this value to check whether current block is packed format
+ *        or VInt. When packed format, payloads and offsets are fetched from .pay, otherwise from .pos. 
+ *        (this value is neglected when total number of positions i.e. totalTermFreq is less or equal 
+ *        to PackedBlockSize).
+ *    <li>SkipFPDelta determines the position of this term's SkipData within the .doc
+ *        file. In particular, it is the length of the TermFreq data.
+ *        SkipDelta is only stored if DocFreq is not smaller than SkipMinimum
+ *        (i.e. 8 in BlockPostingsFormat).</li>
+ * </ul>
+ * </dd>
+ * </dl>
+ *
+ * <a name="Termindex" id="Termindex"></a>
+ * <dl>
+ * <dd>
+ * <b>Term Index</b>
+ * <p>The .tim file format is mentioned in
+ *   <a href="../lucene40/Lucene40PostingsFormat.html#Termindex">Lucene40PostingsFormat:TermIndex</a>
+ * </dd>
+ * </dl>
+ *
+ *
+ * <a name="Frequencies" id="Frequencies"></a>
+ * <dl>
+ * <dd>
+ * <b>Frequencies and Skip Data</b>
+ *
+ * <p>The .doc file contains the lists of documents which contain each term, along
+ * with the frequency of the term in that document (except when frequencies are
+ * omitted: {@link IndexOptions#DOCS_ONLY}). It also saves skip data to the beginning of 
+ * each packed or VInt block, when the length of document list is larger than packed block size.</p>
+ *
+ * <ul>
+ *   <li>docFile(.doc) --&gt; Header, &lt;TermFreqs, SkipData?&gt;<sup>TermCount</sup></li>
+ *   <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
+ *   <li>TermFreqs --&gt; &lt;PackedBlock&gt; <sup>PackedDocBlockNum</sup>,  
+ *                        VIntBlock? </li>
+ *   <li>PackedBlock --&gt; PackedDocDeltaBlock, PackedFreqBlock?
+ *   <li>VIntBlock --&gt; &lt;DocDelta[, Freq?]&gt;<sup>DocFreq-PackedBlockSize*PackedDocBlockNum</sup>
+ *   <li>SkipData --&gt; &lt;&lt;SkipLevelLength, SkipLevel&gt;
+ *       <sup>NumSkipLevels-1</sup>, SkipLevel&gt;, SkipDatum?</li>
+ *   <li>SkipLevel --&gt; &lt;SkipDatum&gt; <sup>TrimmedDocFreq/(PackedBlockSize^(Level + 1))</sup></li>
+ *   <li>SkipDatum --&gt; DocSkip, DocFPSkip, &lt;PosFPSkip, PosBlockOffset, PayLength?, 
+ *                        OffsetStart?, PayFPSkip?&gt;?, SkipChildLevelPointer?</li>
+ *   <li>PackedDocDeltaBlock, PackedFreqBlock --&gt; {@link PackedInts PackedInts}</li>
+ *   <li>DocDelta, Freq, DocSkip, DocFPSkip, PosFPSkip, PosBlockOffset, PayLength, OffsetStart, PayFPSkip 
+ *       --&gt; 
+ *   {@link DataOutput#writeVInt VInt}</li>
+ *   <li>SkipChildLevelPointer --&gt; {@link DataOutput#writeVLong VLong}</li>
+ * </ul>
+ * <p>Notes:</p>
+ * <ul>
+ *   <li>PackedDocDeltaBlock is theoretically generated from two steps: 
+ *     <ol>
+ *       <li>Calculate the difference between each document number and previous one, 
+ *           and get a d-gaps list (for the first document, use absolute value); </li>
+ *       <li>For those d-gaps from first one to PackedDocBlockNum*PackedBlockSize<sup>th</sup>, 
+ *           separately encode as packed blocks.</li>
+ *     </ol>
+ *     If frequencies are not omitted, PackedFreqBlock will be generated without d-gap step.
+ *   </li>
+ *   <li>VIntBlock stores remaining d-gaps (along with frequencies when possible) with a format 
+ *       mentioned in
+ *   <a href="../lucene40/Lucene40PostingsFormat.html#Frequencies">Lucene40PostingsFormat:Frequencies</a>
+ *   </li>
+ *   <li>PackedDocBlockNum is the number of packed blocks for current term's docids or frequencies. 
+ *       In particular, PackedDocBlockNum = floor(DocFreq/PackedBlockSize) </li>
+ *   <li>TrimmedDocFreq = DocFreq % PackedBlockSize == 0 ? DocFreq - 1 : DocFreq. 
+ *       We use this trick since the definition of skip entry is a little different from base interface.
+ *       In {@link MultiLevelSkipListWriter}, skip data is assumed to be saved for
+ *       skipInterval<sup>th</sup>, 2*skipInterval<sup>th</sup> ... posting in the list. However, 
+ *       in BlockPostingsFormat, the skip data is saved for skipInterval+1<sup>th</sup>, 
+ *       2*skipInterval+1<sup>th</sup> ... posting (skipInterval==PackedBlockSize in this case). 
+ *       When DocFreq is multiple of PackedBlockSize, MultiLevelSkipListWriter will expect one 
+ *       more skip data than BlockSkipWriter. </li>
+ *   <li>SkipDatum is the metadata of one skip entry.
+ *      For the first block (no matter packed or VInt), it is omitted.</li>
+ *   <li>DocSkip records the document number of every PackedBlockSize<sup>th</sup> document number in
+ *       the postings (i.e. last document number in each packed block). On disk it is stored as the 
+ *       difference from previous value in the sequence. </li>
+ *   <li>DocFPSkip records the file offsets of each block (excluding )posting at 
+ *       PackedBlockSize+1<sup>th</sup>, 2*PackedBlockSize+1<sup>th</sup> ... , in DocFile. 
+ *       The file offsets are relative to the start of current term's TermFreqs. 
+ *       On disk it is also stored as the difference from previous SkipDatum in the sequence.</li>
+ *   <li>Since positions and payloads are also block encoded, the skip should skip to related block first,
+ *       then fetch the values according to in-block offset. PosFPSkip and PayFPSkip record the file 
+ *       offsets of related block in .pos and .pay, respectively. While PosBlockOffset indicates
+ *       which value to fetch inside the related block (PayBlockOffset is unnecessary since it is always
+ *       equal to PosBlockOffset). Same as DocFPSkip, the file offsets are relative to the start of 
+ *       current term's TermFreqs, and stored as a difference sequence.</li>
+ *   <li>PayLength indicates the length of last payload.</li>
+ *   <li>OffsetStart indicates the first value of last offset pair.</li>
+ * </ul>
+ * </dd>
+ * </dl>
+ *
+ * <a name="Positions" id="Positions"></a>
+ * <dl>
+ * <dd>
+ * <b>Positions</b>
+ * <p>The .pos file contains the lists of positions that each term occurs at within documents. It also
+ *    sometimes stores part of payloads and offsets for speedup.</p>
+ * <ul>
+ *   <li>PosFile(.pos) --&gt; Header, &lt;TermPositions&gt; <sup>TermCount</sup></li>
+ *   <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
+ *   <li>TermPositions --&gt; &lt;PackedPosDeltaBlock&gt; <sup>PackedPosBlockNum</sup>,  
+ *                            VIntBlock? </li>
+ *   <li>VIntBlock --&gt; PosVIntCount, &lt;PosDelta[, PayLength?], PayData?, 
+ *                        OffsetStartDelta?, OffsetLength?&gt;<sup>PosVIntCount</sup>
+ *   <li>PackedPosDeltaBlock --&gt; {@link PackedInts PackedInts}</li>
+ *   <li>PosVIntCount, PosDelta, OffsetStartDelta, OffsetLength --&gt; 
+ *       {@link DataOutput#writeVInt VInt}</li>
+ *   <li>PayData --&gt; {@link DataOutput#writeByte byte}<sup>PayLength</sup></li>
+ * </ul>
+ * <p>Notes:</p>
+ * <ul>
+ *   <li>TermPositions are order by term (terms are implicit, from the term dictionary), and position 
+ *       values for each term document pair are incremental, and ordered by document number.</li>
+ *   <li>PackedPosBlockNum is the number of packed blocks for current term's positions, payloads or offsets. 
+ *       In particular, PackedPosBlockNum = floor(totalTermFreq/PackedBlockSize) </li>
+ *   <li>PosVIntCount is the number of positions encoded as VInt format. In particular, 
+ *       PosVIntCount = totalTermFreq - PackedPosBlockNum*PackedBlockSize</li>
+ *   <li>The procedure how PackedPosDeltaBlock is generated is the same as PackedDocDeltaBlock 
+ *       in chapter <a href="#Frequencies">Frequencies and Skip Data</a>.</li>
+ *   <li>PosDelta is the same as the format mentioned in 
+ *   <a href="../lucene40/Lucene40PostingsFormat.html#Positions">Lucene40PostingsFormat:Positions</a>
+ *   </li>
+ *   <li>OffsetStartDelta is the difference between this position's startOffset from the previous 
+ *       occurrence (or zero, if this is the first occurrence in this document).</li>
+ *   <li>OffsetLength indicates the length of the current offset (endOffset-startOffset).</li>
+ *   <li>PayloadData is the blob of metadata associated with current position.</li>
+ * </ul>
+ * </dd>
+ * </dl>
+ *
+ * <a name="Payloads" id="Payloads"></a>
+ * <dl>
+ * <dd>
+ * <b>Payloads and Offsets</b>
+ * <p>The .pay file will store payloads and offsets associated with certain term-document positions. 
+ *    Some payloads and offsets will be separated out into .pos file, for speedup reason.</p>
+ * <ul>
+ *   <li>PayFile(.pay): --&gt; Header, &lt;TermPayloads, TermOffsets?&gt; <sup>TermCount</sup></li>
+ *   <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
+ *   <li>TermPayloads --&gt; &lt;PackedPayLengthBlock, SumPayLength, PayData&gt; <sup>PackedPayBlockNum</sup>
+ *   <li>TermOffsets --&gt; &lt;PackedOffsetStartDeltaBlock, PackedOffsetLengthBlock&gt; <sup>PackedPayBlockNum</sup>
+ *   <li>PackedPayLengthBlock, PackedOffsetStartDeltaBlock, PackedOffsetLengthBlock --&gt; {@link PackedInts PackedInts}</li>
+ *   <li>SumPayLength --&gt; {@link DataOutput#writeVInt VInt}</li>
+ *   <li>PayData --&gt; {@link DataOutput#writeByte byte}<sup>SumPayLength</sup></li>
+ * </ul>
+ * <p>Notes:</p>
+ * <ul>
+ *   <li>The order of TermPayloads/TermOffsets will be the same as TermPositions, note that part of 
+ *       payload/offsets are stored in .pos.</li>
+ *   <li>The procedure how PackedPayLengthBlock and PackedOffsetLengthBlock are generated is the 
+ *       same as PackedFreqBlock in chapter <a href="#Frequencies">Frequencies and Skip Data</a>. 
+ *       While PackedStartDeltaBlock follows a same procedure as PackedDocDeltaBlock.</li>
+ *   <li>PackedPayBlockNum is always equal to PackedPosBlockNum, for the same term. It is also synonym 
+ *       for PackedOffsetBlockNum.</li>
+ *   <li>SumPayLength is the total length of payloads written within one block, should be the sum
+ *       of PayLengths in one packed block.</li>
+ *   <li>PayLength in PackedPayLengthBlock is the length of each payload, associated with current 
+ *       position.</li>
+ * </ul>
+ * </dd>
+ * </dl>
+ * </p>
+ *
+ * @lucene.experimental
+ */
+
+public final class BlockPostingsFormat extends PostingsFormat {
+  /**
+   * Filename extension for document number, frequencies, and skip data.
+   * See chapter: <a href="#Frequencies">Frequencies and Skip Data</a>
+   */
+  public static final String DOC_EXTENSION = "doc";
+
+  /**
+   * Filename extension for positions. 
+   * See chapter: <a href="#Positions">Positions</a>
+   */
+  public static final String POS_EXTENSION = "pos";
+
+  /**
+   * Filename extension for payloads and offsets.
+   * See chapter: <a href="#Payloads">Payloads and Offsets</a>
+   */
+  public static final String PAY_EXTENSION = "pay";
+
+  private final int minTermBlockSize;
+  private final int maxTermBlockSize;
+
+  /**
+   * Fixed packed block size, number of integers encoded in 
+   * a single packed block.
+   */
+  // NOTE: must be multiple of 64 because of PackedInts long-aligned encoding/decoding
+  public final static int BLOCK_SIZE = 128;
+
+  public BlockPostingsFormat() {
+    this(BlockTreeTermsWriter.DEFAULT_MIN_BLOCK_SIZE, BlockTreeTermsWriter.DEFAULT_MAX_BLOCK_SIZE);
+  }
+
+  public BlockPostingsFormat(int minTermBlockSize, int maxTermBlockSize) {
+    super("Block");
+    this.minTermBlockSize = minTermBlockSize;
+    assert minTermBlockSize > 1;
+    this.maxTermBlockSize = maxTermBlockSize;
+    assert minTermBlockSize <= maxTermBlockSize;
+  }
+
+  @Override
+  public String toString() {
+    return getName() + "(blocksize=" + BLOCK_SIZE + ")";
+  }
+
+  @Override
+  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
+    PostingsWriterBase postingsWriter = new BlockPostingsWriter(state);
+
+    boolean success = false;
+    try {
+      FieldsConsumer ret = new BlockTreeTermsWriter(state, 
+                                                    postingsWriter,
+                                                    minTermBlockSize, 
+                                                    maxTermBlockSize);
+      success = true;
+      return ret;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(postingsWriter);
+      }
+    }
+  }
+
+  @Override
+  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
+    PostingsReaderBase postingsReader = new BlockPostingsReader(state.dir,
+                                                                state.fieldInfos,
+                                                                state.segmentInfo,
+                                                                state.context,
+                                                                state.segmentSuffix);
+    boolean success = false;
+    try {
+      FieldsProducer ret = new BlockTreeTermsReader(state.dir,
+                                                    state.fieldInfos,
+                                                    state.segmentInfo,
+                                                    postingsReader,
+                                                    state.context,
+                                                    state.segmentSuffix,
+                                                    state.termsIndexDivisor);
+      success = true;
+      return ret;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(postingsReader);
+      }
+    }
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/block/BlockPostingsReader.java b/lucene/codecs/src/java/org/apache/lucene/codecs/block/BlockPostingsReader.java
new file mode 100644
index 0000000..100fae2
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/block/BlockPostingsReader.java
@@ -0,0 +1,1507 @@
+package org.apache.lucene.codecs.block;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import static org.apache.lucene.codecs.block.BlockPostingsFormat.BLOCK_SIZE;
+import static org.apache.lucene.codecs.block.ForUtil.MAX_DATA_SIZE;
+import static org.apache.lucene.codecs.block.ForUtil.MAX_ENCODED_SIZE;
+
+import java.io.IOException;
+import java.util.Arrays;
+
+import org.apache.lucene.codecs.BlockTermState;
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.PostingsReaderBase;
+import org.apache.lucene.index.DocsAndPositionsEnum;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.TermState;
+import org.apache.lucene.store.ByteArrayDataInput;
+import org.apache.lucene.store.DataInput;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+
+/**
+ * Concrete class that reads docId(maybe frq,pos,offset,payloads) list
+ * with postings format.
+ *
+ * @see BlockSkipReader for details
+ *
+ */
+final class BlockPostingsReader extends PostingsReaderBase {
+
+  private final IndexInput docIn;
+  private final IndexInput posIn;
+  private final IndexInput payIn;
+
+  private final ForUtil forUtil;
+
+  // public static boolean DEBUG = false;
+
+  public BlockPostingsReader(Directory dir, FieldInfos fieldInfos, SegmentInfo segmentInfo, IOContext ioContext, String segmentSuffix) throws IOException {
+    boolean success = false;
+    IndexInput docIn = null;
+    IndexInput posIn = null;
+    IndexInput payIn = null;
+    try {
+      docIn = dir.openInput(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, BlockPostingsFormat.DOC_EXTENSION),
+                            ioContext);
+      CodecUtil.checkHeader(docIn,
+                            BlockPostingsWriter.DOC_CODEC,
+                            BlockPostingsWriter.VERSION_START,
+                            BlockPostingsWriter.VERSION_START);
+      forUtil = new ForUtil(docIn);
+
+      if (fieldInfos.hasProx()) {
+        posIn = dir.openInput(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, BlockPostingsFormat.POS_EXTENSION),
+                              ioContext);
+        CodecUtil.checkHeader(posIn,
+                              BlockPostingsWriter.POS_CODEC,
+                              BlockPostingsWriter.VERSION_START,
+                              BlockPostingsWriter.VERSION_START);
+
+        if (fieldInfos.hasPayloads() || fieldInfos.hasOffsets()) {
+          payIn = dir.openInput(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, BlockPostingsFormat.PAY_EXTENSION),
+                                ioContext);
+          CodecUtil.checkHeader(payIn,
+                                BlockPostingsWriter.PAY_CODEC,
+                                BlockPostingsWriter.VERSION_START,
+                                BlockPostingsWriter.VERSION_START);
+        }
+      }
+
+      this.docIn = docIn;
+      this.posIn = posIn;
+      this.payIn = payIn;
+      success = true;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(docIn, posIn, payIn);
+      }
+    }
+  }
+
+  @Override
+  public void init(IndexInput termsIn) throws IOException {
+    // Make sure we are talking to the matching postings writer
+    CodecUtil.checkHeader(termsIn,
+                          BlockPostingsWriter.TERMS_CODEC,
+                          BlockPostingsWriter.VERSION_START,
+                          BlockPostingsWriter.VERSION_START);
+    final int indexBlockSize = termsIn.readVInt();
+    if (indexBlockSize != BLOCK_SIZE) {
+      throw new IllegalStateException("index-time BLOCK_SIZE (" + indexBlockSize + ") != read-time BLOCK_SIZE (" + BLOCK_SIZE + ")");
+    }
+  }
+
+  /**
+   * Read values that have been written using variable-length encoding instead of bit-packing.
+   */
+  private static void readVIntBlock(IndexInput docIn, int[] docBuffer,
+      int[] freqBuffer, int num, boolean indexHasFreq) throws IOException {
+    if (indexHasFreq) {
+      for(int i=0;i<num;i++) {
+        final int code = docIn.readVInt();
+        docBuffer[i] = code >>> 1;
+        if ((code & 1) != 0) {
+          freqBuffer[i] = 1;
+        } else {
+          freqBuffer[i] = docIn.readVInt();
+        }
+      }
+    } else {
+      for(int i=0;i<num;i++) {
+        docBuffer[i] = docIn.readVInt();
+      }
+    }
+  }
+
+  // Must keep final because we do non-standard clone
+  private final static class IntBlockTermState extends BlockTermState {
+    long docStartFP;
+    long posStartFP;
+    long payStartFP;
+    int skipOffset;
+    int lastPosBlockOffset;
+
+    // Only used by the "primary" TermState -- clones don't
+    // copy this (basically they are "transient"):
+    ByteArrayDataInput bytesReader;  // TODO: should this NOT be in the TermState...?
+    byte[] bytes;
+
+    @Override
+    public IntBlockTermState clone() {
+      IntBlockTermState other = new IntBlockTermState();
+      other.copyFrom(this);
+      return other;
+    }
+
+    @Override
+    public void copyFrom(TermState _other) {
+      super.copyFrom(_other);
+      IntBlockTermState other = (IntBlockTermState) _other;
+      docStartFP = other.docStartFP;
+      posStartFP = other.posStartFP;
+      payStartFP = other.payStartFP;
+      lastPosBlockOffset = other.lastPosBlockOffset;
+      skipOffset = other.skipOffset;
+
+      // Do not copy bytes, bytesReader (else TermState is
+      // very heavy, ie drags around the entire block's
+      // byte[]).  On seek back, if next() is in fact used
+      // (rare!), they will be re-read from disk.
+    }
+
+    @Override
+    public String toString() {
+      return super.toString() + " docStartFP=" + docStartFP + " posStartFP=" + posStartFP + " payStartFP=" + payStartFP + " lastPosBlockOffset=" + lastPosBlockOffset;
+    }
+  }
+
+  @Override
+  public IntBlockTermState newTermState() {
+    return new IntBlockTermState();
+  }
+
+  @Override
+  public void close() throws IOException {
+    IOUtils.close(docIn, posIn, payIn);
+  }
+
+  /* Reads but does not decode the byte[] blob holding
+     metadata for the current terms block */
+  @Override
+  public void readTermsBlock(IndexInput termsIn, FieldInfo fieldInfo, BlockTermState _termState) throws IOException {
+    final IntBlockTermState termState = (IntBlockTermState) _termState;
+
+    final int numBytes = termsIn.readVInt();
+
+    if (termState.bytes == null) {
+      termState.bytes = new byte[ArrayUtil.oversize(numBytes, 1)];
+      termState.bytesReader = new ByteArrayDataInput();
+    } else if (termState.bytes.length < numBytes) {
+      termState.bytes = new byte[ArrayUtil.oversize(numBytes, 1)];
+    }
+
+    termsIn.readBytes(termState.bytes, 0, numBytes);
+    termState.bytesReader.reset(termState.bytes, 0, numBytes);
+  }
+
+  @Override
+  public void nextTerm(FieldInfo fieldInfo, BlockTermState _termState)
+    throws IOException {
+    final IntBlockTermState termState = (IntBlockTermState) _termState;
+    final boolean isFirstTerm = termState.termBlockOrd == 0;
+    final boolean fieldHasPositions = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;
+    final boolean fieldHasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
+    final boolean fieldHasPayloads = fieldInfo.hasPayloads();
+
+    final DataInput in = termState.bytesReader;
+    if (isFirstTerm) {
+      termState.docStartFP = in.readVLong();
+      if (fieldHasPositions) {
+        termState.posStartFP = in.readVLong();
+        if (termState.totalTermFreq > BLOCK_SIZE) {
+          termState.lastPosBlockOffset = in.readVInt();
+        } else {
+          termState.lastPosBlockOffset = -1;
+        }
+        if ((fieldHasPayloads || fieldHasOffsets) && termState.totalTermFreq >= BLOCK_SIZE) {
+          termState.payStartFP = in.readVLong();
+        } else {
+          termState.payStartFP = -1;
+        }
+      }
+    } else {
+      termState.docStartFP += in.readVLong();
+      if (fieldHasPositions) {
+        termState.posStartFP += in.readVLong();
+        if (termState.totalTermFreq > BLOCK_SIZE) {
+          termState.lastPosBlockOffset = in.readVInt();
+        } else {
+          termState.lastPosBlockOffset = -1;
+        }
+        if ((fieldHasPayloads || fieldHasOffsets) && termState.totalTermFreq >= BLOCK_SIZE) {
+          long delta = in.readVLong();
+          if (termState.payStartFP == -1) {
+            termState.payStartFP = delta;
+          } else {
+            termState.payStartFP += delta;
+          }
+        }
+      }
+    }
+
+    if (termState.docFreq > BLOCK_SIZE) {
+      termState.skipOffset = in.readVInt();
+    } else {
+      termState.skipOffset = -1;
+    }
+  }
+    
+  @Override
+  public DocsEnum docs(FieldInfo fieldInfo, BlockTermState termState, Bits liveDocs, DocsEnum reuse, int flags) throws IOException {
+    BlockDocsEnum docsEnum;
+    if (reuse instanceof BlockDocsEnum) {
+      docsEnum = (BlockDocsEnum) reuse;
+      if (!docsEnum.canReuse(docIn, fieldInfo)) {
+        docsEnum = new BlockDocsEnum(fieldInfo);
+      }
+    } else {
+      docsEnum = new BlockDocsEnum(fieldInfo);
+    }
+    return docsEnum.reset(liveDocs, (IntBlockTermState) termState);
+  }
+
+  // TODO: specialize to liveDocs vs not, and freqs vs not
+  
+  @Override
+  public DocsAndPositionsEnum docsAndPositions(FieldInfo fieldInfo, BlockTermState termState, Bits liveDocs,
+                                               DocsAndPositionsEnum reuse, int flags)
+    throws IOException {
+
+    boolean indexHasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
+    boolean indexHasPayloads = fieldInfo.hasPayloads();
+
+    if ((!indexHasOffsets || (flags & DocsAndPositionsEnum.FLAG_OFFSETS) == 0) &&
+        (!indexHasPayloads || (flags & DocsAndPositionsEnum.FLAG_PAYLOADS) == 0)) {
+      BlockDocsAndPositionsEnum docsAndPositionsEnum;
+      if (reuse instanceof BlockDocsAndPositionsEnum) {
+        docsAndPositionsEnum = (BlockDocsAndPositionsEnum) reuse;
+        if (!docsAndPositionsEnum.canReuse(docIn, fieldInfo)) {
+          docsAndPositionsEnum = new BlockDocsAndPositionsEnum(fieldInfo);
+        }
+      } else {
+        docsAndPositionsEnum = new BlockDocsAndPositionsEnum(fieldInfo);
+      }
+      return docsAndPositionsEnum.reset(liveDocs, (IntBlockTermState) termState);
+    } else {
+      EverythingEnum everythingEnum;
+      if (reuse instanceof EverythingEnum) {
+        everythingEnum = (EverythingEnum) reuse;
+        if (!everythingEnum.canReuse(docIn, fieldInfo)) {
+          everythingEnum = new EverythingEnum(fieldInfo);
+        }
+      } else {
+        everythingEnum = new EverythingEnum(fieldInfo);
+      }
+      return everythingEnum.reset(liveDocs, (IntBlockTermState) termState);
+    }
+  }
+
+  final class BlockDocsEnum extends DocsEnum {
+    private final byte[] encoded;
+    
+    private final int[] docDeltaBuffer = new int[MAX_DATA_SIZE];
+    private final int[] freqBuffer = new int[MAX_DATA_SIZE];
+
+    private int docBufferUpto;
+
+    private BlockSkipReader skipper;
+    private boolean skipped;
+
+    final IndexInput startDocIn;
+
+    final IndexInput docIn;
+    final boolean indexHasFreq;
+    final boolean indexHasPos;
+    final boolean indexHasOffsets;
+    final boolean indexHasPayloads;
+
+    private int docFreq;                              // number of docs in this posting list
+    private int docUpto;                              // how many docs we've read
+    private int doc;                                  // doc we last read
+    private int accum;                                // accumulator for doc deltas
+    private int freq;                                 // freq we last read
+
+    // Where this term's postings start in the .doc file:
+    private long docTermStartFP;
+
+    // Where this term's skip data starts (after
+    // docTermStartFP) in the .doc file (or -1 if there is
+    // no skip data for this term):
+    private int skipOffset;
+
+    // docID for next skip point, we won't use skipper if 
+    // target docID is not larger than this
+    private int nextSkipDoc;
+
+    private Bits liveDocs;
+
+    public BlockDocsEnum(FieldInfo fieldInfo) throws IOException {
+      this.startDocIn = BlockPostingsReader.this.docIn;
+      this.docIn = startDocIn.clone();
+      indexHasFreq = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;
+      indexHasPos = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;
+      indexHasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
+      indexHasPayloads = fieldInfo.hasPayloads();
+      encoded = new byte[MAX_ENCODED_SIZE];    
+    }
+
+    public boolean canReuse(IndexInput docIn, FieldInfo fieldInfo) {
+      return docIn == startDocIn &&
+        indexHasFreq == (fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) >= 0) &&
+        indexHasPos == (fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0) &&
+        indexHasPayloads == fieldInfo.hasPayloads();
+    }
+    
+    public DocsEnum reset(Bits liveDocs, IntBlockTermState termState) throws IOException {
+      this.liveDocs = liveDocs;
+      // if (DEBUG) {
+      //   System.out.println("  FPR.reset: termState=" + termState);
+      // }
+      docFreq = termState.docFreq;
+      docTermStartFP = termState.docStartFP;
+      docIn.seek(docTermStartFP);
+      skipOffset = termState.skipOffset;
+
+      doc = -1;
+      if (!indexHasFreq) {
+        Arrays.fill(freqBuffer, 1);
+      }
+      accum = 0;
+      docUpto = 0;
+      nextSkipDoc = BLOCK_SIZE - 1; // we won't skip if target is found in first block
+      docBufferUpto = BLOCK_SIZE;
+      skipped = false;
+      return this;
+    }
+    
+    @Override
+    public int freq() throws IOException {
+      return freq;
+    }
+
+    @Override
+    public int docID() {
+      return doc;
+    }
+    
+    private void refillDocs() throws IOException {
+      final int left = docFreq - docUpto;
+      assert left > 0;
+
+      if (left >= BLOCK_SIZE) {
+        // if (DEBUG) {
+        //   System.out.println("    fill doc block from fp=" + docIn.getFilePointer());
+        // }
+        forUtil.readBlock(docIn, encoded, docDeltaBuffer);
+
+        if (indexHasFreq) {
+          // if (DEBUG) {
+          //   System.out.println("    fill freq block from fp=" + docIn.getFilePointer());
+          // }
+          forUtil.readBlock(docIn, encoded, freqBuffer);
+        }
+      } else {
+        // Read vInts:
+        // if (DEBUG) {
+        //   System.out.println("    fill last vInt block from fp=" + docIn.getFilePointer());
+        // }
+        readVIntBlock(docIn, docDeltaBuffer, freqBuffer, left, indexHasFreq);
+      }
+      docBufferUpto = 0;
+    }
+
+    @Override
+    public int nextDoc() throws IOException {
+      // if (DEBUG) {
+      //   System.out.println("\nFPR.nextDoc");
+      // }
+      while (true) {
+        // if (DEBUG) {
+        //   System.out.println("  docUpto=" + docUpto + " (of df=" + docFreq + ") docBufferUpto=" + docBufferUpto);
+        // }
+
+        if (docUpto == docFreq) {
+          // if (DEBUG) {
+          //   System.out.println("  return doc=END");
+          // }
+          return doc = NO_MORE_DOCS;
+        }
+        if (docBufferUpto == BLOCK_SIZE) {
+          refillDocs();
+        }
+
+        // if (DEBUG) {
+        //   System.out.println("    accum=" + accum + " docDeltaBuffer[" + docBufferUpto + "]=" + docDeltaBuffer[docBufferUpto]);
+        // }
+        accum += docDeltaBuffer[docBufferUpto];
+        docUpto++;
+
+        if (liveDocs == null || liveDocs.get(accum)) {
+          doc = accum;
+          freq = freqBuffer[docBufferUpto];
+          docBufferUpto++;
+          // if (DEBUG) {
+          //   System.out.println("  return doc=" + doc + " freq=" + freq);
+          // }
+          return doc;
+        }
+        // if (DEBUG) {
+        //   System.out.println("  doc=" + accum + " is deleted; try next doc");
+        // }
+        docBufferUpto++;
+      }
+    }
+
+    @Override
+    public int advance(int target) throws IOException {
+      // TODO: make frq block load lazy/skippable
+      // if (DEBUG) {
+      //   System.out.println("  FPR.advance target=" + target);
+      // }
+
+      // current skip docID < docIDs generated from current buffer <= next skip docID
+      // we don't need to skip if target is buffered already
+      if (docFreq > BLOCK_SIZE && target > nextSkipDoc) {
+
+        // if (DEBUG) {
+        //   System.out.println("load skipper");
+        // }
+
+        if (skipper == null) {
+          // Lazy init: first time this enum has ever been used for skipping
+          skipper = new BlockSkipReader(docIn.clone(),
+                                        BlockPostingsWriter.maxSkipLevels,
+                                        BLOCK_SIZE,
+                                        indexHasPos,
+                                        indexHasOffsets,
+                                        indexHasPayloads);
+        }
+
+        if (!skipped) {
+          assert skipOffset != -1;
+          // This is the first time this enum has skipped
+          // since reset() was called; load the skip data:
+          skipper.init(docTermStartFP+skipOffset, docTermStartFP, 0, 0, docFreq);
+          skipped = true;
+        }
+
+        // always plus one to fix the result, since skip position in BlockSkipReader 
+        // is a little different from MultiLevelSkipListReader
+        final int newDocUpto = skipper.skipTo(target) + 1; 
+
+        if (newDocUpto > docUpto) {
+          // Skipper moved
+          // if (DEBUG) {
+          //   System.out.println("skipper moved to docUpto=" + newDocUpto + " vs current=" + docUpto + "; docID=" + skipper.getDoc() + " fp=" + skipper.getDocPointer());
+          // }
+          assert newDocUpto % BLOCK_SIZE == 0 : "got " + newDocUpto;
+          docUpto = newDocUpto;
+
+          // Force to read next block
+          docBufferUpto = BLOCK_SIZE;
+          accum = skipper.getDoc();               // actually, this is just lastSkipEntry
+          docIn.seek(skipper.getDocPointer());    // now point to the block we want to search
+        }
+        // next time we call advance, this is used to 
+        // foresee whether skipper is necessary.
+        nextSkipDoc = skipper.getNextSkipDoc();
+      }
+      if (docUpto == docFreq) {
+        return doc = NO_MORE_DOCS;
+      }
+      if (docBufferUpto == BLOCK_SIZE) {
+        refillDocs();
+      }
+
+      // Now scan... this is an inlined/pared down version
+      // of nextDoc():
+      while (true) {
+        // if (DEBUG) {
+        //   System.out.println("  scan doc=" + accum + " docBufferUpto=" + docBufferUpto);
+        // }
+        accum += docDeltaBuffer[docBufferUpto];
+        docUpto++;
+
+        if (accum >= target) {
+          break;
+        }
+        docBufferUpto++;
+        if (docUpto == docFreq) {
+          return doc = NO_MORE_DOCS;
+        }
+      }
+
+      if (liveDocs == null || liveDocs.get(accum)) {
+        // if (DEBUG) {
+        //   System.out.println("  return doc=" + accum);
+        // }
+        freq = freqBuffer[docBufferUpto];
+        docBufferUpto++;
+        return doc = accum;
+      } else {
+        // if (DEBUG) {
+        //   System.out.println("  now do nextDoc()");
+        // }
+        docBufferUpto++;
+        return nextDoc();
+      }
+    }
+  }
+
+
+  final class BlockDocsAndPositionsEnum extends DocsAndPositionsEnum {
+    
+    private final byte[] encoded;
+
+    private final int[] docDeltaBuffer = new int[MAX_DATA_SIZE];
+    private final int[] freqBuffer = new int[MAX_DATA_SIZE];
+    private final int[] posDeltaBuffer = new int[MAX_DATA_SIZE];
+
+    private int docBufferUpto;
+    private int posBufferUpto;
+
+    private BlockSkipReader skipper;
+    private boolean skipped;
+
+    final IndexInput startDocIn;
+
+    final IndexInput docIn;
+    final IndexInput posIn;
+
+    final boolean indexHasOffsets;
+    final boolean indexHasPayloads;
+
+    private int docFreq;                              // number of docs in this posting list
+    private int docUpto;                              // how many docs we've read
+    private int doc;                                  // doc we last read
+    private int accum;                                // accumulator for doc deltas
+    private int freq;                                 // freq we last read
+    private int position;                             // current position
+
+    // how many positions "behind" we are; nextPosition must
+    // skip these to "catch up":
+    private int posPendingCount;
+
+    // Lazy pos seek: if != -1 then we must seek to this FP
+    // before reading positions:
+    private long posPendingFP;
+
+    // Where this term's postings start in the .doc file:
+    private long docTermStartFP;
+
+    // Where this term's postings start in the .pos file:
+    private long posTermStartFP;
+
+    // Where this term's payloads/offsets start in the .pay
+    // file:
+    private long payTermStartFP;
+
+    // File pointer where the last (vInt encoded) pos delta
+    // block is.  We need this to know whether to bulk
+    // decode vs vInt decode the block:
+    private long lastPosBlockFP;
+
+    // Where this term's skip data starts (after
+    // docTermStartFP) in the .doc file (or -1 if there is
+    // no skip data for this term):
+    private int skipOffset;
+
+    private int nextSkipDoc;
+
+    private Bits liveDocs;
+    
+    public BlockDocsAndPositionsEnum(FieldInfo fieldInfo) throws IOException {
+      this.startDocIn = BlockPostingsReader.this.docIn;
+      this.docIn = startDocIn.clone();
+      this.posIn = BlockPostingsReader.this.posIn.clone();
+      encoded = new byte[MAX_ENCODED_SIZE];
+      indexHasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
+      indexHasPayloads = fieldInfo.hasPayloads();
+    }
+
+    public boolean canReuse(IndexInput docIn, FieldInfo fieldInfo) {
+      return docIn == startDocIn &&
+        indexHasOffsets == (fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0) &&
+        indexHasPayloads == fieldInfo.hasPayloads();
+    }
+    
+    public DocsAndPositionsEnum reset(Bits liveDocs, IntBlockTermState termState) throws IOException {
+      this.liveDocs = liveDocs;
+      // if (DEBUG) {
+      //   System.out.println("  FPR.reset: termState=" + termState);
+      // }
+      docFreq = termState.docFreq;
+      docTermStartFP = termState.docStartFP;
+      posTermStartFP = termState.posStartFP;
+      payTermStartFP = termState.payStartFP;
+      docIn.seek(docTermStartFP);
+      skipOffset = termState.skipOffset;
+      posPendingFP = posTermStartFP;
+      posPendingCount = 0;
+      if (termState.totalTermFreq < BLOCK_SIZE) {
+        lastPosBlockFP = posTermStartFP;
+      } else if (termState.totalTermFreq == BLOCK_SIZE) {
+        lastPosBlockFP = -1;
+      } else {
+        lastPosBlockFP = posTermStartFP + termState.lastPosBlockOffset;
+      }
+
+      doc = -1;
+      accum = 0;
+      docUpto = 0;
+      nextSkipDoc = BLOCK_SIZE - 1;
+      docBufferUpto = BLOCK_SIZE;
+      skipped = false;
+      return this;
+    }
+    
+    @Override
+    public int freq() throws IOException {
+      return freq;
+    }
+
+    @Override
+    public int docID() {
+      return doc;
+    }
+
+    private void refillDocs() throws IOException {
+      final int left = docFreq - docUpto;
+      assert left > 0;
+
+      if (left >= BLOCK_SIZE) {
+        // if (DEBUG) {
+        //   System.out.println("    fill doc block from fp=" + docIn.getFilePointer());
+        // }
+        forUtil.readBlock(docIn, encoded, docDeltaBuffer);
+        // if (DEBUG) {
+        //   System.out.println("    fill freq block from fp=" + docIn.getFilePointer());
+        // }
+        forUtil.readBlock(docIn, encoded, freqBuffer);
+      } else {
+        // Read vInts:
+        // if (DEBUG) {
+        //   System.out.println("    fill last vInt doc block from fp=" + docIn.getFilePointer());
+        // }
+        readVIntBlock(docIn, docDeltaBuffer, freqBuffer, left, true);
+      }
+      docBufferUpto = 0;
+    }
+    
+    private void refillPositions() throws IOException {
+      // if (DEBUG) {
+      //   System.out.println("      refillPositions");
+      // }
+      if (posIn.getFilePointer() == lastPosBlockFP) {
+        // if (DEBUG) {
+        //   System.out.println("        vInt pos block @ fp=" + posIn.getFilePointer() + " hasPayloads=" + indexHasPayloads + " hasOffsets=" + indexHasOffsets);
+        // }
+        final int count = posIn.readVInt();
+        int payloadLength = 0;
+        for(int i=0;i<count;i++) {
+          int code = posIn.readVInt();
+          if (indexHasPayloads) {
+            if ((code & 1) != 0) {
+              payloadLength = posIn.readVInt();
+            }
+            posDeltaBuffer[i] = code >>> 1;
+            if (payloadLength != 0) {
+              posIn.seek(posIn.getFilePointer() + payloadLength);
+            }
+          } else {
+            posDeltaBuffer[i] = code;
+          }
+          if (indexHasOffsets) {
+            posIn.readVInt();
+            posIn.readVInt();
+          }
+        }
+      } else {
+        // if (DEBUG) {
+        //   System.out.println("        bulk pos block @ fp=" + posIn.getFilePointer());
+        // }
+        forUtil.readBlock(posIn, encoded, posDeltaBuffer);
+      }
+    }
+
+    @Override
+    public int nextDoc() throws IOException {
+      // if (DEBUG) {
+      //   System.out.println("  FPR.nextDoc");
+      // }
+      while (true) {
+        // if (DEBUG) {
+        //   System.out.println("    docUpto=" + docUpto + " (of df=" + docFreq + ") docBufferUpto=" + docBufferUpto);
+        // }
+        if (docUpto == docFreq) {
+          return doc = NO_MORE_DOCS;
+        }
+        if (docBufferUpto == BLOCK_SIZE) {
+          refillDocs();
+        }
+        // if (DEBUG) {
+        //   System.out.println("    accum=" + accum + " docDeltaBuffer[" + docBufferUpto + "]=" + docDeltaBuffer[docBufferUpto]);
+        // }
+        accum += docDeltaBuffer[docBufferUpto];
+        freq = freqBuffer[docBufferUpto];
+        posPendingCount += freq;
+        docBufferUpto++;
+        docUpto++;
+
+        if (liveDocs == null || liveDocs.get(accum)) {
+          doc = accum;
+          position = 0;
+          // if (DEBUG) {
+          //   System.out.println("    return doc=" + doc + " freq=" + freq + " posPendingCount=" + posPendingCount);
+          // }
+          return doc;
+        }
+        // if (DEBUG) {
+        //   System.out.println("    doc=" + accum + " is deleted; try next doc");
+        // }
+      }
+    }
+    
+    @Override
+    public int advance(int target) throws IOException {
+      // TODO: make frq block load lazy/skippable
+      // if (DEBUG) {
+      //   System.out.println("  FPR.advance target=" + target);
+      // }
+
+      if (docFreq > BLOCK_SIZE && target > nextSkipDoc) {
+        // if (DEBUG) {
+        //   System.out.println("    try skipper");
+        // }
+        if (skipper == null) {
+          // Lazy init: first time this enum has ever been used for skipping
+          // if (DEBUG) {
+          //   System.out.println("    create skipper");
+          // }
+          skipper = new BlockSkipReader(docIn.clone(),
+                                        BlockPostingsWriter.maxSkipLevels,
+                                        BLOCK_SIZE,
+                                        true,
+                                        indexHasOffsets,
+                                        indexHasPayloads);
+        }
+
+        if (!skipped) {
+          assert skipOffset != -1;
+          // This is the first time this enum has skipped
+          // since reset() was called; load the skip data:
+          // if (DEBUG) {
+          //   System.out.println("    init skipper");
+          // }
+          skipper.init(docTermStartFP+skipOffset, docTermStartFP, posTermStartFP, payTermStartFP, docFreq);
+          skipped = true;
+        }
+
+        final int newDocUpto = skipper.skipTo(target) + 1; 
+
+        if (newDocUpto > docUpto) {
+          // Skipper moved
+          // if (DEBUG) {
+          //   System.out.println("    skipper moved to docUpto=" + newDocUpto + " vs current=" + docUpto + "; docID=" + skipper.getDoc() + " fp=" + skipper.getDocPointer() + " pos.fp=" + skipper.getPosPointer() + " pos.bufferUpto=" + skipper.getPosBufferUpto());
+          // }
+
+          assert newDocUpto % BLOCK_SIZE == 0 : "got " + newDocUpto;
+          docUpto = newDocUpto;
+
+          // Force to read next block
+          docBufferUpto = BLOCK_SIZE;
+          accum = skipper.getDoc();
+          docIn.seek(skipper.getDocPointer());
+          posPendingFP = skipper.getPosPointer();
+          posPendingCount = skipper.getPosBufferUpto();
+        }
+        nextSkipDoc = skipper.getNextSkipDoc();
+      }
+      if (docUpto == docFreq) {
+        return doc = NO_MORE_DOCS;
+      }
+      if (docBufferUpto == BLOCK_SIZE) {
+        refillDocs();
+      }
+
+      // Now scan... this is an inlined/pared down version
+      // of nextDoc():
+      while (true) {
+        // if (DEBUG) {
+        //   System.out.println("  scan doc=" + accum + " docBufferUpto=" + docBufferUpto);
+        // }
+        if (docUpto == docFreq) {
+          return doc = NO_MORE_DOCS;
+        }
+        accum += docDeltaBuffer[docBufferUpto];
+        freq = freqBuffer[docBufferUpto];
+        posPendingCount += freq;
+        docBufferUpto++;
+        docUpto++;
+
+        if (accum >= target) {
+          break;
+        }
+        if (docUpto == docFreq) {
+          return doc = NO_MORE_DOCS;
+        }
+      }
+
+      if (liveDocs == null || liveDocs.get(accum)) {
+        // if (DEBUG) {
+        //   System.out.println("  return doc=" + accum);
+        // }
+        position = 0;
+        return doc = accum;
+      } else {
+        // if (DEBUG) {
+        //   System.out.println("  now do nextDoc()");
+        // }
+        return nextDoc();
+      }
+    }
+
+    // TODO: in theory we could avoid loading frq block
+    // when not needed, ie, use skip data to load how far to
+    // seek the pos pointer ... instead of having to load frq
+    // blocks only to sum up how many positions to skip
+    private void skipPositions() throws IOException {
+      // Skip positions now:
+      int toSkip = posPendingCount - freq;
+      // if (DEBUG) {
+      //   System.out.println("      FPR.skipPositions: toSkip=" + toSkip);
+      // }
+
+      final int leftInBlock = BLOCK_SIZE - posBufferUpto;
+      if (toSkip < leftInBlock) {
+        posBufferUpto += toSkip;
+        // if (DEBUG) {
+        //   System.out.println("        skip w/in block to posBufferUpto=" + posBufferUpto);
+        // }
+      } else {
+        toSkip -= leftInBlock;
+        while(toSkip >= BLOCK_SIZE) {
+          // if (DEBUG) {
+          //   System.out.println("        skip whole block @ fp=" + posIn.getFilePointer());
+          // }
+          assert posIn.getFilePointer() != lastPosBlockFP;
+          forUtil.skipBlock(posIn);
+          toSkip -= BLOCK_SIZE;
+        }
+        refillPositions();
+        posBufferUpto = toSkip;
+        // if (DEBUG) {
+        //   System.out.println("        skip w/in block to posBufferUpto=" + posBufferUpto);
+        // }
+      }
+
+      position = 0;
+    }
+
+    @Override
+    public int nextPosition() throws IOException {
+      // if (DEBUG) {
+      //   System.out.println("    FPR.nextPosition posPendingCount=" + posPendingCount + " posBufferUpto=" + posBufferUpto);
+      // }
+      if (posPendingFP != -1) {
+        // if (DEBUG) {
+        //   System.out.println("      seek to pendingFP=" + posPendingFP);
+        // }
+        posIn.seek(posPendingFP);
+        posPendingFP = -1;
+
+        // Force buffer refill:
+        posBufferUpto = BLOCK_SIZE;
+      }
+
+      if (posPendingCount > freq) {
+        skipPositions();
+        posPendingCount = freq;
+      }
+
+      if (posBufferUpto == BLOCK_SIZE) {
+        refillPositions();
+        posBufferUpto = 0;
+      }
+      position += posDeltaBuffer[posBufferUpto++];
+      posPendingCount--;
+      // if (DEBUG) {
+      //   System.out.println("      return pos=" + position);
+      // }
+      return position;
+    }
+
+    @Override
+    public int startOffset() {
+      return -1;
+    }
+  
+    @Override
+    public int endOffset() {
+      return -1;
+    }
+  
+    @Override
+    public BytesRef getPayload() {
+      return null;
+    }
+  }
+
+  // Also handles payloads + offsets
+  final class EverythingEnum extends DocsAndPositionsEnum {
+    
+    private final byte[] encoded;
+
+    private final int[] docDeltaBuffer = new int[MAX_DATA_SIZE];
+    private final int[] freqBuffer = new int[MAX_DATA_SIZE];
+    private final int[] posDeltaBuffer = new int[MAX_DATA_SIZE];
+
+    private final int[] payloadLengthBuffer;
+    private final int[] offsetStartDeltaBuffer;
+    private final int[] offsetLengthBuffer;
+
+    private byte[] payloadBytes;
+    private int payloadByteUpto;
+    private int payloadLength;
+
+    private int lastStartOffset;
+    private int startOffset;
+    private int endOffset;
+
+    private int docBufferUpto;
+    private int posBufferUpto;
+
+    private BlockSkipReader skipper;
+    private boolean skipped;
+
+    final IndexInput startDocIn;
+
+    final IndexInput docIn;
+    final IndexInput posIn;
+    final IndexInput payIn;
+    final BytesRef payload;
+
+    final boolean indexHasOffsets;
+    final boolean indexHasPayloads;
+
+    private int docFreq;                              // number of docs in this posting list
+    private int docUpto;                              // how many docs we've read
+    private int doc;                                  // doc we last read
+    private int accum;                                // accumulator for doc deltas
+    private int freq;                                 // freq we last read
+    private int position;                             // current position
+
+    // how many positions "behind" we are; nextPosition must
+    // skip these to "catch up":
+    private int posPendingCount;
+
+    // Lazy pos seek: if != -1 then we must seek to this FP
+    // before reading positions:
+    private long posPendingFP;
+
+    // Lazy pay seek: if != -1 then we must seek to this FP
+    // before reading payloads/offsets:
+    private long payPendingFP;
+
+    // Where this term's postings start in the .doc file:
+    private long docTermStartFP;
+
+    // Where this term's postings start in the .pos file:
+    private long posTermStartFP;
+
+    // Where this term's payloads/offsets start in the .pay
+    // file:
+    private long payTermStartFP;
+
+    // File pointer where the last (vInt encoded) pos delta
+    // block is.  We need this to know whether to bulk
+    // decode vs vInt decode the block:
+    private long lastPosBlockFP;
+
+    // Where this term's skip data starts (after
+    // docTermStartFP) in the .doc file (or -1 if there is
+    // no skip data for this term):
+    private int skipOffset;
+
+    private int nextSkipDoc;
+
+    private Bits liveDocs;
+    
+    public EverythingEnum(FieldInfo fieldInfo) throws IOException {
+      this.startDocIn = BlockPostingsReader.this.docIn;
+      this.docIn = startDocIn.clone();
+      this.posIn = BlockPostingsReader.this.posIn.clone();
+      this.payIn = BlockPostingsReader.this.payIn.clone();
+      encoded = new byte[MAX_ENCODED_SIZE];
+      indexHasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
+      if (indexHasOffsets) {
+        offsetStartDeltaBuffer = new int[MAX_DATA_SIZE];
+        offsetLengthBuffer = new int[MAX_DATA_SIZE];
+      } else {
+        offsetStartDeltaBuffer = null;
+        offsetLengthBuffer = null;
+        startOffset = -1;
+        endOffset = -1;
+      }
+
+      indexHasPayloads = fieldInfo.hasPayloads();
+      if (indexHasPayloads) {
+        payloadLengthBuffer = new int[MAX_DATA_SIZE];
+        payloadBytes = new byte[128];
+        payload = new BytesRef();
+      } else {
+        payloadLengthBuffer = null;
+        payloadBytes = null;
+        payload = null;
+      }
+    }
+
+    public boolean canReuse(IndexInput docIn, FieldInfo fieldInfo) {
+      return docIn == startDocIn &&
+        indexHasOffsets == (fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0) &&
+        indexHasPayloads == fieldInfo.hasPayloads();
+    }
+    
+    public EverythingEnum reset(Bits liveDocs, IntBlockTermState termState) throws IOException {
+      this.liveDocs = liveDocs;
+      // if (DEBUG) {
+      //   System.out.println("  FPR.reset: termState=" + termState);
+      // }
+      docFreq = termState.docFreq;
+      docTermStartFP = termState.docStartFP;
+      posTermStartFP = termState.posStartFP;
+      payTermStartFP = termState.payStartFP;
+      docIn.seek(docTermStartFP);
+      skipOffset = termState.skipOffset;
+      posPendingFP = posTermStartFP;
+      payPendingFP = payTermStartFP;
+      posPendingCount = 0;
+      if (termState.totalTermFreq < BLOCK_SIZE) {
+        lastPosBlockFP = posTermStartFP;
+      } else if (termState.totalTermFreq == BLOCK_SIZE) {
+        lastPosBlockFP = -1;
+      } else {
+        lastPosBlockFP = posTermStartFP + termState.lastPosBlockOffset;
+      }
+
+      doc = -1;
+      accum = 0;
+      docUpto = 0;
+      nextSkipDoc = BLOCK_SIZE - 1;
+      docBufferUpto = BLOCK_SIZE;
+      skipped = false;
+      return this;
+    }
+    
+    @Override
+    public int freq() throws IOException {
+      return freq;
+    }
+
+    @Override
+    public int docID() {
+      return doc;
+    }
+
+    private void refillDocs() throws IOException {
+      final int left = docFreq - docUpto;
+      assert left > 0;
+
+      if (left >= BLOCK_SIZE) {
+        // if (DEBUG) {
+        //   System.out.println("    fill doc block from fp=" + docIn.getFilePointer());
+        // }
+        forUtil.readBlock(docIn, encoded, docDeltaBuffer);
+        // if (DEBUG) {
+        //   System.out.println("    fill freq block from fp=" + docIn.getFilePointer());
+        // }
+        forUtil.readBlock(docIn, encoded, freqBuffer);
+      } else {
+        // if (DEBUG) {
+        //   System.out.println("    fill last vInt doc block from fp=" + docIn.getFilePointer());
+        // }
+        readVIntBlock(docIn, docDeltaBuffer, freqBuffer, left, true);
+      }
+      docBufferUpto = 0;
+    }
+    
+    private void refillPositions() throws IOException {
+      // if (DEBUG) {
+      //   System.out.println("      refillPositions");
+      // }
+      if (posIn.getFilePointer() == lastPosBlockFP) {
+        // if (DEBUG) {
+        //   System.out.println("        vInt pos block @ fp=" + posIn.getFilePointer() + " hasPayloads=" + indexHasPayloads + " hasOffsets=" + indexHasOffsets);
+        // }
+        final int count = posIn.readVInt();
+        int payloadLength = 0;
+        payloadByteUpto = 0;
+        for(int i=0;i<count;i++) {
+          int code = posIn.readVInt();
+          if (indexHasPayloads) {
+            if ((code & 1) != 0) {
+              payloadLength = posIn.readVInt();
+            }
+            // if (DEBUG) {
+            //   System.out.println("        i=" + i + " payloadLen=" + payloadLength);
+            // }
+            payloadLengthBuffer[i] = payloadLength;
+            posDeltaBuffer[i] = code >>> 1;
+            if (payloadLength != 0) {
+              if (payloadByteUpto + payloadLength > payloadBytes.length) {
+                payloadBytes = ArrayUtil.grow(payloadBytes, payloadByteUpto + payloadLength);
+              }
+              //System.out.println("          read payload @ pos.fp=" + posIn.getFilePointer());
+              posIn.readBytes(payloadBytes, payloadByteUpto, payloadLength);
+              payloadByteUpto += payloadLength;
+            }
+          } else {
+            posDeltaBuffer[i] = code;
+          }
+
+          if (indexHasOffsets) {
+            // if (DEBUG) {
+            //   System.out.println("        i=" + i + " read offsets from posIn.fp=" + posIn.getFilePointer());
+            // }
+            offsetStartDeltaBuffer[i] = posIn.readVInt();
+            offsetLengthBuffer[i] = posIn.readVInt();
+            // if (DEBUG) {
+            //   System.out.println("          startOffDelta=" + offsetStartDeltaBuffer[i] + " offsetLen=" + offsetLengthBuffer[i]);
+            // }
+          }
+        }
+        payloadByteUpto = 0;
+      } else {
+        // if (DEBUG) {
+        //   System.out.println("        bulk pos block @ fp=" + posIn.getFilePointer());
+        // }
+        forUtil.readBlock(posIn, encoded, posDeltaBuffer);
+
+        if (indexHasPayloads) {
+          // if (DEBUG) {
+          //   System.out.println("        bulk payload block @ pay.fp=" + payIn.getFilePointer());
+          // }
+          forUtil.readBlock(payIn, encoded, payloadLengthBuffer);
+          int numBytes = payIn.readVInt();
+          // if (DEBUG) {
+          //   System.out.println("        " + numBytes + " payload bytes @ pay.fp=" + payIn.getFilePointer());
+          // }
+          if (numBytes > payloadBytes.length) {
+            payloadBytes = ArrayUtil.grow(payloadBytes, numBytes);
+          }
+          payIn.readBytes(payloadBytes, 0, numBytes);
+          payloadByteUpto = 0;
+        }
+
+        if (indexHasOffsets) {
+          // if (DEBUG) {
+          //   System.out.println("        bulk offset block @ pay.fp=" + payIn.getFilePointer());
+          // }
+          forUtil.readBlock(payIn, encoded, offsetStartDeltaBuffer);
+          forUtil.readBlock(payIn, encoded, offsetLengthBuffer);
+        }
+      }
+    }
+
+    @Override
+    public int nextDoc() throws IOException {
+      // if (DEBUG) {
+      //   System.out.println("  FPR.nextDoc");
+      // }
+      while (true) {
+        // if (DEBUG) {
+        //   System.out.println("    docUpto=" + docUpto + " (of df=" + docFreq + ") docBufferUpto=" + docBufferUpto);
+        // }
+        if (docUpto == docFreq) {
+          return doc = NO_MORE_DOCS;
+        }
+        if (docBufferUpto == BLOCK_SIZE) {
+          refillDocs();
+        }
+        // if (DEBUG) {
+        //   System.out.println("    accum=" + accum + " docDeltaBuffer[" + docBufferUpto + "]=" + docDeltaBuffer[docBufferUpto]);
+        // }
+        accum += docDeltaBuffer[docBufferUpto];
+        freq = freqBuffer[docBufferUpto];
+        posPendingCount += freq;
+        docBufferUpto++;
+        docUpto++;
+
+        if (liveDocs == null || liveDocs.get(accum)) {
+          doc = accum;
+          // if (DEBUG) {
+          //   System.out.println("    return doc=" + doc + " freq=" + freq + " posPendingCount=" + posPendingCount);
+          // }
+          position = 0;
+          lastStartOffset = 0;
+          return doc;
+        }
+
+        // if (DEBUG) {
+        //   System.out.println("    doc=" + accum + " is deleted; try next doc");
+        // }
+      }
+    }
+    
+    @Override
+    public int advance(int target) throws IOException {
+      // TODO: make frq block load lazy/skippable
+      // if (DEBUG) {
+      //   System.out.println("  FPR.advance target=" + target);
+      // }
+
+      if (docFreq > BLOCK_SIZE && target > nextSkipDoc) {
+
+        // if (DEBUG) {
+        //   System.out.println("    try skipper");
+        // }
+
+        if (skipper == null) {
+          // Lazy init: first time this enum has ever been used for skipping
+          // if (DEBUG) {
+          //   System.out.println("    create skipper");
+          // }
+          skipper = new BlockSkipReader(docIn.clone(),
+                                        BlockPostingsWriter.maxSkipLevels,
+                                        BLOCK_SIZE,
+                                        true,
+                                        indexHasOffsets,
+                                        indexHasPayloads);
+        }
+
+        if (!skipped) {
+          assert skipOffset != -1;
+          // This is the first time this enum has skipped
+          // since reset() was called; load the skip data:
+          // if (DEBUG) {
+          //   System.out.println("    init skipper");
+          // }
+          skipper.init(docTermStartFP+skipOffset, docTermStartFP, posTermStartFP, payTermStartFP, docFreq);
+          skipped = true;
+        }
+
+        final int newDocUpto = skipper.skipTo(target) + 1; 
+
+        if (newDocUpto > docUpto) {
+          // Skipper moved
+          // if (DEBUG) {
+          //   System.out.println("    skipper moved to docUpto=" + newDocUpto + " vs current=" + docUpto + "; docID=" + skipper.getDoc() + " fp=" + skipper.getDocPointer() + " pos.fp=" + skipper.getPosPointer() + " pos.bufferUpto=" + skipper.getPosBufferUpto() + " pay.fp=" + skipper.getPayPointer() + " lastStartOffset=" + lastStartOffset);
+          // }
+          assert newDocUpto % BLOCK_SIZE == 0 : "got " + newDocUpto;
+          docUpto = newDocUpto;
+
+          // Force to read next block
+          docBufferUpto = BLOCK_SIZE;
+          accum = skipper.getDoc();
+          docIn.seek(skipper.getDocPointer());
+          posPendingFP = skipper.getPosPointer();
+          payPendingFP = skipper.getPayPointer();
+          posPendingCount = skipper.getPosBufferUpto();
+          lastStartOffset = skipper.getStartOffset();
+          payloadByteUpto = skipper.getPayloadByteUpto();
+        }
+        nextSkipDoc = skipper.getNextSkipDoc();
+      }
+      if (docUpto == docFreq) {
+        return doc = NO_MORE_DOCS;
+      }
+      if (docBufferUpto == BLOCK_SIZE) {
+        refillDocs();
+      }
+
+      // Now scan:
+      while (true) {
+        // if (DEBUG) {
+        //   System.out.println("  scan doc=" + accum + " docBufferUpto=" + docBufferUpto);
+        // }
+        accum += docDeltaBuffer[docBufferUpto];
+        freq = freqBuffer[docBufferUpto];
+        posPendingCount += freq;
+        docBufferUpto++;
+        docUpto++;
+
+        if (accum >= target) {
+          break;
+        }
+        if (docUpto == docFreq) {
+          return doc = NO_MORE_DOCS;
+        }
+      }
+
+      if (liveDocs == null || liveDocs.get(accum)) {
+        // if (DEBUG) {
+        //   System.out.println("  return doc=" + accum);
+        // }
+        position = 0;
+        lastStartOffset = 0;
+        return doc = accum;
+      } else {
+        // if (DEBUG) {
+        //   System.out.println("  now do nextDoc()");
+        // }
+        return nextDoc();
+      }
+    }
+
+    // TODO: in theory we could avoid loading frq block
+    // when not needed, ie, use skip data to load how far to
+    // seek the pos pointer ... instead of having to load frq
+    // blocks only to sum up how many positions to skip
+    private void skipPositions() throws IOException {
+      // Skip positions now:
+      int toSkip = posPendingCount - freq;
+      // if (DEBUG) {
+      //   System.out.println("      FPR.skipPositions: toSkip=" + toSkip);
+      // }
+
+      final int leftInBlock = BLOCK_SIZE - posBufferUpto;
+      if (toSkip < leftInBlock) {
+        int end = posBufferUpto + toSkip;
+        while(posBufferUpto < end) {
+          if (indexHasPayloads) {
+            payloadByteUpto += payloadLengthBuffer[posBufferUpto];
+          }
+          posBufferUpto++;
+        }
+        // if (DEBUG) {
+        //   System.out.println("        skip w/in block to posBufferUpto=" + posBufferUpto);
+        // }
+      } else {
+        toSkip -= leftInBlock;
+        while(toSkip >= BLOCK_SIZE) {
+          // if (DEBUG) {
+          //   System.out.println("        skip whole block @ fp=" + posIn.getFilePointer());
+          // }
+          assert posIn.getFilePointer() != lastPosBlockFP;
+          forUtil.skipBlock(posIn);
+
+          if (indexHasPayloads) {
+            // Skip payloadLength block:
+            forUtil.skipBlock(payIn);
+
+            // Skip payloadBytes block:
+            int numBytes = payIn.readVInt();
+            payIn.seek(payIn.getFilePointer() + numBytes);
+          }
+
+          if (indexHasOffsets) {
+            forUtil.skipBlock(payIn);
+            forUtil.skipBlock(payIn);
+          }
+          toSkip -= BLOCK_SIZE;
+        }
+        refillPositions();
+        payloadByteUpto = 0;
+        posBufferUpto = 0;
+        while(posBufferUpto < toSkip) {
+          if (indexHasPayloads) {
+            payloadByteUpto += payloadLengthBuffer[posBufferUpto];
+          }
+          posBufferUpto++;
+        }
+        // if (DEBUG) {
+        //   System.out.println("        skip w/in block to posBufferUpto=" + posBufferUpto);
+        // }
+      }
+
+      position = 0;
+      lastStartOffset = 0;
+    }
+
+    @Override
+    public int nextPosition() throws IOException {
+      // if (DEBUG) {
+      //   System.out.println("    FPR.nextPosition posPendingCount=" + posPendingCount + " posBufferUpto=" + posBufferUpto + " payloadByteUpto=" + payloadByteUpto)// ;
+      // }
+      if (posPendingFP != -1) {
+        // if (DEBUG) {
+        //   System.out.println("      seek pos to pendingFP=" + posPendingFP);
+        // }
+        posIn.seek(posPendingFP);
+        posPendingFP = -1;
+
+        if (payPendingFP != -1) {
+          // if (DEBUG) {
+          //   System.out.println("      seek pay to pendingFP=" + payPendingFP);
+          // }
+          payIn.seek(payPendingFP);
+          payPendingFP = -1;
+        }
+
+        // Force buffer refill:
+        posBufferUpto = BLOCK_SIZE;
+      }
+
+      if (posPendingCount > freq) {
+        skipPositions();
+        posPendingCount = freq;
+      }
+
+      if (posBufferUpto == BLOCK_SIZE) {
+        refillPositions();
+        posBufferUpto = 0;
+      }
+      position += posDeltaBuffer[posBufferUpto];
+
+      if (indexHasPayloads) {
+        payloadLength = payloadLengthBuffer[posBufferUpto];
+        payload.bytes = payloadBytes;
+        payload.offset = payloadByteUpto;
+        payload.length = payloadLength;
+        payloadByteUpto += payloadLength;
+      }
+
+      if (indexHasOffsets) {
+        startOffset = lastStartOffset + offsetStartDeltaBuffer[posBufferUpto];
+        endOffset = startOffset + offsetLengthBuffer[posBufferUpto];
+        lastStartOffset = startOffset;
+      }
+
+      posBufferUpto++;
+      posPendingCount--;
+      // if (DEBUG) {
+      //   System.out.println("      return pos=" + position);
+      // }
+      return position;
+    }
+
+    @Override
+    public int startOffset() {
+      return startOffset;
+    }
+  
+    @Override
+    public int endOffset() {
+      return endOffset;
+    }
+  
+    @Override
+    public BytesRef getPayload() {
+      // if (DEBUG) {
+      //   System.out.println("    FPR.getPayload payloadLength=" + payloadLength + " payloadByteUpto=" + payloadByteUpto);
+      // }
+      if (payloadLength == 0) {
+        return null;
+      } else {
+        return payload;
+      }
+    }
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/block/BlockPostingsWriter.java b/lucene/codecs/src/java/org/apache/lucene/codecs/block/BlockPostingsWriter.java
new file mode 100644
index 0000000..70ca7ef
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/block/BlockPostingsWriter.java
@@ -0,0 +1,563 @@
+package org.apache.lucene.codecs.block;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import static org.apache.lucene.codecs.block.BlockPostingsFormat.BLOCK_SIZE;
+import static org.apache.lucene.codecs.block.ForUtil.MAX_DATA_SIZE;
+import static org.apache.lucene.codecs.block.ForUtil.MAX_ENCODED_SIZE;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.PostingsWriterBase;
+import org.apache.lucene.codecs.TermStats;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.store.RAMOutputStream;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.packed.PackedInts;
+
+
+/**
+ * Concrete class that writes docId(maybe frq,pos,offset,payloads) list
+ * with postings format.
+ *
+ * Postings list for each term will be stored separately. 
+ *
+ * @see BlockSkipWriter for details about skipping setting and postings layout.
+ *
+ */
+final class BlockPostingsWriter extends PostingsWriterBase {
+
+  /** 
+   * Expert: The maximum number of skip levels. Smaller values result in 
+   * slightly smaller indexes, but slower skipping in big posting lists.
+   */
+  static final int maxSkipLevels = 10;
+
+  final static String TERMS_CODEC = "BlockPostingsWriterTerms";
+  final static String DOC_CODEC = "BlockPostingsWriterDoc";
+  final static String POS_CODEC = "BlockPostingsWriterPos";
+  final static String PAY_CODEC = "BlockPostingsWriterPay";
+
+  // Increment version to change it:
+  final static int VERSION_START = 0;
+  final static int VERSION_CURRENT = VERSION_START;
+
+  final IndexOutput docOut;
+  final IndexOutput posOut;
+  final IndexOutput payOut;
+
+  private IndexOutput termsOut;
+
+  // How current field indexes postings:
+  private boolean fieldHasFreqs;
+  private boolean fieldHasPositions;
+  private boolean fieldHasOffsets;
+  private boolean fieldHasPayloads;
+
+  // Holds starting file pointers for each term:
+  private long docTermStartFP;
+  private long posTermStartFP;
+  private long payTermStartFP;
+
+  final int[] docDeltaBuffer;
+  final int[] freqBuffer;
+  private int docBufferUpto;
+
+  final int[] posDeltaBuffer;
+  final int[] payloadLengthBuffer;
+  final int[] offsetStartDeltaBuffer;
+  final int[] offsetLengthBuffer;
+  private int posBufferUpto;
+
+  private byte[] payloadBytes;
+  private int payloadByteUpto;
+
+  private int lastBlockDocID;
+  private long lastBlockPosFP;
+  private long lastBlockPayFP;
+  private int lastBlockPosBufferUpto;
+  private int lastBlockStartOffset;
+  private int lastBlockPayloadByteUpto;
+
+  private int lastDocID;
+  private int lastPosition;
+  private int lastStartOffset;
+  private int docCount;
+
+  final byte[] encoded;
+
+  private final ForUtil forUtil;
+  private final BlockSkipWriter skipWriter;
+  
+  public BlockPostingsWriter(SegmentWriteState state, float acceptableOverheadRatio) throws IOException {
+    super();
+
+    docOut = state.directory.createOutput(IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, BlockPostingsFormat.DOC_EXTENSION),
+                                          state.context);
+    IndexOutput posOut = null;
+    IndexOutput payOut = null;
+    boolean success = false;
+    try {
+      CodecUtil.writeHeader(docOut, DOC_CODEC, VERSION_CURRENT);
+      forUtil = new ForUtil(acceptableOverheadRatio, docOut);
+      if (state.fieldInfos.hasProx()) {
+        posDeltaBuffer = new int[MAX_DATA_SIZE];
+        posOut = state.directory.createOutput(IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, BlockPostingsFormat.POS_EXTENSION),
+                                              state.context);
+        CodecUtil.writeHeader(posOut, POS_CODEC, VERSION_CURRENT);
+
+        if (state.fieldInfos.hasPayloads()) {
+          payloadBytes = new byte[128];
+          payloadLengthBuffer = new int[MAX_DATA_SIZE];
+        } else {
+          payloadBytes = null;
+          payloadLengthBuffer = null;
+        }
+
+        if (state.fieldInfos.hasOffsets()) {
+          offsetStartDeltaBuffer = new int[MAX_DATA_SIZE];
+          offsetLengthBuffer = new int[MAX_DATA_SIZE];
+        } else {
+          offsetStartDeltaBuffer = null;
+          offsetLengthBuffer = null;
+        }
+
+        if (state.fieldInfos.hasPayloads() || state.fieldInfos.hasOffsets()) {
+          payOut = state.directory.createOutput(IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, BlockPostingsFormat.PAY_EXTENSION),
+                                                state.context);
+          CodecUtil.writeHeader(payOut, PAY_CODEC, VERSION_CURRENT);
+        }
+      } else {
+        posDeltaBuffer = null;
+        payloadLengthBuffer = null;
+        offsetStartDeltaBuffer = null;
+        offsetLengthBuffer = null;
+        payloadBytes = null;
+      }
+      this.payOut = payOut;
+      this.posOut = posOut;
+      success = true;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(docOut, posOut, payOut);
+      }
+    }
+
+    docDeltaBuffer = new int[MAX_DATA_SIZE];
+    freqBuffer = new int[MAX_DATA_SIZE];
+
+    // TODO: should we try skipping every 2/4 blocks...?
+    skipWriter = new BlockSkipWriter(maxSkipLevels,
+                                     BLOCK_SIZE, 
+                                     state.segmentInfo.getDocCount(),
+                                     docOut,
+                                     posOut,
+                                     payOut);
+
+    encoded = new byte[MAX_ENCODED_SIZE];
+  }
+
+  public BlockPostingsWriter(SegmentWriteState state) throws IOException {
+    this(state, PackedInts.COMPACT);
+  }
+
+  @Override
+  public void start(IndexOutput termsOut) throws IOException {
+    this.termsOut = termsOut;
+    CodecUtil.writeHeader(termsOut, TERMS_CODEC, VERSION_CURRENT);
+    termsOut.writeVInt(BLOCK_SIZE);
+  }
+
+  @Override
+  public void setField(FieldInfo fieldInfo) {
+    IndexOptions indexOptions = fieldInfo.getIndexOptions();
+    fieldHasFreqs = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;
+    fieldHasPositions = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;
+    fieldHasOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
+    fieldHasPayloads = fieldInfo.hasPayloads();
+    skipWriter.setField(fieldHasPositions, fieldHasOffsets, fieldHasPayloads);
+  }
+
+  @Override
+  public void startTerm() {
+    docTermStartFP = docOut.getFilePointer();
+    if (fieldHasPositions) {
+      posTermStartFP = posOut.getFilePointer();
+      if (fieldHasPayloads || fieldHasOffsets) {
+        payTermStartFP = payOut.getFilePointer();
+      }
+    }
+    lastDocID = 0;
+    lastBlockDocID = -1;
+    // if (DEBUG) {
+    //   System.out.println("FPW.startTerm startFP=" + docTermStartFP);
+    // }
+    skipWriter.resetSkip();
+  }
+
+  @Override
+  public void startDoc(int docID, int termDocFreq) throws IOException {
+    // if (DEBUG) {
+    //   System.out.println("FPW.startDoc docID["+docBufferUpto+"]=" + docID);
+    // }
+    // Have collected a block of docs, and get a new doc. 
+    // Should write skip data as well as postings list for
+    // current block.
+    if (lastBlockDocID != -1 && docBufferUpto == 0) {
+      // if (DEBUG) {
+      //   System.out.println("  bufferSkip at writeBlock: lastDocID=" + lastBlockDocID + " docCount=" + (docCount-1));
+      // }
+      skipWriter.bufferSkip(lastBlockDocID, docCount, lastBlockPosFP, lastBlockPayFP, lastBlockPosBufferUpto, lastBlockStartOffset, lastBlockPayloadByteUpto);
+    }
+
+    final int docDelta = docID - lastDocID;
+
+    if (docID < 0 || (docCount > 0 && docDelta <= 0)) {
+      throw new CorruptIndexException("docs out of order (" + docID + " <= " + lastDocID + " ) (docOut: " + docOut + ")");
+    }
+
+    docDeltaBuffer[docBufferUpto] = docDelta;
+    // if (DEBUG) {
+    //   System.out.println("  docDeltaBuffer[" + docBufferUpto + "]=" + docDelta);
+    // }
+    if (fieldHasFreqs) {
+      freqBuffer[docBufferUpto] = termDocFreq;
+    }
+    docBufferUpto++;
+    docCount++;
+
+    if (docBufferUpto == BLOCK_SIZE) {
+      // if (DEBUG) {
+      //   System.out.println("  write docDelta block @ fp=" + docOut.getFilePointer());
+      // }
+      forUtil.writeBlock(docDeltaBuffer, encoded, docOut);
+      if (fieldHasFreqs) {
+        // if (DEBUG) {
+        //   System.out.println("  write freq block @ fp=" + docOut.getFilePointer());
+        // }
+        forUtil.writeBlock(freqBuffer, encoded, docOut);
+      }
+      // NOTE: don't set docBufferUpto back to 0 here;
+      // finishDoc will do so (because it needs to see that
+      // the block was filled so it can save skip data)
+    }
+
+
+    lastDocID = docID;
+    lastPosition = 0;
+    lastStartOffset = 0;
+  }
+
+  /** Add a new position & payload */
+  @Override
+  public void addPosition(int position, BytesRef payload, int startOffset, int endOffset) throws IOException {
+    // if (DEBUG) {
+    //   System.out.println("FPW.addPosition pos=" + position + " posBufferUpto=" + posBufferUpto + (fieldHasPayloads ? " payloadByteUpto=" + payloadByteUpto: ""));
+    // }
+    posDeltaBuffer[posBufferUpto] = position - lastPosition;
+    if (fieldHasPayloads) {
+      if (payload == null || payload.length == 0) {
+        // no payload
+        payloadLengthBuffer[posBufferUpto] = 0;
+      } else {
+        payloadLengthBuffer[posBufferUpto] = payload.length;
+        if (payloadByteUpto + payload.length > payloadBytes.length) {
+          payloadBytes = ArrayUtil.grow(payloadBytes, payloadByteUpto + payload.length);
+        }
+        System.arraycopy(payload.bytes, payload.offset, payloadBytes, payloadByteUpto, payload.length);
+        payloadByteUpto += payload.length;
+      }
+    }
+
+    if (fieldHasOffsets) {
+      assert startOffset >= lastStartOffset;
+      assert endOffset >= startOffset;
+      offsetStartDeltaBuffer[posBufferUpto] = startOffset - lastStartOffset;
+      offsetLengthBuffer[posBufferUpto] = endOffset - startOffset;
+      lastStartOffset = startOffset;
+    }
+    
+    posBufferUpto++;
+    lastPosition = position;
+    if (posBufferUpto == BLOCK_SIZE) {
+      // if (DEBUG) {
+      //   System.out.println("  write pos bulk block @ fp=" + posOut.getFilePointer());
+      // }
+      forUtil.writeBlock(posDeltaBuffer, encoded, posOut);
+
+      if (fieldHasPayloads) {
+        forUtil.writeBlock(payloadLengthBuffer, encoded, payOut);
+        payOut.writeVInt(payloadByteUpto);
+        payOut.writeBytes(payloadBytes, 0, payloadByteUpto);
+        payloadByteUpto = 0;
+      }
+      if (fieldHasOffsets) {
+        forUtil.writeBlock(offsetStartDeltaBuffer, encoded, payOut);
+        forUtil.writeBlock(offsetLengthBuffer, encoded, payOut);
+      }
+      posBufferUpto = 0;
+    }
+  }
+
+  @Override
+  public void finishDoc() throws IOException {
+    // Since we don't know df for current term, we had to buffer
+    // those skip data for each block, and when a new doc comes, 
+    // write them to skip file.
+    if (docBufferUpto == BLOCK_SIZE) {
+      lastBlockDocID = lastDocID;
+      if (posOut != null) {
+        if (payOut != null) {
+          lastBlockPayFP = payOut.getFilePointer();
+        }
+        lastBlockPosFP = posOut.getFilePointer();
+        lastBlockPosBufferUpto = posBufferUpto;
+        lastBlockStartOffset = lastStartOffset;
+        lastBlockPayloadByteUpto = payloadByteUpto;
+      }
+      // if (DEBUG) {
+      //   System.out.println("  docBufferUpto="+docBufferUpto+" now get lastBlockDocID="+lastBlockDocID+" lastBlockPosFP=" + lastBlockPosFP + " lastBlockPosBufferUpto=" +  lastBlockPosBufferUpto + " lastBlockPayloadByteUpto=" + lastBlockPayloadByteUpto);
+      // }
+      docBufferUpto = 0;
+    }
+  }
+
+  private static class PendingTerm {
+    public final long docStartFP;
+    public final long posStartFP;
+    public final long payStartFP;
+    public final int skipOffset;
+    public final int lastPosBlockOffset;
+
+    public PendingTerm(long docStartFP, long posStartFP, long payStartFP, int skipOffset, int lastPosBlockOffset) {
+      this.docStartFP = docStartFP;
+      this.posStartFP = posStartFP;
+      this.payStartFP = payStartFP;
+      this.skipOffset = skipOffset;
+      this.lastPosBlockOffset = lastPosBlockOffset;
+    }
+  }
+
+  private final List<PendingTerm> pendingTerms = new ArrayList<PendingTerm>();
+
+  /** Called when we are done adding docs to this term */
+  @Override
+  public void finishTerm(TermStats stats) throws IOException {
+    assert stats.docFreq > 0;
+
+    // TODO: wasteful we are counting this (counting # docs
+    // for this term) in two places?
+    assert stats.docFreq == docCount: stats.docFreq + " vs " + docCount;
+
+    // if (DEBUG) {
+    //   System.out.println("FPW.finishTerm docFreq=" + stats.docFreq);
+    // }
+
+    // if (DEBUG) {
+    //   if (docBufferUpto > 0) {
+    //     System.out.println("  write doc/freq vInt block (count=" + docBufferUpto + ") at fp=" + docOut.getFilePointer() + " docTermStartFP=" + docTermStartFP);
+    //   }
+    // }
+
+    // vInt encode the remaining doc deltas and freqs:
+    for(int i=0;i<docBufferUpto;i++) {
+      final int docDelta = docDeltaBuffer[i];
+      final int freq = freqBuffer[i];
+      if (!fieldHasFreqs) {
+        docOut.writeVInt(docDelta);
+      } else if (freqBuffer[i] == 1) {
+        docOut.writeVInt((docDelta<<1)|1);
+      } else {
+        docOut.writeVInt(docDelta<<1);
+        docOut.writeVInt(freq);
+      }
+    }
+
+    final int lastPosBlockOffset;
+
+    if (fieldHasPositions) {
+      // if (DEBUG) {
+      //   if (posBufferUpto > 0) {
+      //     System.out.println("  write pos vInt block (count=" + posBufferUpto + ") at fp=" + posOut.getFilePointer() + " posTermStartFP=" + posTermStartFP + " hasPayloads=" + fieldHasPayloads + " hasOffsets=" + fieldHasOffsets);
+      //   }
+      // }
+
+      // totalTermFreq is just total number of positions(or payloads, or offsets)
+      // associated with current term.
+      assert stats.totalTermFreq != -1;
+      if (stats.totalTermFreq > BLOCK_SIZE) {
+        // record file offset for last pos in last block
+        lastPosBlockOffset = (int) (posOut.getFilePointer() - posTermStartFP);
+      } else {
+        lastPosBlockOffset = -1;
+      }
+      if (posBufferUpto > 0) {
+        posOut.writeVInt(posBufferUpto);
+        
+        // TODO: should we send offsets/payloads to
+        // .pay...?  seems wasteful (have to store extra
+        // vLong for low (< BLOCK_SIZE) DF terms = vast vast
+        // majority)
+
+        // vInt encode the remaining positions/payloads/offsets:
+        int lastPayloadLength = -1;
+        int payloadBytesReadUpto = 0;
+        for(int i=0;i<posBufferUpto;i++) {
+          final int posDelta = posDeltaBuffer[i];
+          if (fieldHasPayloads) {
+            final int payloadLength = payloadLengthBuffer[i];
+            if (payloadLength != lastPayloadLength) {
+              lastPayloadLength = payloadLength;
+              posOut.writeVInt((posDelta<<1)|1);
+              posOut.writeVInt(payloadLength);
+            } else {
+              posOut.writeVInt(posDelta<<1);
+            }
+
+            // if (DEBUG) {
+            //   System.out.println("        i=" + i + " payloadLen=" + payloadLength);
+            // }
+
+            if (payloadLength != 0) {
+              // if (DEBUG) {
+              //   System.out.println("          write payload @ pos.fp=" + posOut.getFilePointer());
+              // }
+              posOut.writeBytes(payloadBytes, payloadBytesReadUpto, payloadLength);
+              payloadBytesReadUpto += payloadLength;
+            }
+          } else {
+            posOut.writeVInt(posDelta);
+          }
+
+          if (fieldHasOffsets) {
+            // if (DEBUG) {
+            //   System.out.println("          write offset @ pos.fp=" + posOut.getFilePointer());
+            // }
+            posOut.writeVInt(offsetStartDeltaBuffer[i]);
+            posOut.writeVInt(offsetLengthBuffer[i]);
+          }
+        }
+
+        if (fieldHasPayloads) {
+          assert payloadBytesReadUpto == payloadByteUpto;
+          payloadByteUpto = 0;
+        }
+      }
+      // if (DEBUG) {
+      //   System.out.println("  totalTermFreq=" + stats.totalTermFreq + " lastPosBlockOffset=" + lastPosBlockOffset);
+      // }
+    } else {
+      lastPosBlockOffset = -1;
+    }
+
+    int skipOffset;
+    if (docCount > BLOCK_SIZE) {
+      skipOffset = (int) (skipWriter.writeSkip(docOut) - docTermStartFP);
+      
+      // if (DEBUG) {
+      //   System.out.println("skip packet " + (docOut.getFilePointer() - (docTermStartFP + skipOffset)) + " bytes");
+      // }
+    } else {
+      skipOffset = -1;
+      // if (DEBUG) {
+      //   System.out.println("  no skip: docCount=" + docCount);
+      // }
+    }
+
+    long payStartFP;
+    if (stats.totalTermFreq >= BLOCK_SIZE) {
+      payStartFP = payTermStartFP;
+    } else {
+      payStartFP = -1;
+    }
+
+    // if (DEBUG) {
+    //   System.out.println("  payStartFP=" + payStartFP);
+    // }
+
+    pendingTerms.add(new PendingTerm(docTermStartFP, posTermStartFP, payStartFP, skipOffset, lastPosBlockOffset));
+    docBufferUpto = 0;
+    posBufferUpto = 0;
+    lastDocID = 0;
+    docCount = 0;
+  }
+
+  private final RAMOutputStream bytesWriter = new RAMOutputStream();
+
+  @Override
+  public void flushTermsBlock(int start, int count) throws IOException {
+
+    if (count == 0) {
+      termsOut.writeByte((byte) 0);
+      return;
+    }
+
+    assert start <= pendingTerms.size();
+    assert count <= start;
+
+    final int limit = pendingTerms.size() - start + count;
+
+    long lastDocStartFP = 0;
+    long lastPosStartFP = 0;
+    long lastPayStartFP = 0;
+    for(int idx=limit-count; idx<limit; idx++) {
+      PendingTerm term = pendingTerms.get(idx);
+
+      bytesWriter.writeVLong(term.docStartFP - lastDocStartFP);
+      lastDocStartFP = term.docStartFP;
+
+      if (fieldHasPositions) {
+        bytesWriter.writeVLong(term.posStartFP - lastPosStartFP);
+        lastPosStartFP = term.posStartFP;
+        if (term.lastPosBlockOffset != -1) {
+          bytesWriter.writeVInt(term.lastPosBlockOffset);
+        }
+        if ((fieldHasPayloads || fieldHasOffsets) && term.payStartFP != -1) {
+          bytesWriter.writeVLong(term.payStartFP - lastPayStartFP);
+          lastPayStartFP = term.payStartFP;
+        }
+      }
+
+      if (term.skipOffset != -1) {
+        bytesWriter.writeVInt(term.skipOffset);
+      }
+    }
+
+    termsOut.writeVInt((int) bytesWriter.getFilePointer());
+    bytesWriter.writeTo(termsOut);
+    bytesWriter.reset();
+
+    // Remove the terms we just wrote:
+    pendingTerms.subList(limit-count, limit).clear();
+  }
+
+  @Override
+  public void close() throws IOException {
+    IOUtils.close(docOut, posOut, payOut);
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/block/BlockSkipReader.java b/lucene/codecs/src/java/org/apache/lucene/codecs/block/BlockSkipReader.java
new file mode 100644
index 0000000..169219c
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/block/BlockSkipReader.java
@@ -0,0 +1,244 @@
+package org.apache.lucene.codecs.block;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Arrays;
+
+import org.apache.lucene.codecs.MultiLevelSkipListReader;
+import org.apache.lucene.store.IndexInput;
+
+/**
+ * Implements the skip list reader for block postings format
+ * that stores positions and payloads.
+ * 
+ * Although this skipper uses MultiLevelSkipListReader as an interface, 
+ * its definition of skip position will be a little different. 
+ *
+ * For example, when skipInterval = blockSize = 3, df = 2*skipInterval = 6, 
+ * 
+ * 0 1 2 3 4 5
+ * d d d d d d    (posting list)
+ *     ^     ^    (skip point in MultiLeveSkipWriter)
+ *       ^        (skip point in BlockSkipWriter)
+ *
+ * In this case, MultiLevelSkipListReader will use the last document as a skip point, 
+ * while BlockSkipReader should assume no skip point will comes. 
+ *
+ * If we use the interface directly in BlockSkipReader, it may silly try to read 
+ * another skip data after the only skip point is loaded. 
+ *
+ * To illustrate this, we can call skipTo(d[5]), since skip point d[3] has smaller docId,
+ * and numSkipped+blockSize== df, the MultiLevelSkipListReader will assume the skip list
+ * isn't exhausted yet, and try to load a non-existed skip point
+ *
+ * Therefore, we'll trim df before passing it to the interface. see trim(int)
+ *
+ */
+final class BlockSkipReader extends MultiLevelSkipListReader {
+  // private boolean DEBUG = BlockPostingsReader.DEBUG;
+  private final int blockSize;
+
+  private long docPointer[];
+  private long posPointer[];
+  private long payPointer[];
+  private int posBufferUpto[];
+  private int startOffset[];
+  private int payloadByteUpto[];
+
+  private long lastPosPointer;
+  private long lastPayPointer;
+  private int lastStartOffset;
+  private int lastPayloadByteUpto;
+  private long lastDocPointer;
+  private int lastPosBufferUpto;
+
+  public BlockSkipReader(IndexInput skipStream, int maxSkipLevels, int blockSize, boolean hasPos, boolean hasOffsets, boolean hasPayloads) {
+    super(skipStream, maxSkipLevels, blockSize, 8);
+    this.blockSize = blockSize;
+    docPointer = new long[maxSkipLevels];
+    if (hasPos) {
+      posPointer = new long[maxSkipLevels];
+      posBufferUpto = new int[maxSkipLevels];
+      if (hasPayloads) {
+        payloadByteUpto = new int[maxSkipLevels];
+      } else {
+        payloadByteUpto = null;
+      }
+      if (hasOffsets) {
+        startOffset = new int[maxSkipLevels];
+      } else {
+        startOffset = null;
+      }
+      if (hasOffsets || hasPayloads) {
+        payPointer = new long[maxSkipLevels];
+      } else {
+        payPointer = null;
+      }
+    } else {
+      posPointer = null;
+    }
+  }
+
+  /**
+   * Trim original docFreq to tell skipReader read proper number of skip points.
+   *
+   * Since our definition in BlockSkip* is a little different from MultiLevelSkip*
+   * This trimmed docFreq will prevent skipReader from:
+   * 1. silly reading a non-existed skip point after the last block boundary
+   * 2. moving into the vInt block
+   *
+   */
+  protected int trim(int df) {
+    return df % blockSize == 0? df - 1: df;
+  }
+
+  public void init(long skipPointer, long docBasePointer, long posBasePointer, long payBasePointer, int df) {
+    super.init(skipPointer, trim(df));
+    lastDocPointer = docBasePointer;
+    lastPosPointer = posBasePointer;
+    lastPayPointer = payBasePointer;
+
+    Arrays.fill(docPointer, docBasePointer);
+    if (posPointer != null) {
+      Arrays.fill(posPointer, posBasePointer);
+      if (payPointer != null) {
+        Arrays.fill(payPointer, payBasePointer);
+      }
+    } else {
+      assert posBasePointer == 0;
+    }
+  }
+
+  /** Returns the doc pointer of the doc to which the last call of 
+   * {@link MultiLevelSkipListReader#skipTo(int)} has skipped.  */
+  public long getDocPointer() {
+    return lastDocPointer;
+  }
+
+  public long getPosPointer() {
+    return lastPosPointer;
+  }
+
+  public int getPosBufferUpto() {
+    return lastPosBufferUpto;
+  }
+
+  public long getPayPointer() {
+    return lastPayPointer;
+  }
+
+  public int getStartOffset() {
+    return lastStartOffset;
+  }
+
+  public int getPayloadByteUpto() {
+    return lastPayloadByteUpto;
+  }
+
+  public int getNextSkipDoc() {
+    return skipDoc[0];
+  }
+
+  @Override
+  protected void seekChild(int level) throws IOException {
+    super.seekChild(level);
+    // if (DEBUG) {
+    //   System.out.println("seekChild level=" + level);
+    // }
+    docPointer[level] = lastDocPointer;
+    if (posPointer != null) {
+      posPointer[level] = lastPosPointer;
+      posBufferUpto[level] = lastPosBufferUpto;
+      if (startOffset != null) {
+        startOffset[level] = lastStartOffset;
+      }
+      if (payloadByteUpto != null) {
+        payloadByteUpto[level] = lastPayloadByteUpto;
+      }
+      if (payPointer != null) {
+        payPointer[level] = lastPayPointer;
+      }
+    }
+  }
+  
+  @Override
+  protected void setLastSkipData(int level) {
+    super.setLastSkipData(level);
+    lastDocPointer = docPointer[level];
+    // if (DEBUG) {
+    //   System.out.println("setLastSkipData level=" + level);
+    //   System.out.println("  lastDocPointer=" + lastDocPointer);
+    // }
+    if (posPointer != null) {
+      lastPosPointer = posPointer[level];
+      lastPosBufferUpto = posBufferUpto[level];
+      // if (DEBUG) {
+      //   System.out.println("  lastPosPointer=" + lastPosPointer + " lastPosBUfferUpto=" + lastPosBufferUpto);
+      // }
+      if (payPointer != null) {
+        lastPayPointer = payPointer[level];
+      }
+      if (startOffset != null) {
+        lastStartOffset = startOffset[level];
+      }
+      if (payloadByteUpto != null) {
+        lastPayloadByteUpto = payloadByteUpto[level];
+      }
+    }
+  }
+
+  @Override
+  protected int readSkipData(int level, IndexInput skipStream) throws IOException {
+    // if (DEBUG) {
+    //   System.out.println("readSkipData level=" + level);
+    // }
+    int delta = skipStream.readVInt();
+    // if (DEBUG) {
+    //   System.out.println("  delta=" + delta);
+    // }
+    docPointer[level] += skipStream.readVInt();
+    // if (DEBUG) {
+    //   System.out.println("  docFP=" + docPointer[level]);
+    // }
+
+    if (posPointer != null) {
+      posPointer[level] += skipStream.readVInt();
+      // if (DEBUG) {
+      //   System.out.println("  posFP=" + posPointer[level]);
+      // }
+      posBufferUpto[level] = skipStream.readVInt();
+      // if (DEBUG) {
+      //   System.out.println("  posBufferUpto=" + posBufferUpto[level]);
+      // }
+
+      if (payloadByteUpto != null) {
+        payloadByteUpto[level] = skipStream.readVInt();
+      }
+
+      if (startOffset != null) {
+        startOffset[level] += skipStream.readVInt();
+      }
+
+      if (payPointer != null) {
+        payPointer[level] += skipStream.readVInt();
+      }
+    }
+    return delta;
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/block/BlockSkipWriter.java b/lucene/codecs/src/java/org/apache/lucene/codecs/block/BlockSkipWriter.java
new file mode 100644
index 0000000..8ece562
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/block/BlockSkipWriter.java
@@ -0,0 +1,163 @@
+package org.apache.lucene.codecs.block;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Arrays;
+
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.codecs.MultiLevelSkipListWriter;
+
+/**
+ * Write skip lists with multiple levels, and support skip within block ints.
+ *
+ * Assume that docFreq = 28, skipInterval = blockSize = 12
+ *
+ *  |       block#0       | |      block#1        | |vInts|
+ *  d d d d d d d d d d d d d d d d d d d d d d d d d d d d (posting list)
+ *                          ^                       ^       (level 0 skip point)
+ *
+ * Note that skipWriter will ignore first document in block#0, since 
+ * it is useless as a skip point.  Also, we'll never skip into the vInts
+ * block, only record skip data at the start its start point(if it exist).
+ *
+ * For each skip point, we will record: 
+ * 1. docID in former position, i.e. for position 12, record docID[11], etc.
+ * 2. its related file points(position, payload), 
+ * 3. related numbers or uptos(position, payload).
+ * 4. start offset.
+ *
+ */
+final class BlockSkipWriter extends MultiLevelSkipListWriter {
+  // private boolean DEBUG = BlockPostingsReader.DEBUG;
+  
+  private int[] lastSkipDoc;
+  private long[] lastSkipDocPointer;
+  private long[] lastSkipPosPointer;
+  private long[] lastSkipPayPointer;
+  private int[] lastStartOffset;
+  private int[] lastPayloadByteUpto;
+
+  private final IndexOutput docOut;
+  private final IndexOutput posOut;
+  private final IndexOutput payOut;
+
+  private int curDoc;
+  private long curDocPointer;
+  private long curPosPointer;
+  private long curPayPointer;
+  private int curPosBufferUpto;
+  private int curStartOffset;
+  private int curPayloadByteUpto;
+  private boolean fieldHasPositions;
+  private boolean fieldHasOffsets;
+  private boolean fieldHasPayloads;
+
+  public BlockSkipWriter(int maxSkipLevels, int blockSize, int docCount, IndexOutput docOut, IndexOutput posOut, IndexOutput payOut) {
+    super(blockSize, 8, maxSkipLevels, docCount);
+    this.docOut = docOut;
+    this.posOut = posOut;
+    this.payOut = payOut;
+    
+    lastSkipDoc = new int[maxSkipLevels];
+    lastSkipDocPointer = new long[maxSkipLevels];
+    if (posOut != null) {
+      lastSkipPosPointer = new long[maxSkipLevels];
+      if (payOut != null) {
+        lastSkipPayPointer = new long[maxSkipLevels];
+      }
+      lastStartOffset = new int[maxSkipLevels];
+      lastPayloadByteUpto = new int[maxSkipLevels];
+    }
+  }
+
+  public void setField(boolean fieldHasPositions, boolean fieldHasOffsets, boolean fieldHasPayloads) {
+    this.fieldHasPositions = fieldHasPositions;
+    this.fieldHasOffsets = fieldHasOffsets;
+    this.fieldHasPayloads = fieldHasPayloads;
+  }
+
+  @Override
+  public void resetSkip() {
+    super.resetSkip();
+    Arrays.fill(lastSkipDoc, 0);
+    Arrays.fill(lastSkipDocPointer, docOut.getFilePointer());
+    if (fieldHasPositions) {
+      Arrays.fill(lastSkipPosPointer, posOut.getFilePointer());
+      if (fieldHasOffsets) {
+        Arrays.fill(lastStartOffset, 0);
+      }
+      if (fieldHasPayloads) {
+        Arrays.fill(lastPayloadByteUpto, 0);
+      }
+      if (fieldHasOffsets || fieldHasPayloads) {
+        Arrays.fill(lastSkipPayPointer, payOut.getFilePointer());
+      }
+    }
+  }
+
+  /**
+   * Sets the values for the current skip data. 
+   */
+  public void bufferSkip(int doc, int numDocs, long posFP, long payFP, int posBufferUpto, int startOffset, int payloadByteUpto) throws IOException {
+    this.curDoc = doc;
+    this.curDocPointer = docOut.getFilePointer();
+    this.curPosPointer = posFP;
+    this.curPayPointer = payFP;
+    this.curPosBufferUpto = posBufferUpto;
+    this.curPayloadByteUpto = payloadByteUpto;
+    this.curStartOffset = startOffset;
+    bufferSkip(numDocs);
+  }
+  
+  @Override
+  protected void writeSkipData(int level, IndexOutput skipBuffer) throws IOException {
+    int delta = curDoc - lastSkipDoc[level];
+    // if (DEBUG) {
+    //   System.out.println("writeSkipData level=" + level + " lastDoc=" + curDoc + " delta=" + delta + " curDocPointer=" + curDocPointer);
+    // }
+    skipBuffer.writeVInt(delta);
+    lastSkipDoc[level] = curDoc;
+
+    skipBuffer.writeVInt((int) (curDocPointer - lastSkipDocPointer[level]));
+    lastSkipDocPointer[level] = curDocPointer;
+
+    if (fieldHasPositions) {
+      // if (DEBUG) {
+      //   System.out.println("  curPosPointer=" + curPosPointer + " curPosBufferUpto=" + curPosBufferUpto);
+      // }
+      skipBuffer.writeVInt((int) (curPosPointer - lastSkipPosPointer[level]));
+      lastSkipPosPointer[level] = curPosPointer;
+      skipBuffer.writeVInt(curPosBufferUpto);
+
+      if (fieldHasPayloads) {
+        skipBuffer.writeVInt(curPayloadByteUpto);
+      }
+
+      if (fieldHasOffsets) {
+        skipBuffer.writeVInt(curStartOffset - lastStartOffset[level]);
+        lastStartOffset[level] = curStartOffset;
+      }
+
+      if (fieldHasOffsets || fieldHasPayloads) {
+        skipBuffer.writeVInt((int) (curPayPointer - lastSkipPayPointer[level]));
+        lastSkipPayPointer[level] = curPayPointer;
+      }
+    }
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/block/ForUtil.java b/lucene/codecs/src/java/org/apache/lucene/codecs/block/ForUtil.java
new file mode 100644
index 0000000..b403876
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/block/ForUtil.java
@@ -0,0 +1,247 @@
+package org.apache.lucene.codecs.block;
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Arrays;
+
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.store.DataInput;
+import org.apache.lucene.store.DataOutput;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.packed.PackedInts.Decoder;
+import org.apache.lucene.util.packed.PackedInts.FormatAndBits;
+import org.apache.lucene.util.packed.PackedInts;
+
+import static org.apache.lucene.codecs.block.BlockPostingsFormat.BLOCK_SIZE;
+
+/**
+ * Encode all values in normal area with fixed bit width, 
+ * which is determined by the max value in this block.
+ */
+final class ForUtil {
+
+  /**
+   * Special number of bits per value used whenever all values to encode are equal.
+   */
+  private static final int ALL_VALUES_EQUAL = 0;
+
+  /**
+   * Upper limit of the number of bytes that might be required to stored
+   * <code>BLOCK_SIZE</code> encoded values.
+   */
+  static final int MAX_ENCODED_SIZE = BLOCK_SIZE * 4;
+
+  /**
+   * Upper limit of the number of values that might be decoded in a single call to
+   * {@link #readBlock(IndexInput, byte[], int[])}. Although values after
+   * <code>BLOCK_SIZE</code> are garbage, it is necessary to allocate value buffers
+   * whose size is >= MAX_DATA_SIZE to avoid {@link ArrayIndexOutOfBoundsException}s.
+   */
+  static final int MAX_DATA_SIZE;
+  static {
+    int maxDataSize = 0;
+    for(int version=PackedInts.VERSION_START;version<=PackedInts.VERSION_CURRENT;version++) {
+      for (PackedInts.Format format : PackedInts.Format.values()) {
+        for (int bpv = 1; bpv <= 32; ++bpv) {
+          if (!format.isSupported(bpv)) {
+            continue;
+          }
+          final PackedInts.Decoder decoder = PackedInts.getDecoder(format, version, bpv);
+          final int iterations = computeIterations(decoder);
+          maxDataSize = Math.max(maxDataSize, iterations * decoder.valueCount());
+        }
+      }
+    }
+    MAX_DATA_SIZE = maxDataSize;
+  }
+
+  /**
+   * Compute the number of iterations required to decode <code>BLOCK_SIZE</code>
+   * values with the provided {@link Decoder}.
+   */
+  private static int computeIterations(PackedInts.Decoder decoder) {
+    return (int) Math.ceil((float) BLOCK_SIZE / decoder.valueCount());
+  }
+
+  /**
+   * Compute the number of bytes required to encode a block of values that require
+   * <code>bitsPerValue</code> bits per value with format <code>format</code>.
+   */
+  private static int encodedSize(PackedInts.Format format, int bitsPerValue) {
+    return format.nblocks(bitsPerValue, BLOCK_SIZE) << 3;
+  }
+
+  private final int[] encodedSizes;
+  private final PackedInts.Encoder[] encoders;
+  private final PackedInts.Decoder[] decoders;
+  private final int[] iterations;
+
+  /**
+   * Create a new {@link ForUtil} instance and save state into <code>out</code>.
+   */
+  ForUtil(float acceptableOverheadRatio, DataOutput out) throws IOException {
+    out.writeVInt(PackedInts.VERSION_CURRENT);
+    encodedSizes = new int[33];
+    encoders = new PackedInts.Encoder[33];
+    decoders = new PackedInts.Decoder[33];
+    iterations = new int[33];
+
+    for (int bpv = 1; bpv <= 32; ++bpv) {
+      final FormatAndBits formatAndBits = PackedInts.fastestFormatAndBits(
+          BLOCK_SIZE, bpv, acceptableOverheadRatio);
+      assert formatAndBits.format.isSupported(formatAndBits.bitsPerValue);
+      assert formatAndBits.bitsPerValue <= 32;
+      encodedSizes[bpv] = encodedSize(formatAndBits.format, formatAndBits.bitsPerValue);
+      encoders[bpv] = PackedInts.getEncoder(
+          formatAndBits.format, PackedInts.VERSION_CURRENT, formatAndBits.bitsPerValue);
+      decoders[bpv] = PackedInts.getDecoder(
+          formatAndBits.format, PackedInts.VERSION_CURRENT, formatAndBits.bitsPerValue);
+      iterations[bpv] = computeIterations(decoders[bpv]);
+
+      out.writeVInt(formatAndBits.format.getId() << 5 | (formatAndBits.bitsPerValue - 1));
+    }
+  }
+
+  /**
+   * Restore a {@link ForUtil} from a {@link DataInput}.
+   */
+  ForUtil(DataInput in) throws IOException {
+    int packedIntsVersion = in.readVInt();
+    if (packedIntsVersion != PackedInts.VERSION_START) {
+      throw new CorruptIndexException("expected version=" + PackedInts.VERSION_START + " but got version=" + packedIntsVersion);
+    }
+    encodedSizes = new int[33];
+    encoders = new PackedInts.Encoder[33];
+    decoders = new PackedInts.Decoder[33];
+    iterations = new int[33];
+
+    for (int bpv = 1; bpv <= 32; ++bpv) {
+      final int code = in.readVInt();
+      final int formatId = code >>> 5;
+      final int bitsPerValue = (code & 31) + 1;
+
+      final PackedInts.Format format = PackedInts.Format.byId(formatId);
+      assert format.isSupported(bitsPerValue);
+      encodedSizes[bpv] = encodedSize(format, bitsPerValue);
+      encoders[bpv] = PackedInts.getEncoder(
+          format, packedIntsVersion, bitsPerValue);
+      decoders[bpv] = PackedInts.getDecoder(
+          format, packedIntsVersion, bitsPerValue);
+      iterations[bpv] = computeIterations(decoders[bpv]);
+    }
+  }
+
+  /**
+   * Write a block of data (<code>For</code> format).
+   *
+   * @param data     the data to write
+   * @param encoded  a buffer to use to encode data
+   * @param out      the destination output
+   * @throws IOException
+   */
+  void writeBlock(int[] data, byte[] encoded, IndexOutput out) throws IOException {
+    if (isAllEqual(data)) {
+      out.writeVInt(ALL_VALUES_EQUAL);
+      out.writeVInt(data[0]);
+      return;
+    }
+
+    final int numBits = bitsRequired(data);
+    assert numBits > 0 && numBits <= 32 : numBits;
+    final PackedInts.Encoder encoder = encoders[numBits];
+    final int iters = iterations[numBits];
+    assert iters * encoder.valueCount() >= BLOCK_SIZE;
+    final int encodedSize = encodedSizes[numBits];
+    assert (iters * encoder.blockCount()) << 3 >= encodedSize;
+
+    out.writeVInt(numBits);
+
+    encoder.encode(data, 0, encoded, 0, iters);
+    out.writeBytes(encoded, encodedSize);
+  }
+
+  /**
+   * Read the next block of data (<code>For</code> format).
+   *
+   * @param in        the input to use to read data
+   * @param encoded   a buffer that can be used to store encoded data
+   * @param decoded   where to write decoded data
+   * @throws IOException
+   */
+  void readBlock(IndexInput in, byte[] encoded, int[] decoded) throws IOException {
+    final int numBits = in.readVInt();
+    assert numBits <= 32 : numBits;
+
+    if (numBits == ALL_VALUES_EQUAL) {
+      final int value = in.readVInt();
+      Arrays.fill(decoded, 0, BLOCK_SIZE, value);
+      return;
+    }
+
+    final int encodedSize = encodedSizes[numBits];
+    in.readBytes(encoded, 0, encodedSize);
+
+    final PackedInts.Decoder decoder = decoders[numBits];
+    final int iters = iterations[numBits];
+    assert iters * decoder.valueCount() >= BLOCK_SIZE;
+
+    decoder.decode(encoded, 0, decoded, 0, iters);
+  }
+
+  /**
+   * Skip the next block of data.
+   *
+   * @param in      the input where to read data
+   * @throws IOException
+   */
+  void skipBlock(IndexInput in) throws IOException {
+    final int numBits = in.readVInt();
+    if (numBits == ALL_VALUES_EQUAL) {
+      in.readVInt();
+      return;
+    }
+    assert numBits > 0 && numBits <= 32 : numBits;
+    final int encodedSize = encodedSizes[numBits];
+    in.seek(in.getFilePointer() + encodedSize);
+  }
+
+  private static boolean isAllEqual(final int[] data) {
+    final long v = data[0];
+    for (int i = 1; i < BLOCK_SIZE; ++i) {
+      if (data[i] != v) {
+        return false;
+      }
+    }
+    return true;
+  }
+
+  /**
+   * Compute the number of bits required to serialize any of the longs in
+   * <code>data</code>.
+   */
+  private static int bitsRequired(final int[] data) {
+    long or = 0;
+    for (int i = 0; i < BLOCK_SIZE; ++i) {
+      assert data[i] >= 0;
+      or |= data[i];
+    }
+    return PackedInts.bitsRequired(or);
+  }
+
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/block/package.html b/lucene/codecs/src/java/org/apache/lucene/codecs/block/package.html
new file mode 100644
index 0000000..c4fe9c6
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/block/package.html
@@ -0,0 +1,25 @@
+<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+<html>
+<head>
+   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
+</head>
+<body>
+BlockPostingsFormat file format.
+</body>
+</html>
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/bloom/BloomFilterFactory.java b/lucene/codecs/src/java/org/apache/lucene/codecs/bloom/BloomFilterFactory.java
new file mode 100644
index 0000000..43fda30
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/bloom/BloomFilterFactory.java
@@ -0,0 +1,63 @@
+package org.apache.lucene.codecs.bloom;
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.util.FuzzySet;
+
+
+/**
+ * Class used to create index-time {@link FuzzySet} appropriately configured for
+ * each field. Also called to right-size bitsets for serialization.
+ * @lucene.experimental
+ */
+public abstract class BloomFilterFactory {
+  
+  /**
+   * 
+   * @param state  The content to be indexed
+   * @param info
+   *          the field requiring a BloomFilter
+   * @return An appropriately sized set or null if no BloomFiltering required
+   */
+  public abstract FuzzySet getSetForField(SegmentWriteState state, FieldInfo info);
+  
+  /**
+   * Called when downsizing bitsets for serialization
+   * 
+   * @param fieldInfo
+   *          The field with sparse set bits
+   * @param initialSet
+   *          The bits accumulated
+   * @return null or a hopefully more densely packed, smaller bitset
+   */
+  public FuzzySet downsize(FieldInfo fieldInfo, FuzzySet initialSet) {
+    // Aim for a bitset size that would have 10% of bits set (so 90% of searches
+    // would fail-fast)
+    float targetMaxSaturation = 0.1f;
+    return initialSet.downsize(targetMaxSaturation);
+  }
+
+  /**
+   * Used to determine if the given filter has reached saturation and should be retired i.e. not saved any more
+   * @param bloomFilter The bloomFilter being tested
+   * @param fieldInfo The field with which this filter is associated
+   * @return true if the set has reached saturation and should be retired
+   */
+  public abstract boolean isSaturated(FuzzySet bloomFilter, FieldInfo fieldInfo);
+  
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/bloom/BloomFilteringPostingsFormat.java b/lucene/codecs/src/java/org/apache/lucene/codecs/bloom/BloomFilteringPostingsFormat.java
new file mode 100644
index 0000000..72cd94d
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/bloom/BloomFilteringPostingsFormat.java
@@ -0,0 +1,485 @@
+package org.apache.lucene.codecs.bloom;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Comparator;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.Map.Entry;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.FieldsConsumer;
+import org.apache.lucene.codecs.FieldsProducer;
+import org.apache.lucene.codecs.PostingsConsumer;
+import org.apache.lucene.codecs.PostingsFormat;
+import org.apache.lucene.codecs.TermStats;
+import org.apache.lucene.codecs.TermsConsumer;
+import org.apache.lucene.index.DocsAndPositionsEnum;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.index.Terms;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.store.DataOutput;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.FuzzySet;
+import org.apache.lucene.util.FuzzySet.ContainsResult;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.automaton.CompiledAutomaton;
+import org.apache.lucene.util.hash.MurmurHash2;
+
+/**
+ * <p>
+ * A {@link PostingsFormat} useful for low doc-frequency fields such as primary
+ * keys. Bloom filters are maintained in a ".blm" file which offers "fast-fail"
+ * for reads in segments known to have no record of the key. A choice of
+ * delegate PostingsFormat is used to record all other Postings data.
+ * </p>
+ * <p>
+ * A choice of {@link BloomFilterFactory} can be passed to tailor Bloom Filter
+ * settings on a per-field basis. The default configuration is
+ * {@link DefaultBloomFilterFactory} which allocates a ~8mb bitset and hashes
+ * values using {@link MurmurHash2}. This should be suitable for most purposes.
+ * </p>
+ * <p>
+ * The format of the blm file is as follows:
+ * </p>
+ * <ul>
+ * <li>BloomFilter (.blm) --&gt; Header, DelegatePostingsFormatName,
+ * NumFilteredFields, Filter<sup>NumFilteredFields</sup></li>
+ * <li>Filter --&gt; FieldNumber, FuzzySet</li>
+ * <li>FuzzySet --&gt;See {@link FuzzySet#serialize(DataOutput)}</li>
+ * <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
+ * <li>DelegatePostingsFormatName --&gt; {@link DataOutput#writeString(String)
+ * String} The name of a ServiceProvider registered {@link PostingsFormat}</li>
+ * <li>NumFilteredFields --&gt; {@link DataOutput#writeInt Uint32}</li>
+ * <li>FieldNumber --&gt; {@link DataOutput#writeInt Uint32} The number of the
+ * field in this segment</li>
+ * </ul>
+ * @lucene.experimental
+ */
+public class BloomFilteringPostingsFormat extends PostingsFormat {
+  
+  public static final String BLOOM_CODEC_NAME = "BloomFilter";
+  public static final int BLOOM_CODEC_VERSION = 1;
+  
+  /** Extension of Bloom Filters file */
+  static final String BLOOM_EXTENSION = "blm";
+  
+  BloomFilterFactory bloomFilterFactory = new DefaultBloomFilterFactory();
+  private PostingsFormat delegatePostingsFormat;
+  
+  /**
+   * Creates Bloom filters for a selection of fields created in the index. This
+   * is recorded as a set of Bitsets held as a segment summary in an additional
+   * "blm" file. This PostingsFormat delegates to a choice of delegate
+   * PostingsFormat for encoding all other postings data.
+   * 
+   * @param delegatePostingsFormat
+   *          The PostingsFormat that records all the non-bloom filter data i.e.
+   *          postings info.
+   * @param bloomFilterFactory
+   *          The {@link BloomFilterFactory} responsible for sizing BloomFilters
+   *          appropriately
+   */
+  public BloomFilteringPostingsFormat(PostingsFormat delegatePostingsFormat,
+      BloomFilterFactory bloomFilterFactory) {
+    super(BLOOM_CODEC_NAME);
+    this.delegatePostingsFormat = delegatePostingsFormat;
+    this.bloomFilterFactory = bloomFilterFactory;
+  }
+  
+  /**
+   * Creates Bloom filters for a selection of fields created in the index. This
+   * is recorded as a set of Bitsets held as a segment summary in an additional
+   * "blm" file. This PostingsFormat delegates to a choice of delegate
+   * PostingsFormat for encoding all other postings data. This choice of
+   * constructor defaults to the {@link DefaultBloomFilterFactory} for
+   * configuring per-field BloomFilters.
+   * 
+   * @param delegatePostingsFormat
+   *          The PostingsFormat that records all the non-bloom filter data i.e.
+   *          postings info.
+   */
+  public BloomFilteringPostingsFormat(PostingsFormat delegatePostingsFormat) {
+    this(delegatePostingsFormat, new DefaultBloomFilterFactory());
+  }
+  
+  // Used only by core Lucene at read-time via Service Provider instantiation -
+  // do not use at Write-time in application code.
+  public BloomFilteringPostingsFormat() {
+    super(BLOOM_CODEC_NAME);
+  }
+  
+  public FieldsConsumer fieldsConsumer(SegmentWriteState state)
+      throws IOException {
+    if (delegatePostingsFormat == null) {
+      throw new UnsupportedOperationException("Error - " + getClass().getName()
+          + " has been constructed without a choice of PostingsFormat");
+    }
+    return new BloomFilteredFieldsConsumer(
+        delegatePostingsFormat.fieldsConsumer(state), state,
+        delegatePostingsFormat);
+  }
+  
+  public FieldsProducer fieldsProducer(SegmentReadState state)
+      throws IOException {
+    return new BloomFilteredFieldsProducer(state);
+  }
+  
+  public class BloomFilteredFieldsProducer extends FieldsProducer {
+    private FieldsProducer delegateFieldsProducer;
+    HashMap<String,FuzzySet> bloomsByFieldName = new HashMap<String,FuzzySet>();
+    
+    public BloomFilteredFieldsProducer(SegmentReadState state)
+        throws IOException {
+      
+      String bloomFileName = IndexFileNames.segmentFileName(
+          state.segmentInfo.name, state.segmentSuffix, BLOOM_EXTENSION);
+      IndexInput bloomIn = null;
+      try {
+        bloomIn = state.dir.openInput(bloomFileName, state.context);
+        CodecUtil.checkHeader(bloomIn, BLOOM_CODEC_NAME, BLOOM_CODEC_VERSION,
+            BLOOM_CODEC_VERSION);
+        // // Load the hash function used in the BloomFilter
+        // hashFunction = HashFunction.forName(bloomIn.readString());
+        // Load the delegate postings format
+        PostingsFormat delegatePostingsFormat = PostingsFormat.forName(bloomIn
+            .readString());
+        
+        this.delegateFieldsProducer = delegatePostingsFormat
+            .fieldsProducer(state);
+        int numBlooms = bloomIn.readInt();
+        for (int i = 0; i < numBlooms; i++) {
+          int fieldNum = bloomIn.readInt();
+          FuzzySet bloom = FuzzySet.deserialize(bloomIn);
+          FieldInfo fieldInfo = state.fieldInfos.fieldInfo(fieldNum);
+          bloomsByFieldName.put(fieldInfo.name, bloom);
+        }
+      } finally {
+        IOUtils.close(bloomIn);
+      }
+      
+    }
+    
+    public Iterator<String> iterator() {
+      return delegateFieldsProducer.iterator();
+    }
+    
+    public void close() throws IOException {
+      delegateFieldsProducer.close();
+    }
+    
+    public Terms terms(String field) throws IOException {
+      FuzzySet filter = bloomsByFieldName.get(field);
+      if (filter == null) {
+        return delegateFieldsProducer.terms(field);
+      } else {
+        Terms result = delegateFieldsProducer.terms(field);
+        if (result == null) {
+          return null;
+        }
+        return new BloomFilteredTerms(result, filter);
+      }
+    }
+    
+    public int size() {
+      return delegateFieldsProducer.size();
+    }
+    
+    class BloomFilteredTerms extends Terms {
+      private Terms delegateTerms;
+      private FuzzySet filter;
+      
+      public BloomFilteredTerms(Terms terms, FuzzySet filter) {
+        this.delegateTerms = terms;
+        this.filter = filter;
+      }
+      
+      @Override
+      public TermsEnum intersect(CompiledAutomaton compiled,
+          final BytesRef startTerm) throws IOException {
+        return delegateTerms.intersect(compiled, startTerm);
+      }
+      
+      @Override
+      public TermsEnum iterator(TermsEnum reuse) throws IOException {
+        TermsEnum result;
+        if ((reuse != null) && (reuse instanceof BloomFilteredTermsEnum)) {
+          // recycle the existing BloomFilteredTermsEnum by asking the delegate
+          // to recycle its contained TermsEnum
+          BloomFilteredTermsEnum bfte = (BloomFilteredTermsEnum) reuse;
+          if (bfte.filter == filter) {
+            bfte.delegateTermsEnum = delegateTerms
+                .iterator(bfte.delegateTermsEnum);
+            return bfte;
+          }
+        }
+        // We have been handed something we cannot reuse (either null, wrong
+        // class or wrong filter) so allocate a new object
+        result = new BloomFilteredTermsEnum(delegateTerms.iterator(reuse),
+            filter);
+        return result;
+      }
+      
+      @Override
+      public Comparator<BytesRef> getComparator() throws IOException {
+        return delegateTerms.getComparator();
+      }
+      
+      @Override
+      public long size() throws IOException {
+        return delegateTerms.size();
+      }
+      
+      @Override
+      public long getSumTotalTermFreq() throws IOException {
+        return delegateTerms.getSumTotalTermFreq();
+      }
+      
+      @Override
+      public long getSumDocFreq() throws IOException {
+        return delegateTerms.getSumDocFreq();
+      }
+      
+      @Override
+      public int getDocCount() throws IOException {
+        return delegateTerms.getDocCount();
+      }
+
+      @Override
+      public boolean hasOffsets() {
+        return delegateTerms.hasOffsets();
+      }
+
+      @Override
+      public boolean hasPositions() {
+        return delegateTerms.hasPositions();
+      }
+      
+      @Override
+      public boolean hasPayloads() {
+        return delegateTerms.hasPayloads();
+      }
+    }
+    
+    class BloomFilteredTermsEnum extends TermsEnum {
+      
+      TermsEnum delegateTermsEnum;
+      private FuzzySet filter;
+      
+      public BloomFilteredTermsEnum(TermsEnum iterator, FuzzySet filter) {
+        this.delegateTermsEnum = iterator;
+        this.filter = filter;
+      }
+      
+      @Override
+      public final BytesRef next() throws IOException {
+        return delegateTermsEnum.next();
+      }
+      
+      @Override
+      public final Comparator<BytesRef> getComparator() {
+        return delegateTermsEnum.getComparator();
+      }
+      
+      @Override
+      public final boolean seekExact(BytesRef text, boolean useCache)
+          throws IOException {
+        // The magical fail-fast speed up that is the entire point of all of
+        // this code - save a disk seek if there is a match on an in-memory
+        // structure
+        // that may occasionally give a false positive but guaranteed no false
+        // negatives
+        if (filter.contains(text) == ContainsResult.NO) {
+          return false;
+        }
+        return delegateTermsEnum.seekExact(text, useCache);
+      }
+      
+      @Override
+      public final SeekStatus seekCeil(BytesRef text, boolean useCache)
+          throws IOException {
+        return delegateTermsEnum.seekCeil(text, useCache);
+      }
+      
+      @Override
+      public final void seekExact(long ord) throws IOException {
+        delegateTermsEnum.seekExact(ord);
+      }
+      
+      @Override
+      public final BytesRef term() throws IOException {
+        return delegateTermsEnum.term();
+      }
+      
+      @Override
+      public final long ord() throws IOException {
+        return delegateTermsEnum.ord();
+      }
+      
+      @Override
+      public final int docFreq() throws IOException {
+        return delegateTermsEnum.docFreq();
+      }
+      
+      @Override
+      public final long totalTermFreq() throws IOException {
+        return delegateTermsEnum.totalTermFreq();
+      }
+      
+
+      @Override
+      public DocsAndPositionsEnum docsAndPositions(Bits liveDocs,
+          DocsAndPositionsEnum reuse, int flags) throws IOException {
+        return delegateTermsEnum.docsAndPositions(liveDocs, reuse, flags);
+      }
+
+      @Override
+      public DocsEnum docs(Bits liveDocs, DocsEnum reuse, int flags)
+          throws IOException {
+        return delegateTermsEnum.docs(liveDocs, reuse, flags);
+      }
+      
+      
+    }
+    
+  }
+  
+  class BloomFilteredFieldsConsumer extends FieldsConsumer {
+    private FieldsConsumer delegateFieldsConsumer;
+    private Map<FieldInfo,FuzzySet> bloomFilters = new HashMap<FieldInfo,FuzzySet>();
+    private SegmentWriteState state;
+    
+    // private PostingsFormat delegatePostingsFormat;
+    
+    public BloomFilteredFieldsConsumer(FieldsConsumer fieldsConsumer,
+        SegmentWriteState state, PostingsFormat delegatePostingsFormat) {
+      this.delegateFieldsConsumer = fieldsConsumer;
+      // this.delegatePostingsFormat=delegatePostingsFormat;
+      this.state = state;
+    }
+    
+    @Override
+    public TermsConsumer addField(FieldInfo field) throws IOException {
+      FuzzySet bloomFilter = bloomFilterFactory.getSetForField(state,field);
+      if (bloomFilter != null) {
+        assert bloomFilters.containsKey(field) == false;
+        bloomFilters.put(field, bloomFilter);
+        return new WrappedTermsConsumer(delegateFieldsConsumer.addField(field),bloomFilter);
+      } else {
+        // No, use the unfiltered fieldsConsumer - we are not interested in
+        // recording any term Bitsets.
+        return delegateFieldsConsumer.addField(field);
+      }
+    }
+    
+    @Override
+    public void close() throws IOException {
+      delegateFieldsConsumer.close();
+      // Now we are done accumulating values for these fields
+      List<Entry<FieldInfo,FuzzySet>> nonSaturatedBlooms = new ArrayList<Map.Entry<FieldInfo,FuzzySet>>();
+      
+      for (Entry<FieldInfo,FuzzySet> entry : bloomFilters.entrySet()) {
+        FuzzySet bloomFilter = entry.getValue();
+        if(!bloomFilterFactory.isSaturated(bloomFilter,entry.getKey())){          
+          nonSaturatedBlooms.add(entry);
+        }
+      }
+      String bloomFileName = IndexFileNames.segmentFileName(
+          state.segmentInfo.name, state.segmentSuffix, BLOOM_EXTENSION);
+      IndexOutput bloomOutput = null;
+      try {
+        bloomOutput = state.directory
+            .createOutput(bloomFileName, state.context);
+        CodecUtil.writeHeader(bloomOutput, BLOOM_CODEC_NAME,
+            BLOOM_CODEC_VERSION);
+        // remember the name of the postings format we will delegate to
+        bloomOutput.writeString(delegatePostingsFormat.getName());
+        
+        // First field in the output file is the number of fields+blooms saved
+        bloomOutput.writeInt(nonSaturatedBlooms.size());
+        for (Entry<FieldInfo,FuzzySet> entry : nonSaturatedBlooms) {
+          FieldInfo fieldInfo = entry.getKey();
+          FuzzySet bloomFilter = entry.getValue();
+          bloomOutput.writeInt(fieldInfo.number);
+          saveAppropriatelySizedBloomFilter(bloomOutput, bloomFilter, fieldInfo);
+        }
+      } finally {
+        IOUtils.close(bloomOutput);
+      }
+      //We are done with large bitsets so no need to keep them hanging around
+      bloomFilters.clear(); 
+    }
+    
+    private void saveAppropriatelySizedBloomFilter(IndexOutput bloomOutput,
+        FuzzySet bloomFilter, FieldInfo fieldInfo) throws IOException {
+      
+      FuzzySet rightSizedSet = bloomFilterFactory.downsize(fieldInfo,
+          bloomFilter);
+      if (rightSizedSet == null) {
+        rightSizedSet = bloomFilter;
+      }
+      rightSizedSet.serialize(bloomOutput);
+    }
+    
+  }
+  
+  class WrappedTermsConsumer extends TermsConsumer {
+    private TermsConsumer delegateTermsConsumer;
+    private FuzzySet bloomFilter;
+    
+    public WrappedTermsConsumer(TermsConsumer termsConsumer,FuzzySet bloomFilter) {
+      this.delegateTermsConsumer = termsConsumer;
+      this.bloomFilter = bloomFilter;
+    }
+    
+    public PostingsConsumer startTerm(BytesRef text) throws IOException {
+      return delegateTermsConsumer.startTerm(text);
+    }
+    
+    public void finishTerm(BytesRef text, TermStats stats) throws IOException {
+      
+      // Record this term in our BloomFilter
+      if (stats.docFreq > 0) {
+        bloomFilter.addValue(text);
+      }
+      delegateTermsConsumer.finishTerm(text, stats);
+    }
+    
+    public void finish(long sumTotalTermFreq, long sumDocFreq, int docCount)
+        throws IOException {
+      delegateTermsConsumer.finish(sumTotalTermFreq, sumDocFreq, docCount);
+    }
+    
+    public Comparator<BytesRef> getComparator() throws IOException {
+      return delegateTermsConsumer.getComparator();
+    }
+    
+  }
+  
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/bloom/DefaultBloomFilterFactory.java b/lucene/codecs/src/java/org/apache/lucene/codecs/bloom/DefaultBloomFilterFactory.java
new file mode 100644
index 0000000..804f56b
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/bloom/DefaultBloomFilterFactory.java
@@ -0,0 +1,44 @@
+package org.apache.lucene.codecs.bloom;
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.util.FuzzySet;
+import org.apache.lucene.util.hash.HashFunction;
+import org.apache.lucene.util.hash.MurmurHash2;
+
+/**
+ * Default policy is to allocate a bitset with 10% saturation given a unique term per document.
+ * Bits are set via MurmurHash2 hashing function.
+ *  @lucene.experimental
+ */
+public class DefaultBloomFilterFactory extends BloomFilterFactory {
+  
+  @Override
+  public FuzzySet getSetForField(SegmentWriteState state,FieldInfo info) {
+    //Assume all of the docs have a unique term (e.g. a primary key) and we hope to maintain a set with 10% of bits set
+    return FuzzySet.createSetBasedOnQuality(state.segmentInfo.getDocCount(), 0.10f,  new MurmurHash2());
+  }
+  
+  @Override
+  public boolean isSaturated(FuzzySet bloomFilter, FieldInfo fieldInfo) {
+    // Don't bother saving bitsets if >90% of bits are set - we don't want to
+    // throw any more memory at this problem.
+    return bloomFilter.getSaturation() > 0.9f;
+  }
+  
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/bloom/package.html b/lucene/codecs/src/java/org/apache/lucene/codecs/bloom/package.html
new file mode 100644
index 0000000..a0c591a
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/bloom/package.html
@@ -0,0 +1,25 @@
+<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+<html>
+<head>
+   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
+</head>
+<body>
+Codec PostingsFormat for fast access to low-frequency terms such as primary key fields.
+</body>
+</html>
\ No newline at end of file
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/intblock/FixedIntBlockIndexInput.java b/lucene/codecs/src/java/org/apache/lucene/codecs/intblock/FixedIntBlockIndexInput.java
new file mode 100644
index 0000000..33457e4
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/intblock/FixedIntBlockIndexInput.java
@@ -0,0 +1,171 @@
+package org.apache.lucene.codecs.intblock;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/** Naive int block API that writes vInts.  This is
+ *  expected to give poor performance; it's really only for
+ *  testing the pluggability.  One should typically use pfor instead. */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.sep.IntIndexInput;
+import org.apache.lucene.store.DataInput;
+import org.apache.lucene.store.IndexInput;
+
+/** Abstract base class that reads fixed-size blocks of ints
+ *  from an IndexInput.  While this is a simple approach, a
+ *  more performant approach would directly create an impl
+ *  of IntIndexInput inside Directory.  Wrapping a generic
+ *  IndexInput will likely cost performance.
+ *
+ * @lucene.experimental
+ */
+public abstract class FixedIntBlockIndexInput extends IntIndexInput {
+
+  private final IndexInput in;
+  protected final int blockSize;
+  
+  public FixedIntBlockIndexInput(final IndexInput in) throws IOException {
+    this.in = in;
+    blockSize = in.readVInt();
+  }
+
+  @Override
+  public Reader reader() throws IOException {
+    final int[] buffer = new int[blockSize];
+    final IndexInput clone = in.clone();
+    // TODO: can this be simplified?
+    return new Reader(clone, buffer, this.getBlockReader(clone, buffer));
+  }
+
+  @Override
+  public void close() throws IOException {
+    in.close();
+  }
+
+  @Override
+  public Index index() {
+    return new Index();
+  }
+
+  protected abstract BlockReader getBlockReader(IndexInput in, int[] buffer) throws IOException;
+
+  /**
+   * Interface for fixed-size block decoders.
+   * <p>
+   * Implementations should decode into the buffer in {@link #readBlock}.
+   */
+  public interface BlockReader {
+    public void readBlock() throws IOException;
+  }
+
+  private static class Reader extends IntIndexInput.Reader {
+    private final IndexInput in;
+    private final BlockReader blockReader;
+    private final int blockSize;
+    private final int[] pending;
+
+    private int upto;
+    private boolean seekPending;
+    private long pendingFP;
+    private long lastBlockFP = -1;
+
+    public Reader(final IndexInput in, final int[] pending, final BlockReader blockReader) {
+      this.in = in;
+      this.pending = pending;
+      this.blockSize = pending.length;
+      this.blockReader = blockReader;
+      upto = blockSize;
+    }
+
+    void seek(final long fp, final int upto) {
+      assert upto < blockSize;
+      if (seekPending || fp != lastBlockFP) {
+        pendingFP = fp;
+        seekPending = true;
+      }
+      this.upto = upto;
+    }
+
+    @Override
+    public int next() throws IOException {
+      if (seekPending) {
+        // Seek & load new block
+        in.seek(pendingFP);
+        lastBlockFP = pendingFP;
+        blockReader.readBlock();
+        seekPending = false;
+      } else if (upto == blockSize) {
+        // Load new block
+        lastBlockFP = in.getFilePointer();
+        blockReader.readBlock();
+        upto = 0;
+      }
+      return pending[upto++];
+    }
+  }
+
+  private class Index extends IntIndexInput.Index {
+    private long fp;
+    private int upto;
+
+    @Override
+    public void read(final DataInput indexIn, final boolean absolute) throws IOException {
+      if (absolute) {
+        upto = indexIn.readVInt();
+        fp = indexIn.readVLong();
+      } else {
+        final int uptoDelta = indexIn.readVInt();
+        if ((uptoDelta & 1) == 1) {
+          // same block
+          upto += uptoDelta >>> 1;
+        } else {
+          // new block
+          upto = uptoDelta >>> 1;
+          fp += indexIn.readVLong();
+        }
+      }
+      assert upto < blockSize;
+    }
+
+    @Override
+    public void seek(final IntIndexInput.Reader other) throws IOException {
+      ((Reader) other).seek(fp, upto);
+    }
+
+    @Override
+    public void copyFrom(final IntIndexInput.Index other) {
+      final Index idx = (Index) other;
+      fp = idx.fp;
+      upto = idx.upto;
+    }
+
+    @Override
+    public Index clone() {
+      Index other = new Index();
+      other.fp = fp;
+      other.upto = upto;
+      return other;
+    }
+    
+    @Override
+    public String toString() {
+      return "fp=" + fp + " upto=" + upto;
+    }
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/intblock/FixedIntBlockIndexOutput.java b/lucene/codecs/src/java/org/apache/lucene/codecs/intblock/FixedIntBlockIndexOutput.java
new file mode 100644
index 0000000..004e51c
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/intblock/FixedIntBlockIndexOutput.java
@@ -0,0 +1,127 @@
+package org.apache.lucene.codecs.intblock;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/** Naive int block API that writes vInts.  This is
+ *  expected to give poor performance; it's really only for
+ *  testing the pluggability.  One should typically use pfor instead. */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.sep.IntIndexOutput;
+import org.apache.lucene.store.IndexOutput;
+
+/** Abstract base class that writes fixed-size blocks of ints
+ *  to an IndexOutput.  While this is a simple approach, a
+ *  more performant approach would directly create an impl
+ *  of IntIndexOutput inside Directory.  Wrapping a generic
+ *  IndexInput will likely cost performance.
+ *
+ * @lucene.experimental
+ */
+public abstract class FixedIntBlockIndexOutput extends IntIndexOutput {
+
+  protected final IndexOutput out;
+  private final int blockSize;
+  protected final int[] buffer;
+  private int upto;
+
+  protected FixedIntBlockIndexOutput(IndexOutput out, int fixedBlockSize) throws IOException {
+    blockSize = fixedBlockSize;
+    this.out = out;
+    out.writeVInt(blockSize);
+    buffer = new int[blockSize];
+  }
+
+  protected abstract void flushBlock() throws IOException;
+
+  @Override
+  public Index index() throws IOException {
+    return new Index();
+  }
+
+  private class Index extends IntIndexOutput.Index {
+    long fp;
+    int upto;
+    long lastFP;
+    int lastUpto;
+
+    @Override
+    public void mark() throws IOException {
+      fp = out.getFilePointer();
+      upto = FixedIntBlockIndexOutput.this.upto;
+    }
+
+    @Override
+    public void copyFrom(IntIndexOutput.Index other, boolean copyLast) throws IOException {
+      Index idx = (Index) other;
+      fp = idx.fp;
+      upto = idx.upto;
+      if (copyLast) {
+        lastFP = fp;
+        lastUpto = upto;
+      }
+    }
+
+    @Override
+    public void write(IndexOutput indexOut, boolean absolute) throws IOException {
+      if (absolute) {
+        indexOut.writeVInt(upto);
+        indexOut.writeVLong(fp);
+      } else if (fp == lastFP) {
+        // same block
+        assert upto >= lastUpto;
+        int uptoDelta = upto - lastUpto;
+        indexOut.writeVInt(uptoDelta << 1 | 1);
+      } else {      
+        // new block
+        indexOut.writeVInt(upto << 1);
+        indexOut.writeVLong(fp - lastFP);
+      }
+      lastUpto = upto;
+      lastFP = fp;
+    }
+
+    @Override
+    public String toString() {
+      return "fp=" + fp + " upto=" + upto;
+    }
+  }
+
+  @Override
+  public void write(int v) throws IOException {
+    buffer[upto++] = v;
+    if (upto == blockSize) {
+      flushBlock();
+      upto = 0;
+    }
+  }
+
+  @Override
+  public void close() throws IOException {
+    try {
+      if (upto > 0) {
+        // NOTE: entries in the block after current upto are
+        // invalid
+        flushBlock();
+      }
+    } finally {
+      out.close();
+    }
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/intblock/VariableIntBlockIndexInput.java b/lucene/codecs/src/java/org/apache/lucene/codecs/intblock/VariableIntBlockIndexInput.java
new file mode 100644
index 0000000..9505c1e
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/intblock/VariableIntBlockIndexInput.java
@@ -0,0 +1,198 @@
+package org.apache.lucene.codecs.intblock;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/** Naive int block API that writes vInts.  This is
+ *  expected to give poor performance; it's really only for
+ *  testing the pluggability.  One should typically use pfor instead. */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.sep.IntIndexInput;
+import org.apache.lucene.store.DataInput;
+import org.apache.lucene.store.IndexInput;
+
+// TODO: much of this can be shared code w/ the fixed case
+
+/** Abstract base class that reads variable-size blocks of ints
+ *  from an IndexInput.  While this is a simple approach, a
+ *  more performant approach would directly create an impl
+ *  of IntIndexInput inside Directory.  Wrapping a generic
+ *  IndexInput will likely cost performance.
+ *
+ * @lucene.experimental
+ */
+public abstract class VariableIntBlockIndexInput extends IntIndexInput {
+
+  protected final IndexInput in;
+  protected final int maxBlockSize;
+
+  protected VariableIntBlockIndexInput(final IndexInput in) throws IOException {
+    this.in = in;
+    maxBlockSize = in.readInt();
+  }
+
+  @Override
+  public Reader reader() throws IOException {
+    final int[] buffer = new int[maxBlockSize];
+    final IndexInput clone = in.clone();
+    // TODO: can this be simplified?
+    return new Reader(clone, buffer, this.getBlockReader(clone, buffer));
+  }
+
+  @Override
+  public void close() throws IOException {
+    in.close();
+  }
+
+  @Override
+  public Index index() {
+    return new Index();
+  }
+
+  protected abstract BlockReader getBlockReader(IndexInput in, int[] buffer) throws IOException;
+
+  /**
+   * Interface for variable-size block decoders.
+   * <p>
+   * Implementations should decode into the buffer in {@link #readBlock}.
+   */
+  public interface BlockReader {
+    public int readBlock() throws IOException;
+    public void seek(long pos) throws IOException;
+  }
+
+  private static class Reader extends IntIndexInput.Reader {
+    private final IndexInput in;
+
+    public final int[] pending;
+    int upto;
+
+    private boolean seekPending;
+    private long pendingFP;
+    private int pendingUpto;
+    private long lastBlockFP;
+    private int blockSize;
+    private final BlockReader blockReader;
+
+    public Reader(final IndexInput in, final int[] pending, final BlockReader blockReader) {
+      this.in = in;
+      this.pending = pending;
+      this.blockReader = blockReader;
+    }
+
+    void seek(final long fp, final int upto) {
+      // TODO: should we do this in real-time, not lazy?
+      pendingFP = fp;
+      pendingUpto = upto;
+      assert pendingUpto >= 0: "pendingUpto=" + pendingUpto;
+      seekPending = true;
+    }
+
+    private final void maybeSeek() throws IOException {
+      if (seekPending) {
+        if (pendingFP != lastBlockFP) {
+          // need new block
+          in.seek(pendingFP);
+          blockReader.seek(pendingFP);
+          lastBlockFP = pendingFP;
+          blockSize = blockReader.readBlock();
+        }
+        upto = pendingUpto;
+
+        // TODO: if we were more clever when writing the
+        // index, such that a seek point wouldn't be written
+        // until the int encoder "committed", we could avoid
+        // this (likely minor) inefficiency:
+
+        // This is necessary for int encoders that are
+        // non-causal, ie must see future int values to
+        // encode the current ones.
+        while(upto >= blockSize) {
+          upto -= blockSize;
+          lastBlockFP = in.getFilePointer();
+          blockSize = blockReader.readBlock();
+        }
+        seekPending = false;
+      }
+    }
+
+    @Override
+    public int next() throws IOException {
+      this.maybeSeek();
+      if (upto == blockSize) {
+        lastBlockFP = in.getFilePointer();
+        blockSize = blockReader.readBlock();
+        upto = 0;
+      }
+
+      return pending[upto++];
+    }
+  }
+
+  private class Index extends IntIndexInput.Index {
+    private long fp;
+    private int upto;
+
+    @Override
+    public void read(final DataInput indexIn, final boolean absolute) throws IOException {
+      if (absolute) {
+        upto = indexIn.readVInt();
+        fp = indexIn.readVLong();
+      } else {
+        final int uptoDelta = indexIn.readVInt();
+        if ((uptoDelta & 1) == 1) {
+          // same block
+          upto += uptoDelta >>> 1;
+        } else {
+          // new block
+          upto = uptoDelta >>> 1;
+          fp += indexIn.readVLong();
+        }
+      }
+      // TODO: we can't do this assert because non-causal
+      // int encoders can have upto over the buffer size
+      //assert upto < maxBlockSize: "upto=" + upto + " max=" + maxBlockSize;
+    }
+
+    @Override
+    public String toString() {
+      return "VarIntBlock.Index fp=" + fp + " upto=" + upto + " maxBlock=" + maxBlockSize;
+    }
+
+    @Override
+    public void seek(final IntIndexInput.Reader other) throws IOException {
+      ((Reader) other).seek(fp, upto);
+    }
+
+    @Override
+    public void copyFrom(final IntIndexInput.Index other) {
+      final Index idx = (Index) other;
+      fp = idx.fp;
+      upto = idx.upto;
+    }
+
+    @Override
+    public Index clone() {
+      Index other = new Index();
+      other.fp = fp;
+      other.upto = upto;
+      return other;
+    }
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/intblock/VariableIntBlockIndexOutput.java b/lucene/codecs/src/java/org/apache/lucene/codecs/intblock/VariableIntBlockIndexOutput.java
new file mode 100644
index 0000000..9d31c3c
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/intblock/VariableIntBlockIndexOutput.java
@@ -0,0 +1,135 @@
+package org.apache.lucene.codecs.intblock;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/** Naive int block API that writes vInts.  This is
+ *  expected to give poor performance; it's really only for
+ *  testing the pluggability.  One should typically use pfor instead. */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.sep.IntIndexOutput;
+import org.apache.lucene.store.IndexOutput;
+
+// TODO: much of this can be shared code w/ the fixed case
+
+/** Abstract base class that writes variable-size blocks of ints
+ *  to an IndexOutput.  While this is a simple approach, a
+ *  more performant approach would directly create an impl
+ *  of IntIndexOutput inside Directory.  Wrapping a generic
+ *  IndexInput will likely cost performance.
+ *
+ * @lucene.experimental
+ */
+public abstract class VariableIntBlockIndexOutput extends IntIndexOutput {
+
+  protected final IndexOutput out;
+
+  private int upto;
+  private boolean hitExcDuringWrite;
+
+  // TODO what Var-Var codecs exist in practice... and what are there blocksizes like?
+  // if its less than 128 we should set that as max and use byte?
+
+  /** NOTE: maxBlockSize must be the maximum block size 
+   *  plus the max non-causal lookahead of your codec.  EG Simple9
+   *  requires lookahead=1 because on seeing the Nth value
+   *  it knows it must now encode the N-1 values before it. */
+  protected VariableIntBlockIndexOutput(IndexOutput out, int maxBlockSize) throws IOException {
+    this.out = out;
+    out.writeInt(maxBlockSize);
+  }
+
+  /** Called one value at a time.  Return the number of
+   *  buffered input values that have been written to out. */
+  protected abstract int add(int value) throws IOException;
+
+  @Override
+  public Index index() throws IOException {
+    return new Index();
+  }
+
+  private class Index extends IntIndexOutput.Index {
+    long fp;
+    int upto;
+    long lastFP;
+    int lastUpto;
+
+    @Override
+    public void mark() throws IOException {
+      fp = out.getFilePointer();
+      upto = VariableIntBlockIndexOutput.this.upto;
+    }
+
+    @Override
+    public void copyFrom(IntIndexOutput.Index other, boolean copyLast) throws IOException {
+      Index idx = (Index) other;
+      fp = idx.fp;
+      upto = idx.upto;
+      if (copyLast) {
+        lastFP = fp;
+        lastUpto = upto;
+      }
+    }
+
+    @Override
+    public void write(IndexOutput indexOut, boolean absolute) throws IOException {
+      assert upto >= 0;
+      if (absolute) {
+        indexOut.writeVInt(upto);
+        indexOut.writeVLong(fp);
+      } else if (fp == lastFP) {
+        // same block
+        assert upto >= lastUpto;
+        int uptoDelta = upto - lastUpto;
+        indexOut.writeVInt(uptoDelta << 1 | 1);
+      } else {      
+        // new block
+        indexOut.writeVInt(upto << 1);
+        indexOut.writeVLong(fp - lastFP);
+      }
+      lastUpto = upto;
+      lastFP = fp;
+    }
+  }
+
+  @Override
+  public void write(int v) throws IOException {
+    hitExcDuringWrite = true;
+    upto -= add(v)-1;
+    hitExcDuringWrite = false;
+    assert upto >= 0;
+  }
+
+  @Override
+  public void close() throws IOException {
+    try {
+      if (!hitExcDuringWrite) {
+        // stuff 0s in until the "real" data is flushed:
+        int stuffed = 0;
+        while(upto > stuffed) {
+          upto -= add(0)-1;
+          assert upto >= 0;
+          stuffed += 1;
+        }
+      }
+    } finally {
+      out.close();
+    }
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/intblock/package.html b/lucene/codecs/src/java/org/apache/lucene/codecs/intblock/package.html
new file mode 100644
index 0000000..403ea1b
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/intblock/package.html
@@ -0,0 +1,25 @@
+<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+<html>
+<head>
+   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
+</head>
+<body>
+Intblock: base support for fixed or variable length block integer encoders
+</body>
+</html>
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/memory/DirectPostingsFormat.java b/lucene/codecs/src/java/org/apache/lucene/codecs/memory/DirectPostingsFormat.java
new file mode 100644
index 0000000..98e1a36
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/memory/DirectPostingsFormat.java
@@ -0,0 +1,2194 @@
+package org.apache.lucene.codecs.memory;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Collections;
+import java.util.Comparator;
+import java.util.Iterator;
+import java.util.Map;
+import java.util.TreeMap;
+
+import org.apache.lucene.codecs.FieldsConsumer;
+import org.apache.lucene.codecs.FieldsProducer;
+import org.apache.lucene.codecs.PostingsFormat;
+import org.apache.lucene.codecs.lucene40.Lucene40PostingsFormat; // javadocs
+import org.apache.lucene.index.DocsAndPositionsEnum;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.Fields;
+import org.apache.lucene.index.OrdTermState;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.index.TermState;
+import org.apache.lucene.index.Terms;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.RAMOutputStream;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.automaton.CompiledAutomaton;
+import org.apache.lucene.util.automaton.RunAutomaton;
+import org.apache.lucene.util.automaton.Transition;
+
+// TODO: 
+//   - build depth-N prefix hash?
+//   - or: longer dense skip lists than just next byte?
+
+/** Wraps {@link Lucene40PostingsFormat} format for on-disk
+ *  storage, but then at read time loads and stores all
+ *  terms & postings directly in RAM as byte[], int[].
+ *
+ *  <p><b><font color=red>WARNING</font></b>: This is
+ *  exceptionally RAM intensive: it makes no effort to
+ *  compress the postings data, storing terms as separate
+ *  byte[] and postings as separate int[], but as a result it 
+ *  gives substantial increase in search performance.
+ *
+ *  <p>This postings format supports {@link TermsEnum#ord}
+ *  and {@link TermsEnum#seekExact(long)}.
+
+ *  <p>Because this holds all term bytes as a single
+ *  byte[], you cannot have more than 2.1GB worth of term
+ *  bytes in a single segment.
+ *
+ * @lucene.experimental */
+
+public class DirectPostingsFormat extends PostingsFormat {
+
+  private final int minSkipCount;
+  private final int lowFreqCutoff;
+
+  private final static int DEFAULT_MIN_SKIP_COUNT = 8;
+  private final static int DEFAULT_LOW_FREQ_CUTOFF = 32;
+
+  //private static final boolean DEBUG = true;
+
+  // TODO: allow passing/wrapping arbitrary postings format?
+
+  public DirectPostingsFormat() {
+    this(DEFAULT_MIN_SKIP_COUNT, DEFAULT_LOW_FREQ_CUTOFF);
+  }
+  
+  /** minSkipCount is how many terms in a row must have the
+   *  same prefix before we put a skip pointer down.  Terms
+   *  with docFreq <= lowFreqCutoff will use a single int[]
+   *  to hold all docs, freqs, position and offsets; terms
+   *  with higher docFreq will use separate arrays. */
+  public DirectPostingsFormat(int minSkipCount, int lowFreqCutoff) {
+    super("Direct");
+    this.minSkipCount = minSkipCount;
+    this.lowFreqCutoff = lowFreqCutoff;
+  }
+  
+  @Override
+  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
+    return PostingsFormat.forName("Lucene40").fieldsConsumer(state);
+  }
+
+  @Override
+  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
+    FieldsProducer postings = PostingsFormat.forName("Lucene40").fieldsProducer(state);
+    if (state.context.context != IOContext.Context.MERGE) {
+      FieldsProducer loadedPostings;
+      try {
+        loadedPostings = new DirectFields(state, postings, minSkipCount, lowFreqCutoff);
+      } finally {
+        postings.close();
+      }
+      return loadedPostings;
+    } else {
+      // Don't load postings for merge:
+      return postings;
+    }
+  }
+
+  private static final class DirectFields extends FieldsProducer {
+    private final Map<String,DirectField> fields = new TreeMap<String,DirectField>();
+
+    public DirectFields(SegmentReadState state, Fields fields, int minSkipCount, int lowFreqCutoff) throws IOException {
+      for (String field : fields) {
+        this.fields.put(field, new DirectField(state, field, fields.terms(field), minSkipCount, lowFreqCutoff));
+      }
+    }
+
+    @Override
+    public Iterator<String> iterator() {
+      return Collections.unmodifiableSet(fields.keySet()).iterator();
+    }
+
+    @Override
+    public Terms terms(String field) {
+      return fields.get(field);
+    }
+
+    @Override
+    public int size() {
+      return fields.size();
+    }
+
+    @Override
+    public void close() {
+    }
+  }
+
+  private final static class DirectField extends Terms {
+
+    private static abstract class TermAndSkip {
+      public int[] skips;
+    }
+
+    private static final class LowFreqTerm extends TermAndSkip {
+      public final int[] postings;
+      public final byte[] payloads;
+      public final int docFreq;
+      public final int totalTermFreq;
+
+      public LowFreqTerm(int[] postings, byte[] payloads, int docFreq, int totalTermFreq) {
+        this.postings = postings;
+        this.payloads = payloads;
+        this.docFreq = docFreq;
+        this.totalTermFreq = totalTermFreq;
+      }
+    }
+
+    // TODO: maybe specialize into prx/no-prx/no-frq cases?
+    private static final class HighFreqTerm extends TermAndSkip {
+      public final long totalTermFreq;
+      public final int[] docIDs;
+      public final int[] freqs;
+      public final int[][] positions;
+      public final byte[][][] payloads;
+
+      public HighFreqTerm(int[] docIDs, int[] freqs, int[][] positions, byte[][][] payloads, long totalTermFreq) {
+        this.docIDs = docIDs;
+        this.freqs = freqs;
+        this.positions = positions;
+        this.payloads = payloads;
+        this.totalTermFreq = totalTermFreq;
+      }
+    }
+
+    private final byte[] termBytes;
+    private final int[] termOffsets;
+
+    private final int[] skips;
+    private final int[] skipOffsets;
+
+    private final TermAndSkip[] terms;
+    private final boolean hasFreq;
+    private final boolean hasPos;
+    private final boolean hasOffsets;
+    private final boolean hasPayloads;
+    private final long sumTotalTermFreq;
+    private final int docCount;
+    private final long sumDocFreq;
+    private int skipCount;
+
+    // TODO: maybe make a separate builder?  These are only
+    // used during load:
+    private int count;
+    private int[] sameCounts = new int[10];
+    private final int minSkipCount;
+
+    private final static class IntArrayWriter {
+      private int[] ints = new int[10];
+      private int upto;
+
+      public void add(int value) {
+        if (ints.length == upto) {
+          ints = ArrayUtil.grow(ints);
+        }
+        ints[upto++] = value;
+      }
+
+      public int[] get() {
+        final int[] arr = new int[upto];
+        System.arraycopy(ints, 0, arr, 0, upto);
+        upto = 0;
+        return arr;
+      }
+    }
+
+    public DirectField(SegmentReadState state, String field, Terms termsIn, int minSkipCount, int lowFreqCutoff) throws IOException {
+      final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);
+
+      sumTotalTermFreq = termsIn.getSumTotalTermFreq();
+      sumDocFreq = termsIn.getSumDocFreq();
+      docCount = termsIn.getDocCount();
+
+      final int numTerms = (int) termsIn.size();
+      if (numTerms == -1) {
+        throw new IllegalArgumentException("codec does not provide Terms.size()");
+      }
+      terms = new TermAndSkip[numTerms];
+      termOffsets = new int[1+numTerms];
+      
+      byte[] termBytes = new byte[1024];
+
+      this.minSkipCount = minSkipCount;
+
+      hasFreq = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_ONLY) > 0;
+      hasPos = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) > 0;
+      hasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) > 0;
+      hasPayloads = fieldInfo.hasPayloads();
+
+      BytesRef term;
+      DocsEnum docsEnum = null;
+      DocsAndPositionsEnum docsAndPositionsEnum = null;
+      final TermsEnum termsEnum = termsIn.iterator(null);
+      int termOffset = 0;
+
+      final IntArrayWriter scratch = new IntArrayWriter();
+
+      // Used for payloads, if any:
+      final RAMOutputStream ros = new RAMOutputStream();
+
+      // if (DEBUG) {
+      //   System.out.println("\nLOAD terms seg=" + state.segmentInfo.name + " field=" + field + " hasOffsets=" + hasOffsets + " hasFreq=" + hasFreq + " hasPos=" + hasPos + " hasPayloads=" + hasPayloads);
+      // }
+
+      while ((term = termsEnum.next()) != null) {
+        final int docFreq = termsEnum.docFreq();
+        final long totalTermFreq = termsEnum.totalTermFreq();
+
+        // if (DEBUG) {
+        //   System.out.println("  term=" + term.utf8ToString());
+        // }
+
+        termOffsets[count] = termOffset;
+
+        if (termBytes.length < (termOffset + term.length)) {
+          termBytes = ArrayUtil.grow(termBytes, termOffset + term.length);
+        }
+        System.arraycopy(term.bytes, term.offset, termBytes, termOffset, term.length);
+        termOffset += term.length;
+        termOffsets[count+1] = termOffset;
+
+        if (hasPos) {
+          docsAndPositionsEnum = termsEnum.docsAndPositions(null, docsAndPositionsEnum);
+        } else {
+          docsEnum = termsEnum.docs(null, docsEnum);
+        }
+
+        final TermAndSkip ent;
+
+        final DocsEnum docsEnum2;
+        if (hasPos) {
+          docsEnum2 = docsAndPositionsEnum;
+        } else {
+          docsEnum2 = docsEnum;
+        }
+
+        int docID;
+
+        if (docFreq <= lowFreqCutoff) {
+
+          ros.reset();
+
+          // Pack postings for low-freq terms into a single int[]:
+          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {
+            scratch.add(docID);
+            if (hasFreq) {
+              final int freq = docsEnum2.freq();
+              scratch.add(freq);
+              if (hasPos) {
+                for(int pos=0;pos<freq;pos++) {
+                  scratch.add(docsAndPositionsEnum.nextPosition());
+                  if (hasOffsets) {
+                    scratch.add(docsAndPositionsEnum.startOffset());
+                    scratch.add(docsAndPositionsEnum.endOffset());
+                  }
+                  if (hasPayloads) {
+                    final BytesRef payload = docsAndPositionsEnum.getPayload();
+                    if (payload != null) {
+                      scratch.add(payload.length);
+                      ros.writeBytes(payload.bytes, payload.offset, payload.length);
+                    } else {
+                      scratch.add(0);
+                    }
+                  }
+                }
+              }
+            }
+          }
+
+          final byte[] payloads;
+          if (hasPayloads) {
+            ros.flush();
+            payloads = new byte[(int) ros.length()];
+            ros.writeTo(payloads, 0);
+          } else {
+            payloads = null;
+          }
+
+          final int[] postings = scratch.get();
+        
+          ent = new LowFreqTerm(postings, payloads, docFreq, (int) totalTermFreq);
+        } else {
+          final int[] docs = new int[docFreq];
+          final int[] freqs;
+          final int[][] positions;
+          final byte[][][] payloads;
+          if (hasFreq) {
+            freqs = new int[docFreq];
+            if (hasPos) {
+              positions = new int[docFreq][];
+              if (hasPayloads) {
+                payloads = new byte[docFreq][][];
+              } else {
+                payloads = null;
+              }
+            } else {
+              positions = null;
+              payloads = null;
+            }
+          } else {
+            freqs = null;
+            positions = null;
+            payloads = null;
+          }
+
+          // Use separate int[] for the postings for high-freq
+          // terms:
+          int upto = 0;
+          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {
+            docs[upto] = docID;
+            if (hasFreq) {
+              final int freq = docsEnum2.freq();
+              freqs[upto] = freq;
+              if (hasPos) {
+                final int mult;
+                if (hasOffsets) {
+                  mult = 3;
+                } else {
+                  mult = 1;
+                }
+                if (hasPayloads) {
+                  payloads[upto] = new byte[freq][];
+                }
+                positions[upto] = new int[mult*freq];
+                int posUpto = 0;
+                for(int pos=0;pos<freq;pos++) {
+                  positions[upto][posUpto] = docsAndPositionsEnum.nextPosition();
+                  if (hasPayloads) {
+                    BytesRef payload = docsAndPositionsEnum.getPayload();
+                    if (payload != null) {
+                      byte[] payloadBytes = new byte[payload.length];
+                      System.arraycopy(payload.bytes, payload.offset, payloadBytes, 0, payload.length);
+                      payloads[upto][pos] = payloadBytes;
+                    }
+                  }
+                  posUpto++;
+                  if (hasOffsets) {
+                    positions[upto][posUpto++] = docsAndPositionsEnum.startOffset();
+                    positions[upto][posUpto++] = docsAndPositionsEnum.endOffset();
+                  }
+                }
+              }
+            }
+
+            upto++;
+          }
+          assert upto == docFreq;
+          ent = new HighFreqTerm(docs, freqs, positions, payloads, totalTermFreq);
+        }
+
+        terms[count] = ent;
+        setSkips(count, termBytes);
+        count++;
+      }
+
+      // End sentinel:
+      termOffsets[count] = termOffset;
+
+      finishSkips();
+
+      //System.out.println(skipCount + " skips: " + field);
+
+      this.termBytes = new byte[termOffset];
+      System.arraycopy(termBytes, 0, this.termBytes, 0, termOffset);
+
+      // Pack skips:
+      this.skips = new int[skipCount];
+      this.skipOffsets = new int[1+numTerms];
+
+      int skipOffset = 0;
+      for(int i=0;i<numTerms;i++) {
+        final int[] termSkips = terms[i].skips;
+        skipOffsets[i] = skipOffset;
+        if (termSkips != null) {
+          System.arraycopy(termSkips, 0, skips, skipOffset, termSkips.length);
+          skipOffset += termSkips.length;
+          terms[i].skips = null;
+        }
+      }
+      this.skipOffsets[numTerms] = skipOffset;
+      assert skipOffset == skipCount;
+    }
+
+    // Compares in unicode (UTF8) order:
+    int compare(int ord, BytesRef other) {
+      final byte[] otherBytes = other.bytes;
+
+      int upto = termOffsets[ord];
+      final int termLen = termOffsets[1+ord] - upto;
+      int otherUpto = other.offset;
+      
+      final int stop = upto + Math.min(termLen, other.length);
+      while (upto < stop) {
+        int diff = (termBytes[upto++] & 0xFF) - (otherBytes[otherUpto++] & 0xFF);
+        if (diff != 0) {
+          return diff;
+        }
+      }
+    
+      // One is a prefix of the other, or, they are equal:
+      return termLen - other.length;
+    }
+
+    private void setSkips(int termOrd, byte[] termBytes) {
+
+      final int termLength = termOffsets[termOrd+1] - termOffsets[termOrd];
+
+      if (sameCounts.length < termLength) {
+        sameCounts = ArrayUtil.grow(sameCounts, termLength);
+      }
+
+      // Update skip pointers:
+      if (termOrd > 0) {
+        final int lastTermLength = termOffsets[termOrd] - termOffsets[termOrd-1];
+        final int limit = Math.min(termLength, lastTermLength);
+
+        int lastTermOffset = termOffsets[termOrd-1];
+        int termOffset = termOffsets[termOrd];
+
+        int i = 0;
+        for(;i<limit;i++) {
+          if (termBytes[lastTermOffset++] == termBytes[termOffset++]) {
+            sameCounts[i]++;
+          } else {
+            for(;i<limit;i++) {
+              if (sameCounts[i] >= minSkipCount) {
+                // Go back and add a skip pointer:
+                saveSkip(termOrd, sameCounts[i]);
+              }
+              sameCounts[i] = 1;
+            }
+            break;
+          }
+        }
+
+        for(;i<lastTermLength;i++) {
+          if (sameCounts[i] >= minSkipCount) {
+            // Go back and add a skip pointer:
+            saveSkip(termOrd, sameCounts[i]);
+          }
+          sameCounts[i] = 0;
+        }
+        for(int j=limit;j<termLength;j++) {
+          sameCounts[j] = 1;
+        }
+      } else {
+        for(int i=0;i<termLength;i++) {
+          sameCounts[i]++;
+        }
+      }
+    }
+
+    private void finishSkips() {
+      assert count == terms.length;
+      int lastTermOffset = termOffsets[count-1];
+      int lastTermLength = termOffsets[count] - lastTermOffset;
+
+      for(int i=0;i<lastTermLength;i++) {
+        if (sameCounts[i] >= minSkipCount) {
+          // Go back and add a skip pointer:
+          saveSkip(count, sameCounts[i]);
+        }
+      }
+
+      // Reverse the skip pointers so they are "nested":
+      for(int termID=0;termID<terms.length;termID++) {
+        TermAndSkip term = terms[termID];
+        if (term.skips != null && term.skips.length > 1) {
+          for(int pos=0;pos<term.skips.length/2;pos++) {
+            final int otherPos = term.skips.length-pos-1;
+
+            final int temp = term.skips[pos];
+            term.skips[pos] = term.skips[otherPos];
+            term.skips[otherPos] = temp;
+          }
+        }
+      }
+    }
+
+    private void saveSkip(int ord, int backCount) {
+      final TermAndSkip term = terms[ord - backCount];
+      skipCount++;
+      if (term.skips == null) {
+        term.skips = new int[] {ord};
+      } else {
+        // Normally we'd grow at a slight exponential... but
+        // given that the skips themselves are already log(N)
+        // we can grow by only 1 and still have amortized
+        // linear time:
+        final int[] newSkips = new int[term.skips.length+1];
+        System.arraycopy(term.skips, 0, newSkips, 0, term.skips.length);
+        term.skips = newSkips;
+        term.skips[term.skips.length-1] = ord;
+      }
+    }
+
+    @Override
+    public TermsEnum iterator(TermsEnum reuse) {
+      DirectTermsEnum termsEnum;
+      if (reuse != null && reuse instanceof DirectTermsEnum) {
+        termsEnum = (DirectTermsEnum) reuse;
+        if (!termsEnum.canReuse(terms)) {
+          termsEnum = new DirectTermsEnum();
+        }
+      } else {
+        termsEnum = new DirectTermsEnum();
+      }
+      termsEnum.reset();
+      return termsEnum;
+    }
+
+    @Override
+    public TermsEnum intersect(CompiledAutomaton compiled, final BytesRef startTerm) {
+      return new DirectIntersectTermsEnum(compiled, startTerm);
+    }
+
+    @Override
+    public long size() {
+      return terms.length;
+    }
+
+    @Override
+    public long getSumTotalTermFreq() {
+      return sumTotalTermFreq;
+    }
+
+    @Override
+    public long getSumDocFreq() {
+      return sumDocFreq;
+    }
+
+    @Override
+    public int getDocCount() {
+      return docCount;
+    }
+
+    @Override
+    public Comparator<BytesRef> getComparator() {
+      return BytesRef.getUTF8SortedAsUnicodeComparator();
+    }
+
+    @Override
+    public boolean hasOffsets() {
+      return hasOffsets;
+    }
+
+    @Override
+    public boolean hasPositions() {
+      return hasPos;
+    }
+    
+    @Override
+    public boolean hasPayloads() {
+      return hasPayloads;
+    }
+
+    private final class DirectTermsEnum extends TermsEnum {
+
+      private final BytesRef scratch = new BytesRef();
+      private int termOrd;
+
+      boolean canReuse(TermAndSkip[] other) {
+        return DirectField.this.terms == other;
+      }
+
+      private BytesRef setTerm() {
+        scratch.bytes = termBytes;
+        scratch.offset = termOffsets[termOrd];
+        scratch.length = termOffsets[termOrd+1] - termOffsets[termOrd];
+        return scratch;
+      }
+
+      public void reset() {
+        termOrd = -1;
+      }
+
+      public Comparator<BytesRef> getComparator() {
+        return BytesRef.getUTF8SortedAsUnicodeComparator();
+      }
+
+      @Override
+      public BytesRef next() {
+        termOrd++;
+        if (termOrd < terms.length) {
+          return setTerm();
+        } else {
+          return null;
+        }
+      }
+
+      @Override
+      public TermState termState() {
+        OrdTermState state = new OrdTermState();
+        state.ord = termOrd;
+        return state;
+      }
+
+      // If non-negative, exact match; else, -ord-1, where ord
+      // is where you would insert the term.
+      private int findTerm(BytesRef term) {
+
+        // Just do binary search: should be (constant factor)
+        // faster than using the skip list:
+        int low = 0;
+        int high = terms.length-1;
+
+        while (low <= high) {
+          int mid = (low + high) >>> 1;
+          int cmp = compare(mid, term);
+          if (cmp < 0) {
+            low = mid + 1;
+          } else if (cmp > 0) {
+            high = mid - 1;
+          } else {
+            return mid; // key found
+          }
+        }
+
+        return -(low + 1);  // key not found.
+      }
+
+      @Override
+      public SeekStatus seekCeil(BytesRef term, boolean useCache) {
+        // TODO: we should use the skip pointers; should be
+        // faster than bin search; we should also hold
+        // & reuse current state so seeking forwards is
+        // faster
+        final int ord = findTerm(term);
+        // if (DEBUG) {
+        //   System.out.println("  find term=" + term.utf8ToString() + " ord=" + ord);
+        // }
+        if (ord >= 0) {
+          termOrd = ord;
+          setTerm();
+          return SeekStatus.FOUND;
+        } else if (ord == -terms.length-1) {
+          return SeekStatus.END;
+        } else {
+          termOrd = -ord - 1;
+          setTerm();
+          return SeekStatus.NOT_FOUND;
+        }
+      }
+
+      @Override
+      public boolean seekExact(BytesRef term, boolean useCache) {
+        // TODO: we should use the skip pointers; should be
+        // faster than bin search; we should also hold
+        // & reuse current state so seeking forwards is
+        // faster
+        final int ord = findTerm(term);
+        if (ord >= 0) {
+          termOrd = ord;
+          setTerm();
+          return true;
+        } else {
+          return false;
+        }
+      }
+
+      @Override
+      public void seekExact(long ord) {
+        termOrd = (int) ord;
+        setTerm();
+      }
+
+      @Override
+      public void seekExact(BytesRef term, TermState state) throws IOException {
+        termOrd = (int) ((OrdTermState) state).ord;
+        setTerm();
+        assert term.equals(scratch);
+      }
+
+      @Override
+      public BytesRef term() {
+        return scratch;
+      }
+
+      @Override
+      public long ord() {
+        return termOrd;
+      }
+
+      @Override
+      public int docFreq() {
+        if (terms[termOrd] instanceof LowFreqTerm) {
+          return ((LowFreqTerm) terms[termOrd]).docFreq;
+        } else {
+          return ((HighFreqTerm) terms[termOrd]).docIDs.length;
+        }
+      }
+
+      @Override
+      public long totalTermFreq() {
+        if (terms[termOrd] instanceof LowFreqTerm) {
+          return ((LowFreqTerm) terms[termOrd]).totalTermFreq;
+        } else {
+          return ((HighFreqTerm) terms[termOrd]).totalTermFreq;
+        }
+      }
+
+      @Override
+      public DocsEnum docs(Bits liveDocs, DocsEnum reuse, int flags) {
+        // TODO: implement reuse, something like Pulsing:
+        // it's hairy!
+
+        if (terms[termOrd] instanceof LowFreqTerm) {
+          final int[] postings = ((LowFreqTerm) terms[termOrd]).postings;
+          if (hasFreq) {
+            if (hasPos) {
+              int posLen;
+              if (hasOffsets) {
+                posLen = 3;
+              } else {
+                posLen = 1;
+              }
+              if (hasPayloads) {
+                posLen++;
+              }
+              LowFreqDocsEnum docsEnum;
+              if (reuse instanceof LowFreqDocsEnum) {
+                docsEnum = (LowFreqDocsEnum) reuse;
+                if (!docsEnum.canReuse(liveDocs, posLen)) {
+                  docsEnum = new LowFreqDocsEnum(liveDocs, posLen);
+                }
+              } else {
+                docsEnum = new LowFreqDocsEnum(liveDocs, posLen);
+              }
+
+              return docsEnum.reset(postings);
+            } else {
+              LowFreqDocsEnumNoPos docsEnum;
+              if (reuse instanceof LowFreqDocsEnumNoPos) {
+                docsEnum = (LowFreqDocsEnumNoPos) reuse;
+                if (!docsEnum.canReuse(liveDocs)) {
+                  docsEnum = new LowFreqDocsEnumNoPos(liveDocs);
+                }
+              } else {
+                docsEnum = new LowFreqDocsEnumNoPos(liveDocs);
+              }
+
+              return docsEnum.reset(postings);
+            }
+          } else {
+            LowFreqDocsEnumNoTF docsEnum;
+            if (reuse instanceof LowFreqDocsEnumNoTF) {
+              docsEnum = (LowFreqDocsEnumNoTF) reuse;
+              if (!docsEnum.canReuse(liveDocs)) {
+                docsEnum = new LowFreqDocsEnumNoTF(liveDocs);
+              }
+            } else {
+              docsEnum = new LowFreqDocsEnumNoTF(liveDocs);
+            }
+
+            return docsEnum.reset(postings);
+          }
+        } else {
+          final HighFreqTerm term = (HighFreqTerm) terms[termOrd];
+
+          HighFreqDocsEnum docsEnum;
+          if (reuse instanceof HighFreqDocsEnum) {
+            docsEnum = (HighFreqDocsEnum) reuse;
+            if (!docsEnum.canReuse(liveDocs)) {
+              docsEnum = new HighFreqDocsEnum(liveDocs);
+            }
+          } else {
+            docsEnum = new HighFreqDocsEnum(liveDocs);
+          }
+
+          //System.out.println("  DE for term=" + new BytesRef(terms[termOrd].term).utf8ToString() + ": " + term.docIDs.length + " docs");
+          return docsEnum.reset(term.docIDs, term.freqs);
+        }
+      }
+
+      @Override
+      public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum reuse, int flags) {
+        if (!hasPos) {
+          return null;
+        }
+
+        // TODO: implement reuse, something like Pulsing:
+        // it's hairy!
+
+        if (terms[termOrd] instanceof LowFreqTerm) {
+          final LowFreqTerm term = ((LowFreqTerm) terms[termOrd]);
+          final int[] postings = term.postings;
+          final byte[] payloads = term.payloads;
+          return new LowFreqDocsAndPositionsEnum(liveDocs, hasOffsets, hasPayloads).reset(postings, payloads);
+        } else {
+          final HighFreqTerm term = (HighFreqTerm) terms[termOrd];
+          return new HighFreqDocsAndPositionsEnum(liveDocs, hasOffsets).reset(term.docIDs, term.freqs, term.positions, term.payloads);
+        }
+      }
+    }
+
+    private final class DirectIntersectTermsEnum extends TermsEnum {
+      private final RunAutomaton runAutomaton;
+      private final CompiledAutomaton compiledAutomaton;
+      private int termOrd;
+      private final BytesRef scratch = new BytesRef();
+
+      private final class State {
+        int changeOrd;
+        int state;
+        Transition[] transitions;
+        int transitionUpto;
+        int transitionMax;
+        int transitionMin;
+      }
+
+      private State[] states;
+      private int stateUpto;
+
+      public DirectIntersectTermsEnum(CompiledAutomaton compiled, BytesRef startTerm) {
+        runAutomaton = compiled.runAutomaton;
+        compiledAutomaton = compiled;
+        termOrd = -1;
+        states = new State[1];
+        states[0] = new State();
+        states[0].changeOrd = terms.length;
+        states[0].state = runAutomaton.getInitialState();
+        states[0].transitions = compiledAutomaton.sortedTransitions[states[0].state];
+        states[0].transitionUpto = -1;
+        states[0].transitionMax = -1;
+
+        //System.out.println("IE.init startTerm=" + startTerm);
+
+        if (startTerm != null) {
+          int skipUpto = 0;
+          if (startTerm.length == 0) {
+            if (terms.length > 0 && termOffsets[1] == 0) {
+              termOrd = 0;
+            }
+          } else {
+            termOrd++;
+
+            nextLabel:
+            for(int i=0;i<startTerm.length;i++) {
+              final int label = startTerm.bytes[startTerm.offset+i] & 0xFF;
+
+              while (label > states[i].transitionMax) {
+                states[i].transitionUpto++;
+                assert states[i].transitionUpto < states[i].transitions.length;
+                states[i].transitionMin = states[i].transitions[states[i].transitionUpto].getMin();
+                states[i].transitionMax = states[i].transitions[states[i].transitionUpto].getMax();
+                assert states[i].transitionMin >= 0;
+                assert states[i].transitionMin <= 255;
+                assert states[i].transitionMax >= 0;
+                assert states[i].transitionMax <= 255;
+              }
+
+              // Skip forwards until we find a term matching
+              // the label at this position:
+              while (termOrd < terms.length) {
+                final int skipOffset = skipOffsets[termOrd];
+                final int numSkips = skipOffsets[termOrd+1] - skipOffset;
+                final int termOffset = termOffsets[termOrd];
+                final int termLength = termOffsets[1+termOrd] - termOffset;
+
+                // if (DEBUG) {
+                //   System.out.println("  check termOrd=" + termOrd + " term=" + new BytesRef(termBytes, termOffset, termLength).utf8ToString() + " skips=" + Arrays.toString(skips) + " i=" + i);
+                // }
+
+                if (termOrd == states[stateUpto].changeOrd) {
+                  // if (DEBUG) {
+                  //   System.out.println("  end push return");
+                  // }
+                  stateUpto--;
+                  termOrd--;
+                  return;
+                }
+
+                if (termLength == i) {
+                  termOrd++;
+                  skipUpto = 0;
+                  // if (DEBUG) {
+                  //   System.out.println("    term too short; next term");
+                  // }
+                } else if (label < (termBytes[termOffset+i] & 0xFF)) {
+                  termOrd--;
+                  // if (DEBUG) {
+                  //   System.out.println("  no match; already beyond; return termOrd=" + termOrd);
+                  // }
+                  stateUpto -= skipUpto;
+                  assert stateUpto >= 0;
+                  return;
+                } else if (label == (termBytes[termOffset+i] & 0xFF)) {
+                  // if (DEBUG) {
+                  //   System.out.println("    label[" + i + "] matches");
+                  // }
+                  if (skipUpto < numSkips) {
+                    grow();
+
+                    final int nextState = runAutomaton.step(states[stateUpto].state, label);
+
+                    // Automaton is required to accept startTerm:
+                    assert nextState != -1;
+
+                    stateUpto++;
+                    states[stateUpto].changeOrd = skips[skipOffset + skipUpto++];
+                    states[stateUpto].state = nextState;
+                    states[stateUpto].transitions = compiledAutomaton.sortedTransitions[nextState];
+                    states[stateUpto].transitionUpto = -1;
+                    states[stateUpto].transitionMax = -1;
+                    //System.out.println("  push " + states[stateUpto].transitions.length + " trans");
+
+                    // if (DEBUG) {
+                    //   System.out.println("    push skip; changeOrd=" + states[stateUpto].changeOrd);
+                    // }
+
+                    // Match next label at this same term:
+                    continue nextLabel;
+                  } else {
+                    // if (DEBUG) {
+                    //   System.out.println("    linear scan");
+                    // }
+                    // Index exhausted: just scan now (the
+                    // number of scans required will be less
+                    // than the minSkipCount):
+                    final int startTermOrd = termOrd;
+                    while (termOrd < terms.length && compare(termOrd, startTerm) <= 0) {
+                      assert termOrd == startTermOrd || skipOffsets[termOrd] == skipOffsets[termOrd+1];
+                      termOrd++;
+                    }
+                    assert termOrd - startTermOrd < minSkipCount;
+                    termOrd--;
+                    stateUpto -= skipUpto;
+                    // if (DEBUG) {
+                    //   System.out.println("  end termOrd=" + termOrd);
+                    // }
+                    return;
+                  }
+                } else {
+                  if (skipUpto < numSkips) {
+                    termOrd = skips[skipOffset + skipUpto];
+                    // if (DEBUG) {
+                    //   System.out.println("  no match; skip to termOrd=" + termOrd);
+                    // }
+                  } else {
+                    // if (DEBUG) {
+                    //   System.out.println("  no match; next term");
+                    // }
+                    termOrd++;
+                  }
+                  skipUpto = 0;
+                }
+              }
+
+              // startTerm is >= last term so enum will not
+              // return any terms:
+              termOrd--;
+              // if (DEBUG) {
+              //   System.out.println("  beyond end; no terms will match");
+              // }
+              return;
+            }
+          }
+
+          final int termOffset = termOffsets[termOrd];
+          final int termLen = termOffsets[1+termOrd] - termOffset;
+
+          if (termOrd >= 0 && !startTerm.equals(new BytesRef(termBytes, termOffset, termLen))) {
+            stateUpto -= skipUpto;
+            termOrd--;
+          }
+          // if (DEBUG) {
+          //   System.out.println("  loop end; return termOrd=" + termOrd + " stateUpto=" + stateUpto);
+          // }
+        }
+      }
+
+      public Comparator<BytesRef> getComparator() {
+        return BytesRef.getUTF8SortedAsUnicodeComparator();
+      }
+
+      private void grow() {
+        if (states.length == 1+stateUpto) {
+          final State[] newStates = new State[states.length+1];
+          System.arraycopy(states, 0, newStates, 0, states.length);
+          newStates[states.length] = new State();
+          states = newStates;
+        }
+      }
+
+      @Override
+      public BytesRef next() {
+        // if (DEBUG) {
+        //   System.out.println("\nIE.next");
+        // }
+
+        termOrd++;
+        int skipUpto = 0;
+
+        if (termOrd == 0 && termOffsets[1] == 0) {
+          // Special-case empty string:
+          assert stateUpto == 0;
+          // if (DEBUG) {
+          //   System.out.println("  visit empty string");
+          // }
+          if (runAutomaton.isAccept(states[0].state)) {
+            scratch.bytes = termBytes;
+            scratch.offset = 0;
+            scratch.length = 0;
+            return scratch;
+          }
+          termOrd++;
+        }
+
+        nextTerm:
+
+        while (true) {
+          // if (DEBUG) {
+          //   System.out.println("  cycle termOrd=" + termOrd + " stateUpto=" + stateUpto + " skipUpto=" + skipUpto);
+          // }
+          if (termOrd == terms.length) {
+            // if (DEBUG) {
+            //   System.out.println("  return END");
+            // }
+            return null;
+          }
+
+          final State state = states[stateUpto];
+          if (termOrd == state.changeOrd) {
+            // Pop:
+            // if (DEBUG) {
+            //   System.out.println("  pop stateUpto=" + stateUpto);
+            // }
+            stateUpto--;
+            /*
+            if (DEBUG) {
+              try {
+                //System.out.println("    prefix pop " + new BytesRef(terms[termOrd].term, 0, Math.min(stateUpto, terms[termOrd].term.length)).utf8ToString());
+                System.out.println("    prefix pop " + new BytesRef(terms[termOrd].term, 0, Math.min(stateUpto, terms[termOrd].term.length)));
+              } catch (ArrayIndexOutOfBoundsException aioobe) {
+                System.out.println("    prefix pop " + new BytesRef(terms[termOrd].term, 0, Math.min(stateUpto, terms[termOrd].term.length)));
+              }
+            }
+            */
+
+            continue;
+          }
+
+          final int termOffset = termOffsets[termOrd];
+          final int termLength = termOffsets[termOrd+1] - termOffset;
+          final int skipOffset = skipOffsets[termOrd];
+          final int numSkips = skipOffsets[termOrd+1] - skipOffset;
+
+          // if (DEBUG) {
+          //   System.out.println("  term=" + new BytesRef(termBytes, termOffset, termLength).utf8ToString() + " skips=" + Arrays.toString(skips));
+          // }
+        
+          assert termOrd < state.changeOrd;
+
+          assert stateUpto <= termLength: "term.length=" + termLength + "; stateUpto=" + stateUpto;
+          final int label = termBytes[termOffset+stateUpto] & 0xFF;
+
+          while (label > state.transitionMax) {
+            //System.out.println("  label=" + label + " vs max=" + state.transitionMax + " transUpto=" + state.transitionUpto + " vs " + state.transitions.length);
+            state.transitionUpto++;
+            if (state.transitionUpto == state.transitions.length) {
+              // We've exhausted transitions leaving this
+              // state; force pop+next/skip now:
+              //System.out.println("forcepop: stateUpto=" + stateUpto);
+              if (stateUpto == 0) {
+                termOrd = terms.length;
+                return null;
+              } else {
+                assert state.changeOrd > termOrd;
+                // if (DEBUG) {
+                //   System.out.println("  jumpend " + (state.changeOrd - termOrd));
+                // }
+                //System.out.println("  jump to termOrd=" + states[stateUpto].changeOrd + " vs " + termOrd);
+                termOrd = states[stateUpto].changeOrd;
+                skipUpto = 0;
+                stateUpto--;
+              }
+              continue nextTerm;
+            }
+            assert state.transitionUpto < state.transitions.length: " state.transitionUpto=" + state.transitionUpto + " vs " + state.transitions.length;
+            state.transitionMin = state.transitions[state.transitionUpto].getMin();
+            state.transitionMax = state.transitions[state.transitionUpto].getMax();
+            assert state.transitionMin >= 0;
+            assert state.transitionMin <= 255;
+            assert state.transitionMax >= 0;
+            assert state.transitionMax <= 255;
+          }
+
+          /*
+          if (DEBUG) {
+            System.out.println("    check ord=" + termOrd + " term[" + stateUpto + "]=" + (char) label + "(" + label + ") term=" + new BytesRef(terms[termOrd].term).utf8ToString() + " trans " +
+                               (char) state.transitionMin + "(" + state.transitionMin + ")" + "-" + (char) state.transitionMax + "(" + state.transitionMax + ") nextChange=+" + (state.changeOrd - termOrd) + " skips=" + (skips == null ? "null" : Arrays.toString(skips)));
+            System.out.println("    check ord=" + termOrd + " term[" + stateUpto + "]=" + Integer.toHexString(label) + "(" + label + ") term=" + new BytesRef(termBytes, termOffset, termLength) + " trans " +
+                               Integer.toHexString(state.transitionMin) + "(" + state.transitionMin + ")" + "-" + Integer.toHexString(state.transitionMax) + "(" + state.transitionMax + ") nextChange=+" + (state.changeOrd - termOrd) + " skips=" + (skips == null ? "null" : Arrays.toString(skips)));
+          }
+          */
+
+          final int targetLabel = state.transitionMin;
+
+          if ((termBytes[termOffset+stateUpto] & 0xFF) < targetLabel) {
+            // if (DEBUG) {
+            //   System.out.println("    do bin search");
+            // }
+            //int startTermOrd = termOrd;
+            int low = termOrd+1;
+            int high = state.changeOrd-1;
+            while (true) {
+              if (low > high) {
+                // Label not found
+                termOrd = low;
+                // if (DEBUG) {
+                //   System.out.println("      advanced by " + (termOrd - startTermOrd));
+                // }
+                //System.out.println("  jump " + (termOrd - startTermOrd));
+                skipUpto = 0;
+                continue nextTerm;
+              }
+              int mid = (low + high) >>> 1;
+              int cmp = (termBytes[termOffsets[mid] + stateUpto] & 0xFF) - targetLabel;
+              // if (DEBUG) {
+              //   System.out.println("      bin: check label=" + (char) (termBytes[termOffsets[low] + stateUpto] & 0xFF) + " ord=" + mid);
+              // }
+              if (cmp < 0) {
+                low = mid+1;
+              } else if (cmp > 0) {
+                high = mid - 1;
+              } else {
+                // Label found; walk backwards to first
+                // occurrence:
+                while (mid > termOrd && (termBytes[termOffsets[mid-1] + stateUpto] & 0xFF) == targetLabel) {
+                  mid--;
+                }
+                termOrd = mid;
+                // if (DEBUG) {
+                //   System.out.println("      advanced by " + (termOrd - startTermOrd));
+                // }
+                //System.out.println("  jump " + (termOrd - startTermOrd));
+                skipUpto = 0;
+                continue nextTerm;
+              }
+            }
+          }
+
+          int nextState = runAutomaton.step(states[stateUpto].state, label);
+
+          if (nextState == -1) {
+            // Skip
+            // if (DEBUG) {
+            //   System.out.println("  automaton doesn't accept; skip");
+            // }
+            if (skipUpto < numSkips) {
+              // if (DEBUG) {
+              //   System.out.println("  jump " + (skips[skipOffset+skipUpto]-1 - termOrd));
+              // }
+              termOrd = skips[skipOffset+skipUpto];
+            } else {
+              termOrd++;
+            }
+            skipUpto = 0;
+          } else if (skipUpto < numSkips) {
+            // Push:
+            // if (DEBUG) {
+            //   System.out.println("  push");
+            // }
+            /*
+            if (DEBUG) {
+              try {
+                //System.out.println("    prefix push " + new BytesRef(term, 0, stateUpto+1).utf8ToString());
+                System.out.println("    prefix push " + new BytesRef(term, 0, stateUpto+1));
+              } catch (ArrayIndexOutOfBoundsException aioobe) {
+                System.out.println("    prefix push " + new BytesRef(term, 0, stateUpto+1));
+              }
+            }
+            */
+
+            grow();
+            stateUpto++;
+            states[stateUpto].state = nextState;
+            states[stateUpto].changeOrd = skips[skipOffset + skipUpto++];
+            states[stateUpto].transitions = compiledAutomaton.sortedTransitions[nextState];
+            states[stateUpto].transitionUpto = -1;
+            states[stateUpto].transitionMax = -1;
+            
+            if (stateUpto == termLength) {
+              // if (DEBUG) {
+              //   System.out.println("  term ends after push");
+              // }
+              if (runAutomaton.isAccept(nextState)) {
+                // if (DEBUG) {
+                //   System.out.println("  automaton accepts: return");
+                // }
+                scratch.bytes = termBytes;
+                scratch.offset = termOffsets[termOrd];
+                scratch.length = termOffsets[1+termOrd] - scratch.offset;
+                // if (DEBUG) {
+                //   System.out.println("  ret " + scratch.utf8ToString());
+                // }
+                return scratch;
+              } else {
+                // if (DEBUG) {
+                //   System.out.println("  automaton rejects: nextTerm");
+                // }
+                termOrd++;
+                skipUpto = 0;
+              }
+            }
+          } else {
+            // Run the non-indexed tail of this term:
+
+            // TODO: add assert that we don't inc too many times
+
+            if (compiledAutomaton.commonSuffixRef != null) {
+              //System.out.println("suffix " + compiledAutomaton.commonSuffixRef.utf8ToString());
+              assert compiledAutomaton.commonSuffixRef.offset == 0;
+              if (termLength < compiledAutomaton.commonSuffixRef.length) {
+                termOrd++;
+                skipUpto = 0;
+                continue nextTerm;
+              }
+              int offset = termOffset + termLength - compiledAutomaton.commonSuffixRef.length;
+              for(int suffix=0;suffix<compiledAutomaton.commonSuffixRef.length;suffix++) {
+                if (termBytes[offset + suffix] != compiledAutomaton.commonSuffixRef.bytes[suffix]) {
+                  termOrd++;
+                  skipUpto = 0;
+                  continue nextTerm;
+                }
+              }
+            }
+
+            int upto = stateUpto+1;
+            while (upto < termLength) {
+              nextState = runAutomaton.step(nextState, termBytes[termOffset+upto] & 0xFF);
+              if (nextState == -1) {
+                termOrd++;
+                skipUpto = 0;
+                // if (DEBUG) {
+                //   System.out.println("  nomatch tail; next term");
+                // }
+                continue nextTerm;
+              }
+              upto++;
+            }
+
+            if (runAutomaton.isAccept(nextState)) {
+              scratch.bytes = termBytes;
+              scratch.offset = termOffsets[termOrd];
+              scratch.length = termOffsets[1+termOrd] - scratch.offset;
+              // if (DEBUG) {
+              //   System.out.println("  match tail; return " + scratch.utf8ToString());
+              //   System.out.println("  ret2 " + scratch.utf8ToString());
+              // }
+              return scratch;
+            } else {
+              termOrd++;
+              skipUpto = 0;
+              // if (DEBUG) {
+              //   System.out.println("  nomatch tail; next term");
+              // }
+            }
+          }
+        }
+      }
+
+      @Override
+      public TermState termState() {
+        OrdTermState state = new OrdTermState();
+        state.ord = termOrd;
+        return state;
+      }
+
+      @Override
+      public BytesRef term() {
+        return scratch;
+      }
+
+      @Override
+      public long ord() {
+        return termOrd;
+      }
+
+      @Override
+      public int docFreq() {
+        if (terms[termOrd] instanceof LowFreqTerm) {
+          return ((LowFreqTerm) terms[termOrd]).docFreq;
+        } else {
+          return ((HighFreqTerm) terms[termOrd]).docIDs.length;
+        }
+      }
+
+      @Override
+      public long totalTermFreq() {
+        if (terms[termOrd] instanceof LowFreqTerm) {
+          return ((LowFreqTerm) terms[termOrd]).totalTermFreq;
+        } else {
+          return ((HighFreqTerm) terms[termOrd]).totalTermFreq;
+        }
+      }
+
+      @Override
+      public DocsEnum docs(Bits liveDocs, DocsEnum reuse, int flags) {
+        // TODO: implement reuse, something like Pulsing:
+        // it's hairy!
+
+        if (terms[termOrd] instanceof LowFreqTerm) {
+          final int[] postings = ((LowFreqTerm) terms[termOrd]).postings;
+          if (hasFreq) {
+            if (hasPos) {
+              int posLen;
+              if (hasOffsets) {
+                posLen = 3;
+              } else {
+                posLen = 1;
+              }
+              if (hasPayloads) {
+                posLen++;
+              }
+              return new LowFreqDocsEnum(liveDocs, posLen).reset(postings);
+            } else {
+              return new LowFreqDocsEnumNoPos(liveDocs).reset(postings);
+            }
+          } else {
+            return new LowFreqDocsEnumNoTF(liveDocs).reset(postings);
+          }
+        } else {
+          final HighFreqTerm term = (HighFreqTerm) terms[termOrd];
+          //  System.out.println("DE for term=" + new BytesRef(terms[termOrd].term).utf8ToString() + ": " + term.docIDs.length + " docs");
+          return new HighFreqDocsEnum(liveDocs).reset(term.docIDs, term.freqs);
+        }
+      }
+
+      @Override
+      public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum reuse, int flags) {
+        if (!hasPos) {
+          return null;
+        }
+
+        // TODO: implement reuse, something like Pulsing:
+        // it's hairy!
+
+        if (terms[termOrd] instanceof LowFreqTerm) {
+          final LowFreqTerm term = ((LowFreqTerm) terms[termOrd]);
+          final int[] postings = term.postings;
+          final byte[] payloads = term.payloads;
+          return new LowFreqDocsAndPositionsEnum(liveDocs, hasOffsets, hasPayloads).reset(postings, payloads);
+        } else {
+          final HighFreqTerm term = (HighFreqTerm) terms[termOrd];
+          return new HighFreqDocsAndPositionsEnum(liveDocs, hasOffsets).reset(term.docIDs, term.freqs, term.positions, term.payloads);
+        }
+      }
+
+      @Override
+      public SeekStatus seekCeil(BytesRef term, boolean useCache) {
+        throw new UnsupportedOperationException();
+      }
+
+      @Override
+      public void seekExact(long ord) {
+        throw new UnsupportedOperationException();
+      }
+    }
+  }
+
+  // Docs only:
+  private final static class LowFreqDocsEnumNoTF extends DocsEnum {
+    private int[] postings;
+    private final Bits liveDocs;
+    private int upto;
+
+    public LowFreqDocsEnumNoTF(Bits liveDocs) {
+      this.liveDocs = liveDocs;
+    }
+
+    public boolean canReuse(Bits liveDocs) {
+      return liveDocs == this.liveDocs;
+    }
+
+    public DocsEnum reset(int[] postings) {
+      this.postings = postings;
+      upto = -1;
+      return this;
+    }
+
+    // TODO: can do this w/o setting members?
+
+    @Override
+    public int nextDoc() {
+      upto++;
+      if (liveDocs == null) {
+        if (upto < postings.length) {
+          return postings[upto];
+        }
+      } else {
+        while (upto < postings.length) {
+          if (liveDocs.get(postings[upto])) {
+            return postings[upto];
+          }
+          upto++;
+        }
+      }
+      return NO_MORE_DOCS;
+    }
+
+    @Override
+    public int docID() {
+      if (upto < 0) {
+        return -1;
+      } else if (upto < postings.length) {
+        return postings[upto];
+      } else {
+        return NO_MORE_DOCS;
+      }
+    }
+
+    @Override
+    public int freq() {
+      return 1;
+    }
+
+    @Override
+    public int advance(int target) {
+      // Linear scan, but this is low-freq term so it won't
+      // be costly:
+      while(nextDoc() < target) {
+      }
+      return docID();
+    }
+  }
+
+  // Docs + freqs:
+  private final static class LowFreqDocsEnumNoPos extends DocsEnum {
+    private int[] postings;
+    private final Bits liveDocs;
+    private int upto;
+
+    public LowFreqDocsEnumNoPos(Bits liveDocs) {
+      this.liveDocs = liveDocs;
+    }
+
+    public boolean canReuse(Bits liveDocs) {
+      return liveDocs == this.liveDocs;
+    }
+
+    public DocsEnum reset(int[] postings) {
+      this.postings = postings;
+      upto = -2;
+      return this;
+    }
+
+    // TODO: can do this w/o setting members?
+    @Override
+    public int nextDoc() {
+      upto += 2;
+      if (liveDocs == null) {
+        if (upto < postings.length) {
+          return postings[upto];
+        }
+      } else {
+        while (upto < postings.length) {
+          if (liveDocs.get(postings[upto])) {
+            return postings[upto];
+          }
+          upto += 2;
+        }
+      }
+      return NO_MORE_DOCS;
+    }
+
+    @Override
+    public int docID() {
+      if (upto < 0) {
+        return -1;
+      } else if (upto < postings.length) {
+        return postings[upto];
+      } else {
+        return NO_MORE_DOCS;
+      }
+    }
+
+    @Override
+    public int freq() {
+      return postings[upto+1];
+    }
+
+    @Override
+    public int advance(int target) {
+      // Linear scan, but this is low-freq term so it won't
+      // be costly:
+      while(nextDoc() < target) {
+      }
+      return docID();
+    }
+  }
+
+  // Docs + freqs + positions/offets:
+  private final static class LowFreqDocsEnum extends DocsEnum {
+    private int[] postings;
+    private final Bits liveDocs;
+    private final int posMult;
+    private int upto;
+    private int freq;
+
+    public LowFreqDocsEnum(Bits liveDocs, int posMult) {
+      this.liveDocs = liveDocs;
+      this.posMult = posMult;
+      // if (DEBUG) {
+      //   System.out.println("LowFreqDE: posMult=" + posMult);
+      // }
+    }
+
+    public boolean canReuse(Bits liveDocs, int posMult) {
+      return liveDocs == this.liveDocs && posMult == this.posMult;
+    }
+
+    public DocsEnum reset(int[] postings) {
+      this.postings = postings;
+      upto = -2;
+      freq = 0;
+      return this;
+    }
+
+    // TODO: can do this w/o setting members?
+    @Override
+    public int nextDoc() {
+      upto += 2 + freq*posMult;
+      // if (DEBUG) {
+      //   System.out.println("  nextDoc freq=" + freq + " upto=" + upto + " vs " + postings.length);
+      // }
+      if (liveDocs == null) {
+        if (upto < postings.length) {   
+          freq = postings[upto+1];
+          assert freq > 0;
+          return postings[upto];
+        }
+      } else {
+        while (upto < postings.length) {
+          freq = postings[upto+1];
+          assert freq > 0;
+          if (liveDocs.get(postings[upto])) {
+            return postings[upto];
+          }
+          upto += 2 + freq*posMult;
+        }
+      }
+      return NO_MORE_DOCS;
+    }
+
+    @Override
+    public int docID() {
+      // TODO: store docID member?
+      if (upto < 0) {
+        return -1;
+      } else if (upto < postings.length) {
+        return postings[upto];
+      } else {
+        return NO_MORE_DOCS;
+      }
+    }
+
+    @Override
+    public int freq() {
+      // TODO: can I do postings[upto+1]?
+      return freq;
+    }
+
+    @Override
+    public int advance(int target) {
+      // Linear scan, but this is low-freq term so it won't
+      // be costly:
+      while(nextDoc() < target) {
+      }
+      return docID();
+    }
+  }
+
+  private final static class LowFreqDocsAndPositionsEnum extends DocsAndPositionsEnum {
+    private int[] postings;
+    private final Bits liveDocs;
+    private final int posMult;
+    private final boolean hasOffsets;
+    private final boolean hasPayloads;
+    private final BytesRef payload = new BytesRef();
+    private int upto;
+    private int docID;
+    private int freq;
+    private int skipPositions;
+    private int startOffset;
+    private int endOffset;
+    private int lastPayloadOffset;
+    private int payloadOffset;
+    private int payloadLength;
+    private byte[] payloadBytes;
+
+    public LowFreqDocsAndPositionsEnum(Bits liveDocs, boolean hasOffsets, boolean hasPayloads) {
+      this.liveDocs = liveDocs;
+      this.hasOffsets = hasOffsets;
+      this.hasPayloads = hasPayloads;
+      if (hasOffsets) {
+        if (hasPayloads) {
+          posMult = 4;
+        } else {
+          posMult = 3;
+        }
+      } else if (hasPayloads) {
+        posMult = 2;
+      } else {
+        posMult = 1;
+      }
+    }
+
+    public DocsAndPositionsEnum reset(int[] postings, byte[] payloadBytes) {
+      this.postings = postings;
+      upto = 0;
+      skipPositions = 0;
+      startOffset = -1;
+      endOffset = -1;
+      docID = -1;
+      payloadLength = 0;
+      this.payloadBytes = payloadBytes;
+      return this;
+    }
+
+    @Override
+    public int nextDoc() {
+      if (hasPayloads) {
+        for(int i=0;i<skipPositions;i++) {
+          upto++;
+          if (hasOffsets) {
+            upto += 2;
+          }
+          payloadOffset += postings[upto++];
+        }
+      } else {
+        upto += posMult * skipPositions;
+      }
+
+      if (liveDocs == null) {
+        if (upto < postings.length) {
+          docID = postings[upto++];
+          freq = postings[upto++];
+          skipPositions = freq;
+          return docID;
+        }
+      } else {
+        while(upto < postings.length) {
+          docID = postings[upto++];
+          freq = postings[upto++];
+          if (liveDocs.get(docID)) {
+            skipPositions = freq;
+            return docID;
+          }
+          if (hasPayloads) {
+            for(int i=0;i<freq;i++) {
+              upto++;
+              if (hasOffsets) {
+                upto += 2;
+              }
+              payloadOffset += postings[upto++];
+            }
+          } else {
+            upto += posMult * freq;
+          }
+        }
+      }
+
+      return docID = NO_MORE_DOCS;
+    }
+
+    @Override
+    public int docID() {
+      return docID;
+    }
+
+    @Override
+    public int freq() {
+      return freq;
+    }
+
+    @Override
+    public int nextPosition() {
+      assert skipPositions > 0;
+      skipPositions--;
+      final int pos = postings[upto++];
+      if (hasOffsets) {
+        startOffset = postings[upto++];
+        endOffset = postings[upto++];
+      }
+      if (hasPayloads) {
+        payloadLength = postings[upto++];
+        lastPayloadOffset = payloadOffset;
+        payloadOffset += payloadLength;
+      }
+      return pos;
+    }
+
+    @Override
+    public int startOffset() {
+      return startOffset;
+    }
+
+    @Override
+    public int endOffset() {
+      return endOffset;
+    }
+
+    @Override
+    public int advance(int target) {
+      // Linear scan, but this is low-freq term so it won't
+      // be costly:
+      while (nextDoc() < target) {
+      }
+      return docID;
+    }
+
+    @Override
+    public BytesRef getPayload() {
+      if (payloadLength > 0) {
+        payload.bytes = payloadBytes;
+        payload.offset = lastPayloadOffset;
+        payload.length = payloadLength;
+        return payload;
+      } else {
+        return null;
+      }
+    }
+  }
+
+  // Docs + freqs:
+  private final static class HighFreqDocsEnum extends DocsEnum {
+    private int[] docIDs;
+    private int[] freqs;
+    private final Bits liveDocs;
+    private int upto;
+    private int docID = -1;
+
+    public HighFreqDocsEnum(Bits liveDocs) {
+      this.liveDocs = liveDocs;
+    }
+
+    public boolean canReuse(Bits liveDocs) {
+      return liveDocs == this.liveDocs;
+    }
+
+    public int[] getDocIDs() {
+      return docIDs;
+    }
+
+    public int[] getFreqs() {
+      return freqs;
+    }
+
+    public DocsEnum reset(int[] docIDs, int[] freqs) {
+      this.docIDs = docIDs;
+      this.freqs = freqs;
+      docID = upto = -1;
+      return this;
+    }
+
+    @Override
+    public int nextDoc() {
+      upto++;
+      if (liveDocs == null) {
+        try {
+          return docID = docIDs[upto];
+        } catch (ArrayIndexOutOfBoundsException e) {
+        }
+      } else {
+        while (upto < docIDs.length) {
+          if (liveDocs.get(docIDs[upto])) {
+            return docID = docIDs[upto];
+          }
+          upto++;
+        }
+      }
+      return docID = NO_MORE_DOCS;
+    }
+
+    @Override
+    public int docID() {
+      return docID;
+    }
+
+    @Override
+    public int freq() {
+      if (freqs == null) {
+        return 1;
+      } else {
+        return freqs[upto];
+      }
+    }
+
+    @Override
+    public int advance(int target) {
+      /*
+      upto++;
+      if (upto == docIDs.length) {
+        return docID = NO_MORE_DOCS;
+      }
+      final int index = Arrays.binarySearch(docIDs, upto, docIDs.length, target);
+      if (index < 0) {
+        upto = -index - 1;
+      } else {
+        upto = index;
+      }
+      if (liveDocs != null) {
+        while (upto < docIDs.length) {
+          if (liveDocs.get(docIDs[upto])) {
+            break;
+          }
+          upto++;
+        }
+      }
+      if (upto == docIDs.length) {
+        return NO_MORE_DOCS;
+      } else {
+        return docID = docIDs[upto];
+      }
+      */
+
+      //System.out.println("  advance target=" + target + " cur=" + docID() + " upto=" + upto + " of " + docIDs.length);
+      // if (DEBUG) {
+      //   System.out.println("advance target=" + target + " len=" + docIDs.length);
+      // }
+      upto++;
+      if (upto == docIDs.length) {
+        return docID = NO_MORE_DOCS;
+      }
+
+      // First "grow" outwards, since most advances are to
+      // nearby docs:
+      int inc = 10;
+      int nextUpto = upto+10;
+      int low;
+      int high;
+      while (true) {
+        //System.out.println("  grow nextUpto=" + nextUpto + " inc=" + inc);
+        if (nextUpto >= docIDs.length) {
+          low = nextUpto-inc;
+          high = docIDs.length-1;
+          break;
+        }
+        //System.out.println("    docID=" + docIDs[nextUpto]);
+
+        if (target <= docIDs[nextUpto]) {
+          low = nextUpto-inc;
+          high = nextUpto;
+          break;
+        }
+        inc *= 2;
+        nextUpto += inc;
+      }
+
+      // Now do normal binary search
+      //System.out.println("    after fwd: low=" + low + " high=" + high);
+
+      while (true) {
+
+        if (low > high) {
+          // Not exactly found
+          //System.out.println("    break: no match");
+          upto = low;
+          break;
+        }
+
+        int mid = (low + high) >>> 1;
+        int cmp = docIDs[mid] - target;
+        //System.out.println("    bsearch low=" + low + " high=" + high+ ": docIDs[" + mid + "]=" + docIDs[mid]);
+
+        if (cmp < 0) {
+          low = mid + 1;
+        } else if (cmp > 0) {
+          high = mid - 1;
+        } else {
+          // Found target
+          upto = mid;
+          //System.out.println("    break: match");
+          break;
+        }
+      }
+
+      //System.out.println("    end upto=" + upto + " docID=" + (upto >= docIDs.length ? NO_MORE_DOCS : docIDs[upto]));
+
+      if (liveDocs != null) {
+        while (upto < docIDs.length) {
+          if (liveDocs.get(docIDs[upto])) {
+            break;
+          }
+          upto++;
+        }
+      }
+      if (upto == docIDs.length) {
+        //System.out.println("    return END");
+        return docID = NO_MORE_DOCS;
+      } else {
+        //System.out.println("    return docID=" + docIDs[upto] + " upto=" + upto);
+        return docID = docIDs[upto];
+      }
+    }
+  }
+
+  // TODO: specialize offsets and not
+  private final static class HighFreqDocsAndPositionsEnum extends DocsAndPositionsEnum {
+    private int[] docIDs;
+    private int[] freqs;
+    private int[][] positions;
+    private byte[][][] payloads;
+    private final Bits liveDocs;
+    private final boolean hasOffsets;
+    private final int posJump;
+    private int upto;
+    private int docID = -1;
+    private int posUpto;
+    private int[] curPositions;
+
+    public HighFreqDocsAndPositionsEnum(Bits liveDocs, boolean hasOffsets) {
+      this.liveDocs = liveDocs;
+      this.hasOffsets = hasOffsets;
+      posJump = hasOffsets ? 3 : 1;
+    }
+
+    public int[] getDocIDs() {
+      return docIDs;
+    }
+
+    public int[][] getPositions() {
+      return positions;
+    }
+
+    public int getPosJump() {
+      return posJump;
+    }
+
+    public Bits getLiveDocs() {
+      return liveDocs;
+    }
+
+    public DocsAndPositionsEnum reset(int[] docIDs, int[] freqs, int[][] positions, byte[][][] payloads) {
+      this.docIDs = docIDs;
+      this.freqs = freqs;
+      this.positions = positions;
+      this.payloads = payloads;
+      upto = -1;
+      return this;
+    }
+
+    @Override
+    public int nextDoc() {
+      upto++;
+      if (liveDocs == null) {
+        if (upto < docIDs.length) {
+          posUpto = -posJump;   
+          curPositions = positions[upto];
+          return docID = docIDs[upto];
+        }
+      } else {
+        while (upto < docIDs.length) {
+          if (liveDocs.get(docIDs[upto])) {
+            posUpto = -posJump;
+            curPositions = positions[upto];
+            return docID = docIDs[upto];
+          }
+          upto++;
+        }
+      }
+
+      return docID = NO_MORE_DOCS;
+    }
+
+    @Override
+    public int freq() {
+      return freqs[upto];
+    }
+
+    @Override
+    public int docID() {
+      return docID;
+    }
+
+    @Override
+    public int nextPosition() {
+      posUpto += posJump;
+      return curPositions[posUpto];
+    }
+
+    @Override
+    public int startOffset() {
+      if (hasOffsets) {
+        return curPositions[posUpto+1];
+      } else {
+        return -1;
+      }
+    }
+
+    @Override
+    public int endOffset() {
+      if (hasOffsets) {
+        return curPositions[posUpto+2];
+      } else {
+        return -1;
+      }
+    }
+
+    @Override
+    public int advance(int target) {
+
+      /*
+      upto++;
+      if (upto == docIDs.length) {
+        return NO_MORE_DOCS;
+      }
+      final int index = Arrays.binarySearch(docIDs, upto, docIDs.length, target);
+      if (index < 0) {
+        upto = -index - 1;
+      } else {
+        upto = index;
+      }
+      if (liveDocs != null) {
+        while (upto < docIDs.length) {
+          if (liveDocs.get(docIDs[upto])) {
+            break;
+          }
+          upto++;
+        }
+      }
+      posUpto = hasOffsets ? -3 : -1;
+      if (upto == docIDs.length) {
+        return NO_MORE_DOCS;
+      } else {
+        return docID();
+      }
+      */
+
+      //System.out.println("  advance target=" + target + " cur=" + docID() + " upto=" + upto + " of " + docIDs.length);
+      // if (DEBUG) {
+      //   System.out.println("advance target=" + target + " len=" + docIDs.length);
+      // }
+      upto++;
+      if (upto == docIDs.length) {
+        return docID = NO_MORE_DOCS;
+      }
+
+      // First "grow" outwards, since most advances are to
+      // nearby docs:
+      int inc = 10;
+      int nextUpto = upto+10;
+      int low;
+      int high;
+      while (true) {
+        //System.out.println("  grow nextUpto=" + nextUpto + " inc=" + inc);
+        if (nextUpto >= docIDs.length) {
+          low = nextUpto-inc;
+          high = docIDs.length-1;
+          break;
+        }
+        //System.out.println("    docID=" + docIDs[nextUpto]);
+
+        if (target <= docIDs[nextUpto]) {
+          low = nextUpto-inc;
+          high = nextUpto;
+          break;
+        }
+        inc *= 2;
+        nextUpto += inc;
+      }
+
+      // Now do normal binary search
+      //System.out.println("    after fwd: low=" + low + " high=" + high);
+
+      while (true) {
+
+        if (low > high) {
+          // Not exactly found
+          //System.out.println("    break: no match");
+          upto = low;
+          break;
+        }
+
+        int mid = (low + high) >>> 1;
+        int cmp = docIDs[mid] - target;
+        //System.out.println("    bsearch low=" + low + " high=" + high+ ": docIDs[" + mid + "]=" + docIDs[mid]);
+
+        if (cmp < 0) {
+          low = mid + 1;
+        } else if (cmp > 0) {
+          high = mid - 1;
+        } else {
+          // Found target
+          upto = mid;
+          //System.out.println("    break: match");
+          break;
+        }
+      }
+
+      //System.out.println("    end upto=" + upto + " docID=" + (upto >= docIDs.length ? NO_MORE_DOCS : docIDs[upto]));
+
+      if (liveDocs != null) {
+        while (upto < docIDs.length) {
+          if (liveDocs.get(docIDs[upto])) {
+            break;
+          }
+          upto++;
+        }
+      }
+      if (upto == docIDs.length) {
+        //System.out.println("    return END");
+        return docID = NO_MORE_DOCS;
+      } else {
+        //System.out.println("    return docID=" + docIDs[upto] + " upto=" + upto);
+        posUpto = -posJump;
+        curPositions = positions[upto];
+        return docID = docIDs[upto];
+      }
+    }
+
+    private final BytesRef payload = new BytesRef();
+
+    @Override
+    public BytesRef getPayload() {
+      if (payloads == null) {
+        return null;
+      } else {
+        final byte[] payloadBytes = payloads[upto][posUpto/(hasOffsets ? 3:1)];
+        if (payloadBytes == null) {
+          return null;
+        }
+        payload.bytes = payloadBytes;
+        payload.length = payloadBytes.length;
+        payload.offset = 0;
+        return payload;
+      }
+    }
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryPostingsFormat.java b/lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryPostingsFormat.java
new file mode 100644
index 0000000..6bf8bc8
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/memory/MemoryPostingsFormat.java
@@ -0,0 +1,888 @@
+package org.apache.lucene.codecs.memory;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Collections;
+import java.util.Comparator;
+import java.util.Iterator;
+import java.util.SortedMap;
+import java.util.TreeMap;
+
+import org.apache.lucene.codecs.FieldsConsumer;
+import org.apache.lucene.codecs.FieldsProducer;
+import org.apache.lucene.codecs.PostingsConsumer;
+import org.apache.lucene.codecs.PostingsFormat;
+import org.apache.lucene.codecs.TermStats;
+import org.apache.lucene.codecs.TermsConsumer;
+import org.apache.lucene.index.DocsAndPositionsEnum;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.index.Terms;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.store.ByteArrayDataInput;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.store.RAMOutputStream;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IntsRef;
+import org.apache.lucene.util.fst.Builder;
+import org.apache.lucene.util.fst.ByteSequenceOutputs;
+import org.apache.lucene.util.fst.BytesRefFSTEnum;
+import org.apache.lucene.util.fst.FST;
+import org.apache.lucene.util.fst.Util;
+import org.apache.lucene.util.packed.PackedInts;
+
+// TODO: would be nice to somehow allow this to act like
+// InstantiatedIndex, by never writing to disk; ie you write
+// to this Codec in RAM only and then when you open a reader
+// it pulls the FST directly from what you wrote w/o going
+// to disk.
+
+/** Stores terms & postings (docs, positions, payloads) in
+ *  RAM, using an FST.
+ *
+ * <p>Note that this codec implements advance as a linear
+ * scan!  This means if you store large fields in here,
+ * queries that rely on advance will (AND BooleanQuery,
+ * PhraseQuery) will be relatively slow!
+ *
+ * <p><b>NOTE</b>: this codec cannot address more than ~2.1 GB
+ * of postings, because the underlying FST uses an int
+ * to address the underlying byte[].
+ *
+ * @lucene.experimental */
+
+// TODO: Maybe name this 'Cached' or something to reflect
+// the reality that it is actually written to disk, but
+// loads itself in ram?
+public class MemoryPostingsFormat extends PostingsFormat {
+
+  private final boolean doPackFST;
+  private final float acceptableOverheadRatio;
+
+  public MemoryPostingsFormat() {
+    this(false, PackedInts.DEFAULT);
+  }
+
+  public MemoryPostingsFormat(boolean doPackFST, float acceptableOverheadRatio) {
+    super("Memory");
+    this.doPackFST = doPackFST;
+    this.acceptableOverheadRatio = acceptableOverheadRatio;
+  }
+  
+  @Override
+  public String toString() {
+    return "PostingsFormat(name=" + getName() + " doPackFST= " + doPackFST + ")";
+  }
+
+  private final static class TermsWriter extends TermsConsumer {
+    private final IndexOutput out;
+    private final FieldInfo field;
+    private final Builder<BytesRef> builder;
+    private final ByteSequenceOutputs outputs = ByteSequenceOutputs.getSingleton();
+    private final boolean doPackFST;
+    private final float acceptableOverheadRatio;
+    private int termCount;
+
+    public TermsWriter(IndexOutput out, FieldInfo field, boolean doPackFST, float acceptableOverheadRatio) {
+      this.out = out;
+      this.field = field;
+      this.doPackFST = doPackFST;
+      this.acceptableOverheadRatio = acceptableOverheadRatio;
+      builder = new Builder<BytesRef>(FST.INPUT_TYPE.BYTE1, 0, 0, true, true, Integer.MAX_VALUE, outputs, null, doPackFST, acceptableOverheadRatio);
+    }
+
+    private class PostingsWriter extends PostingsConsumer {
+      private int lastDocID;
+      private int lastPos;
+      private int lastPayloadLen;
+
+      // NOTE: not private so we don't pay access check at runtime:
+      int docCount;
+      RAMOutputStream buffer = new RAMOutputStream();
+      
+      int lastOffsetLength;
+      int lastOffset;
+
+      @Override
+      public void startDoc(int docID, int termDocFreq) throws IOException {
+        //System.out.println("    startDoc docID=" + docID + " freq=" + termDocFreq);
+        final int delta = docID - lastDocID;
+        assert docID == 0 || delta > 0;
+        lastDocID = docID;
+        docCount++;
+
+        if (field.getIndexOptions() == IndexOptions.DOCS_ONLY) {
+          buffer.writeVInt(delta);
+        } else if (termDocFreq == 1) {
+          buffer.writeVInt((delta<<1) | 1);
+        } else {
+          buffer.writeVInt(delta<<1);
+          assert termDocFreq > 0;
+          buffer.writeVInt(termDocFreq);
+        }
+
+        lastPos = 0;
+        lastOffset = 0;
+      }
+
+      @Override
+      public void addPosition(int pos, BytesRef payload, int startOffset, int endOffset) throws IOException {
+        assert payload == null || field.hasPayloads();
+
+        //System.out.println("      addPos pos=" + pos + " payload=" + payload);
+
+        final int delta = pos - lastPos;
+        assert delta >= 0;
+        lastPos = pos;
+        
+        int payloadLen = 0;
+        
+        if (field.hasPayloads()) {
+          payloadLen = payload == null ? 0 : payload.length;
+          if (payloadLen != lastPayloadLen) {
+            lastPayloadLen = payloadLen;
+            buffer.writeVInt((delta<<1)|1);
+            buffer.writeVInt(payloadLen);
+          } else {
+            buffer.writeVInt(delta<<1);
+          }
+        } else {
+          buffer.writeVInt(delta);
+        }
+        
+        if (field.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0) {
+          // don't use startOffset - lastEndOffset, because this creates lots of negative vints for synonyms,
+          // and the numbers aren't that much smaller anyways.
+          int offsetDelta = startOffset - lastOffset;
+          int offsetLength = endOffset - startOffset;
+          if (offsetLength != lastOffsetLength) {
+            buffer.writeVInt(offsetDelta << 1 | 1);
+            buffer.writeVInt(offsetLength);
+          } else {
+            buffer.writeVInt(offsetDelta << 1);
+          }
+          lastOffset = startOffset;
+          lastOffsetLength = offsetLength;
+        }
+        
+        if (payloadLen > 0) {
+          buffer.writeBytes(payload.bytes, payload.offset, payloadLen);
+        }
+      }
+
+      @Override
+      public void finishDoc() {
+      }
+
+      public PostingsWriter reset() {
+        assert buffer.getFilePointer() == 0;
+        lastDocID = 0;
+        docCount = 0;
+        lastPayloadLen = 0;
+        // force first offset to write its length
+        lastOffsetLength = -1;
+        return this;
+      }
+    }
+
+    private final PostingsWriter postingsWriter = new PostingsWriter();
+
+    @Override
+    public PostingsConsumer startTerm(BytesRef text) {
+      //System.out.println("  startTerm term=" + text.utf8ToString());
+      return postingsWriter.reset();
+    }
+
+    private final RAMOutputStream buffer2 = new RAMOutputStream();
+    private final BytesRef spare = new BytesRef();
+    private byte[] finalBuffer = new byte[128];
+
+    private final IntsRef scratchIntsRef = new IntsRef();
+
+    @Override
+    public void finishTerm(BytesRef text, TermStats stats) throws IOException {
+
+      assert postingsWriter.docCount == stats.docFreq;
+
+      assert buffer2.getFilePointer() == 0;
+
+      buffer2.writeVInt(stats.docFreq);
+      if (field.getIndexOptions() != IndexOptions.DOCS_ONLY) {
+        buffer2.writeVLong(stats.totalTermFreq-stats.docFreq);
+      }
+      int pos = (int) buffer2.getFilePointer();
+      buffer2.writeTo(finalBuffer, 0);
+      buffer2.reset();
+
+      final int totalBytes = pos + (int) postingsWriter.buffer.getFilePointer();
+      if (totalBytes > finalBuffer.length) {
+        finalBuffer = ArrayUtil.grow(finalBuffer, totalBytes);
+      }
+      postingsWriter.buffer.writeTo(finalBuffer, pos);
+      postingsWriter.buffer.reset();
+
+      spare.bytes = finalBuffer;
+      spare.length = totalBytes;
+
+      //System.out.println("    finishTerm term=" + text.utf8ToString() + " " + totalBytes + " bytes totalTF=" + stats.totalTermFreq);
+      //for(int i=0;i<totalBytes;i++) {
+      //  System.out.println("      " + Integer.toHexString(finalBuffer[i]&0xFF));
+      //}
+
+      builder.add(Util.toIntsRef(text, scratchIntsRef), BytesRef.deepCopyOf(spare));
+      termCount++;
+    }
+
+    @Override
+    public void finish(long sumTotalTermFreq, long sumDocFreq, int docCount) throws IOException {
+      if (termCount > 0) {
+        out.writeVInt(termCount);
+        out.writeVInt(field.number);
+        if (field.getIndexOptions() != IndexOptions.DOCS_ONLY) {
+          out.writeVLong(sumTotalTermFreq);
+        }
+        out.writeVLong(sumDocFreq);
+        out.writeVInt(docCount);
+        FST<BytesRef> fst = builder.finish();
+        if (doPackFST) {
+          fst = fst.pack(3, Math.max(10, fst.getNodeCount()/4), acceptableOverheadRatio);
+        }
+        fst.save(out);
+        //System.out.println("finish field=" + field.name + " fp=" + out.getFilePointer());
+      }
+    }
+
+    @Override
+    public Comparator<BytesRef> getComparator() {
+      return BytesRef.getUTF8SortedAsUnicodeComparator();
+    }
+  }
+
+  private static String EXTENSION = "ram";
+
+  @Override
+  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
+
+    final String fileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, EXTENSION);
+    final IndexOutput out = state.directory.createOutput(fileName, state.context);
+    
+    return new FieldsConsumer() {
+      @Override
+      public TermsConsumer addField(FieldInfo field) {
+        //System.out.println("\naddField field=" + field.name);
+        return new TermsWriter(out, field, doPackFST, acceptableOverheadRatio);
+      }
+
+      @Override
+      public void close() throws IOException {
+        // EOF marker:
+        try {
+          out.writeVInt(0);
+        } finally {
+          out.close();
+        }
+      }
+    };
+  }
+
+  private final static class FSTDocsEnum extends DocsEnum {
+    private final IndexOptions indexOptions;
+    private final boolean storePayloads;
+    private byte[] buffer = new byte[16];
+    private final ByteArrayDataInput in = new ByteArrayDataInput(buffer);
+
+    private Bits liveDocs;
+    private int docUpto;
+    private int docID = -1;
+    private int accum;
+    private int freq;
+    private int payloadLen;
+    private int numDocs;
+
+    public FSTDocsEnum(IndexOptions indexOptions, boolean storePayloads) {
+      this.indexOptions = indexOptions;
+      this.storePayloads = storePayloads;
+    }
+
+    public boolean canReuse(IndexOptions indexOptions, boolean storePayloads) {
+      return indexOptions == this.indexOptions && storePayloads == this.storePayloads;
+    }
+    
+    public FSTDocsEnum reset(BytesRef bufferIn, Bits liveDocs, int numDocs) {
+      assert numDocs > 0;
+      if (buffer.length < bufferIn.length - bufferIn.offset) {
+        buffer = ArrayUtil.grow(buffer, bufferIn.length - bufferIn.offset);
+      }
+      in.reset(buffer, 0, bufferIn.length - bufferIn.offset);
+      System.arraycopy(bufferIn.bytes, bufferIn.offset, buffer, 0, bufferIn.length - bufferIn.offset);
+      this.liveDocs = liveDocs;
+      docID = -1;
+      accum = 0;
+      docUpto = 0;
+      freq = 1;
+      payloadLen = 0;
+      this.numDocs = numDocs;
+      return this;
+    }
+
+    @Override
+    public int nextDoc() {
+      while(true) {
+        //System.out.println("  nextDoc cycle docUpto=" + docUpto + " numDocs=" + numDocs + " fp=" + in.getPosition() + " this=" + this);
+        if (docUpto == numDocs) {
+          // System.out.println("    END");
+          return docID = NO_MORE_DOCS;
+        }
+        docUpto++;
+        if (indexOptions == IndexOptions.DOCS_ONLY) {
+          accum += in.readVInt();
+        } else {
+          final int code = in.readVInt();
+          accum += code >>> 1;
+          //System.out.println("  docID=" + accum + " code=" + code);
+          if ((code & 1) != 0) {
+            freq = 1;
+          } else {
+            freq = in.readVInt();
+            assert freq > 0;
+          }
+
+          if (indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
+            // Skip positions/payloads
+            for(int posUpto=0;posUpto<freq;posUpto++) {
+              if (!storePayloads) {
+                in.readVInt();
+              } else {
+                final int posCode = in.readVInt();
+                if ((posCode & 1) != 0) {
+                  payloadLen = in.readVInt();
+                }
+                in.skipBytes(payloadLen);
+              }
+            }
+          } else if (indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) {
+            // Skip positions/offsets/payloads
+            for(int posUpto=0;posUpto<freq;posUpto++) {
+              int posCode = in.readVInt();
+              if (storePayloads && ((posCode & 1) != 0)) {
+                payloadLen = in.readVInt();
+              }
+              if ((in.readVInt() & 1) != 0) {
+                // new offset length
+                in.readVInt();
+              }
+              if (storePayloads) {
+                in.skipBytes(payloadLen);
+              }
+            }
+          }
+        }
+
+        if (liveDocs == null || liveDocs.get(accum)) {
+          //System.out.println("    return docID=" + accum + " freq=" + freq);
+          return (docID = accum);
+        }
+      }
+    }
+
+    @Override
+    public int docID() {
+      return docID;
+    }
+
+    @Override
+    public int advance(int target) {
+      // TODO: we could make more efficient version, but, it
+      // should be rare that this will matter in practice
+      // since usually apps will not store "big" fields in
+      // this codec!
+      //System.out.println("advance start docID=" + docID + " target=" + target);
+      while(nextDoc() < target) {
+      }
+      return docID;
+    }
+
+    @Override
+    public int freq() {
+      return freq;
+    }
+  }
+
+  private final static class FSTDocsAndPositionsEnum extends DocsAndPositionsEnum {
+    private final boolean storePayloads;
+    private byte[] buffer = new byte[16];
+    private final ByteArrayDataInput in = new ByteArrayDataInput(buffer);
+
+    private Bits liveDocs;
+    private int docUpto;
+    private int docID = -1;
+    private int accum;
+    private int freq;
+    private int numDocs;
+    private int posPending;
+    private int payloadLength;
+    final boolean storeOffsets;
+    int offsetLength;
+    int startOffset;
+
+    private int pos;
+    private final BytesRef payload = new BytesRef();
+
+    public FSTDocsAndPositionsEnum(boolean storePayloads, boolean storeOffsets) {
+      this.storePayloads = storePayloads;
+      this.storeOffsets = storeOffsets;
+    }
+
+    public boolean canReuse(boolean storePayloads, boolean storeOffsets) {
+      return storePayloads == this.storePayloads && storeOffsets == this.storeOffsets;
+    }
+    
+    public FSTDocsAndPositionsEnum reset(BytesRef bufferIn, Bits liveDocs, int numDocs) {
+      assert numDocs > 0;
+
+      // System.out.println("D&P reset bytes this=" + this);
+      // for(int i=bufferIn.offset;i<bufferIn.length;i++) {
+      //   System.out.println("  " + Integer.toHexString(bufferIn.bytes[i]&0xFF));
+      // }
+
+      if (buffer.length < bufferIn.length - bufferIn.offset) {
+        buffer = ArrayUtil.grow(buffer, bufferIn.length - bufferIn.offset);
+      }
+      in.reset(buffer, 0, bufferIn.length - bufferIn.offset);
+      System.arraycopy(bufferIn.bytes, bufferIn.offset, buffer, 0, bufferIn.length - bufferIn.offset);
+      this.liveDocs = liveDocs;
+      docID = -1;
+      accum = 0;
+      docUpto = 0;
+      payload.bytes = buffer;
+      payloadLength = 0;
+      this.numDocs = numDocs;
+      posPending = 0;
+      startOffset = storeOffsets ? 0 : -1; // always return -1 if no offsets are stored
+      offsetLength = 0;
+      return this;
+    }
+
+    @Override
+    public int nextDoc() {
+      while (posPending > 0) {
+        nextPosition();
+      }
+      while(true) {
+        //System.out.println("  nextDoc cycle docUpto=" + docUpto + " numDocs=" + numDocs + " fp=" + in.getPosition() + " this=" + this);
+        if (docUpto == numDocs) {
+          //System.out.println("    END");
+          return docID = NO_MORE_DOCS;
+        }
+        docUpto++;
+        
+        final int code = in.readVInt();
+        accum += code >>> 1;
+        if ((code & 1) != 0) {
+          freq = 1;
+        } else {
+          freq = in.readVInt();
+          assert freq > 0;
+        }
+
+        if (liveDocs == null || liveDocs.get(accum)) {
+          pos = 0;
+          startOffset = storeOffsets ? 0 : -1;
+          posPending = freq;
+          //System.out.println("    return docID=" + accum + " freq=" + freq);
+          return (docID = accum);
+        }
+
+        // Skip positions
+        for(int posUpto=0;posUpto<freq;posUpto++) {
+          if (!storePayloads) {
+            in.readVInt();
+          } else {
+            final int skipCode = in.readVInt();
+            if ((skipCode & 1) != 0) {
+              payloadLength = in.readVInt();
+              //System.out.println("    new payloadLen=" + payloadLength);
+            }
+          }
+          
+          if (storeOffsets) {
+            if ((in.readVInt() & 1) != 0) {
+              // new offset length
+              offsetLength = in.readVInt();
+            }
+          }
+          
+          if (storePayloads) {
+            in.skipBytes(payloadLength);
+          }
+        }
+      }
+    }
+
+    @Override
+    public int nextPosition() {
+      //System.out.println("    nextPos storePayloads=" + storePayloads + " this=" + this);
+      assert posPending > 0;
+      posPending--;
+      if (!storePayloads) {
+        pos += in.readVInt();
+      } else {
+        final int code = in.readVInt();
+        pos += code >>> 1;
+        if ((code & 1) != 0) {
+          payloadLength = in.readVInt();
+          //System.out.println("      new payloadLen=" + payloadLength);
+          //} else {
+          //System.out.println("      same payloadLen=" + payloadLength);
+        }
+      }
+      
+      if (storeOffsets) {
+        int offsetCode = in.readVInt();
+        if ((offsetCode & 1) != 0) {
+          // new offset length
+          offsetLength = in.readVInt();
+        }
+        startOffset += offsetCode >>> 1;
+      }
+      
+      if (storePayloads) {
+        payload.offset = in.getPosition();
+        in.skipBytes(payloadLength);
+        payload.length = payloadLength;
+      }
+
+      //System.out.println("      pos=" + pos + " payload=" + payload + " fp=" + in.getPosition());
+      return pos;
+    }
+
+    @Override
+    public int startOffset() {
+      return startOffset;
+    }
+
+    @Override
+    public int endOffset() {
+      return startOffset + offsetLength;
+    }
+
+    @Override
+    public BytesRef getPayload() {
+      return payload.length > 0 ? payload : null;
+    }
+
+    @Override
+    public int docID() {
+      return docID;
+    }
+
+    @Override
+    public int advance(int target) {
+      // TODO: we could make more efficient version, but, it
+      // should be rare that this will matter in practice
+      // since usually apps will not store "big" fields in
+      // this codec!
+      //System.out.println("advance target=" + target);
+      while(nextDoc() < target) {
+      }
+      //System.out.println("  return " + docID);
+      return docID;
+    }
+
+    @Override
+    public int freq() {
+      return freq;
+    }
+  }
+
+  private final static class FSTTermsEnum extends TermsEnum {
+    private final FieldInfo field;
+    private final BytesRefFSTEnum<BytesRef> fstEnum;
+    private final ByteArrayDataInput buffer = new ByteArrayDataInput();
+    private boolean didDecode;
+
+    private int docFreq;
+    private long totalTermFreq;
+    private BytesRefFSTEnum.InputOutput<BytesRef> current;
+
+    public FSTTermsEnum(FieldInfo field, FST<BytesRef> fst) {
+      this.field = field;
+      fstEnum = new BytesRefFSTEnum<BytesRef>(fst);
+    }
+
+    private void decodeMetaData() {
+      if (!didDecode) {
+        buffer.reset(current.output.bytes, 0, current.output.length);
+        docFreq = buffer.readVInt();
+        if (field.getIndexOptions() != IndexOptions.DOCS_ONLY) {
+          totalTermFreq = docFreq + buffer.readVLong();
+        } else {
+          totalTermFreq = -1;
+        }
+        current.output.offset = buffer.getPosition();
+        //System.out.println("  df=" + docFreq + " totTF=" + totalTermFreq + " offset=" + buffer.getPosition() + " len=" + current.output.length);
+        didDecode = true;
+      }
+    }
+
+    @Override
+    public boolean seekExact(BytesRef text, boolean useCache /* ignored */) throws IOException {
+      //System.out.println("te.seekExact text=" + field.name + ":" + text.utf8ToString() + " this=" + this);
+      current = fstEnum.seekExact(text);
+      didDecode = false;
+      return current != null;
+    }
+
+    @Override
+    public SeekStatus seekCeil(BytesRef text, boolean useCache /* ignored */) throws IOException {
+      //System.out.println("te.seek text=" + field.name + ":" + text.utf8ToString() + " this=" + this);
+      current = fstEnum.seekCeil(text);
+      if (current == null) {
+        return SeekStatus.END;
+      } else {
+
+        // System.out.println("  got term=" + current.input.utf8ToString());
+        // for(int i=0;i<current.output.length;i++) {
+        //   System.out.println("    " + Integer.toHexString(current.output.bytes[i]&0xFF));
+        // }
+
+        didDecode = false;
+
+        if (text.equals(current.input)) {
+          //System.out.println("  found!");
+          return SeekStatus.FOUND;
+        } else {
+          //System.out.println("  not found: " + current.input.utf8ToString());
+          return SeekStatus.NOT_FOUND;
+        }
+      }
+    }
+    
+    @Override
+    public DocsEnum docs(Bits liveDocs, DocsEnum reuse, int flags) {
+      decodeMetaData();
+      FSTDocsEnum docsEnum;
+
+      if (reuse == null || !(reuse instanceof FSTDocsEnum)) {
+        docsEnum = new FSTDocsEnum(field.getIndexOptions(), field.hasPayloads());
+      } else {
+        docsEnum = (FSTDocsEnum) reuse;        
+        if (!docsEnum.canReuse(field.getIndexOptions(), field.hasPayloads())) {
+          docsEnum = new FSTDocsEnum(field.getIndexOptions(), field.hasPayloads());
+        }
+      }
+      return docsEnum.reset(current.output, liveDocs, docFreq);
+    }
+
+    @Override
+    public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum reuse, int flags) {
+
+      boolean hasOffsets = field.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
+      if (field.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) < 0) {
+        return null;
+      }
+      decodeMetaData();
+      FSTDocsAndPositionsEnum docsAndPositionsEnum;
+      if (reuse == null || !(reuse instanceof FSTDocsAndPositionsEnum)) {
+        docsAndPositionsEnum = new FSTDocsAndPositionsEnum(field.hasPayloads(), hasOffsets);
+      } else {
+        docsAndPositionsEnum = (FSTDocsAndPositionsEnum) reuse;        
+        if (!docsAndPositionsEnum.canReuse(field.hasPayloads(), hasOffsets)) {
+          docsAndPositionsEnum = new FSTDocsAndPositionsEnum(field.hasPayloads(), hasOffsets);
+        }
+      }
+      //System.out.println("D&P reset this=" + this);
+      return docsAndPositionsEnum.reset(current.output, liveDocs, docFreq);
+    }
+
+    @Override
+    public BytesRef term() {
+      return current.input;
+    }
+
+    @Override
+    public BytesRef next() throws IOException {
+      //System.out.println("te.next");
+      current = fstEnum.next();
+      if (current == null) {
+        //System.out.println("  END");
+        return null;
+      }
+      didDecode = false;
+      //System.out.println("  term=" + field.name + ":" + current.input.utf8ToString());
+      return current.input;
+    }
+
+    @Override
+    public int docFreq() {
+      decodeMetaData();
+      return docFreq;
+    }
+
+    @Override
+    public long totalTermFreq() {
+      decodeMetaData();
+      return totalTermFreq;
+    }
+
+    @Override
+    public Comparator<BytesRef> getComparator() {
+      return BytesRef.getUTF8SortedAsUnicodeComparator();
+    }
+
+    @Override
+    public void seekExact(long ord) {
+      // NOTE: we could add this...
+      throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public long ord() {
+      // NOTE: we could add this...
+      throw new UnsupportedOperationException();
+    }
+  }
+
+  private final static class TermsReader extends Terms {
+
+    private final long sumTotalTermFreq;
+    private final long sumDocFreq;
+    private final int docCount;
+    private final int termCount;
+    private FST<BytesRef> fst;
+    private final ByteSequenceOutputs outputs = ByteSequenceOutputs.getSingleton();
+    private final FieldInfo field;
+
+    public TermsReader(FieldInfos fieldInfos, IndexInput in, int termCount) throws IOException {
+      this.termCount = termCount;
+      final int fieldNumber = in.readVInt();
+      field = fieldInfos.fieldInfo(fieldNumber);
+      if (field.getIndexOptions() != IndexOptions.DOCS_ONLY) {
+        sumTotalTermFreq = in.readVLong();
+      } else {
+        sumTotalTermFreq = -1;
+      }
+      sumDocFreq = in.readVLong();
+      docCount = in.readVInt();
+      
+      fst = new FST<BytesRef>(in, outputs);
+    }
+
+    @Override
+    public long getSumTotalTermFreq() {
+      return sumTotalTermFreq;
+    }
+
+    @Override
+    public long getSumDocFreq() {
+      return sumDocFreq;
+    }
+
+    @Override
+    public int getDocCount() {
+      return docCount;
+    }
+
+    @Override
+    public long size() {
+      return termCount;
+    }
+
+    @Override
+    public TermsEnum iterator(TermsEnum reuse) {
+      return new FSTTermsEnum(field, fst);
+    }
+
+    @Override
+    public Comparator<BytesRef> getComparator() {
+      return BytesRef.getUTF8SortedAsUnicodeComparator();
+    }
+
+    @Override
+    public boolean hasOffsets() {
+      return field.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
+    }
+
+    @Override
+    public boolean hasPositions() {
+      return field.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;
+    }
+    
+    @Override
+    public boolean hasPayloads() {
+      return field.hasPayloads();
+    }
+  }
+
+  @Override
+  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
+    final String fileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, EXTENSION);
+    final IndexInput in = state.dir.openInput(fileName, IOContext.READONCE);
+
+    final SortedMap<String,TermsReader> fields = new TreeMap<String,TermsReader>();
+
+    try {
+      while(true) {
+        final int termCount = in.readVInt();
+        if (termCount == 0) {
+          break;
+        }
+        final TermsReader termsReader = new TermsReader(state.fieldInfos, in, termCount);
+        // System.out.println("load field=" + termsReader.field.name);
+        fields.put(termsReader.field.name, termsReader);
+      }
+    } finally {
+      in.close();
+    }
+
+    return new FieldsProducer() {
+      @Override
+      public Iterator<String> iterator() {
+        return Collections.unmodifiableSet(fields.keySet()).iterator();
+      }
+
+      @Override
+      public Terms terms(String field) {
+        return fields.get(field);
+      }
+      
+      @Override
+      public int size() {
+        return fields.size();
+      }
+
+      @Override
+      public void close() {
+        // Drop ref to FST:
+        for(TermsReader termsReader : fields.values()) {
+          termsReader.fst = null;
+        }
+      }
+    };
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/memory/package.html b/lucene/codecs/src/java/org/apache/lucene/codecs/memory/package.html
new file mode 100644
index 0000000..340e831
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/memory/package.html
@@ -0,0 +1,25 @@
+<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+<html>
+<head>
+   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
+</head>
+<body>
+Postings format that is read entirely into memory.
+</body>
+</html>
\ No newline at end of file
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/pulsing/Pulsing40PostingsFormat.java b/lucene/codecs/src/java/org/apache/lucene/codecs/pulsing/Pulsing40PostingsFormat.java
new file mode 100644
index 0000000..faf8df2
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/pulsing/Pulsing40PostingsFormat.java
@@ -0,0 +1,45 @@
+package org.apache.lucene.codecs.pulsing;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.codecs.BlockTreeTermsWriter;
+import org.apache.lucene.codecs.lucene40.Lucene40PostingsBaseFormat;
+import org.apache.lucene.codecs.lucene40.Lucene40PostingsFormat; // javadocs
+
+/**
+ * Concrete pulsing implementation over {@link Lucene40PostingsFormat}.
+ * 
+ * @lucene.experimental
+ */
+public class Pulsing40PostingsFormat extends PulsingPostingsFormat {
+
+  /** Inlines docFreq=1 terms, otherwise uses the normal "Lucene40" format. */
+  public Pulsing40PostingsFormat() {
+    this(1);
+  }
+
+  /** Inlines docFreq=<code>freqCutoff</code> terms, otherwise uses the normal "Lucene40" format. */
+  public Pulsing40PostingsFormat(int freqCutoff) {
+    this(freqCutoff, BlockTreeTermsWriter.DEFAULT_MIN_BLOCK_SIZE, BlockTreeTermsWriter.DEFAULT_MAX_BLOCK_SIZE);
+  }
+
+  /** Inlines docFreq=<code>freqCutoff</code> terms, otherwise uses the normal "Lucene40" format. */
+  public Pulsing40PostingsFormat(int freqCutoff, int minBlockSize, int maxBlockSize) {
+    super("Pulsing40", new Lucene40PostingsBaseFormat(), freqCutoff, minBlockSize, maxBlockSize);
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/pulsing/PulsingPostingsFormat.java b/lucene/codecs/src/java/org/apache/lucene/codecs/pulsing/PulsingPostingsFormat.java
new file mode 100644
index 0000000..da53cc5
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/pulsing/PulsingPostingsFormat.java
@@ -0,0 +1,120 @@
+package org.apache.lucene.codecs.pulsing;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.BlockTreeTermsReader;
+import org.apache.lucene.codecs.BlockTreeTermsWriter;
+import org.apache.lucene.codecs.FieldsConsumer;
+import org.apache.lucene.codecs.FieldsProducer;
+import org.apache.lucene.codecs.PostingsBaseFormat;
+import org.apache.lucene.codecs.PostingsFormat;
+import org.apache.lucene.codecs.PostingsReaderBase;
+import org.apache.lucene.codecs.PostingsWriterBase;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.util.IOUtils;
+
+/** This postings format "inlines" the postings for terms that have
+ *  low docFreq.  It wraps another postings format, which is used for
+ *  writing the non-inlined terms.
+ *
+ *  @lucene.experimental */
+
+public abstract class PulsingPostingsFormat extends PostingsFormat {
+
+  private final int freqCutoff;
+  private final int minBlockSize;
+  private final int maxBlockSize;
+  private final PostingsBaseFormat wrappedPostingsBaseFormat;
+  
+  public PulsingPostingsFormat(String name, PostingsBaseFormat wrappedPostingsBaseFormat, int freqCutoff) {
+    this(name, wrappedPostingsBaseFormat, freqCutoff, BlockTreeTermsWriter.DEFAULT_MIN_BLOCK_SIZE, BlockTreeTermsWriter.DEFAULT_MAX_BLOCK_SIZE);
+  }
+
+  /** Terms with freq <= freqCutoff are inlined into terms
+   *  dict. */
+  public PulsingPostingsFormat(String name, PostingsBaseFormat wrappedPostingsBaseFormat, int freqCutoff, int minBlockSize, int maxBlockSize) {
+    super(name);
+    this.freqCutoff = freqCutoff;
+    this.minBlockSize = minBlockSize;
+    assert minBlockSize > 1;
+    this.maxBlockSize = maxBlockSize;
+    this.wrappedPostingsBaseFormat = wrappedPostingsBaseFormat;
+  }
+
+  @Override
+  public String toString() {
+    return getName() + "(freqCutoff=" + freqCutoff + " minBlockSize=" + minBlockSize + " maxBlockSize=" + maxBlockSize + ")";
+  }
+
+  @Override
+  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
+    PostingsWriterBase docsWriter = null;
+
+    // Terms that have <= freqCutoff number of docs are
+    // "pulsed" (inlined):
+    PostingsWriterBase pulsingWriter = null;
+
+    // Terms dict
+    boolean success = false;
+    try {
+      docsWriter = wrappedPostingsBaseFormat.postingsWriterBase(state);
+
+      // Terms that have <= freqCutoff number of docs are
+      // "pulsed" (inlined):
+      pulsingWriter = new PulsingPostingsWriter(freqCutoff, docsWriter);
+      FieldsConsumer ret = new BlockTreeTermsWriter(state, pulsingWriter, minBlockSize, maxBlockSize);
+      success = true;
+      return ret;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(docsWriter, pulsingWriter);
+      }
+    }
+  }
+
+  @Override
+  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
+    PostingsReaderBase docsReader = null;
+    PostingsReaderBase pulsingReader = null;
+
+    boolean success = false;
+    try {
+      docsReader = wrappedPostingsBaseFormat.postingsReaderBase(state);
+      pulsingReader = new PulsingPostingsReader(docsReader);
+      FieldsProducer ret = new BlockTreeTermsReader(
+                                                    state.dir, state.fieldInfos, state.segmentInfo,
+                                                    pulsingReader,
+                                                    state.context,
+                                                    state.segmentSuffix,
+                                                    state.termsIndexDivisor);
+      success = true;
+      return ret;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(docsReader, pulsingReader);
+      }
+    }
+  }
+
+  public int getFreqCutoff() {
+    return freqCutoff;
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/pulsing/PulsingPostingsReader.java b/lucene/codecs/src/java/org/apache/lucene/codecs/pulsing/PulsingPostingsReader.java
new file mode 100644
index 0000000..76fa37a
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/pulsing/PulsingPostingsReader.java
@@ -0,0 +1,629 @@
+package org.apache.lucene.codecs.pulsing;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.IdentityHashMap;
+import java.util.Map;
+
+import org.apache.lucene.codecs.BlockTermState;
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.PostingsReaderBase;
+import org.apache.lucene.index.DocsAndPositionsEnum;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.TermState;
+import org.apache.lucene.store.ByteArrayDataInput;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.Attribute;
+import org.apache.lucene.util.AttributeImpl;
+import org.apache.lucene.util.AttributeSource;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+
+/** Concrete class that reads the current doc/freq/skip
+ *  postings format 
+ *  @lucene.experimental */
+
+// TODO: -- should we switch "hasProx" higher up?  and
+// create two separate docs readers, one that also reads
+// prox and one that doesn't?
+
+public class PulsingPostingsReader extends PostingsReaderBase {
+
+  // Fallback reader for non-pulsed terms:
+  final PostingsReaderBase wrappedPostingsReader;
+  int maxPositions;
+
+  public PulsingPostingsReader(PostingsReaderBase wrappedPostingsReader) {
+    this.wrappedPostingsReader = wrappedPostingsReader;
+  }
+
+  @Override
+  public void init(IndexInput termsIn) throws IOException {
+    CodecUtil.checkHeader(termsIn, PulsingPostingsWriter.CODEC,
+      PulsingPostingsWriter.VERSION_START, PulsingPostingsWriter.VERSION_START);
+    maxPositions = termsIn.readVInt();
+    wrappedPostingsReader.init(termsIn);
+  }
+
+  private static class PulsingTermState extends BlockTermState {
+    private byte[] postings;
+    private int postingsSize;                     // -1 if this term was not inlined
+    private BlockTermState wrappedTermState;
+
+    ByteArrayDataInput inlinedBytesReader;
+    private byte[] inlinedBytes;
+
+    @Override
+    public PulsingTermState clone() {
+      PulsingTermState clone;
+      clone = (PulsingTermState) super.clone();
+      if (postingsSize != -1) {
+        clone.postings = new byte[postingsSize];
+        System.arraycopy(postings, 0, clone.postings, 0, postingsSize);
+      } else {
+        assert wrappedTermState != null;
+        clone.wrappedTermState = (BlockTermState) wrappedTermState.clone();
+      }
+      return clone;
+    }
+
+    @Override
+    public void copyFrom(TermState _other) {
+      super.copyFrom(_other);
+      PulsingTermState other = (PulsingTermState) _other;
+      postingsSize = other.postingsSize;
+      if (other.postingsSize != -1) {
+        if (postings == null || postings.length < other.postingsSize) {
+          postings = new byte[ArrayUtil.oversize(other.postingsSize, 1)];
+        }
+        System.arraycopy(other.postings, 0, postings, 0, other.postingsSize);
+      } else {
+        wrappedTermState.copyFrom(other.wrappedTermState);
+      }
+
+      // NOTE: we do not copy the
+      // inlinedBytes/inlinedBytesReader; these are only
+      // stored on the "primary" TermState.  They are
+      // "transient" to cloned term states.
+    }
+
+    @Override
+    public String toString() {
+      if (postingsSize == -1) {
+        return "PulsingTermState: not inlined: wrapped=" + wrappedTermState;
+      } else {
+        return "PulsingTermState: inlined size=" + postingsSize + " " + super.toString();
+      }
+    }
+  }
+
+  @Override
+  public void readTermsBlock(IndexInput termsIn, FieldInfo fieldInfo, BlockTermState _termState) throws IOException {
+    //System.out.println("PR.readTermsBlock state=" + _termState);
+    final PulsingTermState termState = (PulsingTermState) _termState;
+    if (termState.inlinedBytes == null) {
+      termState.inlinedBytes = new byte[128];
+      termState.inlinedBytesReader = new ByteArrayDataInput();
+    }
+    int len = termsIn.readVInt();
+    //System.out.println("  len=" + len + " fp=" + termsIn.getFilePointer());
+    if (termState.inlinedBytes.length < len) {
+      termState.inlinedBytes = new byte[ArrayUtil.oversize(len, 1)];
+    }
+    termsIn.readBytes(termState.inlinedBytes, 0, len);
+    termState.inlinedBytesReader.reset(termState.inlinedBytes);
+    termState.wrappedTermState.termBlockOrd = 0;
+    wrappedPostingsReader.readTermsBlock(termsIn, fieldInfo, termState.wrappedTermState);
+  }
+
+  @Override
+  public BlockTermState newTermState() throws IOException {
+    PulsingTermState state = new PulsingTermState();
+    state.wrappedTermState = wrappedPostingsReader.newTermState();
+    return state;
+  }
+
+  @Override
+  public void nextTerm(FieldInfo fieldInfo, BlockTermState _termState) throws IOException {
+    //System.out.println("PR nextTerm");
+    PulsingTermState termState = (PulsingTermState) _termState;
+
+    // if we have positions, its total TF, otherwise its computed based on docFreq.
+    long count = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 ? termState.totalTermFreq : termState.docFreq;
+    //System.out.println("  count=" + count + " threshold=" + maxPositions);
+
+    if (count <= maxPositions) {
+
+      // Inlined into terms dict -- just read the byte[] blob in,
+      // but don't decode it now (we only decode when a DocsEnum
+      // or D&PEnum is pulled):
+      termState.postingsSize = termState.inlinedBytesReader.readVInt();
+      if (termState.postings == null || termState.postings.length < termState.postingsSize) {
+        termState.postings = new byte[ArrayUtil.oversize(termState.postingsSize, 1)];
+      }
+      // TODO: sort of silly to copy from one big byte[]
+      // (the blob holding all inlined terms' blobs for
+      // current term block) into another byte[] (just the
+      // blob for this term)...
+      termState.inlinedBytesReader.readBytes(termState.postings, 0, termState.postingsSize);
+      //System.out.println("  inlined bytes=" + termState.postingsSize);
+    } else {
+      //System.out.println("  not inlined");
+      termState.postingsSize = -1;
+      // TODO: should we do full copyFrom?  much heavier...?
+      termState.wrappedTermState.docFreq = termState.docFreq;
+      termState.wrappedTermState.totalTermFreq = termState.totalTermFreq;
+      wrappedPostingsReader.nextTerm(fieldInfo, termState.wrappedTermState);
+      termState.wrappedTermState.termBlockOrd++;
+    }
+  }
+
+  @Override
+  public DocsEnum docs(FieldInfo field, BlockTermState _termState, Bits liveDocs, DocsEnum reuse, int flags) throws IOException {
+    PulsingTermState termState = (PulsingTermState) _termState;
+    if (termState.postingsSize != -1) {
+      PulsingDocsEnum postings;
+      if (reuse instanceof PulsingDocsEnum) {
+        postings = (PulsingDocsEnum) reuse;
+        if (!postings.canReuse(field)) {
+          postings = new PulsingDocsEnum(field);
+        }
+      } else {
+        // the 'reuse' is actually the wrapped enum
+        PulsingDocsEnum previous = (PulsingDocsEnum) getOther(reuse);
+        if (previous != null && previous.canReuse(field)) {
+          postings = previous;
+        } else {
+          postings = new PulsingDocsEnum(field);
+        }
+      }
+      if (reuse != postings) {
+        setOther(postings, reuse); // postings.other = reuse
+      }
+      return postings.reset(liveDocs, termState);
+    } else {
+      if (reuse instanceof PulsingDocsEnum) {
+        DocsEnum wrapped = wrappedPostingsReader.docs(field, termState.wrappedTermState, liveDocs, getOther(reuse), flags);
+        setOther(wrapped, reuse); // wrapped.other = reuse
+        return wrapped;
+      } else {
+        return wrappedPostingsReader.docs(field, termState.wrappedTermState, liveDocs, reuse, flags);
+      }
+    }
+  }
+
+  @Override
+  public DocsAndPositionsEnum docsAndPositions(FieldInfo field, BlockTermState _termState, Bits liveDocs, DocsAndPositionsEnum reuse,
+                                               int flags) throws IOException {
+
+    final PulsingTermState termState = (PulsingTermState) _termState;
+
+    if (termState.postingsSize != -1) {
+      PulsingDocsAndPositionsEnum postings;
+      if (reuse instanceof PulsingDocsAndPositionsEnum) {
+        postings = (PulsingDocsAndPositionsEnum) reuse;
+        if (!postings.canReuse(field)) {
+          postings = new PulsingDocsAndPositionsEnum(field);
+        }
+      } else {
+        // the 'reuse' is actually the wrapped enum
+        PulsingDocsAndPositionsEnum previous = (PulsingDocsAndPositionsEnum) getOther(reuse);
+        if (previous != null && previous.canReuse(field)) {
+          postings = previous;
+        } else {
+          postings = new PulsingDocsAndPositionsEnum(field);
+        }
+      }
+      if (reuse != postings) {
+        setOther(postings, reuse); // postings.other = reuse 
+      }
+      return postings.reset(liveDocs, termState);
+    } else {
+      if (reuse instanceof PulsingDocsAndPositionsEnum) {
+        DocsAndPositionsEnum wrapped = wrappedPostingsReader.docsAndPositions(field, termState.wrappedTermState, liveDocs, (DocsAndPositionsEnum) getOther(reuse),
+                                                                              flags);
+        setOther(wrapped, reuse); // wrapped.other = reuse
+        return wrapped;
+      } else {
+        return wrappedPostingsReader.docsAndPositions(field, termState.wrappedTermState, liveDocs, reuse, flags);
+      }
+    }
+  }
+
+  private static class PulsingDocsEnum extends DocsEnum {
+    private byte[] postingsBytes;
+    private final ByteArrayDataInput postings = new ByteArrayDataInput();
+    private final IndexOptions indexOptions;
+    private final boolean storePayloads;
+    private final boolean storeOffsets;
+    private Bits liveDocs;
+    private int docID = -1;
+    private int accum;
+    private int freq;
+    private int payloadLength;
+
+    public PulsingDocsEnum(FieldInfo fieldInfo) {
+      indexOptions = fieldInfo.getIndexOptions();
+      storePayloads = fieldInfo.hasPayloads();
+      storeOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
+    }
+
+    public PulsingDocsEnum reset(Bits liveDocs, PulsingTermState termState) {
+      //System.out.println("PR docsEnum termState=" + termState + " docFreq=" + termState.docFreq);
+      assert termState.postingsSize != -1;
+
+      // Must make a copy of termState's byte[] so that if
+      // app does TermsEnum.next(), this DocsEnum is not affected
+      if (postingsBytes == null) {
+        postingsBytes = new byte[termState.postingsSize];
+      } else if (postingsBytes.length < termState.postingsSize) {
+        postingsBytes = ArrayUtil.grow(postingsBytes, termState.postingsSize);
+      }
+      System.arraycopy(termState.postings, 0, postingsBytes, 0, termState.postingsSize);
+      postings.reset(postingsBytes, 0, termState.postingsSize);
+      docID = -1;
+      accum = 0;
+      freq = 1;
+      payloadLength = 0;
+      this.liveDocs = liveDocs;
+      return this;
+    }
+
+    boolean canReuse(FieldInfo fieldInfo) {
+      return indexOptions == fieldInfo.getIndexOptions() && storePayloads == fieldInfo.hasPayloads();
+    }
+
+    @Override
+    public int nextDoc() throws IOException {
+      //System.out.println("PR nextDoc this= "+ this);
+      while(true) {
+        if (postings.eof()) {
+          //System.out.println("PR   END");
+          return docID = NO_MORE_DOCS;
+        }
+
+        final int code = postings.readVInt();
+        //System.out.println("  read code=" + code);
+        if (indexOptions == IndexOptions.DOCS_ONLY) {
+          accum += code;
+        } else {
+          accum += code >>> 1;              // shift off low bit
+          if ((code & 1) != 0) {          // if low bit is set
+            freq = 1;                     // freq is one
+          } else {
+            freq = postings.readVInt();     // else read freq
+          }
+
+          if (indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0) {
+            // Skip positions
+            if (storePayloads) {
+              for(int pos=0;pos<freq;pos++) {
+                final int posCode = postings.readVInt();
+                if ((posCode & 1) != 0) {
+                  payloadLength = postings.readVInt();
+                }
+                if (storeOffsets && (postings.readVInt() & 1) != 0) {
+                  // new offset length
+                  postings.readVInt();
+                }
+                if (payloadLength != 0) {
+                  postings.skipBytes(payloadLength);
+                }
+              }
+            } else {
+              for(int pos=0;pos<freq;pos++) {
+                // TODO: skipVInt
+                postings.readVInt();
+                if (storeOffsets && (postings.readVInt() & 1) != 0) {
+                  // new offset length
+                  postings.readVInt();
+                }
+              }
+            }
+          }
+        }
+
+        if (liveDocs == null || liveDocs.get(accum)) {
+          return (docID = accum);
+        }
+      }
+    }
+
+    @Override
+    public int freq() throws IOException {
+      return freq;
+    }
+
+    @Override
+    public int docID() {
+      return docID;
+    }
+
+    @Override
+    public int advance(int target) throws IOException {
+      int doc;
+      while((doc=nextDoc()) != NO_MORE_DOCS) {
+        if (doc >= target)
+          return doc;
+      }
+      return docID = NO_MORE_DOCS;
+    }
+  }
+
+  private static class PulsingDocsAndPositionsEnum extends DocsAndPositionsEnum {
+    private byte[] postingsBytes;
+    private final ByteArrayDataInput postings = new ByteArrayDataInput();
+    private final boolean storePayloads;
+    private final boolean storeOffsets;
+    // note: we could actually reuse across different options, if we passed this to reset()
+    // and re-init'ed storeOffsets accordingly (made it non-final)
+    private final IndexOptions indexOptions;
+
+    private Bits liveDocs;
+    private int docID = -1;
+    private int accum;
+    private int freq;
+    private int posPending;
+    private int position;
+    private int payloadLength;
+    private BytesRef payload;
+    private int startOffset;
+    private int offsetLength;
+
+    private boolean payloadRetrieved;
+
+    public PulsingDocsAndPositionsEnum(FieldInfo fieldInfo) {
+      indexOptions = fieldInfo.getIndexOptions();
+      storePayloads = fieldInfo.hasPayloads();
+      storeOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
+    }
+
+    boolean canReuse(FieldInfo fieldInfo) {
+      return indexOptions == fieldInfo.getIndexOptions() && storePayloads == fieldInfo.hasPayloads();
+    }
+
+    public PulsingDocsAndPositionsEnum reset(Bits liveDocs, PulsingTermState termState) {
+      assert termState.postingsSize != -1;
+      if (postingsBytes == null) {
+        postingsBytes = new byte[termState.postingsSize];
+      } else if (postingsBytes.length < termState.postingsSize) {
+        postingsBytes = ArrayUtil.grow(postingsBytes, termState.postingsSize);
+      }
+      System.arraycopy(termState.postings, 0, postingsBytes, 0, termState.postingsSize);
+      postings.reset(postingsBytes, 0, termState.postingsSize);
+      this.liveDocs = liveDocs;
+      payloadLength = 0;
+      posPending = 0;
+      docID = -1;
+      accum = 0;
+      startOffset = storeOffsets ? 0 : -1; // always return -1 if no offsets are stored
+      offsetLength = 0;
+      //System.out.println("PR d&p reset storesPayloads=" + storePayloads + " bytes=" + bytes.length + " this=" + this);
+      return this;
+    }
+
+    @Override
+    public int nextDoc() throws IOException {
+      //System.out.println("PR d&p nextDoc this=" + this);
+
+      while(true) {
+        //System.out.println("  cycle skip posPending=" + posPending);
+
+        skipPositions();
+
+        if (postings.eof()) {
+          //System.out.println("PR   END");
+          return docID = NO_MORE_DOCS;
+        }
+
+        final int code = postings.readVInt();
+        accum += code >>> 1;            // shift off low bit
+        if ((code & 1) != 0) {          // if low bit is set
+          freq = 1;                     // freq is one
+        } else {
+          freq = postings.readVInt();     // else read freq
+        }
+        posPending = freq;
+        startOffset = storeOffsets ? 0 : -1; // always return -1 if no offsets are stored
+
+        if (liveDocs == null || liveDocs.get(accum)) {
+          //System.out.println("  return docID=" + docID + " freq=" + freq);
+          position = 0;
+          return (docID = accum);
+        }
+      }
+    }
+
+    @Override
+    public int freq() throws IOException {
+      return freq;
+    }
+
+    @Override
+    public int docID() {
+      return docID;
+    }
+
+    @Override
+    public int advance(int target) throws IOException {
+      int doc;
+      while((doc=nextDoc()) != NO_MORE_DOCS) {
+        if (doc >= target) {
+          return docID = doc;
+        }
+      }
+      return docID = NO_MORE_DOCS;
+    }
+
+    @Override
+    public int nextPosition() throws IOException {
+      //System.out.println("PR d&p nextPosition posPending=" + posPending + " vs freq=" + freq);
+      
+      assert posPending > 0;
+      posPending--;
+
+      if (storePayloads) {
+        if (!payloadRetrieved) {
+          //System.out.println("PR     skip payload=" + payloadLength);
+          postings.skipBytes(payloadLength);
+        }
+        final int code = postings.readVInt();
+        //System.out.println("PR     code=" + code);
+        if ((code & 1) != 0) {
+          payloadLength = postings.readVInt();
+          //System.out.println("PR     new payload len=" + payloadLength);
+        }
+        position += code >>> 1;
+        payloadRetrieved = false;
+      } else {
+        position += postings.readVInt();
+      }
+      
+      if (storeOffsets) {
+        int offsetCode = postings.readVInt();
+        if ((offsetCode & 1) != 0) {
+          // new offset length
+          offsetLength = postings.readVInt();
+        }
+        startOffset += offsetCode >>> 1;
+      }
+
+      //System.out.println("PR d&p nextPos return pos=" + position + " this=" + this);
+      return position;
+    }
+
+    @Override
+    public int startOffset() {
+      return startOffset;
+    }
+
+    @Override
+    public int endOffset() {
+      return startOffset + offsetLength;
+    }
+
+    private void skipPositions() throws IOException {
+      while(posPending != 0) {
+        nextPosition();
+      }
+      if (storePayloads && !payloadRetrieved) {
+        //System.out.println("  skip payload len=" + payloadLength);
+        postings.skipBytes(payloadLength);
+        payloadRetrieved = true;
+      }
+    }
+
+    @Override
+    public BytesRef getPayload() throws IOException {
+      //System.out.println("PR  getPayload payloadLength=" + payloadLength + " this=" + this);
+      if (payloadRetrieved) {
+        return payload;
+      } else if (storePayloads && payloadLength > 0) {
+        payloadRetrieved = true;
+        if (payload == null) {
+          payload = new BytesRef(payloadLength);
+        } else {
+          payload.grow(payloadLength);
+        }
+        postings.readBytes(payload.bytes, 0, payloadLength);
+        payload.length = payloadLength;
+        return payload;
+      } else {
+        return null;
+      }
+    }
+  }
+
+  @Override
+  public void close() throws IOException {
+    wrappedPostingsReader.close();
+  }
+  
+  /** for a docsenum, gets the 'other' reused enum.
+   * Example: Pulsing(Standard).
+   * when doing a term range query you are switching back and forth
+   * between Pulsing and Standard
+   * 
+   * The way the reuse works is that Pulsing.other = Standard and
+   * Standard.other = Pulsing.
+   */
+  private DocsEnum getOther(DocsEnum de) {
+    if (de == null) {
+      return null;
+    } else {
+      final AttributeSource atts = de.attributes();
+      return atts.addAttribute(PulsingEnumAttribute.class).enums().get(this);
+    }
+  }
+  
+  /** 
+   * for a docsenum, sets the 'other' reused enum.
+   * see getOther for an example.
+   */
+  private DocsEnum setOther(DocsEnum de, DocsEnum other) {
+    final AttributeSource atts = de.attributes();
+    return atts.addAttribute(PulsingEnumAttribute.class).enums().put(this, other);
+  }
+
+  /** 
+   * A per-docsenum attribute that stores additional reuse information
+   * so that pulsing enums can keep a reference to their wrapped enums,
+   * and vice versa. this way we can always reuse.
+   * 
+   * @lucene.internal */
+  public static interface PulsingEnumAttribute extends Attribute {
+    public Map<PulsingPostingsReader,DocsEnum> enums();
+  }
+    
+  /** 
+   * Implementation of {@link PulsingEnumAttribute} for reuse of
+   * wrapped postings readers underneath pulsing.
+   * 
+   * @lucene.internal */
+  public static final class PulsingEnumAttributeImpl extends AttributeImpl implements PulsingEnumAttribute {
+    // we could store 'other', but what if someone 'chained' multiple postings readers,
+    // this could cause problems?
+    // TODO: we should consider nuking this map and just making it so if you do this,
+    // you don't reuse? and maybe pulsingPostingsReader should throw an exc if it wraps
+    // another pulsing, because this is just stupid and wasteful. 
+    // we still have to be careful in case someone does Pulsing(Stomping(Pulsing(...
+    private final Map<PulsingPostingsReader,DocsEnum> enums = 
+      new IdentityHashMap<PulsingPostingsReader,DocsEnum>();
+      
+    public Map<PulsingPostingsReader,DocsEnum> enums() {
+      return enums;
+    }
+
+    @Override
+    public void clear() {
+      // our state is per-docsenum, so this makes no sense.
+      // its best not to clear, in case a wrapped enum has a per-doc attribute or something
+      // and is calling clearAttributes(), so they don't nuke the reuse information!
+    }
+
+    @Override
+    public void copyTo(AttributeImpl target) {
+      // this makes no sense for us, because our state is per-docsenum.
+      // we don't want to copy any stuff over to another docsenum ever!
+    }
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/pulsing/PulsingPostingsWriter.java b/lucene/codecs/src/java/org/apache/lucene/codecs/pulsing/PulsingPostingsWriter.java
new file mode 100644
index 0000000..6ba0ef6
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/pulsing/PulsingPostingsWriter.java
@@ -0,0 +1,419 @@
+package org.apache.lucene.codecs.pulsing;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.List;
+import java.util.ArrayList;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.PostingsWriterBase;
+import org.apache.lucene.codecs.TermStats;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.store.RAMOutputStream;
+import org.apache.lucene.util.BytesRef;
+
+// TODO: we now inline based on total TF of the term,
+// but it might be better to inline by "net bytes used"
+// so that a term that has only 1 posting but a huge
+// payload would not be inlined.  Though this is
+// presumably rare in practice...
+
+/** 
+ * Writer for the pulsing format. 
+ * <p>
+ * Wraps another postings implementation and decides 
+ * (based on total number of occurrences), whether a terms 
+ * postings should be inlined into the term dictionary,
+ * or passed through to the wrapped writer.
+ *
+ * @lucene.experimental */
+public final class PulsingPostingsWriter extends PostingsWriterBase {
+
+  final static String CODEC = "PulsedPostingsWriter";
+
+  // To add a new version, increment from the last one, and
+  // change VERSION_CURRENT to point to your new version:
+  final static int VERSION_START = 0;
+
+  final static int VERSION_CURRENT = VERSION_START;
+
+  private IndexOutput termsOut;
+
+  private IndexOptions indexOptions;
+  private boolean storePayloads;
+
+  private static class PendingTerm {
+    private final byte[] bytes;
+    public PendingTerm(byte[] bytes) {
+      this.bytes = bytes;
+    }
+  }
+
+  private final List<PendingTerm> pendingTerms = new ArrayList<PendingTerm>();
+
+  // one entry per position
+  private final Position[] pending;
+  private int pendingCount = 0;                           // -1 once we've hit too many positions
+  private Position currentDoc;                    // first Position entry of current doc
+
+  private static final class Position {
+    BytesRef payload;
+    int termFreq;                                 // only incremented on first position for a given doc
+    int pos;
+    int docID;
+    int startOffset;
+    int endOffset;
+  }
+
+  // TODO: -- lazy init this?  ie, if every single term
+  // was inlined (eg for a "primary key" field) then we
+  // never need to use this fallback?  Fallback writer for
+  // non-inlined terms:
+  final PostingsWriterBase wrappedPostingsWriter;
+
+  /** If the total number of positions (summed across all docs
+   *  for this term) is <= maxPositions, then the postings are
+   *  inlined into terms dict */
+  public PulsingPostingsWriter(int maxPositions, PostingsWriterBase wrappedPostingsWriter) {
+    pending = new Position[maxPositions];
+    for(int i=0;i<maxPositions;i++) {
+      pending[i] = new Position();
+    }
+
+    // We simply wrap another postings writer, but only call
+    // on it when tot positions is >= the cutoff:
+    this.wrappedPostingsWriter = wrappedPostingsWriter;
+  }
+
+  @Override
+  public void start(IndexOutput termsOut) throws IOException {
+    this.termsOut = termsOut;
+    CodecUtil.writeHeader(termsOut, CODEC, VERSION_CURRENT);
+    termsOut.writeVInt(pending.length); // encode maxPositions in header
+    wrappedPostingsWriter.start(termsOut);
+  }
+
+  @Override
+  public void startTerm() {
+    //if (DEBUG) System.out.println("PW   startTerm");
+    assert pendingCount == 0;
+  }
+
+  // TODO: -- should we NOT reuse across fields?  would
+  // be cleaner
+
+  // Currently, this instance is re-used across fields, so
+  // our parent calls setField whenever the field changes
+  @Override
+  public void setField(FieldInfo fieldInfo) {
+    this.indexOptions = fieldInfo.getIndexOptions();
+    //if (DEBUG) System.out.println("PW field=" + fieldInfo.name + " indexOptions=" + indexOptions);
+    storePayloads = fieldInfo.hasPayloads();
+    wrappedPostingsWriter.setField(fieldInfo);
+    //DEBUG = BlockTreeTermsWriter.DEBUG;
+  }
+
+  private boolean DEBUG;
+
+  @Override
+  public void startDoc(int docID, int termDocFreq) throws IOException {
+    assert docID >= 0: "got docID=" + docID;
+
+    /*
+    if (termID != -1) {
+      if (docID == 0) {
+        baseDocID = termID;
+      } else if (baseDocID + docID != termID) {
+        throw new RuntimeException("WRITE: baseDocID=" + baseDocID + " docID=" + docID + " termID=" + termID);
+      }
+    }
+    */
+
+    //if (DEBUG) System.out.println("PW     doc=" + docID);
+
+    if (pendingCount == pending.length) {
+      push();
+      //if (DEBUG) System.out.println("PW: wrapped.finishDoc");
+      wrappedPostingsWriter.finishDoc();
+    }
+
+    if (pendingCount != -1) {
+      assert pendingCount < pending.length;
+      currentDoc = pending[pendingCount];
+      currentDoc.docID = docID;
+      if (indexOptions == IndexOptions.DOCS_ONLY) {
+        pendingCount++;
+      } else if (indexOptions == IndexOptions.DOCS_AND_FREQS) { 
+        pendingCount++;
+        currentDoc.termFreq = termDocFreq;
+      } else {
+        currentDoc.termFreq = termDocFreq;
+      }
+    } else {
+      // We've already seen too many docs for this term --
+      // just forward to our fallback writer
+      wrappedPostingsWriter.startDoc(docID, termDocFreq);
+    }
+  }
+
+  @Override
+  public void addPosition(int position, BytesRef payload, int startOffset, int endOffset) throws IOException {
+
+    //if (DEBUG) System.out.println("PW       pos=" + position + " payload=" + (payload == null ? "null" : payload.length + " bytes"));
+    if (pendingCount == pending.length) {
+      push();
+    }
+
+    if (pendingCount == -1) {
+      // We've already seen too many docs for this term --
+      // just forward to our fallback writer
+      wrappedPostingsWriter.addPosition(position, payload, startOffset, endOffset);
+    } else {
+      // buffer up
+      final Position pos = pending[pendingCount++];
+      pos.pos = position;
+      pos.startOffset = startOffset;
+      pos.endOffset = endOffset;
+      pos.docID = currentDoc.docID;
+      if (payload != null && payload.length > 0) {
+        if (pos.payload == null) {
+          pos.payload = BytesRef.deepCopyOf(payload);
+        } else {
+          pos.payload.copyBytes(payload);
+        }
+      } else if (pos.payload != null) {
+        pos.payload.length = 0;
+      }
+    }
+  }
+
+  @Override
+  public void finishDoc() throws IOException {
+    // if (DEBUG) System.out.println("PW     finishDoc");
+    if (pendingCount == -1) {
+      wrappedPostingsWriter.finishDoc();
+    }
+  }
+
+  private final RAMOutputStream buffer = new RAMOutputStream();
+
+  // private int baseDocID;
+
+  /** Called when we are done adding docs to this term */
+  @Override
+  public void finishTerm(TermStats stats) throws IOException {
+    // if (DEBUG) System.out.println("PW   finishTerm docCount=" + stats.docFreq + " pendingCount=" + pendingCount + " pendingTerms.size()=" + pendingTerms.size());
+
+    assert pendingCount > 0 || pendingCount == -1;
+
+    if (pendingCount == -1) {
+      wrappedPostingsWriter.finishTerm(stats);
+      // Must add null entry to record terms that our
+      // wrapped postings impl added
+      pendingTerms.add(null);
+    } else {
+
+      // There were few enough total occurrences for this
+      // term, so we fully inline our postings data into
+      // terms dict, now:
+
+      // TODO: it'd be better to share this encoding logic
+      // in some inner codec that knows how to write a
+      // single doc / single position, etc.  This way if a
+      // given codec wants to store other interesting
+      // stuff, it could use this pulsing codec to do so
+
+      if (indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0) {
+        int lastDocID = 0;
+        int pendingIDX = 0;
+        int lastPayloadLength = -1;
+        int lastOffsetLength = -1;
+        while(pendingIDX < pendingCount) {
+          final Position doc = pending[pendingIDX];
+
+          final int delta = doc.docID - lastDocID;
+          lastDocID = doc.docID;
+
+          // if (DEBUG) System.out.println("  write doc=" + doc.docID + " freq=" + doc.termFreq);
+
+          if (doc.termFreq == 1) {
+            buffer.writeVInt((delta<<1)|1);
+          } else {
+            buffer.writeVInt(delta<<1);
+            buffer.writeVInt(doc.termFreq);
+          }
+
+          int lastPos = 0;
+          int lastOffset = 0;
+          for(int posIDX=0;posIDX<doc.termFreq;posIDX++) {
+            final Position pos = pending[pendingIDX++];
+            assert pos.docID == doc.docID;
+            final int posDelta = pos.pos - lastPos;
+            lastPos = pos.pos;
+            // if (DEBUG) System.out.println("    write pos=" + pos.pos);
+            final int payloadLength = pos.payload == null ? 0 : pos.payload.length;
+            if (storePayloads) {
+              if (payloadLength != lastPayloadLength) {
+                buffer.writeVInt((posDelta << 1)|1);
+                buffer.writeVInt(payloadLength);
+                lastPayloadLength = payloadLength;
+              } else {
+                buffer.writeVInt(posDelta << 1);
+              }
+            } else {
+              buffer.writeVInt(posDelta);
+            }
+            
+            if (indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0) {
+              //System.out.println("write=" + pos.startOffset + "," + pos.endOffset);
+              int offsetDelta = pos.startOffset - lastOffset;
+              int offsetLength = pos.endOffset - pos.startOffset;
+              if (offsetLength != lastOffsetLength) {
+                buffer.writeVInt(offsetDelta << 1 | 1);
+                buffer.writeVInt(offsetLength);
+              } else {
+                buffer.writeVInt(offsetDelta << 1);
+              }
+              lastOffset = pos.startOffset;
+              lastOffsetLength = offsetLength;             
+            }
+            
+            if (payloadLength > 0) {
+              assert storePayloads;
+              buffer.writeBytes(pos.payload.bytes, 0, pos.payload.length);
+            }
+          }
+        }
+      } else if (indexOptions == IndexOptions.DOCS_AND_FREQS) {
+        int lastDocID = 0;
+        for(int posIDX=0;posIDX<pendingCount;posIDX++) {
+          final Position doc = pending[posIDX];
+          final int delta = doc.docID - lastDocID;
+          assert doc.termFreq != 0;
+          if (doc.termFreq == 1) {
+            buffer.writeVInt((delta<<1)|1);
+          } else {
+            buffer.writeVInt(delta<<1);
+            buffer.writeVInt(doc.termFreq);
+          }
+          lastDocID = doc.docID;
+        }
+      } else if (indexOptions == IndexOptions.DOCS_ONLY) {
+        int lastDocID = 0;
+        for(int posIDX=0;posIDX<pendingCount;posIDX++) {
+          final Position doc = pending[posIDX];
+          buffer.writeVInt(doc.docID - lastDocID);
+          lastDocID = doc.docID;
+        }
+      }
+
+      final byte[] bytes = new byte[(int) buffer.getFilePointer()];
+      buffer.writeTo(bytes, 0);
+      pendingTerms.add(new PendingTerm(bytes));
+      buffer.reset();
+    }
+
+    pendingCount = 0;
+  }
+
+  @Override
+  public void close() throws IOException {
+    wrappedPostingsWriter.close();
+  }
+
+  @Override
+  public void flushTermsBlock(int start, int count) throws IOException {
+    // if (DEBUG) System.out.println("PW: flushTermsBlock start=" + start + " count=" + count + " pendingTerms.size()=" + pendingTerms.size());
+    int wrappedCount = 0;
+    assert buffer.getFilePointer() == 0;
+    assert start >= count;
+
+    final int limit = pendingTerms.size() - start + count;
+
+    for(int idx=pendingTerms.size()-start; idx<limit; idx++) {
+      final PendingTerm term = pendingTerms.get(idx);
+      if (term == null) {
+        wrappedCount++;
+      } else {
+        buffer.writeVInt(term.bytes.length);
+        buffer.writeBytes(term.bytes, 0, term.bytes.length);
+      }
+    }
+
+    termsOut.writeVInt((int) buffer.getFilePointer());
+    buffer.writeTo(termsOut);
+    buffer.reset();
+
+    // TDOO: this could be somewhat costly since
+    // pendingTerms.size() could be biggish?
+    int futureWrappedCount = 0;
+    final int limit2 = pendingTerms.size();
+    for(int idx=limit;idx<limit2;idx++) {
+      if (pendingTerms.get(idx) == null) {
+        futureWrappedCount++;
+      }
+    }
+
+    // Remove the terms we just wrote:
+    pendingTerms.subList(pendingTerms.size()-start, limit).clear();
+
+    // if (DEBUG) System.out.println("PW:   len=" + buffer.getFilePointer() + " fp=" + termsOut.getFilePointer() + " futureWrappedCount=" + futureWrappedCount + " wrappedCount=" + wrappedCount);
+    // TODO: can we avoid calling this if all terms
+    // were inlined...?  Eg for a "primary key" field, the
+    // wrapped codec is never invoked...
+    wrappedPostingsWriter.flushTermsBlock(futureWrappedCount+wrappedCount, wrappedCount);
+  }
+
+  // Pushes pending positions to the wrapped codec
+  private void push() throws IOException {
+    // if (DEBUG) System.out.println("PW now push @ " + pendingCount + " wrapped=" + wrappedPostingsWriter);
+    assert pendingCount == pending.length;
+      
+    wrappedPostingsWriter.startTerm();
+      
+    // Flush all buffered docs
+    if (indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0) {
+      Position doc = null;
+      for(Position pos : pending) {
+        if (doc == null) {
+          doc = pos;
+          // if (DEBUG) System.out.println("PW: wrapped.startDoc docID=" + doc.docID + " tf=" + doc.termFreq);
+          wrappedPostingsWriter.startDoc(doc.docID, doc.termFreq);
+        } else if (doc.docID != pos.docID) {
+          assert pos.docID > doc.docID;
+          // if (DEBUG) System.out.println("PW: wrapped.finishDoc");
+          wrappedPostingsWriter.finishDoc();
+          doc = pos;
+          // if (DEBUG) System.out.println("PW: wrapped.startDoc docID=" + doc.docID + " tf=" + doc.termFreq);
+          wrappedPostingsWriter.startDoc(doc.docID, doc.termFreq);
+        }
+        // if (DEBUG) System.out.println("PW:   wrapped.addPos pos=" + pos.pos);
+        wrappedPostingsWriter.addPosition(pos.pos, pos.payload, pos.startOffset, pos.endOffset);
+      }
+      //wrappedPostingsWriter.finishDoc();
+    } else {
+      for(Position doc : pending) {
+        wrappedPostingsWriter.startDoc(doc.docID, indexOptions == IndexOptions.DOCS_ONLY ? 0 : doc.termFreq);
+      }
+    }
+    pendingCount = -1;
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/pulsing/package.html b/lucene/codecs/src/java/org/apache/lucene/codecs/pulsing/package.html
new file mode 100644
index 0000000..4216cc6
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/pulsing/package.html
@@ -0,0 +1,25 @@
+<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+<html>
+<head>
+   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
+</head>
+<body>
+Pulsing Codec: inlines low frequency terms' postings into terms dictionary.
+</body>
+</html>
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/sep/IntIndexInput.java b/lucene/codecs/src/java/org/apache/lucene/codecs/sep/IntIndexInput.java
new file mode 100644
index 0000000..93640e0
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/sep/IntIndexInput.java
@@ -0,0 +1,58 @@
+package org.apache.lucene.codecs.sep;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.Closeable;
+import java.io.IOException;
+
+import org.apache.lucene.store.DataInput;
+
+/** Defines basic API for writing ints to an IndexOutput.
+ *  IntBlockCodec interacts with this API. @see
+ *  IntBlockReader
+ *
+ * @lucene.experimental */
+public abstract class IntIndexInput implements Closeable {
+
+  public abstract Reader reader() throws IOException;
+
+  public abstract void close() throws IOException;
+
+  public abstract Index index() throws IOException;
+  
+  /** Records a single skip-point in the {@link IntIndexInput.Reader}. */
+  public abstract static class Index {
+
+    public abstract void read(DataInput indexIn, boolean absolute) throws IOException;
+
+    /** Seeks primary stream to the last read offset */
+    public abstract void seek(IntIndexInput.Reader stream) throws IOException;
+
+    public abstract void copyFrom(Index other);
+    
+    @Override
+    public abstract Index clone();
+  }
+
+  /** Reads int values. */
+  public abstract static class Reader {
+
+    /** Reads next single int */
+    public abstract int next() throws IOException;
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/sep/IntIndexOutput.java b/lucene/codecs/src/java/org/apache/lucene/codecs/sep/IntIndexOutput.java
new file mode 100644
index 0000000..fd1eb49
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/sep/IntIndexOutput.java
@@ -0,0 +1,60 @@
+package org.apache.lucene.codecs.sep;
+
+/**
+ * LICENSED to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+// TODO: we may want tighter integration w/ IndexOutput --
+// may give better perf:
+
+import org.apache.lucene.store.IndexOutput;
+
+import java.io.IOException;
+import java.io.Closeable;
+
+/** Defines basic API for writing ints to an IndexOutput.
+ *  IntBlockCodec interacts with this API. @see
+ *  IntBlockReader.
+ *
+ * <p>NOTE: block sizes could be variable
+ *
+ * @lucene.experimental */
+public abstract class IntIndexOutput implements Closeable {
+
+  /** Write an int to the primary file.  The value must be
+   * >= 0.  */
+  public abstract void write(int v) throws IOException;
+
+  /** Records a single skip-point in the IndexOutput. */
+  public abstract static class Index {
+
+    /** Internally records the current location */
+    public abstract void mark() throws IOException;
+
+    /** Copies index from other */
+    public abstract void copyFrom(Index other, boolean copyLast) throws IOException;
+
+    /** Writes "location" of current output pointer of primary
+     *  output to different output (out) */
+    public abstract void write(IndexOutput indexOut, boolean absolute) throws IOException;
+  }
+
+  /** If you are indexing the primary output file, call
+   *  this and interact with the returned IndexWriter. */
+  public abstract Index index() throws IOException;
+
+  public abstract void close() throws IOException;
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/sep/IntStreamFactory.java b/lucene/codecs/src/java/org/apache/lucene/codecs/sep/IntStreamFactory.java
new file mode 100644
index 0000000..eace033
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/sep/IntStreamFactory.java
@@ -0,0 +1,36 @@
+package org.apache.lucene.codecs.sep;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+
+import java.io.IOException;
+
+/** Provides int reader and writer to specified files.
+ *
+ * @lucene.experimental */
+public abstract class IntStreamFactory {
+  /** Create an {@link IntIndexInput} on the provided
+   *  fileName. */
+  public abstract IntIndexInput openInput(Directory dir, String fileName, IOContext context) throws IOException;
+
+  /** Create an {@link IntIndexOutput} on the provided
+   *  fileName. */
+  public abstract IntIndexOutput createOutput(Directory dir, String fileName, IOContext context) throws IOException;
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/sep/SepDocValuesConsumer.java b/lucene/codecs/src/java/org/apache/lucene/codecs/sep/SepDocValuesConsumer.java
new file mode 100644
index 0000000..d5dbb4e
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/sep/SepDocValuesConsumer.java
@@ -0,0 +1,47 @@
+package org.apache.lucene.codecs.sep;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.codecs.lucene40.values.DocValuesWriterBase;
+import org.apache.lucene.index.PerDocWriteState;
+import org.apache.lucene.store.Directory;
+
+/**
+ * Implementation of PerDocConsumer that uses separate files.
+ * @lucene.experimental
+ */
+
+public class SepDocValuesConsumer extends DocValuesWriterBase {
+  private final Directory directory;
+
+  public SepDocValuesConsumer(PerDocWriteState state) {
+    super(state);
+    this.directory = state.directory;
+  }
+  
+  @Override
+  protected Directory getDirectory() {
+    return directory;
+  }
+
+  @Override
+  public void abort() {
+    // We don't have to remove files here: IndexFileDeleter
+    // will do so
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/sep/SepDocValuesProducer.java b/lucene/codecs/src/java/org/apache/lucene/codecs/sep/SepDocValuesProducer.java
new file mode 100644
index 0000000..4965c1d
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/sep/SepDocValuesProducer.java
@@ -0,0 +1,91 @@
+package org.apache.lucene.codecs.sep;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+import java.io.Closeable;
+import java.io.IOException;
+import java.util.Collection;
+import java.util.Map;
+import java.util.TreeMap;
+
+import org.apache.lucene.codecs.PerDocProducerBase;
+import org.apache.lucene.codecs.lucene40.values.Bytes;
+import org.apache.lucene.codecs.lucene40.values.Floats;
+import org.apache.lucene.codecs.lucene40.values.Ints;
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.DocValues.Type;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.util.IOUtils;
+
+/**
+ * Implementation of PerDocProducer that uses separate files.
+ * @lucene.experimental
+ */
+public class SepDocValuesProducer extends PerDocProducerBase {
+  private final TreeMap<String, DocValues> docValues;
+
+  /**
+   * Creates a new {@link SepDocValuesProducer} instance and loads all
+   * {@link DocValues} instances for this segment and codec.
+   */
+  public SepDocValuesProducer(SegmentReadState state) throws IOException {
+    docValues = load(state.fieldInfos, state.segmentInfo.name, state.segmentInfo.getDocCount(), state.dir, state.context);
+  }
+  
+  @Override
+  protected Map<String,DocValues> docValues() {
+    return docValues;
+  }
+  
+  @Override
+  protected void closeInternal(Collection<? extends Closeable> closeables) throws IOException {
+    IOUtils.close(closeables);
+  }
+
+  @Override
+  protected DocValues loadDocValues(int docCount, Directory dir, String id,
+      Type type, IOContext context) throws IOException {
+      switch (type) {
+      case FIXED_INTS_16:
+      case FIXED_INTS_32:
+      case FIXED_INTS_64:
+      case FIXED_INTS_8:
+      case VAR_INTS:
+        return Ints.getValues(dir, id, docCount, type, context);
+      case FLOAT_32:
+        return Floats.getValues(dir, id, docCount, context, type);
+      case FLOAT_64:
+        return Floats.getValues(dir, id, docCount, context, type);
+      case BYTES_FIXED_STRAIGHT:
+        return Bytes.getValues(dir, id, Bytes.Mode.STRAIGHT, true, docCount, getComparator(), context);
+      case BYTES_FIXED_DEREF:
+        return Bytes.getValues(dir, id, Bytes.Mode.DEREF, true, docCount, getComparator(), context);
+      case BYTES_FIXED_SORTED:
+        return Bytes.getValues(dir, id, Bytes.Mode.SORTED, true, docCount, getComparator(), context);
+      case BYTES_VAR_STRAIGHT:
+        return Bytes.getValues(dir, id, Bytes.Mode.STRAIGHT, false, docCount, getComparator(), context);
+      case BYTES_VAR_DEREF:
+        return Bytes.getValues(dir, id, Bytes.Mode.DEREF, false, docCount, getComparator(), context);
+      case BYTES_VAR_SORTED:
+        return Bytes.getValues(dir, id, Bytes.Mode.SORTED, false, docCount, getComparator(), context);
+      default:
+        throw new IllegalStateException("unrecognized index values mode " + type);
+      }
+    }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/sep/SepPostingsReader.java b/lucene/codecs/src/java/org/apache/lucene/codecs/sep/SepPostingsReader.java
new file mode 100644
index 0000000..c76de76
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/sep/SepPostingsReader.java
@@ -0,0 +1,743 @@
+package org.apache.lucene.codecs.sep;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.BlockTermState;
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.PostingsReaderBase;
+import org.apache.lucene.index.DocsAndPositionsEnum;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.TermState;
+import org.apache.lucene.store.ByteArrayDataInput;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+
+/** Concrete class that reads the current doc/freq/skip
+ *  postings format.    
+ *
+ * @lucene.experimental
+ */
+
+// TODO: -- should we switch "hasProx" higher up?  and
+// create two separate docs readers, one that also reads
+// prox and one that doesn't?
+
+public class SepPostingsReader extends PostingsReaderBase {
+
+  final IntIndexInput freqIn;
+  final IntIndexInput docIn;
+  final IntIndexInput posIn;
+  final IndexInput payloadIn;
+  final IndexInput skipIn;
+
+  int skipInterval;
+  int maxSkipLevels;
+  int skipMinimum;
+
+  public SepPostingsReader(Directory dir, FieldInfos fieldInfos, SegmentInfo segmentInfo, IOContext context, IntStreamFactory intFactory, String segmentSuffix) throws IOException {
+    boolean success = false;
+    try {
+
+      final String docFileName = IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, SepPostingsWriter.DOC_EXTENSION);
+      docIn = intFactory.openInput(dir, docFileName, context);
+
+      skipIn = dir.openInput(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, SepPostingsWriter.SKIP_EXTENSION), context);
+
+      if (fieldInfos.hasFreq()) {
+        freqIn = intFactory.openInput(dir, IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, SepPostingsWriter.FREQ_EXTENSION), context);        
+      } else {
+        freqIn = null;
+      }
+      if (fieldInfos.hasProx()) {
+        posIn = intFactory.openInput(dir, IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, SepPostingsWriter.POS_EXTENSION), context);
+        payloadIn = dir.openInput(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, SepPostingsWriter.PAYLOAD_EXTENSION), context);
+      } else {
+        posIn = null;
+        payloadIn = null;
+      }
+      success = true;
+    } finally {
+      if (!success) {
+        close();
+      }
+    }
+  }
+
+  @Override
+  public void init(IndexInput termsIn) throws IOException {
+    // Make sure we are talking to the matching past writer
+    CodecUtil.checkHeader(termsIn, SepPostingsWriter.CODEC,
+      SepPostingsWriter.VERSION_START, SepPostingsWriter.VERSION_START);
+    skipInterval = termsIn.readInt();
+    maxSkipLevels = termsIn.readInt();
+    skipMinimum = termsIn.readInt();
+  }
+
+  @Override
+  public void close() throws IOException {
+    try {
+      if (freqIn != null)
+        freqIn.close();
+    } finally {
+      try {
+        if (docIn != null)
+          docIn.close();
+      } finally {
+        try {
+          if (skipIn != null)
+            skipIn.close();
+        } finally {
+          try {
+            if (posIn != null) {
+              posIn.close();
+            }
+          } finally {
+            if (payloadIn != null) {
+              payloadIn.close();
+            }
+          }
+        }
+      }
+    }
+  }
+
+  private static final class SepTermState extends BlockTermState {
+    // We store only the seek point to the docs file because
+    // the rest of the info (freqIndex, posIndex, etc.) is
+    // stored in the docs file:
+    IntIndexInput.Index docIndex;
+    IntIndexInput.Index posIndex;
+    IntIndexInput.Index freqIndex;
+    long payloadFP;
+    long skipFP;
+
+    // Only used for "primary" term state; these are never
+    // copied on clone:
+    
+    // TODO: these should somehow be stored per-TermsEnum
+    // not per TermState; maybe somehow the terms dict
+    // should load/manage the byte[]/DataReader for us?
+    byte[] bytes;
+    ByteArrayDataInput bytesReader;
+
+    @Override
+    public SepTermState clone() {
+      SepTermState other = new SepTermState();
+      other.copyFrom(this);
+      return other;
+    }
+
+    @Override
+    public void copyFrom(TermState _other) {
+      super.copyFrom(_other);
+      SepTermState other = (SepTermState) _other;
+      if (docIndex == null) {
+        docIndex = other.docIndex.clone();
+      } else {
+        docIndex.copyFrom(other.docIndex);
+      }
+      if (other.freqIndex != null) {
+        if (freqIndex == null) {
+          freqIndex = other.freqIndex.clone();
+        } else {
+          freqIndex.copyFrom(other.freqIndex);
+        }
+      } else {
+        freqIndex = null;
+      }
+      if (other.posIndex != null) {
+        if (posIndex == null) {
+          posIndex = other.posIndex.clone();
+        } else {
+          posIndex.copyFrom(other.posIndex);
+        }
+      } else {
+        posIndex = null;
+      }
+      payloadFP = other.payloadFP;
+      skipFP = other.skipFP;
+    }
+
+    @Override
+    public String toString() {
+      return super.toString() + " docIndex=" + docIndex + " freqIndex=" + freqIndex + " posIndex=" + posIndex + " payloadFP=" + payloadFP + " skipFP=" + skipFP;
+    }
+  }
+
+  @Override
+  public BlockTermState newTermState() throws IOException {
+    final SepTermState state = new SepTermState();
+    state.docIndex = docIn.index();
+    if (freqIn != null) {
+      state.freqIndex = freqIn.index();
+    }
+    if (posIn != null) {
+      state.posIndex = posIn.index();
+    }
+    return state;
+  }
+
+  @Override
+  public void readTermsBlock(IndexInput termsIn, FieldInfo fieldInfo, BlockTermState _termState) throws IOException {
+    final SepTermState termState = (SepTermState) _termState;
+    //System.out.println("SEPR: readTermsBlock termsIn.fp=" + termsIn.getFilePointer());
+    final int len = termsIn.readVInt();
+    //System.out.println("  numBytes=" + len);
+    if (termState.bytes == null) {
+      termState.bytes = new byte[ArrayUtil.oversize(len, 1)];
+      termState.bytesReader = new ByteArrayDataInput(termState.bytes);
+    } else if (termState.bytes.length < len) {
+      termState.bytes = new byte[ArrayUtil.oversize(len, 1)];
+    }
+    termState.bytesReader.reset(termState.bytes, 0, len);
+    termsIn.readBytes(termState.bytes, 0, len);
+  }
+
+  @Override
+  public void nextTerm(FieldInfo fieldInfo, BlockTermState _termState) throws IOException {
+    final SepTermState termState = (SepTermState) _termState;
+    final boolean isFirstTerm = termState.termBlockOrd == 0;
+    //System.out.println("SEPR.nextTerm termCount=" + termState.termBlockOrd + " isFirstTerm=" + isFirstTerm + " bytesReader.pos=" + termState.bytesReader.getPosition());
+    //System.out.println("  docFreq=" + termState.docFreq);
+    termState.docIndex.read(termState.bytesReader, isFirstTerm);
+    //System.out.println("  docIndex=" + termState.docIndex);
+    if (fieldInfo.getIndexOptions() != IndexOptions.DOCS_ONLY) {
+      termState.freqIndex.read(termState.bytesReader, isFirstTerm);
+      if (fieldInfo.getIndexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
+        //System.out.println("  freqIndex=" + termState.freqIndex);
+        termState.posIndex.read(termState.bytesReader, isFirstTerm);
+        //System.out.println("  posIndex=" + termState.posIndex);
+        if (fieldInfo.hasPayloads()) {
+          if (isFirstTerm) {
+            termState.payloadFP = termState.bytesReader.readVLong();
+          } else {
+            termState.payloadFP += termState.bytesReader.readVLong();
+          }
+          //System.out.println("  payloadFP=" + termState.payloadFP);
+        }
+      }
+    }
+
+    if (termState.docFreq >= skipMinimum) {
+      //System.out.println("   readSkip @ " + termState.bytesReader.getPosition());
+      if (isFirstTerm) {
+        termState.skipFP = termState.bytesReader.readVLong();
+      } else {
+        termState.skipFP += termState.bytesReader.readVLong();
+      }
+      //System.out.println("  skipFP=" + termState.skipFP);
+    } else if (isFirstTerm) {
+      termState.skipFP = 0;
+    }
+  }
+
+  @Override
+  public DocsEnum docs(FieldInfo fieldInfo, BlockTermState _termState, Bits liveDocs, DocsEnum reuse, int flags) throws IOException {
+    final SepTermState termState = (SepTermState) _termState;
+    SepDocsEnum docsEnum;
+    if (reuse == null || !(reuse instanceof SepDocsEnum)) {
+      docsEnum = new SepDocsEnum();
+    } else {
+      docsEnum = (SepDocsEnum) reuse;
+      if (docsEnum.startDocIn != docIn) {
+        // If you are using ParellelReader, and pass in a
+        // reused DocsAndPositionsEnum, it could have come
+        // from another reader also using sep codec
+        docsEnum = new SepDocsEnum();        
+      }
+    }
+
+    return docsEnum.init(fieldInfo, termState, liveDocs);
+  }
+
+  @Override
+  public DocsAndPositionsEnum docsAndPositions(FieldInfo fieldInfo, BlockTermState _termState, Bits liveDocs,
+                                               DocsAndPositionsEnum reuse, int flags)
+    throws IOException {
+
+    assert fieldInfo.getIndexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;
+    final SepTermState termState = (SepTermState) _termState;
+    SepDocsAndPositionsEnum postingsEnum;
+    if (reuse == null || !(reuse instanceof SepDocsAndPositionsEnum)) {
+      postingsEnum = new SepDocsAndPositionsEnum();
+    } else {
+      postingsEnum = (SepDocsAndPositionsEnum) reuse;
+      if (postingsEnum.startDocIn != docIn) {
+        // If you are using ParellelReader, and pass in a
+        // reused DocsAndPositionsEnum, it could have come
+        // from another reader also using sep codec
+        postingsEnum = new SepDocsAndPositionsEnum();        
+      }
+    }
+
+    return postingsEnum.init(fieldInfo, termState, liveDocs);
+  }
+
+  class SepDocsEnum extends DocsEnum {
+    int docFreq;
+    int doc = -1;
+    int accum;
+    int count;
+    int freq;
+    long freqStart;
+
+    // TODO: -- should we do omitTF with 2 different enum classes?
+    private boolean omitTF;
+    private IndexOptions indexOptions;
+    private boolean storePayloads;
+    private Bits liveDocs;
+    private final IntIndexInput.Reader docReader;
+    private final IntIndexInput.Reader freqReader;
+    private long skipFP;
+
+    private final IntIndexInput.Index docIndex;
+    private final IntIndexInput.Index freqIndex;
+    private final IntIndexInput.Index posIndex;
+    private final IntIndexInput startDocIn;
+
+    // TODO: -- should we do hasProx with 2 different enum classes?
+
+    boolean skipped;
+    SepSkipListReader skipper;
+
+    SepDocsEnum() throws IOException {
+      startDocIn = docIn;
+      docReader = docIn.reader();
+      docIndex = docIn.index();
+      if (freqIn != null) {
+        freqReader = freqIn.reader();
+        freqIndex = freqIn.index();
+      } else {
+        freqReader = null;
+        freqIndex = null;
+      }
+      if (posIn != null) {
+        posIndex = posIn.index();                 // only init this so skipper can read it
+      } else {
+        posIndex = null;
+      }
+    }
+
+    SepDocsEnum init(FieldInfo fieldInfo, SepTermState termState, Bits liveDocs) throws IOException {
+      this.liveDocs = liveDocs;
+      this.indexOptions = fieldInfo.getIndexOptions();
+      omitTF = indexOptions == IndexOptions.DOCS_ONLY;
+      storePayloads = fieldInfo.hasPayloads();
+
+      // TODO: can't we only do this if consumer
+      // skipped consuming the previous docs?
+      docIndex.copyFrom(termState.docIndex);
+      docIndex.seek(docReader);
+
+      if (!omitTF) {
+        freqIndex.copyFrom(termState.freqIndex);
+        freqIndex.seek(freqReader);
+      }
+
+      docFreq = termState.docFreq;
+      // NOTE: unused if docFreq < skipMinimum:
+      skipFP = termState.skipFP;
+      count = 0;
+      doc = -1;
+      accum = 0;
+      freq = 1;
+      skipped = false;
+
+      return this;
+    }
+
+    @Override
+    public int nextDoc() throws IOException {
+
+      while(true) {
+        if (count == docFreq) {
+          return doc = NO_MORE_DOCS;
+        }
+
+        count++;
+
+        // Decode next doc
+        //System.out.println("decode docDelta:");
+        accum += docReader.next();
+          
+        if (!omitTF) {
+          //System.out.println("decode freq:");
+          freq = freqReader.next();
+        }
+
+        if (liveDocs == null || liveDocs.get(accum)) {
+          break;
+        }
+      }
+      return (doc = accum);
+    }
+
+    @Override
+    public int freq() throws IOException {
+      return freq;
+    }
+
+    @Override
+    public int docID() {
+      return doc;
+    }
+
+    @Override
+    public int advance(int target) throws IOException {
+
+      if ((target - skipInterval) >= doc && docFreq >= skipMinimum) {
+
+        // There are enough docs in the posting to have
+        // skip data, and its not too close
+
+        if (skipper == null) {
+          // This DocsEnum has never done any skipping
+          skipper = new SepSkipListReader(skipIn.clone(),
+                                          freqIn,
+                                          docIn,
+                                          posIn,
+                                          maxSkipLevels, skipInterval);
+
+        }
+
+        if (!skipped) {
+          // We haven't yet skipped for this posting
+          skipper.init(skipFP,
+                       docIndex,
+                       freqIndex,
+                       posIndex,
+                       0,
+                       docFreq,
+                       storePayloads);
+          skipper.setIndexOptions(indexOptions);
+
+          skipped = true;
+        }
+
+        final int newCount = skipper.skipTo(target); 
+
+        if (newCount > count) {
+
+          // Skipper did move
+          if (!omitTF) {
+            skipper.getFreqIndex().seek(freqReader);
+          }
+          skipper.getDocIndex().seek(docReader);
+          count = newCount;
+          doc = accum = skipper.getDoc();
+        }
+      }
+        
+      // Now, linear scan for the rest:
+      do {
+        if (nextDoc() == NO_MORE_DOCS) {
+          return NO_MORE_DOCS;
+        }
+      } while (target > doc);
+
+      return doc;
+    }
+  }
+
+  class SepDocsAndPositionsEnum extends DocsAndPositionsEnum {
+    int docFreq;
+    int doc = -1;
+    int accum;
+    int count;
+    int freq;
+    long freqStart;
+
+    private boolean storePayloads;
+    private Bits liveDocs;
+    private final IntIndexInput.Reader docReader;
+    private final IntIndexInput.Reader freqReader;
+    private final IntIndexInput.Reader posReader;
+    private final IndexInput payloadIn;
+    private long skipFP;
+
+    private final IntIndexInput.Index docIndex;
+    private final IntIndexInput.Index freqIndex;
+    private final IntIndexInput.Index posIndex;
+    private final IntIndexInput startDocIn;
+
+    private long payloadFP;
+
+    private int pendingPosCount;
+    private int position;
+    private int payloadLength;
+    private long pendingPayloadBytes;
+
+    private boolean skipped;
+    private SepSkipListReader skipper;
+    private boolean payloadPending;
+    private boolean posSeekPending;
+
+    SepDocsAndPositionsEnum() throws IOException {
+      startDocIn = docIn;
+      docReader = docIn.reader();
+      docIndex = docIn.index();
+      freqReader = freqIn.reader();
+      freqIndex = freqIn.index();
+      posReader = posIn.reader();
+      posIndex = posIn.index();
+      payloadIn = SepPostingsReader.this.payloadIn.clone();
+    }
+
+    SepDocsAndPositionsEnum init(FieldInfo fieldInfo, SepTermState termState, Bits liveDocs) throws IOException {
+      this.liveDocs = liveDocs;
+      storePayloads = fieldInfo.hasPayloads();
+      //System.out.println("Sep D&P init");
+
+      // TODO: can't we only do this if consumer
+      // skipped consuming the previous docs?
+      docIndex.copyFrom(termState.docIndex);
+      docIndex.seek(docReader);
+      //System.out.println("  docIndex=" + docIndex);
+
+      freqIndex.copyFrom(termState.freqIndex);
+      freqIndex.seek(freqReader);
+      //System.out.println("  freqIndex=" + freqIndex);
+
+      posIndex.copyFrom(termState.posIndex);
+      //System.out.println("  posIndex=" + posIndex);
+      posSeekPending = true;
+      payloadPending = false;
+
+      payloadFP = termState.payloadFP;
+      skipFP = termState.skipFP;
+      //System.out.println("  skipFP=" + skipFP);
+
+      docFreq = termState.docFreq;
+      count = 0;
+      doc = -1;
+      accum = 0;
+      pendingPosCount = 0;
+      pendingPayloadBytes = 0;
+      skipped = false;
+
+      return this;
+    }
+
+    @Override
+    public int nextDoc() throws IOException {
+
+      while(true) {
+        if (count == docFreq) {
+          return doc = NO_MORE_DOCS;
+        }
+
+        count++;
+
+        // TODO: maybe we should do the 1-bit trick for encoding
+        // freq=1 case?
+
+        // Decode next doc
+        //System.out.println("  sep d&p read doc");
+        accum += docReader.next();
+
+        //System.out.println("  sep d&p read freq");
+        freq = freqReader.next();
+
+        pendingPosCount += freq;
+
+        if (liveDocs == null || liveDocs.get(accum)) {
+          break;
+        }
+      }
+
+      position = 0;
+      return (doc = accum);
+    }
+
+    @Override
+    public int freq() throws IOException {
+      return freq;
+    }
+
+    @Override
+    public int docID() {
+      return doc;
+    }
+
+    @Override
+    public int advance(int target) throws IOException {
+      //System.out.println("SepD&P advance target=" + target + " vs current=" + doc + " this=" + this);
+
+      if ((target - skipInterval) >= doc && docFreq >= skipMinimum) {
+
+        // There are enough docs in the posting to have
+        // skip data, and its not too close
+
+        if (skipper == null) {
+          //System.out.println("  create skipper");
+          // This DocsEnum has never done any skipping
+          skipper = new SepSkipListReader(skipIn.clone(),
+                                          freqIn,
+                                          docIn,
+                                          posIn,
+                                          maxSkipLevels, skipInterval);
+        }
+
+        if (!skipped) {
+          //System.out.println("  init skip data skipFP=" + skipFP);
+          // We haven't yet skipped for this posting
+          skipper.init(skipFP,
+                       docIndex,
+                       freqIndex,
+                       posIndex,
+                       payloadFP,
+                       docFreq,
+                       storePayloads);
+          skipper.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);
+          skipped = true;
+        }
+        final int newCount = skipper.skipTo(target); 
+        //System.out.println("  skip newCount=" + newCount + " vs " + count);
+
+        if (newCount > count) {
+
+          // Skipper did move
+          skipper.getFreqIndex().seek(freqReader);
+          skipper.getDocIndex().seek(docReader);
+          //System.out.println("  doc seek'd to " + skipper.getDocIndex());
+          // NOTE: don't seek pos here; do it lazily
+          // instead.  Eg a PhraseQuery may skip to many
+          // docs before finally asking for positions...
+          posIndex.copyFrom(skipper.getPosIndex());
+          posSeekPending = true;
+          count = newCount;
+          doc = accum = skipper.getDoc();
+          //System.out.println("    moved to doc=" + doc);
+          //payloadIn.seek(skipper.getPayloadPointer());
+          payloadFP = skipper.getPayloadPointer();
+          pendingPosCount = 0;
+          pendingPayloadBytes = 0;
+          payloadPending = false;
+          payloadLength = skipper.getPayloadLength();
+          //System.out.println("    move payloadLen=" + payloadLength);
+        }
+      }
+        
+      // Now, linear scan for the rest:
+      do {
+        if (nextDoc() == NO_MORE_DOCS) {
+          //System.out.println("  advance nextDoc=END");
+          return NO_MORE_DOCS;
+        }
+        //System.out.println("  advance nextDoc=" + doc);
+      } while (target > doc);
+
+      //System.out.println("  return doc=" + doc);
+      return doc;
+    }
+
+    @Override
+    public int nextPosition() throws IOException {
+      if (posSeekPending) {
+        posIndex.seek(posReader);
+        payloadIn.seek(payloadFP);
+        posSeekPending = false;
+      }
+
+      // scan over any docs that were iterated without their
+      // positions
+      while (pendingPosCount > freq) {
+        final int code = posReader.next();
+        if (storePayloads && (code & 1) != 0) {
+          // Payload length has changed
+          payloadLength = posReader.next();
+          assert payloadLength >= 0;
+        }
+        pendingPosCount--;
+        position = 0;
+        pendingPayloadBytes += payloadLength;
+      }
+
+      final int code = posReader.next();
+
+      if (storePayloads) {
+        if ((code & 1) != 0) {
+          // Payload length has changed
+          payloadLength = posReader.next();
+          assert payloadLength >= 0;
+        }
+        position += code >>> 1;
+        pendingPayloadBytes += payloadLength;
+        payloadPending = payloadLength > 0;
+      } else {
+        position += code;
+      }
+    
+      pendingPosCount--;
+      assert pendingPosCount >= 0;
+      return position;
+    }
+
+    @Override
+    public int startOffset() {
+      return -1;
+    }
+
+    @Override
+    public int endOffset() {
+      return -1;
+    }
+
+    private BytesRef payload;
+
+    @Override
+    public BytesRef getPayload() throws IOException {
+      if (!payloadPending) {
+        return null;
+      }
+      
+      if (pendingPayloadBytes == 0) {
+        return payload;
+      }
+
+      assert pendingPayloadBytes >= payloadLength;
+
+      if (pendingPayloadBytes > payloadLength) {
+        payloadIn.seek(payloadIn.getFilePointer() + (pendingPayloadBytes - payloadLength));
+      }
+
+      if (payload == null) {
+        payload = new BytesRef();
+        payload.bytes = new byte[payloadLength];
+      } else if (payload.bytes.length < payloadLength) {
+        payload.grow(payloadLength);
+      }
+
+      payloadIn.readBytes(payload.bytes, 0, payloadLength);
+      payload.length = payloadLength;
+      pendingPayloadBytes = 0;
+      return payload;
+    }
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/sep/SepPostingsWriter.java b/lucene/codecs/src/java/org/apache/lucene/codecs/sep/SepPostingsWriter.java
new file mode 100644
index 0000000..ef96302
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/sep/SepPostingsWriter.java
@@ -0,0 +1,395 @@
+package org.apache.lucene.codecs.sep;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.PostingsWriterBase;
+import org.apache.lucene.codecs.TermStats;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.store.RAMOutputStream;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+
+/** Writes frq to .frq, docs to .doc, pos to .pos, payloads
+ *  to .pyl, skip data to .skp
+ *
+ * @lucene.experimental */
+public final class SepPostingsWriter extends PostingsWriterBase {
+  final static String CODEC = "SepPostingsWriter";
+
+  final static String DOC_EXTENSION = "doc";
+  final static String SKIP_EXTENSION = "skp";
+  final static String FREQ_EXTENSION = "frq";
+  final static String POS_EXTENSION = "pos";
+  final static String PAYLOAD_EXTENSION = "pyl";
+
+  // Increment version to change it:
+  final static int VERSION_START = 0;
+  final static int VERSION_CURRENT = VERSION_START;
+
+  IntIndexOutput freqOut;
+  IntIndexOutput.Index freqIndex;
+
+  IntIndexOutput posOut;
+  IntIndexOutput.Index posIndex;
+
+  IntIndexOutput docOut;
+  IntIndexOutput.Index docIndex;
+
+  IndexOutput payloadOut;
+
+  IndexOutput skipOut;
+  IndexOutput termsOut;
+
+  final SepSkipListWriter skipListWriter;
+  /** Expert: The fraction of TermDocs entries stored in skip tables,
+   * used to accelerate {@link DocsEnum#advance(int)}.  Larger values result in
+   * smaller indexes, greater acceleration, but fewer accelerable cases, while
+   * smaller values result in bigger indexes, less acceleration and more
+   * accelerable cases. More detailed experiments would be useful here. */
+  final int skipInterval;
+  static final int DEFAULT_SKIP_INTERVAL = 16;
+  
+  /**
+   * Expert: minimum docFreq to write any skip data at all
+   */
+  final int skipMinimum;
+
+  /** Expert: The maximum number of skip levels. Smaller values result in 
+   * slightly smaller indexes, but slower skipping in big posting lists.
+   */
+  final int maxSkipLevels = 10;
+
+  final int totalNumDocs;
+
+  boolean storePayloads;
+  IndexOptions indexOptions;
+
+  FieldInfo fieldInfo;
+
+  int lastPayloadLength;
+  int lastPosition;
+  long payloadStart;
+  int lastDocID;
+  int df;
+
+  // Holds pending byte[] blob for the current terms block
+  private final RAMOutputStream indexBytesWriter = new RAMOutputStream();
+
+  public SepPostingsWriter(SegmentWriteState state, IntStreamFactory factory) throws IOException {
+    this(state, factory, DEFAULT_SKIP_INTERVAL);
+  }
+
+  public SepPostingsWriter(SegmentWriteState state, IntStreamFactory factory, int skipInterval) throws IOException {
+    freqOut = null;
+    freqIndex = null;
+    posOut = null;
+    posIndex = null;
+    payloadOut = null;
+    boolean success = false;
+    try {
+      this.skipInterval = skipInterval;
+      this.skipMinimum = skipInterval; /* set to the same for now */
+      final String docFileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, DOC_EXTENSION);
+      docOut = factory.createOutput(state.directory, docFileName, state.context);
+      docIndex = docOut.index();
+      
+      if (state.fieldInfos.hasFreq()) {
+        final String frqFileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, FREQ_EXTENSION);
+        freqOut = factory.createOutput(state.directory, frqFileName, state.context);
+        freqIndex = freqOut.index();
+      }
+
+      if (state.fieldInfos.hasProx()) {      
+        final String posFileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, POS_EXTENSION);
+        posOut = factory.createOutput(state.directory, posFileName, state.context);
+        posIndex = posOut.index();
+        
+        // TODO: -- only if at least one field stores payloads?
+        final String payloadFileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, PAYLOAD_EXTENSION);
+        payloadOut = state.directory.createOutput(payloadFileName, state.context);
+      }
+      
+      final String skipFileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, SKIP_EXTENSION);
+      skipOut = state.directory.createOutput(skipFileName, state.context);
+      
+      totalNumDocs = state.segmentInfo.getDocCount();
+      
+      skipListWriter = new SepSkipListWriter(skipInterval,
+          maxSkipLevels,
+          totalNumDocs,
+          freqOut, docOut,
+          posOut, payloadOut);
+      
+      success = true;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(docOut, skipOut, freqOut, posOut, payloadOut);
+      }
+    }
+  }
+
+  @Override
+  public void start(IndexOutput termsOut) throws IOException {
+    this.termsOut = termsOut;
+    CodecUtil.writeHeader(termsOut, CODEC, VERSION_CURRENT);
+    // TODO: -- just ask skipper to "start" here
+    termsOut.writeInt(skipInterval);                // write skipInterval
+    termsOut.writeInt(maxSkipLevels);               // write maxSkipLevels
+    termsOut.writeInt(skipMinimum);                 // write skipMinimum
+  }
+
+  @Override
+  public void startTerm() throws IOException {
+    docIndex.mark();
+    //System.out.println("SEPW: startTerm docIndex=" + docIndex);
+
+    if (indexOptions != IndexOptions.DOCS_ONLY) {
+      freqIndex.mark();
+    }
+    
+    if (indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
+      posIndex.mark();
+      payloadStart = payloadOut.getFilePointer();
+      lastPayloadLength = -1;
+    }
+
+    skipListWriter.resetSkip(docIndex, freqIndex, posIndex);
+  }
+
+  // Currently, this instance is re-used across fields, so
+  // our parent calls setField whenever the field changes
+  @Override
+  public void setField(FieldInfo fieldInfo) {
+    this.fieldInfo = fieldInfo;
+    this.indexOptions = fieldInfo.getIndexOptions();
+    if (indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0) {
+      throw new UnsupportedOperationException("this codec cannot index offsets");
+    }
+    skipListWriter.setIndexOptions(indexOptions);
+    storePayloads = indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS && fieldInfo.hasPayloads();
+  }
+
+  /** Adds a new doc in this term.  If this returns null
+   *  then we just skip consuming positions/payloads. */
+  @Override
+  public void startDoc(int docID, int termDocFreq) throws IOException {
+
+    final int delta = docID - lastDocID;
+    //System.out.println("SEPW: startDoc: write doc=" + docID + " delta=" + delta + " out.fp=" + docOut);
+
+    if (docID < 0 || (df > 0 && delta <= 0)) {
+      throw new CorruptIndexException("docs out of order (" + docID + " <= " + lastDocID + " ) (docOut: " + docOut + ")");
+    }
+
+    if ((++df % skipInterval) == 0) {
+      // TODO: -- awkward we have to make these two
+      // separate calls to skipper
+      //System.out.println("    buffer skip lastDocID=" + lastDocID);
+      skipListWriter.setSkipData(lastDocID, storePayloads, lastPayloadLength);
+      skipListWriter.bufferSkip(df);
+    }
+
+    lastDocID = docID;
+    docOut.write(delta);
+    if (indexOptions != IndexOptions.DOCS_ONLY) {
+      //System.out.println("    sepw startDoc: write freq=" + termDocFreq);
+      freqOut.write(termDocFreq);
+    }
+  }
+
+  /** Add a new position & payload */
+  @Override
+  public void addPosition(int position, BytesRef payload, int startOffset, int endOffset) throws IOException {
+    assert indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;
+
+    final int delta = position - lastPosition;
+    assert delta >= 0: "position=" + position + " lastPosition=" + lastPosition;            // not quite right (if pos=0 is repeated twice we don't catch it)
+    lastPosition = position;
+
+    if (storePayloads) {
+      final int payloadLength = payload == null ? 0 : payload.length;
+      if (payloadLength != lastPayloadLength) {
+        lastPayloadLength = payloadLength;
+        // TODO: explore whether we get better compression
+        // by not storing payloadLength into prox stream?
+        posOut.write((delta<<1)|1);
+        posOut.write(payloadLength);
+      } else {
+        posOut.write(delta << 1);
+      }
+
+      if (payloadLength > 0) {
+        payloadOut.writeBytes(payload.bytes, payload.offset, payloadLength);
+      }
+    } else {
+      posOut.write(delta);
+    }
+
+    lastPosition = position;
+  }
+
+  /** Called when we are done adding positions & payloads */
+  @Override
+  public void finishDoc() {       
+    lastPosition = 0;
+  }
+
+  private static class PendingTerm {
+    public final IntIndexOutput.Index docIndex;
+    public final IntIndexOutput.Index freqIndex;
+    public final IntIndexOutput.Index posIndex;
+    public final long payloadFP;
+    public final long skipFP;
+
+    public PendingTerm(IntIndexOutput.Index docIndex, IntIndexOutput.Index freqIndex, IntIndexOutput.Index posIndex, long payloadFP, long skipFP) {
+      this.docIndex = docIndex;
+      this.freqIndex = freqIndex;
+      this.posIndex = posIndex;
+      this.payloadFP = payloadFP;
+      this.skipFP = skipFP;
+    }
+  }
+
+  private final List<PendingTerm> pendingTerms = new ArrayList<PendingTerm>();
+
+  /** Called when we are done adding docs to this term */
+  @Override
+  public void finishTerm(TermStats stats) throws IOException {
+    // TODO: -- wasteful we are counting this in two places?
+    assert stats.docFreq > 0;
+    assert stats.docFreq == df;
+
+    final IntIndexOutput.Index docIndexCopy = docOut.index();
+    docIndexCopy.copyFrom(docIndex, false);
+
+    final IntIndexOutput.Index freqIndexCopy;
+    final IntIndexOutput.Index posIndexCopy;
+    if (indexOptions != IndexOptions.DOCS_ONLY) {
+      freqIndexCopy = freqOut.index();
+      freqIndexCopy.copyFrom(freqIndex, false);
+      if (indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
+        posIndexCopy = posOut.index();
+        posIndexCopy.copyFrom(posIndex, false);
+      } else {
+        posIndexCopy = null;
+      }
+    } else {
+      freqIndexCopy = null;
+      posIndexCopy = null;
+    }
+
+    final long skipFP;
+    if (df >= skipMinimum) {
+      skipFP = skipOut.getFilePointer();
+      //System.out.println("  skipFP=" + skipFP);
+      skipListWriter.writeSkip(skipOut);
+      //System.out.println("    numBytes=" + (skipOut.getFilePointer()-skipFP));
+    } else {
+      skipFP = -1;
+    }
+
+    lastDocID = 0;
+    df = 0;
+
+    pendingTerms.add(new PendingTerm(docIndexCopy,
+                                     freqIndexCopy,
+                                     posIndexCopy,
+                                     payloadStart,
+                                     skipFP));
+  }
+
+  @Override
+  public void flushTermsBlock(int start, int count) throws IOException {
+    //System.out.println("SEPW: flushTermsBlock: start=" + start + " count=" + count + " pendingTerms.size()=" + pendingTerms.size() + " termsOut.fp=" + termsOut.getFilePointer());
+    assert indexBytesWriter.getFilePointer() == 0;
+    final int absStart = pendingTerms.size() - start;
+    final List<PendingTerm> slice = pendingTerms.subList(absStart, absStart+count);
+
+    long lastPayloadFP = 0;
+    long lastSkipFP = 0;
+
+    if (count == 0) {
+      termsOut.writeByte((byte) 0);
+      return;
+    }
+
+    final PendingTerm firstTerm = slice.get(0);
+    final IntIndexOutput.Index docIndexFlush = firstTerm.docIndex;
+    final IntIndexOutput.Index freqIndexFlush = firstTerm.freqIndex;
+    final IntIndexOutput.Index posIndexFlush = firstTerm.posIndex;
+
+    for(int idx=0;idx<slice.size();idx++) {
+      final boolean isFirstTerm = idx == 0;
+      final PendingTerm t = slice.get(idx);
+      //System.out.println("  write idx=" + idx + " docIndex=" + t.docIndex);
+      docIndexFlush.copyFrom(t.docIndex, false);
+      docIndexFlush.write(indexBytesWriter, isFirstTerm);
+      if (indexOptions != IndexOptions.DOCS_ONLY) {
+        freqIndexFlush.copyFrom(t.freqIndex, false);
+        freqIndexFlush.write(indexBytesWriter, isFirstTerm);
+        //System.out.println("    freqIndex=" + t.freqIndex);
+        if (indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
+          posIndexFlush.copyFrom(t.posIndex, false);
+          posIndexFlush.write(indexBytesWriter, isFirstTerm);
+          //System.out.println("    posIndex=" + t.posIndex);
+          if (storePayloads) {
+            //System.out.println("    payloadFP=" + t.payloadFP);
+            if (isFirstTerm) {
+              indexBytesWriter.writeVLong(t.payloadFP);
+            } else {
+              indexBytesWriter.writeVLong(t.payloadFP - lastPayloadFP);
+            }
+            lastPayloadFP = t.payloadFP;
+          }
+        }
+      }
+
+      if (t.skipFP != -1) {
+        if (isFirstTerm) {
+          indexBytesWriter.writeVLong(t.skipFP);
+        } else {
+          indexBytesWriter.writeVLong(t.skipFP - lastSkipFP);
+        }
+        lastSkipFP = t.skipFP;
+        //System.out.println("    skipFP=" + t.skipFP);
+      }
+    }
+
+    //System.out.println("  numBytes=" + indexBytesWriter.getFilePointer());
+    termsOut.writeVLong((int) indexBytesWriter.getFilePointer());
+    indexBytesWriter.writeTo(termsOut);
+    indexBytesWriter.reset();
+    slice.clear();
+  }
+
+  @Override
+  public void close() throws IOException {
+    IOUtils.close(docOut, skipOut, freqOut, posOut, payloadOut);
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/sep/SepSkipListReader.java b/lucene/codecs/src/java/org/apache/lucene/codecs/sep/SepSkipListReader.java
new file mode 100644
index 0000000..e1f8b28
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/sep/SepSkipListReader.java
@@ -0,0 +1,209 @@
+package org.apache.lucene.codecs.sep;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Arrays;
+
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.codecs.MultiLevelSkipListReader;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+
+/**
+ * Implements the skip list reader for the default posting list format
+ * that stores positions and payloads.
+ *
+ * @lucene.experimental
+ */
+
+// TODO: rewrite this as recursive classes?
+class SepSkipListReader extends MultiLevelSkipListReader {
+  private boolean currentFieldStoresPayloads;
+  private IntIndexInput.Index freqIndex[];
+  private IntIndexInput.Index docIndex[];
+  private IntIndexInput.Index posIndex[];
+  private long payloadPointer[];
+  private int payloadLength[];
+
+  private final IntIndexInput.Index lastFreqIndex;
+  private final IntIndexInput.Index lastDocIndex;
+  // TODO: -- make private again
+  final IntIndexInput.Index lastPosIndex;
+  
+  private long lastPayloadPointer;
+  private int lastPayloadLength;
+                           
+  SepSkipListReader(IndexInput skipStream,
+                    IntIndexInput freqIn,
+                    IntIndexInput docIn,
+                    IntIndexInput posIn,
+                    int maxSkipLevels,
+                    int skipInterval)
+    throws IOException {
+    super(skipStream, maxSkipLevels, skipInterval);
+    if (freqIn != null) {
+      freqIndex = new IntIndexInput.Index[maxSkipLevels];
+    }
+    docIndex = new IntIndexInput.Index[maxSkipLevels];
+    if (posIn != null) {
+      posIndex = new IntIndexInput.Index[maxNumberOfSkipLevels];
+    }
+    for(int i=0;i<maxSkipLevels;i++) {
+      if (freqIn != null) {
+        freqIndex[i] = freqIn.index();
+      }
+      docIndex[i] = docIn.index();
+      if (posIn != null) {
+        posIndex[i] = posIn.index();
+      }
+    }
+    payloadPointer = new long[maxSkipLevels];
+    payloadLength = new int[maxSkipLevels];
+
+    if (freqIn != null) {
+      lastFreqIndex = freqIn.index();
+    } else {
+      lastFreqIndex = null;
+    }
+    lastDocIndex = docIn.index();
+    if (posIn != null) {
+      lastPosIndex = posIn.index();
+    } else {
+      lastPosIndex = null;
+    }
+  }
+  
+  IndexOptions indexOptions;
+
+  void setIndexOptions(IndexOptions v) {
+    indexOptions = v;
+  }
+
+  void init(long skipPointer,
+            IntIndexInput.Index docBaseIndex,
+            IntIndexInput.Index freqBaseIndex,
+            IntIndexInput.Index posBaseIndex,
+            long payloadBasePointer,
+            int df,
+            boolean storesPayloads) {
+
+    super.init(skipPointer, df);
+    this.currentFieldStoresPayloads = storesPayloads;
+
+    lastPayloadPointer = payloadBasePointer;
+
+    for(int i=0;i<maxNumberOfSkipLevels;i++) {
+      docIndex[i].copyFrom(docBaseIndex);
+      if (freqIndex != null) {
+        freqIndex[i].copyFrom(freqBaseIndex);
+      }
+      if (posBaseIndex != null) {
+        posIndex[i].copyFrom(posBaseIndex);
+      }
+    }
+    Arrays.fill(payloadPointer, payloadBasePointer);
+    Arrays.fill(payloadLength, 0);
+  }
+
+  long getPayloadPointer() {
+    return lastPayloadPointer;
+  }
+  
+  /** Returns the payload length of the payload stored just before 
+   * the doc to which the last call of {@link MultiLevelSkipListReader#skipTo(int)} 
+   * has skipped.  */
+  int getPayloadLength() {
+    return lastPayloadLength;
+  }
+  
+  @Override
+  protected void seekChild(int level) throws IOException {
+    super.seekChild(level);
+    payloadPointer[level] = lastPayloadPointer;
+    payloadLength[level] = lastPayloadLength;
+  }
+  
+  @Override
+  protected void setLastSkipData(int level) {
+    super.setLastSkipData(level);
+
+    lastPayloadPointer = payloadPointer[level];
+    lastPayloadLength = payloadLength[level];
+    if (freqIndex != null) {
+      lastFreqIndex.copyFrom(freqIndex[level]);
+    }
+    lastDocIndex.copyFrom(docIndex[level]);
+    if (lastPosIndex != null) {
+      lastPosIndex.copyFrom(posIndex[level]);
+    }
+
+    if (level > 0) {
+      if (freqIndex != null) {
+        freqIndex[level-1].copyFrom(freqIndex[level]);
+      }
+      docIndex[level-1].copyFrom(docIndex[level]);
+      if (posIndex != null) {
+        posIndex[level-1].copyFrom(posIndex[level]);
+      }
+    }
+  }
+
+  IntIndexInput.Index getFreqIndex() {
+    return lastFreqIndex;
+  }
+
+  IntIndexInput.Index getPosIndex() {
+    return lastPosIndex;
+  }
+
+  IntIndexInput.Index getDocIndex() {
+    return lastDocIndex;
+  }
+
+  @Override
+  protected int readSkipData(int level, IndexInput skipStream) throws IOException {
+    int delta;
+    assert indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS || !currentFieldStoresPayloads;
+    if (currentFieldStoresPayloads) {
+      // the current field stores payloads.
+      // if the doc delta is odd then we have
+      // to read the current payload length
+      // because it differs from the length of the
+      // previous payload
+      delta = skipStream.readVInt();
+      if ((delta & 1) != 0) {
+        payloadLength[level] = skipStream.readVInt();
+      }
+      delta >>>= 1;
+    } else {
+      delta = skipStream.readVInt();
+    }
+    if (indexOptions != IndexOptions.DOCS_ONLY) {
+      freqIndex[level].read(skipStream, false);
+    }
+    docIndex[level].read(skipStream, false);
+    if (indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
+      posIndex[level].read(skipStream, false);
+      if (currentFieldStoresPayloads) {
+        payloadPointer[level] += skipStream.readVInt();
+      }
+    }
+    
+    return delta;
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/sep/SepSkipListWriter.java b/lucene/codecs/src/java/org/apache/lucene/codecs/sep/SepSkipListWriter.java
new file mode 100644
index 0000000..fd284bd8
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/sep/SepSkipListWriter.java
@@ -0,0 +1,200 @@
+package org.apache.lucene.codecs.sep;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Arrays;
+
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.codecs.MultiLevelSkipListWriter;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+
+// TODO: -- skip data should somehow be more local to the
+// particular stream (doc, freq, pos, payload)
+
+/**
+ * Implements the skip list writer for the default posting list format
+ * that stores positions and payloads.
+ *
+ * @lucene.experimental
+ */
+class SepSkipListWriter extends MultiLevelSkipListWriter {
+  private int[] lastSkipDoc;
+  private int[] lastSkipPayloadLength;
+  private long[] lastSkipPayloadPointer;
+
+  private IntIndexOutput.Index[] docIndex;
+  private IntIndexOutput.Index[] freqIndex;
+  private IntIndexOutput.Index[] posIndex;
+  
+  private IntIndexOutput freqOutput;
+  // TODO: -- private again
+  IntIndexOutput posOutput;
+  // TODO: -- private again
+  IndexOutput payloadOutput;
+
+  private int curDoc;
+  private boolean curStorePayloads;
+  private int curPayloadLength;
+  private long curPayloadPointer;
+  
+  SepSkipListWriter(int skipInterval, int numberOfSkipLevels, int docCount,
+                    IntIndexOutput freqOutput,
+                    IntIndexOutput docOutput,
+                    IntIndexOutput posOutput,
+                    IndexOutput payloadOutput)
+    throws IOException {
+    super(skipInterval, numberOfSkipLevels, docCount);
+
+    this.freqOutput = freqOutput;
+    this.posOutput = posOutput;
+    this.payloadOutput = payloadOutput;
+    
+    lastSkipDoc = new int[numberOfSkipLevels];
+    lastSkipPayloadLength = new int[numberOfSkipLevels];
+    // TODO: -- also cutover normal IndexOutput to use getIndex()?
+    lastSkipPayloadPointer = new long[numberOfSkipLevels];
+
+    freqIndex = new IntIndexOutput.Index[numberOfSkipLevels];
+    docIndex = new IntIndexOutput.Index[numberOfSkipLevels];
+    posIndex = new IntIndexOutput.Index[numberOfSkipLevels];
+
+    for(int i=0;i<numberOfSkipLevels;i++) {
+      if (freqOutput != null) {
+        freqIndex[i] = freqOutput.index();
+      }
+      docIndex[i] = docOutput.index();
+      if (posOutput != null) {
+        posIndex[i] = posOutput.index();
+      }
+    }
+  }
+
+  IndexOptions indexOptions;
+
+  void setIndexOptions(IndexOptions v) {
+    indexOptions = v;
+  }
+
+  void setPosOutput(IntIndexOutput posOutput) throws IOException {
+    this.posOutput = posOutput;
+    for(int i=0;i<numberOfSkipLevels;i++) {
+      posIndex[i] = posOutput.index();
+    }
+  }
+
+  void setPayloadOutput(IndexOutput payloadOutput) {
+    this.payloadOutput = payloadOutput;
+  }
+
+  /**
+   * Sets the values for the current skip data. 
+   */
+  // Called @ every index interval (every 128th (by default)
+  // doc)
+  void setSkipData(int doc, boolean storePayloads, int payloadLength) {
+    this.curDoc = doc;
+    this.curStorePayloads = storePayloads;
+    this.curPayloadLength = payloadLength;
+    if (payloadOutput != null) {
+      this.curPayloadPointer = payloadOutput.getFilePointer();
+    }
+  }
+
+  // Called @ start of new term
+  protected void resetSkip(IntIndexOutput.Index topDocIndex, IntIndexOutput.Index topFreqIndex, IntIndexOutput.Index topPosIndex)
+    throws IOException {
+    super.resetSkip();
+
+    Arrays.fill(lastSkipDoc, 0);
+    Arrays.fill(lastSkipPayloadLength, -1);  // we don't have to write the first length in the skip list
+    for(int i=0;i<numberOfSkipLevels;i++) {
+      docIndex[i].copyFrom(topDocIndex, true);
+      if (freqOutput != null) {
+        freqIndex[i].copyFrom(topFreqIndex, true);
+      }
+      if (posOutput != null) {
+        posIndex[i].copyFrom(topPosIndex, true);
+      }
+    }
+    if (payloadOutput != null) {
+      Arrays.fill(lastSkipPayloadPointer, payloadOutput.getFilePointer());
+    }
+  }
+  
+  @Override
+  protected void writeSkipData(int level, IndexOutput skipBuffer) throws IOException {
+    // To efficiently store payloads in the posting lists we do not store the length of
+    // every payload. Instead we omit the length for a payload if the previous payload had
+    // the same length.
+    // However, in order to support skipping the payload length at every skip point must be known.
+    // So we use the same length encoding that we use for the posting lists for the skip data as well:
+    // Case 1: current field does not store payloads
+    //           SkipDatum                 --> DocSkip, FreqSkip, ProxSkip
+    //           DocSkip,FreqSkip,ProxSkip --> VInt
+    //           DocSkip records the document number before every SkipInterval th  document in TermFreqs. 
+    //           Document numbers are represented as differences from the previous value in the sequence.
+    // Case 2: current field stores payloads
+    //           SkipDatum                 --> DocSkip, PayloadLength?, FreqSkip,ProxSkip
+    //           DocSkip,FreqSkip,ProxSkip --> VInt
+    //           PayloadLength             --> VInt    
+    //         In this case DocSkip/2 is the difference between
+    //         the current and the previous value. If DocSkip
+    //         is odd, then a PayloadLength encoded as VInt follows,
+    //         if DocSkip is even, then it is assumed that the
+    //         current payload length equals the length at the previous
+    //         skip point
+
+    assert indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS || !curStorePayloads;
+
+    if (curStorePayloads) {
+      int delta = curDoc - lastSkipDoc[level];
+      if (curPayloadLength == lastSkipPayloadLength[level]) {
+        // the current payload length equals the length at the previous skip point,
+        // so we don't store the length again
+        skipBuffer.writeVInt(delta << 1);
+      } else {
+        // the payload length is different from the previous one. We shift the DocSkip, 
+        // set the lowest bit and store the current payload length as VInt.
+        skipBuffer.writeVInt(delta << 1 | 1);
+        skipBuffer.writeVInt(curPayloadLength);
+        lastSkipPayloadLength[level] = curPayloadLength;
+      }
+    } else {
+      // current field does not store payloads
+      skipBuffer.writeVInt(curDoc - lastSkipDoc[level]);
+    }
+
+    if (indexOptions != IndexOptions.DOCS_ONLY) {
+      freqIndex[level].mark();
+      freqIndex[level].write(skipBuffer, false);
+    }
+    docIndex[level].mark();
+    docIndex[level].write(skipBuffer, false);
+    if (indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
+      posIndex[level].mark();
+      posIndex[level].write(skipBuffer, false);
+      if (curStorePayloads) {
+        skipBuffer.writeVInt((int) (curPayloadPointer - lastSkipPayloadPointer[level]));
+      }
+    }
+
+    lastSkipDoc[level] = curDoc;
+    lastSkipPayloadPointer[level] = curPayloadPointer;
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/sep/package.html b/lucene/codecs/src/java/org/apache/lucene/codecs/sep/package.html
new file mode 100644
index 0000000..b51d910
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/sep/package.html
@@ -0,0 +1,25 @@
+<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+<html>
+<head>
+   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
+</head>
+<body>
+Sep: base support for separate files (doc,frq,pos,skp,pyl)
+</body>
+</html>
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextCodec.java b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextCodec.java
new file mode 100644
index 0000000..a412b6e
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextCodec.java
@@ -0,0 +1,91 @@
+package org.apache.lucene.codecs.simpletext;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.DocValuesFormat;
+import org.apache.lucene.codecs.FieldInfosFormat;
+import org.apache.lucene.codecs.LiveDocsFormat;
+import org.apache.lucene.codecs.NormsFormat;
+import org.apache.lucene.codecs.PostingsFormat;
+import org.apache.lucene.codecs.SegmentInfoFormat;
+import org.apache.lucene.codecs.StoredFieldsFormat;
+import org.apache.lucene.codecs.TermVectorsFormat;
+
+/**
+ * plain text index format.
+ * <p>
+ * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
+ * @lucene.experimental
+ */
+public final class SimpleTextCodec extends Codec {
+  private final PostingsFormat postings = new SimpleTextPostingsFormat();
+  private final StoredFieldsFormat storedFields = new SimpleTextStoredFieldsFormat();
+  private final SegmentInfoFormat segmentInfos = new SimpleTextSegmentInfoFormat();
+  private final FieldInfosFormat fieldInfosFormat = new SimpleTextFieldInfosFormat();
+  private final TermVectorsFormat vectorsFormat = new SimpleTextTermVectorsFormat();
+  // TODO: need a plain-text impl
+  private final DocValuesFormat docValues = new SimpleTextDocValuesFormat();
+  // TODO: need a plain-text impl (using the above)
+  private final NormsFormat normsFormat = new SimpleTextNormsFormat();
+  private final LiveDocsFormat liveDocs = new SimpleTextLiveDocsFormat();
+  
+  public SimpleTextCodec() {
+    super("SimpleText");
+  }
+  
+  @Override
+  public PostingsFormat postingsFormat() {
+    return postings;
+  }
+
+  @Override
+  public DocValuesFormat docValuesFormat() {
+    return docValues;
+  }
+
+  @Override
+  public StoredFieldsFormat storedFieldsFormat() {
+    return storedFields;
+  }
+  
+  @Override
+  public TermVectorsFormat termVectorsFormat() {
+    return vectorsFormat;
+  }
+  
+  @Override
+  public FieldInfosFormat fieldInfosFormat() {
+    return fieldInfosFormat;
+  }
+
+  @Override
+  public SegmentInfoFormat segmentInfoFormat() {
+    return segmentInfos;
+  }
+
+  @Override
+  public NormsFormat normsFormat() {
+    return normsFormat;
+  }
+  
+  @Override
+  public LiveDocsFormat liveDocsFormat() {
+    return liveDocs;
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesConsumer.java b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesConsumer.java
new file mode 100644
index 0000000..96aac28
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesConsumer.java
@@ -0,0 +1,294 @@
+package org.apache.lucene.codecs.simpletext;
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with this
+ * work for additional information regarding copyright ownership. The ASF
+ * licenses this file to You under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ * 
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * 
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+ * License for the specific language governing permissions and limitations under
+ * the License.
+ */
+import java.io.IOException;
+
+import org.apache.lucene.codecs.DocValuesArraySource;
+import org.apache.lucene.codecs.DocValuesConsumer;
+import org.apache.lucene.index.DocValues.Type;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.StorableField;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.BytesRefHash;
+import org.apache.lucene.util.IOUtils;
+
+/**
+ * Writes plain-text DocValues.
+ * <p>
+ * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
+ * 
+ * @lucene.experimental
+ */
+public class SimpleTextDocValuesConsumer extends DocValuesConsumer {
+
+  static final BytesRef ZERO_DOUBLE = new BytesRef(Double.toString(0d));
+  static final BytesRef ZERO_INT = new BytesRef(Integer.toString(0));
+  static final BytesRef HEADER = new BytesRef("SimpleTextDocValues"); 
+
+  static final BytesRef END = new BytesRef("END");
+  static final BytesRef VALUE_SIZE = new BytesRef("valuesize ");
+  static final BytesRef DOC = new BytesRef("  doc ");
+  static final BytesRef VALUE = new BytesRef("    value ");
+  protected BytesRef scratch = new BytesRef();
+  protected int maxDocId = -1;
+  protected final String segment;
+  protected final Directory dir;
+  protected final IOContext ctx;
+  protected final Type type;
+  protected final BytesRefHash hash;
+  private int[] ords;
+  private int valueSize = Integer.MIN_VALUE;
+  private BytesRef zeroBytes;
+  private final String segmentSuffix;
+  
+
+  public SimpleTextDocValuesConsumer(String segment, Directory dir,
+      IOContext ctx, Type type, String segmentSuffix) {
+    this.ctx = ctx;
+    this.dir = dir;
+    this.segment = segment;
+    this.type = type;
+    hash = new BytesRefHash();
+    ords = new int[0];
+    this.segmentSuffix = segmentSuffix;
+  }
+
+  @Override
+  public void add(int docID, StorableField value) throws IOException {
+    assert docID >= 0;
+    final int ord, vSize;
+    switch (type) {
+    case BYTES_FIXED_DEREF:
+    case BYTES_FIXED_SORTED:
+    case BYTES_FIXED_STRAIGHT:
+      vSize = value.binaryValue().length;
+      ord = hash.add(value.binaryValue());
+      break;
+    case BYTES_VAR_DEREF:
+    case BYTES_VAR_SORTED:
+    case BYTES_VAR_STRAIGHT:
+      vSize = -1;
+      ord = hash.add(value.binaryValue());
+      break;
+    case FIXED_INTS_16:
+      vSize = 2;
+      scratch.grow(2);
+      DocValuesArraySource.copyShort(scratch, value.numericValue().shortValue());
+      ord = hash.add(scratch);
+      break;
+    case FIXED_INTS_32:
+      vSize = 4;
+      scratch.grow(4);
+      DocValuesArraySource.copyInt(scratch, value.numericValue().intValue());
+      ord = hash.add(scratch);
+      break;
+    case FIXED_INTS_8:
+      vSize = 1;
+      scratch.grow(1); 
+      scratch.bytes[scratch.offset] = value.numericValue().byteValue();
+      scratch.length = 1;
+      ord = hash.add(scratch);
+      break;
+    case FIXED_INTS_64:
+      vSize = 8;
+      scratch.grow(8);
+      DocValuesArraySource.copyLong(scratch, value.numericValue().longValue());
+      ord = hash.add(scratch);
+      break;
+    case VAR_INTS:
+      vSize = -1;
+      scratch.grow(8);
+      DocValuesArraySource.copyLong(scratch, value.numericValue().longValue());
+      ord = hash.add(scratch);
+      break;
+    case FLOAT_32:
+      vSize = 4;
+      scratch.grow(4);
+      DocValuesArraySource.copyInt(scratch,
+          Float.floatToRawIntBits(value.numericValue().floatValue()));
+      ord = hash.add(scratch);
+      break;
+    case FLOAT_64:
+      vSize = 8;
+      scratch.grow(8);
+      DocValuesArraySource.copyLong(scratch,
+          Double.doubleToRawLongBits(value.numericValue().doubleValue()));
+      ord = hash.add(scratch);
+      break;
+    default:
+      throw new RuntimeException("should not reach this line");
+    }
+    
+    if (valueSize == Integer.MIN_VALUE) {
+      assert maxDocId == -1;
+      valueSize = vSize;
+    } else {
+      if (valueSize != vSize) {
+        throw new IllegalArgumentException("value size must be " + valueSize + " but was: " + vSize);
+      }
+    }
+    maxDocId = Math.max(docID, maxDocId);
+    ords = grow(ords, docID);
+    
+    ords[docID] = (ord < 0 ? (-ord)-1 : ord) + 1;
+  }
+  
+  protected BytesRef getHeader() {
+    return HEADER;
+  }
+
+  private int[] grow(int[] array, int upto) {
+    if (array.length <= upto) {
+      return ArrayUtil.grow(array, 1 + upto);
+    }
+    return array;
+  }
+
+  private void prepareFlush(int docCount) {
+    assert ords != null;
+    ords = grow(ords, docCount);
+  }
+
+  @Override
+  public void finish(int docCount) throws IOException {
+    final String fileName = IndexFileNames.segmentFileName(segment, "",
+        segmentSuffix);
+    IndexOutput output = dir.createOutput(fileName, ctx);
+    boolean success = false;
+    BytesRef spare = new BytesRef();
+    try {
+      SimpleTextUtil.write(output, getHeader());
+      SimpleTextUtil.writeNewline(output);
+      SimpleTextUtil.write(output, VALUE_SIZE);
+      SimpleTextUtil.write(output, Integer.toString(this.valueSize), scratch);
+      SimpleTextUtil.writeNewline(output);
+      prepareFlush(docCount);
+      for (int i = 0; i < docCount; i++) {
+        SimpleTextUtil.write(output, DOC);
+        SimpleTextUtil.write(output, Integer.toString(i), scratch);
+        SimpleTextUtil.writeNewline(output);
+        SimpleTextUtil.write(output, VALUE);
+        writeDoc(output, i, spare);
+        SimpleTextUtil.writeNewline(output);
+      }
+      SimpleTextUtil.write(output, END);
+      SimpleTextUtil.writeNewline(output);
+      success = true;
+    } finally {
+      hash.close();
+      if (success) {
+        IOUtils.close(output);
+      } else {
+        IOUtils.closeWhileHandlingException(output);
+      }
+    }
+  }
+
+  protected void writeDoc(IndexOutput output, int docId, BytesRef spare) throws IOException {
+    int ord = ords[docId] - 1;
+    if (ord != -1) {
+      assert ord >= 0;
+      hash.get(ord, spare);
+
+      switch (type) {
+      case BYTES_FIXED_DEREF:
+      case BYTES_FIXED_SORTED:
+      case BYTES_FIXED_STRAIGHT:
+      case BYTES_VAR_DEREF:
+      case BYTES_VAR_SORTED:
+      case BYTES_VAR_STRAIGHT:
+        SimpleTextUtil.write(output, spare);
+        break;
+      case FIXED_INTS_16:
+        SimpleTextUtil.write(output,
+            Short.toString(DocValuesArraySource.asShort(spare)), scratch);
+        break;
+      case FIXED_INTS_32:
+        SimpleTextUtil.write(output,
+            Integer.toString(DocValuesArraySource.asInt(spare)), scratch);
+        break;
+      case VAR_INTS:
+      case FIXED_INTS_64:
+        SimpleTextUtil.write(output,
+            Long.toString(DocValuesArraySource.asLong(spare)), scratch);
+        break;
+      case FIXED_INTS_8:
+        assert spare.length == 1 : spare.length;
+        SimpleTextUtil.write(output,
+            Integer.toString(spare.bytes[spare.offset]), scratch);
+        break;
+      case FLOAT_32:
+        float valueFloat = Float.intBitsToFloat(DocValuesArraySource.asInt(spare));
+        SimpleTextUtil.write(output, Float.toString(valueFloat), scratch);
+        break;
+      case FLOAT_64:
+        double valueDouble = Double.longBitsToDouble(DocValuesArraySource
+            .asLong(spare));
+        SimpleTextUtil.write(output, Double.toString(valueDouble), scratch);
+        break;
+      default:
+        throw new IllegalArgumentException("unsupported type: " + type);
+      }
+    } else {
+      switch (type) {
+      case BYTES_FIXED_DEREF:
+      case BYTES_FIXED_SORTED:
+      case BYTES_FIXED_STRAIGHT:
+        if(zeroBytes == null) {
+          assert valueSize > 0;
+          zeroBytes = new BytesRef(new byte[valueSize]);
+        }
+        SimpleTextUtil.write(output, zeroBytes);
+        break;
+      case BYTES_VAR_DEREF:
+      case BYTES_VAR_SORTED:
+      case BYTES_VAR_STRAIGHT:
+        scratch.length = 0;
+        SimpleTextUtil.write(output, scratch);
+        break;
+      case FIXED_INTS_16:
+      case FIXED_INTS_32:
+      case FIXED_INTS_64:
+      case FIXED_INTS_8:
+      case VAR_INTS:
+        SimpleTextUtil.write(output, ZERO_INT);
+        break;
+      case FLOAT_32:
+      case FLOAT_64:
+        SimpleTextUtil.write(output, ZERO_DOUBLE);
+        break;
+      default:
+        throw new IllegalArgumentException("unsupported type: " + type);
+      }
+    }
+
+  }
+
+  @Override
+  protected Type getType() {
+    return type;
+  }
+
+  @Override
+  public int getValueSize() {
+    return valueSize;
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesFormat.java b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesFormat.java
new file mode 100644
index 0000000..033136e
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesFormat.java
@@ -0,0 +1,51 @@
+package org.apache.lucene.codecs.simpletext;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with this
+ * work for additional information regarding copyright ownership. The ASF
+ * licenses this file to You under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ * 
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * 
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+ * License for the specific language governing permissions and limitations under
+ * the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.DocValuesFormat;
+import org.apache.lucene.codecs.PerDocConsumer;
+import org.apache.lucene.codecs.PerDocProducer;
+import org.apache.lucene.index.PerDocWriteState;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.util.BytesRef;
+
+/**
+ * Plain-text DocValues format.
+ * <p>
+ * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
+ * 
+ * @lucene.experimental
+ */
+public class SimpleTextDocValuesFormat extends DocValuesFormat {
+  private static final String DOC_VALUES_SEG_SUFFIX = "dv";
+  @Override
+  public PerDocConsumer docsConsumer(PerDocWriteState state) throws IOException {
+    return new SimpleTextPerDocConsumer(state, DOC_VALUES_SEG_SUFFIX);
+  }
+
+  @Override
+  public PerDocProducer docsProducer(SegmentReadState state) throws IOException {
+    return new SimpleTextPerDocProducer(state, BytesRef.getUTF8SortedAsUnicodeComparator(), DOC_VALUES_SEG_SUFFIX);
+  }
+
+  static String docValuesId(String segmentsName, int fieldId) {
+    return segmentsName + "_" + fieldId;
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldInfosFormat.java b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldInfosFormat.java
new file mode 100644
index 0000000..efe9ff4
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldInfosFormat.java
@@ -0,0 +1,45 @@
+package org.apache.lucene.codecs.simpletext;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.FieldInfosFormat;
+import org.apache.lucene.codecs.FieldInfosReader;
+import org.apache.lucene.codecs.FieldInfosWriter;
+
+/**
+ * plaintext field infos format
+ * <p>
+ * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
+ * @lucene.experimental
+ */
+public class SimpleTextFieldInfosFormat extends FieldInfosFormat {
+  private final FieldInfosReader reader = new SimpleTextFieldInfosReader();
+  private final FieldInfosWriter writer = new SimpleTextFieldInfosWriter();
+
+  @Override
+  public FieldInfosReader getFieldInfosReader() throws IOException {
+    return reader;
+  }
+
+  @Override
+  public FieldInfosWriter getFieldInfosWriter() throws IOException {
+    return writer;
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldInfosReader.java b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldInfosReader.java
new file mode 100644
index 0000000..87891f4
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldInfosReader.java
@@ -0,0 +1,147 @@
+package org.apache.lucene.codecs.simpletext;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.lucene.codecs.FieldInfosReader;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.StringHelper;
+
+import static org.apache.lucene.codecs.simpletext.SimpleTextFieldInfosWriter.*;
+
+/**
+ * reads plaintext field infos files
+ * <p>
+ * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
+ * @lucene.experimental
+ */
+public class SimpleTextFieldInfosReader extends FieldInfosReader {
+
+  @Override
+  public FieldInfos read(Directory directory, String segmentName, IOContext iocontext) throws IOException {
+    final String fileName = IndexFileNames.segmentFileName(segmentName, "", FIELD_INFOS_EXTENSION);
+    IndexInput input = directory.openInput(fileName, iocontext);
+    BytesRef scratch = new BytesRef();
+    
+    try {
+      
+      SimpleTextUtil.readLine(input, scratch);
+      assert StringHelper.startsWith(scratch, NUMFIELDS);
+      final int size = Integer.parseInt(readString(NUMFIELDS.length, scratch));
+      FieldInfo infos[] = new FieldInfo[size];
+
+      for (int i = 0; i < size; i++) {
+        SimpleTextUtil.readLine(input, scratch);
+        assert StringHelper.startsWith(scratch, NAME);
+        String name = readString(NAME.length, scratch);
+        
+        SimpleTextUtil.readLine(input, scratch);
+        assert StringHelper.startsWith(scratch, NUMBER);
+        int fieldNumber = Integer.parseInt(readString(NUMBER.length, scratch));
+
+        SimpleTextUtil.readLine(input, scratch);
+        assert StringHelper.startsWith(scratch, ISINDEXED);
+        boolean isIndexed = Boolean.parseBoolean(readString(ISINDEXED.length, scratch));
+        
+        final IndexOptions indexOptions;
+        if (isIndexed) {
+          SimpleTextUtil.readLine(input, scratch);
+          assert StringHelper.startsWith(scratch, INDEXOPTIONS);
+          indexOptions = IndexOptions.valueOf(readString(INDEXOPTIONS.length, scratch));          
+        } else {
+          indexOptions = null;
+        }
+        
+        SimpleTextUtil.readLine(input, scratch);
+        assert StringHelper.startsWith(scratch, STORETV);
+        boolean storeTermVector = Boolean.parseBoolean(readString(STORETV.length, scratch));
+        
+        SimpleTextUtil.readLine(input, scratch);
+        assert StringHelper.startsWith(scratch, PAYLOADS);
+        boolean storePayloads = Boolean.parseBoolean(readString(PAYLOADS.length, scratch));
+        
+        SimpleTextUtil.readLine(input, scratch);
+        assert StringHelper.startsWith(scratch, NORMS);
+        boolean omitNorms = !Boolean.parseBoolean(readString(NORMS.length, scratch));
+        
+        SimpleTextUtil.readLine(input, scratch);
+        assert StringHelper.startsWith(scratch, NORMS_TYPE);
+        String nrmType = readString(NORMS_TYPE.length, scratch);
+        final DocValues.Type normsType = docValuesType(nrmType);
+        
+        SimpleTextUtil.readLine(input, scratch);
+        assert StringHelper.startsWith(scratch, DOCVALUES);
+        String dvType = readString(DOCVALUES.length, scratch);
+        final DocValues.Type docValuesType = docValuesType(dvType);
+        
+        SimpleTextUtil.readLine(input, scratch);
+        assert StringHelper.startsWith(scratch, NUM_ATTS);
+        int numAtts = Integer.parseInt(readString(NUM_ATTS.length, scratch));
+        Map<String,String> atts = new HashMap<String,String>();
+
+        for (int j = 0; j < numAtts; j++) {
+          SimpleTextUtil.readLine(input, scratch);
+          assert StringHelper.startsWith(scratch, ATT_KEY);
+          String key = readString(ATT_KEY.length, scratch);
+        
+          SimpleTextUtil.readLine(input, scratch);
+          assert StringHelper.startsWith(scratch, ATT_VALUE);
+          String value = readString(ATT_VALUE.length, scratch);
+          atts.put(key, value);
+        }
+
+        infos[i] = new FieldInfo(name, isIndexed, fieldNumber, storeTermVector, 
+          omitNorms, storePayloads, indexOptions, docValuesType, normsType, Collections.unmodifiableMap(atts));
+      }
+
+      if (input.getFilePointer() != input.length()) {
+        throw new CorruptIndexException("did not read all bytes from file \"" + fileName + "\": read " + input.getFilePointer() + " vs size " + input.length() + " (resource: " + input + ")");
+      }
+      
+      return new FieldInfos(infos);
+    } finally {
+      input.close();
+    }
+  }
+
+  public DocValues.Type docValuesType(String dvType) {
+    if ("false".equals(dvType)) {
+      return null;
+    } else {
+      return DocValues.Type.valueOf(dvType);
+    }
+  }
+  
+  private String readString(int offset, BytesRef scratch) {
+    return new String(scratch.bytes, scratch.offset+offset, scratch.length-offset, IOUtils.CHARSET_UTF_8);
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldInfosWriter.java b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldInfosWriter.java
new file mode 100644
index 0000000..d8e4072
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldInfosWriter.java
@@ -0,0 +1,136 @@
+package org.apache.lucene.codecs.simpletext;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+import java.io.IOException;
+import java.util.Map;
+
+import org.apache.lucene.codecs.FieldInfosWriter;
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.BytesRef;
+
+/**
+ * writes plaintext field infos files
+ * <p>
+ * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
+ * @lucene.experimental
+ */
+public class SimpleTextFieldInfosWriter extends FieldInfosWriter {
+  
+  /** Extension of field infos */
+  static final String FIELD_INFOS_EXTENSION = "inf";
+  
+  static final BytesRef NUMFIELDS       =  new BytesRef("number of fields ");
+  static final BytesRef NAME            =  new BytesRef("  name ");
+  static final BytesRef NUMBER          =  new BytesRef("  number ");
+  static final BytesRef ISINDEXED       =  new BytesRef("  indexed ");
+  static final BytesRef STORETV         =  new BytesRef("  term vectors ");
+  static final BytesRef STORETVPOS      =  new BytesRef("  term vector positions ");
+  static final BytesRef STORETVOFF      =  new BytesRef("  term vector offsets ");
+  static final BytesRef PAYLOADS        =  new BytesRef("  payloads ");
+  static final BytesRef NORMS           =  new BytesRef("  norms ");
+  static final BytesRef NORMS_TYPE      =  new BytesRef("  norms type ");
+  static final BytesRef DOCVALUES       =  new BytesRef("  doc values ");
+  static final BytesRef INDEXOPTIONS    =  new BytesRef("  index options ");
+  static final BytesRef NUM_ATTS        =  new BytesRef("  attributes ");
+  final static BytesRef ATT_KEY         =  new BytesRef("    key ");
+  final static BytesRef ATT_VALUE       =  new BytesRef("    value ");
+  
+  @Override
+  public void write(Directory directory, String segmentName, FieldInfos infos, IOContext context) throws IOException {
+    final String fileName = IndexFileNames.segmentFileName(segmentName, "", FIELD_INFOS_EXTENSION);
+    IndexOutput out = directory.createOutput(fileName, context);
+    BytesRef scratch = new BytesRef();
+    try {
+      SimpleTextUtil.write(out, NUMFIELDS);
+      SimpleTextUtil.write(out, Integer.toString(infos.size()), scratch);
+      SimpleTextUtil.writeNewline(out);
+      
+      for (FieldInfo fi : infos) {
+        SimpleTextUtil.write(out, NAME);
+        SimpleTextUtil.write(out, fi.name, scratch);
+        SimpleTextUtil.writeNewline(out);
+        
+        SimpleTextUtil.write(out, NUMBER);
+        SimpleTextUtil.write(out, Integer.toString(fi.number), scratch);
+        SimpleTextUtil.writeNewline(out);
+        
+        SimpleTextUtil.write(out, ISINDEXED);
+        SimpleTextUtil.write(out, Boolean.toString(fi.isIndexed()), scratch);
+        SimpleTextUtil.writeNewline(out);
+        
+        if (fi.isIndexed()) {
+          assert fi.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 || !fi.hasPayloads();
+          SimpleTextUtil.write(out, INDEXOPTIONS);
+          SimpleTextUtil.write(out, fi.getIndexOptions().toString(), scratch);
+          SimpleTextUtil.writeNewline(out);
+        }
+        
+        SimpleTextUtil.write(out, STORETV);
+        SimpleTextUtil.write(out, Boolean.toString(fi.hasVectors()), scratch);
+        SimpleTextUtil.writeNewline(out);
+        
+        SimpleTextUtil.write(out, PAYLOADS);
+        SimpleTextUtil.write(out, Boolean.toString(fi.hasPayloads()), scratch);
+        SimpleTextUtil.writeNewline(out);
+               
+        SimpleTextUtil.write(out, NORMS);
+        SimpleTextUtil.write(out, Boolean.toString(!fi.omitsNorms()), scratch);
+        SimpleTextUtil.writeNewline(out);
+        
+        SimpleTextUtil.write(out, NORMS_TYPE);
+        SimpleTextUtil.write(out, getDocValuesType(fi.getNormType()), scratch);
+        SimpleTextUtil.writeNewline(out);
+        
+        SimpleTextUtil.write(out, DOCVALUES);
+        SimpleTextUtil.write(out, getDocValuesType(fi.getDocValuesType()), scratch);
+        SimpleTextUtil.writeNewline(out);
+               
+        Map<String,String> atts = fi.attributes();
+        int numAtts = atts == null ? 0 : atts.size();
+        SimpleTextUtil.write(out, NUM_ATTS);
+        SimpleTextUtil.write(out, Integer.toString(numAtts), scratch);
+        SimpleTextUtil.writeNewline(out);
+      
+        if (numAtts > 0) {
+          for (Map.Entry<String,String> entry : atts.entrySet()) {
+            SimpleTextUtil.write(out, ATT_KEY);
+            SimpleTextUtil.write(out, entry.getKey(), scratch);
+            SimpleTextUtil.writeNewline(out);
+          
+            SimpleTextUtil.write(out, ATT_VALUE);
+            SimpleTextUtil.write(out, entry.getValue(), scratch);
+            SimpleTextUtil.writeNewline(out);
+          }
+        }
+      }
+    } finally {
+      out.close();
+    }
+  }
+  
+  private static String getDocValuesType(DocValues.Type type) {
+    return type == null ? "false" : type.toString();
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldsReader.java b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldsReader.java
new file mode 100644
index 0000000..c56fd20
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldsReader.java
@@ -0,0 +1,640 @@
+package org.apache.lucene.codecs.simpletext;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Collections;
+import java.util.Comparator;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.Map;
+import java.util.TreeMap;
+import java.util.TreeSet;
+
+import org.apache.lucene.codecs.FieldsProducer;
+import org.apache.lucene.index.DocsAndPositionsEnum;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.Terms;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.CharsRef;
+import org.apache.lucene.util.IntsRef;
+import org.apache.lucene.util.OpenBitSet;
+import org.apache.lucene.util.StringHelper;
+import org.apache.lucene.util.UnicodeUtil;
+import org.apache.lucene.util.fst.Builder;
+import org.apache.lucene.util.fst.BytesRefFSTEnum;
+import org.apache.lucene.util.fst.FST;
+import org.apache.lucene.util.fst.PairOutputs;
+import org.apache.lucene.util.fst.PositiveIntOutputs;
+import org.apache.lucene.util.fst.Util;
+
+class SimpleTextFieldsReader extends FieldsProducer {
+  private final TreeMap<String,Long> fields;
+  private final IndexInput in;
+  private final FieldInfos fieldInfos;
+
+  final static BytesRef END          = SimpleTextFieldsWriter.END;
+  final static BytesRef FIELD        = SimpleTextFieldsWriter.FIELD;
+  final static BytesRef TERM         = SimpleTextFieldsWriter.TERM;
+  final static BytesRef DOC          = SimpleTextFieldsWriter.DOC;
+  final static BytesRef FREQ         = SimpleTextFieldsWriter.FREQ;
+  final static BytesRef POS          = SimpleTextFieldsWriter.POS;
+  final static BytesRef START_OFFSET = SimpleTextFieldsWriter.START_OFFSET;
+  final static BytesRef END_OFFSET   = SimpleTextFieldsWriter.END_OFFSET;
+  final static BytesRef PAYLOAD      = SimpleTextFieldsWriter.PAYLOAD;
+
+  public SimpleTextFieldsReader(SegmentReadState state) throws IOException {
+    in = state.dir.openInput(SimpleTextPostingsFormat.getPostingsFileName(state.segmentInfo.name, state.segmentSuffix), state.context);
+   
+    fieldInfos = state.fieldInfos;
+    fields = readFields(in.clone());
+  }
+  
+  private TreeMap<String,Long> readFields(IndexInput in) throws IOException {
+    BytesRef scratch = new BytesRef(10);
+    TreeMap<String,Long> fields = new TreeMap<String,Long>();
+    
+    while (true) {
+      SimpleTextUtil.readLine(in, scratch);
+      if (scratch.equals(END)) {
+        return fields;
+      } else if (StringHelper.startsWith(scratch, FIELD)) {
+        String fieldName = new String(scratch.bytes, scratch.offset + FIELD.length, scratch.length - FIELD.length, "UTF-8");
+        fields.put(fieldName, in.getFilePointer());
+      }
+    }
+  }
+
+  private class SimpleTextTermsEnum extends TermsEnum {
+    private final IndexOptions indexOptions;
+    private int docFreq;
+    private long totalTermFreq;
+    private long docsStart;
+    private boolean ended;
+    private final BytesRefFSTEnum<PairOutputs.Pair<Long,PairOutputs.Pair<Long,Long>>> fstEnum;
+
+    public SimpleTextTermsEnum(FST<PairOutputs.Pair<Long,PairOutputs.Pair<Long,Long>>> fst, IndexOptions indexOptions) {
+      this.indexOptions = indexOptions;
+      fstEnum = new BytesRefFSTEnum<PairOutputs.Pair<Long,PairOutputs.Pair<Long,Long>>>(fst);
+    }
+
+    @Override
+    public boolean seekExact(BytesRef text, boolean useCache /* ignored */) throws IOException {
+
+      final BytesRefFSTEnum.InputOutput<PairOutputs.Pair<Long,PairOutputs.Pair<Long,Long>>> result = fstEnum.seekExact(text);
+      if (result != null) {
+        PairOutputs.Pair<Long,PairOutputs.Pair<Long,Long>> pair1 = result.output;
+        PairOutputs.Pair<Long,Long> pair2 = pair1.output2;
+        docsStart = pair1.output1;
+        docFreq = pair2.output1.intValue();
+        totalTermFreq = pair2.output2;
+        return true;
+      } else {
+        return false;
+      }
+    }
+
+    @Override
+    public SeekStatus seekCeil(BytesRef text, boolean useCache /* ignored */) throws IOException {
+
+      //System.out.println("seek to text=" + text.utf8ToString());
+      final BytesRefFSTEnum.InputOutput<PairOutputs.Pair<Long,PairOutputs.Pair<Long,Long>>> result = fstEnum.seekCeil(text);
+      if (result == null) {
+        //System.out.println("  end");
+        return SeekStatus.END;
+      } else {
+        //System.out.println("  got text=" + term.utf8ToString());
+        PairOutputs.Pair<Long,PairOutputs.Pair<Long,Long>> pair1 = result.output;
+        PairOutputs.Pair<Long,Long> pair2 = pair1.output2;
+        docsStart = pair1.output1;
+        docFreq = pair2.output1.intValue();
+        totalTermFreq = pair2.output2;
+
+        if (result.input.equals(text)) {
+          //System.out.println("  match docsStart=" + docsStart);
+          return SeekStatus.FOUND;
+        } else {
+          //System.out.println("  not match docsStart=" + docsStart);
+          return SeekStatus.NOT_FOUND;
+        }
+      }
+    }
+
+    @Override
+    public BytesRef next() throws IOException {
+      assert !ended;
+      final BytesRefFSTEnum.InputOutput<PairOutputs.Pair<Long,PairOutputs.Pair<Long,Long>>> result = fstEnum.next();
+      if (result != null) {
+        PairOutputs.Pair<Long,PairOutputs.Pair<Long,Long>> pair1 = result.output;
+        PairOutputs.Pair<Long,Long> pair2 = pair1.output2;
+        docsStart = pair1.output1;
+        docFreq = pair2.output1.intValue();
+        totalTermFreq = pair2.output2;
+        return result.input;
+      } else {
+        return null;
+      }
+    }
+
+    @Override
+    public BytesRef term() {
+      return fstEnum.current().input;
+    }
+
+    @Override
+    public long ord() throws IOException {
+      throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public void seekExact(long ord) {
+      throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public int docFreq() {
+      return docFreq;
+    }
+
+    @Override
+    public long totalTermFreq() {
+      return indexOptions == IndexOptions.DOCS_ONLY ? -1 : totalTermFreq;
+    }
+ 
+    @Override
+    public DocsEnum docs(Bits liveDocs, DocsEnum reuse, int flags) throws IOException {
+      SimpleTextDocsEnum docsEnum;
+      if (reuse != null && reuse instanceof SimpleTextDocsEnum && ((SimpleTextDocsEnum) reuse).canReuse(SimpleTextFieldsReader.this.in)) {
+        docsEnum = (SimpleTextDocsEnum) reuse;
+      } else {
+        docsEnum = new SimpleTextDocsEnum();
+      }
+      return docsEnum.reset(docsStart, liveDocs, indexOptions == IndexOptions.DOCS_ONLY);
+    }
+
+    @Override
+    public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum reuse, int flags) throws IOException {
+
+      if (indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) < 0) {
+        // Positions were not indexed
+        return null;
+      }
+
+      SimpleTextDocsAndPositionsEnum docsAndPositionsEnum;
+      if (reuse != null && reuse instanceof SimpleTextDocsAndPositionsEnum && ((SimpleTextDocsAndPositionsEnum) reuse).canReuse(SimpleTextFieldsReader.this.in)) {
+        docsAndPositionsEnum = (SimpleTextDocsAndPositionsEnum) reuse;
+      } else {
+        docsAndPositionsEnum = new SimpleTextDocsAndPositionsEnum();
+      } 
+      return docsAndPositionsEnum.reset(docsStart, liveDocs, indexOptions);
+    }
+    
+    @Override
+    public Comparator<BytesRef> getComparator() {
+      return BytesRef.getUTF8SortedAsUnicodeComparator();
+    }
+  }
+
+  private class SimpleTextDocsEnum extends DocsEnum {
+    private final IndexInput inStart;
+    private final IndexInput in;
+    private boolean omitTF;
+    private int docID = -1;
+    private int tf;
+    private Bits liveDocs;
+    private final BytesRef scratch = new BytesRef(10);
+    private final CharsRef scratchUTF16 = new CharsRef(10);
+    
+    public SimpleTextDocsEnum() {
+      this.inStart = SimpleTextFieldsReader.this.in;
+      this.in = this.inStart.clone();
+    }
+
+    public boolean canReuse(IndexInput in) {
+      return in == inStart;
+    }
+
+    public SimpleTextDocsEnum reset(long fp, Bits liveDocs, boolean omitTF) throws IOException {
+      this.liveDocs = liveDocs;
+      in.seek(fp);
+      this.omitTF = omitTF;
+      docID = -1;
+      tf = 1;
+      return this;
+    }
+
+    @Override
+    public int docID() {
+      return docID;
+    }
+
+    @Override
+    public int freq() throws IOException {
+      return tf;
+    }
+
+    @Override
+    public int nextDoc() throws IOException {
+      if (docID == NO_MORE_DOCS) {
+        return docID;
+      }
+      boolean first = true;
+      int termFreq = 0;
+      while(true) {
+        final long lineStart = in.getFilePointer();
+        SimpleTextUtil.readLine(in, scratch);
+        if (StringHelper.startsWith(scratch, DOC)) {
+          if (!first && (liveDocs == null || liveDocs.get(docID))) {
+            in.seek(lineStart);
+            if (!omitTF) {
+              tf = termFreq;
+            }
+            return docID;
+          }
+          UnicodeUtil.UTF8toUTF16(scratch.bytes, scratch.offset+DOC.length, scratch.length-DOC.length, scratchUTF16);
+          docID = ArrayUtil.parseInt(scratchUTF16.chars, 0, scratchUTF16.length);
+          termFreq = 0;
+          first = false;
+        } else if (StringHelper.startsWith(scratch, FREQ)) {
+          UnicodeUtil.UTF8toUTF16(scratch.bytes, scratch.offset+FREQ.length, scratch.length-FREQ.length, scratchUTF16);
+          termFreq = ArrayUtil.parseInt(scratchUTF16.chars, 0, scratchUTF16.length);
+        } else if (StringHelper.startsWith(scratch, POS)) {
+          // skip termFreq++;
+        } else if (StringHelper.startsWith(scratch, START_OFFSET)) {
+          // skip
+        } else if (StringHelper.startsWith(scratch, END_OFFSET)) {
+          // skip
+        } else if (StringHelper.startsWith(scratch, PAYLOAD)) {
+          // skip
+        } else {
+          assert StringHelper.startsWith(scratch, TERM) || StringHelper.startsWith(scratch, FIELD) || StringHelper.startsWith(scratch, END): "scratch=" + scratch.utf8ToString();
+          if (!first && (liveDocs == null || liveDocs.get(docID))) {
+            in.seek(lineStart);
+            if (!omitTF) {
+              tf = termFreq;
+            }
+            return docID;
+          }
+          return docID = NO_MORE_DOCS;
+        }
+      }
+    }
+
+    @Override
+    public int advance(int target) throws IOException {
+      // Naive -- better to index skip data
+      while(nextDoc() < target);
+      return docID;
+    }
+  }
+
+  private class SimpleTextDocsAndPositionsEnum extends DocsAndPositionsEnum {
+    private final IndexInput inStart;
+    private final IndexInput in;
+    private int docID = -1;
+    private int tf;
+    private Bits liveDocs;
+    private final BytesRef scratch = new BytesRef(10);
+    private final BytesRef scratch2 = new BytesRef(10);
+    private final CharsRef scratchUTF16 = new CharsRef(10);
+    private final CharsRef scratchUTF16_2 = new CharsRef(10);
+    private BytesRef payload;
+    private long nextDocStart;
+    private boolean readOffsets;
+    private boolean readPositions;
+    private int startOffset;
+    private int endOffset;
+
+    public SimpleTextDocsAndPositionsEnum() {
+      this.inStart = SimpleTextFieldsReader.this.in;
+      this.in = inStart.clone();
+    }
+
+    public boolean canReuse(IndexInput in) {
+      return in == inStart;
+    }
+
+    public SimpleTextDocsAndPositionsEnum reset(long fp, Bits liveDocs, IndexOptions indexOptions) {
+      this.liveDocs = liveDocs;
+      nextDocStart = fp;
+      docID = -1;
+      readPositions = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;
+      readOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
+      if (!readOffsets) {
+        startOffset = -1;
+        endOffset = -1;
+      }
+      return this;
+    }
+
+    @Override
+    public int docID() {
+      return docID;
+    }
+
+    @Override
+    public int freq() throws IOException {
+      return tf;
+    }
+
+    @Override
+    public int nextDoc() throws IOException {
+      boolean first = true;
+      in.seek(nextDocStart);
+      long posStart = 0;
+      while(true) {
+        final long lineStart = in.getFilePointer();
+        SimpleTextUtil.readLine(in, scratch);
+        //System.out.println("NEXT DOC: " + scratch.utf8ToString());
+        if (StringHelper.startsWith(scratch, DOC)) {
+          if (!first && (liveDocs == null || liveDocs.get(docID))) {
+            nextDocStart = lineStart;
+            in.seek(posStart);
+            return docID;
+          }
+          UnicodeUtil.UTF8toUTF16(scratch.bytes, scratch.offset+DOC.length, scratch.length-DOC.length, scratchUTF16);
+          docID = ArrayUtil.parseInt(scratchUTF16.chars, 0, scratchUTF16.length);
+          tf = 0;
+          first = false;
+        } else if (StringHelper.startsWith(scratch, FREQ)) {
+          UnicodeUtil.UTF8toUTF16(scratch.bytes, scratch.offset+FREQ.length, scratch.length-FREQ.length, scratchUTF16);
+          tf = ArrayUtil.parseInt(scratchUTF16.chars, 0, scratchUTF16.length);
+          posStart = in.getFilePointer();
+        } else if (StringHelper.startsWith(scratch, POS)) {
+          // skip
+        } else if (StringHelper.startsWith(scratch, START_OFFSET)) {
+          // skip
+        } else if (StringHelper.startsWith(scratch, END_OFFSET)) {
+          // skip
+        } else if (StringHelper.startsWith(scratch, PAYLOAD)) {
+          // skip
+        } else {
+          assert StringHelper.startsWith(scratch, TERM) || StringHelper.startsWith(scratch, FIELD) || StringHelper.startsWith(scratch, END);
+          if (!first && (liveDocs == null || liveDocs.get(docID))) {
+            nextDocStart = lineStart;
+            in.seek(posStart);
+            return docID;
+          }
+          return docID = NO_MORE_DOCS;
+        }
+      }
+    }
+
+    @Override
+    public int advance(int target) throws IOException {
+      // Naive -- better to index skip data
+      while(nextDoc() < target);
+      return docID;
+    }
+
+    @Override
+    public int nextPosition() throws IOException {
+      final int pos;
+      if (readPositions) {
+        SimpleTextUtil.readLine(in, scratch);
+        assert StringHelper.startsWith(scratch, POS): "got line=" + scratch.utf8ToString();
+        UnicodeUtil.UTF8toUTF16(scratch.bytes, scratch.offset+POS.length, scratch.length-POS.length, scratchUTF16_2);
+        pos = ArrayUtil.parseInt(scratchUTF16_2.chars, 0, scratchUTF16_2.length);
+      } else {
+        pos = -1;
+      }
+
+      if (readOffsets) {
+        SimpleTextUtil.readLine(in, scratch);
+        assert StringHelper.startsWith(scratch, START_OFFSET): "got line=" + scratch.utf8ToString();
+        UnicodeUtil.UTF8toUTF16(scratch.bytes, scratch.offset+START_OFFSET.length, scratch.length-START_OFFSET.length, scratchUTF16_2);
+        startOffset = ArrayUtil.parseInt(scratchUTF16_2.chars, 0, scratchUTF16_2.length);
+        SimpleTextUtil.readLine(in, scratch);
+        assert StringHelper.startsWith(scratch, END_OFFSET): "got line=" + scratch.utf8ToString();
+        UnicodeUtil.UTF8toUTF16(scratch.bytes, scratch.offset+END_OFFSET.length, scratch.length-END_OFFSET.length, scratchUTF16_2);
+        endOffset = ArrayUtil.parseInt(scratchUTF16_2.chars, 0, scratchUTF16_2.length);
+      }
+
+      final long fp = in.getFilePointer();
+      SimpleTextUtil.readLine(in, scratch);
+      if (StringHelper.startsWith(scratch, PAYLOAD)) {
+        final int len = scratch.length - PAYLOAD.length;
+        if (scratch2.bytes.length < len) {
+          scratch2.grow(len);
+        }
+        System.arraycopy(scratch.bytes, PAYLOAD.length, scratch2.bytes, 0, len);
+        scratch2.length = len;
+        payload = scratch2;
+      } else {
+        payload = null;
+        in.seek(fp);
+      }
+      return pos;
+    }
+
+    @Override
+    public int startOffset() throws IOException {
+      return startOffset;
+    }
+
+    @Override
+    public int endOffset() throws IOException {
+      return endOffset;
+    }
+
+    @Override
+    public BytesRef getPayload() {
+      return payload;
+    }
+  }
+
+  static class TermData {
+    public long docsStart;
+    public int docFreq;
+
+    public TermData(long docsStart, int docFreq) {
+      this.docsStart = docsStart;
+      this.docFreq = docFreq;
+    }
+  }
+
+  private class SimpleTextTerms extends Terms {
+    private final long termsStart;
+    private final FieldInfo fieldInfo;
+    private long sumTotalTermFreq;
+    private long sumDocFreq;
+    private int docCount;
+    private FST<PairOutputs.Pair<Long,PairOutputs.Pair<Long,Long>>> fst;
+    private int termCount;
+    private final BytesRef scratch = new BytesRef(10);
+    private final CharsRef scratchUTF16 = new CharsRef(10);
+
+    public SimpleTextTerms(String field, long termsStart) throws IOException {
+      this.termsStart = termsStart;
+      fieldInfo = fieldInfos.fieldInfo(field);
+      loadTerms();
+    }
+
+    private void loadTerms() throws IOException {
+      PositiveIntOutputs posIntOutputs = PositiveIntOutputs.getSingleton(false);
+      final Builder<PairOutputs.Pair<Long,PairOutputs.Pair<Long,Long>>> b;
+      final PairOutputs<Long,Long> outputsInner = new PairOutputs<Long,Long>(posIntOutputs, posIntOutputs);
+      final PairOutputs<Long,PairOutputs.Pair<Long,Long>> outputs = new PairOutputs<Long,PairOutputs.Pair<Long,Long>>(posIntOutputs,
+                                                                                                                      outputsInner);
+      b = new Builder<PairOutputs.Pair<Long,PairOutputs.Pair<Long,Long>>>(FST.INPUT_TYPE.BYTE1, outputs);
+      IndexInput in = SimpleTextFieldsReader.this.in.clone();
+      in.seek(termsStart);
+      final BytesRef lastTerm = new BytesRef(10);
+      long lastDocsStart = -1;
+      int docFreq = 0;
+      long totalTermFreq = 0;
+      OpenBitSet visitedDocs = new OpenBitSet();
+      final IntsRef scratchIntsRef = new IntsRef();
+      while(true) {
+        SimpleTextUtil.readLine(in, scratch);
+        if (scratch.equals(END) || StringHelper.startsWith(scratch, FIELD)) {
+          if (lastDocsStart != -1) {
+            b.add(Util.toIntsRef(lastTerm, scratchIntsRef),
+                  outputs.newPair(lastDocsStart,
+                                  outputsInner.newPair((long) docFreq, totalTermFreq)));
+            sumTotalTermFreq += totalTermFreq;
+          }
+          break;
+        } else if (StringHelper.startsWith(scratch, DOC)) {
+          docFreq++;
+          sumDocFreq++;
+          UnicodeUtil.UTF8toUTF16(scratch.bytes, scratch.offset+DOC.length, scratch.length-DOC.length, scratchUTF16);
+          int docID = ArrayUtil.parseInt(scratchUTF16.chars, 0, scratchUTF16.length);
+          visitedDocs.set(docID);
+        } else if (StringHelper.startsWith(scratch, FREQ)) {
+          UnicodeUtil.UTF8toUTF16(scratch.bytes, scratch.offset+FREQ.length, scratch.length-FREQ.length, scratchUTF16);
+          totalTermFreq += ArrayUtil.parseInt(scratchUTF16.chars, 0, scratchUTF16.length);
+        } else if (StringHelper.startsWith(scratch, TERM)) {
+          if (lastDocsStart != -1) {
+            b.add(Util.toIntsRef(lastTerm, scratchIntsRef), outputs.newPair(lastDocsStart,
+                                                                            outputsInner.newPair((long) docFreq, totalTermFreq)));
+          }
+          lastDocsStart = in.getFilePointer();
+          final int len = scratch.length - TERM.length;
+          if (len > lastTerm.length) {
+            lastTerm.grow(len);
+          }
+          System.arraycopy(scratch.bytes, TERM.length, lastTerm.bytes, 0, len);
+          lastTerm.length = len;
+          docFreq = 0;
+          sumTotalTermFreq += totalTermFreq;
+          totalTermFreq = 0;
+          termCount++;
+        }
+      }
+      docCount = (int) visitedDocs.cardinality();
+      fst = b.finish();
+      /*
+      PrintStream ps = new PrintStream("out.dot");
+      fst.toDot(ps);
+      ps.close();
+      System.out.println("SAVED out.dot");
+      */
+      //System.out.println("FST " + fst.sizeInBytes());
+    }
+
+    @Override
+    public TermsEnum iterator(TermsEnum reuse) throws IOException {
+      if (fst != null) {
+        return new SimpleTextTermsEnum(fst, fieldInfo.getIndexOptions());
+      } else {
+        return TermsEnum.EMPTY;
+      }
+    }
+
+    @Override
+    public Comparator<BytesRef> getComparator() {
+      return BytesRef.getUTF8SortedAsUnicodeComparator();
+    }
+
+    @Override
+    public long size() {
+      return (long) termCount;
+    }
+
+    @Override
+    public long getSumTotalTermFreq() {
+      return fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : sumTotalTermFreq;
+    }
+
+    @Override
+    public long getSumDocFreq() throws IOException {
+      return sumDocFreq;
+    }
+
+    @Override
+    public int getDocCount() throws IOException {
+      return docCount;
+    }
+
+    @Override
+    public boolean hasOffsets() {
+      return fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
+    }
+
+    @Override
+    public boolean hasPositions() {
+      return fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;
+    }
+    
+    @Override
+    public boolean hasPayloads() {
+      return fieldInfo.hasPayloads();
+    }
+  }
+
+  @Override
+  public Iterator<String> iterator() {
+    return Collections.unmodifiableSet(fields.keySet()).iterator();
+  }
+
+  private final Map<String,Terms> termsCache = new HashMap<String,Terms>();
+
+  @Override
+  synchronized public Terms terms(String field) throws IOException {
+    Terms terms = termsCache.get(field);
+    if (terms == null) {
+      Long fp = fields.get(field);
+      if (fp == null) {
+        return null;
+      } else {
+        terms = new SimpleTextTerms(field, fp);
+        termsCache.put(field, terms);
+      }
+    }
+    return terms;
+  }
+
+  @Override
+  public int size() {
+    return -1;
+  }
+
+  @Override
+  public void close() throws IOException {
+    in.close();
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldsWriter.java b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldsWriter.java
new file mode 100644
index 0000000..5a59399
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldsWriter.java
@@ -0,0 +1,187 @@
+package org.apache.lucene.codecs.simpletext;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.codecs.FieldsConsumer;
+import org.apache.lucene.codecs.PostingsConsumer;
+import org.apache.lucene.codecs.TermStats;
+import org.apache.lucene.codecs.TermsConsumer;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.store.IndexOutput;
+
+import java.io.IOException;
+import java.util.Comparator;
+
+class SimpleTextFieldsWriter extends FieldsConsumer {
+  
+  private final IndexOutput out;
+  private final BytesRef scratch = new BytesRef(10);
+
+  final static BytesRef END          = new BytesRef("END");
+  final static BytesRef FIELD        = new BytesRef("field ");
+  final static BytesRef TERM         = new BytesRef("  term ");
+  final static BytesRef DOC          = new BytesRef("    doc ");
+  final static BytesRef FREQ         = new BytesRef("      freq ");
+  final static BytesRef POS          = new BytesRef("      pos ");
+  final static BytesRef START_OFFSET = new BytesRef("      startOffset ");
+  final static BytesRef END_OFFSET   = new BytesRef("      endOffset ");
+  final static BytesRef PAYLOAD      = new BytesRef("        payload ");
+
+  public SimpleTextFieldsWriter(SegmentWriteState state) throws IOException {
+    final String fileName = SimpleTextPostingsFormat.getPostingsFileName(state.segmentInfo.name, state.segmentSuffix);
+    out = state.directory.createOutput(fileName, state.context);
+  }
+
+  private void write(String s) throws IOException {
+    SimpleTextUtil.write(out, s, scratch);
+  }
+
+  private void write(BytesRef b) throws IOException {
+    SimpleTextUtil.write(out, b);
+  }
+
+  private void newline() throws IOException {
+    SimpleTextUtil.writeNewline(out);
+  }
+
+  @Override
+  public TermsConsumer addField(FieldInfo field) throws IOException {
+    write(FIELD);
+    write(field.name);
+    newline();
+    return new SimpleTextTermsWriter(field);
+  }
+
+  private class SimpleTextTermsWriter extends TermsConsumer {
+    private final SimpleTextPostingsWriter postingsWriter;
+    
+    public SimpleTextTermsWriter(FieldInfo field) {
+      postingsWriter = new SimpleTextPostingsWriter(field);
+    }
+
+    @Override
+    public PostingsConsumer startTerm(BytesRef term) throws IOException {
+      return postingsWriter.reset(term);
+    }
+
+    @Override
+    public void finishTerm(BytesRef term, TermStats stats) throws IOException {
+    }
+
+    @Override
+    public void finish(long sumTotalTermFreq, long sumDocFreq, int docCount) throws IOException {
+    }
+
+    @Override
+    public Comparator<BytesRef> getComparator() {
+      return BytesRef.getUTF8SortedAsUnicodeComparator();
+    }
+  }
+
+  private class SimpleTextPostingsWriter extends PostingsConsumer {
+    private BytesRef term;
+    private boolean wroteTerm;
+    private final IndexOptions indexOptions;
+    private final boolean writePositions;
+    private final boolean writeOffsets;
+
+    // for assert:
+    private int lastStartOffset = 0;
+
+    public SimpleTextPostingsWriter(FieldInfo field) {
+      this.indexOptions = field.getIndexOptions();
+      writePositions = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;
+      writeOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
+      //System.out.println("writeOffsets=" + writeOffsets);
+      //System.out.println("writePos=" + writePositions);
+    }
+
+    @Override
+    public void startDoc(int docID, int termDocFreq) throws IOException {
+      if (!wroteTerm) {
+        // we lazily do this, in case the term had zero docs
+        write(TERM);
+        write(term);
+        newline();
+        wroteTerm = true;
+      }
+
+      write(DOC);
+      write(Integer.toString(docID));
+      newline();
+      if (indexOptions != IndexOptions.DOCS_ONLY) {
+        write(FREQ);
+        write(Integer.toString(termDocFreq));
+        newline();
+      }
+
+      lastStartOffset = 0;
+    }
+    
+    public PostingsConsumer reset(BytesRef term) {
+      this.term = term;
+      wroteTerm = false;
+      return this;
+    }
+
+    @Override
+    public void addPosition(int position, BytesRef payload, int startOffset, int endOffset) throws IOException {
+      if (writePositions) {
+        write(POS);
+        write(Integer.toString(position));
+        newline();
+      }
+
+      if (writeOffsets) {
+        assert endOffset >= startOffset;
+        assert startOffset >= lastStartOffset: "startOffset=" + startOffset + " lastStartOffset=" + lastStartOffset;
+        lastStartOffset = startOffset;
+        write(START_OFFSET);
+        write(Integer.toString(startOffset));
+        newline();
+        write(END_OFFSET);
+        write(Integer.toString(endOffset));
+        newline();
+      }
+
+      if (payload != null && payload.length > 0) {
+        assert payload.length != 0;
+        write(PAYLOAD);
+        write(payload);
+        newline();
+      }
+    }
+
+    @Override
+    public void finishDoc() {
+    }
+  }
+
+  @Override
+  public void close() throws IOException {
+    try {
+      write(END);
+      newline();
+    } finally {
+      out.close();
+    }
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextLiveDocsFormat.java b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextLiveDocsFormat.java
new file mode 100644
index 0000000..4d9007d
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextLiveDocsFormat.java
@@ -0,0 +1,185 @@
+package org.apache.lucene.codecs.simpletext;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.BitSet;
+import java.util.Collection;
+
+import org.apache.lucene.codecs.LiveDocsFormat;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentInfoPerCommit;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.CharsRef;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.MutableBits;
+import org.apache.lucene.util.StringHelper;
+import org.apache.lucene.util.UnicodeUtil;
+
+/**
+ * reads/writes plaintext live docs
+ * <p>
+ * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
+ * @lucene.experimental
+ */
+public class SimpleTextLiveDocsFormat extends LiveDocsFormat {
+
+  static final String LIVEDOCS_EXTENSION = "liv";
+  
+  final static BytesRef SIZE             = new BytesRef("size ");
+  final static BytesRef DOC              = new BytesRef("  doc ");
+  final static BytesRef END              = new BytesRef("END");
+  
+  @Override
+  public MutableBits newLiveDocs(int size) throws IOException {
+    return new SimpleTextMutableBits(size);
+  }
+
+  @Override
+  public MutableBits newLiveDocs(Bits existing) throws IOException {
+    final SimpleTextBits bits = (SimpleTextBits) existing;
+    return new SimpleTextMutableBits((BitSet)bits.bits.clone(), bits.size);
+  }
+
+  @Override
+  public Bits readLiveDocs(Directory dir, SegmentInfoPerCommit info, IOContext context) throws IOException {
+    assert info.hasDeletions();
+    BytesRef scratch = new BytesRef();
+    CharsRef scratchUTF16 = new CharsRef();
+    
+    String fileName = IndexFileNames.fileNameFromGeneration(info.info.name, LIVEDOCS_EXTENSION, info.getDelGen());
+    IndexInput in = null;
+    boolean success = false;
+    try {
+      in = dir.openInput(fileName, context);
+      
+      SimpleTextUtil.readLine(in, scratch);
+      assert StringHelper.startsWith(scratch, SIZE);
+      int size = parseIntAt(scratch, SIZE.length, scratchUTF16);
+      
+      BitSet bits = new BitSet(size);
+      
+      SimpleTextUtil.readLine(in, scratch);
+      while (!scratch.equals(END)) {
+        assert StringHelper.startsWith(scratch, DOC);
+        int docid = parseIntAt(scratch, DOC.length, scratchUTF16);
+        bits.set(docid);
+        SimpleTextUtil.readLine(in, scratch);
+      }
+      
+      success = true;
+      return new SimpleTextBits(bits, size);
+    } finally {
+      if (success) {
+        IOUtils.close(in);
+      } else {
+        IOUtils.closeWhileHandlingException(in);
+      }
+    }
+  }
+  
+  private int parseIntAt(BytesRef bytes, int offset, CharsRef scratch) {
+    UnicodeUtil.UTF8toUTF16(bytes.bytes, bytes.offset+offset, bytes.length-offset, scratch);
+    return ArrayUtil.parseInt(scratch.chars, 0, scratch.length);
+  }
+
+  @Override
+  public void writeLiveDocs(MutableBits bits, Directory dir, SegmentInfoPerCommit info, int newDelCount, IOContext context) throws IOException {
+    BitSet set = ((SimpleTextBits) bits).bits;
+    int size = bits.length();
+    BytesRef scratch = new BytesRef();
+    
+    String fileName = IndexFileNames.fileNameFromGeneration(info.info.name, LIVEDOCS_EXTENSION, info.getNextDelGen());
+    IndexOutput out = null;
+    boolean success = false;
+    try {
+      out = dir.createOutput(fileName, context);
+      SimpleTextUtil.write(out, SIZE);
+      SimpleTextUtil.write(out, Integer.toString(size), scratch);
+      SimpleTextUtil.writeNewline(out);
+      
+      for (int i = set.nextSetBit(0); i >= 0; i=set.nextSetBit(i + 1)) { 
+        SimpleTextUtil.write(out, DOC);
+        SimpleTextUtil.write(out, Integer.toString(i), scratch);
+        SimpleTextUtil.writeNewline(out);
+      }
+      
+      SimpleTextUtil.write(out, END);
+      SimpleTextUtil.writeNewline(out);
+      success = true;
+    } finally {
+      if (success) {
+        IOUtils.close(out);
+      } else {
+        IOUtils.closeWhileHandlingException(out);
+      }
+    }
+  }
+
+  @Override
+  public void files(SegmentInfoPerCommit info, Collection<String> files) throws IOException {
+    if (info.hasDeletions()) {
+      files.add(IndexFileNames.fileNameFromGeneration(info.info.name, LIVEDOCS_EXTENSION, info.getDelGen()));
+    }
+  }
+  
+  // read-only
+  static class SimpleTextBits implements Bits {
+    final BitSet bits;
+    final int size;
+    
+    SimpleTextBits(BitSet bits, int size) {
+      this.bits = bits;
+      this.size = size;
+    }
+    
+    @Override
+    public boolean get(int index) {
+      return bits.get(index);
+    }
+
+    @Override
+    public int length() {
+      return size;
+    }
+  }
+  
+  // read-write
+  static class SimpleTextMutableBits extends SimpleTextBits implements MutableBits {
+
+    SimpleTextMutableBits(int size) {
+      this(new BitSet(size), size);
+      bits.set(0, size);
+    }
+    
+    SimpleTextMutableBits(BitSet bits, int size) {
+      super(bits, size);
+    }
+    
+    @Override
+    public void clear(int bit) {
+      bits.clear(bit);
+    }
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextNormsFormat.java b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextNormsFormat.java
new file mode 100644
index 0000000..5ba5f74
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextNormsFormat.java
@@ -0,0 +1,124 @@
+package org.apache.lucene.codecs.simpletext;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Comparator;
+
+import org.apache.lucene.codecs.NormsFormat;
+import org.apache.lucene.codecs.PerDocConsumer;
+import org.apache.lucene.codecs.PerDocProducer;
+import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.DocValues.Type;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.PerDocWriteState;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.util.BytesRef;
+
+/**
+ * plain-text norms format.
+ * <p>
+ * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
+ * 
+ * @lucene.experimental
+ */
+public class SimpleTextNormsFormat extends NormsFormat {
+  private static final String NORMS_SEG_SUFFIX = "len";
+  
+  @Override
+  public PerDocConsumer docsConsumer(PerDocWriteState state) throws IOException {
+    return new SimpleTextNormsPerDocConsumer(state);
+  }
+  
+  @Override
+  public PerDocProducer docsProducer(SegmentReadState state) throws IOException {
+    return new SimpleTextNormsPerDocProducer(state,
+        BytesRef.getUTF8SortedAsUnicodeComparator());
+  }
+  
+  /**
+   * Reads plain-text norms.
+   * <p>
+   * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
+   * 
+   * @lucene.experimental
+   */
+  public static class SimpleTextNormsPerDocProducer extends
+      SimpleTextPerDocProducer {
+    
+    public SimpleTextNormsPerDocProducer(SegmentReadState state,
+        Comparator<BytesRef> comp) throws IOException {
+      super(state, comp, NORMS_SEG_SUFFIX);
+    }
+    
+    @Override
+    protected boolean canLoad(FieldInfo info) {
+      return info.hasNorms();
+    }
+    
+    @Override
+    protected Type getDocValuesType(FieldInfo info) {
+      return info.getNormType();
+    }
+    
+    @Override
+    protected boolean anyDocValuesFields(FieldInfos infos) {
+      return infos.hasNorms();
+    }
+    
+  }
+  
+  /**
+   * Writes plain-text norms.
+   * <p>
+   * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
+   * 
+   * @lucene.experimental
+   */
+  public static class SimpleTextNormsPerDocConsumer extends
+      SimpleTextPerDocConsumer {
+    
+    public SimpleTextNormsPerDocConsumer(PerDocWriteState state) {
+      super(state, NORMS_SEG_SUFFIX);
+    }
+    
+    @Override
+    protected DocValues getDocValuesForMerge(AtomicReader reader, FieldInfo info)
+        throws IOException {
+      return reader.normValues(info.name);
+    }
+    
+    @Override
+    protected boolean canMerge(FieldInfo info) {
+      return info.hasNorms();
+    }
+    
+    @Override
+    protected Type getDocValuesType(FieldInfo info) {
+      return info.getNormType();
+    }
+    
+    @Override
+    public void abort() {
+      // We don't have to remove files here: IndexFileDeleter
+      // will do so
+    }
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPerDocConsumer.java b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPerDocConsumer.java
new file mode 100644
index 0000000..d95e09b
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPerDocConsumer.java
@@ -0,0 +1,61 @@
+package org.apache.lucene.codecs.simpletext;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with this
+ * work for additional information regarding copyright ownership. The ASF
+ * licenses this file to You under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ * 
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * 
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+ * License for the specific language governing permissions and limitations under
+ * the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.DocValuesConsumer;
+import org.apache.lucene.codecs.PerDocConsumer;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.PerDocWriteState;
+import org.apache.lucene.index.DocValues.Type;
+
+/**
+ * @lucene.experimental
+ */
+class SimpleTextPerDocConsumer extends PerDocConsumer {
+
+  protected final PerDocWriteState state;
+  protected final String segmentSuffix;
+  public SimpleTextPerDocConsumer(PerDocWriteState state, String segmentSuffix) {
+    this.state = state;
+    this.segmentSuffix = segmentSuffix;
+  }
+
+  @Override
+  public void close() throws IOException {
+
+  }
+
+  @Override
+  public DocValuesConsumer addValuesField(Type type, FieldInfo field)
+      throws IOException {
+    return new SimpleTextDocValuesConsumer(SimpleTextDocValuesFormat.docValuesId(state.segmentInfo.name,
+        field.number), state.directory, state.context, type, segmentSuffix);
+  }
+
+  @Override
+  public void abort() {
+    // We don't have to remove files here: IndexFileDeleter
+    // will do so
+  }
+  
+  static String docValuesId(String segmentsName, int fieldId) {
+    return segmentsName + "_" + fieldId;
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPerDocProducer.java b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPerDocProducer.java
new file mode 100644
index 0000000..639bc54
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPerDocProducer.java
@@ -0,0 +1,435 @@
+package org.apache.lucene.codecs.simpletext;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with this
+ * work for additional information regarding copyright ownership. The ASF
+ * licenses this file to You under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ * 
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * 
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+ * License for the specific language governing permissions and limitations under
+ * the License.
+ */
+import static org.apache.lucene.codecs.simpletext.SimpleTextDocValuesConsumer.DOC;
+import static org.apache.lucene.codecs.simpletext.SimpleTextDocValuesConsumer.END;
+import static org.apache.lucene.codecs.simpletext.SimpleTextDocValuesConsumer.HEADER;
+import static org.apache.lucene.codecs.simpletext.SimpleTextDocValuesConsumer.VALUE;
+import static org.apache.lucene.codecs.simpletext.SimpleTextDocValuesConsumer.VALUE_SIZE;
+
+import java.io.Closeable;
+import java.io.IOException;
+import java.util.Collection;
+import java.util.Comparator;
+import java.util.Map;
+import java.util.TreeMap;
+
+import org.apache.lucene.codecs.DocValuesArraySource;
+import org.apache.lucene.codecs.PerDocProducerBase;
+import org.apache.lucene.index.DocValues;
+import org.apache.lucene.index.DocValues.SortedSource;
+import org.apache.lucene.index.DocValues.Source;
+import org.apache.lucene.index.DocValues.Type;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.BytesRefHash;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.StringHelper;
+import org.apache.lucene.util.packed.PackedInts.Reader;
+
+/**
+ * Reads plain-text DocValues.
+ * <p>
+ * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
+ * 
+ * @lucene.experimental
+ */
+public class SimpleTextPerDocProducer extends PerDocProducerBase {
+  protected final TreeMap<String, DocValues> docValues;
+  private Comparator<BytesRef> comp;
+  private final String segmentSuffix;
+
+  /**
+   * Creates a new {@link SimpleTextPerDocProducer} instance and loads all
+   * {@link DocValues} instances for this segment and codec.
+   */
+  public SimpleTextPerDocProducer(SegmentReadState state,
+      Comparator<BytesRef> comp, String segmentSuffix) throws IOException {
+    this.comp = comp;
+    this.segmentSuffix = segmentSuffix;
+    if (anyDocValuesFields(state.fieldInfos)) {
+      docValues = load(state.fieldInfos, state.segmentInfo.name,
+                       state.segmentInfo.getDocCount(), state.dir, state.context);
+    } else {
+      docValues = new TreeMap<String, DocValues>();
+    }
+  }
+
+  @Override
+  protected Map<String, DocValues> docValues() {
+    return docValues;
+  }
+
+  protected DocValues loadDocValues(int docCount, Directory dir, String id,
+      DocValues.Type type, IOContext context) throws IOException {
+    return new SimpleTextDocValues(dir, context, type, id, docCount, comp, segmentSuffix);
+  }
+
+  @Override
+  protected void closeInternal(Collection<? extends Closeable> closeables)
+      throws IOException {
+    IOUtils.close(closeables);
+  }
+
+  private static class SimpleTextDocValues extends DocValues {
+
+    private int docCount;
+
+    @Override
+    public void close() throws IOException {
+      try {
+        super.close();
+      } finally {
+        IOUtils.close(input);
+      }
+    }
+
+    private Type type;
+    private Comparator<BytesRef> comp;
+    private int valueSize;
+    private final IndexInput input;
+
+    public SimpleTextDocValues(Directory dir, IOContext ctx, Type type,
+        String id, int docCount, Comparator<BytesRef> comp, String segmentSuffix) throws IOException {
+      this.type = type;
+      this.docCount = docCount;
+      this.comp = comp;
+      final String fileName = IndexFileNames.segmentFileName(id, "", segmentSuffix);
+      boolean success = false;
+      IndexInput in = null;
+      try {
+        in = dir.openInput(fileName, ctx);
+        valueSize = readHeader(in);
+        success = true;
+      } finally {
+        if (!success) {
+          IOUtils.closeWhileHandlingException(in);
+        }
+      }
+      input = in;
+
+    }
+
+    @Override
+    public Source load() throws IOException {
+      boolean success = false;
+      IndexInput in = input.clone();
+      try {
+        Source source = null;
+        switch (type) {
+        case BYTES_FIXED_DEREF:
+        case BYTES_FIXED_SORTED:
+        case BYTES_FIXED_STRAIGHT:
+        case BYTES_VAR_DEREF:
+        case BYTES_VAR_SORTED:
+        case BYTES_VAR_STRAIGHT:
+          source = read(in, new ValueReader(type, docCount, comp));
+          break;
+        case FIXED_INTS_16:
+        case FIXED_INTS_32:
+        case VAR_INTS:
+        case FIXED_INTS_64:
+        case FIXED_INTS_8:
+        case FLOAT_32:
+        case FLOAT_64:
+          source = read(in, new ValueReader(type, docCount, null));
+          break;
+        default:
+          throw new IllegalArgumentException("unknown type: " + type);
+        }
+        assert source != null;
+        success = true;
+        return source;
+      } finally {
+        if (!success) {
+          IOUtils.closeWhileHandlingException(in);
+        } else {
+          IOUtils.close(in);
+        }
+      }
+    }
+
+    private int readHeader(IndexInput in) throws IOException {
+      BytesRef scratch = new BytesRef();
+      SimpleTextUtil.readLine(in, scratch);
+      assert StringHelper.startsWith(scratch, HEADER);
+      SimpleTextUtil.readLine(in, scratch);
+      assert StringHelper.startsWith(scratch, VALUE_SIZE);
+      return Integer.parseInt(readString(scratch.offset + VALUE_SIZE.length,
+          scratch));
+    }
+
+    private Source read(IndexInput in, ValueReader reader) throws IOException {
+      BytesRef scratch = new BytesRef();
+      for (int i = 0; i < docCount; i++) {
+        SimpleTextUtil.readLine(in, scratch);
+
+        assert StringHelper.startsWith(scratch, DOC) : scratch.utf8ToString();
+        SimpleTextUtil.readLine(in, scratch);
+        assert StringHelper.startsWith(scratch, VALUE);
+        reader.fromString(i, scratch, scratch.offset + VALUE.length);
+      }
+      SimpleTextUtil.readLine(in, scratch);
+      assert scratch.equals(END);
+      return reader.getSource();
+    }
+
+    @Override
+    public Source getDirectSource() throws IOException {
+      return this.getSource();
+    }
+
+    @Override
+    public int getValueSize() {
+      return valueSize;
+    }
+
+    @Override
+    public Type getType() {
+      return type;
+    }
+
+  }
+
+  public static String readString(int offset, BytesRef scratch) {
+    return new String(scratch.bytes, scratch.offset + offset, scratch.length
+        - offset, IOUtils.CHARSET_UTF_8);
+  }
+
+  private static final class ValueReader {
+    private final Type type;
+    private byte[] bytes;
+    private short[] shorts;
+    private int[] ints;
+    private long[] longs;
+    private float[] floats;
+    private double[] doubles;
+    private Source source;
+    private BytesRefHash hash;
+    private BytesRef scratch;
+
+    public ValueReader(Type type, int maxDocs, Comparator<BytesRef> comp) {
+      super();
+      this.type = type;
+      Source docValuesArray = null;
+      switch (type) {
+      case FIXED_INTS_16:
+        shorts = new short[maxDocs];
+        docValuesArray = DocValuesArraySource.forType(type)
+            .newFromArray(shorts);
+        break;
+      case FIXED_INTS_32:
+        ints = new int[maxDocs];
+        docValuesArray = DocValuesArraySource.forType(type).newFromArray(ints);
+        break;
+      case FIXED_INTS_64:
+        longs = new long[maxDocs];
+        docValuesArray = DocValuesArraySource.forType(type)
+            .newFromArray(longs);
+        break;
+      case VAR_INTS:
+        longs = new long[maxDocs];
+        docValuesArray = new VarIntsArraySource(type, longs);
+        break;
+      case FIXED_INTS_8:
+        bytes = new byte[maxDocs];
+        docValuesArray = DocValuesArraySource.forType(type).newFromArray(bytes);
+        break;
+      case FLOAT_32:
+        floats = new float[maxDocs];
+        docValuesArray = DocValuesArraySource.forType(type)
+            .newFromArray(floats);
+        break;
+      case FLOAT_64:
+        doubles = new double[maxDocs];
+        docValuesArray = DocValuesArraySource.forType(type).newFromArray(
+            doubles);
+        break;
+      case BYTES_FIXED_DEREF:
+      case BYTES_FIXED_SORTED:
+      case BYTES_FIXED_STRAIGHT:
+      case BYTES_VAR_DEREF:
+      case BYTES_VAR_SORTED:
+      case BYTES_VAR_STRAIGHT:
+        assert comp != null;
+        hash = new BytesRefHash();
+        BytesSource bytesSource = new BytesSource(type, comp, maxDocs, hash);
+        ints = bytesSource.docIdToEntry;
+        source = bytesSource;
+        scratch = new BytesRef();
+        break;
+
+      }
+      if (docValuesArray != null) {
+        assert source == null;
+        this.source = docValuesArray;
+      }
+    }
+
+    public void fromString(int ord, BytesRef ref, int offset) {
+      switch (type) {
+      case FIXED_INTS_16:
+        assert shorts != null;
+        shorts[ord] = Short.parseShort(readString(offset, ref));
+        break;
+      case FIXED_INTS_32:
+        assert ints != null;
+        ints[ord] = Integer.parseInt(readString(offset, ref));
+        break;
+      case FIXED_INTS_64:
+      case VAR_INTS:
+        assert longs != null;
+        longs[ord] = Long.parseLong(readString(offset, ref));
+        break;
+      case FIXED_INTS_8:
+        assert bytes != null;
+        bytes[ord] = (byte) Integer.parseInt(readString(offset, ref));
+        break;
+      case FLOAT_32:
+        assert floats != null;
+        floats[ord] = Float.parseFloat(readString(offset, ref));
+        break;
+      case FLOAT_64:
+        assert doubles != null;
+        doubles[ord] = Double.parseDouble(readString(offset, ref));
+        break;
+      case BYTES_FIXED_DEREF:
+      case BYTES_FIXED_SORTED:
+      case BYTES_FIXED_STRAIGHT:
+      case BYTES_VAR_DEREF:
+      case BYTES_VAR_SORTED:
+      case BYTES_VAR_STRAIGHT:
+        scratch.bytes = ref.bytes;
+        scratch.length = ref.length - offset;
+        scratch.offset = ref.offset + offset;
+        int key = hash.add(scratch);
+        ints[ord] = key < 0 ? (-key) - 1 : key;
+        break;
+      }
+    }
+
+    public Source getSource() {
+      if (source instanceof BytesSource) {
+        ((BytesSource) source).maybeSort();
+      }
+      return source;
+    }
+  }
+
+  private static final class BytesSource extends SortedSource {
+
+    private final BytesRefHash hash;
+    int[] docIdToEntry;
+    int[] sortedEntries;
+    int[] adresses;
+    private final boolean isSorted;
+
+    protected BytesSource(Type type, Comparator<BytesRef> comp, int maxDoc,
+        BytesRefHash hash) {
+      super(type, comp);
+      docIdToEntry = new int[maxDoc];
+      this.hash = hash;
+      isSorted = type == Type.BYTES_FIXED_SORTED
+          || type == Type.BYTES_VAR_SORTED;
+    }
+
+    void maybeSort() {
+      if (isSorted) {
+        adresses = new int[hash.size()];
+        sortedEntries = hash.sort(getComparator());
+        for (int i = 0; i < adresses.length; i++) {
+          int entry = sortedEntries[i];
+          adresses[entry] = i;
+        }
+      }
+
+    }
+
+    @Override
+    public BytesRef getBytes(int docID, BytesRef ref) {
+      if (isSorted) {
+        return hash.get(sortedEntries[ord(docID)], ref);
+      } else {
+        return hash.get(docIdToEntry[docID], ref);
+      }
+    }
+
+    @Override
+    public SortedSource asSortedSource() {
+      if (isSorted) {
+        return this;
+      }
+      return null;
+    }
+
+    @Override
+    public int ord(int docID) {
+      assert isSorted;
+      try {
+        return adresses[docIdToEntry[docID]];
+      } catch (Exception e) {
+
+        return 0;
+      }
+    }
+
+    @Override
+    public BytesRef getByOrd(int ord, BytesRef bytesRef) {
+      assert isSorted;
+      return hash.get(sortedEntries[ord], bytesRef);
+    }
+
+    @Override
+    public Reader getDocToOrd() {
+      return null;
+    }
+
+    @Override
+    public int getValueCount() {
+      return hash.size();
+    }
+
+  }
+  
+  private static class VarIntsArraySource extends Source {
+
+    private final long[] array;
+
+    protected VarIntsArraySource(Type type, long[] array) {
+      super(type);
+      this.array = array;
+    }
+
+    @Override
+    public long getInt(int docID) {
+      return array[docID];
+    }
+
+    @Override
+    public BytesRef getBytes(int docID, BytesRef ref) {
+      DocValuesArraySource.copyLong(ref, getInt(docID));
+      return ref;
+    }
+    
+  }
+
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPostingsFormat.java b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPostingsFormat.java
new file mode 100644
index 0000000..4f7cfe0
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPostingsFormat.java
@@ -0,0 +1,59 @@
+package org.apache.lucene.codecs.simpletext;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.FieldsConsumer;
+import org.apache.lucene.codecs.FieldsProducer;
+import org.apache.lucene.codecs.PostingsFormat;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.index.SegmentReadState;
+import org.apache.lucene.index.IndexFileNames;
+
+/** For debugging, curiosity, transparency only!!  Do not
+ *  use this codec in production.
+ *
+ *  <p>This codec stores all postings data in a single
+ *  human-readable text file (_N.pst).  You can view this in
+ *  any text editor, and even edit it to alter your index.
+ *
+ *  @lucene.experimental */
+public class SimpleTextPostingsFormat extends PostingsFormat {
+  
+  public SimpleTextPostingsFormat() {
+    super("SimpleText");
+  }
+
+  @Override
+  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
+    return new SimpleTextFieldsWriter(state);
+  }
+
+  @Override
+  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
+    return new SimpleTextFieldsReader(state);
+  }
+
+  /** Extension of freq postings file */
+  static final String POSTINGS_EXTENSION = "pst";
+
+  static String getPostingsFileName(String segment, String segmentSuffix) {
+    return IndexFileNames.segmentFileName(segment, segmentSuffix, POSTINGS_EXTENSION);
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextSegmentInfoFormat.java b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextSegmentInfoFormat.java
new file mode 100644
index 0000000..53440e7
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextSegmentInfoFormat.java
@@ -0,0 +1,45 @@
+package org.apache.lucene.codecs.simpletext;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.codecs.SegmentInfoFormat;
+import org.apache.lucene.codecs.SegmentInfoReader;
+import org.apache.lucene.codecs.SegmentInfoWriter;
+
+/**
+ * plain text segments file format.
+ * <p>
+ * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
+ * @lucene.experimental
+ */
+public class SimpleTextSegmentInfoFormat extends SegmentInfoFormat {
+  private final SegmentInfoReader reader = new SimpleTextSegmentInfoReader();
+  private final SegmentInfoWriter writer = new SimpleTextSegmentInfoWriter();
+
+  public static final String SI_EXTENSION = "si";
+  
+  @Override
+  public SegmentInfoReader getSegmentInfoReader() {
+    return reader;
+  }
+
+  @Override
+  public SegmentInfoWriter getSegmentInfoWriter() {
+    return writer;
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextSegmentInfoReader.java b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextSegmentInfoReader.java
new file mode 100644
index 0000000..c020f14
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextSegmentInfoReader.java
@@ -0,0 +1,127 @@
+package org.apache.lucene.codecs.simpletext;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.lucene.codecs.SegmentInfoReader;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.StringHelper;
+
+import static org.apache.lucene.codecs.simpletext.SimpleTextSegmentInfoWriter.*;
+
+/**
+ * reads plaintext segments files
+ * <p>
+ * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
+ * @lucene.experimental
+ */
+public class SimpleTextSegmentInfoReader extends SegmentInfoReader {
+
+  @Override
+  public SegmentInfo read(Directory directory, String segmentName, IOContext context) throws IOException {
+    BytesRef scratch = new BytesRef();
+    String segFileName = IndexFileNames.segmentFileName(segmentName, "", SimpleTextSegmentInfoFormat.SI_EXTENSION);
+    IndexInput input = directory.openInput(segFileName, context);
+    boolean success = false;
+    try {
+      SimpleTextUtil.readLine(input, scratch);
+      assert StringHelper.startsWith(scratch, SI_VERSION);
+      final String version = readString(SI_VERSION.length, scratch);
+    
+      SimpleTextUtil.readLine(input, scratch);
+      assert StringHelper.startsWith(scratch, SI_DOCCOUNT);
+      final int docCount = Integer.parseInt(readString(SI_DOCCOUNT.length, scratch));
+    
+      SimpleTextUtil.readLine(input, scratch);
+      assert StringHelper.startsWith(scratch, SI_USECOMPOUND);
+      final boolean isCompoundFile = Boolean.parseBoolean(readString(SI_USECOMPOUND.length, scratch));
+    
+      SimpleTextUtil.readLine(input, scratch);
+      assert StringHelper.startsWith(scratch, SI_NUM_DIAG);
+      int numDiag = Integer.parseInt(readString(SI_NUM_DIAG.length, scratch));
+      Map<String,String> diagnostics = new HashMap<String,String>();
+
+      for (int i = 0; i < numDiag; i++) {
+        SimpleTextUtil.readLine(input, scratch);
+        assert StringHelper.startsWith(scratch, SI_DIAG_KEY);
+        String key = readString(SI_DIAG_KEY.length, scratch);
+      
+        SimpleTextUtil.readLine(input, scratch);
+        assert StringHelper.startsWith(scratch, SI_DIAG_VALUE);
+        String value = readString(SI_DIAG_VALUE.length, scratch);
+        diagnostics.put(key, value);
+      }
+      
+      SimpleTextUtil.readLine(input, scratch);
+      assert StringHelper.startsWith(scratch, SI_NUM_ATTS);
+      int numAtts = Integer.parseInt(readString(SI_NUM_ATTS.length, scratch));
+      Map<String,String> attributes = new HashMap<String,String>();
+
+      for (int i = 0; i < numAtts; i++) {
+        SimpleTextUtil.readLine(input, scratch);
+        assert StringHelper.startsWith(scratch, SI_ATT_KEY);
+        String key = readString(SI_ATT_KEY.length, scratch);
+      
+        SimpleTextUtil.readLine(input, scratch);
+        assert StringHelper.startsWith(scratch, SI_ATT_VALUE);
+        String value = readString(SI_ATT_VALUE.length, scratch);
+        attributes.put(key, value);
+      }
+
+      SimpleTextUtil.readLine(input, scratch);
+      assert StringHelper.startsWith(scratch, SI_NUM_FILES);
+      int numFiles = Integer.parseInt(readString(SI_NUM_FILES.length, scratch));
+      Set<String> files = new HashSet<String>();
+
+      for (int i = 0; i < numFiles; i++) {
+        SimpleTextUtil.readLine(input, scratch);
+        assert StringHelper.startsWith(scratch, SI_FILE);
+        String fileName = readString(SI_FILE.length, scratch);
+        files.add(fileName);
+      }
+
+      SegmentInfo info = new SegmentInfo(directory, version, segmentName, docCount, 
+                                         isCompoundFile, null, diagnostics, Collections.unmodifiableMap(attributes));
+      info.setFiles(files);
+      success = true;
+      return info;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(input);
+      } else {
+        input.close();
+      }
+    }
+  }
+
+  private String readString(int offset, BytesRef scratch) {
+    return new String(scratch.bytes, scratch.offset+offset, scratch.length-offset, IOUtils.CHARSET_UTF_8);
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextSegmentInfoWriter.java b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextSegmentInfoWriter.java
new file mode 100644
index 0000000..cbad776
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextSegmentInfoWriter.java
@@ -0,0 +1,136 @@
+package org.apache.lucene.codecs.simpletext;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.lucene.codecs.SegmentInfoWriter;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+
+/**
+ * writes plaintext segments files
+ * <p>
+ * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
+ * @lucene.experimental
+ */
+public class SimpleTextSegmentInfoWriter extends SegmentInfoWriter {
+
+  final static BytesRef SI_VERSION          = new BytesRef("    version ");
+  final static BytesRef SI_DOCCOUNT         = new BytesRef("    number of documents ");
+  final static BytesRef SI_USECOMPOUND      = new BytesRef("    uses compound file ");
+  final static BytesRef SI_NUM_DIAG         = new BytesRef("    diagnostics ");
+  final static BytesRef SI_DIAG_KEY         = new BytesRef("      key ");
+  final static BytesRef SI_DIAG_VALUE       = new BytesRef("      value ");
+  final static BytesRef SI_NUM_ATTS         = new BytesRef("    attributes ");
+  final static BytesRef SI_ATT_KEY          = new BytesRef("      key ");
+  final static BytesRef SI_ATT_VALUE        = new BytesRef("      value ");
+  final static BytesRef SI_NUM_FILES        = new BytesRef("    files ");
+  final static BytesRef SI_FILE             = new BytesRef("      file ");
+  
+  @Override
+  public void write(Directory dir, SegmentInfo si, FieldInfos fis, IOContext ioContext) throws IOException {
+
+    String segFileName = IndexFileNames.segmentFileName(si.name, "", SimpleTextSegmentInfoFormat.SI_EXTENSION);
+    si.addFile(segFileName);
+
+    boolean success = false;
+    IndexOutput output = dir.createOutput(segFileName,  ioContext);
+
+    try {
+      BytesRef scratch = new BytesRef();
+    
+      SimpleTextUtil.write(output, SI_VERSION);
+      SimpleTextUtil.write(output, si.getVersion(), scratch);
+      SimpleTextUtil.writeNewline(output);
+    
+      SimpleTextUtil.write(output, SI_DOCCOUNT);
+      SimpleTextUtil.write(output, Integer.toString(si.getDocCount()), scratch);
+      SimpleTextUtil.writeNewline(output);
+    
+      SimpleTextUtil.write(output, SI_USECOMPOUND);
+      SimpleTextUtil.write(output, Boolean.toString(si.getUseCompoundFile()), scratch);
+      SimpleTextUtil.writeNewline(output);
+    
+      Map<String,String> diagnostics = si.getDiagnostics();
+      int numDiagnostics = diagnostics == null ? 0 : diagnostics.size();
+      SimpleTextUtil.write(output, SI_NUM_DIAG);
+      SimpleTextUtil.write(output, Integer.toString(numDiagnostics), scratch);
+      SimpleTextUtil.writeNewline(output);
+    
+      if (numDiagnostics > 0) {
+        for (Map.Entry<String,String> diagEntry : diagnostics.entrySet()) {
+          SimpleTextUtil.write(output, SI_DIAG_KEY);
+          SimpleTextUtil.write(output, diagEntry.getKey(), scratch);
+          SimpleTextUtil.writeNewline(output);
+        
+          SimpleTextUtil.write(output, SI_DIAG_VALUE);
+          SimpleTextUtil.write(output, diagEntry.getValue(), scratch);
+          SimpleTextUtil.writeNewline(output);
+        }
+      }
+      
+      Map<String,String> atts = si.attributes();
+      int numAtts = atts == null ? 0 : atts.size();
+      SimpleTextUtil.write(output, SI_NUM_ATTS);
+      SimpleTextUtil.write(output, Integer.toString(numAtts), scratch);
+      SimpleTextUtil.writeNewline(output);
+    
+      if (numAtts > 0) {
+        for (Map.Entry<String,String> entry : atts.entrySet()) {
+          SimpleTextUtil.write(output, SI_ATT_KEY);
+          SimpleTextUtil.write(output, entry.getKey(), scratch);
+          SimpleTextUtil.writeNewline(output);
+        
+          SimpleTextUtil.write(output, SI_ATT_VALUE);
+          SimpleTextUtil.write(output, entry.getValue(), scratch);
+          SimpleTextUtil.writeNewline(output);
+        }
+      }
+
+      Set<String> files = si.files();
+      int numFiles = files == null ? 0 : files.size();
+      SimpleTextUtil.write(output, SI_NUM_FILES);
+      SimpleTextUtil.write(output, Integer.toString(numFiles), scratch);
+      SimpleTextUtil.writeNewline(output);
+
+      if (numFiles > 0) {
+        for(String fileName : files) {
+          SimpleTextUtil.write(output, SI_FILE);
+          SimpleTextUtil.write(output, fileName, scratch);
+          SimpleTextUtil.writeNewline(output);
+        }
+      }
+      success = true;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(output);
+      } else {
+        output.close();
+      }
+    }
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextStoredFieldsFormat.java b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextStoredFieldsFormat.java
new file mode 100644
index 0000000..13c73b4
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextStoredFieldsFormat.java
@@ -0,0 +1,47 @@
+package org.apache.lucene.codecs.simpletext;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.StoredFieldsFormat;
+import org.apache.lucene.codecs.StoredFieldsReader;
+import org.apache.lucene.codecs.StoredFieldsWriter;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+
+/**
+ * plain text stored fields format.
+ * <p>
+ * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
+ * @lucene.experimental
+ */
+public class SimpleTextStoredFieldsFormat extends StoredFieldsFormat {
+
+  @Override
+  public StoredFieldsReader fieldsReader(Directory directory, SegmentInfo si, FieldInfos fn, IOContext context) throws IOException {;
+    return new SimpleTextStoredFieldsReader(directory, si, fn, context);
+  }
+
+  @Override
+  public StoredFieldsWriter fieldsWriter(Directory directory, SegmentInfo si, IOContext context) throws IOException {
+    return new SimpleTextStoredFieldsWriter(directory, si.name, context);
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextStoredFieldsReader.java b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextStoredFieldsReader.java
new file mode 100644
index 0000000..3cbbb89
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextStoredFieldsReader.java
@@ -0,0 +1,192 @@
+package org.apache.lucene.codecs.simpletext;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+
+import org.apache.lucene.codecs.StoredFieldsReader;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.StoredFieldVisitor;
+import org.apache.lucene.store.AlreadyClosedException;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.CharsRef;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.StringHelper;
+import org.apache.lucene.util.UnicodeUtil;
+
+import static org.apache.lucene.codecs.simpletext.SimpleTextStoredFieldsWriter.*;
+
+/**
+ * reads plaintext stored fields
+ * <p>
+ * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
+ * @lucene.experimental
+ */
+public class SimpleTextStoredFieldsReader extends StoredFieldsReader {
+  private ArrayList<Long> offsets; /* docid -> offset in .fld file */
+  private IndexInput in;
+  private BytesRef scratch = new BytesRef();
+  private CharsRef scratchUTF16 = new CharsRef();
+  private final FieldInfos fieldInfos;
+
+  public SimpleTextStoredFieldsReader(Directory directory, SegmentInfo si, FieldInfos fn, IOContext context) throws IOException {
+    this.fieldInfos = fn;
+    boolean success = false;
+    try {
+      in = directory.openInput(IndexFileNames.segmentFileName(si.name, "", SimpleTextStoredFieldsWriter.FIELDS_EXTENSION), context);
+      success = true;
+    } finally {
+      if (!success) {
+        close();
+      }
+    }
+    readIndex();
+  }
+  
+  // used by clone
+  SimpleTextStoredFieldsReader(ArrayList<Long> offsets, IndexInput in, FieldInfos fieldInfos) {
+    this.offsets = offsets;
+    this.in = in;
+    this.fieldInfos = fieldInfos;
+  }
+  
+  // we don't actually write a .fdx-like index, instead we read the 
+  // stored fields file in entirety up-front and save the offsets 
+  // so we can seek to the documents later.
+  private void readIndex() throws IOException {
+    offsets = new ArrayList<Long>();
+    while (!scratch.equals(END)) {
+      readLine();
+      if (StringHelper.startsWith(scratch, DOC)) {
+        offsets.add(in.getFilePointer());
+      }
+    }
+  }
+  
+  @Override
+  public void visitDocument(int n, StoredFieldVisitor visitor) throws IOException {
+    in.seek(offsets.get(n));
+    readLine();
+    assert StringHelper.startsWith(scratch, NUM);
+    int numFields = parseIntAt(NUM.length);
+    
+    for (int i = 0; i < numFields; i++) {
+      readLine();
+      assert StringHelper.startsWith(scratch, FIELD);
+      int fieldNumber = parseIntAt(FIELD.length);
+      FieldInfo fieldInfo = fieldInfos.fieldInfo(fieldNumber);
+      readLine();
+      assert StringHelper.startsWith(scratch, NAME);
+      readLine();
+      assert StringHelper.startsWith(scratch, TYPE);
+      
+      final BytesRef type;
+      if (equalsAt(TYPE_STRING, scratch, TYPE.length)) {
+        type = TYPE_STRING;
+      } else if (equalsAt(TYPE_BINARY, scratch, TYPE.length)) {
+        type = TYPE_BINARY;
+      } else if (equalsAt(TYPE_INT, scratch, TYPE.length)) {
+        type = TYPE_INT;
+      } else if (equalsAt(TYPE_LONG, scratch, TYPE.length)) {
+        type = TYPE_LONG;
+      } else if (equalsAt(TYPE_FLOAT, scratch, TYPE.length)) {
+        type = TYPE_FLOAT;
+      } else if (equalsAt(TYPE_DOUBLE, scratch, TYPE.length)) {
+        type = TYPE_DOUBLE;
+      } else {
+        throw new RuntimeException("unknown field type");
+      }
+      
+      switch (visitor.needsField(fieldInfo)) {
+        case YES:  
+          readField(type, fieldInfo, visitor);
+          break;
+        case NO:   
+          readLine();
+          assert StringHelper.startsWith(scratch, VALUE);
+          break;
+        case STOP: return;
+      }
+    }
+  }
+  
+  private void readField(BytesRef type, FieldInfo fieldInfo, StoredFieldVisitor visitor) throws IOException {
+    readLine();
+    assert StringHelper.startsWith(scratch, VALUE);
+    if (type == TYPE_STRING) {
+      visitor.stringField(fieldInfo, new String(scratch.bytes, scratch.offset+VALUE.length, scratch.length-VALUE.length, "UTF-8"));
+    } else if (type == TYPE_BINARY) {
+      // TODO: who owns the bytes?
+      byte[] copy = new byte[scratch.length-VALUE.length];
+      System.arraycopy(scratch.bytes, scratch.offset+VALUE.length, copy, 0, copy.length);
+      visitor.binaryField(fieldInfo, copy, 0, copy.length);
+    } else if (type == TYPE_INT) {
+      UnicodeUtil.UTF8toUTF16(scratch.bytes, scratch.offset+VALUE.length, scratch.length-VALUE.length, scratchUTF16);
+      visitor.intField(fieldInfo, Integer.parseInt(scratchUTF16.toString()));
+    } else if (type == TYPE_LONG) {
+      UnicodeUtil.UTF8toUTF16(scratch.bytes, scratch.offset+VALUE.length, scratch.length-VALUE.length, scratchUTF16);
+      visitor.longField(fieldInfo, Long.parseLong(scratchUTF16.toString()));
+    } else if (type == TYPE_FLOAT) {
+      UnicodeUtil.UTF8toUTF16(scratch.bytes, scratch.offset+VALUE.length, scratch.length-VALUE.length, scratchUTF16);
+      visitor.floatField(fieldInfo, Float.parseFloat(scratchUTF16.toString()));
+    } else if (type == TYPE_DOUBLE) {
+      UnicodeUtil.UTF8toUTF16(scratch.bytes, scratch.offset+VALUE.length, scratch.length-VALUE.length, scratchUTF16);
+      visitor.doubleField(fieldInfo, Double.parseDouble(scratchUTF16.toString()));
+    }
+  }
+
+  @Override
+  public StoredFieldsReader clone() {
+    if (in == null) {
+      throw new AlreadyClosedException("this FieldsReader is closed");
+    }
+    return new SimpleTextStoredFieldsReader(offsets, in.clone(), fieldInfos);
+  }
+  
+  @Override
+  public void close() throws IOException {
+    try {
+      IOUtils.close(in); 
+    } finally {
+      in = null;
+      offsets = null;
+    }
+  }
+  
+  private void readLine() throws IOException {
+    SimpleTextUtil.readLine(in, scratch);
+  }
+  
+  private int parseIntAt(int offset) {
+    UnicodeUtil.UTF8toUTF16(scratch.bytes, scratch.offset+offset, scratch.length-offset, scratchUTF16);
+    return ArrayUtil.parseInt(scratchUTF16.chars, 0, scratchUTF16.length);
+  }
+  
+  private boolean equalsAt(BytesRef a, BytesRef b, int bOffset) {
+    return a.length == b.length - bOffset && 
+        ArrayUtil.equals(a.bytes, a.offset, b.bytes, b.offset + bOffset, b.length - bOffset);
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextStoredFieldsWriter.java b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextStoredFieldsWriter.java
new file mode 100644
index 0000000..6a9b223
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextStoredFieldsWriter.java
@@ -0,0 +1,196 @@
+package org.apache.lucene.codecs.simpletext;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.StoredFieldsWriter;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.StorableField;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+
+/**
+ * Writes plain-text stored fields.
+ * <p>
+ * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
+ * @lucene.experimental
+ */
+public class SimpleTextStoredFieldsWriter extends StoredFieldsWriter {
+  private int numDocsWritten = 0;
+  private final Directory directory;
+  private final String segment;
+  private IndexOutput out;
+  
+  final static String FIELDS_EXTENSION = "fld";
+  
+  final static BytesRef TYPE_STRING = new BytesRef("string");
+  final static BytesRef TYPE_BINARY = new BytesRef("binary");
+  final static BytesRef TYPE_INT    = new BytesRef("int");
+  final static BytesRef TYPE_LONG   = new BytesRef("long");
+  final static BytesRef TYPE_FLOAT  = new BytesRef("float");
+  final static BytesRef TYPE_DOUBLE = new BytesRef("double");
+
+  final static BytesRef END     = new BytesRef("END");
+  final static BytesRef DOC     = new BytesRef("doc ");
+  final static BytesRef NUM     = new BytesRef("  numfields ");
+  final static BytesRef FIELD   = new BytesRef("  field ");
+  final static BytesRef NAME    = new BytesRef("    name ");
+  final static BytesRef TYPE    = new BytesRef("    type ");
+  final static BytesRef VALUE   = new BytesRef("    value ");
+  
+  private final BytesRef scratch = new BytesRef();
+  
+  public SimpleTextStoredFieldsWriter(Directory directory, String segment, IOContext context) throws IOException {
+    this.directory = directory;
+    this.segment = segment;
+    boolean success = false;
+    try {
+      out = directory.createOutput(IndexFileNames.segmentFileName(segment, "", FIELDS_EXTENSION), context);
+      success = true;
+    } finally {
+      if (!success) {
+        abort();
+      }
+    }
+  }
+
+  @Override
+  public void startDocument(int numStoredFields) throws IOException {
+    write(DOC);
+    write(Integer.toString(numDocsWritten));
+    newLine();
+    
+    write(NUM);
+    write(Integer.toString(numStoredFields));
+    newLine();
+    
+    numDocsWritten++;
+  }
+
+  @Override
+  public void writeField(FieldInfo info, StorableField field) throws IOException {
+    write(FIELD);
+    write(Integer.toString(info.number));
+    newLine();
+    
+    write(NAME);
+    write(field.name());
+    newLine();
+    
+    write(TYPE);
+    final Number n = field.numericValue();
+
+    if (n != null) {
+      if (n instanceof Byte || n instanceof Short || n instanceof Integer) {
+        write(TYPE_INT);
+        newLine();
+          
+        write(VALUE);
+        write(Integer.toString(n.intValue()));
+        newLine();
+      } else if (n instanceof Long) {
+        write(TYPE_LONG);
+        newLine();
+
+        write(VALUE);
+        write(Long.toString(n.longValue()));
+        newLine();
+      } else if (n instanceof Float) {
+        write(TYPE_FLOAT);
+        newLine();
+          
+        write(VALUE);
+        write(Float.toString(n.floatValue()));
+        newLine();
+      } else if (n instanceof Double) {
+        write(TYPE_DOUBLE);
+        newLine();
+          
+        write(VALUE);
+        write(Double.toString(n.doubleValue()));
+        newLine();
+      } else {
+        throw new IllegalArgumentException("cannot store numeric type " + n.getClass());
+      }
+    } else { 
+      BytesRef bytes = field.binaryValue();
+      if (bytes != null) {
+        write(TYPE_BINARY);
+        newLine();
+        
+        write(VALUE);
+        write(bytes);
+        newLine();
+      } else if (field.stringValue() == null) {
+        throw new IllegalArgumentException("field " + field.name() + " is stored but does not have binaryValue, stringValue nor numericValue");
+      } else {
+        write(TYPE_STRING);
+        newLine();
+        
+        write(VALUE);
+        write(field.stringValue());
+        newLine();
+      }
+    }
+  }
+
+  @Override
+  public void abort() {
+    try {
+      close();
+    } catch (IOException ignored) {}
+    IOUtils.deleteFilesIgnoringExceptions(directory, IndexFileNames.segmentFileName(segment, "", FIELDS_EXTENSION));
+  }
+
+  @Override
+  public void finish(FieldInfos fis, int numDocs) throws IOException {
+    if (numDocsWritten != numDocs) {
+      throw new RuntimeException("mergeFields produced an invalid result: docCount is " + numDocs 
+          + " but only saw " + numDocsWritten + " file=" + out.toString() + "; now aborting this merge to prevent index corruption");
+    }
+    write(END);
+    newLine();
+  }
+
+  @Override
+  public void close() throws IOException {
+    try {
+      IOUtils.close(out);
+    } finally {
+      out = null;
+    }
+  }
+  
+  private void write(String s) throws IOException {
+    SimpleTextUtil.write(out, s, scratch);
+  }
+  
+  private void write(BytesRef bytes) throws IOException {
+    SimpleTextUtil.write(out, bytes);
+  }
+  
+  private void newLine() throws IOException {
+    SimpleTextUtil.writeNewline(out);
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextTermVectorsFormat.java b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextTermVectorsFormat.java
new file mode 100644
index 0000000..a0fe06d
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextTermVectorsFormat.java
@@ -0,0 +1,47 @@
+package org.apache.lucene.codecs.simpletext;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.codecs.TermVectorsFormat;
+import org.apache.lucene.codecs.TermVectorsReader;
+import org.apache.lucene.codecs.TermVectorsWriter;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+
+/**
+ * plain text term vectors format.
+ * <p>
+ * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
+ * @lucene.experimental
+ */
+public class SimpleTextTermVectorsFormat extends TermVectorsFormat {
+
+  @Override
+  public TermVectorsReader vectorsReader(Directory directory, SegmentInfo segmentInfo, FieldInfos fieldInfos, IOContext context) throws IOException {
+    return new SimpleTextTermVectorsReader(directory, segmentInfo, context);
+  }
+
+  @Override
+  public TermVectorsWriter vectorsWriter(Directory directory, SegmentInfo segmentInfo, IOContext context) throws IOException {
+    return new SimpleTextTermVectorsWriter(directory, segmentInfo.name, context);
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextTermVectorsReader.java b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextTermVectorsReader.java
new file mode 100644
index 0000000..3b80d3d
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextTermVectorsReader.java
@@ -0,0 +1,541 @@
+package org.apache.lucene.codecs.simpletext;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.Comparator;
+import java.util.Iterator;
+import java.util.Map;
+import java.util.SortedMap;
+import java.util.TreeMap;
+
+import org.apache.lucene.codecs.TermVectorsReader;
+import org.apache.lucene.index.DocsAndPositionsEnum;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.Fields;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.Terms;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.store.AlreadyClosedException;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.CharsRef;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.StringHelper;
+import org.apache.lucene.util.UnicodeUtil;
+import static org.apache.lucene.codecs.simpletext.SimpleTextTermVectorsWriter.*;
+
+/**
+ * Reads plain-text term vectors.
+ * <p>
+ * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
+ * @lucene.experimental
+ */
+public class SimpleTextTermVectorsReader extends TermVectorsReader {
+  private ArrayList<Long> offsets; /* docid -> offset in .vec file */
+  private IndexInput in;
+  private BytesRef scratch = new BytesRef();
+  private CharsRef scratchUTF16 = new CharsRef();
+  
+  public SimpleTextTermVectorsReader(Directory directory, SegmentInfo si, IOContext context) throws IOException {
+    boolean success = false;
+    try {
+      in = directory.openInput(IndexFileNames.segmentFileName(si.name, "", VECTORS_EXTENSION), context);
+      success = true;
+    } finally {
+      if (!success) {
+        close();
+      }
+    }
+    readIndex();
+  }
+  
+  // used by clone
+  SimpleTextTermVectorsReader(ArrayList<Long> offsets, IndexInput in) {
+    this.offsets = offsets;
+    this.in = in;
+  }
+  
+  // we don't actually write a .tvx-like index, instead we read the 
+  // vectors file in entirety up-front and save the offsets 
+  // so we can seek to the data later.
+  private void readIndex() throws IOException {
+    offsets = new ArrayList<Long>();
+    while (!scratch.equals(END)) {
+      readLine();
+      if (StringHelper.startsWith(scratch, DOC)) {
+        offsets.add(in.getFilePointer());
+      }
+    }
+  }
+  
+  @Override
+  public Fields get(int doc) throws IOException {
+    // TestTV tests for this in testBadParams... but is this
+    // really guaranteed by the API?
+    if (doc < 0 || doc >= offsets.size()) {
+      throw new IllegalArgumentException("doc id out of range");
+    }
+
+    SortedMap<String,SimpleTVTerms> fields = new TreeMap<String,SimpleTVTerms>();
+    in.seek(offsets.get(doc));
+    readLine();
+    assert StringHelper.startsWith(scratch, NUMFIELDS);
+    int numFields = parseIntAt(NUMFIELDS.length);
+    if (numFields == 0) {
+      return null; // no vectors for this doc
+    }
+    for (int i = 0; i < numFields; i++) {
+      readLine();
+      assert StringHelper.startsWith(scratch, FIELD);
+      // skip fieldNumber:
+      parseIntAt(FIELD.length);
+      
+      readLine();
+      assert StringHelper.startsWith(scratch, FIELDNAME);
+      String fieldName = readString(FIELDNAME.length, scratch);
+      
+      readLine();
+      assert StringHelper.startsWith(scratch, FIELDPOSITIONS);
+      boolean positions = Boolean.parseBoolean(readString(FIELDPOSITIONS.length, scratch));
+      
+      readLine();
+      assert StringHelper.startsWith(scratch, FIELDOFFSETS);
+      boolean offsets = Boolean.parseBoolean(readString(FIELDOFFSETS.length, scratch));
+      
+      readLine();
+      assert StringHelper.startsWith(scratch, FIELDPAYLOADS);
+      boolean payloads = Boolean.parseBoolean(readString(FIELDPAYLOADS.length, scratch));
+      
+      readLine();
+      assert StringHelper.startsWith(scratch, FIELDTERMCOUNT);
+      int termCount = parseIntAt(FIELDTERMCOUNT.length);
+      
+      SimpleTVTerms terms = new SimpleTVTerms(offsets, positions, payloads);
+      fields.put(fieldName, terms);
+      
+      for (int j = 0; j < termCount; j++) {
+        readLine();
+        assert StringHelper.startsWith(scratch, TERMTEXT);
+        BytesRef term = new BytesRef();
+        int termLength = scratch.length - TERMTEXT.length;
+        term.grow(termLength);
+        term.length = termLength;
+        System.arraycopy(scratch.bytes, scratch.offset+TERMTEXT.length, term.bytes, term.offset, termLength);
+        
+        SimpleTVPostings postings = new SimpleTVPostings();
+        terms.terms.put(term, postings);
+        
+        readLine();
+        assert StringHelper.startsWith(scratch, TERMFREQ);
+        postings.freq = parseIntAt(TERMFREQ.length);
+        
+        if (positions || offsets) {
+          if (positions) {
+            postings.positions = new int[postings.freq];
+            if (payloads) {
+              postings.payloads = new BytesRef[postings.freq];
+            }
+          }
+        
+          if (offsets) {
+            postings.startOffsets = new int[postings.freq];
+            postings.endOffsets = new int[postings.freq];
+          }
+          
+          for (int k = 0; k < postings.freq; k++) {
+            if (positions) {
+              readLine();
+              assert StringHelper.startsWith(scratch, POSITION);
+              postings.positions[k] = parseIntAt(POSITION.length);
+              if (payloads) {
+                readLine();
+                assert StringHelper.startsWith(scratch, PAYLOAD);
+                if (scratch.length - PAYLOAD.length == 0) {
+                  postings.payloads[k] = null;
+                } else {
+                  byte payloadBytes[] = new byte[scratch.length - PAYLOAD.length];
+                  System.arraycopy(scratch.bytes, scratch.offset+PAYLOAD.length, payloadBytes, 0, payloadBytes.length);
+                  postings.payloads[k] = new BytesRef(payloadBytes);
+                }
+              }
+            }
+            
+            if (offsets) {
+              readLine();
+              assert StringHelper.startsWith(scratch, STARTOFFSET);
+              postings.startOffsets[k] = parseIntAt(STARTOFFSET.length);
+              
+              readLine();
+              assert StringHelper.startsWith(scratch, ENDOFFSET);
+              postings.endOffsets[k] = parseIntAt(ENDOFFSET.length);
+            }
+          }
+        }
+      }
+    }
+    return new SimpleTVFields(fields);
+  }
+
+  @Override
+  public TermVectorsReader clone() {
+    if (in == null) {
+      throw new AlreadyClosedException("this TermVectorsReader is closed");
+    }
+    return new SimpleTextTermVectorsReader(offsets, in.clone());
+  }
+  
+  @Override
+  public void close() throws IOException {
+    try {
+      IOUtils.close(in); 
+    } finally {
+      in = null;
+      offsets = null;
+    }
+  }
+
+  private void readLine() throws IOException {
+    SimpleTextUtil.readLine(in, scratch);
+  }
+  
+  private int parseIntAt(int offset) {
+    UnicodeUtil.UTF8toUTF16(scratch.bytes, scratch.offset+offset, scratch.length-offset, scratchUTF16);
+    return ArrayUtil.parseInt(scratchUTF16.chars, 0, scratchUTF16.length);
+  }
+  
+  private String readString(int offset, BytesRef scratch) {
+    UnicodeUtil.UTF8toUTF16(scratch.bytes, scratch.offset+offset, scratch.length-offset, scratchUTF16);
+    return scratchUTF16.toString();
+  }
+  
+  private class SimpleTVFields extends Fields {
+    private final SortedMap<String,SimpleTVTerms> fields;
+    
+    SimpleTVFields(SortedMap<String,SimpleTVTerms> fields) {
+      this.fields = fields;
+    }
+
+    @Override
+    public Iterator<String> iterator() {
+      return Collections.unmodifiableSet(fields.keySet()).iterator();
+    }
+
+    @Override
+    public Terms terms(String field) throws IOException {
+      return fields.get(field);
+    }
+
+    @Override
+    public int size() {
+      return fields.size();
+    }
+  }
+  
+  private static class SimpleTVTerms extends Terms {
+    final SortedMap<BytesRef,SimpleTVPostings> terms;
+    final boolean hasOffsets;
+    final boolean hasPositions;
+    final boolean hasPayloads;
+    
+    SimpleTVTerms(boolean hasOffsets, boolean hasPositions, boolean hasPayloads) {
+      this.hasOffsets = hasOffsets;
+      this.hasPositions = hasPositions;
+      this.hasPayloads = hasPayloads;
+      terms = new TreeMap<BytesRef,SimpleTVPostings>();
+    }
+    
+    @Override
+    public TermsEnum iterator(TermsEnum reuse) throws IOException {
+      // TODO: reuse
+      return new SimpleTVTermsEnum(terms);
+    }
+
+    @Override
+    public Comparator<BytesRef> getComparator() throws IOException {
+      return BytesRef.getUTF8SortedAsUnicodeComparator();
+    }
+
+    @Override
+    public long size() throws IOException {
+      return terms.size();
+    }
+
+    @Override
+    public long getSumTotalTermFreq() throws IOException {
+      return -1;
+    }
+
+    @Override
+    public long getSumDocFreq() throws IOException {
+      return terms.size();
+    }
+
+    @Override
+    public int getDocCount() throws IOException {
+      return 1;
+    }
+
+    @Override
+    public boolean hasOffsets() {
+      return hasOffsets;
+    }
+
+    @Override
+    public boolean hasPositions() {
+      return hasPositions;
+    }
+    
+    @Override
+    public boolean hasPayloads() {
+      return hasPayloads;
+    }
+  }
+  
+  private static class SimpleTVPostings {
+    private int freq;
+    private int positions[];
+    private int startOffsets[];
+    private int endOffsets[];
+    private BytesRef payloads[];
+  }
+  
+  private static class SimpleTVTermsEnum extends TermsEnum {
+    SortedMap<BytesRef,SimpleTVPostings> terms;
+    Iterator<Map.Entry<BytesRef,SimpleTextTermVectorsReader.SimpleTVPostings>> iterator;
+    Map.Entry<BytesRef,SimpleTextTermVectorsReader.SimpleTVPostings> current;
+    
+    SimpleTVTermsEnum(SortedMap<BytesRef,SimpleTVPostings> terms) {
+      this.terms = terms;
+      this.iterator = terms.entrySet().iterator();
+    }
+    
+    @Override
+    public SeekStatus seekCeil(BytesRef text, boolean useCache) throws IOException {
+      iterator = terms.tailMap(text).entrySet().iterator();
+      if (!iterator.hasNext()) {
+        return SeekStatus.END;
+      } else {
+        return next().equals(text) ? SeekStatus.FOUND : SeekStatus.NOT_FOUND;
+      }
+    }
+
+    @Override
+    public void seekExact(long ord) throws IOException {
+      throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public BytesRef next() throws IOException {
+      if (!iterator.hasNext()) {
+        return null;
+      } else {
+        current = iterator.next();
+        return current.getKey();
+      }
+    }
+
+    @Override
+    public BytesRef term() throws IOException {
+      return current.getKey();
+    }
+
+    @Override
+    public long ord() throws IOException {
+      throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public int docFreq() throws IOException {
+      return 1;
+    }
+
+    @Override
+    public long totalTermFreq() throws IOException {
+      return current.getValue().freq;
+    }
+
+    @Override
+    public DocsEnum docs(Bits liveDocs, DocsEnum reuse, int flags) throws IOException {
+      // TODO: reuse
+      SimpleTVDocsEnum e = new SimpleTVDocsEnum();
+      e.reset(liveDocs, (flags & DocsEnum.FLAG_FREQS) == 0 ? 1 : current.getValue().freq);
+      return e;
+    }
+
+    @Override
+    public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum reuse, int flags) throws IOException {
+      SimpleTVPostings postings = current.getValue();
+      if (postings.positions == null && postings.startOffsets == null) {
+        return null;
+      }
+      // TODO: reuse
+      SimpleTVDocsAndPositionsEnum e = new SimpleTVDocsAndPositionsEnum();
+      e.reset(liveDocs, postings.positions, postings.startOffsets, postings.endOffsets, postings.payloads);
+      return e;
+    }
+
+    @Override
+    public Comparator<BytesRef> getComparator() {
+      return BytesRef.getUTF8SortedAsUnicodeComparator();
+    }
+  }
+  
+  // note: these two enum classes are exactly like the Default impl...
+  private static class SimpleTVDocsEnum extends DocsEnum {
+    private boolean didNext;
+    private int doc = -1;
+    private int freq;
+    private Bits liveDocs;
+
+    @Override
+    public int freq() throws IOException {
+      assert freq != -1;
+      return freq;
+    }
+
+    @Override
+    public int docID() {
+      return doc;
+    }
+
+    @Override
+    public int nextDoc() {
+      if (!didNext && (liveDocs == null || liveDocs.get(0))) {
+        didNext = true;
+        return (doc = 0);
+      } else {
+        return (doc = NO_MORE_DOCS);
+      }
+    }
+
+    @Override
+    public int advance(int target) {
+      if (!didNext && target == 0) {
+        return nextDoc();
+      } else {
+        return (doc = NO_MORE_DOCS);
+      }
+    }
+
+    public void reset(Bits liveDocs, int freq) {
+      this.liveDocs = liveDocs;
+      this.freq = freq;
+      this.doc = -1;
+      didNext = false;
+    }
+  }
+  
+  private static class SimpleTVDocsAndPositionsEnum extends DocsAndPositionsEnum {
+    private boolean didNext;
+    private int doc = -1;
+    private int nextPos;
+    private Bits liveDocs;
+    private int[] positions;
+    private BytesRef[] payloads;
+    private int[] startOffsets;
+    private int[] endOffsets;
+
+    @Override
+    public int freq() throws IOException {
+      if (positions != null) {
+        return positions.length;
+      } else {
+        assert startOffsets != null;
+        return startOffsets.length;
+      }
+    }
+
+    @Override
+    public int docID() {
+      return doc;
+    }
+
+    @Override
+    public int nextDoc() {
+      if (!didNext && (liveDocs == null || liveDocs.get(0))) {
+        didNext = true;
+        return (doc = 0);
+      } else {
+        return (doc = NO_MORE_DOCS);
+      }
+    }
+
+    @Override
+    public int advance(int target) {
+      if (!didNext && target == 0) {
+        return nextDoc();
+      } else {
+        return (doc = NO_MORE_DOCS);
+      }
+    }
+
+    public void reset(Bits liveDocs, int[] positions, int[] startOffsets, int[] endOffsets, BytesRef payloads[]) {
+      this.liveDocs = liveDocs;
+      this.positions = positions;
+      this.startOffsets = startOffsets;
+      this.endOffsets = endOffsets;
+      this.payloads = payloads;
+      this.doc = -1;
+      didNext = false;
+      nextPos = 0;
+    }
+
+    @Override
+    public BytesRef getPayload() {
+      return payloads == null ? null : payloads[nextPos-1];
+    }
+
+    @Override
+    public int nextPosition() {
+      assert (positions != null && nextPos < positions.length) ||
+        startOffsets != null && nextPos < startOffsets.length;
+      if (positions != null) {
+        return positions[nextPos++];
+      } else {
+        nextPos++;
+        return -1;
+      }
+    }
+
+    @Override
+    public int startOffset() {
+      if (startOffsets == null) {
+        return -1;
+      } else {
+        return startOffsets[nextPos-1];
+      }
+    }
+
+    @Override
+    public int endOffset() {
+      if (endOffsets == null) {
+        return -1;
+      } else {
+        return endOffsets[nextPos-1];
+      }
+    }
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextTermVectorsWriter.java b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextTermVectorsWriter.java
new file mode 100644
index 0000000..673ecea
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextTermVectorsWriter.java
@@ -0,0 +1,208 @@
+package org.apache.lucene.codecs.simpletext;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Comparator;
+
+import org.apache.lucene.codecs.TermVectorsWriter;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+
+/**
+ * Writes plain-text term vectors.
+ * <p>
+ * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
+ * @lucene.experimental
+ */
+public class SimpleTextTermVectorsWriter extends TermVectorsWriter {
+  
+  static final BytesRef END                = new BytesRef("END");
+  static final BytesRef DOC                = new BytesRef("doc ");
+  static final BytesRef NUMFIELDS          = new BytesRef("  numfields ");
+  static final BytesRef FIELD              = new BytesRef("  field ");
+  static final BytesRef FIELDNAME          = new BytesRef("    name ");
+  static final BytesRef FIELDPOSITIONS     = new BytesRef("    positions ");
+  static final BytesRef FIELDOFFSETS       = new BytesRef("    offsets   ");
+  static final BytesRef FIELDPAYLOADS      = new BytesRef("    payloads  ");
+  static final BytesRef FIELDTERMCOUNT     = new BytesRef("    numterms ");
+  static final BytesRef TERMTEXT           = new BytesRef("    term ");
+  static final BytesRef TERMFREQ           = new BytesRef("      freq ");
+  static final BytesRef POSITION           = new BytesRef("      position ");
+  static final BytesRef PAYLOAD            = new BytesRef("        payload ");
+  static final BytesRef STARTOFFSET        = new BytesRef("        startoffset ");
+  static final BytesRef ENDOFFSET          = new BytesRef("        endoffset ");
+
+  static final String VECTORS_EXTENSION = "vec";
+  
+  private final Directory directory;
+  private final String segment;
+  private IndexOutput out;
+  private int numDocsWritten = 0;
+  private final BytesRef scratch = new BytesRef();
+  private boolean offsets;
+  private boolean positions;
+  private boolean payloads;
+
+  public SimpleTextTermVectorsWriter(Directory directory, String segment, IOContext context) throws IOException {
+    this.directory = directory;
+    this.segment = segment;
+    boolean success = false;
+    try {
+      out = directory.createOutput(IndexFileNames.segmentFileName(segment, "", VECTORS_EXTENSION), context);
+      success = true;
+    } finally {
+      if (!success) {
+        abort();
+      }
+    }
+  }
+  
+  @Override
+  public void startDocument(int numVectorFields) throws IOException {
+    write(DOC);
+    write(Integer.toString(numDocsWritten));
+    newLine();
+    
+    write(NUMFIELDS);
+    write(Integer.toString(numVectorFields));
+    newLine();
+    numDocsWritten++;
+  }
+
+  @Override
+  public void startField(FieldInfo info, int numTerms, boolean positions, boolean offsets, boolean payloads) throws IOException {  
+    write(FIELD);
+    write(Integer.toString(info.number));
+    newLine();
+    
+    write(FIELDNAME);
+    write(info.name);
+    newLine();
+    
+    write(FIELDPOSITIONS);
+    write(Boolean.toString(positions));
+    newLine();
+    
+    write(FIELDOFFSETS);
+    write(Boolean.toString(offsets));
+    newLine();
+    
+    write(FIELDPAYLOADS);
+    write(Boolean.toString(payloads));
+    newLine();
+    
+    write(FIELDTERMCOUNT);
+    write(Integer.toString(numTerms));
+    newLine();
+    
+    this.positions = positions;
+    this.offsets = offsets;
+    this.payloads = payloads;
+  }
+
+  @Override
+  public void startTerm(BytesRef term, int freq) throws IOException {
+    write(TERMTEXT);
+    write(term);
+    newLine();
+    
+    write(TERMFREQ);
+    write(Integer.toString(freq));
+    newLine();
+  }
+
+  @Override
+  public void addPosition(int position, int startOffset, int endOffset, BytesRef payload) throws IOException {
+    assert positions || offsets;
+    
+    if (positions) {
+      write(POSITION);
+      write(Integer.toString(position));
+      newLine();
+      
+      if (payloads) {
+        write(PAYLOAD);
+        if (payload != null) {
+          assert payload.length > 0;
+          write(payload);
+        }
+        newLine();
+      }
+    }
+    
+    if (offsets) {
+      write(STARTOFFSET);
+      write(Integer.toString(startOffset));
+      newLine();
+      
+      write(ENDOFFSET);
+      write(Integer.toString(endOffset));
+      newLine();
+    }
+  }
+
+  @Override
+  public void abort() {
+    try {
+      close();
+    } catch (IOException ignored) {}
+    IOUtils.deleteFilesIgnoringExceptions(directory, IndexFileNames.segmentFileName(segment, "", VECTORS_EXTENSION));
+  }
+
+  @Override
+  public void finish(FieldInfos fis, int numDocs) throws IOException {
+    if (numDocsWritten != numDocs) {
+      throw new RuntimeException("mergeVectors produced an invalid result: mergedDocs is " + numDocs + " but vec numDocs is " + numDocsWritten + " file=" + out.toString() + "; now aborting this merge to prevent index corruption");
+    }
+    write(END);
+    newLine();
+  }
+  
+  @Override
+  public void close() throws IOException {
+    try {
+      IOUtils.close(out);
+    } finally {
+      out = null;
+    }
+  }
+  
+  @Override
+  public Comparator<BytesRef> getComparator() throws IOException {
+    return BytesRef.getUTF8SortedAsUnicodeComparator();
+  }
+  
+  private void write(String s) throws IOException {
+    SimpleTextUtil.write(out, s, scratch);
+  }
+  
+  private void write(BytesRef bytes) throws IOException {
+    SimpleTextUtil.write(out, bytes);
+  }
+  
+  private void newLine() throws IOException {
+    SimpleTextUtil.writeNewline(out);
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextUtil.java b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextUtil.java
new file mode 100644
index 0000000..c0c7787
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/SimpleTextUtil.java
@@ -0,0 +1,70 @@
+package org.apache.lucene.codecs.simpletext;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.store.DataInput;
+import org.apache.lucene.store.DataOutput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.UnicodeUtil;
+
+class SimpleTextUtil {
+  public final static byte NEWLINE = 10;
+  public final static byte ESCAPE = 92;
+  
+  public static void write(DataOutput out, String s, BytesRef scratch) throws IOException {
+    UnicodeUtil.UTF16toUTF8(s, 0, s.length(), scratch);
+    write(out, scratch);
+  }
+
+  public static void write(DataOutput out, BytesRef b) throws IOException {
+    for(int i=0;i<b.length;i++) {
+      final byte bx = b.bytes[b.offset+i];
+      if (bx == NEWLINE || bx == ESCAPE) {
+        out.writeByte(ESCAPE);
+      }
+      out.writeByte(bx);
+    }
+  }
+
+  public static void writeNewline(DataOutput out) throws IOException {
+    out.writeByte(NEWLINE);
+  }
+  
+  public static void readLine(DataInput in, BytesRef scratch) throws IOException {
+    int upto = 0;
+    while(true) {
+      byte b = in.readByte();
+      if (scratch.bytes.length == upto) {
+        scratch.grow(1+upto);
+      }
+      if (b == ESCAPE) {
+        scratch.bytes[upto++] = in.readByte();
+      } else {
+        if (b == NEWLINE) {
+          break;
+        } else {
+          scratch.bytes[upto++] = b;
+        }
+      }
+    }
+    scratch.offset = 0;
+    scratch.length = upto;
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/package.html b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/package.html
new file mode 100644
index 0000000..88aad68
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/simpletext/package.html
@@ -0,0 +1,25 @@
+<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+<html>
+<head>
+   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
+</head>
+<body>
+Simpletext Codec: writes human readable postings.
+</body>
+</html>
diff --git a/lucene/codecs/src/java/overview.html b/lucene/codecs/src/java/overview.html
new file mode 100644
index 0000000..25a479b
--- /dev/null
+++ b/lucene/codecs/src/java/overview.html
@@ -0,0 +1,24 @@
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+<html>
+<head>
+   <title>Apache Lucene - Codecs</title>
+</head>
+<body>
+Collection of useful codec, postings format and terms dictionary implementations.
+</body>
+</html>
diff --git a/lucene/codecs/src/resources/META-INF/services/org.apache.lucene.codecs.Codec b/lucene/codecs/src/resources/META-INF/services/org.apache.lucene.codecs.Codec
new file mode 100644
index 0000000..6ec1c5e
--- /dev/null
+++ b/lucene/codecs/src/resources/META-INF/services/org.apache.lucene.codecs.Codec
@@ -0,0 +1,17 @@
+#  Licensed to the Apache Software Foundation (ASF) under one or more
+#  contributor license agreements.  See the NOTICE file distributed with
+#  this work for additional information regarding copyright ownership.
+#  The ASF licenses this file to You under the Apache License, Version 2.0
+#  (the "License"); you may not use this file except in compliance with
+#  the License.  You may obtain a copy of the License at
+#
+#       http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+#  limitations under the License.
+
+org.apache.lucene.codecs.simpletext.SimpleTextCodec
+org.apache.lucene.codecs.appending.AppendingCodec
diff --git a/lucene/codecs/src/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat b/lucene/codecs/src/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat
new file mode 100644
index 0000000..72b05c5
--- /dev/null
+++ b/lucene/codecs/src/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat
@@ -0,0 +1,21 @@
+#  Licensed to the Apache Software Foundation (ASF) under one or more
+#  contributor license agreements.  See the NOTICE file distributed with
+#  this work for additional information regarding copyright ownership.
+#  The ASF licenses this file to You under the Apache License, Version 2.0
+#  (the "License"); you may not use this file except in compliance with
+#  the License.  You may obtain a copy of the License at
+#
+#       http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+#  limitations under the License.
+
+org.apache.lucene.codecs.pulsing.Pulsing40PostingsFormat
+org.apache.lucene.codecs.simpletext.SimpleTextPostingsFormat
+org.apache.lucene.codecs.memory.MemoryPostingsFormat
+org.apache.lucene.codecs.bloom.BloomFilteringPostingsFormat
+org.apache.lucene.codecs.memory.DirectPostingsFormat
+org.apache.lucene.codecs.block.BlockPostingsFormat
diff --git a/lucene/codecs/src/test/org/apache/lucene/codecs/appending/TestAppendingCodec.java b/lucene/codecs/src/test/org/apache/lucene/codecs/appending/TestAppendingCodec.java
new file mode 100644
index 0000000..d5ccd2e
--- /dev/null
+++ b/lucene/codecs/src/test/org/apache/lucene/codecs/appending/TestAppendingCodec.java
@@ -0,0 +1,169 @@
+package org.apache.lucene.codecs.appending;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Random;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.codecs.appending.AppendingCodec;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.FieldType;
+import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.Fields;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriter;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.MultiFields;
+import org.apache.lucene.index.StoredDocument;
+import org.apache.lucene.index.Terms;
+import org.apache.lucene.index.TermsEnum.SeekStatus;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.index.TieredMergePolicy;
+import org.apache.lucene.search.DocIdSetIterator;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.store.MockDirectoryWrapper;
+import org.apache.lucene.store.RAMDirectory;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.Version;
+
+public class TestAppendingCodec extends LuceneTestCase {
+  
+    private static class AppendingIndexOutputWrapper extends IndexOutput {
+    IndexOutput wrapped;
+    
+    public AppendingIndexOutputWrapper(IndexOutput wrapped) {
+      this.wrapped = wrapped;
+    }
+
+    @Override
+    public void close() throws IOException {
+      wrapped.close();
+    }
+
+    @Override
+    public void flush() throws IOException {
+      wrapped.flush();
+    }
+
+    @Override
+    public long getFilePointer() {
+      return wrapped.getFilePointer();
+    }
+
+    @Override
+    public long length() throws IOException {
+      return wrapped.length();
+    }
+
+    @Override
+    public void seek(long pos) throws IOException {
+      throw new UnsupportedOperationException("seek() is unsupported");
+    }
+
+    @Override
+    public void writeByte(byte b) throws IOException {
+      wrapped.writeByte(b);
+    }
+
+    @Override
+    public void writeBytes(byte[] b, int offset, int length) throws IOException {
+      wrapped.writeBytes(b, offset, length);
+    }
+    
+  }
+  
+  @SuppressWarnings("serial")
+  private static class AppendingRAMDirectory extends MockDirectoryWrapper {
+
+    public AppendingRAMDirectory(Random random, Directory delegate) {
+      super(random, delegate);
+    }
+
+    @Override
+    public IndexOutput createOutput(String name, IOContext context) throws IOException {
+      return new AppendingIndexOutputWrapper(super.createOutput(name, context));
+    }
+    
+  }
+  
+  private static final String text = "the quick brown fox jumped over the lazy dog";
+
+  public void testCodec() throws Exception {
+    Directory dir = new AppendingRAMDirectory(random(), new RAMDirectory());
+    IndexWriterConfig cfg = new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer(random()));
+    
+    cfg.setCodec(new AppendingCodec());
+    ((TieredMergePolicy)cfg.getMergePolicy()).setUseCompoundFile(false);
+    IndexWriter writer = new IndexWriter(dir, cfg);
+    Document doc = new Document();
+    FieldType storedTextType = new FieldType(TextField.TYPE_STORED);
+    storedTextType.setStoreTermVectors(true);
+    storedTextType.setStoreTermVectorPositions(true);
+    storedTextType.setStoreTermVectorOffsets(true);
+    doc.add(newField("f", text, storedTextType));
+    writer.addDocument(doc);
+    writer.commit();
+    writer.addDocument(doc);
+    writer.forceMerge(1);
+    writer.close();
+    IndexReader reader = DirectoryReader.open(dir, 1);
+    assertEquals(2, reader.numDocs());
+    StoredDocument doc2 = reader.document(0);
+    assertEquals(text, doc2.get("f"));
+    Fields fields = MultiFields.getFields(reader);
+    Terms terms = fields.terms("f");
+    assertNotNull(terms);
+    TermsEnum te = terms.iterator(null);
+    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef("quick")));
+    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef("brown")));
+    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef("fox")));
+    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef("jumped")));
+    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef("over")));
+    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef("lazy")));
+    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef("dog")));
+    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef("the")));
+    DocsEnum de = te.docs(null, null);
+    assertTrue(de.advance(0) != DocIdSetIterator.NO_MORE_DOCS);
+    assertEquals(2, de.freq());
+    assertTrue(de.advance(1) != DocIdSetIterator.NO_MORE_DOCS);
+    assertTrue(de.advance(2) == DocIdSetIterator.NO_MORE_DOCS);
+    reader.close();
+  }
+  
+  public void testCompoundFile() throws Exception {
+    Directory dir = new AppendingRAMDirectory(random(), new RAMDirectory());
+    IndexWriterConfig cfg = new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer(random()));
+    TieredMergePolicy mp = new TieredMergePolicy();
+    mp.setUseCompoundFile(true);
+    mp.setNoCFSRatio(1.0);
+    cfg.setMergePolicy(mp);
+    cfg.setCodec(new AppendingCodec());
+    IndexWriter writer = new IndexWriter(dir, cfg);
+    Document doc = new Document();
+    writer.addDocument(doc);
+    writer.close();
+    assertTrue(dir.fileExists("_0.cfs"));
+    dir.close();
+  }
+}
diff --git a/lucene/codecs/src/test/org/apache/lucene/codecs/block/TestForUtil.java b/lucene/codecs/src/test/org/apache/lucene/codecs/block/TestForUtil.java
new file mode 100644
index 0000000..025a634
--- /dev/null
+++ b/lucene/codecs/src/test/org/apache/lucene/codecs/block/TestForUtil.java
@@ -0,0 +1,94 @@
+package org.apache.lucene.codecs.block;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import static org.apache.lucene.codecs.block.BlockPostingsFormat.BLOCK_SIZE;
+import static org.apache.lucene.codecs.block.ForUtil.MAX_DATA_SIZE;
+import static org.apache.lucene.codecs.block.ForUtil.MAX_ENCODED_SIZE;
+
+import java.io.IOException;
+import java.util.Arrays;
+
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.store.RAMDirectory;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.packed.PackedInts;
+
+import com.carrotsearch.randomizedtesting.generators.RandomInts;
+
+public class TestForUtil extends LuceneTestCase {
+
+  public void testEncodeDecode() throws IOException {
+    final int iterations = RandomInts.randomIntBetween(random(), 1, 1000);
+    final float acceptableOverheadRatio = random().nextFloat();
+    final int[] values = new int[(iterations - 1) * BLOCK_SIZE + ForUtil.MAX_DATA_SIZE];
+    for (int i = 0; i < iterations; ++i) {
+      final int bpv = random().nextInt(32);
+      if (bpv == 0) {
+        final int value = RandomInts.randomIntBetween(random(), 0, Integer.MAX_VALUE);
+        for (int j = 0; j < BLOCK_SIZE; ++j) {
+          values[i * BLOCK_SIZE + j] = value;
+        }
+      } else {
+        for (int j = 0; j < BLOCK_SIZE; ++j) {
+          values[i * BLOCK_SIZE + j] = RandomInts.randomIntBetween(random(),
+              0, (int) PackedInts.maxValue(bpv));
+        }
+      }
+    }
+
+    final Directory d = new RAMDirectory();
+    final long endPointer;
+
+    {
+      // encode
+      IndexOutput out = d.createOutput("test.bin", IOContext.DEFAULT);
+      final ForUtil forUtil = new ForUtil(acceptableOverheadRatio, out);
+      
+      for (int i = 0; i < iterations; ++i) {
+        forUtil.writeBlock(
+            Arrays.copyOfRange(values, i * BLOCK_SIZE, values.length),
+            new byte[MAX_ENCODED_SIZE], out);
+      }
+      endPointer = out.getFilePointer();
+      out.close();
+    }
+
+    {
+      // decode
+      IndexInput in = d.openInput("test.bin", IOContext.READONCE);
+      final ForUtil forUtil = new ForUtil(in);
+      for (int i = 0; i < iterations; ++i) {
+        if (random().nextBoolean()) {
+          forUtil.skipBlock(in);
+          continue;
+        }
+        final int[] restored = new int[MAX_DATA_SIZE];
+        forUtil.readBlock(in, new byte[MAX_ENCODED_SIZE], restored);
+        assertArrayEquals(Arrays.copyOfRange(values, i * BLOCK_SIZE, (i + 1) * BLOCK_SIZE),
+            Arrays.copyOf(restored, BLOCK_SIZE));
+      }
+      assertEquals(endPointer, in.getFilePointer());
+      in.close();
+    }
+  }
+
+}
diff --git a/lucene/codecs/src/test/org/apache/lucene/codecs/intblock/TestIntBlockCodec.java b/lucene/codecs/src/test/org/apache/lucene/codecs/intblock/TestIntBlockCodec.java
new file mode 100644
index 0000000..c85662a
--- /dev/null
+++ b/lucene/codecs/src/test/org/apache/lucene/codecs/intblock/TestIntBlockCodec.java
@@ -0,0 +1,64 @@
+package org.apache.lucene.codecs.intblock;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.store.*;
+import org.apache.lucene.codecs.mockintblock.*;
+import org.apache.lucene.codecs.sep.*;
+
+public class TestIntBlockCodec extends LuceneTestCase {
+
+  public void testSimpleIntBlocks() throws Exception {
+    Directory dir = newDirectory();
+
+    IntStreamFactory f = new MockFixedIntBlockPostingsFormat(128).getIntFactory();
+
+    IntIndexOutput out = f.createOutput(dir, "test", newIOContext(random()));
+    for(int i=0;i<11777;i++) {
+      out.write(i);
+    }
+    out.close();
+
+    IntIndexInput in = f.openInput(dir, "test", newIOContext(random()));
+    IntIndexInput.Reader r = in.reader();
+
+    for(int i=0;i<11777;i++) {
+      assertEquals(i, r.next());
+    }
+    in.close();
+    
+    dir.close();
+  }
+
+  public void testEmptySimpleIntBlocks() throws Exception {
+    Directory dir = newDirectory();
+
+    IntStreamFactory f = new MockFixedIntBlockPostingsFormat(128).getIntFactory();
+    IntIndexOutput out = f.createOutput(dir, "test", newIOContext(random()));
+
+    // write no ints
+    out.close();
+
+    IntIndexInput in = f.openInput(dir, "test", newIOContext(random()));
+    in.reader();
+    // read no ints
+    in.close();
+    dir.close();
+  }
+}
diff --git a/lucene/codecs/src/test/org/apache/lucene/codecs/pulsing/Test10KPulsings.java b/lucene/codecs/src/test/org/apache/lucene/codecs/pulsing/Test10KPulsings.java
new file mode 100644
index 0000000..3e47dc5
--- /dev/null
+++ b/lucene/codecs/src/test/org/apache/lucene/codecs/pulsing/Test10KPulsings.java
@@ -0,0 +1,158 @@
+package org.apache.lucene.codecs.pulsing;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.File;
+import java.text.DecimalFormat;
+import java.text.DecimalFormatSymbols;
+import java.text.NumberFormat;
+import java.util.Locale;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.pulsing.Pulsing40PostingsFormat;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.FieldType;
+import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.MultiFields;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.search.DocIdSetIterator;
+import org.apache.lucene.store.BaseDirectoryWrapper;
+import org.apache.lucene.store.MockDirectoryWrapper;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util._TestUtil;
+
+/**
+ * Pulses 10k terms/docs, 
+ * originally designed to find JRE bugs (https://issues.apache.org/jira/browse/LUCENE-3335)
+ * 
+ * @lucene.experimental
+ */
+@LuceneTestCase.Nightly
+public class Test10KPulsings extends LuceneTestCase {
+  public void test10kPulsed() throws Exception {
+    // we always run this test with pulsing codec.
+    Codec cp = _TestUtil.alwaysPostingsFormat(new Pulsing40PostingsFormat(1));
+    
+    File f = _TestUtil.getTempDir("10kpulsed");
+    BaseDirectoryWrapper dir = newFSDirectory(f);
+    dir.setCheckIndexOnClose(false); // we do this ourselves explicitly
+    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, 
+        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setCodec(cp));
+    
+    Document document = new Document();
+    FieldType ft = new FieldType(TextField.TYPE_STORED);
+    
+    switch(_TestUtil.nextInt(random(), 0, 2)) {
+      case 0: ft.setIndexOptions(IndexOptions.DOCS_ONLY); break;
+      case 1: ft.setIndexOptions(IndexOptions.DOCS_AND_FREQS); break;
+      default: ft.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS); break; 
+    }
+
+    Field field = newField("field", "", ft);
+    document.add(field);
+    
+    NumberFormat df = new DecimalFormat("00000", new DecimalFormatSymbols(Locale.ROOT));
+
+    for (int i = 0; i < 10050; i++) {
+      field.setStringValue(df.format(i));
+      iw.addDocument(document);
+    }
+    
+    IndexReader ir = iw.getReader();
+    iw.close();
+
+    TermsEnum te = MultiFields.getTerms(ir, "field").iterator(null);
+    DocsEnum de = null;
+    
+    for (int i = 0; i < 10050; i++) {
+      String expected = df.format(i);
+      assertEquals(expected, te.next().utf8ToString());
+      de = _TestUtil.docs(random(), te, null, de, 0);
+      assertTrue(de.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);
+      assertEquals(DocIdSetIterator.NO_MORE_DOCS, de.nextDoc());
+    }
+    ir.close();
+
+    _TestUtil.checkIndex(dir);
+    dir.close();
+  }
+  
+  /** a variant, that uses pulsing, but uses a high TF to force pass thru to the underlying codec
+   */
+  public void test10kNotPulsed() throws Exception {
+    // we always run this test with pulsing codec.
+    int freqCutoff = _TestUtil.nextInt(random(), 1, 10);
+    Codec cp = _TestUtil.alwaysPostingsFormat(new Pulsing40PostingsFormat(freqCutoff));
+    
+    File f = _TestUtil.getTempDir("10knotpulsed");
+    BaseDirectoryWrapper dir = newFSDirectory(f);
+    dir.setCheckIndexOnClose(false); // we do this ourselves explicitly
+    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, 
+        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setCodec(cp));
+    
+    Document document = new Document();
+    FieldType ft = new FieldType(TextField.TYPE_STORED);
+    
+    switch(_TestUtil.nextInt(random(), 0, 2)) {
+      case 0: ft.setIndexOptions(IndexOptions.DOCS_ONLY); break;
+      case 1: ft.setIndexOptions(IndexOptions.DOCS_AND_FREQS); break;
+      default: ft.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS); break; 
+    }
+
+    Field field = newField("field", "", ft);
+    document.add(field);
+    
+    NumberFormat df = new DecimalFormat("00000", new DecimalFormatSymbols(Locale.ROOT));
+
+    final int freq = freqCutoff + 1;
+    
+    for (int i = 0; i < 10050; i++) {
+      StringBuilder sb = new StringBuilder();
+      for (int j = 0; j < freq; j++) {
+        sb.append(df.format(i));
+        sb.append(' '); // whitespace
+      }
+      field.setStringValue(sb.toString());
+      iw.addDocument(document);
+    }
+    
+    IndexReader ir = iw.getReader();
+    iw.close();
+
+    TermsEnum te = MultiFields.getTerms(ir, "field").iterator(null);
+    DocsEnum de = null;
+    
+    for (int i = 0; i < 10050; i++) {
+      String expected = df.format(i);
+      assertEquals(expected, te.next().utf8ToString());
+      de = _TestUtil.docs(random(), te, null, de, 0);
+      assertTrue(de.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);
+      assertEquals(DocIdSetIterator.NO_MORE_DOCS, de.nextDoc());
+    }
+    ir.close();
+
+    _TestUtil.checkIndex(dir);
+    dir.close();
+  }
+}
diff --git a/lucene/codecs/src/test/org/apache/lucene/codecs/pulsing/TestPulsingReuse.java b/lucene/codecs/src/test/org/apache/lucene/codecs/pulsing/TestPulsingReuse.java
new file mode 100644
index 0000000..a37770d
--- /dev/null
+++ b/lucene/codecs/src/test/org/apache/lucene/codecs/pulsing/TestPulsingReuse.java
@@ -0,0 +1,126 @@
+package org.apache.lucene.codecs.pulsing;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.IdentityHashMap;
+import java.util.Map;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.codecs.Codec;
+import org.apache.lucene.codecs.nestedpulsing.NestedPulsingPostingsFormat;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.CheckIndex;
+import org.apache.lucene.index.DirectoryReader;
+import org.apache.lucene.index.DocsAndPositionsEnum;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.store.BaseDirectoryWrapper;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util._TestUtil;
+
+/**
+ * Tests that pulsing codec reuses its enums and wrapped enums
+ */
+public class TestPulsingReuse extends LuceneTestCase {
+  // TODO: this is a basic test. this thing is complicated, add more
+  public void testSophisticatedReuse() throws Exception {
+    // we always run this test with pulsing codec.
+    Codec cp = _TestUtil.alwaysPostingsFormat(new Pulsing40PostingsFormat(1));
+    Directory dir = newDirectory();
+    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, 
+        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setCodec(cp));
+    Document doc = new Document();
+    doc.add(new TextField("foo", "a b b c c c d e f g g h i i j j k", Field.Store.NO));
+    iw.addDocument(doc);
+    DirectoryReader ir = iw.getReader();
+    iw.close();
+    
+    AtomicReader segment = getOnlySegmentReader(ir);
+    DocsEnum reuse = null;
+    Map<DocsEnum,Boolean> allEnums = new IdentityHashMap<DocsEnum,Boolean>();
+    TermsEnum te = segment.terms("foo").iterator(null);
+    while (te.next() != null) {
+      reuse = te.docs(null, reuse, 0);
+      allEnums.put(reuse, true);
+    }
+    
+    assertEquals(2, allEnums.size());
+    
+    allEnums.clear();
+    DocsAndPositionsEnum posReuse = null;
+    te = segment.terms("foo").iterator(null);
+    while (te.next() != null) {
+      posReuse = te.docsAndPositions(null, posReuse);
+      allEnums.put(posReuse, true);
+    }
+    
+    assertEquals(2, allEnums.size());
+    
+    ir.close();
+    dir.close();
+  }
+  
+  /** tests reuse with Pulsing1(Pulsing2(Standard)) */
+  public void testNestedPulsing() throws Exception {
+    // we always run this test with pulsing codec.
+    Codec cp = _TestUtil.alwaysPostingsFormat(new NestedPulsingPostingsFormat());
+    BaseDirectoryWrapper dir = newDirectory();
+    dir.setCheckIndexOnClose(false); // will do this ourselves, custom codec
+    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, 
+        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setCodec(cp));
+    Document doc = new Document();
+    doc.add(new TextField("foo", "a b b c c c d e f g g g h i i j j k l l m m m", Field.Store.NO));
+    // note: the reuse is imperfect, here we would have 4 enums (lost reuse when we get an enum for 'm')
+    // this is because we only track the 'last' enum we reused (not all).
+    // but this seems 'good enough' for now.
+    iw.addDocument(doc);
+    DirectoryReader ir = iw.getReader();
+    iw.close();
+    
+    AtomicReader segment = getOnlySegmentReader(ir);
+    DocsEnum reuse = null;
+    Map<DocsEnum,Boolean> allEnums = new IdentityHashMap<DocsEnum,Boolean>();
+    TermsEnum te = segment.terms("foo").iterator(null);
+    while (te.next() != null) {
+      reuse = te.docs(null, reuse, 0);
+      allEnums.put(reuse, true);
+    }
+    
+    assertEquals(4, allEnums.size());
+    
+    allEnums.clear();
+    DocsAndPositionsEnum posReuse = null;
+    te = segment.terms("foo").iterator(null);
+    while (te.next() != null) {
+      posReuse = te.docsAndPositions(null, posReuse);
+      allEnums.put(posReuse, true);
+    }
+    
+    assertEquals(4, allEnums.size());
+    
+    ir.close();
+    CheckIndex ci = new CheckIndex(dir);
+    ci.checkIndex(null);
+    dir.close();
+  }
+}
diff --git a/lucene/common-build.xml b/lucene/common-build.xml
index 1319312..60c62ca 100644
--- a/lucene/common-build.xml
+++ b/lucene/common-build.xml
@@ -576,6 +576,10 @@
     <property name="core-javadocs.uptodate" value="true"/>
   </target>
 
+  <target name="compile-codecs">
+    <ant dir="${common.dir}/codecs" target="compile-core" inheritAll="false"/>
+  </target>
+
   <target name="compile-test-framework" unless="lucene.test.framework.compiled">
     <ant dir="${common.dir}/test-framework" target="compile-core" inheritAll="false">
       <propertyset refid="uptodate.and.compiled.properties"/>
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/BlockTermsReader.java b/lucene/core/src/java/org/apache/lucene/codecs/BlockTermsReader.java
deleted file mode 100644
index 7d1eece..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/BlockTermsReader.java
+++ /dev/null
@@ -1,870 +0,0 @@
-package org.apache.lucene.codecs;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Collections;
-import java.util.Comparator;
-import java.util.Iterator;
-import java.util.TreeMap;
-
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.DocsAndPositionsEnum;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.TermState;
-import org.apache.lucene.index.Terms;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.store.ByteArrayDataInput;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.DoubleBarrelLRUCache;
-
-/** Handles a terms dict, but decouples all details of
- *  doc/freqs/positions reading to an instance of {@link
- *  PostingsReaderBase}.  This class is reusable for
- *  codecs that use a different format for
- *  docs/freqs/positions (though codecs are also free to
- *  make their own terms dict impl).
- *
- * <p>This class also interacts with an instance of {@link
- * TermsIndexReaderBase}, to abstract away the specific
- * implementation of the terms dict index. 
- * @lucene.experimental */
-
-public class BlockTermsReader extends FieldsProducer {
-  // Open input to the main terms dict file (_X.tis)
-  private final IndexInput in;
-
-  // Reads the terms dict entries, to gather state to
-  // produce DocsEnum on demand
-  private final PostingsReaderBase postingsReader;
-
-  private final TreeMap<String,FieldReader> fields = new TreeMap<String,FieldReader>();
-
-  // Caches the most recently looked-up field + terms:
-  private final DoubleBarrelLRUCache<FieldAndTerm,BlockTermState> termsCache;
-
-  // Reads the terms index
-  private TermsIndexReaderBase indexReader;
-
-  // keeps the dirStart offset
-  protected long dirOffset;
-
-  // Used as key for the terms cache
-  private static class FieldAndTerm extends DoubleBarrelLRUCache.CloneableKey {
-    String field;
-    BytesRef term;
-
-    public FieldAndTerm() {
-    }
-
-    public FieldAndTerm(FieldAndTerm other) {
-      field = other.field;
-      term = BytesRef.deepCopyOf(other.term);
-    }
-
-    @Override
-    public boolean equals(Object _other) {
-      FieldAndTerm other = (FieldAndTerm) _other;
-      return other.field.equals(field) && term.bytesEquals(other.term);
-    }
-
-    @Override
-    public FieldAndTerm clone() {
-      return new FieldAndTerm(this);
-    }
-
-    @Override
-    public int hashCode() {
-      return field.hashCode() * 31 + term.hashCode();
-    }
-  }
-  
-  // private String segment;
-  
-  public BlockTermsReader(TermsIndexReaderBase indexReader, Directory dir, FieldInfos fieldInfos, SegmentInfo info, PostingsReaderBase postingsReader, IOContext context,
-                          int termsCacheSize, String segmentSuffix)
-    throws IOException {
-    
-    this.postingsReader = postingsReader;
-    termsCache = new DoubleBarrelLRUCache<FieldAndTerm,BlockTermState>(termsCacheSize);
-
-    // this.segment = segment;
-    in = dir.openInput(IndexFileNames.segmentFileName(info.name, segmentSuffix, BlockTermsWriter.TERMS_EXTENSION),
-                       context);
-
-    boolean success = false;
-    try {
-      readHeader(in);
-
-      // Have PostingsReader init itself
-      postingsReader.init(in);
-
-      // Read per-field details
-      seekDir(in, dirOffset);
-
-      final int numFields = in.readVInt();
-      if (numFields < 0) {
-        throw new CorruptIndexException("invalid number of fields: " + numFields + " (resource=" + in + ")");
-      }
-      for(int i=0;i<numFields;i++) {
-        final int field = in.readVInt();
-        final long numTerms = in.readVLong();
-        assert numTerms >= 0;
-        final long termsStartPointer = in.readVLong();
-        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);
-        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();
-        final long sumDocFreq = in.readVLong();
-        final int docCount = in.readVInt();
-        if (docCount < 0 || docCount > info.getDocCount()) { // #docs with field must be <= #docs
-          throw new CorruptIndexException("invalid docCount: " + docCount + " maxDoc: " + info.getDocCount() + " (resource=" + in + ")");
-        }
-        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field
-          throw new CorruptIndexException("invalid sumDocFreq: " + sumDocFreq + " docCount: " + docCount + " (resource=" + in + ")");
-        }
-        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings
-          throw new CorruptIndexException("invalid sumTotalTermFreq: " + sumTotalTermFreq + " sumDocFreq: " + sumDocFreq + " (resource=" + in + ")");
-        }
-        FieldReader previous = fields.put(fieldInfo.name, new FieldReader(fieldInfo, numTerms, termsStartPointer, sumTotalTermFreq, sumDocFreq, docCount));
-        if (previous != null) {
-          throw new CorruptIndexException("duplicate fields: " + fieldInfo.name + " (resource=" + in + ")");
-        }
-      }
-      success = true;
-    } finally {
-      if (!success) {
-        in.close();
-      }
-    }
-
-    this.indexReader = indexReader;
-  }
-
-  protected void readHeader(IndexInput input) throws IOException {
-    CodecUtil.checkHeader(input, BlockTermsWriter.CODEC_NAME,
-                          BlockTermsWriter.VERSION_START,
-                          BlockTermsWriter.VERSION_CURRENT);
-    dirOffset = input.readLong();
-  }
-  
-  protected void seekDir(IndexInput input, long dirOffset)
-      throws IOException {
-    input.seek(dirOffset);
-  }
-  
-  @Override
-  public void close() throws IOException {
-    try {
-      try {
-        if (indexReader != null) {
-          indexReader.close();
-        }
-      } finally {
-        // null so if an app hangs on to us (ie, we are not
-        // GCable, despite being closed) we still free most
-        // ram
-        indexReader = null;
-        if (in != null) {
-          in.close();
-        }
-      }
-    } finally {
-      if (postingsReader != null) {
-        postingsReader.close();
-      }
-    }
-  }
-
-  @Override
-  public Iterator<String> iterator() {
-    return Collections.unmodifiableSet(fields.keySet()).iterator();
-  }
-
-  @Override
-  public Terms terms(String field) throws IOException {
-    assert field != null;
-    return fields.get(field);
-  }
-
-  @Override
-  public int size() {
-    return fields.size();
-  }
-
-  private class FieldReader extends Terms {
-    final long numTerms;
-    final FieldInfo fieldInfo;
-    final long termsStartPointer;
-    final long sumTotalTermFreq;
-    final long sumDocFreq;
-    final int docCount;
-
-    FieldReader(FieldInfo fieldInfo, long numTerms, long termsStartPointer, long sumTotalTermFreq, long sumDocFreq, int docCount) {
-      assert numTerms > 0;
-      this.fieldInfo = fieldInfo;
-      this.numTerms = numTerms;
-      this.termsStartPointer = termsStartPointer;
-      this.sumTotalTermFreq = sumTotalTermFreq;
-      this.sumDocFreq = sumDocFreq;
-      this.docCount = docCount;
-    }
-
-    @Override
-    public Comparator<BytesRef> getComparator() {
-      return BytesRef.getUTF8SortedAsUnicodeComparator();
-    }
-
-    @Override
-    public TermsEnum iterator(TermsEnum reuse) throws IOException {
-      return new SegmentTermsEnum();
-    }
-
-    @Override
-    public boolean hasOffsets() {
-      return fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
-    }
-
-    @Override
-    public boolean hasPositions() {
-      return fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;
-    }
-    
-    @Override
-    public boolean hasPayloads() {
-      return fieldInfo.hasPayloads();
-    }
-
-    @Override
-    public long size() {
-      return numTerms;
-    }
-
-    @Override
-    public long getSumTotalTermFreq() {
-      return sumTotalTermFreq;
-    }
-
-    @Override
-    public long getSumDocFreq() throws IOException {
-      return sumDocFreq;
-    }
-
-    @Override
-    public int getDocCount() throws IOException {
-      return docCount;
-    }
-
-    // Iterates through terms in this field
-    private final class SegmentTermsEnum extends TermsEnum {
-      private final IndexInput in;
-      private final BlockTermState state;
-      private final boolean doOrd;
-      private final FieldAndTerm fieldTerm = new FieldAndTerm();
-      private final TermsIndexReaderBase.FieldIndexEnum indexEnum;
-      private final BytesRef term = new BytesRef();
-
-      /* This is true if indexEnum is "still" seek'd to the index term
-         for the current term. We set it to true on seeking, and then it
-         remains valid until next() is called enough times to load another
-         terms block: */
-      private boolean indexIsCurrent;
-
-      /* True if we've already called .next() on the indexEnum, to "bracket"
-         the current block of terms: */
-      private boolean didIndexNext;
-
-      /* Next index term, bracketing the current block of terms; this is
-         only valid if didIndexNext is true: */
-      private BytesRef nextIndexTerm;
-
-      /* True after seekExact(TermState), do defer seeking.  If the app then
-         calls next() (which is not "typical"), then we'll do the real seek */
-      private boolean seekPending;
-
-      /* How many blocks we've read since last seek.  Once this
-         is >= indexEnum.getDivisor() we set indexIsCurrent to false (since
-         the index can no long bracket seek-within-block). */
-      private int blocksSinceSeek;
-
-      private byte[] termSuffixes;
-      private ByteArrayDataInput termSuffixesReader = new ByteArrayDataInput();
-
-      /* Common prefix used for all terms in this block. */
-      private int termBlockPrefix;
-
-      /* How many terms in current block */
-      private int blockTermCount;
-
-      private byte[] docFreqBytes;
-      private final ByteArrayDataInput freqReader = new ByteArrayDataInput();
-      private int metaDataUpto;
-
-      public SegmentTermsEnum() throws IOException {
-        in = BlockTermsReader.this.in.clone();
-        in.seek(termsStartPointer);
-        indexEnum = indexReader.getFieldEnum(fieldInfo);
-        doOrd = indexReader.supportsOrd();
-        fieldTerm.field = fieldInfo.name;
-        state = postingsReader.newTermState();
-        state.totalTermFreq = -1;
-        state.ord = -1;
-
-        termSuffixes = new byte[128];
-        docFreqBytes = new byte[64];
-        //System.out.println("BTR.enum init this=" + this + " postingsReader=" + postingsReader);
-      }
-
-      @Override
-      public Comparator<BytesRef> getComparator() {
-        return BytesRef.getUTF8SortedAsUnicodeComparator();
-      }
-
-      // TODO: we may want an alternate mode here which is
-      // "if you are about to return NOT_FOUND I won't use
-      // the terms data from that"; eg FuzzyTermsEnum will
-      // (usually) just immediately call seek again if we
-      // return NOT_FOUND so it's a waste for us to fill in
-      // the term that was actually NOT_FOUND
-      @Override
-      public SeekStatus seekCeil(final BytesRef target, final boolean useCache) throws IOException {
-
-        if (indexEnum == null) {
-          throw new IllegalStateException("terms index was not loaded");
-        }
-   
-        //System.out.println("BTR.seek seg=" + segment + " target=" + fieldInfo.name + ":" + target.utf8ToString() + " " + target + " current=" + term().utf8ToString() + " " + term() + " useCache=" + useCache + " indexIsCurrent=" + indexIsCurrent + " didIndexNext=" + didIndexNext + " seekPending=" + seekPending + " divisor=" + indexReader.getDivisor() + " this="  + this);
-        if (didIndexNext) {
-          if (nextIndexTerm == null) {
-            //System.out.println("  nextIndexTerm=null");
-          } else {
-            //System.out.println("  nextIndexTerm=" + nextIndexTerm.utf8ToString());
-          }
-        }
-
-        // Check cache
-        if (useCache) {
-          fieldTerm.term = target;
-          // TODO: should we differentiate "frozen"
-          // TermState (ie one that was cloned and
-          // cached/returned by termState()) from the
-          // malleable (primary) one?
-          final TermState cachedState = termsCache.get(fieldTerm);
-          if (cachedState != null) {
-            seekPending = true;
-            //System.out.println("  cached!");
-            seekExact(target, cachedState);
-            //System.out.println("  term=" + term.utf8ToString());
-            return SeekStatus.FOUND;
-          }
-        }
-
-        boolean doSeek = true;
-
-        // See if we can avoid seeking, because target term
-        // is after current term but before next index term:
-        if (indexIsCurrent) {
-
-          final int cmp = BytesRef.getUTF8SortedAsUnicodeComparator().compare(term, target);
-
-          if (cmp == 0) {
-            // Already at the requested term
-            return SeekStatus.FOUND;
-          } else if (cmp < 0) {
-
-            // Target term is after current term
-            if (!didIndexNext) {
-              if (indexEnum.next() == -1) {
-                nextIndexTerm = null;
-              } else {
-                nextIndexTerm = indexEnum.term();
-              }
-              //System.out.println("  now do index next() nextIndexTerm=" + (nextIndexTerm == null ? "null" : nextIndexTerm.utf8ToString()));
-              didIndexNext = true;
-            }
-
-            if (nextIndexTerm == null || BytesRef.getUTF8SortedAsUnicodeComparator().compare(target, nextIndexTerm) < 0) {
-              // Optimization: requested term is within the
-              // same term block we are now in; skip seeking
-              // (but do scanning):
-              doSeek = false;
-              //System.out.println("  skip seek: nextIndexTerm=" + (nextIndexTerm == null ? "null" : nextIndexTerm.utf8ToString()));
-            }
-          }
-        }
-
-        if (doSeek) {
-          //System.out.println("  seek");
-
-          // Ask terms index to find biggest indexed term (=
-          // first term in a block) that's <= our text:
-          in.seek(indexEnum.seek(target));
-          boolean result = nextBlock();
-
-          // Block must exist since, at least, the indexed term
-          // is in the block:
-          assert result;
-
-          indexIsCurrent = true;
-          didIndexNext = false;
-          blocksSinceSeek = 0;          
-
-          if (doOrd) {
-            state.ord = indexEnum.ord()-1;
-          }
-
-          term.copyBytes(indexEnum.term());
-          //System.out.println("  seek: term=" + term.utf8ToString());
-        } else {
-          //System.out.println("  skip seek");
-          if (state.termBlockOrd == blockTermCount && !nextBlock()) {
-            indexIsCurrent = false;
-            return SeekStatus.END;
-          }
-        }
-
-        seekPending = false;
-
-        int common = 0;
-
-        // Scan within block.  We could do this by calling
-        // _next() and testing the resulting term, but this
-        // is wasteful.  Instead, we first confirm the
-        // target matches the common prefix of this block,
-        // and then we scan the term bytes directly from the
-        // termSuffixesreader's byte[], saving a copy into
-        // the BytesRef term per term.  Only when we return
-        // do we then copy the bytes into the term.
-
-        while(true) {
-
-          // First, see if target term matches common prefix
-          // in this block:
-          if (common < termBlockPrefix) {
-            final int cmp = (term.bytes[common]&0xFF) - (target.bytes[target.offset + common]&0xFF);
-            if (cmp < 0) {
-
-              // TODO: maybe we should store common prefix
-              // in block header?  (instead of relying on
-              // last term of previous block)
-
-              // Target's prefix is after the common block
-              // prefix, so term cannot be in this block
-              // but it could be in next block.  We
-              // must scan to end-of-block to set common
-              // prefix for next block:
-              if (state.termBlockOrd < blockTermCount) {
-                while(state.termBlockOrd < blockTermCount-1) {
-                  state.termBlockOrd++;
-                  state.ord++;
-                  termSuffixesReader.skipBytes(termSuffixesReader.readVInt());
-                }
-                final int suffix = termSuffixesReader.readVInt();
-                term.length = termBlockPrefix + suffix;
-                if (term.bytes.length < term.length) {
-                  term.grow(term.length);
-                }
-                termSuffixesReader.readBytes(term.bytes, termBlockPrefix, suffix);
-              }
-              state.ord++;
-              
-              if (!nextBlock()) {
-                indexIsCurrent = false;
-                return SeekStatus.END;
-              }
-              common = 0;
-
-            } else if (cmp > 0) {
-              // Target's prefix is before the common prefix
-              // of this block, so we position to start of
-              // block and return NOT_FOUND:
-              assert state.termBlockOrd == 0;
-
-              final int suffix = termSuffixesReader.readVInt();
-              term.length = termBlockPrefix + suffix;
-              if (term.bytes.length < term.length) {
-                term.grow(term.length);
-              }
-              termSuffixesReader.readBytes(term.bytes, termBlockPrefix, suffix);
-              return SeekStatus.NOT_FOUND;
-            } else {
-              common++;
-            }
-
-            continue;
-          }
-
-          // Test every term in this block
-          while (true) {
-            state.termBlockOrd++;
-            state.ord++;
-
-            final int suffix = termSuffixesReader.readVInt();
-            
-            // We know the prefix matches, so just compare the new suffix:
-            final int termLen = termBlockPrefix + suffix;
-            int bytePos = termSuffixesReader.getPosition();
-
-            boolean next = false;
-            final int limit = target.offset + (termLen < target.length ? termLen : target.length);
-            int targetPos = target.offset + termBlockPrefix;
-            while(targetPos < limit) {
-              final int cmp = (termSuffixes[bytePos++]&0xFF) - (target.bytes[targetPos++]&0xFF);
-              if (cmp < 0) {
-                // Current term is still before the target;
-                // keep scanning
-                next = true;
-                break;
-              } else if (cmp > 0) {
-                // Done!  Current term is after target. Stop
-                // here, fill in real term, return NOT_FOUND.
-                term.length = termBlockPrefix + suffix;
-                if (term.bytes.length < term.length) {
-                  term.grow(term.length);
-                }
-                termSuffixesReader.readBytes(term.bytes, termBlockPrefix, suffix);
-                //System.out.println("  NOT_FOUND");
-                return SeekStatus.NOT_FOUND;
-              }
-            }
-
-            if (!next && target.length <= termLen) {
-              term.length = termBlockPrefix + suffix;
-              if (term.bytes.length < term.length) {
-                term.grow(term.length);
-              }
-              termSuffixesReader.readBytes(term.bytes, termBlockPrefix, suffix);
-
-              if (target.length == termLen) {
-                // Done!  Exact match.  Stop here, fill in
-                // real term, return FOUND.
-                //System.out.println("  FOUND");
-
-                if (useCache) {
-                  // Store in cache
-                  decodeMetaData();
-                  //System.out.println("  cache! state=" + state);
-                  termsCache.put(new FieldAndTerm(fieldTerm), (BlockTermState) state.clone());
-                }
-
-                return SeekStatus.FOUND;
-              } else {
-                //System.out.println("  NOT_FOUND");
-                return SeekStatus.NOT_FOUND;
-              }
-            }
-
-            if (state.termBlockOrd == blockTermCount) {
-              // Must pre-fill term for next block's common prefix
-              term.length = termBlockPrefix + suffix;
-              if (term.bytes.length < term.length) {
-                term.grow(term.length);
-              }
-              termSuffixesReader.readBytes(term.bytes, termBlockPrefix, suffix);
-              break;
-            } else {
-              termSuffixesReader.skipBytes(suffix);
-            }
-          }
-
-          // The purpose of the terms dict index is to seek
-          // the enum to the closest index term before the
-          // term we are looking for.  So, we should never
-          // cross another index term (besides the first
-          // one) while we are scanning:
-
-          assert indexIsCurrent;
-
-          if (!nextBlock()) {
-            //System.out.println("  END");
-            indexIsCurrent = false;
-            return SeekStatus.END;
-          }
-          common = 0;
-        }
-      }
-
-      @Override
-      public BytesRef next() throws IOException {
-        //System.out.println("BTR.next() seekPending=" + seekPending + " pendingSeekCount=" + state.termBlockOrd);
-
-        // If seek was previously called and the term was cached,
-        // usually caller is just going to pull a D/&PEnum or get
-        // docFreq, etc.  But, if they then call next(),
-        // this method catches up all internal state so next()
-        // works properly:
-        if (seekPending) {
-          assert !indexIsCurrent;
-          in.seek(state.blockFilePointer);
-          final int pendingSeekCount = state.termBlockOrd;
-          boolean result = nextBlock();
-
-          final long savOrd = state.ord;
-
-          // Block must exist since seek(TermState) was called w/ a
-          // TermState previously returned by this enum when positioned
-          // on a real term:
-          assert result;
-
-          while(state.termBlockOrd < pendingSeekCount) {
-            BytesRef nextResult = _next();
-            assert nextResult != null;
-          }
-          seekPending = false;
-          state.ord = savOrd;
-        }
-        return _next();
-      }
-
-      /* Decodes only the term bytes of the next term.  If caller then asks for
-         metadata, ie docFreq, totalTermFreq or pulls a D/&PEnum, we then (lazily)
-         decode all metadata up to the current term. */
-      private BytesRef _next() throws IOException {
-        //System.out.println("BTR._next seg=" + segment + " this=" + this + " termCount=" + state.termBlockOrd + " (vs " + blockTermCount + ")");
-        if (state.termBlockOrd == blockTermCount && !nextBlock()) {
-          //System.out.println("  eof");
-          indexIsCurrent = false;
-          return null;
-        }
-
-        // TODO: cutover to something better for these ints!  simple64?
-        final int suffix = termSuffixesReader.readVInt();
-        //System.out.println("  suffix=" + suffix);
-
-        term.length = termBlockPrefix + suffix;
-        if (term.bytes.length < term.length) {
-          term.grow(term.length);
-        }
-        termSuffixesReader.readBytes(term.bytes, termBlockPrefix, suffix);
-        state.termBlockOrd++;
-
-        // NOTE: meaningless in the non-ord case
-        state.ord++;
-
-        //System.out.println("  return term=" + fieldInfo.name + ":" + term.utf8ToString() + " " + term + " tbOrd=" + state.termBlockOrd);
-        return term;
-      }
-
-      @Override
-      public BytesRef term() {
-        return term;
-      }
-
-      @Override
-      public int docFreq() throws IOException {
-        //System.out.println("BTR.docFreq");
-        decodeMetaData();
-        //System.out.println("  return " + state.docFreq);
-        return state.docFreq;
-      }
-
-      @Override
-      public long totalTermFreq() throws IOException {
-        decodeMetaData();
-        return state.totalTermFreq;
-      }
-
-      @Override
-      public DocsEnum docs(Bits liveDocs, DocsEnum reuse, int flags) throws IOException {
-        //System.out.println("BTR.docs this=" + this);
-        decodeMetaData();
-        //System.out.println("BTR.docs:  state.docFreq=" + state.docFreq);
-        return postingsReader.docs(fieldInfo, state, liveDocs, reuse, flags);
-      }
-
-      @Override
-      public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum reuse, int flags) throws IOException {
-        if (fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) < 0) {
-          // Positions were not indexed:
-          return null;
-        }
-
-        decodeMetaData();
-        return postingsReader.docsAndPositions(fieldInfo, state, liveDocs, reuse, flags);
-      }
-
-      @Override
-      public void seekExact(BytesRef target, TermState otherState) {
-        //System.out.println("BTR.seekExact termState target=" + target.utf8ToString() + " " + target + " this=" + this);
-        assert otherState != null && otherState instanceof BlockTermState;
-        assert !doOrd || ((BlockTermState) otherState).ord < numTerms;
-        state.copyFrom(otherState);
-        seekPending = true;
-        indexIsCurrent = false;
-        term.copyBytes(target);
-      }
-      
-      @Override
-      public TermState termState() throws IOException {
-        //System.out.println("BTR.termState this=" + this);
-        decodeMetaData();
-        TermState ts = state.clone();
-        //System.out.println("  return ts=" + ts);
-        return ts;
-      }
-
-      @Override
-      public void seekExact(long ord) throws IOException {
-        //System.out.println("BTR.seek by ord ord=" + ord);
-        if (indexEnum == null) {
-          throw new IllegalStateException("terms index was not loaded");
-        }
-
-        assert ord < numTerms;
-
-        // TODO: if ord is in same terms block and
-        // after current ord, we should avoid this seek just
-        // like we do in the seek(BytesRef) case
-        in.seek(indexEnum.seek(ord));
-        boolean result = nextBlock();
-
-        // Block must exist since ord < numTerms:
-        assert result;
-
-        indexIsCurrent = true;
-        didIndexNext = false;
-        blocksSinceSeek = 0;
-        seekPending = false;
-
-        state.ord = indexEnum.ord()-1;
-        assert state.ord >= -1: "ord=" + state.ord;
-        term.copyBytes(indexEnum.term());
-
-        // Now, scan:
-        int left = (int) (ord - state.ord);
-        while(left > 0) {
-          final BytesRef term = _next();
-          assert term != null;
-          left--;
-          assert indexIsCurrent;
-        }
-      }
-
-      @Override
-      public long ord() {
-        if (!doOrd) {
-          throw new UnsupportedOperationException();
-        }
-        return state.ord;
-      }
-
-      /* Does initial decode of next block of terms; this
-         doesn't actually decode the docFreq, totalTermFreq,
-         postings details (frq/prx offset, etc.) metadata;
-         it just loads them as byte[] blobs which are then      
-         decoded on-demand if the metadata is ever requested
-         for any term in this block.  This enables terms-only
-         intensive consumes (eg certain MTQs, respelling) to
-         not pay the price of decoding metadata they won't
-         use. */
-      private boolean nextBlock() throws IOException {
-
-        // TODO: we still lazy-decode the byte[] for each
-        // term (the suffix), but, if we decoded
-        // all N terms up front then seeking could do a fast
-        // bsearch w/in the block...
-
-        //System.out.println("BTR.nextBlock() fp=" + in.getFilePointer() + " this=" + this);
-        state.blockFilePointer = in.getFilePointer();
-        blockTermCount = in.readVInt();
-        //System.out.println("  blockTermCount=" + blockTermCount);
-        if (blockTermCount == 0) {
-          return false;
-        }
-        termBlockPrefix = in.readVInt();
-
-        // term suffixes:
-        int len = in.readVInt();
-        if (termSuffixes.length < len) {
-          termSuffixes = new byte[ArrayUtil.oversize(len, 1)];
-        }
-        //System.out.println("  termSuffixes len=" + len);
-        in.readBytes(termSuffixes, 0, len);
-        termSuffixesReader.reset(termSuffixes, 0, len);
-
-        // docFreq, totalTermFreq
-        len = in.readVInt();
-        if (docFreqBytes.length < len) {
-          docFreqBytes = new byte[ArrayUtil.oversize(len, 1)];
-        }
-        //System.out.println("  freq bytes len=" + len);
-        in.readBytes(docFreqBytes, 0, len);
-        freqReader.reset(docFreqBytes, 0, len);
-        metaDataUpto = 0;
-
-        state.termBlockOrd = 0;
-
-        postingsReader.readTermsBlock(in, fieldInfo, state);
-
-        blocksSinceSeek++;
-        indexIsCurrent = indexIsCurrent && (blocksSinceSeek < indexReader.getDivisor());
-        //System.out.println("  indexIsCurrent=" + indexIsCurrent);
-
-        return true;
-      }
-     
-      private void decodeMetaData() throws IOException {
-        //System.out.println("BTR.decodeMetadata mdUpto=" + metaDataUpto + " vs termCount=" + state.termBlockOrd + " state=" + state);
-        if (!seekPending) {
-          // TODO: cutover to random-access API
-          // here.... really stupid that we have to decode N
-          // wasted term metadata just to get to the N+1th
-          // that we really need...
-
-          // lazily catch up on metadata decode:
-          final int limit = state.termBlockOrd;
-          // We must set/incr state.termCount because
-          // postings impl can look at this
-          state.termBlockOrd = metaDataUpto;
-          // TODO: better API would be "jump straight to term=N"???
-          while (metaDataUpto < limit) {
-            //System.out.println("  decode mdUpto=" + metaDataUpto);
-            // TODO: we could make "tiers" of metadata, ie,
-            // decode docFreq/totalTF but don't decode postings
-            // metadata; this way caller could get
-            // docFreq/totalTF w/o paying decode cost for
-            // postings
-
-            // TODO: if docFreq were bulk decoded we could
-            // just skipN here:
-            state.docFreq = freqReader.readVInt();
-            //System.out.println("    dF=" + state.docFreq);
-            if (fieldInfo.getIndexOptions() != IndexOptions.DOCS_ONLY) {
-              state.totalTermFreq = state.docFreq + freqReader.readVLong();
-              //System.out.println("    totTF=" + state.totalTermFreq);
-            }
-
-            postingsReader.nextTerm(fieldInfo, state);
-            metaDataUpto++;
-            state.termBlockOrd++;
-          }
-        } else {
-          //System.out.println("  skip! seekPending");
-        }
-      }
-    }
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/BlockTermsWriter.java b/lucene/core/src/java/org/apache/lucene/codecs/BlockTermsWriter.java
deleted file mode 100644
index daf49d4..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/BlockTermsWriter.java
+++ /dev/null
@@ -1,318 +0,0 @@
-package org.apache.lucene.codecs;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Comparator;
-import java.util.List;
-
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.store.RAMOutputStream;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.RamUsageEstimator;
-
-// TODO: currently we encode all terms between two indexed
-// terms as a block; but, we could decouple the two, ie
-// allow several blocks in between two indexed terms
-
-/**
- * Writes terms dict, block-encoding (column stride) each
- * term's metadata for each set of terms between two
- * index terms.
- *
- * @lucene.experimental
- */
-
-public class BlockTermsWriter extends FieldsConsumer {
-
-  final static String CODEC_NAME = "BLOCK_TERMS_DICT";
-
-  // Initial format
-  public static final int VERSION_START = 0;
-
-  public static final int VERSION_CURRENT = VERSION_START;
-
-  /** Extension of terms file */
-  static final String TERMS_EXTENSION = "tib";
-
-  protected final IndexOutput out;
-  final PostingsWriterBase postingsWriter;
-  final FieldInfos fieldInfos;
-  FieldInfo currentField;
-  private final TermsIndexWriterBase termsIndexWriter;
-  private final List<TermsWriter> fields = new ArrayList<TermsWriter>();
-
-  // private final String segment;
-
-  public BlockTermsWriter(TermsIndexWriterBase termsIndexWriter,
-      SegmentWriteState state, PostingsWriterBase postingsWriter)
-      throws IOException {
-    final String termsFileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, TERMS_EXTENSION);
-    this.termsIndexWriter = termsIndexWriter;
-    out = state.directory.createOutput(termsFileName, state.context);
-    boolean success = false;
-    try {
-      fieldInfos = state.fieldInfos;
-      writeHeader(out);
-      currentField = null;
-      this.postingsWriter = postingsWriter;
-      // segment = state.segmentName;
-      
-      //System.out.println("BTW.init seg=" + state.segmentName);
-      
-      postingsWriter.start(out); // have consumer write its format/header
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(out);
-      }
-    }
-  }
-  
-  protected void writeHeader(IndexOutput out) throws IOException {
-    CodecUtil.writeHeader(out, CODEC_NAME, VERSION_CURRENT); 
-
-    out.writeLong(0);                             // leave space for end index pointer    
-  }
-
-  @Override
-  public TermsConsumer addField(FieldInfo field) throws IOException {
-    //System.out.println("\nBTW.addField seg=" + segment + " field=" + field.name);
-    assert currentField == null || currentField.name.compareTo(field.name) < 0;
-    currentField = field;
-    TermsIndexWriterBase.FieldWriter fieldIndexWriter = termsIndexWriter.addField(field, out.getFilePointer());
-    final TermsWriter terms = new TermsWriter(fieldIndexWriter, field, postingsWriter);
-    fields.add(terms);
-    return terms;
-  }
-
-  @Override
-  public void close() throws IOException {
-
-    try {
-      
-      int nonZeroCount = 0;
-      for(TermsWriter field : fields) {
-        if (field.numTerms > 0) {
-          nonZeroCount++;
-        }
-      }
-
-      final long dirStart = out.getFilePointer();
-
-      out.writeVInt(nonZeroCount);
-      for(TermsWriter field : fields) {
-        if (field.numTerms > 0) {
-          out.writeVInt(field.fieldInfo.number);
-          out.writeVLong(field.numTerms);
-          out.writeVLong(field.termsStartPointer);
-          if (field.fieldInfo.getIndexOptions() != IndexOptions.DOCS_ONLY) {
-            out.writeVLong(field.sumTotalTermFreq);
-          }
-          out.writeVLong(field.sumDocFreq);
-          out.writeVInt(field.docCount);
-        }
-      }
-      writeTrailer(dirStart);
-    } finally {
-      IOUtils.close(out, postingsWriter, termsIndexWriter);
-    }
-  }
-
-  protected void writeTrailer(long dirStart) throws IOException {
-    out.seek(CodecUtil.headerLength(CODEC_NAME));
-    out.writeLong(dirStart);    
-  }
-  
-  private static class TermEntry {
-    public final BytesRef term = new BytesRef();
-    public TermStats stats;
-  }
-
-  class TermsWriter extends TermsConsumer {
-    private final FieldInfo fieldInfo;
-    private final PostingsWriterBase postingsWriter;
-    private final long termsStartPointer;
-    private long numTerms;
-    private final TermsIndexWriterBase.FieldWriter fieldIndexWriter;
-    long sumTotalTermFreq;
-    long sumDocFreq;
-    int docCount;
-
-    private TermEntry[] pendingTerms;
-
-    private int pendingCount;
-
-    TermsWriter(
-        TermsIndexWriterBase.FieldWriter fieldIndexWriter,
-        FieldInfo fieldInfo,
-        PostingsWriterBase postingsWriter) 
-    {
-      this.fieldInfo = fieldInfo;
-      this.fieldIndexWriter = fieldIndexWriter;
-      pendingTerms = new TermEntry[32];
-      for(int i=0;i<pendingTerms.length;i++) {
-        pendingTerms[i] = new TermEntry();
-      }
-      termsStartPointer = out.getFilePointer();
-      postingsWriter.setField(fieldInfo);
-      this.postingsWriter = postingsWriter;
-    }
-    
-    @Override
-    public Comparator<BytesRef> getComparator() {
-      return BytesRef.getUTF8SortedAsUnicodeComparator();
-    }
-
-    @Override
-    public PostingsConsumer startTerm(BytesRef text) throws IOException {
-      //System.out.println("BTW: startTerm term=" + fieldInfo.name + ":" + text.utf8ToString() + " " + text + " seg=" + segment);
-      postingsWriter.startTerm();
-      return postingsWriter;
-    }
-
-    private final BytesRef lastPrevTerm = new BytesRef();
-
-    @Override
-    public void finishTerm(BytesRef text, TermStats stats) throws IOException {
-
-      assert stats.docFreq > 0;
-      //System.out.println("BTW: finishTerm term=" + fieldInfo.name + ":" + text.utf8ToString() + " " + text + " seg=" + segment + " df=" + stats.docFreq);
-
-      final boolean isIndexTerm = fieldIndexWriter.checkIndexTerm(text, stats);
-
-      if (isIndexTerm) {
-        if (pendingCount > 0) {
-          // Instead of writing each term, live, we gather terms
-          // in RAM in a pending buffer, and then write the
-          // entire block in between index terms:
-          flushBlock();
-        }
-        fieldIndexWriter.add(text, stats, out.getFilePointer());
-        //System.out.println("  index term!");
-      }
-
-      if (pendingTerms.length == pendingCount) {
-        final TermEntry[] newArray = new TermEntry[ArrayUtil.oversize(pendingCount+1, RamUsageEstimator.NUM_BYTES_OBJECT_REF)];
-        System.arraycopy(pendingTerms, 0, newArray, 0, pendingCount);
-        for(int i=pendingCount;i<newArray.length;i++) {
-          newArray[i] = new TermEntry();
-        }
-        pendingTerms = newArray;
-      }
-      final TermEntry te = pendingTerms[pendingCount];
-      te.term.copyBytes(text);
-      te.stats = stats;
-
-      pendingCount++;
-
-      postingsWriter.finishTerm(stats);
-      numTerms++;
-    }
-
-    // Finishes all terms in this field
-    @Override
-    public void finish(long sumTotalTermFreq, long sumDocFreq, int docCount) throws IOException {
-      if (pendingCount > 0) {
-        flushBlock();
-      }
-      // EOF marker:
-      out.writeVInt(0);
-
-      this.sumTotalTermFreq = sumTotalTermFreq;
-      this.sumDocFreq = sumDocFreq;
-      this.docCount = docCount;
-      fieldIndexWriter.finish(out.getFilePointer());
-    }
-
-    private int sharedPrefix(BytesRef term1, BytesRef term2) {
-      assert term1.offset == 0;
-      assert term2.offset == 0;
-      int pos1 = 0;
-      int pos1End = pos1 + Math.min(term1.length, term2.length);
-      int pos2 = 0;
-      while(pos1 < pos1End) {
-        if (term1.bytes[pos1] != term2.bytes[pos2]) {
-          return pos1;
-        }
-        pos1++;
-        pos2++;
-      }
-      return pos1;
-    }
-
-    private final RAMOutputStream bytesWriter = new RAMOutputStream();
-
-    private void flushBlock() throws IOException {
-      //System.out.println("BTW.flushBlock seg=" + segment + " pendingCount=" + pendingCount + " fp=" + out.getFilePointer());
-
-      // First pass: compute common prefix for all terms
-      // in the block, against term before first term in
-      // this block:
-      int commonPrefix = sharedPrefix(lastPrevTerm, pendingTerms[0].term);
-      for(int termCount=1;termCount<pendingCount;termCount++) {
-        commonPrefix = Math.min(commonPrefix,
-                                sharedPrefix(lastPrevTerm,
-                                             pendingTerms[termCount].term));
-      }        
-
-      out.writeVInt(pendingCount);
-      out.writeVInt(commonPrefix);
-
-      // 2nd pass: write suffixes, as separate byte[] blob
-      for(int termCount=0;termCount<pendingCount;termCount++) {
-        final int suffix = pendingTerms[termCount].term.length - commonPrefix;
-        // TODO: cutover to better intblock codec, instead
-        // of interleaving here:
-        bytesWriter.writeVInt(suffix);
-        bytesWriter.writeBytes(pendingTerms[termCount].term.bytes, commonPrefix, suffix);
-      }
-      out.writeVInt((int) bytesWriter.getFilePointer());
-      bytesWriter.writeTo(out);
-      bytesWriter.reset();
-
-      // 3rd pass: write the freqs as byte[] blob
-      // TODO: cutover to better intblock codec.  simple64?
-      // write prefix, suffix first:
-      for(int termCount=0;termCount<pendingCount;termCount++) {
-        final TermStats stats = pendingTerms[termCount].stats;
-        assert stats != null;
-        bytesWriter.writeVInt(stats.docFreq);
-        if (fieldInfo.getIndexOptions() != IndexOptions.DOCS_ONLY) {
-          bytesWriter.writeVLong(stats.totalTermFreq-stats.docFreq);
-        }
-      }
-
-      out.writeVInt((int) bytesWriter.getFilePointer());
-      bytesWriter.writeTo(out);
-      bytesWriter.reset();
-
-      postingsWriter.flushTermsBlock(pendingCount, pendingCount);
-      lastPrevTerm.copyBytes(pendingTerms[pendingCount-1].term);
-      pendingCount = 0;
-    }
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/FixedGapTermsIndexReader.java b/lucene/core/src/java/org/apache/lucene/codecs/FixedGapTermsIndexReader.java
deleted file mode 100644
index c655b93..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/FixedGapTermsIndexReader.java
+++ /dev/null
@@ -1,414 +0,0 @@
-package org.apache.lucene.codecs;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.PagedBytes;
-import org.apache.lucene.util.packed.PackedInts;
-
-import java.util.HashMap;
-import java.util.Comparator;
-import java.io.IOException;
-
-import org.apache.lucene.index.IndexFileNames;
-
-/** 
- * TermsIndexReader for simple every Nth terms indexes.
- *
- * @see FixedGapTermsIndexWriter
- * @lucene.experimental 
- */
-public class FixedGapTermsIndexReader extends TermsIndexReaderBase {
-
-  // NOTE: long is overkill here, since this number is 128
-  // by default and only indexDivisor * 128 if you change
-  // the indexDivisor at search time.  But, we use this in a
-  // number of places to multiply out the actual ord, and we
-  // will overflow int during those multiplies.  So to avoid
-  // having to upgrade each multiple to long in multiple
-  // places (error prone), we use long here:
-  private long totalIndexInterval;
-
-  private int indexDivisor;
-  final private int indexInterval;
-
-  // Closed if indexLoaded is true:
-  private IndexInput in;
-  private volatile boolean indexLoaded;
-
-  private final Comparator<BytesRef> termComp;
-
-  private final static int PAGED_BYTES_BITS = 15;
-
-  // all fields share this single logical byte[]
-  private final PagedBytes termBytes = new PagedBytes(PAGED_BYTES_BITS);
-  private PagedBytes.Reader termBytesReader;
-
-  final HashMap<FieldInfo,FieldIndexData> fields = new HashMap<FieldInfo,FieldIndexData>();
-  
-  // start of the field info data
-  protected long dirOffset;
-
-  public FixedGapTermsIndexReader(Directory dir, FieldInfos fieldInfos, String segment, int indexDivisor, Comparator<BytesRef> termComp, String segmentSuffix, IOContext context)
-    throws IOException {
-
-    this.termComp = termComp;
-
-    assert indexDivisor == -1 || indexDivisor > 0;
-
-    in = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, FixedGapTermsIndexWriter.TERMS_INDEX_EXTENSION), context);
-    
-    boolean success = false;
-
-    try {
-      
-      readHeader(in);
-      indexInterval = in.readInt();
-      if (indexInterval < 1) {
-        throw new CorruptIndexException("invalid indexInterval: " + indexInterval + " (resource=" + in + ")");
-      }
-      this.indexDivisor = indexDivisor;
-
-      if (indexDivisor < 0) {
-        totalIndexInterval = indexInterval;
-      } else {
-        // In case terms index gets loaded, later, on demand
-        totalIndexInterval = indexInterval * indexDivisor;
-      }
-      assert totalIndexInterval > 0;
-      
-      seekDir(in, dirOffset);
-
-      // Read directory
-      final int numFields = in.readVInt();     
-      if (numFields < 0) {
-        throw new CorruptIndexException("invalid numFields: " + numFields + " (resource=" + in + ")");
-      }
-      //System.out.println("FGR: init seg=" + segment + " div=" + indexDivisor + " nF=" + numFields);
-      for(int i=0;i<numFields;i++) {
-        final int field = in.readVInt();
-        final int numIndexTerms = in.readVInt();
-        if (numIndexTerms < 0) {
-          throw new CorruptIndexException("invalid numIndexTerms: " + numIndexTerms + " (resource=" + in + ")");
-        }
-        final long termsStart = in.readVLong();
-        final long indexStart = in.readVLong();
-        final long packedIndexStart = in.readVLong();
-        final long packedOffsetsStart = in.readVLong();
-        if (packedIndexStart < indexStart) {
-          throw new CorruptIndexException("invalid packedIndexStart: " + packedIndexStart + " indexStart: " + indexStart + "numIndexTerms: " + numIndexTerms + " (resource=" + in + ")");
-        }
-        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);
-        FieldIndexData previous = fields.put(fieldInfo, new FieldIndexData(fieldInfo, numIndexTerms, indexStart, termsStart, packedIndexStart, packedOffsetsStart));
-        if (previous != null) {
-          throw new CorruptIndexException("duplicate field: " + fieldInfo.name + " (resource=" + in + ")");
-        }
-      }
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(in);
-      }
-      if (indexDivisor > 0) {
-        in.close();
-        in = null;
-        if (success) {
-          indexLoaded = true;
-        }
-        termBytesReader = termBytes.freeze(true);
-      }
-    }
-  }
-  
-  @Override
-  public int getDivisor() {
-    return indexDivisor;
-  }
-
-  protected void readHeader(IndexInput input) throws IOException {
-    CodecUtil.checkHeader(input, FixedGapTermsIndexWriter.CODEC_NAME,
-      FixedGapTermsIndexWriter.VERSION_START, FixedGapTermsIndexWriter.VERSION_START);
-    dirOffset = input.readLong();
-  }
-
-  private class IndexEnum extends FieldIndexEnum {
-    private final FieldIndexData.CoreFieldIndex fieldIndex;
-    private final BytesRef term = new BytesRef();
-    private long ord;
-
-    public IndexEnum(FieldIndexData.CoreFieldIndex fieldIndex) {
-      this.fieldIndex = fieldIndex;
-    }
-
-    @Override
-    public BytesRef term() {
-      return term;
-    }
-
-    @Override
-    public long seek(BytesRef target) {
-      int lo = 0;				  // binary search
-      int hi = fieldIndex.numIndexTerms - 1;
-      assert totalIndexInterval > 0 : "totalIndexInterval=" + totalIndexInterval;
-
-      while (hi >= lo) {
-        int mid = (lo + hi) >>> 1;
-
-        final long offset = fieldIndex.termOffsets.get(mid);
-        final int length = (int) (fieldIndex.termOffsets.get(1+mid) - offset);
-        termBytesReader.fillSlice(term, fieldIndex.termBytesStart + offset, length);
-
-        int delta = termComp.compare(target, term);
-        if (delta < 0) {
-          hi = mid - 1;
-        } else if (delta > 0) {
-          lo = mid + 1;
-        } else {
-          assert mid >= 0;
-          ord = mid*totalIndexInterval;
-          return fieldIndex.termsStart + fieldIndex.termsDictOffsets.get(mid);
-        }
-      }
-
-      if (hi < 0) {
-        assert hi == -1;
-        hi = 0;
-      }
-
-      final long offset = fieldIndex.termOffsets.get(hi);
-      final int length = (int) (fieldIndex.termOffsets.get(1+hi) - offset);
-      termBytesReader.fillSlice(term, fieldIndex.termBytesStart + offset, length);
-
-      ord = hi*totalIndexInterval;
-      return fieldIndex.termsStart + fieldIndex.termsDictOffsets.get(hi);
-    }
-
-    @Override
-    public long next() {
-      final int idx = 1 + (int) (ord / totalIndexInterval);
-      if (idx >= fieldIndex.numIndexTerms) {
-        return -1;
-      }
-      ord += totalIndexInterval;
-
-      final long offset = fieldIndex.termOffsets.get(idx);
-      final int length = (int) (fieldIndex.termOffsets.get(1+idx) - offset);
-      termBytesReader.fillSlice(term, fieldIndex.termBytesStart + offset, length);
-      return fieldIndex.termsStart + fieldIndex.termsDictOffsets.get(idx);
-    }
-
-    @Override
-    public long ord() {
-      return ord;
-    }
-
-    @Override
-    public long seek(long ord) {
-      int idx = (int) (ord / totalIndexInterval);
-      // caller must ensure ord is in bounds
-      assert idx < fieldIndex.numIndexTerms;
-      final long offset = fieldIndex.termOffsets.get(idx);
-      final int length = (int) (fieldIndex.termOffsets.get(1+idx) - offset);
-      termBytesReader.fillSlice(term, fieldIndex.termBytesStart + offset, length);
-      this.ord = idx * totalIndexInterval;
-      return fieldIndex.termsStart + fieldIndex.termsDictOffsets.get(idx);
-    }
-  }
-
-  @Override
-  public boolean supportsOrd() {
-    return true;
-  }
-
-  private final class FieldIndexData {
-
-    volatile CoreFieldIndex coreIndex;
-
-    private final long indexStart;
-    private final long termsStart;
-    private final long packedIndexStart;
-    private final long packedOffsetsStart;
-
-    private final int numIndexTerms;
-
-    public FieldIndexData(FieldInfo fieldInfo, int numIndexTerms, long indexStart, long termsStart, long packedIndexStart,
-                          long packedOffsetsStart) throws IOException {
-
-      this.termsStart = termsStart;
-      this.indexStart = indexStart;
-      this.packedIndexStart = packedIndexStart;
-      this.packedOffsetsStart = packedOffsetsStart;
-      this.numIndexTerms = numIndexTerms;
-
-      if (indexDivisor > 0) {
-        loadTermsIndex();
-      }
-    }
-
-    private void loadTermsIndex() throws IOException {
-      if (coreIndex == null) {
-        coreIndex = new CoreFieldIndex(indexStart, termsStart, packedIndexStart, packedOffsetsStart, numIndexTerms);
-      }
-    }
-
-    private final class CoreFieldIndex {
-
-      // where this field's terms begin in the packed byte[]
-      // data
-      final long termBytesStart;
-
-      // offset into index termBytes
-      final PackedInts.Reader termOffsets;
-
-      // index pointers into main terms dict
-      final PackedInts.Reader termsDictOffsets;
-
-      final int numIndexTerms;
-      final long termsStart;
-
-      public CoreFieldIndex(long indexStart, long termsStart, long packedIndexStart, long packedOffsetsStart, int numIndexTerms) throws IOException {
-
-        this.termsStart = termsStart;
-        termBytesStart = termBytes.getPointer();
-
-        IndexInput clone = in.clone();
-        clone.seek(indexStart);
-
-        // -1 is passed to mean "don't load term index", but
-        // if we are then later loaded it's overwritten with
-        // a real value
-        assert indexDivisor > 0;
-
-        this.numIndexTerms = 1+(numIndexTerms-1) / indexDivisor;
-
-        assert this.numIndexTerms  > 0: "numIndexTerms=" + numIndexTerms + " indexDivisor=" + indexDivisor;
-
-        if (indexDivisor == 1) {
-          // Default (load all index terms) is fast -- slurp in the images from disk:
-          
-          try {
-            final long numTermBytes = packedIndexStart - indexStart;
-            termBytes.copy(clone, numTermBytes);
-
-            // records offsets into main terms dict file
-            termsDictOffsets = PackedInts.getReader(clone);
-            assert termsDictOffsets.size() == numIndexTerms;
-
-            // records offsets into byte[] term data
-            termOffsets = PackedInts.getReader(clone);
-            assert termOffsets.size() == 1+numIndexTerms;
-          } finally {
-            clone.close();
-          }
-        } else {
-          // Get packed iterators
-          final IndexInput clone1 = in.clone();
-          final IndexInput clone2 = in.clone();
-
-          try {
-            // Subsample the index terms
-            clone1.seek(packedIndexStart);
-            final PackedInts.ReaderIterator termsDictOffsetsIter = PackedInts.getReaderIterator(clone1, PackedInts.DEFAULT_BUFFER_SIZE);
-
-            clone2.seek(packedOffsetsStart);
-            final PackedInts.ReaderIterator termOffsetsIter = PackedInts.getReaderIterator(clone2,  PackedInts.DEFAULT_BUFFER_SIZE);
-
-            // TODO: often we can get by w/ fewer bits per
-            // value, below.. .but this'd be more complex:
-            // we'd have to try @ fewer bits and then grow
-            // if we overflowed it.
-
-            PackedInts.Mutable termsDictOffsetsM = PackedInts.getMutable(this.numIndexTerms, termsDictOffsetsIter.getBitsPerValue(), PackedInts.DEFAULT);
-            PackedInts.Mutable termOffsetsM = PackedInts.getMutable(this.numIndexTerms+1, termOffsetsIter.getBitsPerValue(), PackedInts.DEFAULT);
-
-            termsDictOffsets = termsDictOffsetsM;
-            termOffsets = termOffsetsM;
-
-            int upto = 0;
-
-            long termOffsetUpto = 0;
-
-            while(upto < this.numIndexTerms) {
-              // main file offset copies straight over
-              termsDictOffsetsM.set(upto, termsDictOffsetsIter.next());
-
-              termOffsetsM.set(upto, termOffsetUpto);
-
-              long termOffset = termOffsetsIter.next();
-              long nextTermOffset = termOffsetsIter.next();
-              final int numTermBytes = (int) (nextTermOffset - termOffset);
-
-              clone.seek(indexStart + termOffset);
-              assert indexStart + termOffset < clone.length() : "indexStart=" + indexStart + " termOffset=" + termOffset + " len=" + clone.length();
-              assert indexStart + termOffset + numTermBytes < clone.length();
-
-              termBytes.copy(clone, numTermBytes);
-              termOffsetUpto += numTermBytes;
-
-              upto++;
-              if (upto == this.numIndexTerms) {
-                break;
-              }
-
-              // skip terms:
-              termsDictOffsetsIter.next();
-              for(int i=0;i<indexDivisor-2;i++) {
-                termOffsetsIter.next();
-                termsDictOffsetsIter.next();
-              }
-            }
-            termOffsetsM.set(upto, termOffsetUpto);
-
-          } finally {
-            clone1.close();
-            clone2.close();
-            clone.close();
-          }
-        }
-      }
-    }
-  }
-
-  @Override
-  public FieldIndexEnum getFieldEnum(FieldInfo fieldInfo) {
-    final FieldIndexData fieldData = fields.get(fieldInfo);
-    if (fieldData.coreIndex == null) {
-      return null;
-    } else {
-      return new IndexEnum(fieldData.coreIndex);
-    }
-  }
-
-  @Override
-  public void close() throws IOException {
-    if (in != null && !indexLoaded) {
-      in.close();
-    }
-  }
-
-  protected void seekDir(IndexInput input, long dirOffset) throws IOException {
-    input.seek(dirOffset);
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/FixedGapTermsIndexWriter.java b/lucene/core/src/java/org/apache/lucene/codecs/FixedGapTermsIndexWriter.java
deleted file mode 100644
index fa82964..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/FixedGapTermsIndexWriter.java
+++ /dev/null
@@ -1,255 +0,0 @@
-package org.apache.lucene.codecs;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.packed.PackedInts;
-
-import java.util.List;
-import java.util.ArrayList;
-import java.io.IOException;
-
-/**
- * Selects every Nth term as and index term, and hold term
- * bytes (mostly) fully expanded in memory.  This terms index
- * supports seeking by ord.  See {@link
- * VariableGapTermsIndexWriter} for a more memory efficient
- * terms index that does not support seeking by ord.
- *
- * @lucene.experimental */
-public class FixedGapTermsIndexWriter extends TermsIndexWriterBase {
-  protected final IndexOutput out;
-
-  /** Extension of terms index file */
-  static final String TERMS_INDEX_EXTENSION = "tii";
-
-  final static String CODEC_NAME = "SIMPLE_STANDARD_TERMS_INDEX";
-  final static int VERSION_START = 0;
-  final static int VERSION_CURRENT = VERSION_START;
-
-  final private int termIndexInterval;
-
-  private final List<SimpleFieldWriter> fields = new ArrayList<SimpleFieldWriter>();
-  
-  @SuppressWarnings("unused") private final FieldInfos fieldInfos; // unread
-
-  public FixedGapTermsIndexWriter(SegmentWriteState state) throws IOException {
-    final String indexFileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, TERMS_INDEX_EXTENSION);
-    termIndexInterval = state.termIndexInterval;
-    out = state.directory.createOutput(indexFileName, state.context);
-    boolean success = false;
-    try {
-      fieldInfos = state.fieldInfos;
-      writeHeader(out);
-      out.writeInt(termIndexInterval);
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(out);
-      }
-    }
-  }
-  
-  protected void writeHeader(IndexOutput out) throws IOException {
-    CodecUtil.writeHeader(out, CODEC_NAME, VERSION_CURRENT);
-    // Placeholder for dir offset
-    out.writeLong(0);
-  }
-
-  @Override
-  public FieldWriter addField(FieldInfo field, long termsFilePointer) {
-    //System.out.println("FGW: addFfield=" + field.name);
-    SimpleFieldWriter writer = new SimpleFieldWriter(field, termsFilePointer);
-    fields.add(writer);
-    return writer;
-  }
-
-  /** NOTE: if your codec does not sort in unicode code
-   *  point order, you must override this method, to simply
-   *  return indexedTerm.length. */
-  protected int indexedTermPrefixLength(final BytesRef priorTerm, final BytesRef indexedTerm) {
-    // As long as codec sorts terms in unicode codepoint
-    // order, we can safely strip off the non-distinguishing
-    // suffix to save RAM in the loaded terms index.
-    final int idxTermOffset = indexedTerm.offset;
-    final int priorTermOffset = priorTerm.offset;
-    final int limit = Math.min(priorTerm.length, indexedTerm.length);
-    for(int byteIdx=0;byteIdx<limit;byteIdx++) {
-      if (priorTerm.bytes[priorTermOffset+byteIdx] != indexedTerm.bytes[idxTermOffset+byteIdx]) {
-        return byteIdx+1;
-      }
-    }
-    return Math.min(1+priorTerm.length, indexedTerm.length);
-  }
-
-  private class SimpleFieldWriter extends FieldWriter {
-    final FieldInfo fieldInfo;
-    int numIndexTerms;
-    final long indexStart;
-    final long termsStart;
-    long packedIndexStart;
-    long packedOffsetsStart;
-    private long numTerms;
-
-    // TODO: we could conceivably make a PackedInts wrapper
-    // that auto-grows... then we wouldn't force 6 bytes RAM
-    // per index term:
-    private short[] termLengths;
-    private int[] termsPointerDeltas;
-    private long lastTermsPointer;
-    private long totTermLength;
-
-    private final BytesRef lastTerm = new BytesRef();
-
-    SimpleFieldWriter(FieldInfo fieldInfo, long termsFilePointer) {
-      this.fieldInfo = fieldInfo;
-      indexStart = out.getFilePointer();
-      termsStart = lastTermsPointer = termsFilePointer;
-      termLengths = new short[0];
-      termsPointerDeltas = new int[0];
-    }
-
-    @Override
-    public boolean checkIndexTerm(BytesRef text, TermStats stats) throws IOException {
-      // First term is first indexed term:
-      //System.out.println("FGW: checkIndexTerm text=" + text.utf8ToString());
-      if (0 == (numTerms++ % termIndexInterval)) {
-        return true;
-      } else {
-        if (0 == numTerms % termIndexInterval) {
-          // save last term just before next index term so we
-          // can compute wasted suffix
-          lastTerm.copyBytes(text);
-        }
-        return false;
-      }
-    }
-
-    @Override
-    public void add(BytesRef text, TermStats stats, long termsFilePointer) throws IOException {
-      final int indexedTermLength = indexedTermPrefixLength(lastTerm, text);
-      //System.out.println("FGW: add text=" + text.utf8ToString() + " " + text + " fp=" + termsFilePointer);
-
-      // write only the min prefix that shows the diff
-      // against prior term
-      out.writeBytes(text.bytes, text.offset, indexedTermLength);
-
-      if (termLengths.length == numIndexTerms) {
-        termLengths = ArrayUtil.grow(termLengths);
-      }
-      if (termsPointerDeltas.length == numIndexTerms) {
-        termsPointerDeltas = ArrayUtil.grow(termsPointerDeltas);
-      }
-
-      // save delta terms pointer
-      termsPointerDeltas[numIndexTerms] = (int) (termsFilePointer - lastTermsPointer);
-      lastTermsPointer = termsFilePointer;
-
-      // save term length (in bytes)
-      assert indexedTermLength <= Short.MAX_VALUE;
-      termLengths[numIndexTerms] = (short) indexedTermLength;
-      totTermLength += indexedTermLength;
-
-      lastTerm.copyBytes(text);
-      numIndexTerms++;
-    }
-
-    @Override
-    public void finish(long termsFilePointer) throws IOException {
-
-      // write primary terms dict offsets
-      packedIndexStart = out.getFilePointer();
-
-      PackedInts.Writer w = PackedInts.getWriter(out, numIndexTerms, PackedInts.bitsRequired(termsFilePointer), PackedInts.DEFAULT);
-
-      // relative to our indexStart
-      long upto = 0;
-      for(int i=0;i<numIndexTerms;i++) {
-        upto += termsPointerDeltas[i];
-        w.add(upto);
-      }
-      w.finish();
-
-      packedOffsetsStart = out.getFilePointer();
-
-      // write offsets into the byte[] terms
-      w = PackedInts.getWriter(out, 1+numIndexTerms, PackedInts.bitsRequired(totTermLength), PackedInts.DEFAULT);
-      upto = 0;
-      for(int i=0;i<numIndexTerms;i++) {
-        w.add(upto);
-        upto += termLengths[i];
-      }
-      w.add(upto);
-      w.finish();
-
-      // our referrer holds onto us, while other fields are
-      // being written, so don't tie up this RAM:
-      termLengths = null;
-      termsPointerDeltas = null;
-    }
-  }
-
-  public void close() throws IOException {
-    boolean success = false;
-    try {
-      final long dirStart = out.getFilePointer();
-      final int fieldCount = fields.size();
-      
-      int nonNullFieldCount = 0;
-      for(int i=0;i<fieldCount;i++) {
-        SimpleFieldWriter field = fields.get(i);
-        if (field.numIndexTerms > 0) {
-          nonNullFieldCount++;
-        }
-      }
-      
-      out.writeVInt(nonNullFieldCount);
-      for(int i=0;i<fieldCount;i++) {
-        SimpleFieldWriter field = fields.get(i);
-        if (field.numIndexTerms > 0) {
-          out.writeVInt(field.fieldInfo.number);
-          out.writeVInt(field.numIndexTerms);
-          out.writeVLong(field.termsStart);
-          out.writeVLong(field.indexStart);
-          out.writeVLong(field.packedIndexStart);
-          out.writeVLong(field.packedOffsetsStart);
-        }
-      }
-      writeTrailer(dirStart);
-      success = true;
-    } finally {
-      if (success) {
-        IOUtils.close(out);
-      } else {
-        IOUtils.closeWhileHandlingException(out);
-      }
-    }
-  }
-
-  protected void writeTrailer(long dirStart) throws IOException {
-    out.seek(CodecUtil.headerLength(CODEC_NAME));
-    out.writeLong(dirStart);
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/PostingsWriterBase.java b/lucene/core/src/java/org/apache/lucene/codecs/PostingsWriterBase.java
index c8e39b8..0702dc5 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/PostingsWriterBase.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/PostingsWriterBase.java
@@ -27,10 +27,10 @@ import org.apache.lucene.index.FieldInfo;
  * Extension of {@link PostingsConsumer} to support pluggable term dictionaries.
  * <p>
  * This class contains additional hooks to interact with the provided
- * term dictionaries such as {@link BlockTreeTermsWriter} and 
- * {@link BlockTermsWriter}. If you want to re-use one of these existing
- * implementations and are only interested in customizing the format of
- * the postings list, extend this class instead.
+ * term dictionaries such as {@link BlockTreeTermsWriter}. If you want
+ * to re-use an existing implementation and are only interested in
+ * customizing the format of the postings list, extend this class
+ * instead.
  * 
  * @see PostingsReaderBase
  * @lucene.experimental
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/TermsIndexReaderBase.java b/lucene/core/src/java/org/apache/lucene/codecs/TermsIndexReaderBase.java
deleted file mode 100644
index 3845305..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/TermsIndexReaderBase.java
+++ /dev/null
@@ -1,74 +0,0 @@
-package org.apache.lucene.codecs;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.util.BytesRef;
-
-import java.io.IOException;
-import java.io.Closeable;
-
-
-// TODO
-//   - allow for non-regular index intervals?  eg with a
-//     long string of rare terms, you don't need such
-//     frequent indexing
-
-/**
- * {@link BlockTermsReader} interacts with an instance of this class
- * to manage its terms index.  The writer must accept
- * indexed terms (many pairs of BytesRef text + long
- * fileOffset), and then this reader must be able to
- * retrieve the nearest index term to a provided term
- * text. 
- * @lucene.experimental */
-
-public abstract class TermsIndexReaderBase implements Closeable {
-
-  public abstract FieldIndexEnum getFieldEnum(FieldInfo fieldInfo);
-
-  public abstract void close() throws IOException;
-
-  public abstract boolean supportsOrd();
-
-  public abstract int getDivisor();
-
-  /** 
-   * Similar to TermsEnum, except, the only "metadata" it
-   * reports for a given indexed term is the long fileOffset
-   * into the main terms dictionary file.
-   */
-  public static abstract class FieldIndexEnum {
-
-    /** Seeks to "largest" indexed term that's <=
-     *  term; returns file pointer index (into the main
-     *  terms index file) for that term */
-    public abstract long seek(BytesRef term) throws IOException;
-
-    /** Returns -1 at end */
-    public abstract long next() throws IOException;
-
-    public abstract BytesRef term();
-
-    /** Only implemented if {@link TermsIndexReaderBase#supportsOrd()} returns true. */
-    public abstract long seek(long ord) throws IOException;
-    
-    /** Only implemented if {@link TermsIndexReaderBase#supportsOrd()} returns true. */
-    public abstract long ord();
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/TermsIndexWriterBase.java b/lucene/core/src/java/org/apache/lucene/codecs/TermsIndexWriterBase.java
deleted file mode 100644
index 477dfde..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/TermsIndexWriterBase.java
+++ /dev/null
@@ -1,45 +0,0 @@
-package org.apache.lucene.codecs;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.util.BytesRef;
-
-import java.io.Closeable;
-import java.io.IOException;
-
-/** 
- * Base class for terms index implementations to plug
- * into {@link BlockTermsWriter}.
- * 
- * @see TermsIndexReaderBase
- * @lucene.experimental 
- */
-public abstract class TermsIndexWriterBase implements Closeable {
-
-  /**
-   * Terms index API for a single field.
-   */
-  public abstract class FieldWriter {
-    public abstract boolean checkIndexTerm(BytesRef text, TermStats stats) throws IOException;
-    public abstract void add(BytesRef text, TermStats stats, long termsFilePointer) throws IOException;
-    public abstract void finish(long termsFilePointer) throws IOException;
-  }
-
-  public abstract FieldWriter addField(FieldInfo fieldInfo, long termsFilePointer) throws IOException;
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/VariableGapTermsIndexReader.java b/lucene/core/src/java/org/apache/lucene/codecs/VariableGapTermsIndexReader.java
deleted file mode 100644
index 6a78349..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/VariableGapTermsIndexReader.java
+++ /dev/null
@@ -1,234 +0,0 @@
-package org.apache.lucene.codecs;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.io.FileOutputStream;   // for toDot
-import java.io.OutputStreamWriter; // for toDot
-import java.io.Writer;             // for toDot
-import java.util.HashMap;
-
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IntsRef;
-import org.apache.lucene.util.fst.Builder;
-import org.apache.lucene.util.fst.BytesRefFSTEnum;
-import org.apache.lucene.util.fst.FST;
-import org.apache.lucene.util.fst.PositiveIntOutputs;
-import org.apache.lucene.util.fst.Util; // for toDot
-
-/** See {@link VariableGapTermsIndexWriter}
- * 
- * @lucene.experimental */
-public class VariableGapTermsIndexReader extends TermsIndexReaderBase {
-
-  private final PositiveIntOutputs fstOutputs = PositiveIntOutputs.getSingleton(true);
-  private int indexDivisor;
-
-  // Closed if indexLoaded is true:
-  private IndexInput in;
-  private volatile boolean indexLoaded;
-
-  final HashMap<FieldInfo,FieldIndexData> fields = new HashMap<FieldInfo,FieldIndexData>();
-  
-  // start of the field info data
-  protected long dirOffset;
-
-  final String segment;
-  public VariableGapTermsIndexReader(Directory dir, FieldInfos fieldInfos, String segment, int indexDivisor, String segmentSuffix, IOContext context)
-    throws IOException {
-    in = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, VariableGapTermsIndexWriter.TERMS_INDEX_EXTENSION), new IOContext(context, true));
-    this.segment = segment;
-    boolean success = false;
-    assert indexDivisor == -1 || indexDivisor > 0;
-
-    try {
-      
-      readHeader(in);
-      this.indexDivisor = indexDivisor;
-
-      seekDir(in, dirOffset);
-
-      // Read directory
-      final int numFields = in.readVInt();
-      if (numFields < 0) {
-        throw new CorruptIndexException("invalid numFields: " + numFields + " (resource=" + in + ")");
-      }
-
-      for(int i=0;i<numFields;i++) {
-        final int field = in.readVInt();
-        final long indexStart = in.readVLong();
-        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);
-        FieldIndexData previous = fields.put(fieldInfo, new FieldIndexData(fieldInfo, indexStart));
-        if (previous != null) {
-          throw new CorruptIndexException("duplicate field: " + fieldInfo.name + " (resource=" + in + ")");
-        }
-      }
-      success = true;
-    } finally {
-      if (indexDivisor > 0) {
-        in.close();
-        in = null;
-        if (success) {
-          indexLoaded = true;
-        }
-      }
-    }
-  }
-
-  @Override
-  public int getDivisor() {
-    return indexDivisor;
-  }
-  
-  protected void readHeader(IndexInput input) throws IOException {
-    CodecUtil.checkHeader(input, VariableGapTermsIndexWriter.CODEC_NAME,
-      VariableGapTermsIndexWriter.VERSION_START, VariableGapTermsIndexWriter.VERSION_START);
-    dirOffset = input.readLong();
-  }
-
-  private static class IndexEnum extends FieldIndexEnum {
-    private final BytesRefFSTEnum<Long> fstEnum;
-    private BytesRefFSTEnum.InputOutput<Long> current;
-
-    public IndexEnum(FST<Long> fst) {
-      fstEnum = new BytesRefFSTEnum<Long>(fst);
-    }
-
-    @Override
-    public BytesRef term() {
-      if (current == null) {
-        return null;
-      } else {
-        return current.input;
-      }
-    }
-
-    @Override
-    public long seek(BytesRef target) throws IOException {
-      //System.out.println("VGR: seek field=" + fieldInfo.name + " target=" + target);
-      current = fstEnum.seekFloor(target);
-      //System.out.println("  got input=" + current.input + " output=" + current.output);
-      return current.output;
-    }
-
-    @Override
-    public long next() throws IOException {
-      //System.out.println("VGR: next field=" + fieldInfo.name);
-      current = fstEnum.next();
-      if (current == null) {
-        //System.out.println("  eof");
-        return -1;
-      } else {
-        return current.output;
-      }
-    }
-
-    @Override
-    public long ord() {
-      throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public long seek(long ord) {
-      throw new UnsupportedOperationException();
-    }
-  }
-
-  @Override
-  public boolean supportsOrd() {
-    return false;
-  }
-
-  private final class FieldIndexData {
-
-    private final long indexStart;
-    // Set only if terms index is loaded:
-    private volatile FST<Long> fst;
-
-    public FieldIndexData(FieldInfo fieldInfo, long indexStart) throws IOException {
-      this.indexStart = indexStart;
-
-      if (indexDivisor > 0) {
-        loadTermsIndex();
-      }
-    }
-
-    private void loadTermsIndex() throws IOException {
-      if (fst == null) {
-        IndexInput clone = in.clone();
-        clone.seek(indexStart);
-        fst = new FST<Long>(clone, fstOutputs);
-        clone.close();
-
-        /*
-        final String dotFileName = segment + "_" + fieldInfo.name + ".dot";
-        Writer w = new OutputStreamWriter(new FileOutputStream(dotFileName));
-        Util.toDot(fst, w, false, false);
-        System.out.println("FST INDEX: SAVED to " + dotFileName);
-        w.close();
-        */
-
-        if (indexDivisor > 1) {
-          // subsample
-          final IntsRef scratchIntsRef = new IntsRef();
-          final PositiveIntOutputs outputs = PositiveIntOutputs.getSingleton(true);
-          final Builder<Long> builder = new Builder<Long>(FST.INPUT_TYPE.BYTE1, outputs);
-          final BytesRefFSTEnum<Long> fstEnum = new BytesRefFSTEnum<Long>(fst);
-          BytesRefFSTEnum.InputOutput<Long> result;
-          int count = indexDivisor;
-          while((result = fstEnum.next()) != null) {
-            if (count == indexDivisor) {
-              builder.add(Util.toIntsRef(result.input, scratchIntsRef), result.output);
-              count = 0;
-            }
-            count++;
-          }
-          fst = builder.finish();
-        }
-      }
-    }
-  }
-
-  @Override
-  public FieldIndexEnum getFieldEnum(FieldInfo fieldInfo) {
-    final FieldIndexData fieldData = fields.get(fieldInfo);
-    if (fieldData.fst == null) {
-      return null;
-    } else {
-      return new IndexEnum(fieldData.fst);
-    }
-  }
-
-  @Override
-  public void close() throws IOException {
-    if (in != null && !indexLoaded) {
-      in.close();
-    }
-  }
-
-  protected void seekDir(IndexInput input, long dirOffset) throws IOException {
-    input.seek(dirOffset);
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/VariableGapTermsIndexWriter.java b/lucene/core/src/java/org/apache/lucene/codecs/VariableGapTermsIndexWriter.java
deleted file mode 100644
index 2156fbc..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/VariableGapTermsIndexWriter.java
+++ /dev/null
@@ -1,321 +0,0 @@
-package org.apache.lucene.codecs;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.IntsRef;
-import org.apache.lucene.util.fst.Builder;
-import org.apache.lucene.util.fst.FST;
-import org.apache.lucene.util.fst.PositiveIntOutputs;
-import org.apache.lucene.util.fst.Util;
-
-/**
- * Selects index terms according to provided pluggable
- * {@link IndexTermSelector}, and stores them in a prefix trie that's
- * loaded entirely in RAM stored as an FST.  This terms
- * index only supports unsigned byte term sort order
- * (unicode codepoint order when the bytes are UTF8).
- *
- * @lucene.experimental */
-public class VariableGapTermsIndexWriter extends TermsIndexWriterBase {
-  protected final IndexOutput out;
-
-  /** Extension of terms index file */
-  static final String TERMS_INDEX_EXTENSION = "tiv";
-
-  final static String CODEC_NAME = "VARIABLE_GAP_TERMS_INDEX";
-  final static int VERSION_START = 0;
-  final static int VERSION_CURRENT = VERSION_START;
-
-  private final List<FSTFieldWriter> fields = new ArrayList<FSTFieldWriter>();
-  
-  @SuppressWarnings("unused") private final FieldInfos fieldInfos; // unread
-  private final IndexTermSelector policy;
-
-  /** 
-   * Hook for selecting which terms should be placed in the terms index.
-   * <p>
-   * {@link #newField} is called at the start of each new field, and
-   * {@link #isIndexTerm} for each term in that field.
-   * 
-   * @lucene.experimental 
-   */
-  public static abstract class IndexTermSelector {
-    /** 
-     * Called sequentially on every term being written,
-     * returning true if this term should be indexed
-     */
-    public abstract boolean isIndexTerm(BytesRef term, TermStats stats);
-    /**
-     * Called when a new field is started.
-     */
-    public abstract void newField(FieldInfo fieldInfo);
-  }
-
-  /** Same policy as {@link FixedGapTermsIndexWriter} */
-  public static final class EveryNTermSelector extends IndexTermSelector {
-    private int count;
-    private final int interval;
-
-    public EveryNTermSelector(int interval) {
-      this.interval = interval;
-      // First term is first indexed term:
-      count = interval;
-    }
-
-    @Override
-    public boolean isIndexTerm(BytesRef term, TermStats stats) {
-      if (count >= interval) {
-        count = 1;
-        return true;
-      } else {
-        count++;
-        return false;
-      }
-    }
-
-    @Override
-    public void newField(FieldInfo fieldInfo) {
-      count = interval;
-    }
-  }
-
-  /** Sets an index term when docFreq >= docFreqThresh, or
-   *  every interval terms.  This should reduce seek time
-   *  to high docFreq terms.  */
-  public static final class EveryNOrDocFreqTermSelector extends IndexTermSelector {
-    private int count;
-    private final int docFreqThresh;
-    private final int interval;
-
-    public EveryNOrDocFreqTermSelector(int docFreqThresh, int interval) {
-      this.interval = interval;
-      this.docFreqThresh = docFreqThresh;
-
-      // First term is first indexed term:
-      count = interval;
-    }
-
-    @Override
-    public boolean isIndexTerm(BytesRef term, TermStats stats) {
-      if (stats.docFreq >= docFreqThresh || count >= interval) {
-        count = 1;
-        return true;
-      } else {
-        count++;
-        return false;
-      }
-    }
-
-    @Override
-    public void newField(FieldInfo fieldInfo) {
-      count = interval;
-    }
-  }
-
-  // TODO: it'd be nice to let the FST builder prune based
-  // on term count of each node (the prune1/prune2 that it
-  // accepts), and build the index based on that.  This
-  // should result in a more compact terms index, more like
-  // a prefix trie than the other selectors, because it
-  // only stores enough leading bytes to get down to N
-  // terms that may complete that prefix.  It becomes
-  // "deeper" when terms are dense, and "shallow" when they
-  // are less dense.
-  //
-  // However, it's not easy to make that work this this
-  // API, because that pruning doesn't immediately know on
-  // seeing each term whether that term will be a seek point
-  // or not.  It requires some non-causality in the API, ie
-  // only on seeing some number of future terms will the
-  // builder decide which past terms are seek points.
-  // Somehow the API'd need to be able to return a "I don't
-  // know" value, eg like a Future, which only later on is
-  // flipped (frozen) to true or false.
-  //
-  // We could solve this with a 2-pass approach, where the
-  // first pass would build an FSA (no outputs) solely to
-  // determine which prefixes are the 'leaves' in the
-  // pruning. The 2nd pass would then look at this prefix
-  // trie to mark the seek points and build the FST mapping
-  // to the true output.
-  //
-  // But, one downside to this approach is that it'd result
-  // in uneven index term selection.  EG with prune1=10, the
-  // resulting index terms could be as frequent as every 10
-  // terms or as rare as every <maxArcCount> * 10 (eg 2560),
-  // in the extremes.
-
-  public VariableGapTermsIndexWriter(SegmentWriteState state, IndexTermSelector policy) throws IOException {
-    final String indexFileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, TERMS_INDEX_EXTENSION);
-    out = state.directory.createOutput(indexFileName, state.context);
-    boolean success = false;
-    try {
-      fieldInfos = state.fieldInfos;
-      this.policy = policy;
-      writeHeader(out);
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(out);
-      }
-    }
-  }
-  
-  protected void writeHeader(IndexOutput out) throws IOException {
-    CodecUtil.writeHeader(out, CODEC_NAME, VERSION_CURRENT);
-    // Placeholder for dir offset
-    out.writeLong(0);
-  }
-
-  @Override
-  public FieldWriter addField(FieldInfo field, long termsFilePointer) throws IOException {
-    ////System.out.println("VGW: field=" + field.name);
-    policy.newField(field);
-    FSTFieldWriter writer = new FSTFieldWriter(field, termsFilePointer);
-    fields.add(writer);
-    return writer;
-  }
-
-  /** NOTE: if your codec does not sort in unicode code
-   *  point order, you must override this method, to simply
-   *  return indexedTerm.length. */
-  protected int indexedTermPrefixLength(final BytesRef priorTerm, final BytesRef indexedTerm) {
-    // As long as codec sorts terms in unicode codepoint
-    // order, we can safely strip off the non-distinguishing
-    // suffix to save RAM in the loaded terms index.
-    final int idxTermOffset = indexedTerm.offset;
-    final int priorTermOffset = priorTerm.offset;
-    final int limit = Math.min(priorTerm.length, indexedTerm.length);
-    for(int byteIdx=0;byteIdx<limit;byteIdx++) {
-      if (priorTerm.bytes[priorTermOffset+byteIdx] != indexedTerm.bytes[idxTermOffset+byteIdx]) {
-        return byteIdx+1;
-      }
-    }
-    return Math.min(1+priorTerm.length, indexedTerm.length);
-  }
-
-  private class FSTFieldWriter extends FieldWriter {
-    private final Builder<Long> fstBuilder;
-    private final PositiveIntOutputs fstOutputs;
-    private final long startTermsFilePointer;
-
-    final FieldInfo fieldInfo;
-    FST<Long> fst;
-    final long indexStart;
-
-    private final BytesRef lastTerm = new BytesRef();
-    private boolean first = true;
-
-    public FSTFieldWriter(FieldInfo fieldInfo, long termsFilePointer) throws IOException {
-      this.fieldInfo = fieldInfo;
-      fstOutputs = PositiveIntOutputs.getSingleton(true);
-      fstBuilder = new Builder<Long>(FST.INPUT_TYPE.BYTE1, fstOutputs);
-      indexStart = out.getFilePointer();
-      ////System.out.println("VGW: field=" + fieldInfo.name);
-
-      // Always put empty string in
-      fstBuilder.add(new IntsRef(), termsFilePointer);
-      startTermsFilePointer = termsFilePointer;
-    }
-
-    @Override
-    public boolean checkIndexTerm(BytesRef text, TermStats stats) throws IOException {
-      //System.out.println("VGW: index term=" + text.utf8ToString());
-      // NOTE: we must force the first term per field to be
-      // indexed, in case policy doesn't:
-      if (policy.isIndexTerm(text, stats) || first) {
-        first = false;
-        //System.out.println("  YES");
-        return true;
-      } else {
-        lastTerm.copyBytes(text);
-        return false;
-      }
-    }
-
-    private final IntsRef scratchIntsRef = new IntsRef();
-
-    @Override
-    public void add(BytesRef text, TermStats stats, long termsFilePointer) throws IOException {
-      if (text.length == 0) {
-        // We already added empty string in ctor
-        assert termsFilePointer == startTermsFilePointer;
-        return;
-      }
-      final int lengthSave = text.length;
-      text.length = indexedTermPrefixLength(lastTerm, text);
-      try {
-        fstBuilder.add(Util.toIntsRef(text, scratchIntsRef), termsFilePointer);
-      } finally {
-        text.length = lengthSave;
-      }
-      lastTerm.copyBytes(text);
-    }
-
-    @Override
-    public void finish(long termsFilePointer) throws IOException {
-      fst = fstBuilder.finish();
-      if (fst != null) {
-        fst.save(out);
-      }
-    }
-  }
-
-  public void close() throws IOException {
-    try {
-    final long dirStart = out.getFilePointer();
-    final int fieldCount = fields.size();
-
-    int nonNullFieldCount = 0;
-    for(int i=0;i<fieldCount;i++) {
-      FSTFieldWriter field = fields.get(i);
-      if (field.fst != null) {
-        nonNullFieldCount++;
-      }
-    }
-
-    out.writeVInt(nonNullFieldCount);
-    for(int i=0;i<fieldCount;i++) {
-      FSTFieldWriter field = fields.get(i);
-      if (field.fst != null) {
-        out.writeVInt(field.fieldInfo.number);
-        out.writeVLong(field.indexStart);
-      }
-    }
-    writeTrailer(dirStart);
-    } finally {
-    out.close();
-  }
-  }
-
-  protected void writeTrailer(long dirStart) throws IOException {
-    out.seek(CodecUtil.headerLength(CODEC_NAME));
-    out.writeLong(dirStart);
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/appending/AppendingCodec.java b/lucene/core/src/java/org/apache/lucene/codecs/appending/AppendingCodec.java
deleted file mode 100644
index c811057..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/appending/AppendingCodec.java
+++ /dev/null
@@ -1,97 +0,0 @@
-package org.apache.lucene.codecs.appending;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.codecs.DocValuesFormat;
-import org.apache.lucene.codecs.FieldInfosFormat;
-import org.apache.lucene.codecs.LiveDocsFormat;
-import org.apache.lucene.codecs.NormsFormat;
-import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.codecs.SegmentInfoFormat;
-import org.apache.lucene.codecs.StoredFieldsFormat;
-import org.apache.lucene.codecs.TermVectorsFormat;
-import org.apache.lucene.codecs.lucene40.Lucene40Codec;
-import org.apache.lucene.codecs.lucene40.Lucene40DocValuesFormat;
-import org.apache.lucene.codecs.lucene40.Lucene40FieldInfosFormat;
-import org.apache.lucene.codecs.lucene40.Lucene40LiveDocsFormat;
-import org.apache.lucene.codecs.lucene40.Lucene40NormsFormat;
-import org.apache.lucene.codecs.lucene40.Lucene40SegmentInfoFormat;
-import org.apache.lucene.codecs.lucene40.Lucene40StoredFieldsFormat;
-import org.apache.lucene.codecs.lucene40.Lucene40TermVectorsFormat;
-
-/**
- * This codec extends {@link Lucene40Codec} to work on append-only outputs, such
- * as plain output streams and append-only filesystems.
- * 
- * @lucene.experimental
- */
-public class AppendingCodec extends Codec {
-  public AppendingCodec() {
-    super("Appending");
-  }
-
-  private final PostingsFormat postings = new AppendingPostingsFormat();
-  private final SegmentInfoFormat infos = new Lucene40SegmentInfoFormat();
-  private final StoredFieldsFormat fields = new Lucene40StoredFieldsFormat();
-  private final FieldInfosFormat fieldInfos = new Lucene40FieldInfosFormat();
-  private final TermVectorsFormat vectors = new Lucene40TermVectorsFormat();
-  private final DocValuesFormat docValues = new Lucene40DocValuesFormat();
-  private final NormsFormat norms = new Lucene40NormsFormat();
-  private final LiveDocsFormat liveDocs = new Lucene40LiveDocsFormat();
-  
-  @Override
-  public PostingsFormat postingsFormat() {
-    return postings;
-  }
-
-  @Override
-  public StoredFieldsFormat storedFieldsFormat() {
-    return fields;
-  }
-  
-  @Override
-  public TermVectorsFormat termVectorsFormat() {
-    return vectors;
-  }
-
-  @Override
-  public DocValuesFormat docValuesFormat() {
-    return docValues;
-  }
-
-  @Override
-  public SegmentInfoFormat segmentInfoFormat() {
-    return infos;
-  }
-  
-  @Override
-  public FieldInfosFormat fieldInfosFormat() {
-    return fieldInfos;
-  }
-  
-  @Override
-  public NormsFormat normsFormat() {
-    return norms;
-  }
-  
-  @Override
-  public LiveDocsFormat liveDocsFormat() {
-    return liveDocs;
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/appending/AppendingPostingsFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/appending/AppendingPostingsFormat.java
deleted file mode 100644
index 2f72178..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/appending/AppendingPostingsFormat.java
+++ /dev/null
@@ -1,80 +0,0 @@
-package org.apache.lucene.codecs.appending;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.BlockTreeTermsWriter;
-import org.apache.lucene.codecs.FieldsConsumer;
-import org.apache.lucene.codecs.FieldsProducer;
-import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.codecs.PostingsReaderBase;
-import org.apache.lucene.codecs.PostingsWriterBase;
-import org.apache.lucene.codecs.lucene40.Lucene40PostingsReader;
-import org.apache.lucene.codecs.lucene40.Lucene40PostingsWriter;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SegmentWriteState;
-
-/**
- * Appending postings impl.
- */
-class AppendingPostingsFormat extends PostingsFormat {
-  public static String CODEC_NAME = "Appending";
-  
-  public AppendingPostingsFormat() {
-    super(CODEC_NAME);
-  }
-
-  @Override
-  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-    PostingsWriterBase docsWriter = new Lucene40PostingsWriter(state);
-    boolean success = false;
-    try {
-      FieldsConsumer ret = new AppendingTermsWriter(state, docsWriter, BlockTreeTermsWriter.DEFAULT_MIN_BLOCK_SIZE, BlockTreeTermsWriter.DEFAULT_MAX_BLOCK_SIZE);
-      success = true;
-      return ret;
-    } finally {
-      if (!success) {
-        docsWriter.close();
-      }
-    }
-  }
-
-  @Override
-  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
-    PostingsReaderBase postings = new Lucene40PostingsReader(state.dir, state.fieldInfos, state.segmentInfo, state.context, state.segmentSuffix);
-    
-    boolean success = false;
-    try {
-      FieldsProducer ret = new AppendingTermsReader(
-                                                    state.dir,
-                                                    state.fieldInfos,
-                                                    state.segmentInfo,
-                                                    postings,
-                                                    state.context,
-                                                    state.segmentSuffix,
-                                                    state.termsIndexDivisor);
-      success = true;
-      return ret;
-    } finally {
-      if (!success) {
-        postings.close();
-      }
-    }
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/appending/AppendingTermsReader.java b/lucene/core/src/java/org/apache/lucene/codecs/appending/AppendingTermsReader.java
deleted file mode 100644
index f89eae1..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/appending/AppendingTermsReader.java
+++ /dev/null
@@ -1,62 +0,0 @@
-package org.apache.lucene.codecs.appending;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.BlockTreeTermsReader;
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.PostingsReaderBase;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-
-/**
- * Reads append-only terms from {@link AppendingTermsWriter}
- * @lucene.experimental
- */
-public class AppendingTermsReader extends BlockTreeTermsReader {
-
-  public AppendingTermsReader(Directory dir, FieldInfos fieldInfos, SegmentInfo info, PostingsReaderBase postingsReader, 
-      IOContext ioContext, String segmentSuffix, int indexDivisor) throws IOException {
-    super(dir, fieldInfos, info, postingsReader, ioContext, segmentSuffix, indexDivisor);
-  }
-
-  @Override
-  protected void readHeader(IndexInput input) throws IOException {
-    CodecUtil.checkHeader(input, AppendingTermsWriter.TERMS_CODEC_NAME,
-        AppendingTermsWriter.TERMS_VERSION_START,
-        AppendingTermsWriter.TERMS_VERSION_CURRENT);  
-  }
-
-  @Override
-  protected void readIndexHeader(IndexInput input) throws IOException {
-    CodecUtil.checkHeader(input, AppendingTermsWriter.TERMS_INDEX_CODEC_NAME,
-        AppendingTermsWriter.TERMS_INDEX_VERSION_START,
-        AppendingTermsWriter.TERMS_INDEX_VERSION_CURRENT);
-  }
-  
-  @Override
-  protected void seekDir(IndexInput input, long dirOffset) throws IOException {
-    input.seek(input.length() - Long.SIZE / 8);
-    long offset = input.readLong();
-    input.seek(offset);
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/appending/AppendingTermsWriter.java b/lucene/core/src/java/org/apache/lucene/codecs/appending/AppendingTermsWriter.java
deleted file mode 100644
index 8025a21..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/appending/AppendingTermsWriter.java
+++ /dev/null
@@ -1,64 +0,0 @@
-package org.apache.lucene.codecs.appending;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.BlockTreeTermsWriter;
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.PostingsWriterBase;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.store.IndexOutput;
-
-/**
- * Append-only version of {@link BlockTreeTermsWriter}
- * @lucene.experimental
- */
-public class AppendingTermsWriter extends BlockTreeTermsWriter {
-  final static String TERMS_CODEC_NAME = "APPENDING_TERMS_DICT";
-  final static int TERMS_VERSION_START = 0;
-  final static int TERMS_VERSION_CURRENT = TERMS_VERSION_START;
-  
-  final static String TERMS_INDEX_CODEC_NAME = "APPENDING_TERMS_INDEX";
-  final static int TERMS_INDEX_VERSION_START = 0;
-  final static int TERMS_INDEX_VERSION_CURRENT = TERMS_INDEX_VERSION_START;
-  
-  public AppendingTermsWriter(SegmentWriteState state, PostingsWriterBase postingsWriter, int minItemsInBlock, int maxItemsInBlock) throws IOException {
-    super(state, postingsWriter, minItemsInBlock, maxItemsInBlock);
-  }
-
-  @Override
-  protected void writeHeader(IndexOutput out) throws IOException {
-    CodecUtil.writeHeader(out, TERMS_CODEC_NAME, TERMS_VERSION_CURRENT);
-  }
-
-  @Override
-  protected void writeIndexHeader(IndexOutput out) throws IOException {
-    CodecUtil.writeHeader(out, TERMS_INDEX_CODEC_NAME, TERMS_INDEX_VERSION_CURRENT);
-  }
-
-  @Override
-  protected void writeTrailer(IndexOutput out, long dirStart) throws IOException {
-    out.writeLong(dirStart);
-  }
-
-  @Override
-  protected void writeIndexTrailer(IndexOutput indexOut, long dirStart) throws IOException {
-    indexOut.writeLong(dirStart);
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/appending/package.html b/lucene/core/src/java/org/apache/lucene/codecs/appending/package.html
deleted file mode 100644
index 940808a..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/appending/package.html
+++ /dev/null
@@ -1,25 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
-</head>
-<body>
-Codec for on append-only outputs, such as plain output streams and append-only filesystems.
-</body>
-</html>
\ No newline at end of file
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/block/BlockPostingsFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/block/BlockPostingsFormat.java
deleted file mode 100644
index 6fafbd6..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/block/BlockPostingsFormat.java
+++ /dev/null
@@ -1,422 +0,0 @@
-package org.apache.lucene.codecs.block;
-
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.BlockTreeTermsReader;
-import org.apache.lucene.codecs.BlockTreeTermsWriter;
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.FieldsConsumer;
-import org.apache.lucene.codecs.FieldsProducer;
-import org.apache.lucene.codecs.MultiLevelSkipListWriter;
-import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.codecs.PostingsReaderBase;
-import org.apache.lucene.codecs.PostingsWriterBase;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.store.DataOutput;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.packed.PackedInts;
-
-/**
- * Block postings format, which encodes postings in packed int blocks 
- * for faster decode.
- *
- * <p><b>NOTE</b>: this format is still experimental and
- * subject to change without backwards compatibility.
- *
- * <p>
- * Basic idea:
- * <ul>
- *   <li>
- *   <b>Packed Block and VInt Block</b>: 
- *   <p>In packed block, integers are encoded with the same bit width ({@link PackedInts packed format}), 
- *      the block size (i.e. number of integers inside block) is fixed. </p>
- *   <p>In VInt block, integers are encoded as {@link DataOutput#writeVInt VInt}, 
- *      the block size is variable.</p>
- *   </li>
- *
- *   <li> 
- *   <b>Block structure</b>: 
- *   <p>When the postings is long enough, BlockPostingsFormat will try to encode most integer data 
- *      as packed block.</p> 
- *   <p>Take a term with 259 documents as example, the first 256 document ids are encoded as two packed 
- *      blocks, while the remaining 3 as one VInt block. </p>
- *   <p>Different kinds of data are always encoded separately into different packed blocks, but may 
- *      possible be encoded into a same VInt block. </p>
- *   <p>This strategy is applied to pairs: 
- *      &lt;document number, frequency&gt;,
- *      &lt;position, payload length&gt;, 
- *      &lt;position, offset start, offset length&gt;, and
- *      &lt;position, payload length, offsetstart, offset length&gt;.</p>
- *   </li>
- *
- *   <li>
- *   <b>Skipper setting</b>: 
- *   <p>The structure of skip table is quite similar to Lucene40PostingsFormat. Skip interval is the 
- *      same as block size, and each skip entry points to the beginning of each block. However, for 
- *      the first block, skip data is omitted.</p>
- *   </li>
- *
- *   <li>
- *   <b>Positions, Payloads, and Offsets</b>: 
- *   <p>A position is an integer indicating where the term occurs at within one document. 
- *      A payload is a blob of metadata associated with current position. 
- *      An offset is a pair of integers indicating the tokenized start/end offsets for given term 
- *      in current position. </p>
- *   <p>When payloads and offsets are not omitted, numPositions==numPayloads==numOffsets (assuming a 
- *      null payload contributes one count). As mentioned in block structure, it is possible to encode 
- *      these three either combined or separately. 
- *   <p>For all the cases, payloads and offsets are stored together. When encoded as packed block, 
- *      position data is separated out as .pos, while payloads and offsets are encoded in .pay (payload 
- *      metadata will also be stored directly in .pay). When encoded as VInt block, all these three are 
- *      stored in .pos (so as payload metadata).</p>
- *   <p>With this strategy, the majority of payload and offset data will be outside .pos file. 
- *      So for queries that require only position data, running on a full index with payloads and offsets, 
- *      this reduces disk pre-fetches.</p>
- *   </li>
- * </ul>
- * </p>
- *
- * <p>
- * Files and detailed format:
- * <ul>
- *   <li><tt>.tim</tt>: <a href="#Termdictionary">Term Dictionary</a></li>
- *   <li><tt>.tip</tt>: <a href="#Termindex">Term Index</a></li>
- *   <li><tt>.doc</tt>: <a href="#Frequencies">Frequencies and Skip Data</a></li>
- *   <li><tt>.pos</tt>: <a href="#Positions">Positions</a></li>
- *   <li><tt>.pay</tt>: <a href="#Payloads">Payloads and Offsets</a></li>
- * </ul>
- * </p>
- *
- * <a name="Termdictionary" id="Termdictionary"></a>
- * <dl>
- * <dd>
- * <b>Term Dictionary</b>
- *
- * <p>The .tim file format is quite similar to Lucene40PostingsFormat, 
- *  with minor difference in MetadataBlock</p>
- *
- * <ul>
- * <!-- TODO: expand on this, its not really correct and doesnt explain sub-blocks etc -->
- *   <li>TermDictionary(.tim) --&gt; Header, DirOffset, PostingsHeader, PackedBlockSize, 
- *                                   &lt;Block&gt;<sup>NumBlocks</sup>, FieldSummary</li>
- *   <li>Block --&gt; SuffixBlock, StatsBlock, MetadataBlock</li>
- *   <li>SuffixBlock --&gt; EntryCount, SuffixLength, {@link DataOutput#writeByte byte}<sup>SuffixLength</sup></li>
- *   <li>StatsBlock --&gt; StatsLength, &lt;DocFreq, TotalTermFreq&gt;<sup>EntryCount</sup></li>
- *   <li>MetadataBlock --&gt; MetaLength, &lt;DocFPDelta, 
- *                            &lt;PosFPDelta, PosVIntBlockFPDelta?, PayFPDelta?&gt;?, 
- *                            SkipFPDelta?&gt;<sup>EntryCount</sup></li>
- *   <li>FieldSummary --&gt; NumFields, &lt;FieldNumber, NumTerms, RootCodeLength, 
- *                           {@link DataOutput#writeByte byte}<sup>RootCodeLength</sup>, SumDocFreq, DocCount&gt;
- *                           <sup>NumFields</sup></li>
- *   <li>Header, PostingsHeader --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
- *   <li>DirOffset --&gt; {@link DataOutput#writeLong Uint64}</li>
- *   <li>PackedBlockSize, EntryCount, SuffixLength, StatsLength, DocFreq, MetaLength, 
- *       PosVIntBlockFPDelta, SkipFPDelta, NumFields, FieldNumber, RootCodeLength, DocCount --&gt; 
- *       {@link DataOutput#writeVInt VInt}</li>
- *   <li>TotalTermFreq, DocFPDelta, PosFPDelta, PayFPDelta, NumTerms, SumTotalTermFreq, SumDocFreq --&gt; 
- *       {@link DataOutput#writeVLong VLong}</li>
- * </ul>
- * <p>Notes:</p>
- * <ul>
- *    <li>Here explains MetadataBlock only, other fields are mentioned in 
- *   <a href="../lucene40/Lucene40PostingsFormat.html#Termdictionary">Lucene40PostingsFormat:TermDictionary</a>
- *    </li>
- *    <li>PackedBlockSize is the fixed block size for packed blocks. In packed block, bit width is 
- *        determined by the largest integer. Smaller block size result in smaller variance among width 
- *        of integers hence smaller indexes. Larger block size result in more efficient bulk i/o hence
- *        better acceleration. This value should always be a multiple of 64, currently fixed as 128 as 
- *        a tradeoff. It is also the skip interval used to accelerate {@link DocsEnum#advance(int)}.
- *    <li>DocFPDelta determines the position of this term's TermFreqs within the .doc file. 
- *        In particular, it is the difference of file offset between this term's
- *        data and previous term's data (or zero, for the first term in the block).On disk it is 
- *        stored as the difference from previous value in sequence. </li>
- *    <li>PosFPDelta determines the position of this term's TermPositions within the .pos file.
- *        While PayFPDelta determines the position of this term's &lt;TermPayloads, TermOffsets?&gt; within 
- *        the .pay file. Similar to DocFPDelta, it is the difference between two file positions (or 
- *        neglected, for fields that omit payloads and offsets).</li>
- *    <li>PosVIntBlockFPDelta determines the position of this term's last TermPosition in last pos packed
- *        block within the .pos file. It is synonym for PayVIntBlockFPDelta or OffsetVIntBlockFPDelta. 
- *        This is actually used to indicate whether it is necessary to load following
- *        payloads and offsets from .pos instead of .pay. Every time a new block of positions are to be 
- *        loaded, the PostingsReader will use this value to check whether current block is packed format
- *        or VInt. When packed format, payloads and offsets are fetched from .pay, otherwise from .pos. 
- *        (this value is neglected when total number of positions i.e. totalTermFreq is less or equal 
- *        to PackedBlockSize).
- *    <li>SkipFPDelta determines the position of this term's SkipData within the .doc
- *        file. In particular, it is the length of the TermFreq data.
- *        SkipDelta is only stored if DocFreq is not smaller than SkipMinimum
- *        (i.e. 8 in BlockPostingsFormat).</li>
- * </ul>
- * </dd>
- * </dl>
- *
- * <a name="Termindex" id="Termindex"></a>
- * <dl>
- * <dd>
- * <b>Term Index</b>
- * <p>The .tim file format is mentioned in
- *   <a href="../lucene40/Lucene40PostingsFormat.html#Termindex">Lucene40PostingsFormat:TermIndex</a>
- * </dd>
- * </dl>
- *
- *
- * <a name="Frequencies" id="Frequencies"></a>
- * <dl>
- * <dd>
- * <b>Frequencies and Skip Data</b>
- *
- * <p>The .doc file contains the lists of documents which contain each term, along
- * with the frequency of the term in that document (except when frequencies are
- * omitted: {@link IndexOptions#DOCS_ONLY}). It also saves skip data to the beginning of 
- * each packed or VInt block, when the length of document list is larger than packed block size.</p>
- *
- * <ul>
- *   <li>docFile(.doc) --&gt; Header, &lt;TermFreqs, SkipData?&gt;<sup>TermCount</sup></li>
- *   <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
- *   <li>TermFreqs --&gt; &lt;PackedBlock&gt; <sup>PackedDocBlockNum</sup>,  
- *                        VIntBlock? </li>
- *   <li>PackedBlock --&gt; PackedDocDeltaBlock, PackedFreqBlock?
- *   <li>VIntBlock --&gt; &lt;DocDelta[, Freq?]&gt;<sup>DocFreq-PackedBlockSize*PackedDocBlockNum</sup>
- *   <li>SkipData --&gt; &lt;&lt;SkipLevelLength, SkipLevel&gt;
- *       <sup>NumSkipLevels-1</sup>, SkipLevel&gt;, SkipDatum?</li>
- *   <li>SkipLevel --&gt; &lt;SkipDatum&gt; <sup>TrimmedDocFreq/(PackedBlockSize^(Level + 1))</sup></li>
- *   <li>SkipDatum --&gt; DocSkip, DocFPSkip, &lt;PosFPSkip, PosBlockOffset, PayLength?, 
- *                        OffsetStart?, PayFPSkip?&gt;?, SkipChildLevelPointer?</li>
- *   <li>PackedDocDeltaBlock, PackedFreqBlock --&gt; {@link PackedInts PackedInts}</li>
- *   <li>DocDelta, Freq, DocSkip, DocFPSkip, PosFPSkip, PosBlockOffset, PayLength, OffsetStart, PayFPSkip 
- *       --&gt; 
- *   {@link DataOutput#writeVInt VInt}</li>
- *   <li>SkipChildLevelPointer --&gt; {@link DataOutput#writeVLong VLong}</li>
- * </ul>
- * <p>Notes:</p>
- * <ul>
- *   <li>PackedDocDeltaBlock is theoretically generated from two steps: 
- *     <ol>
- *       <li>Calculate the difference between each document number and previous one, 
- *           and get a d-gaps list (for the first document, use absolute value); </li>
- *       <li>For those d-gaps from first one to PackedDocBlockNum*PackedBlockSize<sup>th</sup>, 
- *           separately encode as packed blocks.</li>
- *     </ol>
- *     If frequencies are not omitted, PackedFreqBlock will be generated without d-gap step.
- *   </li>
- *   <li>VIntBlock stores remaining d-gaps (along with frequencies when possible) with a format 
- *       mentioned in
- *   <a href="../lucene40/Lucene40PostingsFormat.html#Frequencies">Lucene40PostingsFormat:Frequencies</a>
- *   </li>
- *   <li>PackedDocBlockNum is the number of packed blocks for current term's docids or frequencies. 
- *       In particular, PackedDocBlockNum = floor(DocFreq/PackedBlockSize) </li>
- *   <li>TrimmedDocFreq = DocFreq % PackedBlockSize == 0 ? DocFreq - 1 : DocFreq. 
- *       We use this trick since the definition of skip entry is a little different from base interface.
- *       In {@link MultiLevelSkipListWriter}, skip data is assumed to be saved for
- *       skipInterval<sup>th</sup>, 2*skipInterval<sup>th</sup> ... posting in the list. However, 
- *       in BlockPostingsFormat, the skip data is saved for skipInterval+1<sup>th</sup>, 
- *       2*skipInterval+1<sup>th</sup> ... posting (skipInterval==PackedBlockSize in this case). 
- *       When DocFreq is multiple of PackedBlockSize, MultiLevelSkipListWriter will expect one 
- *       more skip data than BlockSkipWriter. </li>
- *   <li>SkipDatum is the metadata of one skip entry.
- *      For the first block (no matter packed or VInt), it is omitted.</li>
- *   <li>DocSkip records the document number of every PackedBlockSize<sup>th</sup> document number in
- *       the postings (i.e. last document number in each packed block). On disk it is stored as the 
- *       difference from previous value in the sequence. </li>
- *   <li>DocFPSkip records the file offsets of each block (excluding )posting at 
- *       PackedBlockSize+1<sup>th</sup>, 2*PackedBlockSize+1<sup>th</sup> ... , in DocFile. 
- *       The file offsets are relative to the start of current term's TermFreqs. 
- *       On disk it is also stored as the difference from previous SkipDatum in the sequence.</li>
- *   <li>Since positions and payloads are also block encoded, the skip should skip to related block first,
- *       then fetch the values according to in-block offset. PosFPSkip and PayFPSkip record the file 
- *       offsets of related block in .pos and .pay, respectively. While PosBlockOffset indicates
- *       which value to fetch inside the related block (PayBlockOffset is unnecessary since it is always
- *       equal to PosBlockOffset). Same as DocFPSkip, the file offsets are relative to the start of 
- *       current term's TermFreqs, and stored as a difference sequence.</li>
- *   <li>PayLength indicates the length of last payload.</li>
- *   <li>OffsetStart indicates the first value of last offset pair.</li>
- * </ul>
- * </dd>
- * </dl>
- *
- * <a name="Positions" id="Positions"></a>
- * <dl>
- * <dd>
- * <b>Positions</b>
- * <p>The .pos file contains the lists of positions that each term occurs at within documents. It also
- *    sometimes stores part of payloads and offsets for speedup.</p>
- * <ul>
- *   <li>PosFile(.pos) --&gt; Header, &lt;TermPositions&gt; <sup>TermCount</sup></li>
- *   <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
- *   <li>TermPositions --&gt; &lt;PackedPosDeltaBlock&gt; <sup>PackedPosBlockNum</sup>,  
- *                            VIntBlock? </li>
- *   <li>VIntBlock --&gt; PosVIntCount, &lt;PosDelta[, PayLength?], PayData?, 
- *                        OffsetStartDelta?, OffsetLength?&gt;<sup>PosVIntCount</sup>
- *   <li>PackedPosDeltaBlock --&gt; {@link PackedInts PackedInts}</li>
- *   <li>PosVIntCount, PosDelta, OffsetStartDelta, OffsetLength --&gt; 
- *       {@link DataOutput#writeVInt VInt}</li>
- *   <li>PayData --&gt; {@link DataOutput#writeByte byte}<sup>PayLength</sup></li>
- * </ul>
- * <p>Notes:</p>
- * <ul>
- *   <li>TermPositions are order by term (terms are implicit, from the term dictionary), and position 
- *       values for each term document pair are incremental, and ordered by document number.</li>
- *   <li>PackedPosBlockNum is the number of packed blocks for current term's positions, payloads or offsets. 
- *       In particular, PackedPosBlockNum = floor(totalTermFreq/PackedBlockSize) </li>
- *   <li>PosVIntCount is the number of positions encoded as VInt format. In particular, 
- *       PosVIntCount = totalTermFreq - PackedPosBlockNum*PackedBlockSize</li>
- *   <li>The procedure how PackedPosDeltaBlock is generated is the same as PackedDocDeltaBlock 
- *       in chapter <a href="#Frequencies">Frequencies and Skip Data</a>.</li>
- *   <li>PosDelta is the same as the format mentioned in 
- *   <a href="../lucene40/Lucene40PostingsFormat.html#Positions">Lucene40PostingsFormat:Positions</a>
- *   </li>
- *   <li>OffsetStartDelta is the difference between this position's startOffset from the previous 
- *       occurrence (or zero, if this is the first occurrence in this document).</li>
- *   <li>OffsetLength indicates the length of the current offset (endOffset-startOffset).</li>
- *   <li>PayloadData is the blob of metadata associated with current position.</li>
- * </ul>
- * </dd>
- * </dl>
- *
- * <a name="Payloads" id="Payloads"></a>
- * <dl>
- * <dd>
- * <b>Payloads and Offsets</b>
- * <p>The .pay file will store payloads and offsets associated with certain term-document positions. 
- *    Some payloads and offsets will be separated out into .pos file, for speedup reason.</p>
- * <ul>
- *   <li>PayFile(.pay): --&gt; Header, &lt;TermPayloads, TermOffsets?&gt; <sup>TermCount</sup></li>
- *   <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
- *   <li>TermPayloads --&gt; &lt;PackedPayLengthBlock, SumPayLength, PayData&gt; <sup>PackedPayBlockNum</sup>
- *   <li>TermOffsets --&gt; &lt;PackedOffsetStartDeltaBlock, PackedOffsetLengthBlock&gt; <sup>PackedPayBlockNum</sup>
- *   <li>PackedPayLengthBlock, PackedOffsetStartDeltaBlock, PackedOffsetLengthBlock --&gt; {@link PackedInts PackedInts}</li>
- *   <li>SumPayLength --&gt; {@link DataOutput#writeVInt VInt}</li>
- *   <li>PayData --&gt; {@link DataOutput#writeByte byte}<sup>SumPayLength</sup></li>
- * </ul>
- * <p>Notes:</p>
- * <ul>
- *   <li>The order of TermPayloads/TermOffsets will be the same as TermPositions, note that part of 
- *       payload/offsets are stored in .pos.</li>
- *   <li>The procedure how PackedPayLengthBlock and PackedOffsetLengthBlock are generated is the 
- *       same as PackedFreqBlock in chapter <a href="#Frequencies">Frequencies and Skip Data</a>. 
- *       While PackedStartDeltaBlock follows a same procedure as PackedDocDeltaBlock.</li>
- *   <li>PackedPayBlockNum is always equal to PackedPosBlockNum, for the same term. It is also synonym 
- *       for PackedOffsetBlockNum.</li>
- *   <li>SumPayLength is the total length of payloads written within one block, should be the sum
- *       of PayLengths in one packed block.</li>
- *   <li>PayLength in PackedPayLengthBlock is the length of each payload, associated with current 
- *       position.</li>
- * </ul>
- * </dd>
- * </dl>
- * </p>
- *
- * @lucene.experimental
- */
-
-public final class BlockPostingsFormat extends PostingsFormat {
-  /**
-   * Filename extension for document number, frequencies, and skip data.
-   * See chapter: <a href="#Frequencies">Frequencies and Skip Data</a>
-   */
-  public static final String DOC_EXTENSION = "doc";
-
-  /**
-   * Filename extension for positions. 
-   * See chapter: <a href="#Positions">Positions</a>
-   */
-  public static final String POS_EXTENSION = "pos";
-
-  /**
-   * Filename extension for payloads and offsets.
-   * See chapter: <a href="#Payloads">Payloads and Offsets</a>
-   */
-  public static final String PAY_EXTENSION = "pay";
-
-  private final int minTermBlockSize;
-  private final int maxTermBlockSize;
-
-  /**
-   * Fixed packed block size, number of integers encoded in 
-   * a single packed block.
-   */
-  // NOTE: must be multiple of 64 because of PackedInts long-aligned encoding/decoding
-  public final static int BLOCK_SIZE = 128;
-
-  public BlockPostingsFormat() {
-    this(BlockTreeTermsWriter.DEFAULT_MIN_BLOCK_SIZE, BlockTreeTermsWriter.DEFAULT_MAX_BLOCK_SIZE);
-  }
-
-  public BlockPostingsFormat(int minTermBlockSize, int maxTermBlockSize) {
-    super("Block");
-    this.minTermBlockSize = minTermBlockSize;
-    assert minTermBlockSize > 1;
-    this.maxTermBlockSize = maxTermBlockSize;
-    assert minTermBlockSize <= maxTermBlockSize;
-  }
-
-  @Override
-  public String toString() {
-    return getName() + "(blocksize=" + BLOCK_SIZE + ")";
-  }
-
-  @Override
-  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-    PostingsWriterBase postingsWriter = new BlockPostingsWriter(state);
-
-    boolean success = false;
-    try {
-      FieldsConsumer ret = new BlockTreeTermsWriter(state, 
-                                                    postingsWriter,
-                                                    minTermBlockSize, 
-                                                    maxTermBlockSize);
-      success = true;
-      return ret;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(postingsWriter);
-      }
-    }
-  }
-
-  @Override
-  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
-    PostingsReaderBase postingsReader = new BlockPostingsReader(state.dir,
-                                                                state.fieldInfos,
-                                                                state.segmentInfo,
-                                                                state.context,
-                                                                state.segmentSuffix);
-    boolean success = false;
-    try {
-      FieldsProducer ret = new BlockTreeTermsReader(state.dir,
-                                                    state.fieldInfos,
-                                                    state.segmentInfo,
-                                                    postingsReader,
-                                                    state.context,
-                                                    state.segmentSuffix,
-                                                    state.termsIndexDivisor);
-      success = true;
-      return ret;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(postingsReader);
-      }
-    }
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/block/BlockPostingsReader.java b/lucene/core/src/java/org/apache/lucene/codecs/block/BlockPostingsReader.java
deleted file mode 100644
index 100fae2..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/block/BlockPostingsReader.java
+++ /dev/null
@@ -1,1507 +0,0 @@
-package org.apache.lucene.codecs.block;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import static org.apache.lucene.codecs.block.BlockPostingsFormat.BLOCK_SIZE;
-import static org.apache.lucene.codecs.block.ForUtil.MAX_DATA_SIZE;
-import static org.apache.lucene.codecs.block.ForUtil.MAX_ENCODED_SIZE;
-
-import java.io.IOException;
-import java.util.Arrays;
-
-import org.apache.lucene.codecs.BlockTermState;
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.PostingsReaderBase;
-import org.apache.lucene.index.DocsAndPositionsEnum;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.TermState;
-import org.apache.lucene.store.ByteArrayDataInput;
-import org.apache.lucene.store.DataInput;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-
-/**
- * Concrete class that reads docId(maybe frq,pos,offset,payloads) list
- * with postings format.
- *
- * @see BlockSkipReader for details
- *
- */
-final class BlockPostingsReader extends PostingsReaderBase {
-
-  private final IndexInput docIn;
-  private final IndexInput posIn;
-  private final IndexInput payIn;
-
-  private final ForUtil forUtil;
-
-  // public static boolean DEBUG = false;
-
-  public BlockPostingsReader(Directory dir, FieldInfos fieldInfos, SegmentInfo segmentInfo, IOContext ioContext, String segmentSuffix) throws IOException {
-    boolean success = false;
-    IndexInput docIn = null;
-    IndexInput posIn = null;
-    IndexInput payIn = null;
-    try {
-      docIn = dir.openInput(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, BlockPostingsFormat.DOC_EXTENSION),
-                            ioContext);
-      CodecUtil.checkHeader(docIn,
-                            BlockPostingsWriter.DOC_CODEC,
-                            BlockPostingsWriter.VERSION_START,
-                            BlockPostingsWriter.VERSION_START);
-      forUtil = new ForUtil(docIn);
-
-      if (fieldInfos.hasProx()) {
-        posIn = dir.openInput(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, BlockPostingsFormat.POS_EXTENSION),
-                              ioContext);
-        CodecUtil.checkHeader(posIn,
-                              BlockPostingsWriter.POS_CODEC,
-                              BlockPostingsWriter.VERSION_START,
-                              BlockPostingsWriter.VERSION_START);
-
-        if (fieldInfos.hasPayloads() || fieldInfos.hasOffsets()) {
-          payIn = dir.openInput(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, BlockPostingsFormat.PAY_EXTENSION),
-                                ioContext);
-          CodecUtil.checkHeader(payIn,
-                                BlockPostingsWriter.PAY_CODEC,
-                                BlockPostingsWriter.VERSION_START,
-                                BlockPostingsWriter.VERSION_START);
-        }
-      }
-
-      this.docIn = docIn;
-      this.posIn = posIn;
-      this.payIn = payIn;
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(docIn, posIn, payIn);
-      }
-    }
-  }
-
-  @Override
-  public void init(IndexInput termsIn) throws IOException {
-    // Make sure we are talking to the matching postings writer
-    CodecUtil.checkHeader(termsIn,
-                          BlockPostingsWriter.TERMS_CODEC,
-                          BlockPostingsWriter.VERSION_START,
-                          BlockPostingsWriter.VERSION_START);
-    final int indexBlockSize = termsIn.readVInt();
-    if (indexBlockSize != BLOCK_SIZE) {
-      throw new IllegalStateException("index-time BLOCK_SIZE (" + indexBlockSize + ") != read-time BLOCK_SIZE (" + BLOCK_SIZE + ")");
-    }
-  }
-
-  /**
-   * Read values that have been written using variable-length encoding instead of bit-packing.
-   */
-  private static void readVIntBlock(IndexInput docIn, int[] docBuffer,
-      int[] freqBuffer, int num, boolean indexHasFreq) throws IOException {
-    if (indexHasFreq) {
-      for(int i=0;i<num;i++) {
-        final int code = docIn.readVInt();
-        docBuffer[i] = code >>> 1;
-        if ((code & 1) != 0) {
-          freqBuffer[i] = 1;
-        } else {
-          freqBuffer[i] = docIn.readVInt();
-        }
-      }
-    } else {
-      for(int i=0;i<num;i++) {
-        docBuffer[i] = docIn.readVInt();
-      }
-    }
-  }
-
-  // Must keep final because we do non-standard clone
-  private final static class IntBlockTermState extends BlockTermState {
-    long docStartFP;
-    long posStartFP;
-    long payStartFP;
-    int skipOffset;
-    int lastPosBlockOffset;
-
-    // Only used by the "primary" TermState -- clones don't
-    // copy this (basically they are "transient"):
-    ByteArrayDataInput bytesReader;  // TODO: should this NOT be in the TermState...?
-    byte[] bytes;
-
-    @Override
-    public IntBlockTermState clone() {
-      IntBlockTermState other = new IntBlockTermState();
-      other.copyFrom(this);
-      return other;
-    }
-
-    @Override
-    public void copyFrom(TermState _other) {
-      super.copyFrom(_other);
-      IntBlockTermState other = (IntBlockTermState) _other;
-      docStartFP = other.docStartFP;
-      posStartFP = other.posStartFP;
-      payStartFP = other.payStartFP;
-      lastPosBlockOffset = other.lastPosBlockOffset;
-      skipOffset = other.skipOffset;
-
-      // Do not copy bytes, bytesReader (else TermState is
-      // very heavy, ie drags around the entire block's
-      // byte[]).  On seek back, if next() is in fact used
-      // (rare!), they will be re-read from disk.
-    }
-
-    @Override
-    public String toString() {
-      return super.toString() + " docStartFP=" + docStartFP + " posStartFP=" + posStartFP + " payStartFP=" + payStartFP + " lastPosBlockOffset=" + lastPosBlockOffset;
-    }
-  }
-
-  @Override
-  public IntBlockTermState newTermState() {
-    return new IntBlockTermState();
-  }
-
-  @Override
-  public void close() throws IOException {
-    IOUtils.close(docIn, posIn, payIn);
-  }
-
-  /* Reads but does not decode the byte[] blob holding
-     metadata for the current terms block */
-  @Override
-  public void readTermsBlock(IndexInput termsIn, FieldInfo fieldInfo, BlockTermState _termState) throws IOException {
-    final IntBlockTermState termState = (IntBlockTermState) _termState;
-
-    final int numBytes = termsIn.readVInt();
-
-    if (termState.bytes == null) {
-      termState.bytes = new byte[ArrayUtil.oversize(numBytes, 1)];
-      termState.bytesReader = new ByteArrayDataInput();
-    } else if (termState.bytes.length < numBytes) {
-      termState.bytes = new byte[ArrayUtil.oversize(numBytes, 1)];
-    }
-
-    termsIn.readBytes(termState.bytes, 0, numBytes);
-    termState.bytesReader.reset(termState.bytes, 0, numBytes);
-  }
-
-  @Override
-  public void nextTerm(FieldInfo fieldInfo, BlockTermState _termState)
-    throws IOException {
-    final IntBlockTermState termState = (IntBlockTermState) _termState;
-    final boolean isFirstTerm = termState.termBlockOrd == 0;
-    final boolean fieldHasPositions = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;
-    final boolean fieldHasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
-    final boolean fieldHasPayloads = fieldInfo.hasPayloads();
-
-    final DataInput in = termState.bytesReader;
-    if (isFirstTerm) {
-      termState.docStartFP = in.readVLong();
-      if (fieldHasPositions) {
-        termState.posStartFP = in.readVLong();
-        if (termState.totalTermFreq > BLOCK_SIZE) {
-          termState.lastPosBlockOffset = in.readVInt();
-        } else {
-          termState.lastPosBlockOffset = -1;
-        }
-        if ((fieldHasPayloads || fieldHasOffsets) && termState.totalTermFreq >= BLOCK_SIZE) {
-          termState.payStartFP = in.readVLong();
-        } else {
-          termState.payStartFP = -1;
-        }
-      }
-    } else {
-      termState.docStartFP += in.readVLong();
-      if (fieldHasPositions) {
-        termState.posStartFP += in.readVLong();
-        if (termState.totalTermFreq > BLOCK_SIZE) {
-          termState.lastPosBlockOffset = in.readVInt();
-        } else {
-          termState.lastPosBlockOffset = -1;
-        }
-        if ((fieldHasPayloads || fieldHasOffsets) && termState.totalTermFreq >= BLOCK_SIZE) {
-          long delta = in.readVLong();
-          if (termState.payStartFP == -1) {
-            termState.payStartFP = delta;
-          } else {
-            termState.payStartFP += delta;
-          }
-        }
-      }
-    }
-
-    if (termState.docFreq > BLOCK_SIZE) {
-      termState.skipOffset = in.readVInt();
-    } else {
-      termState.skipOffset = -1;
-    }
-  }
-    
-  @Override
-  public DocsEnum docs(FieldInfo fieldInfo, BlockTermState termState, Bits liveDocs, DocsEnum reuse, int flags) throws IOException {
-    BlockDocsEnum docsEnum;
-    if (reuse instanceof BlockDocsEnum) {
-      docsEnum = (BlockDocsEnum) reuse;
-      if (!docsEnum.canReuse(docIn, fieldInfo)) {
-        docsEnum = new BlockDocsEnum(fieldInfo);
-      }
-    } else {
-      docsEnum = new BlockDocsEnum(fieldInfo);
-    }
-    return docsEnum.reset(liveDocs, (IntBlockTermState) termState);
-  }
-
-  // TODO: specialize to liveDocs vs not, and freqs vs not
-  
-  @Override
-  public DocsAndPositionsEnum docsAndPositions(FieldInfo fieldInfo, BlockTermState termState, Bits liveDocs,
-                                               DocsAndPositionsEnum reuse, int flags)
-    throws IOException {
-
-    boolean indexHasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
-    boolean indexHasPayloads = fieldInfo.hasPayloads();
-
-    if ((!indexHasOffsets || (flags & DocsAndPositionsEnum.FLAG_OFFSETS) == 0) &&
-        (!indexHasPayloads || (flags & DocsAndPositionsEnum.FLAG_PAYLOADS) == 0)) {
-      BlockDocsAndPositionsEnum docsAndPositionsEnum;
-      if (reuse instanceof BlockDocsAndPositionsEnum) {
-        docsAndPositionsEnum = (BlockDocsAndPositionsEnum) reuse;
-        if (!docsAndPositionsEnum.canReuse(docIn, fieldInfo)) {
-          docsAndPositionsEnum = new BlockDocsAndPositionsEnum(fieldInfo);
-        }
-      } else {
-        docsAndPositionsEnum = new BlockDocsAndPositionsEnum(fieldInfo);
-      }
-      return docsAndPositionsEnum.reset(liveDocs, (IntBlockTermState) termState);
-    } else {
-      EverythingEnum everythingEnum;
-      if (reuse instanceof EverythingEnum) {
-        everythingEnum = (EverythingEnum) reuse;
-        if (!everythingEnum.canReuse(docIn, fieldInfo)) {
-          everythingEnum = new EverythingEnum(fieldInfo);
-        }
-      } else {
-        everythingEnum = new EverythingEnum(fieldInfo);
-      }
-      return everythingEnum.reset(liveDocs, (IntBlockTermState) termState);
-    }
-  }
-
-  final class BlockDocsEnum extends DocsEnum {
-    private final byte[] encoded;
-    
-    private final int[] docDeltaBuffer = new int[MAX_DATA_SIZE];
-    private final int[] freqBuffer = new int[MAX_DATA_SIZE];
-
-    private int docBufferUpto;
-
-    private BlockSkipReader skipper;
-    private boolean skipped;
-
-    final IndexInput startDocIn;
-
-    final IndexInput docIn;
-    final boolean indexHasFreq;
-    final boolean indexHasPos;
-    final boolean indexHasOffsets;
-    final boolean indexHasPayloads;
-
-    private int docFreq;                              // number of docs in this posting list
-    private int docUpto;                              // how many docs we've read
-    private int doc;                                  // doc we last read
-    private int accum;                                // accumulator for doc deltas
-    private int freq;                                 // freq we last read
-
-    // Where this term's postings start in the .doc file:
-    private long docTermStartFP;
-
-    // Where this term's skip data starts (after
-    // docTermStartFP) in the .doc file (or -1 if there is
-    // no skip data for this term):
-    private int skipOffset;
-
-    // docID for next skip point, we won't use skipper if 
-    // target docID is not larger than this
-    private int nextSkipDoc;
-
-    private Bits liveDocs;
-
-    public BlockDocsEnum(FieldInfo fieldInfo) throws IOException {
-      this.startDocIn = BlockPostingsReader.this.docIn;
-      this.docIn = startDocIn.clone();
-      indexHasFreq = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;
-      indexHasPos = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;
-      indexHasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
-      indexHasPayloads = fieldInfo.hasPayloads();
-      encoded = new byte[MAX_ENCODED_SIZE];    
-    }
-
-    public boolean canReuse(IndexInput docIn, FieldInfo fieldInfo) {
-      return docIn == startDocIn &&
-        indexHasFreq == (fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) >= 0) &&
-        indexHasPos == (fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0) &&
-        indexHasPayloads == fieldInfo.hasPayloads();
-    }
-    
-    public DocsEnum reset(Bits liveDocs, IntBlockTermState termState) throws IOException {
-      this.liveDocs = liveDocs;
-      // if (DEBUG) {
-      //   System.out.println("  FPR.reset: termState=" + termState);
-      // }
-      docFreq = termState.docFreq;
-      docTermStartFP = termState.docStartFP;
-      docIn.seek(docTermStartFP);
-      skipOffset = termState.skipOffset;
-
-      doc = -1;
-      if (!indexHasFreq) {
-        Arrays.fill(freqBuffer, 1);
-      }
-      accum = 0;
-      docUpto = 0;
-      nextSkipDoc = BLOCK_SIZE - 1; // we won't skip if target is found in first block
-      docBufferUpto = BLOCK_SIZE;
-      skipped = false;
-      return this;
-    }
-    
-    @Override
-    public int freq() throws IOException {
-      return freq;
-    }
-
-    @Override
-    public int docID() {
-      return doc;
-    }
-    
-    private void refillDocs() throws IOException {
-      final int left = docFreq - docUpto;
-      assert left > 0;
-
-      if (left >= BLOCK_SIZE) {
-        // if (DEBUG) {
-        //   System.out.println("    fill doc block from fp=" + docIn.getFilePointer());
-        // }
-        forUtil.readBlock(docIn, encoded, docDeltaBuffer);
-
-        if (indexHasFreq) {
-          // if (DEBUG) {
-          //   System.out.println("    fill freq block from fp=" + docIn.getFilePointer());
-          // }
-          forUtil.readBlock(docIn, encoded, freqBuffer);
-        }
-      } else {
-        // Read vInts:
-        // if (DEBUG) {
-        //   System.out.println("    fill last vInt block from fp=" + docIn.getFilePointer());
-        // }
-        readVIntBlock(docIn, docDeltaBuffer, freqBuffer, left, indexHasFreq);
-      }
-      docBufferUpto = 0;
-    }
-
-    @Override
-    public int nextDoc() throws IOException {
-      // if (DEBUG) {
-      //   System.out.println("\nFPR.nextDoc");
-      // }
-      while (true) {
-        // if (DEBUG) {
-        //   System.out.println("  docUpto=" + docUpto + " (of df=" + docFreq + ") docBufferUpto=" + docBufferUpto);
-        // }
-
-        if (docUpto == docFreq) {
-          // if (DEBUG) {
-          //   System.out.println("  return doc=END");
-          // }
-          return doc = NO_MORE_DOCS;
-        }
-        if (docBufferUpto == BLOCK_SIZE) {
-          refillDocs();
-        }
-
-        // if (DEBUG) {
-        //   System.out.println("    accum=" + accum + " docDeltaBuffer[" + docBufferUpto + "]=" + docDeltaBuffer[docBufferUpto]);
-        // }
-        accum += docDeltaBuffer[docBufferUpto];
-        docUpto++;
-
-        if (liveDocs == null || liveDocs.get(accum)) {
-          doc = accum;
-          freq = freqBuffer[docBufferUpto];
-          docBufferUpto++;
-          // if (DEBUG) {
-          //   System.out.println("  return doc=" + doc + " freq=" + freq);
-          // }
-          return doc;
-        }
-        // if (DEBUG) {
-        //   System.out.println("  doc=" + accum + " is deleted; try next doc");
-        // }
-        docBufferUpto++;
-      }
-    }
-
-    @Override
-    public int advance(int target) throws IOException {
-      // TODO: make frq block load lazy/skippable
-      // if (DEBUG) {
-      //   System.out.println("  FPR.advance target=" + target);
-      // }
-
-      // current skip docID < docIDs generated from current buffer <= next skip docID
-      // we don't need to skip if target is buffered already
-      if (docFreq > BLOCK_SIZE && target > nextSkipDoc) {
-
-        // if (DEBUG) {
-        //   System.out.println("load skipper");
-        // }
-
-        if (skipper == null) {
-          // Lazy init: first time this enum has ever been used for skipping
-          skipper = new BlockSkipReader(docIn.clone(),
-                                        BlockPostingsWriter.maxSkipLevels,
-                                        BLOCK_SIZE,
-                                        indexHasPos,
-                                        indexHasOffsets,
-                                        indexHasPayloads);
-        }
-
-        if (!skipped) {
-          assert skipOffset != -1;
-          // This is the first time this enum has skipped
-          // since reset() was called; load the skip data:
-          skipper.init(docTermStartFP+skipOffset, docTermStartFP, 0, 0, docFreq);
-          skipped = true;
-        }
-
-        // always plus one to fix the result, since skip position in BlockSkipReader 
-        // is a little different from MultiLevelSkipListReader
-        final int newDocUpto = skipper.skipTo(target) + 1; 
-
-        if (newDocUpto > docUpto) {
-          // Skipper moved
-          // if (DEBUG) {
-          //   System.out.println("skipper moved to docUpto=" + newDocUpto + " vs current=" + docUpto + "; docID=" + skipper.getDoc() + " fp=" + skipper.getDocPointer());
-          // }
-          assert newDocUpto % BLOCK_SIZE == 0 : "got " + newDocUpto;
-          docUpto = newDocUpto;
-
-          // Force to read next block
-          docBufferUpto = BLOCK_SIZE;
-          accum = skipper.getDoc();               // actually, this is just lastSkipEntry
-          docIn.seek(skipper.getDocPointer());    // now point to the block we want to search
-        }
-        // next time we call advance, this is used to 
-        // foresee whether skipper is necessary.
-        nextSkipDoc = skipper.getNextSkipDoc();
-      }
-      if (docUpto == docFreq) {
-        return doc = NO_MORE_DOCS;
-      }
-      if (docBufferUpto == BLOCK_SIZE) {
-        refillDocs();
-      }
-
-      // Now scan... this is an inlined/pared down version
-      // of nextDoc():
-      while (true) {
-        // if (DEBUG) {
-        //   System.out.println("  scan doc=" + accum + " docBufferUpto=" + docBufferUpto);
-        // }
-        accum += docDeltaBuffer[docBufferUpto];
-        docUpto++;
-
-        if (accum >= target) {
-          break;
-        }
-        docBufferUpto++;
-        if (docUpto == docFreq) {
-          return doc = NO_MORE_DOCS;
-        }
-      }
-
-      if (liveDocs == null || liveDocs.get(accum)) {
-        // if (DEBUG) {
-        //   System.out.println("  return doc=" + accum);
-        // }
-        freq = freqBuffer[docBufferUpto];
-        docBufferUpto++;
-        return doc = accum;
-      } else {
-        // if (DEBUG) {
-        //   System.out.println("  now do nextDoc()");
-        // }
-        docBufferUpto++;
-        return nextDoc();
-      }
-    }
-  }
-
-
-  final class BlockDocsAndPositionsEnum extends DocsAndPositionsEnum {
-    
-    private final byte[] encoded;
-
-    private final int[] docDeltaBuffer = new int[MAX_DATA_SIZE];
-    private final int[] freqBuffer = new int[MAX_DATA_SIZE];
-    private final int[] posDeltaBuffer = new int[MAX_DATA_SIZE];
-
-    private int docBufferUpto;
-    private int posBufferUpto;
-
-    private BlockSkipReader skipper;
-    private boolean skipped;
-
-    final IndexInput startDocIn;
-
-    final IndexInput docIn;
-    final IndexInput posIn;
-
-    final boolean indexHasOffsets;
-    final boolean indexHasPayloads;
-
-    private int docFreq;                              // number of docs in this posting list
-    private int docUpto;                              // how many docs we've read
-    private int doc;                                  // doc we last read
-    private int accum;                                // accumulator for doc deltas
-    private int freq;                                 // freq we last read
-    private int position;                             // current position
-
-    // how many positions "behind" we are; nextPosition must
-    // skip these to "catch up":
-    private int posPendingCount;
-
-    // Lazy pos seek: if != -1 then we must seek to this FP
-    // before reading positions:
-    private long posPendingFP;
-
-    // Where this term's postings start in the .doc file:
-    private long docTermStartFP;
-
-    // Where this term's postings start in the .pos file:
-    private long posTermStartFP;
-
-    // Where this term's payloads/offsets start in the .pay
-    // file:
-    private long payTermStartFP;
-
-    // File pointer where the last (vInt encoded) pos delta
-    // block is.  We need this to know whether to bulk
-    // decode vs vInt decode the block:
-    private long lastPosBlockFP;
-
-    // Where this term's skip data starts (after
-    // docTermStartFP) in the .doc file (or -1 if there is
-    // no skip data for this term):
-    private int skipOffset;
-
-    private int nextSkipDoc;
-
-    private Bits liveDocs;
-    
-    public BlockDocsAndPositionsEnum(FieldInfo fieldInfo) throws IOException {
-      this.startDocIn = BlockPostingsReader.this.docIn;
-      this.docIn = startDocIn.clone();
-      this.posIn = BlockPostingsReader.this.posIn.clone();
-      encoded = new byte[MAX_ENCODED_SIZE];
-      indexHasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
-      indexHasPayloads = fieldInfo.hasPayloads();
-    }
-
-    public boolean canReuse(IndexInput docIn, FieldInfo fieldInfo) {
-      return docIn == startDocIn &&
-        indexHasOffsets == (fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0) &&
-        indexHasPayloads == fieldInfo.hasPayloads();
-    }
-    
-    public DocsAndPositionsEnum reset(Bits liveDocs, IntBlockTermState termState) throws IOException {
-      this.liveDocs = liveDocs;
-      // if (DEBUG) {
-      //   System.out.println("  FPR.reset: termState=" + termState);
-      // }
-      docFreq = termState.docFreq;
-      docTermStartFP = termState.docStartFP;
-      posTermStartFP = termState.posStartFP;
-      payTermStartFP = termState.payStartFP;
-      docIn.seek(docTermStartFP);
-      skipOffset = termState.skipOffset;
-      posPendingFP = posTermStartFP;
-      posPendingCount = 0;
-      if (termState.totalTermFreq < BLOCK_SIZE) {
-        lastPosBlockFP = posTermStartFP;
-      } else if (termState.totalTermFreq == BLOCK_SIZE) {
-        lastPosBlockFP = -1;
-      } else {
-        lastPosBlockFP = posTermStartFP + termState.lastPosBlockOffset;
-      }
-
-      doc = -1;
-      accum = 0;
-      docUpto = 0;
-      nextSkipDoc = BLOCK_SIZE - 1;
-      docBufferUpto = BLOCK_SIZE;
-      skipped = false;
-      return this;
-    }
-    
-    @Override
-    public int freq() throws IOException {
-      return freq;
-    }
-
-    @Override
-    public int docID() {
-      return doc;
-    }
-
-    private void refillDocs() throws IOException {
-      final int left = docFreq - docUpto;
-      assert left > 0;
-
-      if (left >= BLOCK_SIZE) {
-        // if (DEBUG) {
-        //   System.out.println("    fill doc block from fp=" + docIn.getFilePointer());
-        // }
-        forUtil.readBlock(docIn, encoded, docDeltaBuffer);
-        // if (DEBUG) {
-        //   System.out.println("    fill freq block from fp=" + docIn.getFilePointer());
-        // }
-        forUtil.readBlock(docIn, encoded, freqBuffer);
-      } else {
-        // Read vInts:
-        // if (DEBUG) {
-        //   System.out.println("    fill last vInt doc block from fp=" + docIn.getFilePointer());
-        // }
-        readVIntBlock(docIn, docDeltaBuffer, freqBuffer, left, true);
-      }
-      docBufferUpto = 0;
-    }
-    
-    private void refillPositions() throws IOException {
-      // if (DEBUG) {
-      //   System.out.println("      refillPositions");
-      // }
-      if (posIn.getFilePointer() == lastPosBlockFP) {
-        // if (DEBUG) {
-        //   System.out.println("        vInt pos block @ fp=" + posIn.getFilePointer() + " hasPayloads=" + indexHasPayloads + " hasOffsets=" + indexHasOffsets);
-        // }
-        final int count = posIn.readVInt();
-        int payloadLength = 0;
-        for(int i=0;i<count;i++) {
-          int code = posIn.readVInt();
-          if (indexHasPayloads) {
-            if ((code & 1) != 0) {
-              payloadLength = posIn.readVInt();
-            }
-            posDeltaBuffer[i] = code >>> 1;
-            if (payloadLength != 0) {
-              posIn.seek(posIn.getFilePointer() + payloadLength);
-            }
-          } else {
-            posDeltaBuffer[i] = code;
-          }
-          if (indexHasOffsets) {
-            posIn.readVInt();
-            posIn.readVInt();
-          }
-        }
-      } else {
-        // if (DEBUG) {
-        //   System.out.println("        bulk pos block @ fp=" + posIn.getFilePointer());
-        // }
-        forUtil.readBlock(posIn, encoded, posDeltaBuffer);
-      }
-    }
-
-    @Override
-    public int nextDoc() throws IOException {
-      // if (DEBUG) {
-      //   System.out.println("  FPR.nextDoc");
-      // }
-      while (true) {
-        // if (DEBUG) {
-        //   System.out.println("    docUpto=" + docUpto + " (of df=" + docFreq + ") docBufferUpto=" + docBufferUpto);
-        // }
-        if (docUpto == docFreq) {
-          return doc = NO_MORE_DOCS;
-        }
-        if (docBufferUpto == BLOCK_SIZE) {
-          refillDocs();
-        }
-        // if (DEBUG) {
-        //   System.out.println("    accum=" + accum + " docDeltaBuffer[" + docBufferUpto + "]=" + docDeltaBuffer[docBufferUpto]);
-        // }
-        accum += docDeltaBuffer[docBufferUpto];
-        freq = freqBuffer[docBufferUpto];
-        posPendingCount += freq;
-        docBufferUpto++;
-        docUpto++;
-
-        if (liveDocs == null || liveDocs.get(accum)) {
-          doc = accum;
-          position = 0;
-          // if (DEBUG) {
-          //   System.out.println("    return doc=" + doc + " freq=" + freq + " posPendingCount=" + posPendingCount);
-          // }
-          return doc;
-        }
-        // if (DEBUG) {
-        //   System.out.println("    doc=" + accum + " is deleted; try next doc");
-        // }
-      }
-    }
-    
-    @Override
-    public int advance(int target) throws IOException {
-      // TODO: make frq block load lazy/skippable
-      // if (DEBUG) {
-      //   System.out.println("  FPR.advance target=" + target);
-      // }
-
-      if (docFreq > BLOCK_SIZE && target > nextSkipDoc) {
-        // if (DEBUG) {
-        //   System.out.println("    try skipper");
-        // }
-        if (skipper == null) {
-          // Lazy init: first time this enum has ever been used for skipping
-          // if (DEBUG) {
-          //   System.out.println("    create skipper");
-          // }
-          skipper = new BlockSkipReader(docIn.clone(),
-                                        BlockPostingsWriter.maxSkipLevels,
-                                        BLOCK_SIZE,
-                                        true,
-                                        indexHasOffsets,
-                                        indexHasPayloads);
-        }
-
-        if (!skipped) {
-          assert skipOffset != -1;
-          // This is the first time this enum has skipped
-          // since reset() was called; load the skip data:
-          // if (DEBUG) {
-          //   System.out.println("    init skipper");
-          // }
-          skipper.init(docTermStartFP+skipOffset, docTermStartFP, posTermStartFP, payTermStartFP, docFreq);
-          skipped = true;
-        }
-
-        final int newDocUpto = skipper.skipTo(target) + 1; 
-
-        if (newDocUpto > docUpto) {
-          // Skipper moved
-          // if (DEBUG) {
-          //   System.out.println("    skipper moved to docUpto=" + newDocUpto + " vs current=" + docUpto + "; docID=" + skipper.getDoc() + " fp=" + skipper.getDocPointer() + " pos.fp=" + skipper.getPosPointer() + " pos.bufferUpto=" + skipper.getPosBufferUpto());
-          // }
-
-          assert newDocUpto % BLOCK_SIZE == 0 : "got " + newDocUpto;
-          docUpto = newDocUpto;
-
-          // Force to read next block
-          docBufferUpto = BLOCK_SIZE;
-          accum = skipper.getDoc();
-          docIn.seek(skipper.getDocPointer());
-          posPendingFP = skipper.getPosPointer();
-          posPendingCount = skipper.getPosBufferUpto();
-        }
-        nextSkipDoc = skipper.getNextSkipDoc();
-      }
-      if (docUpto == docFreq) {
-        return doc = NO_MORE_DOCS;
-      }
-      if (docBufferUpto == BLOCK_SIZE) {
-        refillDocs();
-      }
-
-      // Now scan... this is an inlined/pared down version
-      // of nextDoc():
-      while (true) {
-        // if (DEBUG) {
-        //   System.out.println("  scan doc=" + accum + " docBufferUpto=" + docBufferUpto);
-        // }
-        if (docUpto == docFreq) {
-          return doc = NO_MORE_DOCS;
-        }
-        accum += docDeltaBuffer[docBufferUpto];
-        freq = freqBuffer[docBufferUpto];
-        posPendingCount += freq;
-        docBufferUpto++;
-        docUpto++;
-
-        if (accum >= target) {
-          break;
-        }
-        if (docUpto == docFreq) {
-          return doc = NO_MORE_DOCS;
-        }
-      }
-
-      if (liveDocs == null || liveDocs.get(accum)) {
-        // if (DEBUG) {
-        //   System.out.println("  return doc=" + accum);
-        // }
-        position = 0;
-        return doc = accum;
-      } else {
-        // if (DEBUG) {
-        //   System.out.println("  now do nextDoc()");
-        // }
-        return nextDoc();
-      }
-    }
-
-    // TODO: in theory we could avoid loading frq block
-    // when not needed, ie, use skip data to load how far to
-    // seek the pos pointer ... instead of having to load frq
-    // blocks only to sum up how many positions to skip
-    private void skipPositions() throws IOException {
-      // Skip positions now:
-      int toSkip = posPendingCount - freq;
-      // if (DEBUG) {
-      //   System.out.println("      FPR.skipPositions: toSkip=" + toSkip);
-      // }
-
-      final int leftInBlock = BLOCK_SIZE - posBufferUpto;
-      if (toSkip < leftInBlock) {
-        posBufferUpto += toSkip;
-        // if (DEBUG) {
-        //   System.out.println("        skip w/in block to posBufferUpto=" + posBufferUpto);
-        // }
-      } else {
-        toSkip -= leftInBlock;
-        while(toSkip >= BLOCK_SIZE) {
-          // if (DEBUG) {
-          //   System.out.println("        skip whole block @ fp=" + posIn.getFilePointer());
-          // }
-          assert posIn.getFilePointer() != lastPosBlockFP;
-          forUtil.skipBlock(posIn);
-          toSkip -= BLOCK_SIZE;
-        }
-        refillPositions();
-        posBufferUpto = toSkip;
-        // if (DEBUG) {
-        //   System.out.println("        skip w/in block to posBufferUpto=" + posBufferUpto);
-        // }
-      }
-
-      position = 0;
-    }
-
-    @Override
-    public int nextPosition() throws IOException {
-      // if (DEBUG) {
-      //   System.out.println("    FPR.nextPosition posPendingCount=" + posPendingCount + " posBufferUpto=" + posBufferUpto);
-      // }
-      if (posPendingFP != -1) {
-        // if (DEBUG) {
-        //   System.out.println("      seek to pendingFP=" + posPendingFP);
-        // }
-        posIn.seek(posPendingFP);
-        posPendingFP = -1;
-
-        // Force buffer refill:
-        posBufferUpto = BLOCK_SIZE;
-      }
-
-      if (posPendingCount > freq) {
-        skipPositions();
-        posPendingCount = freq;
-      }
-
-      if (posBufferUpto == BLOCK_SIZE) {
-        refillPositions();
-        posBufferUpto = 0;
-      }
-      position += posDeltaBuffer[posBufferUpto++];
-      posPendingCount--;
-      // if (DEBUG) {
-      //   System.out.println("      return pos=" + position);
-      // }
-      return position;
-    }
-
-    @Override
-    public int startOffset() {
-      return -1;
-    }
-  
-    @Override
-    public int endOffset() {
-      return -1;
-    }
-  
-    @Override
-    public BytesRef getPayload() {
-      return null;
-    }
-  }
-
-  // Also handles payloads + offsets
-  final class EverythingEnum extends DocsAndPositionsEnum {
-    
-    private final byte[] encoded;
-
-    private final int[] docDeltaBuffer = new int[MAX_DATA_SIZE];
-    private final int[] freqBuffer = new int[MAX_DATA_SIZE];
-    private final int[] posDeltaBuffer = new int[MAX_DATA_SIZE];
-
-    private final int[] payloadLengthBuffer;
-    private final int[] offsetStartDeltaBuffer;
-    private final int[] offsetLengthBuffer;
-
-    private byte[] payloadBytes;
-    private int payloadByteUpto;
-    private int payloadLength;
-
-    private int lastStartOffset;
-    private int startOffset;
-    private int endOffset;
-
-    private int docBufferUpto;
-    private int posBufferUpto;
-
-    private BlockSkipReader skipper;
-    private boolean skipped;
-
-    final IndexInput startDocIn;
-
-    final IndexInput docIn;
-    final IndexInput posIn;
-    final IndexInput payIn;
-    final BytesRef payload;
-
-    final boolean indexHasOffsets;
-    final boolean indexHasPayloads;
-
-    private int docFreq;                              // number of docs in this posting list
-    private int docUpto;                              // how many docs we've read
-    private int doc;                                  // doc we last read
-    private int accum;                                // accumulator for doc deltas
-    private int freq;                                 // freq we last read
-    private int position;                             // current position
-
-    // how many positions "behind" we are; nextPosition must
-    // skip these to "catch up":
-    private int posPendingCount;
-
-    // Lazy pos seek: if != -1 then we must seek to this FP
-    // before reading positions:
-    private long posPendingFP;
-
-    // Lazy pay seek: if != -1 then we must seek to this FP
-    // before reading payloads/offsets:
-    private long payPendingFP;
-
-    // Where this term's postings start in the .doc file:
-    private long docTermStartFP;
-
-    // Where this term's postings start in the .pos file:
-    private long posTermStartFP;
-
-    // Where this term's payloads/offsets start in the .pay
-    // file:
-    private long payTermStartFP;
-
-    // File pointer where the last (vInt encoded) pos delta
-    // block is.  We need this to know whether to bulk
-    // decode vs vInt decode the block:
-    private long lastPosBlockFP;
-
-    // Where this term's skip data starts (after
-    // docTermStartFP) in the .doc file (or -1 if there is
-    // no skip data for this term):
-    private int skipOffset;
-
-    private int nextSkipDoc;
-
-    private Bits liveDocs;
-    
-    public EverythingEnum(FieldInfo fieldInfo) throws IOException {
-      this.startDocIn = BlockPostingsReader.this.docIn;
-      this.docIn = startDocIn.clone();
-      this.posIn = BlockPostingsReader.this.posIn.clone();
-      this.payIn = BlockPostingsReader.this.payIn.clone();
-      encoded = new byte[MAX_ENCODED_SIZE];
-      indexHasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
-      if (indexHasOffsets) {
-        offsetStartDeltaBuffer = new int[MAX_DATA_SIZE];
-        offsetLengthBuffer = new int[MAX_DATA_SIZE];
-      } else {
-        offsetStartDeltaBuffer = null;
-        offsetLengthBuffer = null;
-        startOffset = -1;
-        endOffset = -1;
-      }
-
-      indexHasPayloads = fieldInfo.hasPayloads();
-      if (indexHasPayloads) {
-        payloadLengthBuffer = new int[MAX_DATA_SIZE];
-        payloadBytes = new byte[128];
-        payload = new BytesRef();
-      } else {
-        payloadLengthBuffer = null;
-        payloadBytes = null;
-        payload = null;
-      }
-    }
-
-    public boolean canReuse(IndexInput docIn, FieldInfo fieldInfo) {
-      return docIn == startDocIn &&
-        indexHasOffsets == (fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0) &&
-        indexHasPayloads == fieldInfo.hasPayloads();
-    }
-    
-    public EverythingEnum reset(Bits liveDocs, IntBlockTermState termState) throws IOException {
-      this.liveDocs = liveDocs;
-      // if (DEBUG) {
-      //   System.out.println("  FPR.reset: termState=" + termState);
-      // }
-      docFreq = termState.docFreq;
-      docTermStartFP = termState.docStartFP;
-      posTermStartFP = termState.posStartFP;
-      payTermStartFP = termState.payStartFP;
-      docIn.seek(docTermStartFP);
-      skipOffset = termState.skipOffset;
-      posPendingFP = posTermStartFP;
-      payPendingFP = payTermStartFP;
-      posPendingCount = 0;
-      if (termState.totalTermFreq < BLOCK_SIZE) {
-        lastPosBlockFP = posTermStartFP;
-      } else if (termState.totalTermFreq == BLOCK_SIZE) {
-        lastPosBlockFP = -1;
-      } else {
-        lastPosBlockFP = posTermStartFP + termState.lastPosBlockOffset;
-      }
-
-      doc = -1;
-      accum = 0;
-      docUpto = 0;
-      nextSkipDoc = BLOCK_SIZE - 1;
-      docBufferUpto = BLOCK_SIZE;
-      skipped = false;
-      return this;
-    }
-    
-    @Override
-    public int freq() throws IOException {
-      return freq;
-    }
-
-    @Override
-    public int docID() {
-      return doc;
-    }
-
-    private void refillDocs() throws IOException {
-      final int left = docFreq - docUpto;
-      assert left > 0;
-
-      if (left >= BLOCK_SIZE) {
-        // if (DEBUG) {
-        //   System.out.println("    fill doc block from fp=" + docIn.getFilePointer());
-        // }
-        forUtil.readBlock(docIn, encoded, docDeltaBuffer);
-        // if (DEBUG) {
-        //   System.out.println("    fill freq block from fp=" + docIn.getFilePointer());
-        // }
-        forUtil.readBlock(docIn, encoded, freqBuffer);
-      } else {
-        // if (DEBUG) {
-        //   System.out.println("    fill last vInt doc block from fp=" + docIn.getFilePointer());
-        // }
-        readVIntBlock(docIn, docDeltaBuffer, freqBuffer, left, true);
-      }
-      docBufferUpto = 0;
-    }
-    
-    private void refillPositions() throws IOException {
-      // if (DEBUG) {
-      //   System.out.println("      refillPositions");
-      // }
-      if (posIn.getFilePointer() == lastPosBlockFP) {
-        // if (DEBUG) {
-        //   System.out.println("        vInt pos block @ fp=" + posIn.getFilePointer() + " hasPayloads=" + indexHasPayloads + " hasOffsets=" + indexHasOffsets);
-        // }
-        final int count = posIn.readVInt();
-        int payloadLength = 0;
-        payloadByteUpto = 0;
-        for(int i=0;i<count;i++) {
-          int code = posIn.readVInt();
-          if (indexHasPayloads) {
-            if ((code & 1) != 0) {
-              payloadLength = posIn.readVInt();
-            }
-            // if (DEBUG) {
-            //   System.out.println("        i=" + i + " payloadLen=" + payloadLength);
-            // }
-            payloadLengthBuffer[i] = payloadLength;
-            posDeltaBuffer[i] = code >>> 1;
-            if (payloadLength != 0) {
-              if (payloadByteUpto + payloadLength > payloadBytes.length) {
-                payloadBytes = ArrayUtil.grow(payloadBytes, payloadByteUpto + payloadLength);
-              }
-              //System.out.println("          read payload @ pos.fp=" + posIn.getFilePointer());
-              posIn.readBytes(payloadBytes, payloadByteUpto, payloadLength);
-              payloadByteUpto += payloadLength;
-            }
-          } else {
-            posDeltaBuffer[i] = code;
-          }
-
-          if (indexHasOffsets) {
-            // if (DEBUG) {
-            //   System.out.println("        i=" + i + " read offsets from posIn.fp=" + posIn.getFilePointer());
-            // }
-            offsetStartDeltaBuffer[i] = posIn.readVInt();
-            offsetLengthBuffer[i] = posIn.readVInt();
-            // if (DEBUG) {
-            //   System.out.println("          startOffDelta=" + offsetStartDeltaBuffer[i] + " offsetLen=" + offsetLengthBuffer[i]);
-            // }
-          }
-        }
-        payloadByteUpto = 0;
-      } else {
-        // if (DEBUG) {
-        //   System.out.println("        bulk pos block @ fp=" + posIn.getFilePointer());
-        // }
-        forUtil.readBlock(posIn, encoded, posDeltaBuffer);
-
-        if (indexHasPayloads) {
-          // if (DEBUG) {
-          //   System.out.println("        bulk payload block @ pay.fp=" + payIn.getFilePointer());
-          // }
-          forUtil.readBlock(payIn, encoded, payloadLengthBuffer);
-          int numBytes = payIn.readVInt();
-          // if (DEBUG) {
-          //   System.out.println("        " + numBytes + " payload bytes @ pay.fp=" + payIn.getFilePointer());
-          // }
-          if (numBytes > payloadBytes.length) {
-            payloadBytes = ArrayUtil.grow(payloadBytes, numBytes);
-          }
-          payIn.readBytes(payloadBytes, 0, numBytes);
-          payloadByteUpto = 0;
-        }
-
-        if (indexHasOffsets) {
-          // if (DEBUG) {
-          //   System.out.println("        bulk offset block @ pay.fp=" + payIn.getFilePointer());
-          // }
-          forUtil.readBlock(payIn, encoded, offsetStartDeltaBuffer);
-          forUtil.readBlock(payIn, encoded, offsetLengthBuffer);
-        }
-      }
-    }
-
-    @Override
-    public int nextDoc() throws IOException {
-      // if (DEBUG) {
-      //   System.out.println("  FPR.nextDoc");
-      // }
-      while (true) {
-        // if (DEBUG) {
-        //   System.out.println("    docUpto=" + docUpto + " (of df=" + docFreq + ") docBufferUpto=" + docBufferUpto);
-        // }
-        if (docUpto == docFreq) {
-          return doc = NO_MORE_DOCS;
-        }
-        if (docBufferUpto == BLOCK_SIZE) {
-          refillDocs();
-        }
-        // if (DEBUG) {
-        //   System.out.println("    accum=" + accum + " docDeltaBuffer[" + docBufferUpto + "]=" + docDeltaBuffer[docBufferUpto]);
-        // }
-        accum += docDeltaBuffer[docBufferUpto];
-        freq = freqBuffer[docBufferUpto];
-        posPendingCount += freq;
-        docBufferUpto++;
-        docUpto++;
-
-        if (liveDocs == null || liveDocs.get(accum)) {
-          doc = accum;
-          // if (DEBUG) {
-          //   System.out.println("    return doc=" + doc + " freq=" + freq + " posPendingCount=" + posPendingCount);
-          // }
-          position = 0;
-          lastStartOffset = 0;
-          return doc;
-        }
-
-        // if (DEBUG) {
-        //   System.out.println("    doc=" + accum + " is deleted; try next doc");
-        // }
-      }
-    }
-    
-    @Override
-    public int advance(int target) throws IOException {
-      // TODO: make frq block load lazy/skippable
-      // if (DEBUG) {
-      //   System.out.println("  FPR.advance target=" + target);
-      // }
-
-      if (docFreq > BLOCK_SIZE && target > nextSkipDoc) {
-
-        // if (DEBUG) {
-        //   System.out.println("    try skipper");
-        // }
-
-        if (skipper == null) {
-          // Lazy init: first time this enum has ever been used for skipping
-          // if (DEBUG) {
-          //   System.out.println("    create skipper");
-          // }
-          skipper = new BlockSkipReader(docIn.clone(),
-                                        BlockPostingsWriter.maxSkipLevels,
-                                        BLOCK_SIZE,
-                                        true,
-                                        indexHasOffsets,
-                                        indexHasPayloads);
-        }
-
-        if (!skipped) {
-          assert skipOffset != -1;
-          // This is the first time this enum has skipped
-          // since reset() was called; load the skip data:
-          // if (DEBUG) {
-          //   System.out.println("    init skipper");
-          // }
-          skipper.init(docTermStartFP+skipOffset, docTermStartFP, posTermStartFP, payTermStartFP, docFreq);
-          skipped = true;
-        }
-
-        final int newDocUpto = skipper.skipTo(target) + 1; 
-
-        if (newDocUpto > docUpto) {
-          // Skipper moved
-          // if (DEBUG) {
-          //   System.out.println("    skipper moved to docUpto=" + newDocUpto + " vs current=" + docUpto + "; docID=" + skipper.getDoc() + " fp=" + skipper.getDocPointer() + " pos.fp=" + skipper.getPosPointer() + " pos.bufferUpto=" + skipper.getPosBufferUpto() + " pay.fp=" + skipper.getPayPointer() + " lastStartOffset=" + lastStartOffset);
-          // }
-          assert newDocUpto % BLOCK_SIZE == 0 : "got " + newDocUpto;
-          docUpto = newDocUpto;
-
-          // Force to read next block
-          docBufferUpto = BLOCK_SIZE;
-          accum = skipper.getDoc();
-          docIn.seek(skipper.getDocPointer());
-          posPendingFP = skipper.getPosPointer();
-          payPendingFP = skipper.getPayPointer();
-          posPendingCount = skipper.getPosBufferUpto();
-          lastStartOffset = skipper.getStartOffset();
-          payloadByteUpto = skipper.getPayloadByteUpto();
-        }
-        nextSkipDoc = skipper.getNextSkipDoc();
-      }
-      if (docUpto == docFreq) {
-        return doc = NO_MORE_DOCS;
-      }
-      if (docBufferUpto == BLOCK_SIZE) {
-        refillDocs();
-      }
-
-      // Now scan:
-      while (true) {
-        // if (DEBUG) {
-        //   System.out.println("  scan doc=" + accum + " docBufferUpto=" + docBufferUpto);
-        // }
-        accum += docDeltaBuffer[docBufferUpto];
-        freq = freqBuffer[docBufferUpto];
-        posPendingCount += freq;
-        docBufferUpto++;
-        docUpto++;
-
-        if (accum >= target) {
-          break;
-        }
-        if (docUpto == docFreq) {
-          return doc = NO_MORE_DOCS;
-        }
-      }
-
-      if (liveDocs == null || liveDocs.get(accum)) {
-        // if (DEBUG) {
-        //   System.out.println("  return doc=" + accum);
-        // }
-        position = 0;
-        lastStartOffset = 0;
-        return doc = accum;
-      } else {
-        // if (DEBUG) {
-        //   System.out.println("  now do nextDoc()");
-        // }
-        return nextDoc();
-      }
-    }
-
-    // TODO: in theory we could avoid loading frq block
-    // when not needed, ie, use skip data to load how far to
-    // seek the pos pointer ... instead of having to load frq
-    // blocks only to sum up how many positions to skip
-    private void skipPositions() throws IOException {
-      // Skip positions now:
-      int toSkip = posPendingCount - freq;
-      // if (DEBUG) {
-      //   System.out.println("      FPR.skipPositions: toSkip=" + toSkip);
-      // }
-
-      final int leftInBlock = BLOCK_SIZE - posBufferUpto;
-      if (toSkip < leftInBlock) {
-        int end = posBufferUpto + toSkip;
-        while(posBufferUpto < end) {
-          if (indexHasPayloads) {
-            payloadByteUpto += payloadLengthBuffer[posBufferUpto];
-          }
-          posBufferUpto++;
-        }
-        // if (DEBUG) {
-        //   System.out.println("        skip w/in block to posBufferUpto=" + posBufferUpto);
-        // }
-      } else {
-        toSkip -= leftInBlock;
-        while(toSkip >= BLOCK_SIZE) {
-          // if (DEBUG) {
-          //   System.out.println("        skip whole block @ fp=" + posIn.getFilePointer());
-          // }
-          assert posIn.getFilePointer() != lastPosBlockFP;
-          forUtil.skipBlock(posIn);
-
-          if (indexHasPayloads) {
-            // Skip payloadLength block:
-            forUtil.skipBlock(payIn);
-
-            // Skip payloadBytes block:
-            int numBytes = payIn.readVInt();
-            payIn.seek(payIn.getFilePointer() + numBytes);
-          }
-
-          if (indexHasOffsets) {
-            forUtil.skipBlock(payIn);
-            forUtil.skipBlock(payIn);
-          }
-          toSkip -= BLOCK_SIZE;
-        }
-        refillPositions();
-        payloadByteUpto = 0;
-        posBufferUpto = 0;
-        while(posBufferUpto < toSkip) {
-          if (indexHasPayloads) {
-            payloadByteUpto += payloadLengthBuffer[posBufferUpto];
-          }
-          posBufferUpto++;
-        }
-        // if (DEBUG) {
-        //   System.out.println("        skip w/in block to posBufferUpto=" + posBufferUpto);
-        // }
-      }
-
-      position = 0;
-      lastStartOffset = 0;
-    }
-
-    @Override
-    public int nextPosition() throws IOException {
-      // if (DEBUG) {
-      //   System.out.println("    FPR.nextPosition posPendingCount=" + posPendingCount + " posBufferUpto=" + posBufferUpto + " payloadByteUpto=" + payloadByteUpto)// ;
-      // }
-      if (posPendingFP != -1) {
-        // if (DEBUG) {
-        //   System.out.println("      seek pos to pendingFP=" + posPendingFP);
-        // }
-        posIn.seek(posPendingFP);
-        posPendingFP = -1;
-
-        if (payPendingFP != -1) {
-          // if (DEBUG) {
-          //   System.out.println("      seek pay to pendingFP=" + payPendingFP);
-          // }
-          payIn.seek(payPendingFP);
-          payPendingFP = -1;
-        }
-
-        // Force buffer refill:
-        posBufferUpto = BLOCK_SIZE;
-      }
-
-      if (posPendingCount > freq) {
-        skipPositions();
-        posPendingCount = freq;
-      }
-
-      if (posBufferUpto == BLOCK_SIZE) {
-        refillPositions();
-        posBufferUpto = 0;
-      }
-      position += posDeltaBuffer[posBufferUpto];
-
-      if (indexHasPayloads) {
-        payloadLength = payloadLengthBuffer[posBufferUpto];
-        payload.bytes = payloadBytes;
-        payload.offset = payloadByteUpto;
-        payload.length = payloadLength;
-        payloadByteUpto += payloadLength;
-      }
-
-      if (indexHasOffsets) {
-        startOffset = lastStartOffset + offsetStartDeltaBuffer[posBufferUpto];
-        endOffset = startOffset + offsetLengthBuffer[posBufferUpto];
-        lastStartOffset = startOffset;
-      }
-
-      posBufferUpto++;
-      posPendingCount--;
-      // if (DEBUG) {
-      //   System.out.println("      return pos=" + position);
-      // }
-      return position;
-    }
-
-    @Override
-    public int startOffset() {
-      return startOffset;
-    }
-  
-    @Override
-    public int endOffset() {
-      return endOffset;
-    }
-  
-    @Override
-    public BytesRef getPayload() {
-      // if (DEBUG) {
-      //   System.out.println("    FPR.getPayload payloadLength=" + payloadLength + " payloadByteUpto=" + payloadByteUpto);
-      // }
-      if (payloadLength == 0) {
-        return null;
-      } else {
-        return payload;
-      }
-    }
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/block/BlockPostingsWriter.java b/lucene/core/src/java/org/apache/lucene/codecs/block/BlockPostingsWriter.java
deleted file mode 100644
index 70ca7ef..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/block/BlockPostingsWriter.java
+++ /dev/null
@@ -1,563 +0,0 @@
-package org.apache.lucene.codecs.block;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import static org.apache.lucene.codecs.block.BlockPostingsFormat.BLOCK_SIZE;
-import static org.apache.lucene.codecs.block.ForUtil.MAX_DATA_SIZE;
-import static org.apache.lucene.codecs.block.ForUtil.MAX_ENCODED_SIZE;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.PostingsWriterBase;
-import org.apache.lucene.codecs.TermStats;
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.store.RAMOutputStream;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.packed.PackedInts;
-
-
-/**
- * Concrete class that writes docId(maybe frq,pos,offset,payloads) list
- * with postings format.
- *
- * Postings list for each term will be stored separately. 
- *
- * @see BlockSkipWriter for details about skipping setting and postings layout.
- *
- */
-final class BlockPostingsWriter extends PostingsWriterBase {
-
-  /** 
-   * Expert: The maximum number of skip levels. Smaller values result in 
-   * slightly smaller indexes, but slower skipping in big posting lists.
-   */
-  static final int maxSkipLevels = 10;
-
-  final static String TERMS_CODEC = "BlockPostingsWriterTerms";
-  final static String DOC_CODEC = "BlockPostingsWriterDoc";
-  final static String POS_CODEC = "BlockPostingsWriterPos";
-  final static String PAY_CODEC = "BlockPostingsWriterPay";
-
-  // Increment version to change it:
-  final static int VERSION_START = 0;
-  final static int VERSION_CURRENT = VERSION_START;
-
-  final IndexOutput docOut;
-  final IndexOutput posOut;
-  final IndexOutput payOut;
-
-  private IndexOutput termsOut;
-
-  // How current field indexes postings:
-  private boolean fieldHasFreqs;
-  private boolean fieldHasPositions;
-  private boolean fieldHasOffsets;
-  private boolean fieldHasPayloads;
-
-  // Holds starting file pointers for each term:
-  private long docTermStartFP;
-  private long posTermStartFP;
-  private long payTermStartFP;
-
-  final int[] docDeltaBuffer;
-  final int[] freqBuffer;
-  private int docBufferUpto;
-
-  final int[] posDeltaBuffer;
-  final int[] payloadLengthBuffer;
-  final int[] offsetStartDeltaBuffer;
-  final int[] offsetLengthBuffer;
-  private int posBufferUpto;
-
-  private byte[] payloadBytes;
-  private int payloadByteUpto;
-
-  private int lastBlockDocID;
-  private long lastBlockPosFP;
-  private long lastBlockPayFP;
-  private int lastBlockPosBufferUpto;
-  private int lastBlockStartOffset;
-  private int lastBlockPayloadByteUpto;
-
-  private int lastDocID;
-  private int lastPosition;
-  private int lastStartOffset;
-  private int docCount;
-
-  final byte[] encoded;
-
-  private final ForUtil forUtil;
-  private final BlockSkipWriter skipWriter;
-  
-  public BlockPostingsWriter(SegmentWriteState state, float acceptableOverheadRatio) throws IOException {
-    super();
-
-    docOut = state.directory.createOutput(IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, BlockPostingsFormat.DOC_EXTENSION),
-                                          state.context);
-    IndexOutput posOut = null;
-    IndexOutput payOut = null;
-    boolean success = false;
-    try {
-      CodecUtil.writeHeader(docOut, DOC_CODEC, VERSION_CURRENT);
-      forUtil = new ForUtil(acceptableOverheadRatio, docOut);
-      if (state.fieldInfos.hasProx()) {
-        posDeltaBuffer = new int[MAX_DATA_SIZE];
-        posOut = state.directory.createOutput(IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, BlockPostingsFormat.POS_EXTENSION),
-                                              state.context);
-        CodecUtil.writeHeader(posOut, POS_CODEC, VERSION_CURRENT);
-
-        if (state.fieldInfos.hasPayloads()) {
-          payloadBytes = new byte[128];
-          payloadLengthBuffer = new int[MAX_DATA_SIZE];
-        } else {
-          payloadBytes = null;
-          payloadLengthBuffer = null;
-        }
-
-        if (state.fieldInfos.hasOffsets()) {
-          offsetStartDeltaBuffer = new int[MAX_DATA_SIZE];
-          offsetLengthBuffer = new int[MAX_DATA_SIZE];
-        } else {
-          offsetStartDeltaBuffer = null;
-          offsetLengthBuffer = null;
-        }
-
-        if (state.fieldInfos.hasPayloads() || state.fieldInfos.hasOffsets()) {
-          payOut = state.directory.createOutput(IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, BlockPostingsFormat.PAY_EXTENSION),
-                                                state.context);
-          CodecUtil.writeHeader(payOut, PAY_CODEC, VERSION_CURRENT);
-        }
-      } else {
-        posDeltaBuffer = null;
-        payloadLengthBuffer = null;
-        offsetStartDeltaBuffer = null;
-        offsetLengthBuffer = null;
-        payloadBytes = null;
-      }
-      this.payOut = payOut;
-      this.posOut = posOut;
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(docOut, posOut, payOut);
-      }
-    }
-
-    docDeltaBuffer = new int[MAX_DATA_SIZE];
-    freqBuffer = new int[MAX_DATA_SIZE];
-
-    // TODO: should we try skipping every 2/4 blocks...?
-    skipWriter = new BlockSkipWriter(maxSkipLevels,
-                                     BLOCK_SIZE, 
-                                     state.segmentInfo.getDocCount(),
-                                     docOut,
-                                     posOut,
-                                     payOut);
-
-    encoded = new byte[MAX_ENCODED_SIZE];
-  }
-
-  public BlockPostingsWriter(SegmentWriteState state) throws IOException {
-    this(state, PackedInts.COMPACT);
-  }
-
-  @Override
-  public void start(IndexOutput termsOut) throws IOException {
-    this.termsOut = termsOut;
-    CodecUtil.writeHeader(termsOut, TERMS_CODEC, VERSION_CURRENT);
-    termsOut.writeVInt(BLOCK_SIZE);
-  }
-
-  @Override
-  public void setField(FieldInfo fieldInfo) {
-    IndexOptions indexOptions = fieldInfo.getIndexOptions();
-    fieldHasFreqs = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS) >= 0;
-    fieldHasPositions = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;
-    fieldHasOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
-    fieldHasPayloads = fieldInfo.hasPayloads();
-    skipWriter.setField(fieldHasPositions, fieldHasOffsets, fieldHasPayloads);
-  }
-
-  @Override
-  public void startTerm() {
-    docTermStartFP = docOut.getFilePointer();
-    if (fieldHasPositions) {
-      posTermStartFP = posOut.getFilePointer();
-      if (fieldHasPayloads || fieldHasOffsets) {
-        payTermStartFP = payOut.getFilePointer();
-      }
-    }
-    lastDocID = 0;
-    lastBlockDocID = -1;
-    // if (DEBUG) {
-    //   System.out.println("FPW.startTerm startFP=" + docTermStartFP);
-    // }
-    skipWriter.resetSkip();
-  }
-
-  @Override
-  public void startDoc(int docID, int termDocFreq) throws IOException {
-    // if (DEBUG) {
-    //   System.out.println("FPW.startDoc docID["+docBufferUpto+"]=" + docID);
-    // }
-    // Have collected a block of docs, and get a new doc. 
-    // Should write skip data as well as postings list for
-    // current block.
-    if (lastBlockDocID != -1 && docBufferUpto == 0) {
-      // if (DEBUG) {
-      //   System.out.println("  bufferSkip at writeBlock: lastDocID=" + lastBlockDocID + " docCount=" + (docCount-1));
-      // }
-      skipWriter.bufferSkip(lastBlockDocID, docCount, lastBlockPosFP, lastBlockPayFP, lastBlockPosBufferUpto, lastBlockStartOffset, lastBlockPayloadByteUpto);
-    }
-
-    final int docDelta = docID - lastDocID;
-
-    if (docID < 0 || (docCount > 0 && docDelta <= 0)) {
-      throw new CorruptIndexException("docs out of order (" + docID + " <= " + lastDocID + " ) (docOut: " + docOut + ")");
-    }
-
-    docDeltaBuffer[docBufferUpto] = docDelta;
-    // if (DEBUG) {
-    //   System.out.println("  docDeltaBuffer[" + docBufferUpto + "]=" + docDelta);
-    // }
-    if (fieldHasFreqs) {
-      freqBuffer[docBufferUpto] = termDocFreq;
-    }
-    docBufferUpto++;
-    docCount++;
-
-    if (docBufferUpto == BLOCK_SIZE) {
-      // if (DEBUG) {
-      //   System.out.println("  write docDelta block @ fp=" + docOut.getFilePointer());
-      // }
-      forUtil.writeBlock(docDeltaBuffer, encoded, docOut);
-      if (fieldHasFreqs) {
-        // if (DEBUG) {
-        //   System.out.println("  write freq block @ fp=" + docOut.getFilePointer());
-        // }
-        forUtil.writeBlock(freqBuffer, encoded, docOut);
-      }
-      // NOTE: don't set docBufferUpto back to 0 here;
-      // finishDoc will do so (because it needs to see that
-      // the block was filled so it can save skip data)
-    }
-
-
-    lastDocID = docID;
-    lastPosition = 0;
-    lastStartOffset = 0;
-  }
-
-  /** Add a new position & payload */
-  @Override
-  public void addPosition(int position, BytesRef payload, int startOffset, int endOffset) throws IOException {
-    // if (DEBUG) {
-    //   System.out.println("FPW.addPosition pos=" + position + " posBufferUpto=" + posBufferUpto + (fieldHasPayloads ? " payloadByteUpto=" + payloadByteUpto: ""));
-    // }
-    posDeltaBuffer[posBufferUpto] = position - lastPosition;
-    if (fieldHasPayloads) {
-      if (payload == null || payload.length == 0) {
-        // no payload
-        payloadLengthBuffer[posBufferUpto] = 0;
-      } else {
-        payloadLengthBuffer[posBufferUpto] = payload.length;
-        if (payloadByteUpto + payload.length > payloadBytes.length) {
-          payloadBytes = ArrayUtil.grow(payloadBytes, payloadByteUpto + payload.length);
-        }
-        System.arraycopy(payload.bytes, payload.offset, payloadBytes, payloadByteUpto, payload.length);
-        payloadByteUpto += payload.length;
-      }
-    }
-
-    if (fieldHasOffsets) {
-      assert startOffset >= lastStartOffset;
-      assert endOffset >= startOffset;
-      offsetStartDeltaBuffer[posBufferUpto] = startOffset - lastStartOffset;
-      offsetLengthBuffer[posBufferUpto] = endOffset - startOffset;
-      lastStartOffset = startOffset;
-    }
-    
-    posBufferUpto++;
-    lastPosition = position;
-    if (posBufferUpto == BLOCK_SIZE) {
-      // if (DEBUG) {
-      //   System.out.println("  write pos bulk block @ fp=" + posOut.getFilePointer());
-      // }
-      forUtil.writeBlock(posDeltaBuffer, encoded, posOut);
-
-      if (fieldHasPayloads) {
-        forUtil.writeBlock(payloadLengthBuffer, encoded, payOut);
-        payOut.writeVInt(payloadByteUpto);
-        payOut.writeBytes(payloadBytes, 0, payloadByteUpto);
-        payloadByteUpto = 0;
-      }
-      if (fieldHasOffsets) {
-        forUtil.writeBlock(offsetStartDeltaBuffer, encoded, payOut);
-        forUtil.writeBlock(offsetLengthBuffer, encoded, payOut);
-      }
-      posBufferUpto = 0;
-    }
-  }
-
-  @Override
-  public void finishDoc() throws IOException {
-    // Since we don't know df for current term, we had to buffer
-    // those skip data for each block, and when a new doc comes, 
-    // write them to skip file.
-    if (docBufferUpto == BLOCK_SIZE) {
-      lastBlockDocID = lastDocID;
-      if (posOut != null) {
-        if (payOut != null) {
-          lastBlockPayFP = payOut.getFilePointer();
-        }
-        lastBlockPosFP = posOut.getFilePointer();
-        lastBlockPosBufferUpto = posBufferUpto;
-        lastBlockStartOffset = lastStartOffset;
-        lastBlockPayloadByteUpto = payloadByteUpto;
-      }
-      // if (DEBUG) {
-      //   System.out.println("  docBufferUpto="+docBufferUpto+" now get lastBlockDocID="+lastBlockDocID+" lastBlockPosFP=" + lastBlockPosFP + " lastBlockPosBufferUpto=" +  lastBlockPosBufferUpto + " lastBlockPayloadByteUpto=" + lastBlockPayloadByteUpto);
-      // }
-      docBufferUpto = 0;
-    }
-  }
-
-  private static class PendingTerm {
-    public final long docStartFP;
-    public final long posStartFP;
-    public final long payStartFP;
-    public final int skipOffset;
-    public final int lastPosBlockOffset;
-
-    public PendingTerm(long docStartFP, long posStartFP, long payStartFP, int skipOffset, int lastPosBlockOffset) {
-      this.docStartFP = docStartFP;
-      this.posStartFP = posStartFP;
-      this.payStartFP = payStartFP;
-      this.skipOffset = skipOffset;
-      this.lastPosBlockOffset = lastPosBlockOffset;
-    }
-  }
-
-  private final List<PendingTerm> pendingTerms = new ArrayList<PendingTerm>();
-
-  /** Called when we are done adding docs to this term */
-  @Override
-  public void finishTerm(TermStats stats) throws IOException {
-    assert stats.docFreq > 0;
-
-    // TODO: wasteful we are counting this (counting # docs
-    // for this term) in two places?
-    assert stats.docFreq == docCount: stats.docFreq + " vs " + docCount;
-
-    // if (DEBUG) {
-    //   System.out.println("FPW.finishTerm docFreq=" + stats.docFreq);
-    // }
-
-    // if (DEBUG) {
-    //   if (docBufferUpto > 0) {
-    //     System.out.println("  write doc/freq vInt block (count=" + docBufferUpto + ") at fp=" + docOut.getFilePointer() + " docTermStartFP=" + docTermStartFP);
-    //   }
-    // }
-
-    // vInt encode the remaining doc deltas and freqs:
-    for(int i=0;i<docBufferUpto;i++) {
-      final int docDelta = docDeltaBuffer[i];
-      final int freq = freqBuffer[i];
-      if (!fieldHasFreqs) {
-        docOut.writeVInt(docDelta);
-      } else if (freqBuffer[i] == 1) {
-        docOut.writeVInt((docDelta<<1)|1);
-      } else {
-        docOut.writeVInt(docDelta<<1);
-        docOut.writeVInt(freq);
-      }
-    }
-
-    final int lastPosBlockOffset;
-
-    if (fieldHasPositions) {
-      // if (DEBUG) {
-      //   if (posBufferUpto > 0) {
-      //     System.out.println("  write pos vInt block (count=" + posBufferUpto + ") at fp=" + posOut.getFilePointer() + " posTermStartFP=" + posTermStartFP + " hasPayloads=" + fieldHasPayloads + " hasOffsets=" + fieldHasOffsets);
-      //   }
-      // }
-
-      // totalTermFreq is just total number of positions(or payloads, or offsets)
-      // associated with current term.
-      assert stats.totalTermFreq != -1;
-      if (stats.totalTermFreq > BLOCK_SIZE) {
-        // record file offset for last pos in last block
-        lastPosBlockOffset = (int) (posOut.getFilePointer() - posTermStartFP);
-      } else {
-        lastPosBlockOffset = -1;
-      }
-      if (posBufferUpto > 0) {
-        posOut.writeVInt(posBufferUpto);
-        
-        // TODO: should we send offsets/payloads to
-        // .pay...?  seems wasteful (have to store extra
-        // vLong for low (< BLOCK_SIZE) DF terms = vast vast
-        // majority)
-
-        // vInt encode the remaining positions/payloads/offsets:
-        int lastPayloadLength = -1;
-        int payloadBytesReadUpto = 0;
-        for(int i=0;i<posBufferUpto;i++) {
-          final int posDelta = posDeltaBuffer[i];
-          if (fieldHasPayloads) {
-            final int payloadLength = payloadLengthBuffer[i];
-            if (payloadLength != lastPayloadLength) {
-              lastPayloadLength = payloadLength;
-              posOut.writeVInt((posDelta<<1)|1);
-              posOut.writeVInt(payloadLength);
-            } else {
-              posOut.writeVInt(posDelta<<1);
-            }
-
-            // if (DEBUG) {
-            //   System.out.println("        i=" + i + " payloadLen=" + payloadLength);
-            // }
-
-            if (payloadLength != 0) {
-              // if (DEBUG) {
-              //   System.out.println("          write payload @ pos.fp=" + posOut.getFilePointer());
-              // }
-              posOut.writeBytes(payloadBytes, payloadBytesReadUpto, payloadLength);
-              payloadBytesReadUpto += payloadLength;
-            }
-          } else {
-            posOut.writeVInt(posDelta);
-          }
-
-          if (fieldHasOffsets) {
-            // if (DEBUG) {
-            //   System.out.println("          write offset @ pos.fp=" + posOut.getFilePointer());
-            // }
-            posOut.writeVInt(offsetStartDeltaBuffer[i]);
-            posOut.writeVInt(offsetLengthBuffer[i]);
-          }
-        }
-
-        if (fieldHasPayloads) {
-          assert payloadBytesReadUpto == payloadByteUpto;
-          payloadByteUpto = 0;
-        }
-      }
-      // if (DEBUG) {
-      //   System.out.println("  totalTermFreq=" + stats.totalTermFreq + " lastPosBlockOffset=" + lastPosBlockOffset);
-      // }
-    } else {
-      lastPosBlockOffset = -1;
-    }
-
-    int skipOffset;
-    if (docCount > BLOCK_SIZE) {
-      skipOffset = (int) (skipWriter.writeSkip(docOut) - docTermStartFP);
-      
-      // if (DEBUG) {
-      //   System.out.println("skip packet " + (docOut.getFilePointer() - (docTermStartFP + skipOffset)) + " bytes");
-      // }
-    } else {
-      skipOffset = -1;
-      // if (DEBUG) {
-      //   System.out.println("  no skip: docCount=" + docCount);
-      // }
-    }
-
-    long payStartFP;
-    if (stats.totalTermFreq >= BLOCK_SIZE) {
-      payStartFP = payTermStartFP;
-    } else {
-      payStartFP = -1;
-    }
-
-    // if (DEBUG) {
-    //   System.out.println("  payStartFP=" + payStartFP);
-    // }
-
-    pendingTerms.add(new PendingTerm(docTermStartFP, posTermStartFP, payStartFP, skipOffset, lastPosBlockOffset));
-    docBufferUpto = 0;
-    posBufferUpto = 0;
-    lastDocID = 0;
-    docCount = 0;
-  }
-
-  private final RAMOutputStream bytesWriter = new RAMOutputStream();
-
-  @Override
-  public void flushTermsBlock(int start, int count) throws IOException {
-
-    if (count == 0) {
-      termsOut.writeByte((byte) 0);
-      return;
-    }
-
-    assert start <= pendingTerms.size();
-    assert count <= start;
-
-    final int limit = pendingTerms.size() - start + count;
-
-    long lastDocStartFP = 0;
-    long lastPosStartFP = 0;
-    long lastPayStartFP = 0;
-    for(int idx=limit-count; idx<limit; idx++) {
-      PendingTerm term = pendingTerms.get(idx);
-
-      bytesWriter.writeVLong(term.docStartFP - lastDocStartFP);
-      lastDocStartFP = term.docStartFP;
-
-      if (fieldHasPositions) {
-        bytesWriter.writeVLong(term.posStartFP - lastPosStartFP);
-        lastPosStartFP = term.posStartFP;
-        if (term.lastPosBlockOffset != -1) {
-          bytesWriter.writeVInt(term.lastPosBlockOffset);
-        }
-        if ((fieldHasPayloads || fieldHasOffsets) && term.payStartFP != -1) {
-          bytesWriter.writeVLong(term.payStartFP - lastPayStartFP);
-          lastPayStartFP = term.payStartFP;
-        }
-      }
-
-      if (term.skipOffset != -1) {
-        bytesWriter.writeVInt(term.skipOffset);
-      }
-    }
-
-    termsOut.writeVInt((int) bytesWriter.getFilePointer());
-    bytesWriter.writeTo(termsOut);
-    bytesWriter.reset();
-
-    // Remove the terms we just wrote:
-    pendingTerms.subList(limit-count, limit).clear();
-  }
-
-  @Override
-  public void close() throws IOException {
-    IOUtils.close(docOut, posOut, payOut);
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/block/BlockSkipReader.java b/lucene/core/src/java/org/apache/lucene/codecs/block/BlockSkipReader.java
deleted file mode 100644
index 169219c..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/block/BlockSkipReader.java
+++ /dev/null
@@ -1,244 +0,0 @@
-package org.apache.lucene.codecs.block;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Arrays;
-
-import org.apache.lucene.codecs.MultiLevelSkipListReader;
-import org.apache.lucene.store.IndexInput;
-
-/**
- * Implements the skip list reader for block postings format
- * that stores positions and payloads.
- * 
- * Although this skipper uses MultiLevelSkipListReader as an interface, 
- * its definition of skip position will be a little different. 
- *
- * For example, when skipInterval = blockSize = 3, df = 2*skipInterval = 6, 
- * 
- * 0 1 2 3 4 5
- * d d d d d d    (posting list)
- *     ^     ^    (skip point in MultiLeveSkipWriter)
- *       ^        (skip point in BlockSkipWriter)
- *
- * In this case, MultiLevelSkipListReader will use the last document as a skip point, 
- * while BlockSkipReader should assume no skip point will comes. 
- *
- * If we use the interface directly in BlockSkipReader, it may silly try to read 
- * another skip data after the only skip point is loaded. 
- *
- * To illustrate this, we can call skipTo(d[5]), since skip point d[3] has smaller docId,
- * and numSkipped+blockSize== df, the MultiLevelSkipListReader will assume the skip list
- * isn't exhausted yet, and try to load a non-existed skip point
- *
- * Therefore, we'll trim df before passing it to the interface. see trim(int)
- *
- */
-final class BlockSkipReader extends MultiLevelSkipListReader {
-  // private boolean DEBUG = BlockPostingsReader.DEBUG;
-  private final int blockSize;
-
-  private long docPointer[];
-  private long posPointer[];
-  private long payPointer[];
-  private int posBufferUpto[];
-  private int startOffset[];
-  private int payloadByteUpto[];
-
-  private long lastPosPointer;
-  private long lastPayPointer;
-  private int lastStartOffset;
-  private int lastPayloadByteUpto;
-  private long lastDocPointer;
-  private int lastPosBufferUpto;
-
-  public BlockSkipReader(IndexInput skipStream, int maxSkipLevels, int blockSize, boolean hasPos, boolean hasOffsets, boolean hasPayloads) {
-    super(skipStream, maxSkipLevels, blockSize, 8);
-    this.blockSize = blockSize;
-    docPointer = new long[maxSkipLevels];
-    if (hasPos) {
-      posPointer = new long[maxSkipLevels];
-      posBufferUpto = new int[maxSkipLevels];
-      if (hasPayloads) {
-        payloadByteUpto = new int[maxSkipLevels];
-      } else {
-        payloadByteUpto = null;
-      }
-      if (hasOffsets) {
-        startOffset = new int[maxSkipLevels];
-      } else {
-        startOffset = null;
-      }
-      if (hasOffsets || hasPayloads) {
-        payPointer = new long[maxSkipLevels];
-      } else {
-        payPointer = null;
-      }
-    } else {
-      posPointer = null;
-    }
-  }
-
-  /**
-   * Trim original docFreq to tell skipReader read proper number of skip points.
-   *
-   * Since our definition in BlockSkip* is a little different from MultiLevelSkip*
-   * This trimmed docFreq will prevent skipReader from:
-   * 1. silly reading a non-existed skip point after the last block boundary
-   * 2. moving into the vInt block
-   *
-   */
-  protected int trim(int df) {
-    return df % blockSize == 0? df - 1: df;
-  }
-
-  public void init(long skipPointer, long docBasePointer, long posBasePointer, long payBasePointer, int df) {
-    super.init(skipPointer, trim(df));
-    lastDocPointer = docBasePointer;
-    lastPosPointer = posBasePointer;
-    lastPayPointer = payBasePointer;
-
-    Arrays.fill(docPointer, docBasePointer);
-    if (posPointer != null) {
-      Arrays.fill(posPointer, posBasePointer);
-      if (payPointer != null) {
-        Arrays.fill(payPointer, payBasePointer);
-      }
-    } else {
-      assert posBasePointer == 0;
-    }
-  }
-
-  /** Returns the doc pointer of the doc to which the last call of 
-   * {@link MultiLevelSkipListReader#skipTo(int)} has skipped.  */
-  public long getDocPointer() {
-    return lastDocPointer;
-  }
-
-  public long getPosPointer() {
-    return lastPosPointer;
-  }
-
-  public int getPosBufferUpto() {
-    return lastPosBufferUpto;
-  }
-
-  public long getPayPointer() {
-    return lastPayPointer;
-  }
-
-  public int getStartOffset() {
-    return lastStartOffset;
-  }
-
-  public int getPayloadByteUpto() {
-    return lastPayloadByteUpto;
-  }
-
-  public int getNextSkipDoc() {
-    return skipDoc[0];
-  }
-
-  @Override
-  protected void seekChild(int level) throws IOException {
-    super.seekChild(level);
-    // if (DEBUG) {
-    //   System.out.println("seekChild level=" + level);
-    // }
-    docPointer[level] = lastDocPointer;
-    if (posPointer != null) {
-      posPointer[level] = lastPosPointer;
-      posBufferUpto[level] = lastPosBufferUpto;
-      if (startOffset != null) {
-        startOffset[level] = lastStartOffset;
-      }
-      if (payloadByteUpto != null) {
-        payloadByteUpto[level] = lastPayloadByteUpto;
-      }
-      if (payPointer != null) {
-        payPointer[level] = lastPayPointer;
-      }
-    }
-  }
-  
-  @Override
-  protected void setLastSkipData(int level) {
-    super.setLastSkipData(level);
-    lastDocPointer = docPointer[level];
-    // if (DEBUG) {
-    //   System.out.println("setLastSkipData level=" + level);
-    //   System.out.println("  lastDocPointer=" + lastDocPointer);
-    // }
-    if (posPointer != null) {
-      lastPosPointer = posPointer[level];
-      lastPosBufferUpto = posBufferUpto[level];
-      // if (DEBUG) {
-      //   System.out.println("  lastPosPointer=" + lastPosPointer + " lastPosBUfferUpto=" + lastPosBufferUpto);
-      // }
-      if (payPointer != null) {
-        lastPayPointer = payPointer[level];
-      }
-      if (startOffset != null) {
-        lastStartOffset = startOffset[level];
-      }
-      if (payloadByteUpto != null) {
-        lastPayloadByteUpto = payloadByteUpto[level];
-      }
-    }
-  }
-
-  @Override
-  protected int readSkipData(int level, IndexInput skipStream) throws IOException {
-    // if (DEBUG) {
-    //   System.out.println("readSkipData level=" + level);
-    // }
-    int delta = skipStream.readVInt();
-    // if (DEBUG) {
-    //   System.out.println("  delta=" + delta);
-    // }
-    docPointer[level] += skipStream.readVInt();
-    // if (DEBUG) {
-    //   System.out.println("  docFP=" + docPointer[level]);
-    // }
-
-    if (posPointer != null) {
-      posPointer[level] += skipStream.readVInt();
-      // if (DEBUG) {
-      //   System.out.println("  posFP=" + posPointer[level]);
-      // }
-      posBufferUpto[level] = skipStream.readVInt();
-      // if (DEBUG) {
-      //   System.out.println("  posBufferUpto=" + posBufferUpto[level]);
-      // }
-
-      if (payloadByteUpto != null) {
-        payloadByteUpto[level] = skipStream.readVInt();
-      }
-
-      if (startOffset != null) {
-        startOffset[level] += skipStream.readVInt();
-      }
-
-      if (payPointer != null) {
-        payPointer[level] += skipStream.readVInt();
-      }
-    }
-    return delta;
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/block/BlockSkipWriter.java b/lucene/core/src/java/org/apache/lucene/codecs/block/BlockSkipWriter.java
deleted file mode 100644
index 8ece562..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/block/BlockSkipWriter.java
+++ /dev/null
@@ -1,163 +0,0 @@
-package org.apache.lucene.codecs.block;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Arrays;
-
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.codecs.MultiLevelSkipListWriter;
-
-/**
- * Write skip lists with multiple levels, and support skip within block ints.
- *
- * Assume that docFreq = 28, skipInterval = blockSize = 12
- *
- *  |       block#0       | |      block#1        | |vInts|
- *  d d d d d d d d d d d d d d d d d d d d d d d d d d d d (posting list)
- *                          ^                       ^       (level 0 skip point)
- *
- * Note that skipWriter will ignore first document in block#0, since 
- * it is useless as a skip point.  Also, we'll never skip into the vInts
- * block, only record skip data at the start its start point(if it exist).
- *
- * For each skip point, we will record: 
- * 1. docID in former position, i.e. for position 12, record docID[11], etc.
- * 2. its related file points(position, payload), 
- * 3. related numbers or uptos(position, payload).
- * 4. start offset.
- *
- */
-final class BlockSkipWriter extends MultiLevelSkipListWriter {
-  // private boolean DEBUG = BlockPostingsReader.DEBUG;
-  
-  private int[] lastSkipDoc;
-  private long[] lastSkipDocPointer;
-  private long[] lastSkipPosPointer;
-  private long[] lastSkipPayPointer;
-  private int[] lastStartOffset;
-  private int[] lastPayloadByteUpto;
-
-  private final IndexOutput docOut;
-  private final IndexOutput posOut;
-  private final IndexOutput payOut;
-
-  private int curDoc;
-  private long curDocPointer;
-  private long curPosPointer;
-  private long curPayPointer;
-  private int curPosBufferUpto;
-  private int curStartOffset;
-  private int curPayloadByteUpto;
-  private boolean fieldHasPositions;
-  private boolean fieldHasOffsets;
-  private boolean fieldHasPayloads;
-
-  public BlockSkipWriter(int maxSkipLevels, int blockSize, int docCount, IndexOutput docOut, IndexOutput posOut, IndexOutput payOut) {
-    super(blockSize, 8, maxSkipLevels, docCount);
-    this.docOut = docOut;
-    this.posOut = posOut;
-    this.payOut = payOut;
-    
-    lastSkipDoc = new int[maxSkipLevels];
-    lastSkipDocPointer = new long[maxSkipLevels];
-    if (posOut != null) {
-      lastSkipPosPointer = new long[maxSkipLevels];
-      if (payOut != null) {
-        lastSkipPayPointer = new long[maxSkipLevels];
-      }
-      lastStartOffset = new int[maxSkipLevels];
-      lastPayloadByteUpto = new int[maxSkipLevels];
-    }
-  }
-
-  public void setField(boolean fieldHasPositions, boolean fieldHasOffsets, boolean fieldHasPayloads) {
-    this.fieldHasPositions = fieldHasPositions;
-    this.fieldHasOffsets = fieldHasOffsets;
-    this.fieldHasPayloads = fieldHasPayloads;
-  }
-
-  @Override
-  public void resetSkip() {
-    super.resetSkip();
-    Arrays.fill(lastSkipDoc, 0);
-    Arrays.fill(lastSkipDocPointer, docOut.getFilePointer());
-    if (fieldHasPositions) {
-      Arrays.fill(lastSkipPosPointer, posOut.getFilePointer());
-      if (fieldHasOffsets) {
-        Arrays.fill(lastStartOffset, 0);
-      }
-      if (fieldHasPayloads) {
-        Arrays.fill(lastPayloadByteUpto, 0);
-      }
-      if (fieldHasOffsets || fieldHasPayloads) {
-        Arrays.fill(lastSkipPayPointer, payOut.getFilePointer());
-      }
-    }
-  }
-
-  /**
-   * Sets the values for the current skip data. 
-   */
-  public void bufferSkip(int doc, int numDocs, long posFP, long payFP, int posBufferUpto, int startOffset, int payloadByteUpto) throws IOException {
-    this.curDoc = doc;
-    this.curDocPointer = docOut.getFilePointer();
-    this.curPosPointer = posFP;
-    this.curPayPointer = payFP;
-    this.curPosBufferUpto = posBufferUpto;
-    this.curPayloadByteUpto = payloadByteUpto;
-    this.curStartOffset = startOffset;
-    bufferSkip(numDocs);
-  }
-  
-  @Override
-  protected void writeSkipData(int level, IndexOutput skipBuffer) throws IOException {
-    int delta = curDoc - lastSkipDoc[level];
-    // if (DEBUG) {
-    //   System.out.println("writeSkipData level=" + level + " lastDoc=" + curDoc + " delta=" + delta + " curDocPointer=" + curDocPointer);
-    // }
-    skipBuffer.writeVInt(delta);
-    lastSkipDoc[level] = curDoc;
-
-    skipBuffer.writeVInt((int) (curDocPointer - lastSkipDocPointer[level]));
-    lastSkipDocPointer[level] = curDocPointer;
-
-    if (fieldHasPositions) {
-      // if (DEBUG) {
-      //   System.out.println("  curPosPointer=" + curPosPointer + " curPosBufferUpto=" + curPosBufferUpto);
-      // }
-      skipBuffer.writeVInt((int) (curPosPointer - lastSkipPosPointer[level]));
-      lastSkipPosPointer[level] = curPosPointer;
-      skipBuffer.writeVInt(curPosBufferUpto);
-
-      if (fieldHasPayloads) {
-        skipBuffer.writeVInt(curPayloadByteUpto);
-      }
-
-      if (fieldHasOffsets) {
-        skipBuffer.writeVInt(curStartOffset - lastStartOffset[level]);
-        lastStartOffset[level] = curStartOffset;
-      }
-
-      if (fieldHasOffsets || fieldHasPayloads) {
-        skipBuffer.writeVInt((int) (curPayPointer - lastSkipPayPointer[level]));
-        lastSkipPayPointer[level] = curPayPointer;
-      }
-    }
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/block/ForUtil.java b/lucene/core/src/java/org/apache/lucene/codecs/block/ForUtil.java
deleted file mode 100644
index b403876..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/block/ForUtil.java
+++ /dev/null
@@ -1,247 +0,0 @@
-package org.apache.lucene.codecs.block;
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Arrays;
-
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.store.DataInput;
-import org.apache.lucene.store.DataOutput;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.packed.PackedInts.Decoder;
-import org.apache.lucene.util.packed.PackedInts.FormatAndBits;
-import org.apache.lucene.util.packed.PackedInts;
-
-import static org.apache.lucene.codecs.block.BlockPostingsFormat.BLOCK_SIZE;
-
-/**
- * Encode all values in normal area with fixed bit width, 
- * which is determined by the max value in this block.
- */
-final class ForUtil {
-
-  /**
-   * Special number of bits per value used whenever all values to encode are equal.
-   */
-  private static final int ALL_VALUES_EQUAL = 0;
-
-  /**
-   * Upper limit of the number of bytes that might be required to stored
-   * <code>BLOCK_SIZE</code> encoded values.
-   */
-  static final int MAX_ENCODED_SIZE = BLOCK_SIZE * 4;
-
-  /**
-   * Upper limit of the number of values that might be decoded in a single call to
-   * {@link #readBlock(IndexInput, byte[], int[])}. Although values after
-   * <code>BLOCK_SIZE</code> are garbage, it is necessary to allocate value buffers
-   * whose size is >= MAX_DATA_SIZE to avoid {@link ArrayIndexOutOfBoundsException}s.
-   */
-  static final int MAX_DATA_SIZE;
-  static {
-    int maxDataSize = 0;
-    for(int version=PackedInts.VERSION_START;version<=PackedInts.VERSION_CURRENT;version++) {
-      for (PackedInts.Format format : PackedInts.Format.values()) {
-        for (int bpv = 1; bpv <= 32; ++bpv) {
-          if (!format.isSupported(bpv)) {
-            continue;
-          }
-          final PackedInts.Decoder decoder = PackedInts.getDecoder(format, version, bpv);
-          final int iterations = computeIterations(decoder);
-          maxDataSize = Math.max(maxDataSize, iterations * decoder.valueCount());
-        }
-      }
-    }
-    MAX_DATA_SIZE = maxDataSize;
-  }
-
-  /**
-   * Compute the number of iterations required to decode <code>BLOCK_SIZE</code>
-   * values with the provided {@link Decoder}.
-   */
-  private static int computeIterations(PackedInts.Decoder decoder) {
-    return (int) Math.ceil((float) BLOCK_SIZE / decoder.valueCount());
-  }
-
-  /**
-   * Compute the number of bytes required to encode a block of values that require
-   * <code>bitsPerValue</code> bits per value with format <code>format</code>.
-   */
-  private static int encodedSize(PackedInts.Format format, int bitsPerValue) {
-    return format.nblocks(bitsPerValue, BLOCK_SIZE) << 3;
-  }
-
-  private final int[] encodedSizes;
-  private final PackedInts.Encoder[] encoders;
-  private final PackedInts.Decoder[] decoders;
-  private final int[] iterations;
-
-  /**
-   * Create a new {@link ForUtil} instance and save state into <code>out</code>.
-   */
-  ForUtil(float acceptableOverheadRatio, DataOutput out) throws IOException {
-    out.writeVInt(PackedInts.VERSION_CURRENT);
-    encodedSizes = new int[33];
-    encoders = new PackedInts.Encoder[33];
-    decoders = new PackedInts.Decoder[33];
-    iterations = new int[33];
-
-    for (int bpv = 1; bpv <= 32; ++bpv) {
-      final FormatAndBits formatAndBits = PackedInts.fastestFormatAndBits(
-          BLOCK_SIZE, bpv, acceptableOverheadRatio);
-      assert formatAndBits.format.isSupported(formatAndBits.bitsPerValue);
-      assert formatAndBits.bitsPerValue <= 32;
-      encodedSizes[bpv] = encodedSize(formatAndBits.format, formatAndBits.bitsPerValue);
-      encoders[bpv] = PackedInts.getEncoder(
-          formatAndBits.format, PackedInts.VERSION_CURRENT, formatAndBits.bitsPerValue);
-      decoders[bpv] = PackedInts.getDecoder(
-          formatAndBits.format, PackedInts.VERSION_CURRENT, formatAndBits.bitsPerValue);
-      iterations[bpv] = computeIterations(decoders[bpv]);
-
-      out.writeVInt(formatAndBits.format.getId() << 5 | (formatAndBits.bitsPerValue - 1));
-    }
-  }
-
-  /**
-   * Restore a {@link ForUtil} from a {@link DataInput}.
-   */
-  ForUtil(DataInput in) throws IOException {
-    int packedIntsVersion = in.readVInt();
-    if (packedIntsVersion != PackedInts.VERSION_START) {
-      throw new CorruptIndexException("expected version=" + PackedInts.VERSION_START + " but got version=" + packedIntsVersion);
-    }
-    encodedSizes = new int[33];
-    encoders = new PackedInts.Encoder[33];
-    decoders = new PackedInts.Decoder[33];
-    iterations = new int[33];
-
-    for (int bpv = 1; bpv <= 32; ++bpv) {
-      final int code = in.readVInt();
-      final int formatId = code >>> 5;
-      final int bitsPerValue = (code & 31) + 1;
-
-      final PackedInts.Format format = PackedInts.Format.byId(formatId);
-      assert format.isSupported(bitsPerValue);
-      encodedSizes[bpv] = encodedSize(format, bitsPerValue);
-      encoders[bpv] = PackedInts.getEncoder(
-          format, packedIntsVersion, bitsPerValue);
-      decoders[bpv] = PackedInts.getDecoder(
-          format, packedIntsVersion, bitsPerValue);
-      iterations[bpv] = computeIterations(decoders[bpv]);
-    }
-  }
-
-  /**
-   * Write a block of data (<code>For</code> format).
-   *
-   * @param data     the data to write
-   * @param encoded  a buffer to use to encode data
-   * @param out      the destination output
-   * @throws IOException
-   */
-  void writeBlock(int[] data, byte[] encoded, IndexOutput out) throws IOException {
-    if (isAllEqual(data)) {
-      out.writeVInt(ALL_VALUES_EQUAL);
-      out.writeVInt(data[0]);
-      return;
-    }
-
-    final int numBits = bitsRequired(data);
-    assert numBits > 0 && numBits <= 32 : numBits;
-    final PackedInts.Encoder encoder = encoders[numBits];
-    final int iters = iterations[numBits];
-    assert iters * encoder.valueCount() >= BLOCK_SIZE;
-    final int encodedSize = encodedSizes[numBits];
-    assert (iters * encoder.blockCount()) << 3 >= encodedSize;
-
-    out.writeVInt(numBits);
-
-    encoder.encode(data, 0, encoded, 0, iters);
-    out.writeBytes(encoded, encodedSize);
-  }
-
-  /**
-   * Read the next block of data (<code>For</code> format).
-   *
-   * @param in        the input to use to read data
-   * @param encoded   a buffer that can be used to store encoded data
-   * @param decoded   where to write decoded data
-   * @throws IOException
-   */
-  void readBlock(IndexInput in, byte[] encoded, int[] decoded) throws IOException {
-    final int numBits = in.readVInt();
-    assert numBits <= 32 : numBits;
-
-    if (numBits == ALL_VALUES_EQUAL) {
-      final int value = in.readVInt();
-      Arrays.fill(decoded, 0, BLOCK_SIZE, value);
-      return;
-    }
-
-    final int encodedSize = encodedSizes[numBits];
-    in.readBytes(encoded, 0, encodedSize);
-
-    final PackedInts.Decoder decoder = decoders[numBits];
-    final int iters = iterations[numBits];
-    assert iters * decoder.valueCount() >= BLOCK_SIZE;
-
-    decoder.decode(encoded, 0, decoded, 0, iters);
-  }
-
-  /**
-   * Skip the next block of data.
-   *
-   * @param in      the input where to read data
-   * @throws IOException
-   */
-  void skipBlock(IndexInput in) throws IOException {
-    final int numBits = in.readVInt();
-    if (numBits == ALL_VALUES_EQUAL) {
-      in.readVInt();
-      return;
-    }
-    assert numBits > 0 && numBits <= 32 : numBits;
-    final int encodedSize = encodedSizes[numBits];
-    in.seek(in.getFilePointer() + encodedSize);
-  }
-
-  private static boolean isAllEqual(final int[] data) {
-    final long v = data[0];
-    for (int i = 1; i < BLOCK_SIZE; ++i) {
-      if (data[i] != v) {
-        return false;
-      }
-    }
-    return true;
-  }
-
-  /**
-   * Compute the number of bits required to serialize any of the longs in
-   * <code>data</code>.
-   */
-  private static int bitsRequired(final int[] data) {
-    long or = 0;
-    for (int i = 0; i < BLOCK_SIZE; ++i) {
-      assert data[i] >= 0;
-      or |= data[i];
-    }
-    return PackedInts.bitsRequired(or);
-  }
-
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/block/package.html b/lucene/core/src/java/org/apache/lucene/codecs/block/package.html
deleted file mode 100644
index c4fe9c6..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/block/package.html
+++ /dev/null
@@ -1,25 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
-</head>
-<body>
-BlockPostingsFormat file format.
-</body>
-</html>
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/bloom/BloomFilterFactory.java b/lucene/core/src/java/org/apache/lucene/codecs/bloom/BloomFilterFactory.java
deleted file mode 100644
index 43fda30..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/bloom/BloomFilterFactory.java
+++ /dev/null
@@ -1,63 +0,0 @@
-package org.apache.lucene.codecs.bloom;
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.util.FuzzySet;
-
-
-/**
- * Class used to create index-time {@link FuzzySet} appropriately configured for
- * each field. Also called to right-size bitsets for serialization.
- * @lucene.experimental
- */
-public abstract class BloomFilterFactory {
-  
-  /**
-   * 
-   * @param state  The content to be indexed
-   * @param info
-   *          the field requiring a BloomFilter
-   * @return An appropriately sized set or null if no BloomFiltering required
-   */
-  public abstract FuzzySet getSetForField(SegmentWriteState state, FieldInfo info);
-  
-  /**
-   * Called when downsizing bitsets for serialization
-   * 
-   * @param fieldInfo
-   *          The field with sparse set bits
-   * @param initialSet
-   *          The bits accumulated
-   * @return null or a hopefully more densely packed, smaller bitset
-   */
-  public FuzzySet downsize(FieldInfo fieldInfo, FuzzySet initialSet) {
-    // Aim for a bitset size that would have 10% of bits set (so 90% of searches
-    // would fail-fast)
-    float targetMaxSaturation = 0.1f;
-    return initialSet.downsize(targetMaxSaturation);
-  }
-
-  /**
-   * Used to determine if the given filter has reached saturation and should be retired i.e. not saved any more
-   * @param bloomFilter The bloomFilter being tested
-   * @param fieldInfo The field with which this filter is associated
-   * @return true if the set has reached saturation and should be retired
-   */
-  public abstract boolean isSaturated(FuzzySet bloomFilter, FieldInfo fieldInfo);
-  
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/bloom/BloomFilteringPostingsFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/bloom/BloomFilteringPostingsFormat.java
deleted file mode 100644
index 72cd94d..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/bloom/BloomFilteringPostingsFormat.java
+++ /dev/null
@@ -1,485 +0,0 @@
-package org.apache.lucene.codecs.bloom;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Comparator;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map;
-import java.util.Map.Entry;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.FieldsConsumer;
-import org.apache.lucene.codecs.FieldsProducer;
-import org.apache.lucene.codecs.PostingsConsumer;
-import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.codecs.TermStats;
-import org.apache.lucene.codecs.TermsConsumer;
-import org.apache.lucene.index.DocsAndPositionsEnum;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.index.Terms;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.store.DataOutput;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.FuzzySet;
-import org.apache.lucene.util.FuzzySet.ContainsResult;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.automaton.CompiledAutomaton;
-import org.apache.lucene.util.hash.MurmurHash2;
-
-/**
- * <p>
- * A {@link PostingsFormat} useful for low doc-frequency fields such as primary
- * keys. Bloom filters are maintained in a ".blm" file which offers "fast-fail"
- * for reads in segments known to have no record of the key. A choice of
- * delegate PostingsFormat is used to record all other Postings data.
- * </p>
- * <p>
- * A choice of {@link BloomFilterFactory} can be passed to tailor Bloom Filter
- * settings on a per-field basis. The default configuration is
- * {@link DefaultBloomFilterFactory} which allocates a ~8mb bitset and hashes
- * values using {@link MurmurHash2}. This should be suitable for most purposes.
- * </p>
- * <p>
- * The format of the blm file is as follows:
- * </p>
- * <ul>
- * <li>BloomFilter (.blm) --&gt; Header, DelegatePostingsFormatName,
- * NumFilteredFields, Filter<sup>NumFilteredFields</sup></li>
- * <li>Filter --&gt; FieldNumber, FuzzySet</li>
- * <li>FuzzySet --&gt;See {@link FuzzySet#serialize(DataOutput)}</li>
- * <li>Header --&gt; {@link CodecUtil#writeHeader CodecHeader}</li>
- * <li>DelegatePostingsFormatName --&gt; {@link DataOutput#writeString(String)
- * String} The name of a ServiceProvider registered {@link PostingsFormat}</li>
- * <li>NumFilteredFields --&gt; {@link DataOutput#writeInt Uint32}</li>
- * <li>FieldNumber --&gt; {@link DataOutput#writeInt Uint32} The number of the
- * field in this segment</li>
- * </ul>
- * @lucene.experimental
- */
-public class BloomFilteringPostingsFormat extends PostingsFormat {
-  
-  public static final String BLOOM_CODEC_NAME = "BloomFilter";
-  public static final int BLOOM_CODEC_VERSION = 1;
-  
-  /** Extension of Bloom Filters file */
-  static final String BLOOM_EXTENSION = "blm";
-  
-  BloomFilterFactory bloomFilterFactory = new DefaultBloomFilterFactory();
-  private PostingsFormat delegatePostingsFormat;
-  
-  /**
-   * Creates Bloom filters for a selection of fields created in the index. This
-   * is recorded as a set of Bitsets held as a segment summary in an additional
-   * "blm" file. This PostingsFormat delegates to a choice of delegate
-   * PostingsFormat for encoding all other postings data.
-   * 
-   * @param delegatePostingsFormat
-   *          The PostingsFormat that records all the non-bloom filter data i.e.
-   *          postings info.
-   * @param bloomFilterFactory
-   *          The {@link BloomFilterFactory} responsible for sizing BloomFilters
-   *          appropriately
-   */
-  public BloomFilteringPostingsFormat(PostingsFormat delegatePostingsFormat,
-      BloomFilterFactory bloomFilterFactory) {
-    super(BLOOM_CODEC_NAME);
-    this.delegatePostingsFormat = delegatePostingsFormat;
-    this.bloomFilterFactory = bloomFilterFactory;
-  }
-  
-  /**
-   * Creates Bloom filters for a selection of fields created in the index. This
-   * is recorded as a set of Bitsets held as a segment summary in an additional
-   * "blm" file. This PostingsFormat delegates to a choice of delegate
-   * PostingsFormat for encoding all other postings data. This choice of
-   * constructor defaults to the {@link DefaultBloomFilterFactory} for
-   * configuring per-field BloomFilters.
-   * 
-   * @param delegatePostingsFormat
-   *          The PostingsFormat that records all the non-bloom filter data i.e.
-   *          postings info.
-   */
-  public BloomFilteringPostingsFormat(PostingsFormat delegatePostingsFormat) {
-    this(delegatePostingsFormat, new DefaultBloomFilterFactory());
-  }
-  
-  // Used only by core Lucene at read-time via Service Provider instantiation -
-  // do not use at Write-time in application code.
-  public BloomFilteringPostingsFormat() {
-    super(BLOOM_CODEC_NAME);
-  }
-  
-  public FieldsConsumer fieldsConsumer(SegmentWriteState state)
-      throws IOException {
-    if (delegatePostingsFormat == null) {
-      throw new UnsupportedOperationException("Error - " + getClass().getName()
-          + " has been constructed without a choice of PostingsFormat");
-    }
-    return new BloomFilteredFieldsConsumer(
-        delegatePostingsFormat.fieldsConsumer(state), state,
-        delegatePostingsFormat);
-  }
-  
-  public FieldsProducer fieldsProducer(SegmentReadState state)
-      throws IOException {
-    return new BloomFilteredFieldsProducer(state);
-  }
-  
-  public class BloomFilteredFieldsProducer extends FieldsProducer {
-    private FieldsProducer delegateFieldsProducer;
-    HashMap<String,FuzzySet> bloomsByFieldName = new HashMap<String,FuzzySet>();
-    
-    public BloomFilteredFieldsProducer(SegmentReadState state)
-        throws IOException {
-      
-      String bloomFileName = IndexFileNames.segmentFileName(
-          state.segmentInfo.name, state.segmentSuffix, BLOOM_EXTENSION);
-      IndexInput bloomIn = null;
-      try {
-        bloomIn = state.dir.openInput(bloomFileName, state.context);
-        CodecUtil.checkHeader(bloomIn, BLOOM_CODEC_NAME, BLOOM_CODEC_VERSION,
-            BLOOM_CODEC_VERSION);
-        // // Load the hash function used in the BloomFilter
-        // hashFunction = HashFunction.forName(bloomIn.readString());
-        // Load the delegate postings format
-        PostingsFormat delegatePostingsFormat = PostingsFormat.forName(bloomIn
-            .readString());
-        
-        this.delegateFieldsProducer = delegatePostingsFormat
-            .fieldsProducer(state);
-        int numBlooms = bloomIn.readInt();
-        for (int i = 0; i < numBlooms; i++) {
-          int fieldNum = bloomIn.readInt();
-          FuzzySet bloom = FuzzySet.deserialize(bloomIn);
-          FieldInfo fieldInfo = state.fieldInfos.fieldInfo(fieldNum);
-          bloomsByFieldName.put(fieldInfo.name, bloom);
-        }
-      } finally {
-        IOUtils.close(bloomIn);
-      }
-      
-    }
-    
-    public Iterator<String> iterator() {
-      return delegateFieldsProducer.iterator();
-    }
-    
-    public void close() throws IOException {
-      delegateFieldsProducer.close();
-    }
-    
-    public Terms terms(String field) throws IOException {
-      FuzzySet filter = bloomsByFieldName.get(field);
-      if (filter == null) {
-        return delegateFieldsProducer.terms(field);
-      } else {
-        Terms result = delegateFieldsProducer.terms(field);
-        if (result == null) {
-          return null;
-        }
-        return new BloomFilteredTerms(result, filter);
-      }
-    }
-    
-    public int size() {
-      return delegateFieldsProducer.size();
-    }
-    
-    class BloomFilteredTerms extends Terms {
-      private Terms delegateTerms;
-      private FuzzySet filter;
-      
-      public BloomFilteredTerms(Terms terms, FuzzySet filter) {
-        this.delegateTerms = terms;
-        this.filter = filter;
-      }
-      
-      @Override
-      public TermsEnum intersect(CompiledAutomaton compiled,
-          final BytesRef startTerm) throws IOException {
-        return delegateTerms.intersect(compiled, startTerm);
-      }
-      
-      @Override
-      public TermsEnum iterator(TermsEnum reuse) throws IOException {
-        TermsEnum result;
-        if ((reuse != null) && (reuse instanceof BloomFilteredTermsEnum)) {
-          // recycle the existing BloomFilteredTermsEnum by asking the delegate
-          // to recycle its contained TermsEnum
-          BloomFilteredTermsEnum bfte = (BloomFilteredTermsEnum) reuse;
-          if (bfte.filter == filter) {
-            bfte.delegateTermsEnum = delegateTerms
-                .iterator(bfte.delegateTermsEnum);
-            return bfte;
-          }
-        }
-        // We have been handed something we cannot reuse (either null, wrong
-        // class or wrong filter) so allocate a new object
-        result = new BloomFilteredTermsEnum(delegateTerms.iterator(reuse),
-            filter);
-        return result;
-      }
-      
-      @Override
-      public Comparator<BytesRef> getComparator() throws IOException {
-        return delegateTerms.getComparator();
-      }
-      
-      @Override
-      public long size() throws IOException {
-        return delegateTerms.size();
-      }
-      
-      @Override
-      public long getSumTotalTermFreq() throws IOException {
-        return delegateTerms.getSumTotalTermFreq();
-      }
-      
-      @Override
-      public long getSumDocFreq() throws IOException {
-        return delegateTerms.getSumDocFreq();
-      }
-      
-      @Override
-      public int getDocCount() throws IOException {
-        return delegateTerms.getDocCount();
-      }
-
-      @Override
-      public boolean hasOffsets() {
-        return delegateTerms.hasOffsets();
-      }
-
-      @Override
-      public boolean hasPositions() {
-        return delegateTerms.hasPositions();
-      }
-      
-      @Override
-      public boolean hasPayloads() {
-        return delegateTerms.hasPayloads();
-      }
-    }
-    
-    class BloomFilteredTermsEnum extends TermsEnum {
-      
-      TermsEnum delegateTermsEnum;
-      private FuzzySet filter;
-      
-      public BloomFilteredTermsEnum(TermsEnum iterator, FuzzySet filter) {
-        this.delegateTermsEnum = iterator;
-        this.filter = filter;
-      }
-      
-      @Override
-      public final BytesRef next() throws IOException {
-        return delegateTermsEnum.next();
-      }
-      
-      @Override
-      public final Comparator<BytesRef> getComparator() {
-        return delegateTermsEnum.getComparator();
-      }
-      
-      @Override
-      public final boolean seekExact(BytesRef text, boolean useCache)
-          throws IOException {
-        // The magical fail-fast speed up that is the entire point of all of
-        // this code - save a disk seek if there is a match on an in-memory
-        // structure
-        // that may occasionally give a false positive but guaranteed no false
-        // negatives
-        if (filter.contains(text) == ContainsResult.NO) {
-          return false;
-        }
-        return delegateTermsEnum.seekExact(text, useCache);
-      }
-      
-      @Override
-      public final SeekStatus seekCeil(BytesRef text, boolean useCache)
-          throws IOException {
-        return delegateTermsEnum.seekCeil(text, useCache);
-      }
-      
-      @Override
-      public final void seekExact(long ord) throws IOException {
-        delegateTermsEnum.seekExact(ord);
-      }
-      
-      @Override
-      public final BytesRef term() throws IOException {
-        return delegateTermsEnum.term();
-      }
-      
-      @Override
-      public final long ord() throws IOException {
-        return delegateTermsEnum.ord();
-      }
-      
-      @Override
-      public final int docFreq() throws IOException {
-        return delegateTermsEnum.docFreq();
-      }
-      
-      @Override
-      public final long totalTermFreq() throws IOException {
-        return delegateTermsEnum.totalTermFreq();
-      }
-      
-
-      @Override
-      public DocsAndPositionsEnum docsAndPositions(Bits liveDocs,
-          DocsAndPositionsEnum reuse, int flags) throws IOException {
-        return delegateTermsEnum.docsAndPositions(liveDocs, reuse, flags);
-      }
-
-      @Override
-      public DocsEnum docs(Bits liveDocs, DocsEnum reuse, int flags)
-          throws IOException {
-        return delegateTermsEnum.docs(liveDocs, reuse, flags);
-      }
-      
-      
-    }
-    
-  }
-  
-  class BloomFilteredFieldsConsumer extends FieldsConsumer {
-    private FieldsConsumer delegateFieldsConsumer;
-    private Map<FieldInfo,FuzzySet> bloomFilters = new HashMap<FieldInfo,FuzzySet>();
-    private SegmentWriteState state;
-    
-    // private PostingsFormat delegatePostingsFormat;
-    
-    public BloomFilteredFieldsConsumer(FieldsConsumer fieldsConsumer,
-        SegmentWriteState state, PostingsFormat delegatePostingsFormat) {
-      this.delegateFieldsConsumer = fieldsConsumer;
-      // this.delegatePostingsFormat=delegatePostingsFormat;
-      this.state = state;
-    }
-    
-    @Override
-    public TermsConsumer addField(FieldInfo field) throws IOException {
-      FuzzySet bloomFilter = bloomFilterFactory.getSetForField(state,field);
-      if (bloomFilter != null) {
-        assert bloomFilters.containsKey(field) == false;
-        bloomFilters.put(field, bloomFilter);
-        return new WrappedTermsConsumer(delegateFieldsConsumer.addField(field),bloomFilter);
-      } else {
-        // No, use the unfiltered fieldsConsumer - we are not interested in
-        // recording any term Bitsets.
-        return delegateFieldsConsumer.addField(field);
-      }
-    }
-    
-    @Override
-    public void close() throws IOException {
-      delegateFieldsConsumer.close();
-      // Now we are done accumulating values for these fields
-      List<Entry<FieldInfo,FuzzySet>> nonSaturatedBlooms = new ArrayList<Map.Entry<FieldInfo,FuzzySet>>();
-      
-      for (Entry<FieldInfo,FuzzySet> entry : bloomFilters.entrySet()) {
-        FuzzySet bloomFilter = entry.getValue();
-        if(!bloomFilterFactory.isSaturated(bloomFilter,entry.getKey())){          
-          nonSaturatedBlooms.add(entry);
-        }
-      }
-      String bloomFileName = IndexFileNames.segmentFileName(
-          state.segmentInfo.name, state.segmentSuffix, BLOOM_EXTENSION);
-      IndexOutput bloomOutput = null;
-      try {
-        bloomOutput = state.directory
-            .createOutput(bloomFileName, state.context);
-        CodecUtil.writeHeader(bloomOutput, BLOOM_CODEC_NAME,
-            BLOOM_CODEC_VERSION);
-        // remember the name of the postings format we will delegate to
-        bloomOutput.writeString(delegatePostingsFormat.getName());
-        
-        // First field in the output file is the number of fields+blooms saved
-        bloomOutput.writeInt(nonSaturatedBlooms.size());
-        for (Entry<FieldInfo,FuzzySet> entry : nonSaturatedBlooms) {
-          FieldInfo fieldInfo = entry.getKey();
-          FuzzySet bloomFilter = entry.getValue();
-          bloomOutput.writeInt(fieldInfo.number);
-          saveAppropriatelySizedBloomFilter(bloomOutput, bloomFilter, fieldInfo);
-        }
-      } finally {
-        IOUtils.close(bloomOutput);
-      }
-      //We are done with large bitsets so no need to keep them hanging around
-      bloomFilters.clear(); 
-    }
-    
-    private void saveAppropriatelySizedBloomFilter(IndexOutput bloomOutput,
-        FuzzySet bloomFilter, FieldInfo fieldInfo) throws IOException {
-      
-      FuzzySet rightSizedSet = bloomFilterFactory.downsize(fieldInfo,
-          bloomFilter);
-      if (rightSizedSet == null) {
-        rightSizedSet = bloomFilter;
-      }
-      rightSizedSet.serialize(bloomOutput);
-    }
-    
-  }
-  
-  class WrappedTermsConsumer extends TermsConsumer {
-    private TermsConsumer delegateTermsConsumer;
-    private FuzzySet bloomFilter;
-    
-    public WrappedTermsConsumer(TermsConsumer termsConsumer,FuzzySet bloomFilter) {
-      this.delegateTermsConsumer = termsConsumer;
-      this.bloomFilter = bloomFilter;
-    }
-    
-    public PostingsConsumer startTerm(BytesRef text) throws IOException {
-      return delegateTermsConsumer.startTerm(text);
-    }
-    
-    public void finishTerm(BytesRef text, TermStats stats) throws IOException {
-      
-      // Record this term in our BloomFilter
-      if (stats.docFreq > 0) {
-        bloomFilter.addValue(text);
-      }
-      delegateTermsConsumer.finishTerm(text, stats);
-    }
-    
-    public void finish(long sumTotalTermFreq, long sumDocFreq, int docCount)
-        throws IOException {
-      delegateTermsConsumer.finish(sumTotalTermFreq, sumDocFreq, docCount);
-    }
-    
-    public Comparator<BytesRef> getComparator() throws IOException {
-      return delegateTermsConsumer.getComparator();
-    }
-    
-  }
-  
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/bloom/DefaultBloomFilterFactory.java b/lucene/core/src/java/org/apache/lucene/codecs/bloom/DefaultBloomFilterFactory.java
deleted file mode 100644
index 804f56b..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/bloom/DefaultBloomFilterFactory.java
+++ /dev/null
@@ -1,44 +0,0 @@
-package org.apache.lucene.codecs.bloom;
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.util.FuzzySet;
-import org.apache.lucene.util.hash.HashFunction;
-import org.apache.lucene.util.hash.MurmurHash2;
-
-/**
- * Default policy is to allocate a bitset with 10% saturation given a unique term per document.
- * Bits are set via MurmurHash2 hashing function.
- *  @lucene.experimental
- */
-public class DefaultBloomFilterFactory extends BloomFilterFactory {
-  
-  @Override
-  public FuzzySet getSetForField(SegmentWriteState state,FieldInfo info) {
-    //Assume all of the docs have a unique term (e.g. a primary key) and we hope to maintain a set with 10% of bits set
-    return FuzzySet.createSetBasedOnQuality(state.segmentInfo.getDocCount(), 0.10f,  new MurmurHash2());
-  }
-  
-  @Override
-  public boolean isSaturated(FuzzySet bloomFilter, FieldInfo fieldInfo) {
-    // Don't bother saving bitsets if >90% of bits are set - we don't want to
-    // throw any more memory at this problem.
-    return bloomFilter.getSaturation() > 0.9f;
-  }
-  
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/bloom/package.html b/lucene/core/src/java/org/apache/lucene/codecs/bloom/package.html
deleted file mode 100644
index a0c591a..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/bloom/package.html
+++ /dev/null
@@ -1,25 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
-</head>
-<body>
-Codec PostingsFormat for fast access to low-frequency terms such as primary key fields.
-</body>
-</html>
\ No newline at end of file
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/intblock/FixedIntBlockIndexInput.java b/lucene/core/src/java/org/apache/lucene/codecs/intblock/FixedIntBlockIndexInput.java
deleted file mode 100644
index 33457e4..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/intblock/FixedIntBlockIndexInput.java
+++ /dev/null
@@ -1,171 +0,0 @@
-package org.apache.lucene.codecs.intblock;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/** Naive int block API that writes vInts.  This is
- *  expected to give poor performance; it's really only for
- *  testing the pluggability.  One should typically use pfor instead. */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.sep.IntIndexInput;
-import org.apache.lucene.store.DataInput;
-import org.apache.lucene.store.IndexInput;
-
-/** Abstract base class that reads fixed-size blocks of ints
- *  from an IndexInput.  While this is a simple approach, a
- *  more performant approach would directly create an impl
- *  of IntIndexInput inside Directory.  Wrapping a generic
- *  IndexInput will likely cost performance.
- *
- * @lucene.experimental
- */
-public abstract class FixedIntBlockIndexInput extends IntIndexInput {
-
-  private final IndexInput in;
-  protected final int blockSize;
-  
-  public FixedIntBlockIndexInput(final IndexInput in) throws IOException {
-    this.in = in;
-    blockSize = in.readVInt();
-  }
-
-  @Override
-  public Reader reader() throws IOException {
-    final int[] buffer = new int[blockSize];
-    final IndexInput clone = in.clone();
-    // TODO: can this be simplified?
-    return new Reader(clone, buffer, this.getBlockReader(clone, buffer));
-  }
-
-  @Override
-  public void close() throws IOException {
-    in.close();
-  }
-
-  @Override
-  public Index index() {
-    return new Index();
-  }
-
-  protected abstract BlockReader getBlockReader(IndexInput in, int[] buffer) throws IOException;
-
-  /**
-   * Interface for fixed-size block decoders.
-   * <p>
-   * Implementations should decode into the buffer in {@link #readBlock}.
-   */
-  public interface BlockReader {
-    public void readBlock() throws IOException;
-  }
-
-  private static class Reader extends IntIndexInput.Reader {
-    private final IndexInput in;
-    private final BlockReader blockReader;
-    private final int blockSize;
-    private final int[] pending;
-
-    private int upto;
-    private boolean seekPending;
-    private long pendingFP;
-    private long lastBlockFP = -1;
-
-    public Reader(final IndexInput in, final int[] pending, final BlockReader blockReader) {
-      this.in = in;
-      this.pending = pending;
-      this.blockSize = pending.length;
-      this.blockReader = blockReader;
-      upto = blockSize;
-    }
-
-    void seek(final long fp, final int upto) {
-      assert upto < blockSize;
-      if (seekPending || fp != lastBlockFP) {
-        pendingFP = fp;
-        seekPending = true;
-      }
-      this.upto = upto;
-    }
-
-    @Override
-    public int next() throws IOException {
-      if (seekPending) {
-        // Seek & load new block
-        in.seek(pendingFP);
-        lastBlockFP = pendingFP;
-        blockReader.readBlock();
-        seekPending = false;
-      } else if (upto == blockSize) {
-        // Load new block
-        lastBlockFP = in.getFilePointer();
-        blockReader.readBlock();
-        upto = 0;
-      }
-      return pending[upto++];
-    }
-  }
-
-  private class Index extends IntIndexInput.Index {
-    private long fp;
-    private int upto;
-
-    @Override
-    public void read(final DataInput indexIn, final boolean absolute) throws IOException {
-      if (absolute) {
-        upto = indexIn.readVInt();
-        fp = indexIn.readVLong();
-      } else {
-        final int uptoDelta = indexIn.readVInt();
-        if ((uptoDelta & 1) == 1) {
-          // same block
-          upto += uptoDelta >>> 1;
-        } else {
-          // new block
-          upto = uptoDelta >>> 1;
-          fp += indexIn.readVLong();
-        }
-      }
-      assert upto < blockSize;
-    }
-
-    @Override
-    public void seek(final IntIndexInput.Reader other) throws IOException {
-      ((Reader) other).seek(fp, upto);
-    }
-
-    @Override
-    public void copyFrom(final IntIndexInput.Index other) {
-      final Index idx = (Index) other;
-      fp = idx.fp;
-      upto = idx.upto;
-    }
-
-    @Override
-    public Index clone() {
-      Index other = new Index();
-      other.fp = fp;
-      other.upto = upto;
-      return other;
-    }
-    
-    @Override
-    public String toString() {
-      return "fp=" + fp + " upto=" + upto;
-    }
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/intblock/FixedIntBlockIndexOutput.java b/lucene/core/src/java/org/apache/lucene/codecs/intblock/FixedIntBlockIndexOutput.java
deleted file mode 100644
index 004e51c..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/intblock/FixedIntBlockIndexOutput.java
+++ /dev/null
@@ -1,127 +0,0 @@
-package org.apache.lucene.codecs.intblock;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/** Naive int block API that writes vInts.  This is
- *  expected to give poor performance; it's really only for
- *  testing the pluggability.  One should typically use pfor instead. */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.sep.IntIndexOutput;
-import org.apache.lucene.store.IndexOutput;
-
-/** Abstract base class that writes fixed-size blocks of ints
- *  to an IndexOutput.  While this is a simple approach, a
- *  more performant approach would directly create an impl
- *  of IntIndexOutput inside Directory.  Wrapping a generic
- *  IndexInput will likely cost performance.
- *
- * @lucene.experimental
- */
-public abstract class FixedIntBlockIndexOutput extends IntIndexOutput {
-
-  protected final IndexOutput out;
-  private final int blockSize;
-  protected final int[] buffer;
-  private int upto;
-
-  protected FixedIntBlockIndexOutput(IndexOutput out, int fixedBlockSize) throws IOException {
-    blockSize = fixedBlockSize;
-    this.out = out;
-    out.writeVInt(blockSize);
-    buffer = new int[blockSize];
-  }
-
-  protected abstract void flushBlock() throws IOException;
-
-  @Override
-  public Index index() throws IOException {
-    return new Index();
-  }
-
-  private class Index extends IntIndexOutput.Index {
-    long fp;
-    int upto;
-    long lastFP;
-    int lastUpto;
-
-    @Override
-    public void mark() throws IOException {
-      fp = out.getFilePointer();
-      upto = FixedIntBlockIndexOutput.this.upto;
-    }
-
-    @Override
-    public void copyFrom(IntIndexOutput.Index other, boolean copyLast) throws IOException {
-      Index idx = (Index) other;
-      fp = idx.fp;
-      upto = idx.upto;
-      if (copyLast) {
-        lastFP = fp;
-        lastUpto = upto;
-      }
-    }
-
-    @Override
-    public void write(IndexOutput indexOut, boolean absolute) throws IOException {
-      if (absolute) {
-        indexOut.writeVInt(upto);
-        indexOut.writeVLong(fp);
-      } else if (fp == lastFP) {
-        // same block
-        assert upto >= lastUpto;
-        int uptoDelta = upto - lastUpto;
-        indexOut.writeVInt(uptoDelta << 1 | 1);
-      } else {      
-        // new block
-        indexOut.writeVInt(upto << 1);
-        indexOut.writeVLong(fp - lastFP);
-      }
-      lastUpto = upto;
-      lastFP = fp;
-    }
-
-    @Override
-    public String toString() {
-      return "fp=" + fp + " upto=" + upto;
-    }
-  }
-
-  @Override
-  public void write(int v) throws IOException {
-    buffer[upto++] = v;
-    if (upto == blockSize) {
-      flushBlock();
-      upto = 0;
-    }
-  }
-
-  @Override
-  public void close() throws IOException {
-    try {
-      if (upto > 0) {
-        // NOTE: entries in the block after current upto are
-        // invalid
-        flushBlock();
-      }
-    } finally {
-      out.close();
-    }
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/intblock/VariableIntBlockIndexInput.java b/lucene/core/src/java/org/apache/lucene/codecs/intblock/VariableIntBlockIndexInput.java
deleted file mode 100644
index 9505c1e..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/intblock/VariableIntBlockIndexInput.java
+++ /dev/null
@@ -1,198 +0,0 @@
-package org.apache.lucene.codecs.intblock;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/** Naive int block API that writes vInts.  This is
- *  expected to give poor performance; it's really only for
- *  testing the pluggability.  One should typically use pfor instead. */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.sep.IntIndexInput;
-import org.apache.lucene.store.DataInput;
-import org.apache.lucene.store.IndexInput;
-
-// TODO: much of this can be shared code w/ the fixed case
-
-/** Abstract base class that reads variable-size blocks of ints
- *  from an IndexInput.  While this is a simple approach, a
- *  more performant approach would directly create an impl
- *  of IntIndexInput inside Directory.  Wrapping a generic
- *  IndexInput will likely cost performance.
- *
- * @lucene.experimental
- */
-public abstract class VariableIntBlockIndexInput extends IntIndexInput {
-
-  protected final IndexInput in;
-  protected final int maxBlockSize;
-
-  protected VariableIntBlockIndexInput(final IndexInput in) throws IOException {
-    this.in = in;
-    maxBlockSize = in.readInt();
-  }
-
-  @Override
-  public Reader reader() throws IOException {
-    final int[] buffer = new int[maxBlockSize];
-    final IndexInput clone = in.clone();
-    // TODO: can this be simplified?
-    return new Reader(clone, buffer, this.getBlockReader(clone, buffer));
-  }
-
-  @Override
-  public void close() throws IOException {
-    in.close();
-  }
-
-  @Override
-  public Index index() {
-    return new Index();
-  }
-
-  protected abstract BlockReader getBlockReader(IndexInput in, int[] buffer) throws IOException;
-
-  /**
-   * Interface for variable-size block decoders.
-   * <p>
-   * Implementations should decode into the buffer in {@link #readBlock}.
-   */
-  public interface BlockReader {
-    public int readBlock() throws IOException;
-    public void seek(long pos) throws IOException;
-  }
-
-  private static class Reader extends IntIndexInput.Reader {
-    private final IndexInput in;
-
-    public final int[] pending;
-    int upto;
-
-    private boolean seekPending;
-    private long pendingFP;
-    private int pendingUpto;
-    private long lastBlockFP;
-    private int blockSize;
-    private final BlockReader blockReader;
-
-    public Reader(final IndexInput in, final int[] pending, final BlockReader blockReader) {
-      this.in = in;
-      this.pending = pending;
-      this.blockReader = blockReader;
-    }
-
-    void seek(final long fp, final int upto) {
-      // TODO: should we do this in real-time, not lazy?
-      pendingFP = fp;
-      pendingUpto = upto;
-      assert pendingUpto >= 0: "pendingUpto=" + pendingUpto;
-      seekPending = true;
-    }
-
-    private final void maybeSeek() throws IOException {
-      if (seekPending) {
-        if (pendingFP != lastBlockFP) {
-          // need new block
-          in.seek(pendingFP);
-          blockReader.seek(pendingFP);
-          lastBlockFP = pendingFP;
-          blockSize = blockReader.readBlock();
-        }
-        upto = pendingUpto;
-
-        // TODO: if we were more clever when writing the
-        // index, such that a seek point wouldn't be written
-        // until the int encoder "committed", we could avoid
-        // this (likely minor) inefficiency:
-
-        // This is necessary for int encoders that are
-        // non-causal, ie must see future int values to
-        // encode the current ones.
-        while(upto >= blockSize) {
-          upto -= blockSize;
-          lastBlockFP = in.getFilePointer();
-          blockSize = blockReader.readBlock();
-        }
-        seekPending = false;
-      }
-    }
-
-    @Override
-    public int next() throws IOException {
-      this.maybeSeek();
-      if (upto == blockSize) {
-        lastBlockFP = in.getFilePointer();
-        blockSize = blockReader.readBlock();
-        upto = 0;
-      }
-
-      return pending[upto++];
-    }
-  }
-
-  private class Index extends IntIndexInput.Index {
-    private long fp;
-    private int upto;
-
-    @Override
-    public void read(final DataInput indexIn, final boolean absolute) throws IOException {
-      if (absolute) {
-        upto = indexIn.readVInt();
-        fp = indexIn.readVLong();
-      } else {
-        final int uptoDelta = indexIn.readVInt();
-        if ((uptoDelta & 1) == 1) {
-          // same block
-          upto += uptoDelta >>> 1;
-        } else {
-          // new block
-          upto = uptoDelta >>> 1;
-          fp += indexIn.readVLong();
-        }
-      }
-      // TODO: we can't do this assert because non-causal
-      // int encoders can have upto over the buffer size
-      //assert upto < maxBlockSize: "upto=" + upto + " max=" + maxBlockSize;
-    }
-
-    @Override
-    public String toString() {
-      return "VarIntBlock.Index fp=" + fp + " upto=" + upto + " maxBlock=" + maxBlockSize;
-    }
-
-    @Override
-    public void seek(final IntIndexInput.Reader other) throws IOException {
-      ((Reader) other).seek(fp, upto);
-    }
-
-    @Override
-    public void copyFrom(final IntIndexInput.Index other) {
-      final Index idx = (Index) other;
-      fp = idx.fp;
-      upto = idx.upto;
-    }
-
-    @Override
-    public Index clone() {
-      Index other = new Index();
-      other.fp = fp;
-      other.upto = upto;
-      return other;
-    }
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/intblock/VariableIntBlockIndexOutput.java b/lucene/core/src/java/org/apache/lucene/codecs/intblock/VariableIntBlockIndexOutput.java
deleted file mode 100644
index 9d31c3c..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/intblock/VariableIntBlockIndexOutput.java
+++ /dev/null
@@ -1,135 +0,0 @@
-package org.apache.lucene.codecs.intblock;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/** Naive int block API that writes vInts.  This is
- *  expected to give poor performance; it's really only for
- *  testing the pluggability.  One should typically use pfor instead. */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.sep.IntIndexOutput;
-import org.apache.lucene.store.IndexOutput;
-
-// TODO: much of this can be shared code w/ the fixed case
-
-/** Abstract base class that writes variable-size blocks of ints
- *  to an IndexOutput.  While this is a simple approach, a
- *  more performant approach would directly create an impl
- *  of IntIndexOutput inside Directory.  Wrapping a generic
- *  IndexInput will likely cost performance.
- *
- * @lucene.experimental
- */
-public abstract class VariableIntBlockIndexOutput extends IntIndexOutput {
-
-  protected final IndexOutput out;
-
-  private int upto;
-  private boolean hitExcDuringWrite;
-
-  // TODO what Var-Var codecs exist in practice... and what are there blocksizes like?
-  // if its less than 128 we should set that as max and use byte?
-
-  /** NOTE: maxBlockSize must be the maximum block size 
-   *  plus the max non-causal lookahead of your codec.  EG Simple9
-   *  requires lookahead=1 because on seeing the Nth value
-   *  it knows it must now encode the N-1 values before it. */
-  protected VariableIntBlockIndexOutput(IndexOutput out, int maxBlockSize) throws IOException {
-    this.out = out;
-    out.writeInt(maxBlockSize);
-  }
-
-  /** Called one value at a time.  Return the number of
-   *  buffered input values that have been written to out. */
-  protected abstract int add(int value) throws IOException;
-
-  @Override
-  public Index index() throws IOException {
-    return new Index();
-  }
-
-  private class Index extends IntIndexOutput.Index {
-    long fp;
-    int upto;
-    long lastFP;
-    int lastUpto;
-
-    @Override
-    public void mark() throws IOException {
-      fp = out.getFilePointer();
-      upto = VariableIntBlockIndexOutput.this.upto;
-    }
-
-    @Override
-    public void copyFrom(IntIndexOutput.Index other, boolean copyLast) throws IOException {
-      Index idx = (Index) other;
-      fp = idx.fp;
-      upto = idx.upto;
-      if (copyLast) {
-        lastFP = fp;
-        lastUpto = upto;
-      }
-    }
-
-    @Override
-    public void write(IndexOutput indexOut, boolean absolute) throws IOException {
-      assert upto >= 0;
-      if (absolute) {
-        indexOut.writeVInt(upto);
-        indexOut.writeVLong(fp);
-      } else if (fp == lastFP) {
-        // same block
-        assert upto >= lastUpto;
-        int uptoDelta = upto - lastUpto;
-        indexOut.writeVInt(uptoDelta << 1 | 1);
-      } else {      
-        // new block
-        indexOut.writeVInt(upto << 1);
-        indexOut.writeVLong(fp - lastFP);
-      }
-      lastUpto = upto;
-      lastFP = fp;
-    }
-  }
-
-  @Override
-  public void write(int v) throws IOException {
-    hitExcDuringWrite = true;
-    upto -= add(v)-1;
-    hitExcDuringWrite = false;
-    assert upto >= 0;
-  }
-
-  @Override
-  public void close() throws IOException {
-    try {
-      if (!hitExcDuringWrite) {
-        // stuff 0s in until the "real" data is flushed:
-        int stuffed = 0;
-        while(upto > stuffed) {
-          upto -= add(0)-1;
-          assert upto >= 0;
-          stuffed += 1;
-        }
-      }
-    } finally {
-      out.close();
-    }
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/intblock/package.html b/lucene/core/src/java/org/apache/lucene/codecs/intblock/package.html
deleted file mode 100644
index 403ea1b..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/intblock/package.html
+++ /dev/null
@@ -1,25 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
-</head>
-<body>
-Intblock: base support for fixed or variable length block integer encoders
-</body>
-</html>
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/memory/DirectPostingsFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/memory/DirectPostingsFormat.java
deleted file mode 100644
index 98e1a36..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/memory/DirectPostingsFormat.java
+++ /dev/null
@@ -1,2194 +0,0 @@
-package org.apache.lucene.codecs.memory;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Collections;
-import java.util.Comparator;
-import java.util.Iterator;
-import java.util.Map;
-import java.util.TreeMap;
-
-import org.apache.lucene.codecs.FieldsConsumer;
-import org.apache.lucene.codecs.FieldsProducer;
-import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.codecs.lucene40.Lucene40PostingsFormat; // javadocs
-import org.apache.lucene.index.DocsAndPositionsEnum;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.Fields;
-import org.apache.lucene.index.OrdTermState;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.index.TermState;
-import org.apache.lucene.index.Terms;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.RAMOutputStream;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.automaton.CompiledAutomaton;
-import org.apache.lucene.util.automaton.RunAutomaton;
-import org.apache.lucene.util.automaton.Transition;
-
-// TODO: 
-//   - build depth-N prefix hash?
-//   - or: longer dense skip lists than just next byte?
-
-/** Wraps {@link Lucene40PostingsFormat} format for on-disk
- *  storage, but then at read time loads and stores all
- *  terms & postings directly in RAM as byte[], int[].
- *
- *  <p><b><font color=red>WARNING</font></b>: This is
- *  exceptionally RAM intensive: it makes no effort to
- *  compress the postings data, storing terms as separate
- *  byte[] and postings as separate int[], but as a result it 
- *  gives substantial increase in search performance.
- *
- *  <p>This postings format supports {@link TermsEnum#ord}
- *  and {@link TermsEnum#seekExact(long)}.
-
- *  <p>Because this holds all term bytes as a single
- *  byte[], you cannot have more than 2.1GB worth of term
- *  bytes in a single segment.
- *
- * @lucene.experimental */
-
-public class DirectPostingsFormat extends PostingsFormat {
-
-  private final int minSkipCount;
-  private final int lowFreqCutoff;
-
-  private final static int DEFAULT_MIN_SKIP_COUNT = 8;
-  private final static int DEFAULT_LOW_FREQ_CUTOFF = 32;
-
-  //private static final boolean DEBUG = true;
-
-  // TODO: allow passing/wrapping arbitrary postings format?
-
-  public DirectPostingsFormat() {
-    this(DEFAULT_MIN_SKIP_COUNT, DEFAULT_LOW_FREQ_CUTOFF);
-  }
-  
-  /** minSkipCount is how many terms in a row must have the
-   *  same prefix before we put a skip pointer down.  Terms
-   *  with docFreq <= lowFreqCutoff will use a single int[]
-   *  to hold all docs, freqs, position and offsets; terms
-   *  with higher docFreq will use separate arrays. */
-  public DirectPostingsFormat(int minSkipCount, int lowFreqCutoff) {
-    super("Direct");
-    this.minSkipCount = minSkipCount;
-    this.lowFreqCutoff = lowFreqCutoff;
-  }
-  
-  @Override
-  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-    return PostingsFormat.forName("Lucene40").fieldsConsumer(state);
-  }
-
-  @Override
-  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
-    FieldsProducer postings = PostingsFormat.forName("Lucene40").fieldsProducer(state);
-    if (state.context.context != IOContext.Context.MERGE) {
-      FieldsProducer loadedPostings;
-      try {
-        loadedPostings = new DirectFields(state, postings, minSkipCount, lowFreqCutoff);
-      } finally {
-        postings.close();
-      }
-      return loadedPostings;
-    } else {
-      // Don't load postings for merge:
-      return postings;
-    }
-  }
-
-  private static final class DirectFields extends FieldsProducer {
-    private final Map<String,DirectField> fields = new TreeMap<String,DirectField>();
-
-    public DirectFields(SegmentReadState state, Fields fields, int minSkipCount, int lowFreqCutoff) throws IOException {
-      for (String field : fields) {
-        this.fields.put(field, new DirectField(state, field, fields.terms(field), minSkipCount, lowFreqCutoff));
-      }
-    }
-
-    @Override
-    public Iterator<String> iterator() {
-      return Collections.unmodifiableSet(fields.keySet()).iterator();
-    }
-
-    @Override
-    public Terms terms(String field) {
-      return fields.get(field);
-    }
-
-    @Override
-    public int size() {
-      return fields.size();
-    }
-
-    @Override
-    public void close() {
-    }
-  }
-
-  private final static class DirectField extends Terms {
-
-    private static abstract class TermAndSkip {
-      public int[] skips;
-    }
-
-    private static final class LowFreqTerm extends TermAndSkip {
-      public final int[] postings;
-      public final byte[] payloads;
-      public final int docFreq;
-      public final int totalTermFreq;
-
-      public LowFreqTerm(int[] postings, byte[] payloads, int docFreq, int totalTermFreq) {
-        this.postings = postings;
-        this.payloads = payloads;
-        this.docFreq = docFreq;
-        this.totalTermFreq = totalTermFreq;
-      }
-    }
-
-    // TODO: maybe specialize into prx/no-prx/no-frq cases?
-    private static final class HighFreqTerm extends TermAndSkip {
-      public final long totalTermFreq;
-      public final int[] docIDs;
-      public final int[] freqs;
-      public final int[][] positions;
-      public final byte[][][] payloads;
-
-      public HighFreqTerm(int[] docIDs, int[] freqs, int[][] positions, byte[][][] payloads, long totalTermFreq) {
-        this.docIDs = docIDs;
-        this.freqs = freqs;
-        this.positions = positions;
-        this.payloads = payloads;
-        this.totalTermFreq = totalTermFreq;
-      }
-    }
-
-    private final byte[] termBytes;
-    private final int[] termOffsets;
-
-    private final int[] skips;
-    private final int[] skipOffsets;
-
-    private final TermAndSkip[] terms;
-    private final boolean hasFreq;
-    private final boolean hasPos;
-    private final boolean hasOffsets;
-    private final boolean hasPayloads;
-    private final long sumTotalTermFreq;
-    private final int docCount;
-    private final long sumDocFreq;
-    private int skipCount;
-
-    // TODO: maybe make a separate builder?  These are only
-    // used during load:
-    private int count;
-    private int[] sameCounts = new int[10];
-    private final int minSkipCount;
-
-    private final static class IntArrayWriter {
-      private int[] ints = new int[10];
-      private int upto;
-
-      public void add(int value) {
-        if (ints.length == upto) {
-          ints = ArrayUtil.grow(ints);
-        }
-        ints[upto++] = value;
-      }
-
-      public int[] get() {
-        final int[] arr = new int[upto];
-        System.arraycopy(ints, 0, arr, 0, upto);
-        upto = 0;
-        return arr;
-      }
-    }
-
-    public DirectField(SegmentReadState state, String field, Terms termsIn, int minSkipCount, int lowFreqCutoff) throws IOException {
-      final FieldInfo fieldInfo = state.fieldInfos.fieldInfo(field);
-
-      sumTotalTermFreq = termsIn.getSumTotalTermFreq();
-      sumDocFreq = termsIn.getSumDocFreq();
-      docCount = termsIn.getDocCount();
-
-      final int numTerms = (int) termsIn.size();
-      if (numTerms == -1) {
-        throw new IllegalArgumentException("codec does not provide Terms.size()");
-      }
-      terms = new TermAndSkip[numTerms];
-      termOffsets = new int[1+numTerms];
-      
-      byte[] termBytes = new byte[1024];
-
-      this.minSkipCount = minSkipCount;
-
-      hasFreq = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_ONLY) > 0;
-      hasPos = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS) > 0;
-      hasOffsets = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) > 0;
-      hasPayloads = fieldInfo.hasPayloads();
-
-      BytesRef term;
-      DocsEnum docsEnum = null;
-      DocsAndPositionsEnum docsAndPositionsEnum = null;
-      final TermsEnum termsEnum = termsIn.iterator(null);
-      int termOffset = 0;
-
-      final IntArrayWriter scratch = new IntArrayWriter();
-
-      // Used for payloads, if any:
-      final RAMOutputStream ros = new RAMOutputStream();
-
-      // if (DEBUG) {
-      //   System.out.println("\nLOAD terms seg=" + state.segmentInfo.name + " field=" + field + " hasOffsets=" + hasOffsets + " hasFreq=" + hasFreq + " hasPos=" + hasPos + " hasPayloads=" + hasPayloads);
-      // }
-
-      while ((term = termsEnum.next()) != null) {
-        final int docFreq = termsEnum.docFreq();
-        final long totalTermFreq = termsEnum.totalTermFreq();
-
-        // if (DEBUG) {
-        //   System.out.println("  term=" + term.utf8ToString());
-        // }
-
-        termOffsets[count] = termOffset;
-
-        if (termBytes.length < (termOffset + term.length)) {
-          termBytes = ArrayUtil.grow(termBytes, termOffset + term.length);
-        }
-        System.arraycopy(term.bytes, term.offset, termBytes, termOffset, term.length);
-        termOffset += term.length;
-        termOffsets[count+1] = termOffset;
-
-        if (hasPos) {
-          docsAndPositionsEnum = termsEnum.docsAndPositions(null, docsAndPositionsEnum);
-        } else {
-          docsEnum = termsEnum.docs(null, docsEnum);
-        }
-
-        final TermAndSkip ent;
-
-        final DocsEnum docsEnum2;
-        if (hasPos) {
-          docsEnum2 = docsAndPositionsEnum;
-        } else {
-          docsEnum2 = docsEnum;
-        }
-
-        int docID;
-
-        if (docFreq <= lowFreqCutoff) {
-
-          ros.reset();
-
-          // Pack postings for low-freq terms into a single int[]:
-          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {
-            scratch.add(docID);
-            if (hasFreq) {
-              final int freq = docsEnum2.freq();
-              scratch.add(freq);
-              if (hasPos) {
-                for(int pos=0;pos<freq;pos++) {
-                  scratch.add(docsAndPositionsEnum.nextPosition());
-                  if (hasOffsets) {
-                    scratch.add(docsAndPositionsEnum.startOffset());
-                    scratch.add(docsAndPositionsEnum.endOffset());
-                  }
-                  if (hasPayloads) {
-                    final BytesRef payload = docsAndPositionsEnum.getPayload();
-                    if (payload != null) {
-                      scratch.add(payload.length);
-                      ros.writeBytes(payload.bytes, payload.offset, payload.length);
-                    } else {
-                      scratch.add(0);
-                    }
-                  }
-                }
-              }
-            }
-          }
-
-          final byte[] payloads;
-          if (hasPayloads) {
-            ros.flush();
-            payloads = new byte[(int) ros.length()];
-            ros.writeTo(payloads, 0);
-          } else {
-            payloads = null;
-          }
-
-          final int[] postings = scratch.get();
-        
-          ent = new LowFreqTerm(postings, payloads, docFreq, (int) totalTermFreq);
-        } else {
-          final int[] docs = new int[docFreq];
-          final int[] freqs;
-          final int[][] positions;
-          final byte[][][] payloads;
-          if (hasFreq) {
-            freqs = new int[docFreq];
-            if (hasPos) {
-              positions = new int[docFreq][];
-              if (hasPayloads) {
-                payloads = new byte[docFreq][][];
-              } else {
-                payloads = null;
-              }
-            } else {
-              positions = null;
-              payloads = null;
-            }
-          } else {
-            freqs = null;
-            positions = null;
-            payloads = null;
-          }
-
-          // Use separate int[] for the postings for high-freq
-          // terms:
-          int upto = 0;
-          while ((docID = docsEnum2.nextDoc()) != DocsEnum.NO_MORE_DOCS) {
-            docs[upto] = docID;
-            if (hasFreq) {
-              final int freq = docsEnum2.freq();
-              freqs[upto] = freq;
-              if (hasPos) {
-                final int mult;
-                if (hasOffsets) {
-                  mult = 3;
-                } else {
-                  mult = 1;
-                }
-                if (hasPayloads) {
-                  payloads[upto] = new byte[freq][];
-                }
-                positions[upto] = new int[mult*freq];
-                int posUpto = 0;
-                for(int pos=0;pos<freq;pos++) {
-                  positions[upto][posUpto] = docsAndPositionsEnum.nextPosition();
-                  if (hasPayloads) {
-                    BytesRef payload = docsAndPositionsEnum.getPayload();
-                    if (payload != null) {
-                      byte[] payloadBytes = new byte[payload.length];
-                      System.arraycopy(payload.bytes, payload.offset, payloadBytes, 0, payload.length);
-                      payloads[upto][pos] = payloadBytes;
-                    }
-                  }
-                  posUpto++;
-                  if (hasOffsets) {
-                    positions[upto][posUpto++] = docsAndPositionsEnum.startOffset();
-                    positions[upto][posUpto++] = docsAndPositionsEnum.endOffset();
-                  }
-                }
-              }
-            }
-
-            upto++;
-          }
-          assert upto == docFreq;
-          ent = new HighFreqTerm(docs, freqs, positions, payloads, totalTermFreq);
-        }
-
-        terms[count] = ent;
-        setSkips(count, termBytes);
-        count++;
-      }
-
-      // End sentinel:
-      termOffsets[count] = termOffset;
-
-      finishSkips();
-
-      //System.out.println(skipCount + " skips: " + field);
-
-      this.termBytes = new byte[termOffset];
-      System.arraycopy(termBytes, 0, this.termBytes, 0, termOffset);
-
-      // Pack skips:
-      this.skips = new int[skipCount];
-      this.skipOffsets = new int[1+numTerms];
-
-      int skipOffset = 0;
-      for(int i=0;i<numTerms;i++) {
-        final int[] termSkips = terms[i].skips;
-        skipOffsets[i] = skipOffset;
-        if (termSkips != null) {
-          System.arraycopy(termSkips, 0, skips, skipOffset, termSkips.length);
-          skipOffset += termSkips.length;
-          terms[i].skips = null;
-        }
-      }
-      this.skipOffsets[numTerms] = skipOffset;
-      assert skipOffset == skipCount;
-    }
-
-    // Compares in unicode (UTF8) order:
-    int compare(int ord, BytesRef other) {
-      final byte[] otherBytes = other.bytes;
-
-      int upto = termOffsets[ord];
-      final int termLen = termOffsets[1+ord] - upto;
-      int otherUpto = other.offset;
-      
-      final int stop = upto + Math.min(termLen, other.length);
-      while (upto < stop) {
-        int diff = (termBytes[upto++] & 0xFF) - (otherBytes[otherUpto++] & 0xFF);
-        if (diff != 0) {
-          return diff;
-        }
-      }
-    
-      // One is a prefix of the other, or, they are equal:
-      return termLen - other.length;
-    }
-
-    private void setSkips(int termOrd, byte[] termBytes) {
-
-      final int termLength = termOffsets[termOrd+1] - termOffsets[termOrd];
-
-      if (sameCounts.length < termLength) {
-        sameCounts = ArrayUtil.grow(sameCounts, termLength);
-      }
-
-      // Update skip pointers:
-      if (termOrd > 0) {
-        final int lastTermLength = termOffsets[termOrd] - termOffsets[termOrd-1];
-        final int limit = Math.min(termLength, lastTermLength);
-
-        int lastTermOffset = termOffsets[termOrd-1];
-        int termOffset = termOffsets[termOrd];
-
-        int i = 0;
-        for(;i<limit;i++) {
-          if (termBytes[lastTermOffset++] == termBytes[termOffset++]) {
-            sameCounts[i]++;
-          } else {
-            for(;i<limit;i++) {
-              if (sameCounts[i] >= minSkipCount) {
-                // Go back and add a skip pointer:
-                saveSkip(termOrd, sameCounts[i]);
-              }
-              sameCounts[i] = 1;
-            }
-            break;
-          }
-        }
-
-        for(;i<lastTermLength;i++) {
-          if (sameCounts[i] >= minSkipCount) {
-            // Go back and add a skip pointer:
-            saveSkip(termOrd, sameCounts[i]);
-          }
-          sameCounts[i] = 0;
-        }
-        for(int j=limit;j<termLength;j++) {
-          sameCounts[j] = 1;
-        }
-      } else {
-        for(int i=0;i<termLength;i++) {
-          sameCounts[i]++;
-        }
-      }
-    }
-
-    private void finishSkips() {
-      assert count == terms.length;
-      int lastTermOffset = termOffsets[count-1];
-      int lastTermLength = termOffsets[count] - lastTermOffset;
-
-      for(int i=0;i<lastTermLength;i++) {
-        if (sameCounts[i] >= minSkipCount) {
-          // Go back and add a skip pointer:
-          saveSkip(count, sameCounts[i]);
-        }
-      }
-
-      // Reverse the skip pointers so they are "nested":
-      for(int termID=0;termID<terms.length;termID++) {
-        TermAndSkip term = terms[termID];
-        if (term.skips != null && term.skips.length > 1) {
-          for(int pos=0;pos<term.skips.length/2;pos++) {
-            final int otherPos = term.skips.length-pos-1;
-
-            final int temp = term.skips[pos];
-            term.skips[pos] = term.skips[otherPos];
-            term.skips[otherPos] = temp;
-          }
-        }
-      }
-    }
-
-    private void saveSkip(int ord, int backCount) {
-      final TermAndSkip term = terms[ord - backCount];
-      skipCount++;
-      if (term.skips == null) {
-        term.skips = new int[] {ord};
-      } else {
-        // Normally we'd grow at a slight exponential... but
-        // given that the skips themselves are already log(N)
-        // we can grow by only 1 and still have amortized
-        // linear time:
-        final int[] newSkips = new int[term.skips.length+1];
-        System.arraycopy(term.skips, 0, newSkips, 0, term.skips.length);
-        term.skips = newSkips;
-        term.skips[term.skips.length-1] = ord;
-      }
-    }
-
-    @Override
-    public TermsEnum iterator(TermsEnum reuse) {
-      DirectTermsEnum termsEnum;
-      if (reuse != null && reuse instanceof DirectTermsEnum) {
-        termsEnum = (DirectTermsEnum) reuse;
-        if (!termsEnum.canReuse(terms)) {
-          termsEnum = new DirectTermsEnum();
-        }
-      } else {
-        termsEnum = new DirectTermsEnum();
-      }
-      termsEnum.reset();
-      return termsEnum;
-    }
-
-    @Override
-    public TermsEnum intersect(CompiledAutomaton compiled, final BytesRef startTerm) {
-      return new DirectIntersectTermsEnum(compiled, startTerm);
-    }
-
-    @Override
-    public long size() {
-      return terms.length;
-    }
-
-    @Override
-    public long getSumTotalTermFreq() {
-      return sumTotalTermFreq;
-    }
-
-    @Override
-    public long getSumDocFreq() {
-      return sumDocFreq;
-    }
-
-    @Override
-    public int getDocCount() {
-      return docCount;
-    }
-
-    @Override
-    public Comparator<BytesRef> getComparator() {
-      return BytesRef.getUTF8SortedAsUnicodeComparator();
-    }
-
-    @Override
-    public boolean hasOffsets() {
-      return hasOffsets;
-    }
-
-    @Override
-    public boolean hasPositions() {
-      return hasPos;
-    }
-    
-    @Override
-    public boolean hasPayloads() {
-      return hasPayloads;
-    }
-
-    private final class DirectTermsEnum extends TermsEnum {
-
-      private final BytesRef scratch = new BytesRef();
-      private int termOrd;
-
-      boolean canReuse(TermAndSkip[] other) {
-        return DirectField.this.terms == other;
-      }
-
-      private BytesRef setTerm() {
-        scratch.bytes = termBytes;
-        scratch.offset = termOffsets[termOrd];
-        scratch.length = termOffsets[termOrd+1] - termOffsets[termOrd];
-        return scratch;
-      }
-
-      public void reset() {
-        termOrd = -1;
-      }
-
-      public Comparator<BytesRef> getComparator() {
-        return BytesRef.getUTF8SortedAsUnicodeComparator();
-      }
-
-      @Override
-      public BytesRef next() {
-        termOrd++;
-        if (termOrd < terms.length) {
-          return setTerm();
-        } else {
-          return null;
-        }
-      }
-
-      @Override
-      public TermState termState() {
-        OrdTermState state = new OrdTermState();
-        state.ord = termOrd;
-        return state;
-      }
-
-      // If non-negative, exact match; else, -ord-1, where ord
-      // is where you would insert the term.
-      private int findTerm(BytesRef term) {
-
-        // Just do binary search: should be (constant factor)
-        // faster than using the skip list:
-        int low = 0;
-        int high = terms.length-1;
-
-        while (low <= high) {
-          int mid = (low + high) >>> 1;
-          int cmp = compare(mid, term);
-          if (cmp < 0) {
-            low = mid + 1;
-          } else if (cmp > 0) {
-            high = mid - 1;
-          } else {
-            return mid; // key found
-          }
-        }
-
-        return -(low + 1);  // key not found.
-      }
-
-      @Override
-      public SeekStatus seekCeil(BytesRef term, boolean useCache) {
-        // TODO: we should use the skip pointers; should be
-        // faster than bin search; we should also hold
-        // & reuse current state so seeking forwards is
-        // faster
-        final int ord = findTerm(term);
-        // if (DEBUG) {
-        //   System.out.println("  find term=" + term.utf8ToString() + " ord=" + ord);
-        // }
-        if (ord >= 0) {
-          termOrd = ord;
-          setTerm();
-          return SeekStatus.FOUND;
-        } else if (ord == -terms.length-1) {
-          return SeekStatus.END;
-        } else {
-          termOrd = -ord - 1;
-          setTerm();
-          return SeekStatus.NOT_FOUND;
-        }
-      }
-
-      @Override
-      public boolean seekExact(BytesRef term, boolean useCache) {
-        // TODO: we should use the skip pointers; should be
-        // faster than bin search; we should also hold
-        // & reuse current state so seeking forwards is
-        // faster
-        final int ord = findTerm(term);
-        if (ord >= 0) {
-          termOrd = ord;
-          setTerm();
-          return true;
-        } else {
-          return false;
-        }
-      }
-
-      @Override
-      public void seekExact(long ord) {
-        termOrd = (int) ord;
-        setTerm();
-      }
-
-      @Override
-      public void seekExact(BytesRef term, TermState state) throws IOException {
-        termOrd = (int) ((OrdTermState) state).ord;
-        setTerm();
-        assert term.equals(scratch);
-      }
-
-      @Override
-      public BytesRef term() {
-        return scratch;
-      }
-
-      @Override
-      public long ord() {
-        return termOrd;
-      }
-
-      @Override
-      public int docFreq() {
-        if (terms[termOrd] instanceof LowFreqTerm) {
-          return ((LowFreqTerm) terms[termOrd]).docFreq;
-        } else {
-          return ((HighFreqTerm) terms[termOrd]).docIDs.length;
-        }
-      }
-
-      @Override
-      public long totalTermFreq() {
-        if (terms[termOrd] instanceof LowFreqTerm) {
-          return ((LowFreqTerm) terms[termOrd]).totalTermFreq;
-        } else {
-          return ((HighFreqTerm) terms[termOrd]).totalTermFreq;
-        }
-      }
-
-      @Override
-      public DocsEnum docs(Bits liveDocs, DocsEnum reuse, int flags) {
-        // TODO: implement reuse, something like Pulsing:
-        // it's hairy!
-
-        if (terms[termOrd] instanceof LowFreqTerm) {
-          final int[] postings = ((LowFreqTerm) terms[termOrd]).postings;
-          if (hasFreq) {
-            if (hasPos) {
-              int posLen;
-              if (hasOffsets) {
-                posLen = 3;
-              } else {
-                posLen = 1;
-              }
-              if (hasPayloads) {
-                posLen++;
-              }
-              LowFreqDocsEnum docsEnum;
-              if (reuse instanceof LowFreqDocsEnum) {
-                docsEnum = (LowFreqDocsEnum) reuse;
-                if (!docsEnum.canReuse(liveDocs, posLen)) {
-                  docsEnum = new LowFreqDocsEnum(liveDocs, posLen);
-                }
-              } else {
-                docsEnum = new LowFreqDocsEnum(liveDocs, posLen);
-              }
-
-              return docsEnum.reset(postings);
-            } else {
-              LowFreqDocsEnumNoPos docsEnum;
-              if (reuse instanceof LowFreqDocsEnumNoPos) {
-                docsEnum = (LowFreqDocsEnumNoPos) reuse;
-                if (!docsEnum.canReuse(liveDocs)) {
-                  docsEnum = new LowFreqDocsEnumNoPos(liveDocs);
-                }
-              } else {
-                docsEnum = new LowFreqDocsEnumNoPos(liveDocs);
-              }
-
-              return docsEnum.reset(postings);
-            }
-          } else {
-            LowFreqDocsEnumNoTF docsEnum;
-            if (reuse instanceof LowFreqDocsEnumNoTF) {
-              docsEnum = (LowFreqDocsEnumNoTF) reuse;
-              if (!docsEnum.canReuse(liveDocs)) {
-                docsEnum = new LowFreqDocsEnumNoTF(liveDocs);
-              }
-            } else {
-              docsEnum = new LowFreqDocsEnumNoTF(liveDocs);
-            }
-
-            return docsEnum.reset(postings);
-          }
-        } else {
-          final HighFreqTerm term = (HighFreqTerm) terms[termOrd];
-
-          HighFreqDocsEnum docsEnum;
-          if (reuse instanceof HighFreqDocsEnum) {
-            docsEnum = (HighFreqDocsEnum) reuse;
-            if (!docsEnum.canReuse(liveDocs)) {
-              docsEnum = new HighFreqDocsEnum(liveDocs);
-            }
-          } else {
-            docsEnum = new HighFreqDocsEnum(liveDocs);
-          }
-
-          //System.out.println("  DE for term=" + new BytesRef(terms[termOrd].term).utf8ToString() + ": " + term.docIDs.length + " docs");
-          return docsEnum.reset(term.docIDs, term.freqs);
-        }
-      }
-
-      @Override
-      public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum reuse, int flags) {
-        if (!hasPos) {
-          return null;
-        }
-
-        // TODO: implement reuse, something like Pulsing:
-        // it's hairy!
-
-        if (terms[termOrd] instanceof LowFreqTerm) {
-          final LowFreqTerm term = ((LowFreqTerm) terms[termOrd]);
-          final int[] postings = term.postings;
-          final byte[] payloads = term.payloads;
-          return new LowFreqDocsAndPositionsEnum(liveDocs, hasOffsets, hasPayloads).reset(postings, payloads);
-        } else {
-          final HighFreqTerm term = (HighFreqTerm) terms[termOrd];
-          return new HighFreqDocsAndPositionsEnum(liveDocs, hasOffsets).reset(term.docIDs, term.freqs, term.positions, term.payloads);
-        }
-      }
-    }
-
-    private final class DirectIntersectTermsEnum extends TermsEnum {
-      private final RunAutomaton runAutomaton;
-      private final CompiledAutomaton compiledAutomaton;
-      private int termOrd;
-      private final BytesRef scratch = new BytesRef();
-
-      private final class State {
-        int changeOrd;
-        int state;
-        Transition[] transitions;
-        int transitionUpto;
-        int transitionMax;
-        int transitionMin;
-      }
-
-      private State[] states;
-      private int stateUpto;
-
-      public DirectIntersectTermsEnum(CompiledAutomaton compiled, BytesRef startTerm) {
-        runAutomaton = compiled.runAutomaton;
-        compiledAutomaton = compiled;
-        termOrd = -1;
-        states = new State[1];
-        states[0] = new State();
-        states[0].changeOrd = terms.length;
-        states[0].state = runAutomaton.getInitialState();
-        states[0].transitions = compiledAutomaton.sortedTransitions[states[0].state];
-        states[0].transitionUpto = -1;
-        states[0].transitionMax = -1;
-
-        //System.out.println("IE.init startTerm=" + startTerm);
-
-        if (startTerm != null) {
-          int skipUpto = 0;
-          if (startTerm.length == 0) {
-            if (terms.length > 0 && termOffsets[1] == 0) {
-              termOrd = 0;
-            }
-          } else {
-            termOrd++;
-
-            nextLabel:
-            for(int i=0;i<startTerm.length;i++) {
-              final int label = startTerm.bytes[startTerm.offset+i] & 0xFF;
-
-              while (label > states[i].transitionMax) {
-                states[i].transitionUpto++;
-                assert states[i].transitionUpto < states[i].transitions.length;
-                states[i].transitionMin = states[i].transitions[states[i].transitionUpto].getMin();
-                states[i].transitionMax = states[i].transitions[states[i].transitionUpto].getMax();
-                assert states[i].transitionMin >= 0;
-                assert states[i].transitionMin <= 255;
-                assert states[i].transitionMax >= 0;
-                assert states[i].transitionMax <= 255;
-              }
-
-              // Skip forwards until we find a term matching
-              // the label at this position:
-              while (termOrd < terms.length) {
-                final int skipOffset = skipOffsets[termOrd];
-                final int numSkips = skipOffsets[termOrd+1] - skipOffset;
-                final int termOffset = termOffsets[termOrd];
-                final int termLength = termOffsets[1+termOrd] - termOffset;
-
-                // if (DEBUG) {
-                //   System.out.println("  check termOrd=" + termOrd + " term=" + new BytesRef(termBytes, termOffset, termLength).utf8ToString() + " skips=" + Arrays.toString(skips) + " i=" + i);
-                // }
-
-                if (termOrd == states[stateUpto].changeOrd) {
-                  // if (DEBUG) {
-                  //   System.out.println("  end push return");
-                  // }
-                  stateUpto--;
-                  termOrd--;
-                  return;
-                }
-
-                if (termLength == i) {
-                  termOrd++;
-                  skipUpto = 0;
-                  // if (DEBUG) {
-                  //   System.out.println("    term too short; next term");
-                  // }
-                } else if (label < (termBytes[termOffset+i] & 0xFF)) {
-                  termOrd--;
-                  // if (DEBUG) {
-                  //   System.out.println("  no match; already beyond; return termOrd=" + termOrd);
-                  // }
-                  stateUpto -= skipUpto;
-                  assert stateUpto >= 0;
-                  return;
-                } else if (label == (termBytes[termOffset+i] & 0xFF)) {
-                  // if (DEBUG) {
-                  //   System.out.println("    label[" + i + "] matches");
-                  // }
-                  if (skipUpto < numSkips) {
-                    grow();
-
-                    final int nextState = runAutomaton.step(states[stateUpto].state, label);
-
-                    // Automaton is required to accept startTerm:
-                    assert nextState != -1;
-
-                    stateUpto++;
-                    states[stateUpto].changeOrd = skips[skipOffset + skipUpto++];
-                    states[stateUpto].state = nextState;
-                    states[stateUpto].transitions = compiledAutomaton.sortedTransitions[nextState];
-                    states[stateUpto].transitionUpto = -1;
-                    states[stateUpto].transitionMax = -1;
-                    //System.out.println("  push " + states[stateUpto].transitions.length + " trans");
-
-                    // if (DEBUG) {
-                    //   System.out.println("    push skip; changeOrd=" + states[stateUpto].changeOrd);
-                    // }
-
-                    // Match next label at this same term:
-                    continue nextLabel;
-                  } else {
-                    // if (DEBUG) {
-                    //   System.out.println("    linear scan");
-                    // }
-                    // Index exhausted: just scan now (the
-                    // number of scans required will be less
-                    // than the minSkipCount):
-                    final int startTermOrd = termOrd;
-                    while (termOrd < terms.length && compare(termOrd, startTerm) <= 0) {
-                      assert termOrd == startTermOrd || skipOffsets[termOrd] == skipOffsets[termOrd+1];
-                      termOrd++;
-                    }
-                    assert termOrd - startTermOrd < minSkipCount;
-                    termOrd--;
-                    stateUpto -= skipUpto;
-                    // if (DEBUG) {
-                    //   System.out.println("  end termOrd=" + termOrd);
-                    // }
-                    return;
-                  }
-                } else {
-                  if (skipUpto < numSkips) {
-                    termOrd = skips[skipOffset + skipUpto];
-                    // if (DEBUG) {
-                    //   System.out.println("  no match; skip to termOrd=" + termOrd);
-                    // }
-                  } else {
-                    // if (DEBUG) {
-                    //   System.out.println("  no match; next term");
-                    // }
-                    termOrd++;
-                  }
-                  skipUpto = 0;
-                }
-              }
-
-              // startTerm is >= last term so enum will not
-              // return any terms:
-              termOrd--;
-              // if (DEBUG) {
-              //   System.out.println("  beyond end; no terms will match");
-              // }
-              return;
-            }
-          }
-
-          final int termOffset = termOffsets[termOrd];
-          final int termLen = termOffsets[1+termOrd] - termOffset;
-
-          if (termOrd >= 0 && !startTerm.equals(new BytesRef(termBytes, termOffset, termLen))) {
-            stateUpto -= skipUpto;
-            termOrd--;
-          }
-          // if (DEBUG) {
-          //   System.out.println("  loop end; return termOrd=" + termOrd + " stateUpto=" + stateUpto);
-          // }
-        }
-      }
-
-      public Comparator<BytesRef> getComparator() {
-        return BytesRef.getUTF8SortedAsUnicodeComparator();
-      }
-
-      private void grow() {
-        if (states.length == 1+stateUpto) {
-          final State[] newStates = new State[states.length+1];
-          System.arraycopy(states, 0, newStates, 0, states.length);
-          newStates[states.length] = new State();
-          states = newStates;
-        }
-      }
-
-      @Override
-      public BytesRef next() {
-        // if (DEBUG) {
-        //   System.out.println("\nIE.next");
-        // }
-
-        termOrd++;
-        int skipUpto = 0;
-
-        if (termOrd == 0 && termOffsets[1] == 0) {
-          // Special-case empty string:
-          assert stateUpto == 0;
-          // if (DEBUG) {
-          //   System.out.println("  visit empty string");
-          // }
-          if (runAutomaton.isAccept(states[0].state)) {
-            scratch.bytes = termBytes;
-            scratch.offset = 0;
-            scratch.length = 0;
-            return scratch;
-          }
-          termOrd++;
-        }
-
-        nextTerm:
-
-        while (true) {
-          // if (DEBUG) {
-          //   System.out.println("  cycle termOrd=" + termOrd + " stateUpto=" + stateUpto + " skipUpto=" + skipUpto);
-          // }
-          if (termOrd == terms.length) {
-            // if (DEBUG) {
-            //   System.out.println("  return END");
-            // }
-            return null;
-          }
-
-          final State state = states[stateUpto];
-          if (termOrd == state.changeOrd) {
-            // Pop:
-            // if (DEBUG) {
-            //   System.out.println("  pop stateUpto=" + stateUpto);
-            // }
-            stateUpto--;
-            /*
-            if (DEBUG) {
-              try {
-                //System.out.println("    prefix pop " + new BytesRef(terms[termOrd].term, 0, Math.min(stateUpto, terms[termOrd].term.length)).utf8ToString());
-                System.out.println("    prefix pop " + new BytesRef(terms[termOrd].term, 0, Math.min(stateUpto, terms[termOrd].term.length)));
-              } catch (ArrayIndexOutOfBoundsException aioobe) {
-                System.out.println("    prefix pop " + new BytesRef(terms[termOrd].term, 0, Math.min(stateUpto, terms[termOrd].term.length)));
-              }
-            }
-            */
-
-            continue;
-          }
-
-          final int termOffset = termOffsets[termOrd];
-          final int termLength = termOffsets[termOrd+1] - termOffset;
-          final int skipOffset = skipOffsets[termOrd];
-          final int numSkips = skipOffsets[termOrd+1] - skipOffset;
-
-          // if (DEBUG) {
-          //   System.out.println("  term=" + new BytesRef(termBytes, termOffset, termLength).utf8ToString() + " skips=" + Arrays.toString(skips));
-          // }
-        
-          assert termOrd < state.changeOrd;
-
-          assert stateUpto <= termLength: "term.length=" + termLength + "; stateUpto=" + stateUpto;
-          final int label = termBytes[termOffset+stateUpto] & 0xFF;
-
-          while (label > state.transitionMax) {
-            //System.out.println("  label=" + label + " vs max=" + state.transitionMax + " transUpto=" + state.transitionUpto + " vs " + state.transitions.length);
-            state.transitionUpto++;
-            if (state.transitionUpto == state.transitions.length) {
-              // We've exhausted transitions leaving this
-              // state; force pop+next/skip now:
-              //System.out.println("forcepop: stateUpto=" + stateUpto);
-              if (stateUpto == 0) {
-                termOrd = terms.length;
-                return null;
-              } else {
-                assert state.changeOrd > termOrd;
-                // if (DEBUG) {
-                //   System.out.println("  jumpend " + (state.changeOrd - termOrd));
-                // }
-                //System.out.println("  jump to termOrd=" + states[stateUpto].changeOrd + " vs " + termOrd);
-                termOrd = states[stateUpto].changeOrd;
-                skipUpto = 0;
-                stateUpto--;
-              }
-              continue nextTerm;
-            }
-            assert state.transitionUpto < state.transitions.length: " state.transitionUpto=" + state.transitionUpto + " vs " + state.transitions.length;
-            state.transitionMin = state.transitions[state.transitionUpto].getMin();
-            state.transitionMax = state.transitions[state.transitionUpto].getMax();
-            assert state.transitionMin >= 0;
-            assert state.transitionMin <= 255;
-            assert state.transitionMax >= 0;
-            assert state.transitionMax <= 255;
-          }
-
-          /*
-          if (DEBUG) {
-            System.out.println("    check ord=" + termOrd + " term[" + stateUpto + "]=" + (char) label + "(" + label + ") term=" + new BytesRef(terms[termOrd].term).utf8ToString() + " trans " +
-                               (char) state.transitionMin + "(" + state.transitionMin + ")" + "-" + (char) state.transitionMax + "(" + state.transitionMax + ") nextChange=+" + (state.changeOrd - termOrd) + " skips=" + (skips == null ? "null" : Arrays.toString(skips)));
-            System.out.println("    check ord=" + termOrd + " term[" + stateUpto + "]=" + Integer.toHexString(label) + "(" + label + ") term=" + new BytesRef(termBytes, termOffset, termLength) + " trans " +
-                               Integer.toHexString(state.transitionMin) + "(" + state.transitionMin + ")" + "-" + Integer.toHexString(state.transitionMax) + "(" + state.transitionMax + ") nextChange=+" + (state.changeOrd - termOrd) + " skips=" + (skips == null ? "null" : Arrays.toString(skips)));
-          }
-          */
-
-          final int targetLabel = state.transitionMin;
-
-          if ((termBytes[termOffset+stateUpto] & 0xFF) < targetLabel) {
-            // if (DEBUG) {
-            //   System.out.println("    do bin search");
-            // }
-            //int startTermOrd = termOrd;
-            int low = termOrd+1;
-            int high = state.changeOrd-1;
-            while (true) {
-              if (low > high) {
-                // Label not found
-                termOrd = low;
-                // if (DEBUG) {
-                //   System.out.println("      advanced by " + (termOrd - startTermOrd));
-                // }
-                //System.out.println("  jump " + (termOrd - startTermOrd));
-                skipUpto = 0;
-                continue nextTerm;
-              }
-              int mid = (low + high) >>> 1;
-              int cmp = (termBytes[termOffsets[mid] + stateUpto] & 0xFF) - targetLabel;
-              // if (DEBUG) {
-              //   System.out.println("      bin: check label=" + (char) (termBytes[termOffsets[low] + stateUpto] & 0xFF) + " ord=" + mid);
-              // }
-              if (cmp < 0) {
-                low = mid+1;
-              } else if (cmp > 0) {
-                high = mid - 1;
-              } else {
-                // Label found; walk backwards to first
-                // occurrence:
-                while (mid > termOrd && (termBytes[termOffsets[mid-1] + stateUpto] & 0xFF) == targetLabel) {
-                  mid--;
-                }
-                termOrd = mid;
-                // if (DEBUG) {
-                //   System.out.println("      advanced by " + (termOrd - startTermOrd));
-                // }
-                //System.out.println("  jump " + (termOrd - startTermOrd));
-                skipUpto = 0;
-                continue nextTerm;
-              }
-            }
-          }
-
-          int nextState = runAutomaton.step(states[stateUpto].state, label);
-
-          if (nextState == -1) {
-            // Skip
-            // if (DEBUG) {
-            //   System.out.println("  automaton doesn't accept; skip");
-            // }
-            if (skipUpto < numSkips) {
-              // if (DEBUG) {
-              //   System.out.println("  jump " + (skips[skipOffset+skipUpto]-1 - termOrd));
-              // }
-              termOrd = skips[skipOffset+skipUpto];
-            } else {
-              termOrd++;
-            }
-            skipUpto = 0;
-          } else if (skipUpto < numSkips) {
-            // Push:
-            // if (DEBUG) {
-            //   System.out.println("  push");
-            // }
-            /*
-            if (DEBUG) {
-              try {
-                //System.out.println("    prefix push " + new BytesRef(term, 0, stateUpto+1).utf8ToString());
-                System.out.println("    prefix push " + new BytesRef(term, 0, stateUpto+1));
-              } catch (ArrayIndexOutOfBoundsException aioobe) {
-                System.out.println("    prefix push " + new BytesRef(term, 0, stateUpto+1));
-              }
-            }
-            */
-
-            grow();
-            stateUpto++;
-            states[stateUpto].state = nextState;
-            states[stateUpto].changeOrd = skips[skipOffset + skipUpto++];
-            states[stateUpto].transitions = compiledAutomaton.sortedTransitions[nextState];
-            states[stateUpto].transitionUpto = -1;
-            states[stateUpto].transitionMax = -1;
-            
-            if (stateUpto == termLength) {
-              // if (DEBUG) {
-              //   System.out.println("  term ends after push");
-              // }
-              if (runAutomaton.isAccept(nextState)) {
-                // if (DEBUG) {
-                //   System.out.println("  automaton accepts: return");
-                // }
-                scratch.bytes = termBytes;
-                scratch.offset = termOffsets[termOrd];
-                scratch.length = termOffsets[1+termOrd] - scratch.offset;
-                // if (DEBUG) {
-                //   System.out.println("  ret " + scratch.utf8ToString());
-                // }
-                return scratch;
-              } else {
-                // if (DEBUG) {
-                //   System.out.println("  automaton rejects: nextTerm");
-                // }
-                termOrd++;
-                skipUpto = 0;
-              }
-            }
-          } else {
-            // Run the non-indexed tail of this term:
-
-            // TODO: add assert that we don't inc too many times
-
-            if (compiledAutomaton.commonSuffixRef != null) {
-              //System.out.println("suffix " + compiledAutomaton.commonSuffixRef.utf8ToString());
-              assert compiledAutomaton.commonSuffixRef.offset == 0;
-              if (termLength < compiledAutomaton.commonSuffixRef.length) {
-                termOrd++;
-                skipUpto = 0;
-                continue nextTerm;
-              }
-              int offset = termOffset + termLength - compiledAutomaton.commonSuffixRef.length;
-              for(int suffix=0;suffix<compiledAutomaton.commonSuffixRef.length;suffix++) {
-                if (termBytes[offset + suffix] != compiledAutomaton.commonSuffixRef.bytes[suffix]) {
-                  termOrd++;
-                  skipUpto = 0;
-                  continue nextTerm;
-                }
-              }
-            }
-
-            int upto = stateUpto+1;
-            while (upto < termLength) {
-              nextState = runAutomaton.step(nextState, termBytes[termOffset+upto] & 0xFF);
-              if (nextState == -1) {
-                termOrd++;
-                skipUpto = 0;
-                // if (DEBUG) {
-                //   System.out.println("  nomatch tail; next term");
-                // }
-                continue nextTerm;
-              }
-              upto++;
-            }
-
-            if (runAutomaton.isAccept(nextState)) {
-              scratch.bytes = termBytes;
-              scratch.offset = termOffsets[termOrd];
-              scratch.length = termOffsets[1+termOrd] - scratch.offset;
-              // if (DEBUG) {
-              //   System.out.println("  match tail; return " + scratch.utf8ToString());
-              //   System.out.println("  ret2 " + scratch.utf8ToString());
-              // }
-              return scratch;
-            } else {
-              termOrd++;
-              skipUpto = 0;
-              // if (DEBUG) {
-              //   System.out.println("  nomatch tail; next term");
-              // }
-            }
-          }
-        }
-      }
-
-      @Override
-      public TermState termState() {
-        OrdTermState state = new OrdTermState();
-        state.ord = termOrd;
-        return state;
-      }
-
-      @Override
-      public BytesRef term() {
-        return scratch;
-      }
-
-      @Override
-      public long ord() {
-        return termOrd;
-      }
-
-      @Override
-      public int docFreq() {
-        if (terms[termOrd] instanceof LowFreqTerm) {
-          return ((LowFreqTerm) terms[termOrd]).docFreq;
-        } else {
-          return ((HighFreqTerm) terms[termOrd]).docIDs.length;
-        }
-      }
-
-      @Override
-      public long totalTermFreq() {
-        if (terms[termOrd] instanceof LowFreqTerm) {
-          return ((LowFreqTerm) terms[termOrd]).totalTermFreq;
-        } else {
-          return ((HighFreqTerm) terms[termOrd]).totalTermFreq;
-        }
-      }
-
-      @Override
-      public DocsEnum docs(Bits liveDocs, DocsEnum reuse, int flags) {
-        // TODO: implement reuse, something like Pulsing:
-        // it's hairy!
-
-        if (terms[termOrd] instanceof LowFreqTerm) {
-          final int[] postings = ((LowFreqTerm) terms[termOrd]).postings;
-          if (hasFreq) {
-            if (hasPos) {
-              int posLen;
-              if (hasOffsets) {
-                posLen = 3;
-              } else {
-                posLen = 1;
-              }
-              if (hasPayloads) {
-                posLen++;
-              }
-              return new LowFreqDocsEnum(liveDocs, posLen).reset(postings);
-            } else {
-              return new LowFreqDocsEnumNoPos(liveDocs).reset(postings);
-            }
-          } else {
-            return new LowFreqDocsEnumNoTF(liveDocs).reset(postings);
-          }
-        } else {
-          final HighFreqTerm term = (HighFreqTerm) terms[termOrd];
-          //  System.out.println("DE for term=" + new BytesRef(terms[termOrd].term).utf8ToString() + ": " + term.docIDs.length + " docs");
-          return new HighFreqDocsEnum(liveDocs).reset(term.docIDs, term.freqs);
-        }
-      }
-
-      @Override
-      public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum reuse, int flags) {
-        if (!hasPos) {
-          return null;
-        }
-
-        // TODO: implement reuse, something like Pulsing:
-        // it's hairy!
-
-        if (terms[termOrd] instanceof LowFreqTerm) {
-          final LowFreqTerm term = ((LowFreqTerm) terms[termOrd]);
-          final int[] postings = term.postings;
-          final byte[] payloads = term.payloads;
-          return new LowFreqDocsAndPositionsEnum(liveDocs, hasOffsets, hasPayloads).reset(postings, payloads);
-        } else {
-          final HighFreqTerm term = (HighFreqTerm) terms[termOrd];
-          return new HighFreqDocsAndPositionsEnum(liveDocs, hasOffsets).reset(term.docIDs, term.freqs, term.positions, term.payloads);
-        }
-      }
-
-      @Override
-      public SeekStatus seekCeil(BytesRef term, boolean useCache) {
-        throw new UnsupportedOperationException();
-      }
-
-      @Override
-      public void seekExact(long ord) {
-        throw new UnsupportedOperationException();
-      }
-    }
-  }
-
-  // Docs only:
-  private final static class LowFreqDocsEnumNoTF extends DocsEnum {
-    private int[] postings;
-    private final Bits liveDocs;
-    private int upto;
-
-    public LowFreqDocsEnumNoTF(Bits liveDocs) {
-      this.liveDocs = liveDocs;
-    }
-
-    public boolean canReuse(Bits liveDocs) {
-      return liveDocs == this.liveDocs;
-    }
-
-    public DocsEnum reset(int[] postings) {
-      this.postings = postings;
-      upto = -1;
-      return this;
-    }
-
-    // TODO: can do this w/o setting members?
-
-    @Override
-    public int nextDoc() {
-      upto++;
-      if (liveDocs == null) {
-        if (upto < postings.length) {
-          return postings[upto];
-        }
-      } else {
-        while (upto < postings.length) {
-          if (liveDocs.get(postings[upto])) {
-            return postings[upto];
-          }
-          upto++;
-        }
-      }
-      return NO_MORE_DOCS;
-    }
-
-    @Override
-    public int docID() {
-      if (upto < 0) {
-        return -1;
-      } else if (upto < postings.length) {
-        return postings[upto];
-      } else {
-        return NO_MORE_DOCS;
-      }
-    }
-
-    @Override
-    public int freq() {
-      return 1;
-    }
-
-    @Override
-    public int advance(int target) {
-      // Linear scan, but this is low-freq term so it won't
-      // be costly:
-      while(nextDoc() < target) {
-      }
-      return docID();
-    }
-  }
-
-  // Docs + freqs:
-  private final static class LowFreqDocsEnumNoPos extends DocsEnum {
-    private int[] postings;
-    private final Bits liveDocs;
-    private int upto;
-
-    public LowFreqDocsEnumNoPos(Bits liveDocs) {
-      this.liveDocs = liveDocs;
-    }
-
-    public boolean canReuse(Bits liveDocs) {
-      return liveDocs == this.liveDocs;
-    }
-
-    public DocsEnum reset(int[] postings) {
-      this.postings = postings;
-      upto = -2;
-      return this;
-    }
-
-    // TODO: can do this w/o setting members?
-    @Override
-    public int nextDoc() {
-      upto += 2;
-      if (liveDocs == null) {
-        if (upto < postings.length) {
-          return postings[upto];
-        }
-      } else {
-        while (upto < postings.length) {
-          if (liveDocs.get(postings[upto])) {
-            return postings[upto];
-          }
-          upto += 2;
-        }
-      }
-      return NO_MORE_DOCS;
-    }
-
-    @Override
-    public int docID() {
-      if (upto < 0) {
-        return -1;
-      } else if (upto < postings.length) {
-        return postings[upto];
-      } else {
-        return NO_MORE_DOCS;
-      }
-    }
-
-    @Override
-    public int freq() {
-      return postings[upto+1];
-    }
-
-    @Override
-    public int advance(int target) {
-      // Linear scan, but this is low-freq term so it won't
-      // be costly:
-      while(nextDoc() < target) {
-      }
-      return docID();
-    }
-  }
-
-  // Docs + freqs + positions/offets:
-  private final static class LowFreqDocsEnum extends DocsEnum {
-    private int[] postings;
-    private final Bits liveDocs;
-    private final int posMult;
-    private int upto;
-    private int freq;
-
-    public LowFreqDocsEnum(Bits liveDocs, int posMult) {
-      this.liveDocs = liveDocs;
-      this.posMult = posMult;
-      // if (DEBUG) {
-      //   System.out.println("LowFreqDE: posMult=" + posMult);
-      // }
-    }
-
-    public boolean canReuse(Bits liveDocs, int posMult) {
-      return liveDocs == this.liveDocs && posMult == this.posMult;
-    }
-
-    public DocsEnum reset(int[] postings) {
-      this.postings = postings;
-      upto = -2;
-      freq = 0;
-      return this;
-    }
-
-    // TODO: can do this w/o setting members?
-    @Override
-    public int nextDoc() {
-      upto += 2 + freq*posMult;
-      // if (DEBUG) {
-      //   System.out.println("  nextDoc freq=" + freq + " upto=" + upto + " vs " + postings.length);
-      // }
-      if (liveDocs == null) {
-        if (upto < postings.length) {   
-          freq = postings[upto+1];
-          assert freq > 0;
-          return postings[upto];
-        }
-      } else {
-        while (upto < postings.length) {
-          freq = postings[upto+1];
-          assert freq > 0;
-          if (liveDocs.get(postings[upto])) {
-            return postings[upto];
-          }
-          upto += 2 + freq*posMult;
-        }
-      }
-      return NO_MORE_DOCS;
-    }
-
-    @Override
-    public int docID() {
-      // TODO: store docID member?
-      if (upto < 0) {
-        return -1;
-      } else if (upto < postings.length) {
-        return postings[upto];
-      } else {
-        return NO_MORE_DOCS;
-      }
-    }
-
-    @Override
-    public int freq() {
-      // TODO: can I do postings[upto+1]?
-      return freq;
-    }
-
-    @Override
-    public int advance(int target) {
-      // Linear scan, but this is low-freq term so it won't
-      // be costly:
-      while(nextDoc() < target) {
-      }
-      return docID();
-    }
-  }
-
-  private final static class LowFreqDocsAndPositionsEnum extends DocsAndPositionsEnum {
-    private int[] postings;
-    private final Bits liveDocs;
-    private final int posMult;
-    private final boolean hasOffsets;
-    private final boolean hasPayloads;
-    private final BytesRef payload = new BytesRef();
-    private int upto;
-    private int docID;
-    private int freq;
-    private int skipPositions;
-    private int startOffset;
-    private int endOffset;
-    private int lastPayloadOffset;
-    private int payloadOffset;
-    private int payloadLength;
-    private byte[] payloadBytes;
-
-    public LowFreqDocsAndPositionsEnum(Bits liveDocs, boolean hasOffsets, boolean hasPayloads) {
-      this.liveDocs = liveDocs;
-      this.hasOffsets = hasOffsets;
-      this.hasPayloads = hasPayloads;
-      if (hasOffsets) {
-        if (hasPayloads) {
-          posMult = 4;
-        } else {
-          posMult = 3;
-        }
-      } else if (hasPayloads) {
-        posMult = 2;
-      } else {
-        posMult = 1;
-      }
-    }
-
-    public DocsAndPositionsEnum reset(int[] postings, byte[] payloadBytes) {
-      this.postings = postings;
-      upto = 0;
-      skipPositions = 0;
-      startOffset = -1;
-      endOffset = -1;
-      docID = -1;
-      payloadLength = 0;
-      this.payloadBytes = payloadBytes;
-      return this;
-    }
-
-    @Override
-    public int nextDoc() {
-      if (hasPayloads) {
-        for(int i=0;i<skipPositions;i++) {
-          upto++;
-          if (hasOffsets) {
-            upto += 2;
-          }
-          payloadOffset += postings[upto++];
-        }
-      } else {
-        upto += posMult * skipPositions;
-      }
-
-      if (liveDocs == null) {
-        if (upto < postings.length) {
-          docID = postings[upto++];
-          freq = postings[upto++];
-          skipPositions = freq;
-          return docID;
-        }
-      } else {
-        while(upto < postings.length) {
-          docID = postings[upto++];
-          freq = postings[upto++];
-          if (liveDocs.get(docID)) {
-            skipPositions = freq;
-            return docID;
-          }
-          if (hasPayloads) {
-            for(int i=0;i<freq;i++) {
-              upto++;
-              if (hasOffsets) {
-                upto += 2;
-              }
-              payloadOffset += postings[upto++];
-            }
-          } else {
-            upto += posMult * freq;
-          }
-        }
-      }
-
-      return docID = NO_MORE_DOCS;
-    }
-
-    @Override
-    public int docID() {
-      return docID;
-    }
-
-    @Override
-    public int freq() {
-      return freq;
-    }
-
-    @Override
-    public int nextPosition() {
-      assert skipPositions > 0;
-      skipPositions--;
-      final int pos = postings[upto++];
-      if (hasOffsets) {
-        startOffset = postings[upto++];
-        endOffset = postings[upto++];
-      }
-      if (hasPayloads) {
-        payloadLength = postings[upto++];
-        lastPayloadOffset = payloadOffset;
-        payloadOffset += payloadLength;
-      }
-      return pos;
-    }
-
-    @Override
-    public int startOffset() {
-      return startOffset;
-    }
-
-    @Override
-    public int endOffset() {
-      return endOffset;
-    }
-
-    @Override
-    public int advance(int target) {
-      // Linear scan, but this is low-freq term so it won't
-      // be costly:
-      while (nextDoc() < target) {
-      }
-      return docID;
-    }
-
-    @Override
-    public BytesRef getPayload() {
-      if (payloadLength > 0) {
-        payload.bytes = payloadBytes;
-        payload.offset = lastPayloadOffset;
-        payload.length = payloadLength;
-        return payload;
-      } else {
-        return null;
-      }
-    }
-  }
-
-  // Docs + freqs:
-  private final static class HighFreqDocsEnum extends DocsEnum {
-    private int[] docIDs;
-    private int[] freqs;
-    private final Bits liveDocs;
-    private int upto;
-    private int docID = -1;
-
-    public HighFreqDocsEnum(Bits liveDocs) {
-      this.liveDocs = liveDocs;
-    }
-
-    public boolean canReuse(Bits liveDocs) {
-      return liveDocs == this.liveDocs;
-    }
-
-    public int[] getDocIDs() {
-      return docIDs;
-    }
-
-    public int[] getFreqs() {
-      return freqs;
-    }
-
-    public DocsEnum reset(int[] docIDs, int[] freqs) {
-      this.docIDs = docIDs;
-      this.freqs = freqs;
-      docID = upto = -1;
-      return this;
-    }
-
-    @Override
-    public int nextDoc() {
-      upto++;
-      if (liveDocs == null) {
-        try {
-          return docID = docIDs[upto];
-        } catch (ArrayIndexOutOfBoundsException e) {
-        }
-      } else {
-        while (upto < docIDs.length) {
-          if (liveDocs.get(docIDs[upto])) {
-            return docID = docIDs[upto];
-          }
-          upto++;
-        }
-      }
-      return docID = NO_MORE_DOCS;
-    }
-
-    @Override
-    public int docID() {
-      return docID;
-    }
-
-    @Override
-    public int freq() {
-      if (freqs == null) {
-        return 1;
-      } else {
-        return freqs[upto];
-      }
-    }
-
-    @Override
-    public int advance(int target) {
-      /*
-      upto++;
-      if (upto == docIDs.length) {
-        return docID = NO_MORE_DOCS;
-      }
-      final int index = Arrays.binarySearch(docIDs, upto, docIDs.length, target);
-      if (index < 0) {
-        upto = -index - 1;
-      } else {
-        upto = index;
-      }
-      if (liveDocs != null) {
-        while (upto < docIDs.length) {
-          if (liveDocs.get(docIDs[upto])) {
-            break;
-          }
-          upto++;
-        }
-      }
-      if (upto == docIDs.length) {
-        return NO_MORE_DOCS;
-      } else {
-        return docID = docIDs[upto];
-      }
-      */
-
-      //System.out.println("  advance target=" + target + " cur=" + docID() + " upto=" + upto + " of " + docIDs.length);
-      // if (DEBUG) {
-      //   System.out.println("advance target=" + target + " len=" + docIDs.length);
-      // }
-      upto++;
-      if (upto == docIDs.length) {
-        return docID = NO_MORE_DOCS;
-      }
-
-      // First "grow" outwards, since most advances are to
-      // nearby docs:
-      int inc = 10;
-      int nextUpto = upto+10;
-      int low;
-      int high;
-      while (true) {
-        //System.out.println("  grow nextUpto=" + nextUpto + " inc=" + inc);
-        if (nextUpto >= docIDs.length) {
-          low = nextUpto-inc;
-          high = docIDs.length-1;
-          break;
-        }
-        //System.out.println("    docID=" + docIDs[nextUpto]);
-
-        if (target <= docIDs[nextUpto]) {
-          low = nextUpto-inc;
-          high = nextUpto;
-          break;
-        }
-        inc *= 2;
-        nextUpto += inc;
-      }
-
-      // Now do normal binary search
-      //System.out.println("    after fwd: low=" + low + " high=" + high);
-
-      while (true) {
-
-        if (low > high) {
-          // Not exactly found
-          //System.out.println("    break: no match");
-          upto = low;
-          break;
-        }
-
-        int mid = (low + high) >>> 1;
-        int cmp = docIDs[mid] - target;
-        //System.out.println("    bsearch low=" + low + " high=" + high+ ": docIDs[" + mid + "]=" + docIDs[mid]);
-
-        if (cmp < 0) {
-          low = mid + 1;
-        } else if (cmp > 0) {
-          high = mid - 1;
-        } else {
-          // Found target
-          upto = mid;
-          //System.out.println("    break: match");
-          break;
-        }
-      }
-
-      //System.out.println("    end upto=" + upto + " docID=" + (upto >= docIDs.length ? NO_MORE_DOCS : docIDs[upto]));
-
-      if (liveDocs != null) {
-        while (upto < docIDs.length) {
-          if (liveDocs.get(docIDs[upto])) {
-            break;
-          }
-          upto++;
-        }
-      }
-      if (upto == docIDs.length) {
-        //System.out.println("    return END");
-        return docID = NO_MORE_DOCS;
-      } else {
-        //System.out.println("    return docID=" + docIDs[upto] + " upto=" + upto);
-        return docID = docIDs[upto];
-      }
-    }
-  }
-
-  // TODO: specialize offsets and not
-  private final static class HighFreqDocsAndPositionsEnum extends DocsAndPositionsEnum {
-    private int[] docIDs;
-    private int[] freqs;
-    private int[][] positions;
-    private byte[][][] payloads;
-    private final Bits liveDocs;
-    private final boolean hasOffsets;
-    private final int posJump;
-    private int upto;
-    private int docID = -1;
-    private int posUpto;
-    private int[] curPositions;
-
-    public HighFreqDocsAndPositionsEnum(Bits liveDocs, boolean hasOffsets) {
-      this.liveDocs = liveDocs;
-      this.hasOffsets = hasOffsets;
-      posJump = hasOffsets ? 3 : 1;
-    }
-
-    public int[] getDocIDs() {
-      return docIDs;
-    }
-
-    public int[][] getPositions() {
-      return positions;
-    }
-
-    public int getPosJump() {
-      return posJump;
-    }
-
-    public Bits getLiveDocs() {
-      return liveDocs;
-    }
-
-    public DocsAndPositionsEnum reset(int[] docIDs, int[] freqs, int[][] positions, byte[][][] payloads) {
-      this.docIDs = docIDs;
-      this.freqs = freqs;
-      this.positions = positions;
-      this.payloads = payloads;
-      upto = -1;
-      return this;
-    }
-
-    @Override
-    public int nextDoc() {
-      upto++;
-      if (liveDocs == null) {
-        if (upto < docIDs.length) {
-          posUpto = -posJump;   
-          curPositions = positions[upto];
-          return docID = docIDs[upto];
-        }
-      } else {
-        while (upto < docIDs.length) {
-          if (liveDocs.get(docIDs[upto])) {
-            posUpto = -posJump;
-            curPositions = positions[upto];
-            return docID = docIDs[upto];
-          }
-          upto++;
-        }
-      }
-
-      return docID = NO_MORE_DOCS;
-    }
-
-    @Override
-    public int freq() {
-      return freqs[upto];
-    }
-
-    @Override
-    public int docID() {
-      return docID;
-    }
-
-    @Override
-    public int nextPosition() {
-      posUpto += posJump;
-      return curPositions[posUpto];
-    }
-
-    @Override
-    public int startOffset() {
-      if (hasOffsets) {
-        return curPositions[posUpto+1];
-      } else {
-        return -1;
-      }
-    }
-
-    @Override
-    public int endOffset() {
-      if (hasOffsets) {
-        return curPositions[posUpto+2];
-      } else {
-        return -1;
-      }
-    }
-
-    @Override
-    public int advance(int target) {
-
-      /*
-      upto++;
-      if (upto == docIDs.length) {
-        return NO_MORE_DOCS;
-      }
-      final int index = Arrays.binarySearch(docIDs, upto, docIDs.length, target);
-      if (index < 0) {
-        upto = -index - 1;
-      } else {
-        upto = index;
-      }
-      if (liveDocs != null) {
-        while (upto < docIDs.length) {
-          if (liveDocs.get(docIDs[upto])) {
-            break;
-          }
-          upto++;
-        }
-      }
-      posUpto = hasOffsets ? -3 : -1;
-      if (upto == docIDs.length) {
-        return NO_MORE_DOCS;
-      } else {
-        return docID();
-      }
-      */
-
-      //System.out.println("  advance target=" + target + " cur=" + docID() + " upto=" + upto + " of " + docIDs.length);
-      // if (DEBUG) {
-      //   System.out.println("advance target=" + target + " len=" + docIDs.length);
-      // }
-      upto++;
-      if (upto == docIDs.length) {
-        return docID = NO_MORE_DOCS;
-      }
-
-      // First "grow" outwards, since most advances are to
-      // nearby docs:
-      int inc = 10;
-      int nextUpto = upto+10;
-      int low;
-      int high;
-      while (true) {
-        //System.out.println("  grow nextUpto=" + nextUpto + " inc=" + inc);
-        if (nextUpto >= docIDs.length) {
-          low = nextUpto-inc;
-          high = docIDs.length-1;
-          break;
-        }
-        //System.out.println("    docID=" + docIDs[nextUpto]);
-
-        if (target <= docIDs[nextUpto]) {
-          low = nextUpto-inc;
-          high = nextUpto;
-          break;
-        }
-        inc *= 2;
-        nextUpto += inc;
-      }
-
-      // Now do normal binary search
-      //System.out.println("    after fwd: low=" + low + " high=" + high);
-
-      while (true) {
-
-        if (low > high) {
-          // Not exactly found
-          //System.out.println("    break: no match");
-          upto = low;
-          break;
-        }
-
-        int mid = (low + high) >>> 1;
-        int cmp = docIDs[mid] - target;
-        //System.out.println("    bsearch low=" + low + " high=" + high+ ": docIDs[" + mid + "]=" + docIDs[mid]);
-
-        if (cmp < 0) {
-          low = mid + 1;
-        } else if (cmp > 0) {
-          high = mid - 1;
-        } else {
-          // Found target
-          upto = mid;
-          //System.out.println("    break: match");
-          break;
-        }
-      }
-
-      //System.out.println("    end upto=" + upto + " docID=" + (upto >= docIDs.length ? NO_MORE_DOCS : docIDs[upto]));
-
-      if (liveDocs != null) {
-        while (upto < docIDs.length) {
-          if (liveDocs.get(docIDs[upto])) {
-            break;
-          }
-          upto++;
-        }
-      }
-      if (upto == docIDs.length) {
-        //System.out.println("    return END");
-        return docID = NO_MORE_DOCS;
-      } else {
-        //System.out.println("    return docID=" + docIDs[upto] + " upto=" + upto);
-        posUpto = -posJump;
-        curPositions = positions[upto];
-        return docID = docIDs[upto];
-      }
-    }
-
-    private final BytesRef payload = new BytesRef();
-
-    @Override
-    public BytesRef getPayload() {
-      if (payloads == null) {
-        return null;
-      } else {
-        final byte[] payloadBytes = payloads[upto][posUpto/(hasOffsets ? 3:1)];
-        if (payloadBytes == null) {
-          return null;
-        }
-        payload.bytes = payloadBytes;
-        payload.length = payloadBytes.length;
-        payload.offset = 0;
-        return payload;
-      }
-    }
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/memory/MemoryPostingsFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/memory/MemoryPostingsFormat.java
deleted file mode 100644
index 6bf8bc8..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/memory/MemoryPostingsFormat.java
+++ /dev/null
@@ -1,888 +0,0 @@
-package org.apache.lucene.codecs.memory;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Collections;
-import java.util.Comparator;
-import java.util.Iterator;
-import java.util.SortedMap;
-import java.util.TreeMap;
-
-import org.apache.lucene.codecs.FieldsConsumer;
-import org.apache.lucene.codecs.FieldsProducer;
-import org.apache.lucene.codecs.PostingsConsumer;
-import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.codecs.TermStats;
-import org.apache.lucene.codecs.TermsConsumer;
-import org.apache.lucene.index.DocsAndPositionsEnum;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.index.Terms;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.store.ByteArrayDataInput;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.store.RAMOutputStream;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IntsRef;
-import org.apache.lucene.util.fst.Builder;
-import org.apache.lucene.util.fst.ByteSequenceOutputs;
-import org.apache.lucene.util.fst.BytesRefFSTEnum;
-import org.apache.lucene.util.fst.FST;
-import org.apache.lucene.util.fst.Util;
-import org.apache.lucene.util.packed.PackedInts;
-
-// TODO: would be nice to somehow allow this to act like
-// InstantiatedIndex, by never writing to disk; ie you write
-// to this Codec in RAM only and then when you open a reader
-// it pulls the FST directly from what you wrote w/o going
-// to disk.
-
-/** Stores terms & postings (docs, positions, payloads) in
- *  RAM, using an FST.
- *
- * <p>Note that this codec implements advance as a linear
- * scan!  This means if you store large fields in here,
- * queries that rely on advance will (AND BooleanQuery,
- * PhraseQuery) will be relatively slow!
- *
- * <p><b>NOTE</b>: this codec cannot address more than ~2.1 GB
- * of postings, because the underlying FST uses an int
- * to address the underlying byte[].
- *
- * @lucene.experimental */
-
-// TODO: Maybe name this 'Cached' or something to reflect
-// the reality that it is actually written to disk, but
-// loads itself in ram?
-public class MemoryPostingsFormat extends PostingsFormat {
-
-  private final boolean doPackFST;
-  private final float acceptableOverheadRatio;
-
-  public MemoryPostingsFormat() {
-    this(false, PackedInts.DEFAULT);
-  }
-
-  public MemoryPostingsFormat(boolean doPackFST, float acceptableOverheadRatio) {
-    super("Memory");
-    this.doPackFST = doPackFST;
-    this.acceptableOverheadRatio = acceptableOverheadRatio;
-  }
-  
-  @Override
-  public String toString() {
-    return "PostingsFormat(name=" + getName() + " doPackFST= " + doPackFST + ")";
-  }
-
-  private final static class TermsWriter extends TermsConsumer {
-    private final IndexOutput out;
-    private final FieldInfo field;
-    private final Builder<BytesRef> builder;
-    private final ByteSequenceOutputs outputs = ByteSequenceOutputs.getSingleton();
-    private final boolean doPackFST;
-    private final float acceptableOverheadRatio;
-    private int termCount;
-
-    public TermsWriter(IndexOutput out, FieldInfo field, boolean doPackFST, float acceptableOverheadRatio) {
-      this.out = out;
-      this.field = field;
-      this.doPackFST = doPackFST;
-      this.acceptableOverheadRatio = acceptableOverheadRatio;
-      builder = new Builder<BytesRef>(FST.INPUT_TYPE.BYTE1, 0, 0, true, true, Integer.MAX_VALUE, outputs, null, doPackFST, acceptableOverheadRatio);
-    }
-
-    private class PostingsWriter extends PostingsConsumer {
-      private int lastDocID;
-      private int lastPos;
-      private int lastPayloadLen;
-
-      // NOTE: not private so we don't pay access check at runtime:
-      int docCount;
-      RAMOutputStream buffer = new RAMOutputStream();
-      
-      int lastOffsetLength;
-      int lastOffset;
-
-      @Override
-      public void startDoc(int docID, int termDocFreq) throws IOException {
-        //System.out.println("    startDoc docID=" + docID + " freq=" + termDocFreq);
-        final int delta = docID - lastDocID;
-        assert docID == 0 || delta > 0;
-        lastDocID = docID;
-        docCount++;
-
-        if (field.getIndexOptions() == IndexOptions.DOCS_ONLY) {
-          buffer.writeVInt(delta);
-        } else if (termDocFreq == 1) {
-          buffer.writeVInt((delta<<1) | 1);
-        } else {
-          buffer.writeVInt(delta<<1);
-          assert termDocFreq > 0;
-          buffer.writeVInt(termDocFreq);
-        }
-
-        lastPos = 0;
-        lastOffset = 0;
-      }
-
-      @Override
-      public void addPosition(int pos, BytesRef payload, int startOffset, int endOffset) throws IOException {
-        assert payload == null || field.hasPayloads();
-
-        //System.out.println("      addPos pos=" + pos + " payload=" + payload);
-
-        final int delta = pos - lastPos;
-        assert delta >= 0;
-        lastPos = pos;
-        
-        int payloadLen = 0;
-        
-        if (field.hasPayloads()) {
-          payloadLen = payload == null ? 0 : payload.length;
-          if (payloadLen != lastPayloadLen) {
-            lastPayloadLen = payloadLen;
-            buffer.writeVInt((delta<<1)|1);
-            buffer.writeVInt(payloadLen);
-          } else {
-            buffer.writeVInt(delta<<1);
-          }
-        } else {
-          buffer.writeVInt(delta);
-        }
-        
-        if (field.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0) {
-          // don't use startOffset - lastEndOffset, because this creates lots of negative vints for synonyms,
-          // and the numbers aren't that much smaller anyways.
-          int offsetDelta = startOffset - lastOffset;
-          int offsetLength = endOffset - startOffset;
-          if (offsetLength != lastOffsetLength) {
-            buffer.writeVInt(offsetDelta << 1 | 1);
-            buffer.writeVInt(offsetLength);
-          } else {
-            buffer.writeVInt(offsetDelta << 1);
-          }
-          lastOffset = startOffset;
-          lastOffsetLength = offsetLength;
-        }
-        
-        if (payloadLen > 0) {
-          buffer.writeBytes(payload.bytes, payload.offset, payloadLen);
-        }
-      }
-
-      @Override
-      public void finishDoc() {
-      }
-
-      public PostingsWriter reset() {
-        assert buffer.getFilePointer() == 0;
-        lastDocID = 0;
-        docCount = 0;
-        lastPayloadLen = 0;
-        // force first offset to write its length
-        lastOffsetLength = -1;
-        return this;
-      }
-    }
-
-    private final PostingsWriter postingsWriter = new PostingsWriter();
-
-    @Override
-    public PostingsConsumer startTerm(BytesRef text) {
-      //System.out.println("  startTerm term=" + text.utf8ToString());
-      return postingsWriter.reset();
-    }
-
-    private final RAMOutputStream buffer2 = new RAMOutputStream();
-    private final BytesRef spare = new BytesRef();
-    private byte[] finalBuffer = new byte[128];
-
-    private final IntsRef scratchIntsRef = new IntsRef();
-
-    @Override
-    public void finishTerm(BytesRef text, TermStats stats) throws IOException {
-
-      assert postingsWriter.docCount == stats.docFreq;
-
-      assert buffer2.getFilePointer() == 0;
-
-      buffer2.writeVInt(stats.docFreq);
-      if (field.getIndexOptions() != IndexOptions.DOCS_ONLY) {
-        buffer2.writeVLong(stats.totalTermFreq-stats.docFreq);
-      }
-      int pos = (int) buffer2.getFilePointer();
-      buffer2.writeTo(finalBuffer, 0);
-      buffer2.reset();
-
-      final int totalBytes = pos + (int) postingsWriter.buffer.getFilePointer();
-      if (totalBytes > finalBuffer.length) {
-        finalBuffer = ArrayUtil.grow(finalBuffer, totalBytes);
-      }
-      postingsWriter.buffer.writeTo(finalBuffer, pos);
-      postingsWriter.buffer.reset();
-
-      spare.bytes = finalBuffer;
-      spare.length = totalBytes;
-
-      //System.out.println("    finishTerm term=" + text.utf8ToString() + " " + totalBytes + " bytes totalTF=" + stats.totalTermFreq);
-      //for(int i=0;i<totalBytes;i++) {
-      //  System.out.println("      " + Integer.toHexString(finalBuffer[i]&0xFF));
-      //}
-
-      builder.add(Util.toIntsRef(text, scratchIntsRef), BytesRef.deepCopyOf(spare));
-      termCount++;
-    }
-
-    @Override
-    public void finish(long sumTotalTermFreq, long sumDocFreq, int docCount) throws IOException {
-      if (termCount > 0) {
-        out.writeVInt(termCount);
-        out.writeVInt(field.number);
-        if (field.getIndexOptions() != IndexOptions.DOCS_ONLY) {
-          out.writeVLong(sumTotalTermFreq);
-        }
-        out.writeVLong(sumDocFreq);
-        out.writeVInt(docCount);
-        FST<BytesRef> fst = builder.finish();
-        if (doPackFST) {
-          fst = fst.pack(3, Math.max(10, fst.getNodeCount()/4), acceptableOverheadRatio);
-        }
-        fst.save(out);
-        //System.out.println("finish field=" + field.name + " fp=" + out.getFilePointer());
-      }
-    }
-
-    @Override
-    public Comparator<BytesRef> getComparator() {
-      return BytesRef.getUTF8SortedAsUnicodeComparator();
-    }
-  }
-
-  private static String EXTENSION = "ram";
-
-  @Override
-  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-
-    final String fileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, EXTENSION);
-    final IndexOutput out = state.directory.createOutput(fileName, state.context);
-    
-    return new FieldsConsumer() {
-      @Override
-      public TermsConsumer addField(FieldInfo field) {
-        //System.out.println("\naddField field=" + field.name);
-        return new TermsWriter(out, field, doPackFST, acceptableOverheadRatio);
-      }
-
-      @Override
-      public void close() throws IOException {
-        // EOF marker:
-        try {
-          out.writeVInt(0);
-        } finally {
-          out.close();
-        }
-      }
-    };
-  }
-
-  private final static class FSTDocsEnum extends DocsEnum {
-    private final IndexOptions indexOptions;
-    private final boolean storePayloads;
-    private byte[] buffer = new byte[16];
-    private final ByteArrayDataInput in = new ByteArrayDataInput(buffer);
-
-    private Bits liveDocs;
-    private int docUpto;
-    private int docID = -1;
-    private int accum;
-    private int freq;
-    private int payloadLen;
-    private int numDocs;
-
-    public FSTDocsEnum(IndexOptions indexOptions, boolean storePayloads) {
-      this.indexOptions = indexOptions;
-      this.storePayloads = storePayloads;
-    }
-
-    public boolean canReuse(IndexOptions indexOptions, boolean storePayloads) {
-      return indexOptions == this.indexOptions && storePayloads == this.storePayloads;
-    }
-    
-    public FSTDocsEnum reset(BytesRef bufferIn, Bits liveDocs, int numDocs) {
-      assert numDocs > 0;
-      if (buffer.length < bufferIn.length - bufferIn.offset) {
-        buffer = ArrayUtil.grow(buffer, bufferIn.length - bufferIn.offset);
-      }
-      in.reset(buffer, 0, bufferIn.length - bufferIn.offset);
-      System.arraycopy(bufferIn.bytes, bufferIn.offset, buffer, 0, bufferIn.length - bufferIn.offset);
-      this.liveDocs = liveDocs;
-      docID = -1;
-      accum = 0;
-      docUpto = 0;
-      freq = 1;
-      payloadLen = 0;
-      this.numDocs = numDocs;
-      return this;
-    }
-
-    @Override
-    public int nextDoc() {
-      while(true) {
-        //System.out.println("  nextDoc cycle docUpto=" + docUpto + " numDocs=" + numDocs + " fp=" + in.getPosition() + " this=" + this);
-        if (docUpto == numDocs) {
-          // System.out.println("    END");
-          return docID = NO_MORE_DOCS;
-        }
-        docUpto++;
-        if (indexOptions == IndexOptions.DOCS_ONLY) {
-          accum += in.readVInt();
-        } else {
-          final int code = in.readVInt();
-          accum += code >>> 1;
-          //System.out.println("  docID=" + accum + " code=" + code);
-          if ((code & 1) != 0) {
-            freq = 1;
-          } else {
-            freq = in.readVInt();
-            assert freq > 0;
-          }
-
-          if (indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
-            // Skip positions/payloads
-            for(int posUpto=0;posUpto<freq;posUpto++) {
-              if (!storePayloads) {
-                in.readVInt();
-              } else {
-                final int posCode = in.readVInt();
-                if ((posCode & 1) != 0) {
-                  payloadLen = in.readVInt();
-                }
-                in.skipBytes(payloadLen);
-              }
-            }
-          } else if (indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) {
-            // Skip positions/offsets/payloads
-            for(int posUpto=0;posUpto<freq;posUpto++) {
-              int posCode = in.readVInt();
-              if (storePayloads && ((posCode & 1) != 0)) {
-                payloadLen = in.readVInt();
-              }
-              if ((in.readVInt() & 1) != 0) {
-                // new offset length
-                in.readVInt();
-              }
-              if (storePayloads) {
-                in.skipBytes(payloadLen);
-              }
-            }
-          }
-        }
-
-        if (liveDocs == null || liveDocs.get(accum)) {
-          //System.out.println("    return docID=" + accum + " freq=" + freq);
-          return (docID = accum);
-        }
-      }
-    }
-
-    @Override
-    public int docID() {
-      return docID;
-    }
-
-    @Override
-    public int advance(int target) {
-      // TODO: we could make more efficient version, but, it
-      // should be rare that this will matter in practice
-      // since usually apps will not store "big" fields in
-      // this codec!
-      //System.out.println("advance start docID=" + docID + " target=" + target);
-      while(nextDoc() < target) {
-      }
-      return docID;
-    }
-
-    @Override
-    public int freq() {
-      return freq;
-    }
-  }
-
-  private final static class FSTDocsAndPositionsEnum extends DocsAndPositionsEnum {
-    private final boolean storePayloads;
-    private byte[] buffer = new byte[16];
-    private final ByteArrayDataInput in = new ByteArrayDataInput(buffer);
-
-    private Bits liveDocs;
-    private int docUpto;
-    private int docID = -1;
-    private int accum;
-    private int freq;
-    private int numDocs;
-    private int posPending;
-    private int payloadLength;
-    final boolean storeOffsets;
-    int offsetLength;
-    int startOffset;
-
-    private int pos;
-    private final BytesRef payload = new BytesRef();
-
-    public FSTDocsAndPositionsEnum(boolean storePayloads, boolean storeOffsets) {
-      this.storePayloads = storePayloads;
-      this.storeOffsets = storeOffsets;
-    }
-
-    public boolean canReuse(boolean storePayloads, boolean storeOffsets) {
-      return storePayloads == this.storePayloads && storeOffsets == this.storeOffsets;
-    }
-    
-    public FSTDocsAndPositionsEnum reset(BytesRef bufferIn, Bits liveDocs, int numDocs) {
-      assert numDocs > 0;
-
-      // System.out.println("D&P reset bytes this=" + this);
-      // for(int i=bufferIn.offset;i<bufferIn.length;i++) {
-      //   System.out.println("  " + Integer.toHexString(bufferIn.bytes[i]&0xFF));
-      // }
-
-      if (buffer.length < bufferIn.length - bufferIn.offset) {
-        buffer = ArrayUtil.grow(buffer, bufferIn.length - bufferIn.offset);
-      }
-      in.reset(buffer, 0, bufferIn.length - bufferIn.offset);
-      System.arraycopy(bufferIn.bytes, bufferIn.offset, buffer, 0, bufferIn.length - bufferIn.offset);
-      this.liveDocs = liveDocs;
-      docID = -1;
-      accum = 0;
-      docUpto = 0;
-      payload.bytes = buffer;
-      payloadLength = 0;
-      this.numDocs = numDocs;
-      posPending = 0;
-      startOffset = storeOffsets ? 0 : -1; // always return -1 if no offsets are stored
-      offsetLength = 0;
-      return this;
-    }
-
-    @Override
-    public int nextDoc() {
-      while (posPending > 0) {
-        nextPosition();
-      }
-      while(true) {
-        //System.out.println("  nextDoc cycle docUpto=" + docUpto + " numDocs=" + numDocs + " fp=" + in.getPosition() + " this=" + this);
-        if (docUpto == numDocs) {
-          //System.out.println("    END");
-          return docID = NO_MORE_DOCS;
-        }
-        docUpto++;
-        
-        final int code = in.readVInt();
-        accum += code >>> 1;
-        if ((code & 1) != 0) {
-          freq = 1;
-        } else {
-          freq = in.readVInt();
-          assert freq > 0;
-        }
-
-        if (liveDocs == null || liveDocs.get(accum)) {
-          pos = 0;
-          startOffset = storeOffsets ? 0 : -1;
-          posPending = freq;
-          //System.out.println("    return docID=" + accum + " freq=" + freq);
-          return (docID = accum);
-        }
-
-        // Skip positions
-        for(int posUpto=0;posUpto<freq;posUpto++) {
-          if (!storePayloads) {
-            in.readVInt();
-          } else {
-            final int skipCode = in.readVInt();
-            if ((skipCode & 1) != 0) {
-              payloadLength = in.readVInt();
-              //System.out.println("    new payloadLen=" + payloadLength);
-            }
-          }
-          
-          if (storeOffsets) {
-            if ((in.readVInt() & 1) != 0) {
-              // new offset length
-              offsetLength = in.readVInt();
-            }
-          }
-          
-          if (storePayloads) {
-            in.skipBytes(payloadLength);
-          }
-        }
-      }
-    }
-
-    @Override
-    public int nextPosition() {
-      //System.out.println("    nextPos storePayloads=" + storePayloads + " this=" + this);
-      assert posPending > 0;
-      posPending--;
-      if (!storePayloads) {
-        pos += in.readVInt();
-      } else {
-        final int code = in.readVInt();
-        pos += code >>> 1;
-        if ((code & 1) != 0) {
-          payloadLength = in.readVInt();
-          //System.out.println("      new payloadLen=" + payloadLength);
-          //} else {
-          //System.out.println("      same payloadLen=" + payloadLength);
-        }
-      }
-      
-      if (storeOffsets) {
-        int offsetCode = in.readVInt();
-        if ((offsetCode & 1) != 0) {
-          // new offset length
-          offsetLength = in.readVInt();
-        }
-        startOffset += offsetCode >>> 1;
-      }
-      
-      if (storePayloads) {
-        payload.offset = in.getPosition();
-        in.skipBytes(payloadLength);
-        payload.length = payloadLength;
-      }
-
-      //System.out.println("      pos=" + pos + " payload=" + payload + " fp=" + in.getPosition());
-      return pos;
-    }
-
-    @Override
-    public int startOffset() {
-      return startOffset;
-    }
-
-    @Override
-    public int endOffset() {
-      return startOffset + offsetLength;
-    }
-
-    @Override
-    public BytesRef getPayload() {
-      return payload.length > 0 ? payload : null;
-    }
-
-    @Override
-    public int docID() {
-      return docID;
-    }
-
-    @Override
-    public int advance(int target) {
-      // TODO: we could make more efficient version, but, it
-      // should be rare that this will matter in practice
-      // since usually apps will not store "big" fields in
-      // this codec!
-      //System.out.println("advance target=" + target);
-      while(nextDoc() < target) {
-      }
-      //System.out.println("  return " + docID);
-      return docID;
-    }
-
-    @Override
-    public int freq() {
-      return freq;
-    }
-  }
-
-  private final static class FSTTermsEnum extends TermsEnum {
-    private final FieldInfo field;
-    private final BytesRefFSTEnum<BytesRef> fstEnum;
-    private final ByteArrayDataInput buffer = new ByteArrayDataInput();
-    private boolean didDecode;
-
-    private int docFreq;
-    private long totalTermFreq;
-    private BytesRefFSTEnum.InputOutput<BytesRef> current;
-
-    public FSTTermsEnum(FieldInfo field, FST<BytesRef> fst) {
-      this.field = field;
-      fstEnum = new BytesRefFSTEnum<BytesRef>(fst);
-    }
-
-    private void decodeMetaData() {
-      if (!didDecode) {
-        buffer.reset(current.output.bytes, 0, current.output.length);
-        docFreq = buffer.readVInt();
-        if (field.getIndexOptions() != IndexOptions.DOCS_ONLY) {
-          totalTermFreq = docFreq + buffer.readVLong();
-        } else {
-          totalTermFreq = -1;
-        }
-        current.output.offset = buffer.getPosition();
-        //System.out.println("  df=" + docFreq + " totTF=" + totalTermFreq + " offset=" + buffer.getPosition() + " len=" + current.output.length);
-        didDecode = true;
-      }
-    }
-
-    @Override
-    public boolean seekExact(BytesRef text, boolean useCache /* ignored */) throws IOException {
-      //System.out.println("te.seekExact text=" + field.name + ":" + text.utf8ToString() + " this=" + this);
-      current = fstEnum.seekExact(text);
-      didDecode = false;
-      return current != null;
-    }
-
-    @Override
-    public SeekStatus seekCeil(BytesRef text, boolean useCache /* ignored */) throws IOException {
-      //System.out.println("te.seek text=" + field.name + ":" + text.utf8ToString() + " this=" + this);
-      current = fstEnum.seekCeil(text);
-      if (current == null) {
-        return SeekStatus.END;
-      } else {
-
-        // System.out.println("  got term=" + current.input.utf8ToString());
-        // for(int i=0;i<current.output.length;i++) {
-        //   System.out.println("    " + Integer.toHexString(current.output.bytes[i]&0xFF));
-        // }
-
-        didDecode = false;
-
-        if (text.equals(current.input)) {
-          //System.out.println("  found!");
-          return SeekStatus.FOUND;
-        } else {
-          //System.out.println("  not found: " + current.input.utf8ToString());
-          return SeekStatus.NOT_FOUND;
-        }
-      }
-    }
-    
-    @Override
-    public DocsEnum docs(Bits liveDocs, DocsEnum reuse, int flags) {
-      decodeMetaData();
-      FSTDocsEnum docsEnum;
-
-      if (reuse == null || !(reuse instanceof FSTDocsEnum)) {
-        docsEnum = new FSTDocsEnum(field.getIndexOptions(), field.hasPayloads());
-      } else {
-        docsEnum = (FSTDocsEnum) reuse;        
-        if (!docsEnum.canReuse(field.getIndexOptions(), field.hasPayloads())) {
-          docsEnum = new FSTDocsEnum(field.getIndexOptions(), field.hasPayloads());
-        }
-      }
-      return docsEnum.reset(current.output, liveDocs, docFreq);
-    }
-
-    @Override
-    public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum reuse, int flags) {
-
-      boolean hasOffsets = field.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
-      if (field.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) < 0) {
-        return null;
-      }
-      decodeMetaData();
-      FSTDocsAndPositionsEnum docsAndPositionsEnum;
-      if (reuse == null || !(reuse instanceof FSTDocsAndPositionsEnum)) {
-        docsAndPositionsEnum = new FSTDocsAndPositionsEnum(field.hasPayloads(), hasOffsets);
-      } else {
-        docsAndPositionsEnum = (FSTDocsAndPositionsEnum) reuse;        
-        if (!docsAndPositionsEnum.canReuse(field.hasPayloads(), hasOffsets)) {
-          docsAndPositionsEnum = new FSTDocsAndPositionsEnum(field.hasPayloads(), hasOffsets);
-        }
-      }
-      //System.out.println("D&P reset this=" + this);
-      return docsAndPositionsEnum.reset(current.output, liveDocs, docFreq);
-    }
-
-    @Override
-    public BytesRef term() {
-      return current.input;
-    }
-
-    @Override
-    public BytesRef next() throws IOException {
-      //System.out.println("te.next");
-      current = fstEnum.next();
-      if (current == null) {
-        //System.out.println("  END");
-        return null;
-      }
-      didDecode = false;
-      //System.out.println("  term=" + field.name + ":" + current.input.utf8ToString());
-      return current.input;
-    }
-
-    @Override
-    public int docFreq() {
-      decodeMetaData();
-      return docFreq;
-    }
-
-    @Override
-    public long totalTermFreq() {
-      decodeMetaData();
-      return totalTermFreq;
-    }
-
-    @Override
-    public Comparator<BytesRef> getComparator() {
-      return BytesRef.getUTF8SortedAsUnicodeComparator();
-    }
-
-    @Override
-    public void seekExact(long ord) {
-      // NOTE: we could add this...
-      throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public long ord() {
-      // NOTE: we could add this...
-      throw new UnsupportedOperationException();
-    }
-  }
-
-  private final static class TermsReader extends Terms {
-
-    private final long sumTotalTermFreq;
-    private final long sumDocFreq;
-    private final int docCount;
-    private final int termCount;
-    private FST<BytesRef> fst;
-    private final ByteSequenceOutputs outputs = ByteSequenceOutputs.getSingleton();
-    private final FieldInfo field;
-
-    public TermsReader(FieldInfos fieldInfos, IndexInput in, int termCount) throws IOException {
-      this.termCount = termCount;
-      final int fieldNumber = in.readVInt();
-      field = fieldInfos.fieldInfo(fieldNumber);
-      if (field.getIndexOptions() != IndexOptions.DOCS_ONLY) {
-        sumTotalTermFreq = in.readVLong();
-      } else {
-        sumTotalTermFreq = -1;
-      }
-      sumDocFreq = in.readVLong();
-      docCount = in.readVInt();
-      
-      fst = new FST<BytesRef>(in, outputs);
-    }
-
-    @Override
-    public long getSumTotalTermFreq() {
-      return sumTotalTermFreq;
-    }
-
-    @Override
-    public long getSumDocFreq() {
-      return sumDocFreq;
-    }
-
-    @Override
-    public int getDocCount() {
-      return docCount;
-    }
-
-    @Override
-    public long size() {
-      return termCount;
-    }
-
-    @Override
-    public TermsEnum iterator(TermsEnum reuse) {
-      return new FSTTermsEnum(field, fst);
-    }
-
-    @Override
-    public Comparator<BytesRef> getComparator() {
-      return BytesRef.getUTF8SortedAsUnicodeComparator();
-    }
-
-    @Override
-    public boolean hasOffsets() {
-      return field.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
-    }
-
-    @Override
-    public boolean hasPositions() {
-      return field.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;
-    }
-    
-    @Override
-    public boolean hasPayloads() {
-      return field.hasPayloads();
-    }
-  }
-
-  @Override
-  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
-    final String fileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, EXTENSION);
-    final IndexInput in = state.dir.openInput(fileName, IOContext.READONCE);
-
-    final SortedMap<String,TermsReader> fields = new TreeMap<String,TermsReader>();
-
-    try {
-      while(true) {
-        final int termCount = in.readVInt();
-        if (termCount == 0) {
-          break;
-        }
-        final TermsReader termsReader = new TermsReader(state.fieldInfos, in, termCount);
-        // System.out.println("load field=" + termsReader.field.name);
-        fields.put(termsReader.field.name, termsReader);
-      }
-    } finally {
-      in.close();
-    }
-
-    return new FieldsProducer() {
-      @Override
-      public Iterator<String> iterator() {
-        return Collections.unmodifiableSet(fields.keySet()).iterator();
-      }
-
-      @Override
-      public Terms terms(String field) {
-        return fields.get(field);
-      }
-      
-      @Override
-      public int size() {
-        return fields.size();
-      }
-
-      @Override
-      public void close() {
-        // Drop ref to FST:
-        for(TermsReader termsReader : fields.values()) {
-          termsReader.fst = null;
-        }
-      }
-    };
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/memory/package.html b/lucene/core/src/java/org/apache/lucene/codecs/memory/package.html
deleted file mode 100644
index 340e831..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/memory/package.html
+++ /dev/null
@@ -1,25 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
-</head>
-<body>
-Postings format that is read entirely into memory.
-</body>
-</html>
\ No newline at end of file
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/pulsing/Pulsing40PostingsFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/pulsing/Pulsing40PostingsFormat.java
deleted file mode 100644
index faf8df2..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/pulsing/Pulsing40PostingsFormat.java
+++ /dev/null
@@ -1,45 +0,0 @@
-package org.apache.lucene.codecs.pulsing;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.codecs.BlockTreeTermsWriter;
-import org.apache.lucene.codecs.lucene40.Lucene40PostingsBaseFormat;
-import org.apache.lucene.codecs.lucene40.Lucene40PostingsFormat; // javadocs
-
-/**
- * Concrete pulsing implementation over {@link Lucene40PostingsFormat}.
- * 
- * @lucene.experimental
- */
-public class Pulsing40PostingsFormat extends PulsingPostingsFormat {
-
-  /** Inlines docFreq=1 terms, otherwise uses the normal "Lucene40" format. */
-  public Pulsing40PostingsFormat() {
-    this(1);
-  }
-
-  /** Inlines docFreq=<code>freqCutoff</code> terms, otherwise uses the normal "Lucene40" format. */
-  public Pulsing40PostingsFormat(int freqCutoff) {
-    this(freqCutoff, BlockTreeTermsWriter.DEFAULT_MIN_BLOCK_SIZE, BlockTreeTermsWriter.DEFAULT_MAX_BLOCK_SIZE);
-  }
-
-  /** Inlines docFreq=<code>freqCutoff</code> terms, otherwise uses the normal "Lucene40" format. */
-  public Pulsing40PostingsFormat(int freqCutoff, int minBlockSize, int maxBlockSize) {
-    super("Pulsing40", new Lucene40PostingsBaseFormat(), freqCutoff, minBlockSize, maxBlockSize);
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/pulsing/PulsingPostingsFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/pulsing/PulsingPostingsFormat.java
deleted file mode 100644
index da53cc5..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/pulsing/PulsingPostingsFormat.java
+++ /dev/null
@@ -1,120 +0,0 @@
-package org.apache.lucene.codecs.pulsing;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.BlockTreeTermsReader;
-import org.apache.lucene.codecs.BlockTreeTermsWriter;
-import org.apache.lucene.codecs.FieldsConsumer;
-import org.apache.lucene.codecs.FieldsProducer;
-import org.apache.lucene.codecs.PostingsBaseFormat;
-import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.codecs.PostingsReaderBase;
-import org.apache.lucene.codecs.PostingsWriterBase;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.util.IOUtils;
-
-/** This postings format "inlines" the postings for terms that have
- *  low docFreq.  It wraps another postings format, which is used for
- *  writing the non-inlined terms.
- *
- *  @lucene.experimental */
-
-public abstract class PulsingPostingsFormat extends PostingsFormat {
-
-  private final int freqCutoff;
-  private final int minBlockSize;
-  private final int maxBlockSize;
-  private final PostingsBaseFormat wrappedPostingsBaseFormat;
-  
-  public PulsingPostingsFormat(String name, PostingsBaseFormat wrappedPostingsBaseFormat, int freqCutoff) {
-    this(name, wrappedPostingsBaseFormat, freqCutoff, BlockTreeTermsWriter.DEFAULT_MIN_BLOCK_SIZE, BlockTreeTermsWriter.DEFAULT_MAX_BLOCK_SIZE);
-  }
-
-  /** Terms with freq <= freqCutoff are inlined into terms
-   *  dict. */
-  public PulsingPostingsFormat(String name, PostingsBaseFormat wrappedPostingsBaseFormat, int freqCutoff, int minBlockSize, int maxBlockSize) {
-    super(name);
-    this.freqCutoff = freqCutoff;
-    this.minBlockSize = minBlockSize;
-    assert minBlockSize > 1;
-    this.maxBlockSize = maxBlockSize;
-    this.wrappedPostingsBaseFormat = wrappedPostingsBaseFormat;
-  }
-
-  @Override
-  public String toString() {
-    return getName() + "(freqCutoff=" + freqCutoff + " minBlockSize=" + minBlockSize + " maxBlockSize=" + maxBlockSize + ")";
-  }
-
-  @Override
-  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-    PostingsWriterBase docsWriter = null;
-
-    // Terms that have <= freqCutoff number of docs are
-    // "pulsed" (inlined):
-    PostingsWriterBase pulsingWriter = null;
-
-    // Terms dict
-    boolean success = false;
-    try {
-      docsWriter = wrappedPostingsBaseFormat.postingsWriterBase(state);
-
-      // Terms that have <= freqCutoff number of docs are
-      // "pulsed" (inlined):
-      pulsingWriter = new PulsingPostingsWriter(freqCutoff, docsWriter);
-      FieldsConsumer ret = new BlockTreeTermsWriter(state, pulsingWriter, minBlockSize, maxBlockSize);
-      success = true;
-      return ret;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(docsWriter, pulsingWriter);
-      }
-    }
-  }
-
-  @Override
-  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
-    PostingsReaderBase docsReader = null;
-    PostingsReaderBase pulsingReader = null;
-
-    boolean success = false;
-    try {
-      docsReader = wrappedPostingsBaseFormat.postingsReaderBase(state);
-      pulsingReader = new PulsingPostingsReader(docsReader);
-      FieldsProducer ret = new BlockTreeTermsReader(
-                                                    state.dir, state.fieldInfos, state.segmentInfo,
-                                                    pulsingReader,
-                                                    state.context,
-                                                    state.segmentSuffix,
-                                                    state.termsIndexDivisor);
-      success = true;
-      return ret;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(docsReader, pulsingReader);
-      }
-    }
-  }
-
-  public int getFreqCutoff() {
-    return freqCutoff;
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/pulsing/PulsingPostingsReader.java b/lucene/core/src/java/org/apache/lucene/codecs/pulsing/PulsingPostingsReader.java
deleted file mode 100644
index 76fa37a..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/pulsing/PulsingPostingsReader.java
+++ /dev/null
@@ -1,629 +0,0 @@
-package org.apache.lucene.codecs.pulsing;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.IdentityHashMap;
-import java.util.Map;
-
-import org.apache.lucene.codecs.BlockTermState;
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.PostingsReaderBase;
-import org.apache.lucene.index.DocsAndPositionsEnum;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.TermState;
-import org.apache.lucene.store.ByteArrayDataInput;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.Attribute;
-import org.apache.lucene.util.AttributeImpl;
-import org.apache.lucene.util.AttributeSource;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-
-/** Concrete class that reads the current doc/freq/skip
- *  postings format 
- *  @lucene.experimental */
-
-// TODO: -- should we switch "hasProx" higher up?  and
-// create two separate docs readers, one that also reads
-// prox and one that doesn't?
-
-public class PulsingPostingsReader extends PostingsReaderBase {
-
-  // Fallback reader for non-pulsed terms:
-  final PostingsReaderBase wrappedPostingsReader;
-  int maxPositions;
-
-  public PulsingPostingsReader(PostingsReaderBase wrappedPostingsReader) {
-    this.wrappedPostingsReader = wrappedPostingsReader;
-  }
-
-  @Override
-  public void init(IndexInput termsIn) throws IOException {
-    CodecUtil.checkHeader(termsIn, PulsingPostingsWriter.CODEC,
-      PulsingPostingsWriter.VERSION_START, PulsingPostingsWriter.VERSION_START);
-    maxPositions = termsIn.readVInt();
-    wrappedPostingsReader.init(termsIn);
-  }
-
-  private static class PulsingTermState extends BlockTermState {
-    private byte[] postings;
-    private int postingsSize;                     // -1 if this term was not inlined
-    private BlockTermState wrappedTermState;
-
-    ByteArrayDataInput inlinedBytesReader;
-    private byte[] inlinedBytes;
-
-    @Override
-    public PulsingTermState clone() {
-      PulsingTermState clone;
-      clone = (PulsingTermState) super.clone();
-      if (postingsSize != -1) {
-        clone.postings = new byte[postingsSize];
-        System.arraycopy(postings, 0, clone.postings, 0, postingsSize);
-      } else {
-        assert wrappedTermState != null;
-        clone.wrappedTermState = (BlockTermState) wrappedTermState.clone();
-      }
-      return clone;
-    }
-
-    @Override
-    public void copyFrom(TermState _other) {
-      super.copyFrom(_other);
-      PulsingTermState other = (PulsingTermState) _other;
-      postingsSize = other.postingsSize;
-      if (other.postingsSize != -1) {
-        if (postings == null || postings.length < other.postingsSize) {
-          postings = new byte[ArrayUtil.oversize(other.postingsSize, 1)];
-        }
-        System.arraycopy(other.postings, 0, postings, 0, other.postingsSize);
-      } else {
-        wrappedTermState.copyFrom(other.wrappedTermState);
-      }
-
-      // NOTE: we do not copy the
-      // inlinedBytes/inlinedBytesReader; these are only
-      // stored on the "primary" TermState.  They are
-      // "transient" to cloned term states.
-    }
-
-    @Override
-    public String toString() {
-      if (postingsSize == -1) {
-        return "PulsingTermState: not inlined: wrapped=" + wrappedTermState;
-      } else {
-        return "PulsingTermState: inlined size=" + postingsSize + " " + super.toString();
-      }
-    }
-  }
-
-  @Override
-  public void readTermsBlock(IndexInput termsIn, FieldInfo fieldInfo, BlockTermState _termState) throws IOException {
-    //System.out.println("PR.readTermsBlock state=" + _termState);
-    final PulsingTermState termState = (PulsingTermState) _termState;
-    if (termState.inlinedBytes == null) {
-      termState.inlinedBytes = new byte[128];
-      termState.inlinedBytesReader = new ByteArrayDataInput();
-    }
-    int len = termsIn.readVInt();
-    //System.out.println("  len=" + len + " fp=" + termsIn.getFilePointer());
-    if (termState.inlinedBytes.length < len) {
-      termState.inlinedBytes = new byte[ArrayUtil.oversize(len, 1)];
-    }
-    termsIn.readBytes(termState.inlinedBytes, 0, len);
-    termState.inlinedBytesReader.reset(termState.inlinedBytes);
-    termState.wrappedTermState.termBlockOrd = 0;
-    wrappedPostingsReader.readTermsBlock(termsIn, fieldInfo, termState.wrappedTermState);
-  }
-
-  @Override
-  public BlockTermState newTermState() throws IOException {
-    PulsingTermState state = new PulsingTermState();
-    state.wrappedTermState = wrappedPostingsReader.newTermState();
-    return state;
-  }
-
-  @Override
-  public void nextTerm(FieldInfo fieldInfo, BlockTermState _termState) throws IOException {
-    //System.out.println("PR nextTerm");
-    PulsingTermState termState = (PulsingTermState) _termState;
-
-    // if we have positions, its total TF, otherwise its computed based on docFreq.
-    long count = fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 ? termState.totalTermFreq : termState.docFreq;
-    //System.out.println("  count=" + count + " threshold=" + maxPositions);
-
-    if (count <= maxPositions) {
-
-      // Inlined into terms dict -- just read the byte[] blob in,
-      // but don't decode it now (we only decode when a DocsEnum
-      // or D&PEnum is pulled):
-      termState.postingsSize = termState.inlinedBytesReader.readVInt();
-      if (termState.postings == null || termState.postings.length < termState.postingsSize) {
-        termState.postings = new byte[ArrayUtil.oversize(termState.postingsSize, 1)];
-      }
-      // TODO: sort of silly to copy from one big byte[]
-      // (the blob holding all inlined terms' blobs for
-      // current term block) into another byte[] (just the
-      // blob for this term)...
-      termState.inlinedBytesReader.readBytes(termState.postings, 0, termState.postingsSize);
-      //System.out.println("  inlined bytes=" + termState.postingsSize);
-    } else {
-      //System.out.println("  not inlined");
-      termState.postingsSize = -1;
-      // TODO: should we do full copyFrom?  much heavier...?
-      termState.wrappedTermState.docFreq = termState.docFreq;
-      termState.wrappedTermState.totalTermFreq = termState.totalTermFreq;
-      wrappedPostingsReader.nextTerm(fieldInfo, termState.wrappedTermState);
-      termState.wrappedTermState.termBlockOrd++;
-    }
-  }
-
-  @Override
-  public DocsEnum docs(FieldInfo field, BlockTermState _termState, Bits liveDocs, DocsEnum reuse, int flags) throws IOException {
-    PulsingTermState termState = (PulsingTermState) _termState;
-    if (termState.postingsSize != -1) {
-      PulsingDocsEnum postings;
-      if (reuse instanceof PulsingDocsEnum) {
-        postings = (PulsingDocsEnum) reuse;
-        if (!postings.canReuse(field)) {
-          postings = new PulsingDocsEnum(field);
-        }
-      } else {
-        // the 'reuse' is actually the wrapped enum
-        PulsingDocsEnum previous = (PulsingDocsEnum) getOther(reuse);
-        if (previous != null && previous.canReuse(field)) {
-          postings = previous;
-        } else {
-          postings = new PulsingDocsEnum(field);
-        }
-      }
-      if (reuse != postings) {
-        setOther(postings, reuse); // postings.other = reuse
-      }
-      return postings.reset(liveDocs, termState);
-    } else {
-      if (reuse instanceof PulsingDocsEnum) {
-        DocsEnum wrapped = wrappedPostingsReader.docs(field, termState.wrappedTermState, liveDocs, getOther(reuse), flags);
-        setOther(wrapped, reuse); // wrapped.other = reuse
-        return wrapped;
-      } else {
-        return wrappedPostingsReader.docs(field, termState.wrappedTermState, liveDocs, reuse, flags);
-      }
-    }
-  }
-
-  @Override
-  public DocsAndPositionsEnum docsAndPositions(FieldInfo field, BlockTermState _termState, Bits liveDocs, DocsAndPositionsEnum reuse,
-                                               int flags) throws IOException {
-
-    final PulsingTermState termState = (PulsingTermState) _termState;
-
-    if (termState.postingsSize != -1) {
-      PulsingDocsAndPositionsEnum postings;
-      if (reuse instanceof PulsingDocsAndPositionsEnum) {
-        postings = (PulsingDocsAndPositionsEnum) reuse;
-        if (!postings.canReuse(field)) {
-          postings = new PulsingDocsAndPositionsEnum(field);
-        }
-      } else {
-        // the 'reuse' is actually the wrapped enum
-        PulsingDocsAndPositionsEnum previous = (PulsingDocsAndPositionsEnum) getOther(reuse);
-        if (previous != null && previous.canReuse(field)) {
-          postings = previous;
-        } else {
-          postings = new PulsingDocsAndPositionsEnum(field);
-        }
-      }
-      if (reuse != postings) {
-        setOther(postings, reuse); // postings.other = reuse 
-      }
-      return postings.reset(liveDocs, termState);
-    } else {
-      if (reuse instanceof PulsingDocsAndPositionsEnum) {
-        DocsAndPositionsEnum wrapped = wrappedPostingsReader.docsAndPositions(field, termState.wrappedTermState, liveDocs, (DocsAndPositionsEnum) getOther(reuse),
-                                                                              flags);
-        setOther(wrapped, reuse); // wrapped.other = reuse
-        return wrapped;
-      } else {
-        return wrappedPostingsReader.docsAndPositions(field, termState.wrappedTermState, liveDocs, reuse, flags);
-      }
-    }
-  }
-
-  private static class PulsingDocsEnum extends DocsEnum {
-    private byte[] postingsBytes;
-    private final ByteArrayDataInput postings = new ByteArrayDataInput();
-    private final IndexOptions indexOptions;
-    private final boolean storePayloads;
-    private final boolean storeOffsets;
-    private Bits liveDocs;
-    private int docID = -1;
-    private int accum;
-    private int freq;
-    private int payloadLength;
-
-    public PulsingDocsEnum(FieldInfo fieldInfo) {
-      indexOptions = fieldInfo.getIndexOptions();
-      storePayloads = fieldInfo.hasPayloads();
-      storeOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
-    }
-
-    public PulsingDocsEnum reset(Bits liveDocs, PulsingTermState termState) {
-      //System.out.println("PR docsEnum termState=" + termState + " docFreq=" + termState.docFreq);
-      assert termState.postingsSize != -1;
-
-      // Must make a copy of termState's byte[] so that if
-      // app does TermsEnum.next(), this DocsEnum is not affected
-      if (postingsBytes == null) {
-        postingsBytes = new byte[termState.postingsSize];
-      } else if (postingsBytes.length < termState.postingsSize) {
-        postingsBytes = ArrayUtil.grow(postingsBytes, termState.postingsSize);
-      }
-      System.arraycopy(termState.postings, 0, postingsBytes, 0, termState.postingsSize);
-      postings.reset(postingsBytes, 0, termState.postingsSize);
-      docID = -1;
-      accum = 0;
-      freq = 1;
-      payloadLength = 0;
-      this.liveDocs = liveDocs;
-      return this;
-    }
-
-    boolean canReuse(FieldInfo fieldInfo) {
-      return indexOptions == fieldInfo.getIndexOptions() && storePayloads == fieldInfo.hasPayloads();
-    }
-
-    @Override
-    public int nextDoc() throws IOException {
-      //System.out.println("PR nextDoc this= "+ this);
-      while(true) {
-        if (postings.eof()) {
-          //System.out.println("PR   END");
-          return docID = NO_MORE_DOCS;
-        }
-
-        final int code = postings.readVInt();
-        //System.out.println("  read code=" + code);
-        if (indexOptions == IndexOptions.DOCS_ONLY) {
-          accum += code;
-        } else {
-          accum += code >>> 1;              // shift off low bit
-          if ((code & 1) != 0) {          // if low bit is set
-            freq = 1;                     // freq is one
-          } else {
-            freq = postings.readVInt();     // else read freq
-          }
-
-          if (indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0) {
-            // Skip positions
-            if (storePayloads) {
-              for(int pos=0;pos<freq;pos++) {
-                final int posCode = postings.readVInt();
-                if ((posCode & 1) != 0) {
-                  payloadLength = postings.readVInt();
-                }
-                if (storeOffsets && (postings.readVInt() & 1) != 0) {
-                  // new offset length
-                  postings.readVInt();
-                }
-                if (payloadLength != 0) {
-                  postings.skipBytes(payloadLength);
-                }
-              }
-            } else {
-              for(int pos=0;pos<freq;pos++) {
-                // TODO: skipVInt
-                postings.readVInt();
-                if (storeOffsets && (postings.readVInt() & 1) != 0) {
-                  // new offset length
-                  postings.readVInt();
-                }
-              }
-            }
-          }
-        }
-
-        if (liveDocs == null || liveDocs.get(accum)) {
-          return (docID = accum);
-        }
-      }
-    }
-
-    @Override
-    public int freq() throws IOException {
-      return freq;
-    }
-
-    @Override
-    public int docID() {
-      return docID;
-    }
-
-    @Override
-    public int advance(int target) throws IOException {
-      int doc;
-      while((doc=nextDoc()) != NO_MORE_DOCS) {
-        if (doc >= target)
-          return doc;
-      }
-      return docID = NO_MORE_DOCS;
-    }
-  }
-
-  private static class PulsingDocsAndPositionsEnum extends DocsAndPositionsEnum {
-    private byte[] postingsBytes;
-    private final ByteArrayDataInput postings = new ByteArrayDataInput();
-    private final boolean storePayloads;
-    private final boolean storeOffsets;
-    // note: we could actually reuse across different options, if we passed this to reset()
-    // and re-init'ed storeOffsets accordingly (made it non-final)
-    private final IndexOptions indexOptions;
-
-    private Bits liveDocs;
-    private int docID = -1;
-    private int accum;
-    private int freq;
-    private int posPending;
-    private int position;
-    private int payloadLength;
-    private BytesRef payload;
-    private int startOffset;
-    private int offsetLength;
-
-    private boolean payloadRetrieved;
-
-    public PulsingDocsAndPositionsEnum(FieldInfo fieldInfo) {
-      indexOptions = fieldInfo.getIndexOptions();
-      storePayloads = fieldInfo.hasPayloads();
-      storeOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
-    }
-
-    boolean canReuse(FieldInfo fieldInfo) {
-      return indexOptions == fieldInfo.getIndexOptions() && storePayloads == fieldInfo.hasPayloads();
-    }
-
-    public PulsingDocsAndPositionsEnum reset(Bits liveDocs, PulsingTermState termState) {
-      assert termState.postingsSize != -1;
-      if (postingsBytes == null) {
-        postingsBytes = new byte[termState.postingsSize];
-      } else if (postingsBytes.length < termState.postingsSize) {
-        postingsBytes = ArrayUtil.grow(postingsBytes, termState.postingsSize);
-      }
-      System.arraycopy(termState.postings, 0, postingsBytes, 0, termState.postingsSize);
-      postings.reset(postingsBytes, 0, termState.postingsSize);
-      this.liveDocs = liveDocs;
-      payloadLength = 0;
-      posPending = 0;
-      docID = -1;
-      accum = 0;
-      startOffset = storeOffsets ? 0 : -1; // always return -1 if no offsets are stored
-      offsetLength = 0;
-      //System.out.println("PR d&p reset storesPayloads=" + storePayloads + " bytes=" + bytes.length + " this=" + this);
-      return this;
-    }
-
-    @Override
-    public int nextDoc() throws IOException {
-      //System.out.println("PR d&p nextDoc this=" + this);
-
-      while(true) {
-        //System.out.println("  cycle skip posPending=" + posPending);
-
-        skipPositions();
-
-        if (postings.eof()) {
-          //System.out.println("PR   END");
-          return docID = NO_MORE_DOCS;
-        }
-
-        final int code = postings.readVInt();
-        accum += code >>> 1;            // shift off low bit
-        if ((code & 1) != 0) {          // if low bit is set
-          freq = 1;                     // freq is one
-        } else {
-          freq = postings.readVInt();     // else read freq
-        }
-        posPending = freq;
-        startOffset = storeOffsets ? 0 : -1; // always return -1 if no offsets are stored
-
-        if (liveDocs == null || liveDocs.get(accum)) {
-          //System.out.println("  return docID=" + docID + " freq=" + freq);
-          position = 0;
-          return (docID = accum);
-        }
-      }
-    }
-
-    @Override
-    public int freq() throws IOException {
-      return freq;
-    }
-
-    @Override
-    public int docID() {
-      return docID;
-    }
-
-    @Override
-    public int advance(int target) throws IOException {
-      int doc;
-      while((doc=nextDoc()) != NO_MORE_DOCS) {
-        if (doc >= target) {
-          return docID = doc;
-        }
-      }
-      return docID = NO_MORE_DOCS;
-    }
-
-    @Override
-    public int nextPosition() throws IOException {
-      //System.out.println("PR d&p nextPosition posPending=" + posPending + " vs freq=" + freq);
-      
-      assert posPending > 0;
-      posPending--;
-
-      if (storePayloads) {
-        if (!payloadRetrieved) {
-          //System.out.println("PR     skip payload=" + payloadLength);
-          postings.skipBytes(payloadLength);
-        }
-        final int code = postings.readVInt();
-        //System.out.println("PR     code=" + code);
-        if ((code & 1) != 0) {
-          payloadLength = postings.readVInt();
-          //System.out.println("PR     new payload len=" + payloadLength);
-        }
-        position += code >>> 1;
-        payloadRetrieved = false;
-      } else {
-        position += postings.readVInt();
-      }
-      
-      if (storeOffsets) {
-        int offsetCode = postings.readVInt();
-        if ((offsetCode & 1) != 0) {
-          // new offset length
-          offsetLength = postings.readVInt();
-        }
-        startOffset += offsetCode >>> 1;
-      }
-
-      //System.out.println("PR d&p nextPos return pos=" + position + " this=" + this);
-      return position;
-    }
-
-    @Override
-    public int startOffset() {
-      return startOffset;
-    }
-
-    @Override
-    public int endOffset() {
-      return startOffset + offsetLength;
-    }
-
-    private void skipPositions() throws IOException {
-      while(posPending != 0) {
-        nextPosition();
-      }
-      if (storePayloads && !payloadRetrieved) {
-        //System.out.println("  skip payload len=" + payloadLength);
-        postings.skipBytes(payloadLength);
-        payloadRetrieved = true;
-      }
-    }
-
-    @Override
-    public BytesRef getPayload() throws IOException {
-      //System.out.println("PR  getPayload payloadLength=" + payloadLength + " this=" + this);
-      if (payloadRetrieved) {
-        return payload;
-      } else if (storePayloads && payloadLength > 0) {
-        payloadRetrieved = true;
-        if (payload == null) {
-          payload = new BytesRef(payloadLength);
-        } else {
-          payload.grow(payloadLength);
-        }
-        postings.readBytes(payload.bytes, 0, payloadLength);
-        payload.length = payloadLength;
-        return payload;
-      } else {
-        return null;
-      }
-    }
-  }
-
-  @Override
-  public void close() throws IOException {
-    wrappedPostingsReader.close();
-  }
-  
-  /** for a docsenum, gets the 'other' reused enum.
-   * Example: Pulsing(Standard).
-   * when doing a term range query you are switching back and forth
-   * between Pulsing and Standard
-   * 
-   * The way the reuse works is that Pulsing.other = Standard and
-   * Standard.other = Pulsing.
-   */
-  private DocsEnum getOther(DocsEnum de) {
-    if (de == null) {
-      return null;
-    } else {
-      final AttributeSource atts = de.attributes();
-      return atts.addAttribute(PulsingEnumAttribute.class).enums().get(this);
-    }
-  }
-  
-  /** 
-   * for a docsenum, sets the 'other' reused enum.
-   * see getOther for an example.
-   */
-  private DocsEnum setOther(DocsEnum de, DocsEnum other) {
-    final AttributeSource atts = de.attributes();
-    return atts.addAttribute(PulsingEnumAttribute.class).enums().put(this, other);
-  }
-
-  /** 
-   * A per-docsenum attribute that stores additional reuse information
-   * so that pulsing enums can keep a reference to their wrapped enums,
-   * and vice versa. this way we can always reuse.
-   * 
-   * @lucene.internal */
-  public static interface PulsingEnumAttribute extends Attribute {
-    public Map<PulsingPostingsReader,DocsEnum> enums();
-  }
-    
-  /** 
-   * Implementation of {@link PulsingEnumAttribute} for reuse of
-   * wrapped postings readers underneath pulsing.
-   * 
-   * @lucene.internal */
-  public static final class PulsingEnumAttributeImpl extends AttributeImpl implements PulsingEnumAttribute {
-    // we could store 'other', but what if someone 'chained' multiple postings readers,
-    // this could cause problems?
-    // TODO: we should consider nuking this map and just making it so if you do this,
-    // you don't reuse? and maybe pulsingPostingsReader should throw an exc if it wraps
-    // another pulsing, because this is just stupid and wasteful. 
-    // we still have to be careful in case someone does Pulsing(Stomping(Pulsing(...
-    private final Map<PulsingPostingsReader,DocsEnum> enums = 
-      new IdentityHashMap<PulsingPostingsReader,DocsEnum>();
-      
-    public Map<PulsingPostingsReader,DocsEnum> enums() {
-      return enums;
-    }
-
-    @Override
-    public void clear() {
-      // our state is per-docsenum, so this makes no sense.
-      // its best not to clear, in case a wrapped enum has a per-doc attribute or something
-      // and is calling clearAttributes(), so they don't nuke the reuse information!
-    }
-
-    @Override
-    public void copyTo(AttributeImpl target) {
-      // this makes no sense for us, because our state is per-docsenum.
-      // we don't want to copy any stuff over to another docsenum ever!
-    }
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/pulsing/PulsingPostingsWriter.java b/lucene/core/src/java/org/apache/lucene/codecs/pulsing/PulsingPostingsWriter.java
deleted file mode 100644
index 6ba0ef6..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/pulsing/PulsingPostingsWriter.java
+++ /dev/null
@@ -1,419 +0,0 @@
-package org.apache.lucene.codecs.pulsing;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.List;
-import java.util.ArrayList;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.PostingsWriterBase;
-import org.apache.lucene.codecs.TermStats;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.store.RAMOutputStream;
-import org.apache.lucene.util.BytesRef;
-
-// TODO: we now inline based on total TF of the term,
-// but it might be better to inline by "net bytes used"
-// so that a term that has only 1 posting but a huge
-// payload would not be inlined.  Though this is
-// presumably rare in practice...
-
-/** 
- * Writer for the pulsing format. 
- * <p>
- * Wraps another postings implementation and decides 
- * (based on total number of occurrences), whether a terms 
- * postings should be inlined into the term dictionary,
- * or passed through to the wrapped writer.
- *
- * @lucene.experimental */
-public final class PulsingPostingsWriter extends PostingsWriterBase {
-
-  final static String CODEC = "PulsedPostingsWriter";
-
-  // To add a new version, increment from the last one, and
-  // change VERSION_CURRENT to point to your new version:
-  final static int VERSION_START = 0;
-
-  final static int VERSION_CURRENT = VERSION_START;
-
-  private IndexOutput termsOut;
-
-  private IndexOptions indexOptions;
-  private boolean storePayloads;
-
-  private static class PendingTerm {
-    private final byte[] bytes;
-    public PendingTerm(byte[] bytes) {
-      this.bytes = bytes;
-    }
-  }
-
-  private final List<PendingTerm> pendingTerms = new ArrayList<PendingTerm>();
-
-  // one entry per position
-  private final Position[] pending;
-  private int pendingCount = 0;                           // -1 once we've hit too many positions
-  private Position currentDoc;                    // first Position entry of current doc
-
-  private static final class Position {
-    BytesRef payload;
-    int termFreq;                                 // only incremented on first position for a given doc
-    int pos;
-    int docID;
-    int startOffset;
-    int endOffset;
-  }
-
-  // TODO: -- lazy init this?  ie, if every single term
-  // was inlined (eg for a "primary key" field) then we
-  // never need to use this fallback?  Fallback writer for
-  // non-inlined terms:
-  final PostingsWriterBase wrappedPostingsWriter;
-
-  /** If the total number of positions (summed across all docs
-   *  for this term) is <= maxPositions, then the postings are
-   *  inlined into terms dict */
-  public PulsingPostingsWriter(int maxPositions, PostingsWriterBase wrappedPostingsWriter) {
-    pending = new Position[maxPositions];
-    for(int i=0;i<maxPositions;i++) {
-      pending[i] = new Position();
-    }
-
-    // We simply wrap another postings writer, but only call
-    // on it when tot positions is >= the cutoff:
-    this.wrappedPostingsWriter = wrappedPostingsWriter;
-  }
-
-  @Override
-  public void start(IndexOutput termsOut) throws IOException {
-    this.termsOut = termsOut;
-    CodecUtil.writeHeader(termsOut, CODEC, VERSION_CURRENT);
-    termsOut.writeVInt(pending.length); // encode maxPositions in header
-    wrappedPostingsWriter.start(termsOut);
-  }
-
-  @Override
-  public void startTerm() {
-    //if (DEBUG) System.out.println("PW   startTerm");
-    assert pendingCount == 0;
-  }
-
-  // TODO: -- should we NOT reuse across fields?  would
-  // be cleaner
-
-  // Currently, this instance is re-used across fields, so
-  // our parent calls setField whenever the field changes
-  @Override
-  public void setField(FieldInfo fieldInfo) {
-    this.indexOptions = fieldInfo.getIndexOptions();
-    //if (DEBUG) System.out.println("PW field=" + fieldInfo.name + " indexOptions=" + indexOptions);
-    storePayloads = fieldInfo.hasPayloads();
-    wrappedPostingsWriter.setField(fieldInfo);
-    //DEBUG = BlockTreeTermsWriter.DEBUG;
-  }
-
-  private boolean DEBUG;
-
-  @Override
-  public void startDoc(int docID, int termDocFreq) throws IOException {
-    assert docID >= 0: "got docID=" + docID;
-
-    /*
-    if (termID != -1) {
-      if (docID == 0) {
-        baseDocID = termID;
-      } else if (baseDocID + docID != termID) {
-        throw new RuntimeException("WRITE: baseDocID=" + baseDocID + " docID=" + docID + " termID=" + termID);
-      }
-    }
-    */
-
-    //if (DEBUG) System.out.println("PW     doc=" + docID);
-
-    if (pendingCount == pending.length) {
-      push();
-      //if (DEBUG) System.out.println("PW: wrapped.finishDoc");
-      wrappedPostingsWriter.finishDoc();
-    }
-
-    if (pendingCount != -1) {
-      assert pendingCount < pending.length;
-      currentDoc = pending[pendingCount];
-      currentDoc.docID = docID;
-      if (indexOptions == IndexOptions.DOCS_ONLY) {
-        pendingCount++;
-      } else if (indexOptions == IndexOptions.DOCS_AND_FREQS) { 
-        pendingCount++;
-        currentDoc.termFreq = termDocFreq;
-      } else {
-        currentDoc.termFreq = termDocFreq;
-      }
-    } else {
-      // We've already seen too many docs for this term --
-      // just forward to our fallback writer
-      wrappedPostingsWriter.startDoc(docID, termDocFreq);
-    }
-  }
-
-  @Override
-  public void addPosition(int position, BytesRef payload, int startOffset, int endOffset) throws IOException {
-
-    //if (DEBUG) System.out.println("PW       pos=" + position + " payload=" + (payload == null ? "null" : payload.length + " bytes"));
-    if (pendingCount == pending.length) {
-      push();
-    }
-
-    if (pendingCount == -1) {
-      // We've already seen too many docs for this term --
-      // just forward to our fallback writer
-      wrappedPostingsWriter.addPosition(position, payload, startOffset, endOffset);
-    } else {
-      // buffer up
-      final Position pos = pending[pendingCount++];
-      pos.pos = position;
-      pos.startOffset = startOffset;
-      pos.endOffset = endOffset;
-      pos.docID = currentDoc.docID;
-      if (payload != null && payload.length > 0) {
-        if (pos.payload == null) {
-          pos.payload = BytesRef.deepCopyOf(payload);
-        } else {
-          pos.payload.copyBytes(payload);
-        }
-      } else if (pos.payload != null) {
-        pos.payload.length = 0;
-      }
-    }
-  }
-
-  @Override
-  public void finishDoc() throws IOException {
-    // if (DEBUG) System.out.println("PW     finishDoc");
-    if (pendingCount == -1) {
-      wrappedPostingsWriter.finishDoc();
-    }
-  }
-
-  private final RAMOutputStream buffer = new RAMOutputStream();
-
-  // private int baseDocID;
-
-  /** Called when we are done adding docs to this term */
-  @Override
-  public void finishTerm(TermStats stats) throws IOException {
-    // if (DEBUG) System.out.println("PW   finishTerm docCount=" + stats.docFreq + " pendingCount=" + pendingCount + " pendingTerms.size()=" + pendingTerms.size());
-
-    assert pendingCount > 0 || pendingCount == -1;
-
-    if (pendingCount == -1) {
-      wrappedPostingsWriter.finishTerm(stats);
-      // Must add null entry to record terms that our
-      // wrapped postings impl added
-      pendingTerms.add(null);
-    } else {
-
-      // There were few enough total occurrences for this
-      // term, so we fully inline our postings data into
-      // terms dict, now:
-
-      // TODO: it'd be better to share this encoding logic
-      // in some inner codec that knows how to write a
-      // single doc / single position, etc.  This way if a
-      // given codec wants to store other interesting
-      // stuff, it could use this pulsing codec to do so
-
-      if (indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0) {
-        int lastDocID = 0;
-        int pendingIDX = 0;
-        int lastPayloadLength = -1;
-        int lastOffsetLength = -1;
-        while(pendingIDX < pendingCount) {
-          final Position doc = pending[pendingIDX];
-
-          final int delta = doc.docID - lastDocID;
-          lastDocID = doc.docID;
-
-          // if (DEBUG) System.out.println("  write doc=" + doc.docID + " freq=" + doc.termFreq);
-
-          if (doc.termFreq == 1) {
-            buffer.writeVInt((delta<<1)|1);
-          } else {
-            buffer.writeVInt(delta<<1);
-            buffer.writeVInt(doc.termFreq);
-          }
-
-          int lastPos = 0;
-          int lastOffset = 0;
-          for(int posIDX=0;posIDX<doc.termFreq;posIDX++) {
-            final Position pos = pending[pendingIDX++];
-            assert pos.docID == doc.docID;
-            final int posDelta = pos.pos - lastPos;
-            lastPos = pos.pos;
-            // if (DEBUG) System.out.println("    write pos=" + pos.pos);
-            final int payloadLength = pos.payload == null ? 0 : pos.payload.length;
-            if (storePayloads) {
-              if (payloadLength != lastPayloadLength) {
-                buffer.writeVInt((posDelta << 1)|1);
-                buffer.writeVInt(payloadLength);
-                lastPayloadLength = payloadLength;
-              } else {
-                buffer.writeVInt(posDelta << 1);
-              }
-            } else {
-              buffer.writeVInt(posDelta);
-            }
-            
-            if (indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0) {
-              //System.out.println("write=" + pos.startOffset + "," + pos.endOffset);
-              int offsetDelta = pos.startOffset - lastOffset;
-              int offsetLength = pos.endOffset - pos.startOffset;
-              if (offsetLength != lastOffsetLength) {
-                buffer.writeVInt(offsetDelta << 1 | 1);
-                buffer.writeVInt(offsetLength);
-              } else {
-                buffer.writeVInt(offsetDelta << 1);
-              }
-              lastOffset = pos.startOffset;
-              lastOffsetLength = offsetLength;             
-            }
-            
-            if (payloadLength > 0) {
-              assert storePayloads;
-              buffer.writeBytes(pos.payload.bytes, 0, pos.payload.length);
-            }
-          }
-        }
-      } else if (indexOptions == IndexOptions.DOCS_AND_FREQS) {
-        int lastDocID = 0;
-        for(int posIDX=0;posIDX<pendingCount;posIDX++) {
-          final Position doc = pending[posIDX];
-          final int delta = doc.docID - lastDocID;
-          assert doc.termFreq != 0;
-          if (doc.termFreq == 1) {
-            buffer.writeVInt((delta<<1)|1);
-          } else {
-            buffer.writeVInt(delta<<1);
-            buffer.writeVInt(doc.termFreq);
-          }
-          lastDocID = doc.docID;
-        }
-      } else if (indexOptions == IndexOptions.DOCS_ONLY) {
-        int lastDocID = 0;
-        for(int posIDX=0;posIDX<pendingCount;posIDX++) {
-          final Position doc = pending[posIDX];
-          buffer.writeVInt(doc.docID - lastDocID);
-          lastDocID = doc.docID;
-        }
-      }
-
-      final byte[] bytes = new byte[(int) buffer.getFilePointer()];
-      buffer.writeTo(bytes, 0);
-      pendingTerms.add(new PendingTerm(bytes));
-      buffer.reset();
-    }
-
-    pendingCount = 0;
-  }
-
-  @Override
-  public void close() throws IOException {
-    wrappedPostingsWriter.close();
-  }
-
-  @Override
-  public void flushTermsBlock(int start, int count) throws IOException {
-    // if (DEBUG) System.out.println("PW: flushTermsBlock start=" + start + " count=" + count + " pendingTerms.size()=" + pendingTerms.size());
-    int wrappedCount = 0;
-    assert buffer.getFilePointer() == 0;
-    assert start >= count;
-
-    final int limit = pendingTerms.size() - start + count;
-
-    for(int idx=pendingTerms.size()-start; idx<limit; idx++) {
-      final PendingTerm term = pendingTerms.get(idx);
-      if (term == null) {
-        wrappedCount++;
-      } else {
-        buffer.writeVInt(term.bytes.length);
-        buffer.writeBytes(term.bytes, 0, term.bytes.length);
-      }
-    }
-
-    termsOut.writeVInt((int) buffer.getFilePointer());
-    buffer.writeTo(termsOut);
-    buffer.reset();
-
-    // TDOO: this could be somewhat costly since
-    // pendingTerms.size() could be biggish?
-    int futureWrappedCount = 0;
-    final int limit2 = pendingTerms.size();
-    for(int idx=limit;idx<limit2;idx++) {
-      if (pendingTerms.get(idx) == null) {
-        futureWrappedCount++;
-      }
-    }
-
-    // Remove the terms we just wrote:
-    pendingTerms.subList(pendingTerms.size()-start, limit).clear();
-
-    // if (DEBUG) System.out.println("PW:   len=" + buffer.getFilePointer() + " fp=" + termsOut.getFilePointer() + " futureWrappedCount=" + futureWrappedCount + " wrappedCount=" + wrappedCount);
-    // TODO: can we avoid calling this if all terms
-    // were inlined...?  Eg for a "primary key" field, the
-    // wrapped codec is never invoked...
-    wrappedPostingsWriter.flushTermsBlock(futureWrappedCount+wrappedCount, wrappedCount);
-  }
-
-  // Pushes pending positions to the wrapped codec
-  private void push() throws IOException {
-    // if (DEBUG) System.out.println("PW now push @ " + pendingCount + " wrapped=" + wrappedPostingsWriter);
-    assert pendingCount == pending.length;
-      
-    wrappedPostingsWriter.startTerm();
-      
-    // Flush all buffered docs
-    if (indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0) {
-      Position doc = null;
-      for(Position pos : pending) {
-        if (doc == null) {
-          doc = pos;
-          // if (DEBUG) System.out.println("PW: wrapped.startDoc docID=" + doc.docID + " tf=" + doc.termFreq);
-          wrappedPostingsWriter.startDoc(doc.docID, doc.termFreq);
-        } else if (doc.docID != pos.docID) {
-          assert pos.docID > doc.docID;
-          // if (DEBUG) System.out.println("PW: wrapped.finishDoc");
-          wrappedPostingsWriter.finishDoc();
-          doc = pos;
-          // if (DEBUG) System.out.println("PW: wrapped.startDoc docID=" + doc.docID + " tf=" + doc.termFreq);
-          wrappedPostingsWriter.startDoc(doc.docID, doc.termFreq);
-        }
-        // if (DEBUG) System.out.println("PW:   wrapped.addPos pos=" + pos.pos);
-        wrappedPostingsWriter.addPosition(pos.pos, pos.payload, pos.startOffset, pos.endOffset);
-      }
-      //wrappedPostingsWriter.finishDoc();
-    } else {
-      for(Position doc : pending) {
-        wrappedPostingsWriter.startDoc(doc.docID, indexOptions == IndexOptions.DOCS_ONLY ? 0 : doc.termFreq);
-      }
-    }
-    pendingCount = -1;
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/pulsing/package.html b/lucene/core/src/java/org/apache/lucene/codecs/pulsing/package.html
deleted file mode 100644
index 4216cc6..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/pulsing/package.html
+++ /dev/null
@@ -1,25 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
-</head>
-<body>
-Pulsing Codec: inlines low frequency terms' postings into terms dictionary.
-</body>
-</html>
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/sep/IntIndexInput.java b/lucene/core/src/java/org/apache/lucene/codecs/sep/IntIndexInput.java
deleted file mode 100644
index 93640e0..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/sep/IntIndexInput.java
+++ /dev/null
@@ -1,58 +0,0 @@
-package org.apache.lucene.codecs.sep;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.Closeable;
-import java.io.IOException;
-
-import org.apache.lucene.store.DataInput;
-
-/** Defines basic API for writing ints to an IndexOutput.
- *  IntBlockCodec interacts with this API. @see
- *  IntBlockReader
- *
- * @lucene.experimental */
-public abstract class IntIndexInput implements Closeable {
-
-  public abstract Reader reader() throws IOException;
-
-  public abstract void close() throws IOException;
-
-  public abstract Index index() throws IOException;
-  
-  /** Records a single skip-point in the {@link IntIndexInput.Reader}. */
-  public abstract static class Index {
-
-    public abstract void read(DataInput indexIn, boolean absolute) throws IOException;
-
-    /** Seeks primary stream to the last read offset */
-    public abstract void seek(IntIndexInput.Reader stream) throws IOException;
-
-    public abstract void copyFrom(Index other);
-    
-    @Override
-    public abstract Index clone();
-  }
-
-  /** Reads int values. */
-  public abstract static class Reader {
-
-    /** Reads next single int */
-    public abstract int next() throws IOException;
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/sep/IntIndexOutput.java b/lucene/core/src/java/org/apache/lucene/codecs/sep/IntIndexOutput.java
deleted file mode 100644
index fd1eb49..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/sep/IntIndexOutput.java
+++ /dev/null
@@ -1,60 +0,0 @@
-package org.apache.lucene.codecs.sep;
-
-/**
- * LICENSED to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-// TODO: we may want tighter integration w/ IndexOutput --
-// may give better perf:
-
-import org.apache.lucene.store.IndexOutput;
-
-import java.io.IOException;
-import java.io.Closeable;
-
-/** Defines basic API for writing ints to an IndexOutput.
- *  IntBlockCodec interacts with this API. @see
- *  IntBlockReader.
- *
- * <p>NOTE: block sizes could be variable
- *
- * @lucene.experimental */
-public abstract class IntIndexOutput implements Closeable {
-
-  /** Write an int to the primary file.  The value must be
-   * >= 0.  */
-  public abstract void write(int v) throws IOException;
-
-  /** Records a single skip-point in the IndexOutput. */
-  public abstract static class Index {
-
-    /** Internally records the current location */
-    public abstract void mark() throws IOException;
-
-    /** Copies index from other */
-    public abstract void copyFrom(Index other, boolean copyLast) throws IOException;
-
-    /** Writes "location" of current output pointer of primary
-     *  output to different output (out) */
-    public abstract void write(IndexOutput indexOut, boolean absolute) throws IOException;
-  }
-
-  /** If you are indexing the primary output file, call
-   *  this and interact with the returned IndexWriter. */
-  public abstract Index index() throws IOException;
-
-  public abstract void close() throws IOException;
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/sep/IntStreamFactory.java b/lucene/core/src/java/org/apache/lucene/codecs/sep/IntStreamFactory.java
deleted file mode 100644
index eace033..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/sep/IntStreamFactory.java
+++ /dev/null
@@ -1,36 +0,0 @@
-package org.apache.lucene.codecs.sep;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-
-import java.io.IOException;
-
-/** Provides int reader and writer to specified files.
- *
- * @lucene.experimental */
-public abstract class IntStreamFactory {
-  /** Create an {@link IntIndexInput} on the provided
-   *  fileName. */
-  public abstract IntIndexInput openInput(Directory dir, String fileName, IOContext context) throws IOException;
-
-  /** Create an {@link IntIndexOutput} on the provided
-   *  fileName. */
-  public abstract IntIndexOutput createOutput(Directory dir, String fileName, IOContext context) throws IOException;
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/sep/SepDocValuesConsumer.java b/lucene/core/src/java/org/apache/lucene/codecs/sep/SepDocValuesConsumer.java
deleted file mode 100644
index d5dbb4e..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/sep/SepDocValuesConsumer.java
+++ /dev/null
@@ -1,47 +0,0 @@
-package org.apache.lucene.codecs.sep;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.codecs.lucene40.values.DocValuesWriterBase;
-import org.apache.lucene.index.PerDocWriteState;
-import org.apache.lucene.store.Directory;
-
-/**
- * Implementation of PerDocConsumer that uses separate files.
- * @lucene.experimental
- */
-
-public class SepDocValuesConsumer extends DocValuesWriterBase {
-  private final Directory directory;
-
-  public SepDocValuesConsumer(PerDocWriteState state) {
-    super(state);
-    this.directory = state.directory;
-  }
-  
-  @Override
-  protected Directory getDirectory() {
-    return directory;
-  }
-
-  @Override
-  public void abort() {
-    // We don't have to remove files here: IndexFileDeleter
-    // will do so
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/sep/SepDocValuesProducer.java b/lucene/core/src/java/org/apache/lucene/codecs/sep/SepDocValuesProducer.java
deleted file mode 100644
index 4965c1d..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/sep/SepDocValuesProducer.java
+++ /dev/null
@@ -1,91 +0,0 @@
-package org.apache.lucene.codecs.sep;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-import java.io.Closeable;
-import java.io.IOException;
-import java.util.Collection;
-import java.util.Map;
-import java.util.TreeMap;
-
-import org.apache.lucene.codecs.PerDocProducerBase;
-import org.apache.lucene.codecs.lucene40.values.Bytes;
-import org.apache.lucene.codecs.lucene40.values.Floats;
-import org.apache.lucene.codecs.lucene40.values.Ints;
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.DocValues.Type;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.util.IOUtils;
-
-/**
- * Implementation of PerDocProducer that uses separate files.
- * @lucene.experimental
- */
-public class SepDocValuesProducer extends PerDocProducerBase {
-  private final TreeMap<String, DocValues> docValues;
-
-  /**
-   * Creates a new {@link SepDocValuesProducer} instance and loads all
-   * {@link DocValues} instances for this segment and codec.
-   */
-  public SepDocValuesProducer(SegmentReadState state) throws IOException {
-    docValues = load(state.fieldInfos, state.segmentInfo.name, state.segmentInfo.getDocCount(), state.dir, state.context);
-  }
-  
-  @Override
-  protected Map<String,DocValues> docValues() {
-    return docValues;
-  }
-  
-  @Override
-  protected void closeInternal(Collection<? extends Closeable> closeables) throws IOException {
-    IOUtils.close(closeables);
-  }
-
-  @Override
-  protected DocValues loadDocValues(int docCount, Directory dir, String id,
-      Type type, IOContext context) throws IOException {
-      switch (type) {
-      case FIXED_INTS_16:
-      case FIXED_INTS_32:
-      case FIXED_INTS_64:
-      case FIXED_INTS_8:
-      case VAR_INTS:
-        return Ints.getValues(dir, id, docCount, type, context);
-      case FLOAT_32:
-        return Floats.getValues(dir, id, docCount, context, type);
-      case FLOAT_64:
-        return Floats.getValues(dir, id, docCount, context, type);
-      case BYTES_FIXED_STRAIGHT:
-        return Bytes.getValues(dir, id, Bytes.Mode.STRAIGHT, true, docCount, getComparator(), context);
-      case BYTES_FIXED_DEREF:
-        return Bytes.getValues(dir, id, Bytes.Mode.DEREF, true, docCount, getComparator(), context);
-      case BYTES_FIXED_SORTED:
-        return Bytes.getValues(dir, id, Bytes.Mode.SORTED, true, docCount, getComparator(), context);
-      case BYTES_VAR_STRAIGHT:
-        return Bytes.getValues(dir, id, Bytes.Mode.STRAIGHT, false, docCount, getComparator(), context);
-      case BYTES_VAR_DEREF:
-        return Bytes.getValues(dir, id, Bytes.Mode.DEREF, false, docCount, getComparator(), context);
-      case BYTES_VAR_SORTED:
-        return Bytes.getValues(dir, id, Bytes.Mode.SORTED, false, docCount, getComparator(), context);
-      default:
-        throw new IllegalStateException("unrecognized index values mode " + type);
-      }
-    }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/sep/SepPostingsReader.java b/lucene/core/src/java/org/apache/lucene/codecs/sep/SepPostingsReader.java
deleted file mode 100644
index c76de76..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/sep/SepPostingsReader.java
+++ /dev/null
@@ -1,743 +0,0 @@
-package org.apache.lucene.codecs.sep;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.BlockTermState;
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.PostingsReaderBase;
-import org.apache.lucene.index.DocsAndPositionsEnum;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.TermState;
-import org.apache.lucene.store.ByteArrayDataInput;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-
-/** Concrete class that reads the current doc/freq/skip
- *  postings format.    
- *
- * @lucene.experimental
- */
-
-// TODO: -- should we switch "hasProx" higher up?  and
-// create two separate docs readers, one that also reads
-// prox and one that doesn't?
-
-public class SepPostingsReader extends PostingsReaderBase {
-
-  final IntIndexInput freqIn;
-  final IntIndexInput docIn;
-  final IntIndexInput posIn;
-  final IndexInput payloadIn;
-  final IndexInput skipIn;
-
-  int skipInterval;
-  int maxSkipLevels;
-  int skipMinimum;
-
-  public SepPostingsReader(Directory dir, FieldInfos fieldInfos, SegmentInfo segmentInfo, IOContext context, IntStreamFactory intFactory, String segmentSuffix) throws IOException {
-    boolean success = false;
-    try {
-
-      final String docFileName = IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, SepPostingsWriter.DOC_EXTENSION);
-      docIn = intFactory.openInput(dir, docFileName, context);
-
-      skipIn = dir.openInput(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, SepPostingsWriter.SKIP_EXTENSION), context);
-
-      if (fieldInfos.hasFreq()) {
-        freqIn = intFactory.openInput(dir, IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, SepPostingsWriter.FREQ_EXTENSION), context);        
-      } else {
-        freqIn = null;
-      }
-      if (fieldInfos.hasProx()) {
-        posIn = intFactory.openInput(dir, IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, SepPostingsWriter.POS_EXTENSION), context);
-        payloadIn = dir.openInput(IndexFileNames.segmentFileName(segmentInfo.name, segmentSuffix, SepPostingsWriter.PAYLOAD_EXTENSION), context);
-      } else {
-        posIn = null;
-        payloadIn = null;
-      }
-      success = true;
-    } finally {
-      if (!success) {
-        close();
-      }
-    }
-  }
-
-  @Override
-  public void init(IndexInput termsIn) throws IOException {
-    // Make sure we are talking to the matching past writer
-    CodecUtil.checkHeader(termsIn, SepPostingsWriter.CODEC,
-      SepPostingsWriter.VERSION_START, SepPostingsWriter.VERSION_START);
-    skipInterval = termsIn.readInt();
-    maxSkipLevels = termsIn.readInt();
-    skipMinimum = termsIn.readInt();
-  }
-
-  @Override
-  public void close() throws IOException {
-    try {
-      if (freqIn != null)
-        freqIn.close();
-    } finally {
-      try {
-        if (docIn != null)
-          docIn.close();
-      } finally {
-        try {
-          if (skipIn != null)
-            skipIn.close();
-        } finally {
-          try {
-            if (posIn != null) {
-              posIn.close();
-            }
-          } finally {
-            if (payloadIn != null) {
-              payloadIn.close();
-            }
-          }
-        }
-      }
-    }
-  }
-
-  private static final class SepTermState extends BlockTermState {
-    // We store only the seek point to the docs file because
-    // the rest of the info (freqIndex, posIndex, etc.) is
-    // stored in the docs file:
-    IntIndexInput.Index docIndex;
-    IntIndexInput.Index posIndex;
-    IntIndexInput.Index freqIndex;
-    long payloadFP;
-    long skipFP;
-
-    // Only used for "primary" term state; these are never
-    // copied on clone:
-    
-    // TODO: these should somehow be stored per-TermsEnum
-    // not per TermState; maybe somehow the terms dict
-    // should load/manage the byte[]/DataReader for us?
-    byte[] bytes;
-    ByteArrayDataInput bytesReader;
-
-    @Override
-    public SepTermState clone() {
-      SepTermState other = new SepTermState();
-      other.copyFrom(this);
-      return other;
-    }
-
-    @Override
-    public void copyFrom(TermState _other) {
-      super.copyFrom(_other);
-      SepTermState other = (SepTermState) _other;
-      if (docIndex == null) {
-        docIndex = other.docIndex.clone();
-      } else {
-        docIndex.copyFrom(other.docIndex);
-      }
-      if (other.freqIndex != null) {
-        if (freqIndex == null) {
-          freqIndex = other.freqIndex.clone();
-        } else {
-          freqIndex.copyFrom(other.freqIndex);
-        }
-      } else {
-        freqIndex = null;
-      }
-      if (other.posIndex != null) {
-        if (posIndex == null) {
-          posIndex = other.posIndex.clone();
-        } else {
-          posIndex.copyFrom(other.posIndex);
-        }
-      } else {
-        posIndex = null;
-      }
-      payloadFP = other.payloadFP;
-      skipFP = other.skipFP;
-    }
-
-    @Override
-    public String toString() {
-      return super.toString() + " docIndex=" + docIndex + " freqIndex=" + freqIndex + " posIndex=" + posIndex + " payloadFP=" + payloadFP + " skipFP=" + skipFP;
-    }
-  }
-
-  @Override
-  public BlockTermState newTermState() throws IOException {
-    final SepTermState state = new SepTermState();
-    state.docIndex = docIn.index();
-    if (freqIn != null) {
-      state.freqIndex = freqIn.index();
-    }
-    if (posIn != null) {
-      state.posIndex = posIn.index();
-    }
-    return state;
-  }
-
-  @Override
-  public void readTermsBlock(IndexInput termsIn, FieldInfo fieldInfo, BlockTermState _termState) throws IOException {
-    final SepTermState termState = (SepTermState) _termState;
-    //System.out.println("SEPR: readTermsBlock termsIn.fp=" + termsIn.getFilePointer());
-    final int len = termsIn.readVInt();
-    //System.out.println("  numBytes=" + len);
-    if (termState.bytes == null) {
-      termState.bytes = new byte[ArrayUtil.oversize(len, 1)];
-      termState.bytesReader = new ByteArrayDataInput(termState.bytes);
-    } else if (termState.bytes.length < len) {
-      termState.bytes = new byte[ArrayUtil.oversize(len, 1)];
-    }
-    termState.bytesReader.reset(termState.bytes, 0, len);
-    termsIn.readBytes(termState.bytes, 0, len);
-  }
-
-  @Override
-  public void nextTerm(FieldInfo fieldInfo, BlockTermState _termState) throws IOException {
-    final SepTermState termState = (SepTermState) _termState;
-    final boolean isFirstTerm = termState.termBlockOrd == 0;
-    //System.out.println("SEPR.nextTerm termCount=" + termState.termBlockOrd + " isFirstTerm=" + isFirstTerm + " bytesReader.pos=" + termState.bytesReader.getPosition());
-    //System.out.println("  docFreq=" + termState.docFreq);
-    termState.docIndex.read(termState.bytesReader, isFirstTerm);
-    //System.out.println("  docIndex=" + termState.docIndex);
-    if (fieldInfo.getIndexOptions() != IndexOptions.DOCS_ONLY) {
-      termState.freqIndex.read(termState.bytesReader, isFirstTerm);
-      if (fieldInfo.getIndexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
-        //System.out.println("  freqIndex=" + termState.freqIndex);
-        termState.posIndex.read(termState.bytesReader, isFirstTerm);
-        //System.out.println("  posIndex=" + termState.posIndex);
-        if (fieldInfo.hasPayloads()) {
-          if (isFirstTerm) {
-            termState.payloadFP = termState.bytesReader.readVLong();
-          } else {
-            termState.payloadFP += termState.bytesReader.readVLong();
-          }
-          //System.out.println("  payloadFP=" + termState.payloadFP);
-        }
-      }
-    }
-
-    if (termState.docFreq >= skipMinimum) {
-      //System.out.println("   readSkip @ " + termState.bytesReader.getPosition());
-      if (isFirstTerm) {
-        termState.skipFP = termState.bytesReader.readVLong();
-      } else {
-        termState.skipFP += termState.bytesReader.readVLong();
-      }
-      //System.out.println("  skipFP=" + termState.skipFP);
-    } else if (isFirstTerm) {
-      termState.skipFP = 0;
-    }
-  }
-
-  @Override
-  public DocsEnum docs(FieldInfo fieldInfo, BlockTermState _termState, Bits liveDocs, DocsEnum reuse, int flags) throws IOException {
-    final SepTermState termState = (SepTermState) _termState;
-    SepDocsEnum docsEnum;
-    if (reuse == null || !(reuse instanceof SepDocsEnum)) {
-      docsEnum = new SepDocsEnum();
-    } else {
-      docsEnum = (SepDocsEnum) reuse;
-      if (docsEnum.startDocIn != docIn) {
-        // If you are using ParellelReader, and pass in a
-        // reused DocsAndPositionsEnum, it could have come
-        // from another reader also using sep codec
-        docsEnum = new SepDocsEnum();        
-      }
-    }
-
-    return docsEnum.init(fieldInfo, termState, liveDocs);
-  }
-
-  @Override
-  public DocsAndPositionsEnum docsAndPositions(FieldInfo fieldInfo, BlockTermState _termState, Bits liveDocs,
-                                               DocsAndPositionsEnum reuse, int flags)
-    throws IOException {
-
-    assert fieldInfo.getIndexOptions() == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;
-    final SepTermState termState = (SepTermState) _termState;
-    SepDocsAndPositionsEnum postingsEnum;
-    if (reuse == null || !(reuse instanceof SepDocsAndPositionsEnum)) {
-      postingsEnum = new SepDocsAndPositionsEnum();
-    } else {
-      postingsEnum = (SepDocsAndPositionsEnum) reuse;
-      if (postingsEnum.startDocIn != docIn) {
-        // If you are using ParellelReader, and pass in a
-        // reused DocsAndPositionsEnum, it could have come
-        // from another reader also using sep codec
-        postingsEnum = new SepDocsAndPositionsEnum();        
-      }
-    }
-
-    return postingsEnum.init(fieldInfo, termState, liveDocs);
-  }
-
-  class SepDocsEnum extends DocsEnum {
-    int docFreq;
-    int doc = -1;
-    int accum;
-    int count;
-    int freq;
-    long freqStart;
-
-    // TODO: -- should we do omitTF with 2 different enum classes?
-    private boolean omitTF;
-    private IndexOptions indexOptions;
-    private boolean storePayloads;
-    private Bits liveDocs;
-    private final IntIndexInput.Reader docReader;
-    private final IntIndexInput.Reader freqReader;
-    private long skipFP;
-
-    private final IntIndexInput.Index docIndex;
-    private final IntIndexInput.Index freqIndex;
-    private final IntIndexInput.Index posIndex;
-    private final IntIndexInput startDocIn;
-
-    // TODO: -- should we do hasProx with 2 different enum classes?
-
-    boolean skipped;
-    SepSkipListReader skipper;
-
-    SepDocsEnum() throws IOException {
-      startDocIn = docIn;
-      docReader = docIn.reader();
-      docIndex = docIn.index();
-      if (freqIn != null) {
-        freqReader = freqIn.reader();
-        freqIndex = freqIn.index();
-      } else {
-        freqReader = null;
-        freqIndex = null;
-      }
-      if (posIn != null) {
-        posIndex = posIn.index();                 // only init this so skipper can read it
-      } else {
-        posIndex = null;
-      }
-    }
-
-    SepDocsEnum init(FieldInfo fieldInfo, SepTermState termState, Bits liveDocs) throws IOException {
-      this.liveDocs = liveDocs;
-      this.indexOptions = fieldInfo.getIndexOptions();
-      omitTF = indexOptions == IndexOptions.DOCS_ONLY;
-      storePayloads = fieldInfo.hasPayloads();
-
-      // TODO: can't we only do this if consumer
-      // skipped consuming the previous docs?
-      docIndex.copyFrom(termState.docIndex);
-      docIndex.seek(docReader);
-
-      if (!omitTF) {
-        freqIndex.copyFrom(termState.freqIndex);
-        freqIndex.seek(freqReader);
-      }
-
-      docFreq = termState.docFreq;
-      // NOTE: unused if docFreq < skipMinimum:
-      skipFP = termState.skipFP;
-      count = 0;
-      doc = -1;
-      accum = 0;
-      freq = 1;
-      skipped = false;
-
-      return this;
-    }
-
-    @Override
-    public int nextDoc() throws IOException {
-
-      while(true) {
-        if (count == docFreq) {
-          return doc = NO_MORE_DOCS;
-        }
-
-        count++;
-
-        // Decode next doc
-        //System.out.println("decode docDelta:");
-        accum += docReader.next();
-          
-        if (!omitTF) {
-          //System.out.println("decode freq:");
-          freq = freqReader.next();
-        }
-
-        if (liveDocs == null || liveDocs.get(accum)) {
-          break;
-        }
-      }
-      return (doc = accum);
-    }
-
-    @Override
-    public int freq() throws IOException {
-      return freq;
-    }
-
-    @Override
-    public int docID() {
-      return doc;
-    }
-
-    @Override
-    public int advance(int target) throws IOException {
-
-      if ((target - skipInterval) >= doc && docFreq >= skipMinimum) {
-
-        // There are enough docs in the posting to have
-        // skip data, and its not too close
-
-        if (skipper == null) {
-          // This DocsEnum has never done any skipping
-          skipper = new SepSkipListReader(skipIn.clone(),
-                                          freqIn,
-                                          docIn,
-                                          posIn,
-                                          maxSkipLevels, skipInterval);
-
-        }
-
-        if (!skipped) {
-          // We haven't yet skipped for this posting
-          skipper.init(skipFP,
-                       docIndex,
-                       freqIndex,
-                       posIndex,
-                       0,
-                       docFreq,
-                       storePayloads);
-          skipper.setIndexOptions(indexOptions);
-
-          skipped = true;
-        }
-
-        final int newCount = skipper.skipTo(target); 
-
-        if (newCount > count) {
-
-          // Skipper did move
-          if (!omitTF) {
-            skipper.getFreqIndex().seek(freqReader);
-          }
-          skipper.getDocIndex().seek(docReader);
-          count = newCount;
-          doc = accum = skipper.getDoc();
-        }
-      }
-        
-      // Now, linear scan for the rest:
-      do {
-        if (nextDoc() == NO_MORE_DOCS) {
-          return NO_MORE_DOCS;
-        }
-      } while (target > doc);
-
-      return doc;
-    }
-  }
-
-  class SepDocsAndPositionsEnum extends DocsAndPositionsEnum {
-    int docFreq;
-    int doc = -1;
-    int accum;
-    int count;
-    int freq;
-    long freqStart;
-
-    private boolean storePayloads;
-    private Bits liveDocs;
-    private final IntIndexInput.Reader docReader;
-    private final IntIndexInput.Reader freqReader;
-    private final IntIndexInput.Reader posReader;
-    private final IndexInput payloadIn;
-    private long skipFP;
-
-    private final IntIndexInput.Index docIndex;
-    private final IntIndexInput.Index freqIndex;
-    private final IntIndexInput.Index posIndex;
-    private final IntIndexInput startDocIn;
-
-    private long payloadFP;
-
-    private int pendingPosCount;
-    private int position;
-    private int payloadLength;
-    private long pendingPayloadBytes;
-
-    private boolean skipped;
-    private SepSkipListReader skipper;
-    private boolean payloadPending;
-    private boolean posSeekPending;
-
-    SepDocsAndPositionsEnum() throws IOException {
-      startDocIn = docIn;
-      docReader = docIn.reader();
-      docIndex = docIn.index();
-      freqReader = freqIn.reader();
-      freqIndex = freqIn.index();
-      posReader = posIn.reader();
-      posIndex = posIn.index();
-      payloadIn = SepPostingsReader.this.payloadIn.clone();
-    }
-
-    SepDocsAndPositionsEnum init(FieldInfo fieldInfo, SepTermState termState, Bits liveDocs) throws IOException {
-      this.liveDocs = liveDocs;
-      storePayloads = fieldInfo.hasPayloads();
-      //System.out.println("Sep D&P init");
-
-      // TODO: can't we only do this if consumer
-      // skipped consuming the previous docs?
-      docIndex.copyFrom(termState.docIndex);
-      docIndex.seek(docReader);
-      //System.out.println("  docIndex=" + docIndex);
-
-      freqIndex.copyFrom(termState.freqIndex);
-      freqIndex.seek(freqReader);
-      //System.out.println("  freqIndex=" + freqIndex);
-
-      posIndex.copyFrom(termState.posIndex);
-      //System.out.println("  posIndex=" + posIndex);
-      posSeekPending = true;
-      payloadPending = false;
-
-      payloadFP = termState.payloadFP;
-      skipFP = termState.skipFP;
-      //System.out.println("  skipFP=" + skipFP);
-
-      docFreq = termState.docFreq;
-      count = 0;
-      doc = -1;
-      accum = 0;
-      pendingPosCount = 0;
-      pendingPayloadBytes = 0;
-      skipped = false;
-
-      return this;
-    }
-
-    @Override
-    public int nextDoc() throws IOException {
-
-      while(true) {
-        if (count == docFreq) {
-          return doc = NO_MORE_DOCS;
-        }
-
-        count++;
-
-        // TODO: maybe we should do the 1-bit trick for encoding
-        // freq=1 case?
-
-        // Decode next doc
-        //System.out.println("  sep d&p read doc");
-        accum += docReader.next();
-
-        //System.out.println("  sep d&p read freq");
-        freq = freqReader.next();
-
-        pendingPosCount += freq;
-
-        if (liveDocs == null || liveDocs.get(accum)) {
-          break;
-        }
-      }
-
-      position = 0;
-      return (doc = accum);
-    }
-
-    @Override
-    public int freq() throws IOException {
-      return freq;
-    }
-
-    @Override
-    public int docID() {
-      return doc;
-    }
-
-    @Override
-    public int advance(int target) throws IOException {
-      //System.out.println("SepD&P advance target=" + target + " vs current=" + doc + " this=" + this);
-
-      if ((target - skipInterval) >= doc && docFreq >= skipMinimum) {
-
-        // There are enough docs in the posting to have
-        // skip data, and its not too close
-
-        if (skipper == null) {
-          //System.out.println("  create skipper");
-          // This DocsEnum has never done any skipping
-          skipper = new SepSkipListReader(skipIn.clone(),
-                                          freqIn,
-                                          docIn,
-                                          posIn,
-                                          maxSkipLevels, skipInterval);
-        }
-
-        if (!skipped) {
-          //System.out.println("  init skip data skipFP=" + skipFP);
-          // We haven't yet skipped for this posting
-          skipper.init(skipFP,
-                       docIndex,
-                       freqIndex,
-                       posIndex,
-                       payloadFP,
-                       docFreq,
-                       storePayloads);
-          skipper.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);
-          skipped = true;
-        }
-        final int newCount = skipper.skipTo(target); 
-        //System.out.println("  skip newCount=" + newCount + " vs " + count);
-
-        if (newCount > count) {
-
-          // Skipper did move
-          skipper.getFreqIndex().seek(freqReader);
-          skipper.getDocIndex().seek(docReader);
-          //System.out.println("  doc seek'd to " + skipper.getDocIndex());
-          // NOTE: don't seek pos here; do it lazily
-          // instead.  Eg a PhraseQuery may skip to many
-          // docs before finally asking for positions...
-          posIndex.copyFrom(skipper.getPosIndex());
-          posSeekPending = true;
-          count = newCount;
-          doc = accum = skipper.getDoc();
-          //System.out.println("    moved to doc=" + doc);
-          //payloadIn.seek(skipper.getPayloadPointer());
-          payloadFP = skipper.getPayloadPointer();
-          pendingPosCount = 0;
-          pendingPayloadBytes = 0;
-          payloadPending = false;
-          payloadLength = skipper.getPayloadLength();
-          //System.out.println("    move payloadLen=" + payloadLength);
-        }
-      }
-        
-      // Now, linear scan for the rest:
-      do {
-        if (nextDoc() == NO_MORE_DOCS) {
-          //System.out.println("  advance nextDoc=END");
-          return NO_MORE_DOCS;
-        }
-        //System.out.println("  advance nextDoc=" + doc);
-      } while (target > doc);
-
-      //System.out.println("  return doc=" + doc);
-      return doc;
-    }
-
-    @Override
-    public int nextPosition() throws IOException {
-      if (posSeekPending) {
-        posIndex.seek(posReader);
-        payloadIn.seek(payloadFP);
-        posSeekPending = false;
-      }
-
-      // scan over any docs that were iterated without their
-      // positions
-      while (pendingPosCount > freq) {
-        final int code = posReader.next();
-        if (storePayloads && (code & 1) != 0) {
-          // Payload length has changed
-          payloadLength = posReader.next();
-          assert payloadLength >= 0;
-        }
-        pendingPosCount--;
-        position = 0;
-        pendingPayloadBytes += payloadLength;
-      }
-
-      final int code = posReader.next();
-
-      if (storePayloads) {
-        if ((code & 1) != 0) {
-          // Payload length has changed
-          payloadLength = posReader.next();
-          assert payloadLength >= 0;
-        }
-        position += code >>> 1;
-        pendingPayloadBytes += payloadLength;
-        payloadPending = payloadLength > 0;
-      } else {
-        position += code;
-      }
-    
-      pendingPosCount--;
-      assert pendingPosCount >= 0;
-      return position;
-    }
-
-    @Override
-    public int startOffset() {
-      return -1;
-    }
-
-    @Override
-    public int endOffset() {
-      return -1;
-    }
-
-    private BytesRef payload;
-
-    @Override
-    public BytesRef getPayload() throws IOException {
-      if (!payloadPending) {
-        return null;
-      }
-      
-      if (pendingPayloadBytes == 0) {
-        return payload;
-      }
-
-      assert pendingPayloadBytes >= payloadLength;
-
-      if (pendingPayloadBytes > payloadLength) {
-        payloadIn.seek(payloadIn.getFilePointer() + (pendingPayloadBytes - payloadLength));
-      }
-
-      if (payload == null) {
-        payload = new BytesRef();
-        payload.bytes = new byte[payloadLength];
-      } else if (payload.bytes.length < payloadLength) {
-        payload.grow(payloadLength);
-      }
-
-      payloadIn.readBytes(payload.bytes, 0, payloadLength);
-      payload.length = payloadLength;
-      pendingPayloadBytes = 0;
-      return payload;
-    }
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/sep/SepPostingsWriter.java b/lucene/core/src/java/org/apache/lucene/codecs/sep/SepPostingsWriter.java
deleted file mode 100644
index ef96302..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/sep/SepPostingsWriter.java
+++ /dev/null
@@ -1,395 +0,0 @@
-package org.apache.lucene.codecs.sep;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.lucene.codecs.CodecUtil;
-import org.apache.lucene.codecs.PostingsWriterBase;
-import org.apache.lucene.codecs.TermStats;
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.store.RAMOutputStream;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-
-/** Writes frq to .frq, docs to .doc, pos to .pos, payloads
- *  to .pyl, skip data to .skp
- *
- * @lucene.experimental */
-public final class SepPostingsWriter extends PostingsWriterBase {
-  final static String CODEC = "SepPostingsWriter";
-
-  final static String DOC_EXTENSION = "doc";
-  final static String SKIP_EXTENSION = "skp";
-  final static String FREQ_EXTENSION = "frq";
-  final static String POS_EXTENSION = "pos";
-  final static String PAYLOAD_EXTENSION = "pyl";
-
-  // Increment version to change it:
-  final static int VERSION_START = 0;
-  final static int VERSION_CURRENT = VERSION_START;
-
-  IntIndexOutput freqOut;
-  IntIndexOutput.Index freqIndex;
-
-  IntIndexOutput posOut;
-  IntIndexOutput.Index posIndex;
-
-  IntIndexOutput docOut;
-  IntIndexOutput.Index docIndex;
-
-  IndexOutput payloadOut;
-
-  IndexOutput skipOut;
-  IndexOutput termsOut;
-
-  final SepSkipListWriter skipListWriter;
-  /** Expert: The fraction of TermDocs entries stored in skip tables,
-   * used to accelerate {@link DocsEnum#advance(int)}.  Larger values result in
-   * smaller indexes, greater acceleration, but fewer accelerable cases, while
-   * smaller values result in bigger indexes, less acceleration and more
-   * accelerable cases. More detailed experiments would be useful here. */
-  final int skipInterval;
-  static final int DEFAULT_SKIP_INTERVAL = 16;
-  
-  /**
-   * Expert: minimum docFreq to write any skip data at all
-   */
-  final int skipMinimum;
-
-  /** Expert: The maximum number of skip levels. Smaller values result in 
-   * slightly smaller indexes, but slower skipping in big posting lists.
-   */
-  final int maxSkipLevels = 10;
-
-  final int totalNumDocs;
-
-  boolean storePayloads;
-  IndexOptions indexOptions;
-
-  FieldInfo fieldInfo;
-
-  int lastPayloadLength;
-  int lastPosition;
-  long payloadStart;
-  int lastDocID;
-  int df;
-
-  // Holds pending byte[] blob for the current terms block
-  private final RAMOutputStream indexBytesWriter = new RAMOutputStream();
-
-  public SepPostingsWriter(SegmentWriteState state, IntStreamFactory factory) throws IOException {
-    this(state, factory, DEFAULT_SKIP_INTERVAL);
-  }
-
-  public SepPostingsWriter(SegmentWriteState state, IntStreamFactory factory, int skipInterval) throws IOException {
-    freqOut = null;
-    freqIndex = null;
-    posOut = null;
-    posIndex = null;
-    payloadOut = null;
-    boolean success = false;
-    try {
-      this.skipInterval = skipInterval;
-      this.skipMinimum = skipInterval; /* set to the same for now */
-      final String docFileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, DOC_EXTENSION);
-      docOut = factory.createOutput(state.directory, docFileName, state.context);
-      docIndex = docOut.index();
-      
-      if (state.fieldInfos.hasFreq()) {
-        final String frqFileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, FREQ_EXTENSION);
-        freqOut = factory.createOutput(state.directory, frqFileName, state.context);
-        freqIndex = freqOut.index();
-      }
-
-      if (state.fieldInfos.hasProx()) {      
-        final String posFileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, POS_EXTENSION);
-        posOut = factory.createOutput(state.directory, posFileName, state.context);
-        posIndex = posOut.index();
-        
-        // TODO: -- only if at least one field stores payloads?
-        final String payloadFileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, PAYLOAD_EXTENSION);
-        payloadOut = state.directory.createOutput(payloadFileName, state.context);
-      }
-      
-      final String skipFileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, SKIP_EXTENSION);
-      skipOut = state.directory.createOutput(skipFileName, state.context);
-      
-      totalNumDocs = state.segmentInfo.getDocCount();
-      
-      skipListWriter = new SepSkipListWriter(skipInterval,
-          maxSkipLevels,
-          totalNumDocs,
-          freqOut, docOut,
-          posOut, payloadOut);
-      
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(docOut, skipOut, freqOut, posOut, payloadOut);
-      }
-    }
-  }
-
-  @Override
-  public void start(IndexOutput termsOut) throws IOException {
-    this.termsOut = termsOut;
-    CodecUtil.writeHeader(termsOut, CODEC, VERSION_CURRENT);
-    // TODO: -- just ask skipper to "start" here
-    termsOut.writeInt(skipInterval);                // write skipInterval
-    termsOut.writeInt(maxSkipLevels);               // write maxSkipLevels
-    termsOut.writeInt(skipMinimum);                 // write skipMinimum
-  }
-
-  @Override
-  public void startTerm() throws IOException {
-    docIndex.mark();
-    //System.out.println("SEPW: startTerm docIndex=" + docIndex);
-
-    if (indexOptions != IndexOptions.DOCS_ONLY) {
-      freqIndex.mark();
-    }
-    
-    if (indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
-      posIndex.mark();
-      payloadStart = payloadOut.getFilePointer();
-      lastPayloadLength = -1;
-    }
-
-    skipListWriter.resetSkip(docIndex, freqIndex, posIndex);
-  }
-
-  // Currently, this instance is re-used across fields, so
-  // our parent calls setField whenever the field changes
-  @Override
-  public void setField(FieldInfo fieldInfo) {
-    this.fieldInfo = fieldInfo;
-    this.indexOptions = fieldInfo.getIndexOptions();
-    if (indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0) {
-      throw new UnsupportedOperationException("this codec cannot index offsets");
-    }
-    skipListWriter.setIndexOptions(indexOptions);
-    storePayloads = indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS && fieldInfo.hasPayloads();
-  }
-
-  /** Adds a new doc in this term.  If this returns null
-   *  then we just skip consuming positions/payloads. */
-  @Override
-  public void startDoc(int docID, int termDocFreq) throws IOException {
-
-    final int delta = docID - lastDocID;
-    //System.out.println("SEPW: startDoc: write doc=" + docID + " delta=" + delta + " out.fp=" + docOut);
-
-    if (docID < 0 || (df > 0 && delta <= 0)) {
-      throw new CorruptIndexException("docs out of order (" + docID + " <= " + lastDocID + " ) (docOut: " + docOut + ")");
-    }
-
-    if ((++df % skipInterval) == 0) {
-      // TODO: -- awkward we have to make these two
-      // separate calls to skipper
-      //System.out.println("    buffer skip lastDocID=" + lastDocID);
-      skipListWriter.setSkipData(lastDocID, storePayloads, lastPayloadLength);
-      skipListWriter.bufferSkip(df);
-    }
-
-    lastDocID = docID;
-    docOut.write(delta);
-    if (indexOptions != IndexOptions.DOCS_ONLY) {
-      //System.out.println("    sepw startDoc: write freq=" + termDocFreq);
-      freqOut.write(termDocFreq);
-    }
-  }
-
-  /** Add a new position & payload */
-  @Override
-  public void addPosition(int position, BytesRef payload, int startOffset, int endOffset) throws IOException {
-    assert indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS;
-
-    final int delta = position - lastPosition;
-    assert delta >= 0: "position=" + position + " lastPosition=" + lastPosition;            // not quite right (if pos=0 is repeated twice we don't catch it)
-    lastPosition = position;
-
-    if (storePayloads) {
-      final int payloadLength = payload == null ? 0 : payload.length;
-      if (payloadLength != lastPayloadLength) {
-        lastPayloadLength = payloadLength;
-        // TODO: explore whether we get better compression
-        // by not storing payloadLength into prox stream?
-        posOut.write((delta<<1)|1);
-        posOut.write(payloadLength);
-      } else {
-        posOut.write(delta << 1);
-      }
-
-      if (payloadLength > 0) {
-        payloadOut.writeBytes(payload.bytes, payload.offset, payloadLength);
-      }
-    } else {
-      posOut.write(delta);
-    }
-
-    lastPosition = position;
-  }
-
-  /** Called when we are done adding positions & payloads */
-  @Override
-  public void finishDoc() {       
-    lastPosition = 0;
-  }
-
-  private static class PendingTerm {
-    public final IntIndexOutput.Index docIndex;
-    public final IntIndexOutput.Index freqIndex;
-    public final IntIndexOutput.Index posIndex;
-    public final long payloadFP;
-    public final long skipFP;
-
-    public PendingTerm(IntIndexOutput.Index docIndex, IntIndexOutput.Index freqIndex, IntIndexOutput.Index posIndex, long payloadFP, long skipFP) {
-      this.docIndex = docIndex;
-      this.freqIndex = freqIndex;
-      this.posIndex = posIndex;
-      this.payloadFP = payloadFP;
-      this.skipFP = skipFP;
-    }
-  }
-
-  private final List<PendingTerm> pendingTerms = new ArrayList<PendingTerm>();
-
-  /** Called when we are done adding docs to this term */
-  @Override
-  public void finishTerm(TermStats stats) throws IOException {
-    // TODO: -- wasteful we are counting this in two places?
-    assert stats.docFreq > 0;
-    assert stats.docFreq == df;
-
-    final IntIndexOutput.Index docIndexCopy = docOut.index();
-    docIndexCopy.copyFrom(docIndex, false);
-
-    final IntIndexOutput.Index freqIndexCopy;
-    final IntIndexOutput.Index posIndexCopy;
-    if (indexOptions != IndexOptions.DOCS_ONLY) {
-      freqIndexCopy = freqOut.index();
-      freqIndexCopy.copyFrom(freqIndex, false);
-      if (indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
-        posIndexCopy = posOut.index();
-        posIndexCopy.copyFrom(posIndex, false);
-      } else {
-        posIndexCopy = null;
-      }
-    } else {
-      freqIndexCopy = null;
-      posIndexCopy = null;
-    }
-
-    final long skipFP;
-    if (df >= skipMinimum) {
-      skipFP = skipOut.getFilePointer();
-      //System.out.println("  skipFP=" + skipFP);
-      skipListWriter.writeSkip(skipOut);
-      //System.out.println("    numBytes=" + (skipOut.getFilePointer()-skipFP));
-    } else {
-      skipFP = -1;
-    }
-
-    lastDocID = 0;
-    df = 0;
-
-    pendingTerms.add(new PendingTerm(docIndexCopy,
-                                     freqIndexCopy,
-                                     posIndexCopy,
-                                     payloadStart,
-                                     skipFP));
-  }
-
-  @Override
-  public void flushTermsBlock(int start, int count) throws IOException {
-    //System.out.println("SEPW: flushTermsBlock: start=" + start + " count=" + count + " pendingTerms.size()=" + pendingTerms.size() + " termsOut.fp=" + termsOut.getFilePointer());
-    assert indexBytesWriter.getFilePointer() == 0;
-    final int absStart = pendingTerms.size() - start;
-    final List<PendingTerm> slice = pendingTerms.subList(absStart, absStart+count);
-
-    long lastPayloadFP = 0;
-    long lastSkipFP = 0;
-
-    if (count == 0) {
-      termsOut.writeByte((byte) 0);
-      return;
-    }
-
-    final PendingTerm firstTerm = slice.get(0);
-    final IntIndexOutput.Index docIndexFlush = firstTerm.docIndex;
-    final IntIndexOutput.Index freqIndexFlush = firstTerm.freqIndex;
-    final IntIndexOutput.Index posIndexFlush = firstTerm.posIndex;
-
-    for(int idx=0;idx<slice.size();idx++) {
-      final boolean isFirstTerm = idx == 0;
-      final PendingTerm t = slice.get(idx);
-      //System.out.println("  write idx=" + idx + " docIndex=" + t.docIndex);
-      docIndexFlush.copyFrom(t.docIndex, false);
-      docIndexFlush.write(indexBytesWriter, isFirstTerm);
-      if (indexOptions != IndexOptions.DOCS_ONLY) {
-        freqIndexFlush.copyFrom(t.freqIndex, false);
-        freqIndexFlush.write(indexBytesWriter, isFirstTerm);
-        //System.out.println("    freqIndex=" + t.freqIndex);
-        if (indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
-          posIndexFlush.copyFrom(t.posIndex, false);
-          posIndexFlush.write(indexBytesWriter, isFirstTerm);
-          //System.out.println("    posIndex=" + t.posIndex);
-          if (storePayloads) {
-            //System.out.println("    payloadFP=" + t.payloadFP);
-            if (isFirstTerm) {
-              indexBytesWriter.writeVLong(t.payloadFP);
-            } else {
-              indexBytesWriter.writeVLong(t.payloadFP - lastPayloadFP);
-            }
-            lastPayloadFP = t.payloadFP;
-          }
-        }
-      }
-
-      if (t.skipFP != -1) {
-        if (isFirstTerm) {
-          indexBytesWriter.writeVLong(t.skipFP);
-        } else {
-          indexBytesWriter.writeVLong(t.skipFP - lastSkipFP);
-        }
-        lastSkipFP = t.skipFP;
-        //System.out.println("    skipFP=" + t.skipFP);
-      }
-    }
-
-    //System.out.println("  numBytes=" + indexBytesWriter.getFilePointer());
-    termsOut.writeVLong((int) indexBytesWriter.getFilePointer());
-    indexBytesWriter.writeTo(termsOut);
-    indexBytesWriter.reset();
-    slice.clear();
-  }
-
-  @Override
-  public void close() throws IOException {
-    IOUtils.close(docOut, skipOut, freqOut, posOut, payloadOut);
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/sep/SepSkipListReader.java b/lucene/core/src/java/org/apache/lucene/codecs/sep/SepSkipListReader.java
deleted file mode 100644
index e1f8b28..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/sep/SepSkipListReader.java
+++ /dev/null
@@ -1,209 +0,0 @@
-package org.apache.lucene.codecs.sep;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Arrays;
-
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.codecs.MultiLevelSkipListReader;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-
-/**
- * Implements the skip list reader for the default posting list format
- * that stores positions and payloads.
- *
- * @lucene.experimental
- */
-
-// TODO: rewrite this as recursive classes?
-class SepSkipListReader extends MultiLevelSkipListReader {
-  private boolean currentFieldStoresPayloads;
-  private IntIndexInput.Index freqIndex[];
-  private IntIndexInput.Index docIndex[];
-  private IntIndexInput.Index posIndex[];
-  private long payloadPointer[];
-  private int payloadLength[];
-
-  private final IntIndexInput.Index lastFreqIndex;
-  private final IntIndexInput.Index lastDocIndex;
-  // TODO: -- make private again
-  final IntIndexInput.Index lastPosIndex;
-  
-  private long lastPayloadPointer;
-  private int lastPayloadLength;
-                           
-  SepSkipListReader(IndexInput skipStream,
-                    IntIndexInput freqIn,
-                    IntIndexInput docIn,
-                    IntIndexInput posIn,
-                    int maxSkipLevels,
-                    int skipInterval)
-    throws IOException {
-    super(skipStream, maxSkipLevels, skipInterval);
-    if (freqIn != null) {
-      freqIndex = new IntIndexInput.Index[maxSkipLevels];
-    }
-    docIndex = new IntIndexInput.Index[maxSkipLevels];
-    if (posIn != null) {
-      posIndex = new IntIndexInput.Index[maxNumberOfSkipLevels];
-    }
-    for(int i=0;i<maxSkipLevels;i++) {
-      if (freqIn != null) {
-        freqIndex[i] = freqIn.index();
-      }
-      docIndex[i] = docIn.index();
-      if (posIn != null) {
-        posIndex[i] = posIn.index();
-      }
-    }
-    payloadPointer = new long[maxSkipLevels];
-    payloadLength = new int[maxSkipLevels];
-
-    if (freqIn != null) {
-      lastFreqIndex = freqIn.index();
-    } else {
-      lastFreqIndex = null;
-    }
-    lastDocIndex = docIn.index();
-    if (posIn != null) {
-      lastPosIndex = posIn.index();
-    } else {
-      lastPosIndex = null;
-    }
-  }
-  
-  IndexOptions indexOptions;
-
-  void setIndexOptions(IndexOptions v) {
-    indexOptions = v;
-  }
-
-  void init(long skipPointer,
-            IntIndexInput.Index docBaseIndex,
-            IntIndexInput.Index freqBaseIndex,
-            IntIndexInput.Index posBaseIndex,
-            long payloadBasePointer,
-            int df,
-            boolean storesPayloads) {
-
-    super.init(skipPointer, df);
-    this.currentFieldStoresPayloads = storesPayloads;
-
-    lastPayloadPointer = payloadBasePointer;
-
-    for(int i=0;i<maxNumberOfSkipLevels;i++) {
-      docIndex[i].copyFrom(docBaseIndex);
-      if (freqIndex != null) {
-        freqIndex[i].copyFrom(freqBaseIndex);
-      }
-      if (posBaseIndex != null) {
-        posIndex[i].copyFrom(posBaseIndex);
-      }
-    }
-    Arrays.fill(payloadPointer, payloadBasePointer);
-    Arrays.fill(payloadLength, 0);
-  }
-
-  long getPayloadPointer() {
-    return lastPayloadPointer;
-  }
-  
-  /** Returns the payload length of the payload stored just before 
-   * the doc to which the last call of {@link MultiLevelSkipListReader#skipTo(int)} 
-   * has skipped.  */
-  int getPayloadLength() {
-    return lastPayloadLength;
-  }
-  
-  @Override
-  protected void seekChild(int level) throws IOException {
-    super.seekChild(level);
-    payloadPointer[level] = lastPayloadPointer;
-    payloadLength[level] = lastPayloadLength;
-  }
-  
-  @Override
-  protected void setLastSkipData(int level) {
-    super.setLastSkipData(level);
-
-    lastPayloadPointer = payloadPointer[level];
-    lastPayloadLength = payloadLength[level];
-    if (freqIndex != null) {
-      lastFreqIndex.copyFrom(freqIndex[level]);
-    }
-    lastDocIndex.copyFrom(docIndex[level]);
-    if (lastPosIndex != null) {
-      lastPosIndex.copyFrom(posIndex[level]);
-    }
-
-    if (level > 0) {
-      if (freqIndex != null) {
-        freqIndex[level-1].copyFrom(freqIndex[level]);
-      }
-      docIndex[level-1].copyFrom(docIndex[level]);
-      if (posIndex != null) {
-        posIndex[level-1].copyFrom(posIndex[level]);
-      }
-    }
-  }
-
-  IntIndexInput.Index getFreqIndex() {
-    return lastFreqIndex;
-  }
-
-  IntIndexInput.Index getPosIndex() {
-    return lastPosIndex;
-  }
-
-  IntIndexInput.Index getDocIndex() {
-    return lastDocIndex;
-  }
-
-  @Override
-  protected int readSkipData(int level, IndexInput skipStream) throws IOException {
-    int delta;
-    assert indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS || !currentFieldStoresPayloads;
-    if (currentFieldStoresPayloads) {
-      // the current field stores payloads.
-      // if the doc delta is odd then we have
-      // to read the current payload length
-      // because it differs from the length of the
-      // previous payload
-      delta = skipStream.readVInt();
-      if ((delta & 1) != 0) {
-        payloadLength[level] = skipStream.readVInt();
-      }
-      delta >>>= 1;
-    } else {
-      delta = skipStream.readVInt();
-    }
-    if (indexOptions != IndexOptions.DOCS_ONLY) {
-      freqIndex[level].read(skipStream, false);
-    }
-    docIndex[level].read(skipStream, false);
-    if (indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
-      posIndex[level].read(skipStream, false);
-      if (currentFieldStoresPayloads) {
-        payloadPointer[level] += skipStream.readVInt();
-      }
-    }
-    
-    return delta;
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/sep/SepSkipListWriter.java b/lucene/core/src/java/org/apache/lucene/codecs/sep/SepSkipListWriter.java
deleted file mode 100644
index fd284bd8..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/sep/SepSkipListWriter.java
+++ /dev/null
@@ -1,200 +0,0 @@
-package org.apache.lucene.codecs.sep;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Arrays;
-
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.codecs.MultiLevelSkipListWriter;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-
-// TODO: -- skip data should somehow be more local to the
-// particular stream (doc, freq, pos, payload)
-
-/**
- * Implements the skip list writer for the default posting list format
- * that stores positions and payloads.
- *
- * @lucene.experimental
- */
-class SepSkipListWriter extends MultiLevelSkipListWriter {
-  private int[] lastSkipDoc;
-  private int[] lastSkipPayloadLength;
-  private long[] lastSkipPayloadPointer;
-
-  private IntIndexOutput.Index[] docIndex;
-  private IntIndexOutput.Index[] freqIndex;
-  private IntIndexOutput.Index[] posIndex;
-  
-  private IntIndexOutput freqOutput;
-  // TODO: -- private again
-  IntIndexOutput posOutput;
-  // TODO: -- private again
-  IndexOutput payloadOutput;
-
-  private int curDoc;
-  private boolean curStorePayloads;
-  private int curPayloadLength;
-  private long curPayloadPointer;
-  
-  SepSkipListWriter(int skipInterval, int numberOfSkipLevels, int docCount,
-                    IntIndexOutput freqOutput,
-                    IntIndexOutput docOutput,
-                    IntIndexOutput posOutput,
-                    IndexOutput payloadOutput)
-    throws IOException {
-    super(skipInterval, numberOfSkipLevels, docCount);
-
-    this.freqOutput = freqOutput;
-    this.posOutput = posOutput;
-    this.payloadOutput = payloadOutput;
-    
-    lastSkipDoc = new int[numberOfSkipLevels];
-    lastSkipPayloadLength = new int[numberOfSkipLevels];
-    // TODO: -- also cutover normal IndexOutput to use getIndex()?
-    lastSkipPayloadPointer = new long[numberOfSkipLevels];
-
-    freqIndex = new IntIndexOutput.Index[numberOfSkipLevels];
-    docIndex = new IntIndexOutput.Index[numberOfSkipLevels];
-    posIndex = new IntIndexOutput.Index[numberOfSkipLevels];
-
-    for(int i=0;i<numberOfSkipLevels;i++) {
-      if (freqOutput != null) {
-        freqIndex[i] = freqOutput.index();
-      }
-      docIndex[i] = docOutput.index();
-      if (posOutput != null) {
-        posIndex[i] = posOutput.index();
-      }
-    }
-  }
-
-  IndexOptions indexOptions;
-
-  void setIndexOptions(IndexOptions v) {
-    indexOptions = v;
-  }
-
-  void setPosOutput(IntIndexOutput posOutput) throws IOException {
-    this.posOutput = posOutput;
-    for(int i=0;i<numberOfSkipLevels;i++) {
-      posIndex[i] = posOutput.index();
-    }
-  }
-
-  void setPayloadOutput(IndexOutput payloadOutput) {
-    this.payloadOutput = payloadOutput;
-  }
-
-  /**
-   * Sets the values for the current skip data. 
-   */
-  // Called @ every index interval (every 128th (by default)
-  // doc)
-  void setSkipData(int doc, boolean storePayloads, int payloadLength) {
-    this.curDoc = doc;
-    this.curStorePayloads = storePayloads;
-    this.curPayloadLength = payloadLength;
-    if (payloadOutput != null) {
-      this.curPayloadPointer = payloadOutput.getFilePointer();
-    }
-  }
-
-  // Called @ start of new term
-  protected void resetSkip(IntIndexOutput.Index topDocIndex, IntIndexOutput.Index topFreqIndex, IntIndexOutput.Index topPosIndex)
-    throws IOException {
-    super.resetSkip();
-
-    Arrays.fill(lastSkipDoc, 0);
-    Arrays.fill(lastSkipPayloadLength, -1);  // we don't have to write the first length in the skip list
-    for(int i=0;i<numberOfSkipLevels;i++) {
-      docIndex[i].copyFrom(topDocIndex, true);
-      if (freqOutput != null) {
-        freqIndex[i].copyFrom(topFreqIndex, true);
-      }
-      if (posOutput != null) {
-        posIndex[i].copyFrom(topPosIndex, true);
-      }
-    }
-    if (payloadOutput != null) {
-      Arrays.fill(lastSkipPayloadPointer, payloadOutput.getFilePointer());
-    }
-  }
-  
-  @Override
-  protected void writeSkipData(int level, IndexOutput skipBuffer) throws IOException {
-    // To efficiently store payloads in the posting lists we do not store the length of
-    // every payload. Instead we omit the length for a payload if the previous payload had
-    // the same length.
-    // However, in order to support skipping the payload length at every skip point must be known.
-    // So we use the same length encoding that we use for the posting lists for the skip data as well:
-    // Case 1: current field does not store payloads
-    //           SkipDatum                 --> DocSkip, FreqSkip, ProxSkip
-    //           DocSkip,FreqSkip,ProxSkip --> VInt
-    //           DocSkip records the document number before every SkipInterval th  document in TermFreqs. 
-    //           Document numbers are represented as differences from the previous value in the sequence.
-    // Case 2: current field stores payloads
-    //           SkipDatum                 --> DocSkip, PayloadLength?, FreqSkip,ProxSkip
-    //           DocSkip,FreqSkip,ProxSkip --> VInt
-    //           PayloadLength             --> VInt    
-    //         In this case DocSkip/2 is the difference between
-    //         the current and the previous value. If DocSkip
-    //         is odd, then a PayloadLength encoded as VInt follows,
-    //         if DocSkip is even, then it is assumed that the
-    //         current payload length equals the length at the previous
-    //         skip point
-
-    assert indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS || !curStorePayloads;
-
-    if (curStorePayloads) {
-      int delta = curDoc - lastSkipDoc[level];
-      if (curPayloadLength == lastSkipPayloadLength[level]) {
-        // the current payload length equals the length at the previous skip point,
-        // so we don't store the length again
-        skipBuffer.writeVInt(delta << 1);
-      } else {
-        // the payload length is different from the previous one. We shift the DocSkip, 
-        // set the lowest bit and store the current payload length as VInt.
-        skipBuffer.writeVInt(delta << 1 | 1);
-        skipBuffer.writeVInt(curPayloadLength);
-        lastSkipPayloadLength[level] = curPayloadLength;
-      }
-    } else {
-      // current field does not store payloads
-      skipBuffer.writeVInt(curDoc - lastSkipDoc[level]);
-    }
-
-    if (indexOptions != IndexOptions.DOCS_ONLY) {
-      freqIndex[level].mark();
-      freqIndex[level].write(skipBuffer, false);
-    }
-    docIndex[level].mark();
-    docIndex[level].write(skipBuffer, false);
-    if (indexOptions == IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) {
-      posIndex[level].mark();
-      posIndex[level].write(skipBuffer, false);
-      if (curStorePayloads) {
-        skipBuffer.writeVInt((int) (curPayloadPointer - lastSkipPayloadPointer[level]));
-      }
-    }
-
-    lastSkipDoc[level] = curDoc;
-    lastSkipPayloadPointer[level] = curPayloadPointer;
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/sep/package.html b/lucene/core/src/java/org/apache/lucene/codecs/sep/package.html
deleted file mode 100644
index b51d910..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/sep/package.html
+++ /dev/null
@@ -1,25 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
-</head>
-<body>
-Sep: base support for separate files (doc,frq,pos,skp,pyl)
-</body>
-</html>
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextCodec.java b/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextCodec.java
deleted file mode 100644
index a412b6e..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextCodec.java
+++ /dev/null
@@ -1,91 +0,0 @@
-package org.apache.lucene.codecs.simpletext;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.codecs.DocValuesFormat;
-import org.apache.lucene.codecs.FieldInfosFormat;
-import org.apache.lucene.codecs.LiveDocsFormat;
-import org.apache.lucene.codecs.NormsFormat;
-import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.codecs.SegmentInfoFormat;
-import org.apache.lucene.codecs.StoredFieldsFormat;
-import org.apache.lucene.codecs.TermVectorsFormat;
-
-/**
- * plain text index format.
- * <p>
- * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
- * @lucene.experimental
- */
-public final class SimpleTextCodec extends Codec {
-  private final PostingsFormat postings = new SimpleTextPostingsFormat();
-  private final StoredFieldsFormat storedFields = new SimpleTextStoredFieldsFormat();
-  private final SegmentInfoFormat segmentInfos = new SimpleTextSegmentInfoFormat();
-  private final FieldInfosFormat fieldInfosFormat = new SimpleTextFieldInfosFormat();
-  private final TermVectorsFormat vectorsFormat = new SimpleTextTermVectorsFormat();
-  // TODO: need a plain-text impl
-  private final DocValuesFormat docValues = new SimpleTextDocValuesFormat();
-  // TODO: need a plain-text impl (using the above)
-  private final NormsFormat normsFormat = new SimpleTextNormsFormat();
-  private final LiveDocsFormat liveDocs = new SimpleTextLiveDocsFormat();
-  
-  public SimpleTextCodec() {
-    super("SimpleText");
-  }
-  
-  @Override
-  public PostingsFormat postingsFormat() {
-    return postings;
-  }
-
-  @Override
-  public DocValuesFormat docValuesFormat() {
-    return docValues;
-  }
-
-  @Override
-  public StoredFieldsFormat storedFieldsFormat() {
-    return storedFields;
-  }
-  
-  @Override
-  public TermVectorsFormat termVectorsFormat() {
-    return vectorsFormat;
-  }
-  
-  @Override
-  public FieldInfosFormat fieldInfosFormat() {
-    return fieldInfosFormat;
-  }
-
-  @Override
-  public SegmentInfoFormat segmentInfoFormat() {
-    return segmentInfos;
-  }
-
-  @Override
-  public NormsFormat normsFormat() {
-    return normsFormat;
-  }
-  
-  @Override
-  public LiveDocsFormat liveDocsFormat() {
-    return liveDocs;
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesConsumer.java b/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesConsumer.java
deleted file mode 100644
index 96aac28..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesConsumer.java
+++ /dev/null
@@ -1,294 +0,0 @@
-package org.apache.lucene.codecs.simpletext;
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to You under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-import java.io.IOException;
-
-import org.apache.lucene.codecs.DocValuesArraySource;
-import org.apache.lucene.codecs.DocValuesConsumer;
-import org.apache.lucene.index.DocValues.Type;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.StorableField;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.BytesRefHash;
-import org.apache.lucene.util.IOUtils;
-
-/**
- * Writes plain-text DocValues.
- * <p>
- * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
- * 
- * @lucene.experimental
- */
-public class SimpleTextDocValuesConsumer extends DocValuesConsumer {
-
-  static final BytesRef ZERO_DOUBLE = new BytesRef(Double.toString(0d));
-  static final BytesRef ZERO_INT = new BytesRef(Integer.toString(0));
-  static final BytesRef HEADER = new BytesRef("SimpleTextDocValues"); 
-
-  static final BytesRef END = new BytesRef("END");
-  static final BytesRef VALUE_SIZE = new BytesRef("valuesize ");
-  static final BytesRef DOC = new BytesRef("  doc ");
-  static final BytesRef VALUE = new BytesRef("    value ");
-  protected BytesRef scratch = new BytesRef();
-  protected int maxDocId = -1;
-  protected final String segment;
-  protected final Directory dir;
-  protected final IOContext ctx;
-  protected final Type type;
-  protected final BytesRefHash hash;
-  private int[] ords;
-  private int valueSize = Integer.MIN_VALUE;
-  private BytesRef zeroBytes;
-  private final String segmentSuffix;
-  
-
-  public SimpleTextDocValuesConsumer(String segment, Directory dir,
-      IOContext ctx, Type type, String segmentSuffix) {
-    this.ctx = ctx;
-    this.dir = dir;
-    this.segment = segment;
-    this.type = type;
-    hash = new BytesRefHash();
-    ords = new int[0];
-    this.segmentSuffix = segmentSuffix;
-  }
-
-  @Override
-  public void add(int docID, StorableField value) throws IOException {
-    assert docID >= 0;
-    final int ord, vSize;
-    switch (type) {
-    case BYTES_FIXED_DEREF:
-    case BYTES_FIXED_SORTED:
-    case BYTES_FIXED_STRAIGHT:
-      vSize = value.binaryValue().length;
-      ord = hash.add(value.binaryValue());
-      break;
-    case BYTES_VAR_DEREF:
-    case BYTES_VAR_SORTED:
-    case BYTES_VAR_STRAIGHT:
-      vSize = -1;
-      ord = hash.add(value.binaryValue());
-      break;
-    case FIXED_INTS_16:
-      vSize = 2;
-      scratch.grow(2);
-      DocValuesArraySource.copyShort(scratch, value.numericValue().shortValue());
-      ord = hash.add(scratch);
-      break;
-    case FIXED_INTS_32:
-      vSize = 4;
-      scratch.grow(4);
-      DocValuesArraySource.copyInt(scratch, value.numericValue().intValue());
-      ord = hash.add(scratch);
-      break;
-    case FIXED_INTS_8:
-      vSize = 1;
-      scratch.grow(1); 
-      scratch.bytes[scratch.offset] = value.numericValue().byteValue();
-      scratch.length = 1;
-      ord = hash.add(scratch);
-      break;
-    case FIXED_INTS_64:
-      vSize = 8;
-      scratch.grow(8);
-      DocValuesArraySource.copyLong(scratch, value.numericValue().longValue());
-      ord = hash.add(scratch);
-      break;
-    case VAR_INTS:
-      vSize = -1;
-      scratch.grow(8);
-      DocValuesArraySource.copyLong(scratch, value.numericValue().longValue());
-      ord = hash.add(scratch);
-      break;
-    case FLOAT_32:
-      vSize = 4;
-      scratch.grow(4);
-      DocValuesArraySource.copyInt(scratch,
-          Float.floatToRawIntBits(value.numericValue().floatValue()));
-      ord = hash.add(scratch);
-      break;
-    case FLOAT_64:
-      vSize = 8;
-      scratch.grow(8);
-      DocValuesArraySource.copyLong(scratch,
-          Double.doubleToRawLongBits(value.numericValue().doubleValue()));
-      ord = hash.add(scratch);
-      break;
-    default:
-      throw new RuntimeException("should not reach this line");
-    }
-    
-    if (valueSize == Integer.MIN_VALUE) {
-      assert maxDocId == -1;
-      valueSize = vSize;
-    } else {
-      if (valueSize != vSize) {
-        throw new IllegalArgumentException("value size must be " + valueSize + " but was: " + vSize);
-      }
-    }
-    maxDocId = Math.max(docID, maxDocId);
-    ords = grow(ords, docID);
-    
-    ords[docID] = (ord < 0 ? (-ord)-1 : ord) + 1;
-  }
-  
-  protected BytesRef getHeader() {
-    return HEADER;
-  }
-
-  private int[] grow(int[] array, int upto) {
-    if (array.length <= upto) {
-      return ArrayUtil.grow(array, 1 + upto);
-    }
-    return array;
-  }
-
-  private void prepareFlush(int docCount) {
-    assert ords != null;
-    ords = grow(ords, docCount);
-  }
-
-  @Override
-  public void finish(int docCount) throws IOException {
-    final String fileName = IndexFileNames.segmentFileName(segment, "",
-        segmentSuffix);
-    IndexOutput output = dir.createOutput(fileName, ctx);
-    boolean success = false;
-    BytesRef spare = new BytesRef();
-    try {
-      SimpleTextUtil.write(output, getHeader());
-      SimpleTextUtil.writeNewline(output);
-      SimpleTextUtil.write(output, VALUE_SIZE);
-      SimpleTextUtil.write(output, Integer.toString(this.valueSize), scratch);
-      SimpleTextUtil.writeNewline(output);
-      prepareFlush(docCount);
-      for (int i = 0; i < docCount; i++) {
-        SimpleTextUtil.write(output, DOC);
-        SimpleTextUtil.write(output, Integer.toString(i), scratch);
-        SimpleTextUtil.writeNewline(output);
-        SimpleTextUtil.write(output, VALUE);
-        writeDoc(output, i, spare);
-        SimpleTextUtil.writeNewline(output);
-      }
-      SimpleTextUtil.write(output, END);
-      SimpleTextUtil.writeNewline(output);
-      success = true;
-    } finally {
-      hash.close();
-      if (success) {
-        IOUtils.close(output);
-      } else {
-        IOUtils.closeWhileHandlingException(output);
-      }
-    }
-  }
-
-  protected void writeDoc(IndexOutput output, int docId, BytesRef spare) throws IOException {
-    int ord = ords[docId] - 1;
-    if (ord != -1) {
-      assert ord >= 0;
-      hash.get(ord, spare);
-
-      switch (type) {
-      case BYTES_FIXED_DEREF:
-      case BYTES_FIXED_SORTED:
-      case BYTES_FIXED_STRAIGHT:
-      case BYTES_VAR_DEREF:
-      case BYTES_VAR_SORTED:
-      case BYTES_VAR_STRAIGHT:
-        SimpleTextUtil.write(output, spare);
-        break;
-      case FIXED_INTS_16:
-        SimpleTextUtil.write(output,
-            Short.toString(DocValuesArraySource.asShort(spare)), scratch);
-        break;
-      case FIXED_INTS_32:
-        SimpleTextUtil.write(output,
-            Integer.toString(DocValuesArraySource.asInt(spare)), scratch);
-        break;
-      case VAR_INTS:
-      case FIXED_INTS_64:
-        SimpleTextUtil.write(output,
-            Long.toString(DocValuesArraySource.asLong(spare)), scratch);
-        break;
-      case FIXED_INTS_8:
-        assert spare.length == 1 : spare.length;
-        SimpleTextUtil.write(output,
-            Integer.toString(spare.bytes[spare.offset]), scratch);
-        break;
-      case FLOAT_32:
-        float valueFloat = Float.intBitsToFloat(DocValuesArraySource.asInt(spare));
-        SimpleTextUtil.write(output, Float.toString(valueFloat), scratch);
-        break;
-      case FLOAT_64:
-        double valueDouble = Double.longBitsToDouble(DocValuesArraySource
-            .asLong(spare));
-        SimpleTextUtil.write(output, Double.toString(valueDouble), scratch);
-        break;
-      default:
-        throw new IllegalArgumentException("unsupported type: " + type);
-      }
-    } else {
-      switch (type) {
-      case BYTES_FIXED_DEREF:
-      case BYTES_FIXED_SORTED:
-      case BYTES_FIXED_STRAIGHT:
-        if(zeroBytes == null) {
-          assert valueSize > 0;
-          zeroBytes = new BytesRef(new byte[valueSize]);
-        }
-        SimpleTextUtil.write(output, zeroBytes);
-        break;
-      case BYTES_VAR_DEREF:
-      case BYTES_VAR_SORTED:
-      case BYTES_VAR_STRAIGHT:
-        scratch.length = 0;
-        SimpleTextUtil.write(output, scratch);
-        break;
-      case FIXED_INTS_16:
-      case FIXED_INTS_32:
-      case FIXED_INTS_64:
-      case FIXED_INTS_8:
-      case VAR_INTS:
-        SimpleTextUtil.write(output, ZERO_INT);
-        break;
-      case FLOAT_32:
-      case FLOAT_64:
-        SimpleTextUtil.write(output, ZERO_DOUBLE);
-        break;
-      default:
-        throw new IllegalArgumentException("unsupported type: " + type);
-      }
-    }
-
-  }
-
-  @Override
-  protected Type getType() {
-    return type;
-  }
-
-  @Override
-  public int getValueSize() {
-    return valueSize;
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesFormat.java
deleted file mode 100644
index 033136e..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextDocValuesFormat.java
+++ /dev/null
@@ -1,51 +0,0 @@
-package org.apache.lucene.codecs.simpletext;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to You under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.DocValuesFormat;
-import org.apache.lucene.codecs.PerDocConsumer;
-import org.apache.lucene.codecs.PerDocProducer;
-import org.apache.lucene.index.PerDocWriteState;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.util.BytesRef;
-
-/**
- * Plain-text DocValues format.
- * <p>
- * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
- * 
- * @lucene.experimental
- */
-public class SimpleTextDocValuesFormat extends DocValuesFormat {
-  private static final String DOC_VALUES_SEG_SUFFIX = "dv";
-  @Override
-  public PerDocConsumer docsConsumer(PerDocWriteState state) throws IOException {
-    return new SimpleTextPerDocConsumer(state, DOC_VALUES_SEG_SUFFIX);
-  }
-
-  @Override
-  public PerDocProducer docsProducer(SegmentReadState state) throws IOException {
-    return new SimpleTextPerDocProducer(state, BytesRef.getUTF8SortedAsUnicodeComparator(), DOC_VALUES_SEG_SUFFIX);
-  }
-
-  static String docValuesId(String segmentsName, int fieldId) {
-    return segmentsName + "_" + fieldId;
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldInfosFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldInfosFormat.java
deleted file mode 100644
index efe9ff4..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldInfosFormat.java
+++ /dev/null
@@ -1,45 +0,0 @@
-package org.apache.lucene.codecs.simpletext;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.FieldInfosFormat;
-import org.apache.lucene.codecs.FieldInfosReader;
-import org.apache.lucene.codecs.FieldInfosWriter;
-
-/**
- * plaintext field infos format
- * <p>
- * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
- * @lucene.experimental
- */
-public class SimpleTextFieldInfosFormat extends FieldInfosFormat {
-  private final FieldInfosReader reader = new SimpleTextFieldInfosReader();
-  private final FieldInfosWriter writer = new SimpleTextFieldInfosWriter();
-
-  @Override
-  public FieldInfosReader getFieldInfosReader() throws IOException {
-    return reader;
-  }
-
-  @Override
-  public FieldInfosWriter getFieldInfosWriter() throws IOException {
-    return writer;
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldInfosReader.java b/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldInfosReader.java
deleted file mode 100644
index 87891f4..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldInfosReader.java
+++ /dev/null
@@ -1,147 +0,0 @@
-package org.apache.lucene.codecs.simpletext;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.Map;
-
-import org.apache.lucene.codecs.FieldInfosReader;
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.StringHelper;
-
-import static org.apache.lucene.codecs.simpletext.SimpleTextFieldInfosWriter.*;
-
-/**
- * reads plaintext field infos files
- * <p>
- * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
- * @lucene.experimental
- */
-public class SimpleTextFieldInfosReader extends FieldInfosReader {
-
-  @Override
-  public FieldInfos read(Directory directory, String segmentName, IOContext iocontext) throws IOException {
-    final String fileName = IndexFileNames.segmentFileName(segmentName, "", FIELD_INFOS_EXTENSION);
-    IndexInput input = directory.openInput(fileName, iocontext);
-    BytesRef scratch = new BytesRef();
-    
-    try {
-      
-      SimpleTextUtil.readLine(input, scratch);
-      assert StringHelper.startsWith(scratch, NUMFIELDS);
-      final int size = Integer.parseInt(readString(NUMFIELDS.length, scratch));
-      FieldInfo infos[] = new FieldInfo[size];
-
-      for (int i = 0; i < size; i++) {
-        SimpleTextUtil.readLine(input, scratch);
-        assert StringHelper.startsWith(scratch, NAME);
-        String name = readString(NAME.length, scratch);
-        
-        SimpleTextUtil.readLine(input, scratch);
-        assert StringHelper.startsWith(scratch, NUMBER);
-        int fieldNumber = Integer.parseInt(readString(NUMBER.length, scratch));
-
-        SimpleTextUtil.readLine(input, scratch);
-        assert StringHelper.startsWith(scratch, ISINDEXED);
-        boolean isIndexed = Boolean.parseBoolean(readString(ISINDEXED.length, scratch));
-        
-        final IndexOptions indexOptions;
-        if (isIndexed) {
-          SimpleTextUtil.readLine(input, scratch);
-          assert StringHelper.startsWith(scratch, INDEXOPTIONS);
-          indexOptions = IndexOptions.valueOf(readString(INDEXOPTIONS.length, scratch));          
-        } else {
-          indexOptions = null;
-        }
-        
-        SimpleTextUtil.readLine(input, scratch);
-        assert StringHelper.startsWith(scratch, STORETV);
-        boolean storeTermVector = Boolean.parseBoolean(readString(STORETV.length, scratch));
-        
-        SimpleTextUtil.readLine(input, scratch);
-        assert StringHelper.startsWith(scratch, PAYLOADS);
-        boolean storePayloads = Boolean.parseBoolean(readString(PAYLOADS.length, scratch));
-        
-        SimpleTextUtil.readLine(input, scratch);
-        assert StringHelper.startsWith(scratch, NORMS);
-        boolean omitNorms = !Boolean.parseBoolean(readString(NORMS.length, scratch));
-        
-        SimpleTextUtil.readLine(input, scratch);
-        assert StringHelper.startsWith(scratch, NORMS_TYPE);
-        String nrmType = readString(NORMS_TYPE.length, scratch);
-        final DocValues.Type normsType = docValuesType(nrmType);
-        
-        SimpleTextUtil.readLine(input, scratch);
-        assert StringHelper.startsWith(scratch, DOCVALUES);
-        String dvType = readString(DOCVALUES.length, scratch);
-        final DocValues.Type docValuesType = docValuesType(dvType);
-        
-        SimpleTextUtil.readLine(input, scratch);
-        assert StringHelper.startsWith(scratch, NUM_ATTS);
-        int numAtts = Integer.parseInt(readString(NUM_ATTS.length, scratch));
-        Map<String,String> atts = new HashMap<String,String>();
-
-        for (int j = 0; j < numAtts; j++) {
-          SimpleTextUtil.readLine(input, scratch);
-          assert StringHelper.startsWith(scratch, ATT_KEY);
-          String key = readString(ATT_KEY.length, scratch);
-        
-          SimpleTextUtil.readLine(input, scratch);
-          assert StringHelper.startsWith(scratch, ATT_VALUE);
-          String value = readString(ATT_VALUE.length, scratch);
-          atts.put(key, value);
-        }
-
-        infos[i] = new FieldInfo(name, isIndexed, fieldNumber, storeTermVector, 
-          omitNorms, storePayloads, indexOptions, docValuesType, normsType, Collections.unmodifiableMap(atts));
-      }
-
-      if (input.getFilePointer() != input.length()) {
-        throw new CorruptIndexException("did not read all bytes from file \"" + fileName + "\": read " + input.getFilePointer() + " vs size " + input.length() + " (resource: " + input + ")");
-      }
-      
-      return new FieldInfos(infos);
-    } finally {
-      input.close();
-    }
-  }
-
-  public DocValues.Type docValuesType(String dvType) {
-    if ("false".equals(dvType)) {
-      return null;
-    } else {
-      return DocValues.Type.valueOf(dvType);
-    }
-  }
-  
-  private String readString(int offset, BytesRef scratch) {
-    return new String(scratch.bytes, scratch.offset+offset, scratch.length-offset, IOUtils.CHARSET_UTF_8);
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldInfosWriter.java b/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldInfosWriter.java
deleted file mode 100644
index d8e4072..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldInfosWriter.java
+++ /dev/null
@@ -1,136 +0,0 @@
-package org.apache.lucene.codecs.simpletext;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-import java.io.IOException;
-import java.util.Map;
-
-import org.apache.lucene.codecs.FieldInfosWriter;
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.BytesRef;
-
-/**
- * writes plaintext field infos files
- * <p>
- * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
- * @lucene.experimental
- */
-public class SimpleTextFieldInfosWriter extends FieldInfosWriter {
-  
-  /** Extension of field infos */
-  static final String FIELD_INFOS_EXTENSION = "inf";
-  
-  static final BytesRef NUMFIELDS       =  new BytesRef("number of fields ");
-  static final BytesRef NAME            =  new BytesRef("  name ");
-  static final BytesRef NUMBER          =  new BytesRef("  number ");
-  static final BytesRef ISINDEXED       =  new BytesRef("  indexed ");
-  static final BytesRef STORETV         =  new BytesRef("  term vectors ");
-  static final BytesRef STORETVPOS      =  new BytesRef("  term vector positions ");
-  static final BytesRef STORETVOFF      =  new BytesRef("  term vector offsets ");
-  static final BytesRef PAYLOADS        =  new BytesRef("  payloads ");
-  static final BytesRef NORMS           =  new BytesRef("  norms ");
-  static final BytesRef NORMS_TYPE      =  new BytesRef("  norms type ");
-  static final BytesRef DOCVALUES       =  new BytesRef("  doc values ");
-  static final BytesRef INDEXOPTIONS    =  new BytesRef("  index options ");
-  static final BytesRef NUM_ATTS        =  new BytesRef("  attributes ");
-  final static BytesRef ATT_KEY         =  new BytesRef("    key ");
-  final static BytesRef ATT_VALUE       =  new BytesRef("    value ");
-  
-  @Override
-  public void write(Directory directory, String segmentName, FieldInfos infos, IOContext context) throws IOException {
-    final String fileName = IndexFileNames.segmentFileName(segmentName, "", FIELD_INFOS_EXTENSION);
-    IndexOutput out = directory.createOutput(fileName, context);
-    BytesRef scratch = new BytesRef();
-    try {
-      SimpleTextUtil.write(out, NUMFIELDS);
-      SimpleTextUtil.write(out, Integer.toString(infos.size()), scratch);
-      SimpleTextUtil.writeNewline(out);
-      
-      for (FieldInfo fi : infos) {
-        SimpleTextUtil.write(out, NAME);
-        SimpleTextUtil.write(out, fi.name, scratch);
-        SimpleTextUtil.writeNewline(out);
-        
-        SimpleTextUtil.write(out, NUMBER);
-        SimpleTextUtil.write(out, Integer.toString(fi.number), scratch);
-        SimpleTextUtil.writeNewline(out);
-        
-        SimpleTextUtil.write(out, ISINDEXED);
-        SimpleTextUtil.write(out, Boolean.toString(fi.isIndexed()), scratch);
-        SimpleTextUtil.writeNewline(out);
-        
-        if (fi.isIndexed()) {
-          assert fi.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0 || !fi.hasPayloads();
-          SimpleTextUtil.write(out, INDEXOPTIONS);
-          SimpleTextUtil.write(out, fi.getIndexOptions().toString(), scratch);
-          SimpleTextUtil.writeNewline(out);
-        }
-        
-        SimpleTextUtil.write(out, STORETV);
-        SimpleTextUtil.write(out, Boolean.toString(fi.hasVectors()), scratch);
-        SimpleTextUtil.writeNewline(out);
-        
-        SimpleTextUtil.write(out, PAYLOADS);
-        SimpleTextUtil.write(out, Boolean.toString(fi.hasPayloads()), scratch);
-        SimpleTextUtil.writeNewline(out);
-               
-        SimpleTextUtil.write(out, NORMS);
-        SimpleTextUtil.write(out, Boolean.toString(!fi.omitsNorms()), scratch);
-        SimpleTextUtil.writeNewline(out);
-        
-        SimpleTextUtil.write(out, NORMS_TYPE);
-        SimpleTextUtil.write(out, getDocValuesType(fi.getNormType()), scratch);
-        SimpleTextUtil.writeNewline(out);
-        
-        SimpleTextUtil.write(out, DOCVALUES);
-        SimpleTextUtil.write(out, getDocValuesType(fi.getDocValuesType()), scratch);
-        SimpleTextUtil.writeNewline(out);
-               
-        Map<String,String> atts = fi.attributes();
-        int numAtts = atts == null ? 0 : atts.size();
-        SimpleTextUtil.write(out, NUM_ATTS);
-        SimpleTextUtil.write(out, Integer.toString(numAtts), scratch);
-        SimpleTextUtil.writeNewline(out);
-      
-        if (numAtts > 0) {
-          for (Map.Entry<String,String> entry : atts.entrySet()) {
-            SimpleTextUtil.write(out, ATT_KEY);
-            SimpleTextUtil.write(out, entry.getKey(), scratch);
-            SimpleTextUtil.writeNewline(out);
-          
-            SimpleTextUtil.write(out, ATT_VALUE);
-            SimpleTextUtil.write(out, entry.getValue(), scratch);
-            SimpleTextUtil.writeNewline(out);
-          }
-        }
-      }
-    } finally {
-      out.close();
-    }
-  }
-  
-  private static String getDocValuesType(DocValues.Type type) {
-    return type == null ? "false" : type.toString();
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldsReader.java b/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldsReader.java
deleted file mode 100644
index c56fd20..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldsReader.java
+++ /dev/null
@@ -1,640 +0,0 @@
-package org.apache.lucene.codecs.simpletext;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Collections;
-import java.util.Comparator;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.Map;
-import java.util.TreeMap;
-import java.util.TreeSet;
-
-import org.apache.lucene.codecs.FieldsProducer;
-import org.apache.lucene.index.DocsAndPositionsEnum;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.Terms;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.CharsRef;
-import org.apache.lucene.util.IntsRef;
-import org.apache.lucene.util.OpenBitSet;
-import org.apache.lucene.util.StringHelper;
-import org.apache.lucene.util.UnicodeUtil;
-import org.apache.lucene.util.fst.Builder;
-import org.apache.lucene.util.fst.BytesRefFSTEnum;
-import org.apache.lucene.util.fst.FST;
-import org.apache.lucene.util.fst.PairOutputs;
-import org.apache.lucene.util.fst.PositiveIntOutputs;
-import org.apache.lucene.util.fst.Util;
-
-class SimpleTextFieldsReader extends FieldsProducer {
-  private final TreeMap<String,Long> fields;
-  private final IndexInput in;
-  private final FieldInfos fieldInfos;
-
-  final static BytesRef END          = SimpleTextFieldsWriter.END;
-  final static BytesRef FIELD        = SimpleTextFieldsWriter.FIELD;
-  final static BytesRef TERM         = SimpleTextFieldsWriter.TERM;
-  final static BytesRef DOC          = SimpleTextFieldsWriter.DOC;
-  final static BytesRef FREQ         = SimpleTextFieldsWriter.FREQ;
-  final static BytesRef POS          = SimpleTextFieldsWriter.POS;
-  final static BytesRef START_OFFSET = SimpleTextFieldsWriter.START_OFFSET;
-  final static BytesRef END_OFFSET   = SimpleTextFieldsWriter.END_OFFSET;
-  final static BytesRef PAYLOAD      = SimpleTextFieldsWriter.PAYLOAD;
-
-  public SimpleTextFieldsReader(SegmentReadState state) throws IOException {
-    in = state.dir.openInput(SimpleTextPostingsFormat.getPostingsFileName(state.segmentInfo.name, state.segmentSuffix), state.context);
-   
-    fieldInfos = state.fieldInfos;
-    fields = readFields(in.clone());
-  }
-  
-  private TreeMap<String,Long> readFields(IndexInput in) throws IOException {
-    BytesRef scratch = new BytesRef(10);
-    TreeMap<String,Long> fields = new TreeMap<String,Long>();
-    
-    while (true) {
-      SimpleTextUtil.readLine(in, scratch);
-      if (scratch.equals(END)) {
-        return fields;
-      } else if (StringHelper.startsWith(scratch, FIELD)) {
-        String fieldName = new String(scratch.bytes, scratch.offset + FIELD.length, scratch.length - FIELD.length, "UTF-8");
-        fields.put(fieldName, in.getFilePointer());
-      }
-    }
-  }
-
-  private class SimpleTextTermsEnum extends TermsEnum {
-    private final IndexOptions indexOptions;
-    private int docFreq;
-    private long totalTermFreq;
-    private long docsStart;
-    private boolean ended;
-    private final BytesRefFSTEnum<PairOutputs.Pair<Long,PairOutputs.Pair<Long,Long>>> fstEnum;
-
-    public SimpleTextTermsEnum(FST<PairOutputs.Pair<Long,PairOutputs.Pair<Long,Long>>> fst, IndexOptions indexOptions) {
-      this.indexOptions = indexOptions;
-      fstEnum = new BytesRefFSTEnum<PairOutputs.Pair<Long,PairOutputs.Pair<Long,Long>>>(fst);
-    }
-
-    @Override
-    public boolean seekExact(BytesRef text, boolean useCache /* ignored */) throws IOException {
-
-      final BytesRefFSTEnum.InputOutput<PairOutputs.Pair<Long,PairOutputs.Pair<Long,Long>>> result = fstEnum.seekExact(text);
-      if (result != null) {
-        PairOutputs.Pair<Long,PairOutputs.Pair<Long,Long>> pair1 = result.output;
-        PairOutputs.Pair<Long,Long> pair2 = pair1.output2;
-        docsStart = pair1.output1;
-        docFreq = pair2.output1.intValue();
-        totalTermFreq = pair2.output2;
-        return true;
-      } else {
-        return false;
-      }
-    }
-
-    @Override
-    public SeekStatus seekCeil(BytesRef text, boolean useCache /* ignored */) throws IOException {
-
-      //System.out.println("seek to text=" + text.utf8ToString());
-      final BytesRefFSTEnum.InputOutput<PairOutputs.Pair<Long,PairOutputs.Pair<Long,Long>>> result = fstEnum.seekCeil(text);
-      if (result == null) {
-        //System.out.println("  end");
-        return SeekStatus.END;
-      } else {
-        //System.out.println("  got text=" + term.utf8ToString());
-        PairOutputs.Pair<Long,PairOutputs.Pair<Long,Long>> pair1 = result.output;
-        PairOutputs.Pair<Long,Long> pair2 = pair1.output2;
-        docsStart = pair1.output1;
-        docFreq = pair2.output1.intValue();
-        totalTermFreq = pair2.output2;
-
-        if (result.input.equals(text)) {
-          //System.out.println("  match docsStart=" + docsStart);
-          return SeekStatus.FOUND;
-        } else {
-          //System.out.println("  not match docsStart=" + docsStart);
-          return SeekStatus.NOT_FOUND;
-        }
-      }
-    }
-
-    @Override
-    public BytesRef next() throws IOException {
-      assert !ended;
-      final BytesRefFSTEnum.InputOutput<PairOutputs.Pair<Long,PairOutputs.Pair<Long,Long>>> result = fstEnum.next();
-      if (result != null) {
-        PairOutputs.Pair<Long,PairOutputs.Pair<Long,Long>> pair1 = result.output;
-        PairOutputs.Pair<Long,Long> pair2 = pair1.output2;
-        docsStart = pair1.output1;
-        docFreq = pair2.output1.intValue();
-        totalTermFreq = pair2.output2;
-        return result.input;
-      } else {
-        return null;
-      }
-    }
-
-    @Override
-    public BytesRef term() {
-      return fstEnum.current().input;
-    }
-
-    @Override
-    public long ord() throws IOException {
-      throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public void seekExact(long ord) {
-      throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public int docFreq() {
-      return docFreq;
-    }
-
-    @Override
-    public long totalTermFreq() {
-      return indexOptions == IndexOptions.DOCS_ONLY ? -1 : totalTermFreq;
-    }
- 
-    @Override
-    public DocsEnum docs(Bits liveDocs, DocsEnum reuse, int flags) throws IOException {
-      SimpleTextDocsEnum docsEnum;
-      if (reuse != null && reuse instanceof SimpleTextDocsEnum && ((SimpleTextDocsEnum) reuse).canReuse(SimpleTextFieldsReader.this.in)) {
-        docsEnum = (SimpleTextDocsEnum) reuse;
-      } else {
-        docsEnum = new SimpleTextDocsEnum();
-      }
-      return docsEnum.reset(docsStart, liveDocs, indexOptions == IndexOptions.DOCS_ONLY);
-    }
-
-    @Override
-    public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum reuse, int flags) throws IOException {
-
-      if (indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) < 0) {
-        // Positions were not indexed
-        return null;
-      }
-
-      SimpleTextDocsAndPositionsEnum docsAndPositionsEnum;
-      if (reuse != null && reuse instanceof SimpleTextDocsAndPositionsEnum && ((SimpleTextDocsAndPositionsEnum) reuse).canReuse(SimpleTextFieldsReader.this.in)) {
-        docsAndPositionsEnum = (SimpleTextDocsAndPositionsEnum) reuse;
-      } else {
-        docsAndPositionsEnum = new SimpleTextDocsAndPositionsEnum();
-      } 
-      return docsAndPositionsEnum.reset(docsStart, liveDocs, indexOptions);
-    }
-    
-    @Override
-    public Comparator<BytesRef> getComparator() {
-      return BytesRef.getUTF8SortedAsUnicodeComparator();
-    }
-  }
-
-  private class SimpleTextDocsEnum extends DocsEnum {
-    private final IndexInput inStart;
-    private final IndexInput in;
-    private boolean omitTF;
-    private int docID = -1;
-    private int tf;
-    private Bits liveDocs;
-    private final BytesRef scratch = new BytesRef(10);
-    private final CharsRef scratchUTF16 = new CharsRef(10);
-    
-    public SimpleTextDocsEnum() {
-      this.inStart = SimpleTextFieldsReader.this.in;
-      this.in = this.inStart.clone();
-    }
-
-    public boolean canReuse(IndexInput in) {
-      return in == inStart;
-    }
-
-    public SimpleTextDocsEnum reset(long fp, Bits liveDocs, boolean omitTF) throws IOException {
-      this.liveDocs = liveDocs;
-      in.seek(fp);
-      this.omitTF = omitTF;
-      docID = -1;
-      tf = 1;
-      return this;
-    }
-
-    @Override
-    public int docID() {
-      return docID;
-    }
-
-    @Override
-    public int freq() throws IOException {
-      return tf;
-    }
-
-    @Override
-    public int nextDoc() throws IOException {
-      if (docID == NO_MORE_DOCS) {
-        return docID;
-      }
-      boolean first = true;
-      int termFreq = 0;
-      while(true) {
-        final long lineStart = in.getFilePointer();
-        SimpleTextUtil.readLine(in, scratch);
-        if (StringHelper.startsWith(scratch, DOC)) {
-          if (!first && (liveDocs == null || liveDocs.get(docID))) {
-            in.seek(lineStart);
-            if (!omitTF) {
-              tf = termFreq;
-            }
-            return docID;
-          }
-          UnicodeUtil.UTF8toUTF16(scratch.bytes, scratch.offset+DOC.length, scratch.length-DOC.length, scratchUTF16);
-          docID = ArrayUtil.parseInt(scratchUTF16.chars, 0, scratchUTF16.length);
-          termFreq = 0;
-          first = false;
-        } else if (StringHelper.startsWith(scratch, FREQ)) {
-          UnicodeUtil.UTF8toUTF16(scratch.bytes, scratch.offset+FREQ.length, scratch.length-FREQ.length, scratchUTF16);
-          termFreq = ArrayUtil.parseInt(scratchUTF16.chars, 0, scratchUTF16.length);
-        } else if (StringHelper.startsWith(scratch, POS)) {
-          // skip termFreq++;
-        } else if (StringHelper.startsWith(scratch, START_OFFSET)) {
-          // skip
-        } else if (StringHelper.startsWith(scratch, END_OFFSET)) {
-          // skip
-        } else if (StringHelper.startsWith(scratch, PAYLOAD)) {
-          // skip
-        } else {
-          assert StringHelper.startsWith(scratch, TERM) || StringHelper.startsWith(scratch, FIELD) || StringHelper.startsWith(scratch, END): "scratch=" + scratch.utf8ToString();
-          if (!first && (liveDocs == null || liveDocs.get(docID))) {
-            in.seek(lineStart);
-            if (!omitTF) {
-              tf = termFreq;
-            }
-            return docID;
-          }
-          return docID = NO_MORE_DOCS;
-        }
-      }
-    }
-
-    @Override
-    public int advance(int target) throws IOException {
-      // Naive -- better to index skip data
-      while(nextDoc() < target);
-      return docID;
-    }
-  }
-
-  private class SimpleTextDocsAndPositionsEnum extends DocsAndPositionsEnum {
-    private final IndexInput inStart;
-    private final IndexInput in;
-    private int docID = -1;
-    private int tf;
-    private Bits liveDocs;
-    private final BytesRef scratch = new BytesRef(10);
-    private final BytesRef scratch2 = new BytesRef(10);
-    private final CharsRef scratchUTF16 = new CharsRef(10);
-    private final CharsRef scratchUTF16_2 = new CharsRef(10);
-    private BytesRef payload;
-    private long nextDocStart;
-    private boolean readOffsets;
-    private boolean readPositions;
-    private int startOffset;
-    private int endOffset;
-
-    public SimpleTextDocsAndPositionsEnum() {
-      this.inStart = SimpleTextFieldsReader.this.in;
-      this.in = inStart.clone();
-    }
-
-    public boolean canReuse(IndexInput in) {
-      return in == inStart;
-    }
-
-    public SimpleTextDocsAndPositionsEnum reset(long fp, Bits liveDocs, IndexOptions indexOptions) {
-      this.liveDocs = liveDocs;
-      nextDocStart = fp;
-      docID = -1;
-      readPositions = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;
-      readOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
-      if (!readOffsets) {
-        startOffset = -1;
-        endOffset = -1;
-      }
-      return this;
-    }
-
-    @Override
-    public int docID() {
-      return docID;
-    }
-
-    @Override
-    public int freq() throws IOException {
-      return tf;
-    }
-
-    @Override
-    public int nextDoc() throws IOException {
-      boolean first = true;
-      in.seek(nextDocStart);
-      long posStart = 0;
-      while(true) {
-        final long lineStart = in.getFilePointer();
-        SimpleTextUtil.readLine(in, scratch);
-        //System.out.println("NEXT DOC: " + scratch.utf8ToString());
-        if (StringHelper.startsWith(scratch, DOC)) {
-          if (!first && (liveDocs == null || liveDocs.get(docID))) {
-            nextDocStart = lineStart;
-            in.seek(posStart);
-            return docID;
-          }
-          UnicodeUtil.UTF8toUTF16(scratch.bytes, scratch.offset+DOC.length, scratch.length-DOC.length, scratchUTF16);
-          docID = ArrayUtil.parseInt(scratchUTF16.chars, 0, scratchUTF16.length);
-          tf = 0;
-          first = false;
-        } else if (StringHelper.startsWith(scratch, FREQ)) {
-          UnicodeUtil.UTF8toUTF16(scratch.bytes, scratch.offset+FREQ.length, scratch.length-FREQ.length, scratchUTF16);
-          tf = ArrayUtil.parseInt(scratchUTF16.chars, 0, scratchUTF16.length);
-          posStart = in.getFilePointer();
-        } else if (StringHelper.startsWith(scratch, POS)) {
-          // skip
-        } else if (StringHelper.startsWith(scratch, START_OFFSET)) {
-          // skip
-        } else if (StringHelper.startsWith(scratch, END_OFFSET)) {
-          // skip
-        } else if (StringHelper.startsWith(scratch, PAYLOAD)) {
-          // skip
-        } else {
-          assert StringHelper.startsWith(scratch, TERM) || StringHelper.startsWith(scratch, FIELD) || StringHelper.startsWith(scratch, END);
-          if (!first && (liveDocs == null || liveDocs.get(docID))) {
-            nextDocStart = lineStart;
-            in.seek(posStart);
-            return docID;
-          }
-          return docID = NO_MORE_DOCS;
-        }
-      }
-    }
-
-    @Override
-    public int advance(int target) throws IOException {
-      // Naive -- better to index skip data
-      while(nextDoc() < target);
-      return docID;
-    }
-
-    @Override
-    public int nextPosition() throws IOException {
-      final int pos;
-      if (readPositions) {
-        SimpleTextUtil.readLine(in, scratch);
-        assert StringHelper.startsWith(scratch, POS): "got line=" + scratch.utf8ToString();
-        UnicodeUtil.UTF8toUTF16(scratch.bytes, scratch.offset+POS.length, scratch.length-POS.length, scratchUTF16_2);
-        pos = ArrayUtil.parseInt(scratchUTF16_2.chars, 0, scratchUTF16_2.length);
-      } else {
-        pos = -1;
-      }
-
-      if (readOffsets) {
-        SimpleTextUtil.readLine(in, scratch);
-        assert StringHelper.startsWith(scratch, START_OFFSET): "got line=" + scratch.utf8ToString();
-        UnicodeUtil.UTF8toUTF16(scratch.bytes, scratch.offset+START_OFFSET.length, scratch.length-START_OFFSET.length, scratchUTF16_2);
-        startOffset = ArrayUtil.parseInt(scratchUTF16_2.chars, 0, scratchUTF16_2.length);
-        SimpleTextUtil.readLine(in, scratch);
-        assert StringHelper.startsWith(scratch, END_OFFSET): "got line=" + scratch.utf8ToString();
-        UnicodeUtil.UTF8toUTF16(scratch.bytes, scratch.offset+END_OFFSET.length, scratch.length-END_OFFSET.length, scratchUTF16_2);
-        endOffset = ArrayUtil.parseInt(scratchUTF16_2.chars, 0, scratchUTF16_2.length);
-      }
-
-      final long fp = in.getFilePointer();
-      SimpleTextUtil.readLine(in, scratch);
-      if (StringHelper.startsWith(scratch, PAYLOAD)) {
-        final int len = scratch.length - PAYLOAD.length;
-        if (scratch2.bytes.length < len) {
-          scratch2.grow(len);
-        }
-        System.arraycopy(scratch.bytes, PAYLOAD.length, scratch2.bytes, 0, len);
-        scratch2.length = len;
-        payload = scratch2;
-      } else {
-        payload = null;
-        in.seek(fp);
-      }
-      return pos;
-    }
-
-    @Override
-    public int startOffset() throws IOException {
-      return startOffset;
-    }
-
-    @Override
-    public int endOffset() throws IOException {
-      return endOffset;
-    }
-
-    @Override
-    public BytesRef getPayload() {
-      return payload;
-    }
-  }
-
-  static class TermData {
-    public long docsStart;
-    public int docFreq;
-
-    public TermData(long docsStart, int docFreq) {
-      this.docsStart = docsStart;
-      this.docFreq = docFreq;
-    }
-  }
-
-  private class SimpleTextTerms extends Terms {
-    private final long termsStart;
-    private final FieldInfo fieldInfo;
-    private long sumTotalTermFreq;
-    private long sumDocFreq;
-    private int docCount;
-    private FST<PairOutputs.Pair<Long,PairOutputs.Pair<Long,Long>>> fst;
-    private int termCount;
-    private final BytesRef scratch = new BytesRef(10);
-    private final CharsRef scratchUTF16 = new CharsRef(10);
-
-    public SimpleTextTerms(String field, long termsStart) throws IOException {
-      this.termsStart = termsStart;
-      fieldInfo = fieldInfos.fieldInfo(field);
-      loadTerms();
-    }
-
-    private void loadTerms() throws IOException {
-      PositiveIntOutputs posIntOutputs = PositiveIntOutputs.getSingleton(false);
-      final Builder<PairOutputs.Pair<Long,PairOutputs.Pair<Long,Long>>> b;
-      final PairOutputs<Long,Long> outputsInner = new PairOutputs<Long,Long>(posIntOutputs, posIntOutputs);
-      final PairOutputs<Long,PairOutputs.Pair<Long,Long>> outputs = new PairOutputs<Long,PairOutputs.Pair<Long,Long>>(posIntOutputs,
-                                                                                                                      outputsInner);
-      b = new Builder<PairOutputs.Pair<Long,PairOutputs.Pair<Long,Long>>>(FST.INPUT_TYPE.BYTE1, outputs);
-      IndexInput in = SimpleTextFieldsReader.this.in.clone();
-      in.seek(termsStart);
-      final BytesRef lastTerm = new BytesRef(10);
-      long lastDocsStart = -1;
-      int docFreq = 0;
-      long totalTermFreq = 0;
-      OpenBitSet visitedDocs = new OpenBitSet();
-      final IntsRef scratchIntsRef = new IntsRef();
-      while(true) {
-        SimpleTextUtil.readLine(in, scratch);
-        if (scratch.equals(END) || StringHelper.startsWith(scratch, FIELD)) {
-          if (lastDocsStart != -1) {
-            b.add(Util.toIntsRef(lastTerm, scratchIntsRef),
-                  outputs.newPair(lastDocsStart,
-                                  outputsInner.newPair((long) docFreq, totalTermFreq)));
-            sumTotalTermFreq += totalTermFreq;
-          }
-          break;
-        } else if (StringHelper.startsWith(scratch, DOC)) {
-          docFreq++;
-          sumDocFreq++;
-          UnicodeUtil.UTF8toUTF16(scratch.bytes, scratch.offset+DOC.length, scratch.length-DOC.length, scratchUTF16);
-          int docID = ArrayUtil.parseInt(scratchUTF16.chars, 0, scratchUTF16.length);
-          visitedDocs.set(docID);
-        } else if (StringHelper.startsWith(scratch, FREQ)) {
-          UnicodeUtil.UTF8toUTF16(scratch.bytes, scratch.offset+FREQ.length, scratch.length-FREQ.length, scratchUTF16);
-          totalTermFreq += ArrayUtil.parseInt(scratchUTF16.chars, 0, scratchUTF16.length);
-        } else if (StringHelper.startsWith(scratch, TERM)) {
-          if (lastDocsStart != -1) {
-            b.add(Util.toIntsRef(lastTerm, scratchIntsRef), outputs.newPair(lastDocsStart,
-                                                                            outputsInner.newPair((long) docFreq, totalTermFreq)));
-          }
-          lastDocsStart = in.getFilePointer();
-          final int len = scratch.length - TERM.length;
-          if (len > lastTerm.length) {
-            lastTerm.grow(len);
-          }
-          System.arraycopy(scratch.bytes, TERM.length, lastTerm.bytes, 0, len);
-          lastTerm.length = len;
-          docFreq = 0;
-          sumTotalTermFreq += totalTermFreq;
-          totalTermFreq = 0;
-          termCount++;
-        }
-      }
-      docCount = (int) visitedDocs.cardinality();
-      fst = b.finish();
-      /*
-      PrintStream ps = new PrintStream("out.dot");
-      fst.toDot(ps);
-      ps.close();
-      System.out.println("SAVED out.dot");
-      */
-      //System.out.println("FST " + fst.sizeInBytes());
-    }
-
-    @Override
-    public TermsEnum iterator(TermsEnum reuse) throws IOException {
-      if (fst != null) {
-        return new SimpleTextTermsEnum(fst, fieldInfo.getIndexOptions());
-      } else {
-        return TermsEnum.EMPTY;
-      }
-    }
-
-    @Override
-    public Comparator<BytesRef> getComparator() {
-      return BytesRef.getUTF8SortedAsUnicodeComparator();
-    }
-
-    @Override
-    public long size() {
-      return (long) termCount;
-    }
-
-    @Override
-    public long getSumTotalTermFreq() {
-      return fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : sumTotalTermFreq;
-    }
-
-    @Override
-    public long getSumDocFreq() throws IOException {
-      return sumDocFreq;
-    }
-
-    @Override
-    public int getDocCount() throws IOException {
-      return docCount;
-    }
-
-    @Override
-    public boolean hasOffsets() {
-      return fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
-    }
-
-    @Override
-    public boolean hasPositions() {
-      return fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;
-    }
-    
-    @Override
-    public boolean hasPayloads() {
-      return fieldInfo.hasPayloads();
-    }
-  }
-
-  @Override
-  public Iterator<String> iterator() {
-    return Collections.unmodifiableSet(fields.keySet()).iterator();
-  }
-
-  private final Map<String,Terms> termsCache = new HashMap<String,Terms>();
-
-  @Override
-  synchronized public Terms terms(String field) throws IOException {
-    Terms terms = termsCache.get(field);
-    if (terms == null) {
-      Long fp = fields.get(field);
-      if (fp == null) {
-        return null;
-      } else {
-        terms = new SimpleTextTerms(field, fp);
-        termsCache.put(field, terms);
-      }
-    }
-    return terms;
-  }
-
-  @Override
-  public int size() {
-    return -1;
-  }
-
-  @Override
-  public void close() throws IOException {
-    in.close();
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldsWriter.java b/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldsWriter.java
deleted file mode 100644
index 5a59399..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextFieldsWriter.java
+++ /dev/null
@@ -1,187 +0,0 @@
-package org.apache.lucene.codecs.simpletext;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.codecs.FieldsConsumer;
-import org.apache.lucene.codecs.PostingsConsumer;
-import org.apache.lucene.codecs.TermStats;
-import org.apache.lucene.codecs.TermsConsumer;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.store.IndexOutput;
-
-import java.io.IOException;
-import java.util.Comparator;
-
-class SimpleTextFieldsWriter extends FieldsConsumer {
-  
-  private final IndexOutput out;
-  private final BytesRef scratch = new BytesRef(10);
-
-  final static BytesRef END          = new BytesRef("END");
-  final static BytesRef FIELD        = new BytesRef("field ");
-  final static BytesRef TERM         = new BytesRef("  term ");
-  final static BytesRef DOC          = new BytesRef("    doc ");
-  final static BytesRef FREQ         = new BytesRef("      freq ");
-  final static BytesRef POS          = new BytesRef("      pos ");
-  final static BytesRef START_OFFSET = new BytesRef("      startOffset ");
-  final static BytesRef END_OFFSET   = new BytesRef("      endOffset ");
-  final static BytesRef PAYLOAD      = new BytesRef("        payload ");
-
-  public SimpleTextFieldsWriter(SegmentWriteState state) throws IOException {
-    final String fileName = SimpleTextPostingsFormat.getPostingsFileName(state.segmentInfo.name, state.segmentSuffix);
-    out = state.directory.createOutput(fileName, state.context);
-  }
-
-  private void write(String s) throws IOException {
-    SimpleTextUtil.write(out, s, scratch);
-  }
-
-  private void write(BytesRef b) throws IOException {
-    SimpleTextUtil.write(out, b);
-  }
-
-  private void newline() throws IOException {
-    SimpleTextUtil.writeNewline(out);
-  }
-
-  @Override
-  public TermsConsumer addField(FieldInfo field) throws IOException {
-    write(FIELD);
-    write(field.name);
-    newline();
-    return new SimpleTextTermsWriter(field);
-  }
-
-  private class SimpleTextTermsWriter extends TermsConsumer {
-    private final SimpleTextPostingsWriter postingsWriter;
-    
-    public SimpleTextTermsWriter(FieldInfo field) {
-      postingsWriter = new SimpleTextPostingsWriter(field);
-    }
-
-    @Override
-    public PostingsConsumer startTerm(BytesRef term) throws IOException {
-      return postingsWriter.reset(term);
-    }
-
-    @Override
-    public void finishTerm(BytesRef term, TermStats stats) throws IOException {
-    }
-
-    @Override
-    public void finish(long sumTotalTermFreq, long sumDocFreq, int docCount) throws IOException {
-    }
-
-    @Override
-    public Comparator<BytesRef> getComparator() {
-      return BytesRef.getUTF8SortedAsUnicodeComparator();
-    }
-  }
-
-  private class SimpleTextPostingsWriter extends PostingsConsumer {
-    private BytesRef term;
-    private boolean wroteTerm;
-    private final IndexOptions indexOptions;
-    private final boolean writePositions;
-    private final boolean writeOffsets;
-
-    // for assert:
-    private int lastStartOffset = 0;
-
-    public SimpleTextPostingsWriter(FieldInfo field) {
-      this.indexOptions = field.getIndexOptions();
-      writePositions = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;
-      writeOffsets = indexOptions.compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
-      //System.out.println("writeOffsets=" + writeOffsets);
-      //System.out.println("writePos=" + writePositions);
-    }
-
-    @Override
-    public void startDoc(int docID, int termDocFreq) throws IOException {
-      if (!wroteTerm) {
-        // we lazily do this, in case the term had zero docs
-        write(TERM);
-        write(term);
-        newline();
-        wroteTerm = true;
-      }
-
-      write(DOC);
-      write(Integer.toString(docID));
-      newline();
-      if (indexOptions != IndexOptions.DOCS_ONLY) {
-        write(FREQ);
-        write(Integer.toString(termDocFreq));
-        newline();
-      }
-
-      lastStartOffset = 0;
-    }
-    
-    public PostingsConsumer reset(BytesRef term) {
-      this.term = term;
-      wroteTerm = false;
-      return this;
-    }
-
-    @Override
-    public void addPosition(int position, BytesRef payload, int startOffset, int endOffset) throws IOException {
-      if (writePositions) {
-        write(POS);
-        write(Integer.toString(position));
-        newline();
-      }
-
-      if (writeOffsets) {
-        assert endOffset >= startOffset;
-        assert startOffset >= lastStartOffset: "startOffset=" + startOffset + " lastStartOffset=" + lastStartOffset;
-        lastStartOffset = startOffset;
-        write(START_OFFSET);
-        write(Integer.toString(startOffset));
-        newline();
-        write(END_OFFSET);
-        write(Integer.toString(endOffset));
-        newline();
-      }
-
-      if (payload != null && payload.length > 0) {
-        assert payload.length != 0;
-        write(PAYLOAD);
-        write(payload);
-        newline();
-      }
-    }
-
-    @Override
-    public void finishDoc() {
-    }
-  }
-
-  @Override
-  public void close() throws IOException {
-    try {
-      write(END);
-      newline();
-    } finally {
-      out.close();
-    }
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextLiveDocsFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextLiveDocsFormat.java
deleted file mode 100644
index 4d9007d..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextLiveDocsFormat.java
+++ /dev/null
@@ -1,185 +0,0 @@
-package org.apache.lucene.codecs.simpletext;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.BitSet;
-import java.util.Collection;
-
-import org.apache.lucene.codecs.LiveDocsFormat;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentInfoPerCommit;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.CharsRef;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.MutableBits;
-import org.apache.lucene.util.StringHelper;
-import org.apache.lucene.util.UnicodeUtil;
-
-/**
- * reads/writes plaintext live docs
- * <p>
- * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
- * @lucene.experimental
- */
-public class SimpleTextLiveDocsFormat extends LiveDocsFormat {
-
-  static final String LIVEDOCS_EXTENSION = "liv";
-  
-  final static BytesRef SIZE             = new BytesRef("size ");
-  final static BytesRef DOC              = new BytesRef("  doc ");
-  final static BytesRef END              = new BytesRef("END");
-  
-  @Override
-  public MutableBits newLiveDocs(int size) throws IOException {
-    return new SimpleTextMutableBits(size);
-  }
-
-  @Override
-  public MutableBits newLiveDocs(Bits existing) throws IOException {
-    final SimpleTextBits bits = (SimpleTextBits) existing;
-    return new SimpleTextMutableBits((BitSet)bits.bits.clone(), bits.size);
-  }
-
-  @Override
-  public Bits readLiveDocs(Directory dir, SegmentInfoPerCommit info, IOContext context) throws IOException {
-    assert info.hasDeletions();
-    BytesRef scratch = new BytesRef();
-    CharsRef scratchUTF16 = new CharsRef();
-    
-    String fileName = IndexFileNames.fileNameFromGeneration(info.info.name, LIVEDOCS_EXTENSION, info.getDelGen());
-    IndexInput in = null;
-    boolean success = false;
-    try {
-      in = dir.openInput(fileName, context);
-      
-      SimpleTextUtil.readLine(in, scratch);
-      assert StringHelper.startsWith(scratch, SIZE);
-      int size = parseIntAt(scratch, SIZE.length, scratchUTF16);
-      
-      BitSet bits = new BitSet(size);
-      
-      SimpleTextUtil.readLine(in, scratch);
-      while (!scratch.equals(END)) {
-        assert StringHelper.startsWith(scratch, DOC);
-        int docid = parseIntAt(scratch, DOC.length, scratchUTF16);
-        bits.set(docid);
-        SimpleTextUtil.readLine(in, scratch);
-      }
-      
-      success = true;
-      return new SimpleTextBits(bits, size);
-    } finally {
-      if (success) {
-        IOUtils.close(in);
-      } else {
-        IOUtils.closeWhileHandlingException(in);
-      }
-    }
-  }
-  
-  private int parseIntAt(BytesRef bytes, int offset, CharsRef scratch) {
-    UnicodeUtil.UTF8toUTF16(bytes.bytes, bytes.offset+offset, bytes.length-offset, scratch);
-    return ArrayUtil.parseInt(scratch.chars, 0, scratch.length);
-  }
-
-  @Override
-  public void writeLiveDocs(MutableBits bits, Directory dir, SegmentInfoPerCommit info, int newDelCount, IOContext context) throws IOException {
-    BitSet set = ((SimpleTextBits) bits).bits;
-    int size = bits.length();
-    BytesRef scratch = new BytesRef();
-    
-    String fileName = IndexFileNames.fileNameFromGeneration(info.info.name, LIVEDOCS_EXTENSION, info.getNextDelGen());
-    IndexOutput out = null;
-    boolean success = false;
-    try {
-      out = dir.createOutput(fileName, context);
-      SimpleTextUtil.write(out, SIZE);
-      SimpleTextUtil.write(out, Integer.toString(size), scratch);
-      SimpleTextUtil.writeNewline(out);
-      
-      for (int i = set.nextSetBit(0); i >= 0; i=set.nextSetBit(i + 1)) { 
-        SimpleTextUtil.write(out, DOC);
-        SimpleTextUtil.write(out, Integer.toString(i), scratch);
-        SimpleTextUtil.writeNewline(out);
-      }
-      
-      SimpleTextUtil.write(out, END);
-      SimpleTextUtil.writeNewline(out);
-      success = true;
-    } finally {
-      if (success) {
-        IOUtils.close(out);
-      } else {
-        IOUtils.closeWhileHandlingException(out);
-      }
-    }
-  }
-
-  @Override
-  public void files(SegmentInfoPerCommit info, Collection<String> files) throws IOException {
-    if (info.hasDeletions()) {
-      files.add(IndexFileNames.fileNameFromGeneration(info.info.name, LIVEDOCS_EXTENSION, info.getDelGen()));
-    }
-  }
-  
-  // read-only
-  static class SimpleTextBits implements Bits {
-    final BitSet bits;
-    final int size;
-    
-    SimpleTextBits(BitSet bits, int size) {
-      this.bits = bits;
-      this.size = size;
-    }
-    
-    @Override
-    public boolean get(int index) {
-      return bits.get(index);
-    }
-
-    @Override
-    public int length() {
-      return size;
-    }
-  }
-  
-  // read-write
-  static class SimpleTextMutableBits extends SimpleTextBits implements MutableBits {
-
-    SimpleTextMutableBits(int size) {
-      this(new BitSet(size), size);
-      bits.set(0, size);
-    }
-    
-    SimpleTextMutableBits(BitSet bits, int size) {
-      super(bits, size);
-    }
-    
-    @Override
-    public void clear(int bit) {
-      bits.clear(bit);
-    }
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextNormsFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextNormsFormat.java
deleted file mode 100644
index 5ba5f74..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextNormsFormat.java
+++ /dev/null
@@ -1,124 +0,0 @@
-package org.apache.lucene.codecs.simpletext;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Comparator;
-
-import org.apache.lucene.codecs.NormsFormat;
-import org.apache.lucene.codecs.PerDocConsumer;
-import org.apache.lucene.codecs.PerDocProducer;
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.DocValues.Type;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.PerDocWriteState;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.util.BytesRef;
-
-/**
- * plain-text norms format.
- * <p>
- * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
- * 
- * @lucene.experimental
- */
-public class SimpleTextNormsFormat extends NormsFormat {
-  private static final String NORMS_SEG_SUFFIX = "len";
-  
-  @Override
-  public PerDocConsumer docsConsumer(PerDocWriteState state) throws IOException {
-    return new SimpleTextNormsPerDocConsumer(state);
-  }
-  
-  @Override
-  public PerDocProducer docsProducer(SegmentReadState state) throws IOException {
-    return new SimpleTextNormsPerDocProducer(state,
-        BytesRef.getUTF8SortedAsUnicodeComparator());
-  }
-  
-  /**
-   * Reads plain-text norms.
-   * <p>
-   * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
-   * 
-   * @lucene.experimental
-   */
-  public static class SimpleTextNormsPerDocProducer extends
-      SimpleTextPerDocProducer {
-    
-    public SimpleTextNormsPerDocProducer(SegmentReadState state,
-        Comparator<BytesRef> comp) throws IOException {
-      super(state, comp, NORMS_SEG_SUFFIX);
-    }
-    
-    @Override
-    protected boolean canLoad(FieldInfo info) {
-      return info.hasNorms();
-    }
-    
-    @Override
-    protected Type getDocValuesType(FieldInfo info) {
-      return info.getNormType();
-    }
-    
-    @Override
-    protected boolean anyDocValuesFields(FieldInfos infos) {
-      return infos.hasNorms();
-    }
-    
-  }
-  
-  /**
-   * Writes plain-text norms.
-   * <p>
-   * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
-   * 
-   * @lucene.experimental
-   */
-  public static class SimpleTextNormsPerDocConsumer extends
-      SimpleTextPerDocConsumer {
-    
-    public SimpleTextNormsPerDocConsumer(PerDocWriteState state) {
-      super(state, NORMS_SEG_SUFFIX);
-    }
-    
-    @Override
-    protected DocValues getDocValuesForMerge(AtomicReader reader, FieldInfo info)
-        throws IOException {
-      return reader.normValues(info.name);
-    }
-    
-    @Override
-    protected boolean canMerge(FieldInfo info) {
-      return info.hasNorms();
-    }
-    
-    @Override
-    protected Type getDocValuesType(FieldInfo info) {
-      return info.getNormType();
-    }
-    
-    @Override
-    public void abort() {
-      // We don't have to remove files here: IndexFileDeleter
-      // will do so
-    }
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPerDocConsumer.java b/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPerDocConsumer.java
deleted file mode 100644
index d95e09b..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPerDocConsumer.java
+++ /dev/null
@@ -1,61 +0,0 @@
-package org.apache.lucene.codecs.simpletext;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to You under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.DocValuesConsumer;
-import org.apache.lucene.codecs.PerDocConsumer;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.PerDocWriteState;
-import org.apache.lucene.index.DocValues.Type;
-
-/**
- * @lucene.experimental
- */
-class SimpleTextPerDocConsumer extends PerDocConsumer {
-
-  protected final PerDocWriteState state;
-  protected final String segmentSuffix;
-  public SimpleTextPerDocConsumer(PerDocWriteState state, String segmentSuffix) {
-    this.state = state;
-    this.segmentSuffix = segmentSuffix;
-  }
-
-  @Override
-  public void close() throws IOException {
-
-  }
-
-  @Override
-  public DocValuesConsumer addValuesField(Type type, FieldInfo field)
-      throws IOException {
-    return new SimpleTextDocValuesConsumer(SimpleTextDocValuesFormat.docValuesId(state.segmentInfo.name,
-        field.number), state.directory, state.context, type, segmentSuffix);
-  }
-
-  @Override
-  public void abort() {
-    // We don't have to remove files here: IndexFileDeleter
-    // will do so
-  }
-  
-  static String docValuesId(String segmentsName, int fieldId) {
-    return segmentsName + "_" + fieldId;
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPerDocProducer.java b/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPerDocProducer.java
deleted file mode 100644
index 639bc54..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPerDocProducer.java
+++ /dev/null
@@ -1,435 +0,0 @@
-package org.apache.lucene.codecs.simpletext;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to You under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-import static org.apache.lucene.codecs.simpletext.SimpleTextDocValuesConsumer.DOC;
-import static org.apache.lucene.codecs.simpletext.SimpleTextDocValuesConsumer.END;
-import static org.apache.lucene.codecs.simpletext.SimpleTextDocValuesConsumer.HEADER;
-import static org.apache.lucene.codecs.simpletext.SimpleTextDocValuesConsumer.VALUE;
-import static org.apache.lucene.codecs.simpletext.SimpleTextDocValuesConsumer.VALUE_SIZE;
-
-import java.io.Closeable;
-import java.io.IOException;
-import java.util.Collection;
-import java.util.Comparator;
-import java.util.Map;
-import java.util.TreeMap;
-
-import org.apache.lucene.codecs.DocValuesArraySource;
-import org.apache.lucene.codecs.PerDocProducerBase;
-import org.apache.lucene.index.DocValues;
-import org.apache.lucene.index.DocValues.SortedSource;
-import org.apache.lucene.index.DocValues.Source;
-import org.apache.lucene.index.DocValues.Type;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.BytesRefHash;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.StringHelper;
-import org.apache.lucene.util.packed.PackedInts.Reader;
-
-/**
- * Reads plain-text DocValues.
- * <p>
- * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
- * 
- * @lucene.experimental
- */
-public class SimpleTextPerDocProducer extends PerDocProducerBase {
-  protected final TreeMap<String, DocValues> docValues;
-  private Comparator<BytesRef> comp;
-  private final String segmentSuffix;
-
-  /**
-   * Creates a new {@link SimpleTextPerDocProducer} instance and loads all
-   * {@link DocValues} instances for this segment and codec.
-   */
-  public SimpleTextPerDocProducer(SegmentReadState state,
-      Comparator<BytesRef> comp, String segmentSuffix) throws IOException {
-    this.comp = comp;
-    this.segmentSuffix = segmentSuffix;
-    if (anyDocValuesFields(state.fieldInfos)) {
-      docValues = load(state.fieldInfos, state.segmentInfo.name,
-                       state.segmentInfo.getDocCount(), state.dir, state.context);
-    } else {
-      docValues = new TreeMap<String, DocValues>();
-    }
-  }
-
-  @Override
-  protected Map<String, DocValues> docValues() {
-    return docValues;
-  }
-
-  protected DocValues loadDocValues(int docCount, Directory dir, String id,
-      DocValues.Type type, IOContext context) throws IOException {
-    return new SimpleTextDocValues(dir, context, type, id, docCount, comp, segmentSuffix);
-  }
-
-  @Override
-  protected void closeInternal(Collection<? extends Closeable> closeables)
-      throws IOException {
-    IOUtils.close(closeables);
-  }
-
-  private static class SimpleTextDocValues extends DocValues {
-
-    private int docCount;
-
-    @Override
-    public void close() throws IOException {
-      try {
-        super.close();
-      } finally {
-        IOUtils.close(input);
-      }
-    }
-
-    private Type type;
-    private Comparator<BytesRef> comp;
-    private int valueSize;
-    private final IndexInput input;
-
-    public SimpleTextDocValues(Directory dir, IOContext ctx, Type type,
-        String id, int docCount, Comparator<BytesRef> comp, String segmentSuffix) throws IOException {
-      this.type = type;
-      this.docCount = docCount;
-      this.comp = comp;
-      final String fileName = IndexFileNames.segmentFileName(id, "", segmentSuffix);
-      boolean success = false;
-      IndexInput in = null;
-      try {
-        in = dir.openInput(fileName, ctx);
-        valueSize = readHeader(in);
-        success = true;
-      } finally {
-        if (!success) {
-          IOUtils.closeWhileHandlingException(in);
-        }
-      }
-      input = in;
-
-    }
-
-    @Override
-    public Source load() throws IOException {
-      boolean success = false;
-      IndexInput in = input.clone();
-      try {
-        Source source = null;
-        switch (type) {
-        case BYTES_FIXED_DEREF:
-        case BYTES_FIXED_SORTED:
-        case BYTES_FIXED_STRAIGHT:
-        case BYTES_VAR_DEREF:
-        case BYTES_VAR_SORTED:
-        case BYTES_VAR_STRAIGHT:
-          source = read(in, new ValueReader(type, docCount, comp));
-          break;
-        case FIXED_INTS_16:
-        case FIXED_INTS_32:
-        case VAR_INTS:
-        case FIXED_INTS_64:
-        case FIXED_INTS_8:
-        case FLOAT_32:
-        case FLOAT_64:
-          source = read(in, new ValueReader(type, docCount, null));
-          break;
-        default:
-          throw new IllegalArgumentException("unknown type: " + type);
-        }
-        assert source != null;
-        success = true;
-        return source;
-      } finally {
-        if (!success) {
-          IOUtils.closeWhileHandlingException(in);
-        } else {
-          IOUtils.close(in);
-        }
-      }
-    }
-
-    private int readHeader(IndexInput in) throws IOException {
-      BytesRef scratch = new BytesRef();
-      SimpleTextUtil.readLine(in, scratch);
-      assert StringHelper.startsWith(scratch, HEADER);
-      SimpleTextUtil.readLine(in, scratch);
-      assert StringHelper.startsWith(scratch, VALUE_SIZE);
-      return Integer.parseInt(readString(scratch.offset + VALUE_SIZE.length,
-          scratch));
-    }
-
-    private Source read(IndexInput in, ValueReader reader) throws IOException {
-      BytesRef scratch = new BytesRef();
-      for (int i = 0; i < docCount; i++) {
-        SimpleTextUtil.readLine(in, scratch);
-
-        assert StringHelper.startsWith(scratch, DOC) : scratch.utf8ToString();
-        SimpleTextUtil.readLine(in, scratch);
-        assert StringHelper.startsWith(scratch, VALUE);
-        reader.fromString(i, scratch, scratch.offset + VALUE.length);
-      }
-      SimpleTextUtil.readLine(in, scratch);
-      assert scratch.equals(END);
-      return reader.getSource();
-    }
-
-    @Override
-    public Source getDirectSource() throws IOException {
-      return this.getSource();
-    }
-
-    @Override
-    public int getValueSize() {
-      return valueSize;
-    }
-
-    @Override
-    public Type getType() {
-      return type;
-    }
-
-  }
-
-  public static String readString(int offset, BytesRef scratch) {
-    return new String(scratch.bytes, scratch.offset + offset, scratch.length
-        - offset, IOUtils.CHARSET_UTF_8);
-  }
-
-  private static final class ValueReader {
-    private final Type type;
-    private byte[] bytes;
-    private short[] shorts;
-    private int[] ints;
-    private long[] longs;
-    private float[] floats;
-    private double[] doubles;
-    private Source source;
-    private BytesRefHash hash;
-    private BytesRef scratch;
-
-    public ValueReader(Type type, int maxDocs, Comparator<BytesRef> comp) {
-      super();
-      this.type = type;
-      Source docValuesArray = null;
-      switch (type) {
-      case FIXED_INTS_16:
-        shorts = new short[maxDocs];
-        docValuesArray = DocValuesArraySource.forType(type)
-            .newFromArray(shorts);
-        break;
-      case FIXED_INTS_32:
-        ints = new int[maxDocs];
-        docValuesArray = DocValuesArraySource.forType(type).newFromArray(ints);
-        break;
-      case FIXED_INTS_64:
-        longs = new long[maxDocs];
-        docValuesArray = DocValuesArraySource.forType(type)
-            .newFromArray(longs);
-        break;
-      case VAR_INTS:
-        longs = new long[maxDocs];
-        docValuesArray = new VarIntsArraySource(type, longs);
-        break;
-      case FIXED_INTS_8:
-        bytes = new byte[maxDocs];
-        docValuesArray = DocValuesArraySource.forType(type).newFromArray(bytes);
-        break;
-      case FLOAT_32:
-        floats = new float[maxDocs];
-        docValuesArray = DocValuesArraySource.forType(type)
-            .newFromArray(floats);
-        break;
-      case FLOAT_64:
-        doubles = new double[maxDocs];
-        docValuesArray = DocValuesArraySource.forType(type).newFromArray(
-            doubles);
-        break;
-      case BYTES_FIXED_DEREF:
-      case BYTES_FIXED_SORTED:
-      case BYTES_FIXED_STRAIGHT:
-      case BYTES_VAR_DEREF:
-      case BYTES_VAR_SORTED:
-      case BYTES_VAR_STRAIGHT:
-        assert comp != null;
-        hash = new BytesRefHash();
-        BytesSource bytesSource = new BytesSource(type, comp, maxDocs, hash);
-        ints = bytesSource.docIdToEntry;
-        source = bytesSource;
-        scratch = new BytesRef();
-        break;
-
-      }
-      if (docValuesArray != null) {
-        assert source == null;
-        this.source = docValuesArray;
-      }
-    }
-
-    public void fromString(int ord, BytesRef ref, int offset) {
-      switch (type) {
-      case FIXED_INTS_16:
-        assert shorts != null;
-        shorts[ord] = Short.parseShort(readString(offset, ref));
-        break;
-      case FIXED_INTS_32:
-        assert ints != null;
-        ints[ord] = Integer.parseInt(readString(offset, ref));
-        break;
-      case FIXED_INTS_64:
-      case VAR_INTS:
-        assert longs != null;
-        longs[ord] = Long.parseLong(readString(offset, ref));
-        break;
-      case FIXED_INTS_8:
-        assert bytes != null;
-        bytes[ord] = (byte) Integer.parseInt(readString(offset, ref));
-        break;
-      case FLOAT_32:
-        assert floats != null;
-        floats[ord] = Float.parseFloat(readString(offset, ref));
-        break;
-      case FLOAT_64:
-        assert doubles != null;
-        doubles[ord] = Double.parseDouble(readString(offset, ref));
-        break;
-      case BYTES_FIXED_DEREF:
-      case BYTES_FIXED_SORTED:
-      case BYTES_FIXED_STRAIGHT:
-      case BYTES_VAR_DEREF:
-      case BYTES_VAR_SORTED:
-      case BYTES_VAR_STRAIGHT:
-        scratch.bytes = ref.bytes;
-        scratch.length = ref.length - offset;
-        scratch.offset = ref.offset + offset;
-        int key = hash.add(scratch);
-        ints[ord] = key < 0 ? (-key) - 1 : key;
-        break;
-      }
-    }
-
-    public Source getSource() {
-      if (source instanceof BytesSource) {
-        ((BytesSource) source).maybeSort();
-      }
-      return source;
-    }
-  }
-
-  private static final class BytesSource extends SortedSource {
-
-    private final BytesRefHash hash;
-    int[] docIdToEntry;
-    int[] sortedEntries;
-    int[] adresses;
-    private final boolean isSorted;
-
-    protected BytesSource(Type type, Comparator<BytesRef> comp, int maxDoc,
-        BytesRefHash hash) {
-      super(type, comp);
-      docIdToEntry = new int[maxDoc];
-      this.hash = hash;
-      isSorted = type == Type.BYTES_FIXED_SORTED
-          || type == Type.BYTES_VAR_SORTED;
-    }
-
-    void maybeSort() {
-      if (isSorted) {
-        adresses = new int[hash.size()];
-        sortedEntries = hash.sort(getComparator());
-        for (int i = 0; i < adresses.length; i++) {
-          int entry = sortedEntries[i];
-          adresses[entry] = i;
-        }
-      }
-
-    }
-
-    @Override
-    public BytesRef getBytes(int docID, BytesRef ref) {
-      if (isSorted) {
-        return hash.get(sortedEntries[ord(docID)], ref);
-      } else {
-        return hash.get(docIdToEntry[docID], ref);
-      }
-    }
-
-    @Override
-    public SortedSource asSortedSource() {
-      if (isSorted) {
-        return this;
-      }
-      return null;
-    }
-
-    @Override
-    public int ord(int docID) {
-      assert isSorted;
-      try {
-        return adresses[docIdToEntry[docID]];
-      } catch (Exception e) {
-
-        return 0;
-      }
-    }
-
-    @Override
-    public BytesRef getByOrd(int ord, BytesRef bytesRef) {
-      assert isSorted;
-      return hash.get(sortedEntries[ord], bytesRef);
-    }
-
-    @Override
-    public Reader getDocToOrd() {
-      return null;
-    }
-
-    @Override
-    public int getValueCount() {
-      return hash.size();
-    }
-
-  }
-  
-  private static class VarIntsArraySource extends Source {
-
-    private final long[] array;
-
-    protected VarIntsArraySource(Type type, long[] array) {
-      super(type);
-      this.array = array;
-    }
-
-    @Override
-    public long getInt(int docID) {
-      return array[docID];
-    }
-
-    @Override
-    public BytesRef getBytes(int docID, BytesRef ref) {
-      DocValuesArraySource.copyLong(ref, getInt(docID));
-      return ref;
-    }
-    
-  }
-
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPostingsFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPostingsFormat.java
deleted file mode 100644
index 4f7cfe0..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextPostingsFormat.java
+++ /dev/null
@@ -1,59 +0,0 @@
-package org.apache.lucene.codecs.simpletext;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.FieldsConsumer;
-import org.apache.lucene.codecs.FieldsProducer;
-import org.apache.lucene.codecs.PostingsFormat;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.index.SegmentReadState;
-import org.apache.lucene.index.IndexFileNames;
-
-/** For debugging, curiosity, transparency only!!  Do not
- *  use this codec in production.
- *
- *  <p>This codec stores all postings data in a single
- *  human-readable text file (_N.pst).  You can view this in
- *  any text editor, and even edit it to alter your index.
- *
- *  @lucene.experimental */
-public class SimpleTextPostingsFormat extends PostingsFormat {
-  
-  public SimpleTextPostingsFormat() {
-    super("SimpleText");
-  }
-
-  @Override
-  public FieldsConsumer fieldsConsumer(SegmentWriteState state) throws IOException {
-    return new SimpleTextFieldsWriter(state);
-  }
-
-  @Override
-  public FieldsProducer fieldsProducer(SegmentReadState state) throws IOException {
-    return new SimpleTextFieldsReader(state);
-  }
-
-  /** Extension of freq postings file */
-  static final String POSTINGS_EXTENSION = "pst";
-
-  static String getPostingsFileName(String segment, String segmentSuffix) {
-    return IndexFileNames.segmentFileName(segment, segmentSuffix, POSTINGS_EXTENSION);
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextSegmentInfoFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextSegmentInfoFormat.java
deleted file mode 100644
index 53440e7..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextSegmentInfoFormat.java
+++ /dev/null
@@ -1,45 +0,0 @@
-package org.apache.lucene.codecs.simpletext;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.codecs.SegmentInfoFormat;
-import org.apache.lucene.codecs.SegmentInfoReader;
-import org.apache.lucene.codecs.SegmentInfoWriter;
-
-/**
- * plain text segments file format.
- * <p>
- * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
- * @lucene.experimental
- */
-public class SimpleTextSegmentInfoFormat extends SegmentInfoFormat {
-  private final SegmentInfoReader reader = new SimpleTextSegmentInfoReader();
-  private final SegmentInfoWriter writer = new SimpleTextSegmentInfoWriter();
-
-  public static final String SI_EXTENSION = "si";
-  
-  @Override
-  public SegmentInfoReader getSegmentInfoReader() {
-    return reader;
-  }
-
-  @Override
-  public SegmentInfoWriter getSegmentInfoWriter() {
-    return writer;
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextSegmentInfoReader.java b/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextSegmentInfoReader.java
deleted file mode 100644
index c020f14..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextSegmentInfoReader.java
+++ /dev/null
@@ -1,127 +0,0 @@
-package org.apache.lucene.codecs.simpletext;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.Map;
-import java.util.Set;
-
-import org.apache.lucene.codecs.SegmentInfoReader;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.StringHelper;
-
-import static org.apache.lucene.codecs.simpletext.SimpleTextSegmentInfoWriter.*;
-
-/**
- * reads plaintext segments files
- * <p>
- * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
- * @lucene.experimental
- */
-public class SimpleTextSegmentInfoReader extends SegmentInfoReader {
-
-  @Override
-  public SegmentInfo read(Directory directory, String segmentName, IOContext context) throws IOException {
-    BytesRef scratch = new BytesRef();
-    String segFileName = IndexFileNames.segmentFileName(segmentName, "", SimpleTextSegmentInfoFormat.SI_EXTENSION);
-    IndexInput input = directory.openInput(segFileName, context);
-    boolean success = false;
-    try {
-      SimpleTextUtil.readLine(input, scratch);
-      assert StringHelper.startsWith(scratch, SI_VERSION);
-      final String version = readString(SI_VERSION.length, scratch);
-    
-      SimpleTextUtil.readLine(input, scratch);
-      assert StringHelper.startsWith(scratch, SI_DOCCOUNT);
-      final int docCount = Integer.parseInt(readString(SI_DOCCOUNT.length, scratch));
-    
-      SimpleTextUtil.readLine(input, scratch);
-      assert StringHelper.startsWith(scratch, SI_USECOMPOUND);
-      final boolean isCompoundFile = Boolean.parseBoolean(readString(SI_USECOMPOUND.length, scratch));
-    
-      SimpleTextUtil.readLine(input, scratch);
-      assert StringHelper.startsWith(scratch, SI_NUM_DIAG);
-      int numDiag = Integer.parseInt(readString(SI_NUM_DIAG.length, scratch));
-      Map<String,String> diagnostics = new HashMap<String,String>();
-
-      for (int i = 0; i < numDiag; i++) {
-        SimpleTextUtil.readLine(input, scratch);
-        assert StringHelper.startsWith(scratch, SI_DIAG_KEY);
-        String key = readString(SI_DIAG_KEY.length, scratch);
-      
-        SimpleTextUtil.readLine(input, scratch);
-        assert StringHelper.startsWith(scratch, SI_DIAG_VALUE);
-        String value = readString(SI_DIAG_VALUE.length, scratch);
-        diagnostics.put(key, value);
-      }
-      
-      SimpleTextUtil.readLine(input, scratch);
-      assert StringHelper.startsWith(scratch, SI_NUM_ATTS);
-      int numAtts = Integer.parseInt(readString(SI_NUM_ATTS.length, scratch));
-      Map<String,String> attributes = new HashMap<String,String>();
-
-      for (int i = 0; i < numAtts; i++) {
-        SimpleTextUtil.readLine(input, scratch);
-        assert StringHelper.startsWith(scratch, SI_ATT_KEY);
-        String key = readString(SI_ATT_KEY.length, scratch);
-      
-        SimpleTextUtil.readLine(input, scratch);
-        assert StringHelper.startsWith(scratch, SI_ATT_VALUE);
-        String value = readString(SI_ATT_VALUE.length, scratch);
-        attributes.put(key, value);
-      }
-
-      SimpleTextUtil.readLine(input, scratch);
-      assert StringHelper.startsWith(scratch, SI_NUM_FILES);
-      int numFiles = Integer.parseInt(readString(SI_NUM_FILES.length, scratch));
-      Set<String> files = new HashSet<String>();
-
-      for (int i = 0; i < numFiles; i++) {
-        SimpleTextUtil.readLine(input, scratch);
-        assert StringHelper.startsWith(scratch, SI_FILE);
-        String fileName = readString(SI_FILE.length, scratch);
-        files.add(fileName);
-      }
-
-      SegmentInfo info = new SegmentInfo(directory, version, segmentName, docCount, 
-                                         isCompoundFile, null, diagnostics, Collections.unmodifiableMap(attributes));
-      info.setFiles(files);
-      success = true;
-      return info;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(input);
-      } else {
-        input.close();
-      }
-    }
-  }
-
-  private String readString(int offset, BytesRef scratch) {
-    return new String(scratch.bytes, scratch.offset+offset, scratch.length-offset, IOUtils.CHARSET_UTF_8);
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextSegmentInfoWriter.java b/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextSegmentInfoWriter.java
deleted file mode 100644
index cbad776..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextSegmentInfoWriter.java
+++ /dev/null
@@ -1,136 +0,0 @@
-package org.apache.lucene.codecs.simpletext;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Map;
-import java.util.Set;
-
-import org.apache.lucene.codecs.SegmentInfoWriter;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-
-/**
- * writes plaintext segments files
- * <p>
- * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
- * @lucene.experimental
- */
-public class SimpleTextSegmentInfoWriter extends SegmentInfoWriter {
-
-  final static BytesRef SI_VERSION          = new BytesRef("    version ");
-  final static BytesRef SI_DOCCOUNT         = new BytesRef("    number of documents ");
-  final static BytesRef SI_USECOMPOUND      = new BytesRef("    uses compound file ");
-  final static BytesRef SI_NUM_DIAG         = new BytesRef("    diagnostics ");
-  final static BytesRef SI_DIAG_KEY         = new BytesRef("      key ");
-  final static BytesRef SI_DIAG_VALUE       = new BytesRef("      value ");
-  final static BytesRef SI_NUM_ATTS         = new BytesRef("    attributes ");
-  final static BytesRef SI_ATT_KEY          = new BytesRef("      key ");
-  final static BytesRef SI_ATT_VALUE        = new BytesRef("      value ");
-  final static BytesRef SI_NUM_FILES        = new BytesRef("    files ");
-  final static BytesRef SI_FILE             = new BytesRef("      file ");
-  
-  @Override
-  public void write(Directory dir, SegmentInfo si, FieldInfos fis, IOContext ioContext) throws IOException {
-
-    String segFileName = IndexFileNames.segmentFileName(si.name, "", SimpleTextSegmentInfoFormat.SI_EXTENSION);
-    si.addFile(segFileName);
-
-    boolean success = false;
-    IndexOutput output = dir.createOutput(segFileName,  ioContext);
-
-    try {
-      BytesRef scratch = new BytesRef();
-    
-      SimpleTextUtil.write(output, SI_VERSION);
-      SimpleTextUtil.write(output, si.getVersion(), scratch);
-      SimpleTextUtil.writeNewline(output);
-    
-      SimpleTextUtil.write(output, SI_DOCCOUNT);
-      SimpleTextUtil.write(output, Integer.toString(si.getDocCount()), scratch);
-      SimpleTextUtil.writeNewline(output);
-    
-      SimpleTextUtil.write(output, SI_USECOMPOUND);
-      SimpleTextUtil.write(output, Boolean.toString(si.getUseCompoundFile()), scratch);
-      SimpleTextUtil.writeNewline(output);
-    
-      Map<String,String> diagnostics = si.getDiagnostics();
-      int numDiagnostics = diagnostics == null ? 0 : diagnostics.size();
-      SimpleTextUtil.write(output, SI_NUM_DIAG);
-      SimpleTextUtil.write(output, Integer.toString(numDiagnostics), scratch);
-      SimpleTextUtil.writeNewline(output);
-    
-      if (numDiagnostics > 0) {
-        for (Map.Entry<String,String> diagEntry : diagnostics.entrySet()) {
-          SimpleTextUtil.write(output, SI_DIAG_KEY);
-          SimpleTextUtil.write(output, diagEntry.getKey(), scratch);
-          SimpleTextUtil.writeNewline(output);
-        
-          SimpleTextUtil.write(output, SI_DIAG_VALUE);
-          SimpleTextUtil.write(output, diagEntry.getValue(), scratch);
-          SimpleTextUtil.writeNewline(output);
-        }
-      }
-      
-      Map<String,String> atts = si.attributes();
-      int numAtts = atts == null ? 0 : atts.size();
-      SimpleTextUtil.write(output, SI_NUM_ATTS);
-      SimpleTextUtil.write(output, Integer.toString(numAtts), scratch);
-      SimpleTextUtil.writeNewline(output);
-    
-      if (numAtts > 0) {
-        for (Map.Entry<String,String> entry : atts.entrySet()) {
-          SimpleTextUtil.write(output, SI_ATT_KEY);
-          SimpleTextUtil.write(output, entry.getKey(), scratch);
-          SimpleTextUtil.writeNewline(output);
-        
-          SimpleTextUtil.write(output, SI_ATT_VALUE);
-          SimpleTextUtil.write(output, entry.getValue(), scratch);
-          SimpleTextUtil.writeNewline(output);
-        }
-      }
-
-      Set<String> files = si.files();
-      int numFiles = files == null ? 0 : files.size();
-      SimpleTextUtil.write(output, SI_NUM_FILES);
-      SimpleTextUtil.write(output, Integer.toString(numFiles), scratch);
-      SimpleTextUtil.writeNewline(output);
-
-      if (numFiles > 0) {
-        for(String fileName : files) {
-          SimpleTextUtil.write(output, SI_FILE);
-          SimpleTextUtil.write(output, fileName, scratch);
-          SimpleTextUtil.writeNewline(output);
-        }
-      }
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(output);
-      } else {
-        output.close();
-      }
-    }
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextStoredFieldsFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextStoredFieldsFormat.java
deleted file mode 100644
index 13c73b4..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextStoredFieldsFormat.java
+++ /dev/null
@@ -1,47 +0,0 @@
-package org.apache.lucene.codecs.simpletext;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.StoredFieldsFormat;
-import org.apache.lucene.codecs.StoredFieldsReader;
-import org.apache.lucene.codecs.StoredFieldsWriter;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-
-/**
- * plain text stored fields format.
- * <p>
- * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
- * @lucene.experimental
- */
-public class SimpleTextStoredFieldsFormat extends StoredFieldsFormat {
-
-  @Override
-  public StoredFieldsReader fieldsReader(Directory directory, SegmentInfo si, FieldInfos fn, IOContext context) throws IOException {;
-    return new SimpleTextStoredFieldsReader(directory, si, fn, context);
-  }
-
-  @Override
-  public StoredFieldsWriter fieldsWriter(Directory directory, SegmentInfo si, IOContext context) throws IOException {
-    return new SimpleTextStoredFieldsWriter(directory, si.name, context);
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextStoredFieldsReader.java b/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextStoredFieldsReader.java
deleted file mode 100644
index 3cbbb89..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextStoredFieldsReader.java
+++ /dev/null
@@ -1,192 +0,0 @@
-package org.apache.lucene.codecs.simpletext;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.ArrayList;
-
-import org.apache.lucene.codecs.StoredFieldsReader;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.StoredFieldVisitor;
-import org.apache.lucene.store.AlreadyClosedException;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.CharsRef;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.StringHelper;
-import org.apache.lucene.util.UnicodeUtil;
-
-import static org.apache.lucene.codecs.simpletext.SimpleTextStoredFieldsWriter.*;
-
-/**
- * reads plaintext stored fields
- * <p>
- * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
- * @lucene.experimental
- */
-public class SimpleTextStoredFieldsReader extends StoredFieldsReader {
-  private ArrayList<Long> offsets; /* docid -> offset in .fld file */
-  private IndexInput in;
-  private BytesRef scratch = new BytesRef();
-  private CharsRef scratchUTF16 = new CharsRef();
-  private final FieldInfos fieldInfos;
-
-  public SimpleTextStoredFieldsReader(Directory directory, SegmentInfo si, FieldInfos fn, IOContext context) throws IOException {
-    this.fieldInfos = fn;
-    boolean success = false;
-    try {
-      in = directory.openInput(IndexFileNames.segmentFileName(si.name, "", SimpleTextStoredFieldsWriter.FIELDS_EXTENSION), context);
-      success = true;
-    } finally {
-      if (!success) {
-        close();
-      }
-    }
-    readIndex();
-  }
-  
-  // used by clone
-  SimpleTextStoredFieldsReader(ArrayList<Long> offsets, IndexInput in, FieldInfos fieldInfos) {
-    this.offsets = offsets;
-    this.in = in;
-    this.fieldInfos = fieldInfos;
-  }
-  
-  // we don't actually write a .fdx-like index, instead we read the 
-  // stored fields file in entirety up-front and save the offsets 
-  // so we can seek to the documents later.
-  private void readIndex() throws IOException {
-    offsets = new ArrayList<Long>();
-    while (!scratch.equals(END)) {
-      readLine();
-      if (StringHelper.startsWith(scratch, DOC)) {
-        offsets.add(in.getFilePointer());
-      }
-    }
-  }
-  
-  @Override
-  public void visitDocument(int n, StoredFieldVisitor visitor) throws IOException {
-    in.seek(offsets.get(n));
-    readLine();
-    assert StringHelper.startsWith(scratch, NUM);
-    int numFields = parseIntAt(NUM.length);
-    
-    for (int i = 0; i < numFields; i++) {
-      readLine();
-      assert StringHelper.startsWith(scratch, FIELD);
-      int fieldNumber = parseIntAt(FIELD.length);
-      FieldInfo fieldInfo = fieldInfos.fieldInfo(fieldNumber);
-      readLine();
-      assert StringHelper.startsWith(scratch, NAME);
-      readLine();
-      assert StringHelper.startsWith(scratch, TYPE);
-      
-      final BytesRef type;
-      if (equalsAt(TYPE_STRING, scratch, TYPE.length)) {
-        type = TYPE_STRING;
-      } else if (equalsAt(TYPE_BINARY, scratch, TYPE.length)) {
-        type = TYPE_BINARY;
-      } else if (equalsAt(TYPE_INT, scratch, TYPE.length)) {
-        type = TYPE_INT;
-      } else if (equalsAt(TYPE_LONG, scratch, TYPE.length)) {
-        type = TYPE_LONG;
-      } else if (equalsAt(TYPE_FLOAT, scratch, TYPE.length)) {
-        type = TYPE_FLOAT;
-      } else if (equalsAt(TYPE_DOUBLE, scratch, TYPE.length)) {
-        type = TYPE_DOUBLE;
-      } else {
-        throw new RuntimeException("unknown field type");
-      }
-      
-      switch (visitor.needsField(fieldInfo)) {
-        case YES:  
-          readField(type, fieldInfo, visitor);
-          break;
-        case NO:   
-          readLine();
-          assert StringHelper.startsWith(scratch, VALUE);
-          break;
-        case STOP: return;
-      }
-    }
-  }
-  
-  private void readField(BytesRef type, FieldInfo fieldInfo, StoredFieldVisitor visitor) throws IOException {
-    readLine();
-    assert StringHelper.startsWith(scratch, VALUE);
-    if (type == TYPE_STRING) {
-      visitor.stringField(fieldInfo, new String(scratch.bytes, scratch.offset+VALUE.length, scratch.length-VALUE.length, "UTF-8"));
-    } else if (type == TYPE_BINARY) {
-      // TODO: who owns the bytes?
-      byte[] copy = new byte[scratch.length-VALUE.length];
-      System.arraycopy(scratch.bytes, scratch.offset+VALUE.length, copy, 0, copy.length);
-      visitor.binaryField(fieldInfo, copy, 0, copy.length);
-    } else if (type == TYPE_INT) {
-      UnicodeUtil.UTF8toUTF16(scratch.bytes, scratch.offset+VALUE.length, scratch.length-VALUE.length, scratchUTF16);
-      visitor.intField(fieldInfo, Integer.parseInt(scratchUTF16.toString()));
-    } else if (type == TYPE_LONG) {
-      UnicodeUtil.UTF8toUTF16(scratch.bytes, scratch.offset+VALUE.length, scratch.length-VALUE.length, scratchUTF16);
-      visitor.longField(fieldInfo, Long.parseLong(scratchUTF16.toString()));
-    } else if (type == TYPE_FLOAT) {
-      UnicodeUtil.UTF8toUTF16(scratch.bytes, scratch.offset+VALUE.length, scratch.length-VALUE.length, scratchUTF16);
-      visitor.floatField(fieldInfo, Float.parseFloat(scratchUTF16.toString()));
-    } else if (type == TYPE_DOUBLE) {
-      UnicodeUtil.UTF8toUTF16(scratch.bytes, scratch.offset+VALUE.length, scratch.length-VALUE.length, scratchUTF16);
-      visitor.doubleField(fieldInfo, Double.parseDouble(scratchUTF16.toString()));
-    }
-  }
-
-  @Override
-  public StoredFieldsReader clone() {
-    if (in == null) {
-      throw new AlreadyClosedException("this FieldsReader is closed");
-    }
-    return new SimpleTextStoredFieldsReader(offsets, in.clone(), fieldInfos);
-  }
-  
-  @Override
-  public void close() throws IOException {
-    try {
-      IOUtils.close(in); 
-    } finally {
-      in = null;
-      offsets = null;
-    }
-  }
-  
-  private void readLine() throws IOException {
-    SimpleTextUtil.readLine(in, scratch);
-  }
-  
-  private int parseIntAt(int offset) {
-    UnicodeUtil.UTF8toUTF16(scratch.bytes, scratch.offset+offset, scratch.length-offset, scratchUTF16);
-    return ArrayUtil.parseInt(scratchUTF16.chars, 0, scratchUTF16.length);
-  }
-  
-  private boolean equalsAt(BytesRef a, BytesRef b, int bOffset) {
-    return a.length == b.length - bOffset && 
-        ArrayUtil.equals(a.bytes, a.offset, b.bytes, b.offset + bOffset, b.length - bOffset);
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextStoredFieldsWriter.java b/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextStoredFieldsWriter.java
deleted file mode 100644
index 6a9b223..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextStoredFieldsWriter.java
+++ /dev/null
@@ -1,196 +0,0 @@
-package org.apache.lucene.codecs.simpletext;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.StoredFieldsWriter;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.StorableField;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-
-/**
- * Writes plain-text stored fields.
- * <p>
- * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
- * @lucene.experimental
- */
-public class SimpleTextStoredFieldsWriter extends StoredFieldsWriter {
-  private int numDocsWritten = 0;
-  private final Directory directory;
-  private final String segment;
-  private IndexOutput out;
-  
-  final static String FIELDS_EXTENSION = "fld";
-  
-  final static BytesRef TYPE_STRING = new BytesRef("string");
-  final static BytesRef TYPE_BINARY = new BytesRef("binary");
-  final static BytesRef TYPE_INT    = new BytesRef("int");
-  final static BytesRef TYPE_LONG   = new BytesRef("long");
-  final static BytesRef TYPE_FLOAT  = new BytesRef("float");
-  final static BytesRef TYPE_DOUBLE = new BytesRef("double");
-
-  final static BytesRef END     = new BytesRef("END");
-  final static BytesRef DOC     = new BytesRef("doc ");
-  final static BytesRef NUM     = new BytesRef("  numfields ");
-  final static BytesRef FIELD   = new BytesRef("  field ");
-  final static BytesRef NAME    = new BytesRef("    name ");
-  final static BytesRef TYPE    = new BytesRef("    type ");
-  final static BytesRef VALUE   = new BytesRef("    value ");
-  
-  private final BytesRef scratch = new BytesRef();
-  
-  public SimpleTextStoredFieldsWriter(Directory directory, String segment, IOContext context) throws IOException {
-    this.directory = directory;
-    this.segment = segment;
-    boolean success = false;
-    try {
-      out = directory.createOutput(IndexFileNames.segmentFileName(segment, "", FIELDS_EXTENSION), context);
-      success = true;
-    } finally {
-      if (!success) {
-        abort();
-      }
-    }
-  }
-
-  @Override
-  public void startDocument(int numStoredFields) throws IOException {
-    write(DOC);
-    write(Integer.toString(numDocsWritten));
-    newLine();
-    
-    write(NUM);
-    write(Integer.toString(numStoredFields));
-    newLine();
-    
-    numDocsWritten++;
-  }
-
-  @Override
-  public void writeField(FieldInfo info, StorableField field) throws IOException {
-    write(FIELD);
-    write(Integer.toString(info.number));
-    newLine();
-    
-    write(NAME);
-    write(field.name());
-    newLine();
-    
-    write(TYPE);
-    final Number n = field.numericValue();
-
-    if (n != null) {
-      if (n instanceof Byte || n instanceof Short || n instanceof Integer) {
-        write(TYPE_INT);
-        newLine();
-          
-        write(VALUE);
-        write(Integer.toString(n.intValue()));
-        newLine();
-      } else if (n instanceof Long) {
-        write(TYPE_LONG);
-        newLine();
-
-        write(VALUE);
-        write(Long.toString(n.longValue()));
-        newLine();
-      } else if (n instanceof Float) {
-        write(TYPE_FLOAT);
-        newLine();
-          
-        write(VALUE);
-        write(Float.toString(n.floatValue()));
-        newLine();
-      } else if (n instanceof Double) {
-        write(TYPE_DOUBLE);
-        newLine();
-          
-        write(VALUE);
-        write(Double.toString(n.doubleValue()));
-        newLine();
-      } else {
-        throw new IllegalArgumentException("cannot store numeric type " + n.getClass());
-      }
-    } else { 
-      BytesRef bytes = field.binaryValue();
-      if (bytes != null) {
-        write(TYPE_BINARY);
-        newLine();
-        
-        write(VALUE);
-        write(bytes);
-        newLine();
-      } else if (field.stringValue() == null) {
-        throw new IllegalArgumentException("field " + field.name() + " is stored but does not have binaryValue, stringValue nor numericValue");
-      } else {
-        write(TYPE_STRING);
-        newLine();
-        
-        write(VALUE);
-        write(field.stringValue());
-        newLine();
-      }
-    }
-  }
-
-  @Override
-  public void abort() {
-    try {
-      close();
-    } catch (IOException ignored) {}
-    IOUtils.deleteFilesIgnoringExceptions(directory, IndexFileNames.segmentFileName(segment, "", FIELDS_EXTENSION));
-  }
-
-  @Override
-  public void finish(FieldInfos fis, int numDocs) throws IOException {
-    if (numDocsWritten != numDocs) {
-      throw new RuntimeException("mergeFields produced an invalid result: docCount is " + numDocs 
-          + " but only saw " + numDocsWritten + " file=" + out.toString() + "; now aborting this merge to prevent index corruption");
-    }
-    write(END);
-    newLine();
-  }
-
-  @Override
-  public void close() throws IOException {
-    try {
-      IOUtils.close(out);
-    } finally {
-      out = null;
-    }
-  }
-  
-  private void write(String s) throws IOException {
-    SimpleTextUtil.write(out, s, scratch);
-  }
-  
-  private void write(BytesRef bytes) throws IOException {
-    SimpleTextUtil.write(out, bytes);
-  }
-  
-  private void newLine() throws IOException {
-    SimpleTextUtil.writeNewline(out);
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextTermVectorsFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextTermVectorsFormat.java
deleted file mode 100644
index a0fe06d..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextTermVectorsFormat.java
+++ /dev/null
@@ -1,47 +0,0 @@
-package org.apache.lucene.codecs.simpletext;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.codecs.TermVectorsFormat;
-import org.apache.lucene.codecs.TermVectorsReader;
-import org.apache.lucene.codecs.TermVectorsWriter;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-
-/**
- * plain text term vectors format.
- * <p>
- * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
- * @lucene.experimental
- */
-public class SimpleTextTermVectorsFormat extends TermVectorsFormat {
-
-  @Override
-  public TermVectorsReader vectorsReader(Directory directory, SegmentInfo segmentInfo, FieldInfos fieldInfos, IOContext context) throws IOException {
-    return new SimpleTextTermVectorsReader(directory, segmentInfo, context);
-  }
-
-  @Override
-  public TermVectorsWriter vectorsWriter(Directory directory, SegmentInfo segmentInfo, IOContext context) throws IOException {
-    return new SimpleTextTermVectorsWriter(directory, segmentInfo.name, context);
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextTermVectorsReader.java b/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextTermVectorsReader.java
deleted file mode 100644
index 3b80d3d..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextTermVectorsReader.java
+++ /dev/null
@@ -1,541 +0,0 @@
-package org.apache.lucene.codecs.simpletext;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.Comparator;
-import java.util.Iterator;
-import java.util.Map;
-import java.util.SortedMap;
-import java.util.TreeMap;
-
-import org.apache.lucene.codecs.TermVectorsReader;
-import org.apache.lucene.index.DocsAndPositionsEnum;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.Fields;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.Terms;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.store.AlreadyClosedException;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.CharsRef;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.StringHelper;
-import org.apache.lucene.util.UnicodeUtil;
-import static org.apache.lucene.codecs.simpletext.SimpleTextTermVectorsWriter.*;
-
-/**
- * Reads plain-text term vectors.
- * <p>
- * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
- * @lucene.experimental
- */
-public class SimpleTextTermVectorsReader extends TermVectorsReader {
-  private ArrayList<Long> offsets; /* docid -> offset in .vec file */
-  private IndexInput in;
-  private BytesRef scratch = new BytesRef();
-  private CharsRef scratchUTF16 = new CharsRef();
-  
-  public SimpleTextTermVectorsReader(Directory directory, SegmentInfo si, IOContext context) throws IOException {
-    boolean success = false;
-    try {
-      in = directory.openInput(IndexFileNames.segmentFileName(si.name, "", VECTORS_EXTENSION), context);
-      success = true;
-    } finally {
-      if (!success) {
-        close();
-      }
-    }
-    readIndex();
-  }
-  
-  // used by clone
-  SimpleTextTermVectorsReader(ArrayList<Long> offsets, IndexInput in) {
-    this.offsets = offsets;
-    this.in = in;
-  }
-  
-  // we don't actually write a .tvx-like index, instead we read the 
-  // vectors file in entirety up-front and save the offsets 
-  // so we can seek to the data later.
-  private void readIndex() throws IOException {
-    offsets = new ArrayList<Long>();
-    while (!scratch.equals(END)) {
-      readLine();
-      if (StringHelper.startsWith(scratch, DOC)) {
-        offsets.add(in.getFilePointer());
-      }
-    }
-  }
-  
-  @Override
-  public Fields get(int doc) throws IOException {
-    // TestTV tests for this in testBadParams... but is this
-    // really guaranteed by the API?
-    if (doc < 0 || doc >= offsets.size()) {
-      throw new IllegalArgumentException("doc id out of range");
-    }
-
-    SortedMap<String,SimpleTVTerms> fields = new TreeMap<String,SimpleTVTerms>();
-    in.seek(offsets.get(doc));
-    readLine();
-    assert StringHelper.startsWith(scratch, NUMFIELDS);
-    int numFields = parseIntAt(NUMFIELDS.length);
-    if (numFields == 0) {
-      return null; // no vectors for this doc
-    }
-    for (int i = 0; i < numFields; i++) {
-      readLine();
-      assert StringHelper.startsWith(scratch, FIELD);
-      // skip fieldNumber:
-      parseIntAt(FIELD.length);
-      
-      readLine();
-      assert StringHelper.startsWith(scratch, FIELDNAME);
-      String fieldName = readString(FIELDNAME.length, scratch);
-      
-      readLine();
-      assert StringHelper.startsWith(scratch, FIELDPOSITIONS);
-      boolean positions = Boolean.parseBoolean(readString(FIELDPOSITIONS.length, scratch));
-      
-      readLine();
-      assert StringHelper.startsWith(scratch, FIELDOFFSETS);
-      boolean offsets = Boolean.parseBoolean(readString(FIELDOFFSETS.length, scratch));
-      
-      readLine();
-      assert StringHelper.startsWith(scratch, FIELDPAYLOADS);
-      boolean payloads = Boolean.parseBoolean(readString(FIELDPAYLOADS.length, scratch));
-      
-      readLine();
-      assert StringHelper.startsWith(scratch, FIELDTERMCOUNT);
-      int termCount = parseIntAt(FIELDTERMCOUNT.length);
-      
-      SimpleTVTerms terms = new SimpleTVTerms(offsets, positions, payloads);
-      fields.put(fieldName, terms);
-      
-      for (int j = 0; j < termCount; j++) {
-        readLine();
-        assert StringHelper.startsWith(scratch, TERMTEXT);
-        BytesRef term = new BytesRef();
-        int termLength = scratch.length - TERMTEXT.length;
-        term.grow(termLength);
-        term.length = termLength;
-        System.arraycopy(scratch.bytes, scratch.offset+TERMTEXT.length, term.bytes, term.offset, termLength);
-        
-        SimpleTVPostings postings = new SimpleTVPostings();
-        terms.terms.put(term, postings);
-        
-        readLine();
-        assert StringHelper.startsWith(scratch, TERMFREQ);
-        postings.freq = parseIntAt(TERMFREQ.length);
-        
-        if (positions || offsets) {
-          if (positions) {
-            postings.positions = new int[postings.freq];
-            if (payloads) {
-              postings.payloads = new BytesRef[postings.freq];
-            }
-          }
-        
-          if (offsets) {
-            postings.startOffsets = new int[postings.freq];
-            postings.endOffsets = new int[postings.freq];
-          }
-          
-          for (int k = 0; k < postings.freq; k++) {
-            if (positions) {
-              readLine();
-              assert StringHelper.startsWith(scratch, POSITION);
-              postings.positions[k] = parseIntAt(POSITION.length);
-              if (payloads) {
-                readLine();
-                assert StringHelper.startsWith(scratch, PAYLOAD);
-                if (scratch.length - PAYLOAD.length == 0) {
-                  postings.payloads[k] = null;
-                } else {
-                  byte payloadBytes[] = new byte[scratch.length - PAYLOAD.length];
-                  System.arraycopy(scratch.bytes, scratch.offset+PAYLOAD.length, payloadBytes, 0, payloadBytes.length);
-                  postings.payloads[k] = new BytesRef(payloadBytes);
-                }
-              }
-            }
-            
-            if (offsets) {
-              readLine();
-              assert StringHelper.startsWith(scratch, STARTOFFSET);
-              postings.startOffsets[k] = parseIntAt(STARTOFFSET.length);
-              
-              readLine();
-              assert StringHelper.startsWith(scratch, ENDOFFSET);
-              postings.endOffsets[k] = parseIntAt(ENDOFFSET.length);
-            }
-          }
-        }
-      }
-    }
-    return new SimpleTVFields(fields);
-  }
-
-  @Override
-  public TermVectorsReader clone() {
-    if (in == null) {
-      throw new AlreadyClosedException("this TermVectorsReader is closed");
-    }
-    return new SimpleTextTermVectorsReader(offsets, in.clone());
-  }
-  
-  @Override
-  public void close() throws IOException {
-    try {
-      IOUtils.close(in); 
-    } finally {
-      in = null;
-      offsets = null;
-    }
-  }
-
-  private void readLine() throws IOException {
-    SimpleTextUtil.readLine(in, scratch);
-  }
-  
-  private int parseIntAt(int offset) {
-    UnicodeUtil.UTF8toUTF16(scratch.bytes, scratch.offset+offset, scratch.length-offset, scratchUTF16);
-    return ArrayUtil.parseInt(scratchUTF16.chars, 0, scratchUTF16.length);
-  }
-  
-  private String readString(int offset, BytesRef scratch) {
-    UnicodeUtil.UTF8toUTF16(scratch.bytes, scratch.offset+offset, scratch.length-offset, scratchUTF16);
-    return scratchUTF16.toString();
-  }
-  
-  private class SimpleTVFields extends Fields {
-    private final SortedMap<String,SimpleTVTerms> fields;
-    
-    SimpleTVFields(SortedMap<String,SimpleTVTerms> fields) {
-      this.fields = fields;
-    }
-
-    @Override
-    public Iterator<String> iterator() {
-      return Collections.unmodifiableSet(fields.keySet()).iterator();
-    }
-
-    @Override
-    public Terms terms(String field) throws IOException {
-      return fields.get(field);
-    }
-
-    @Override
-    public int size() {
-      return fields.size();
-    }
-  }
-  
-  private static class SimpleTVTerms extends Terms {
-    final SortedMap<BytesRef,SimpleTVPostings> terms;
-    final boolean hasOffsets;
-    final boolean hasPositions;
-    final boolean hasPayloads;
-    
-    SimpleTVTerms(boolean hasOffsets, boolean hasPositions, boolean hasPayloads) {
-      this.hasOffsets = hasOffsets;
-      this.hasPositions = hasPositions;
-      this.hasPayloads = hasPayloads;
-      terms = new TreeMap<BytesRef,SimpleTVPostings>();
-    }
-    
-    @Override
-    public TermsEnum iterator(TermsEnum reuse) throws IOException {
-      // TODO: reuse
-      return new SimpleTVTermsEnum(terms);
-    }
-
-    @Override
-    public Comparator<BytesRef> getComparator() throws IOException {
-      return BytesRef.getUTF8SortedAsUnicodeComparator();
-    }
-
-    @Override
-    public long size() throws IOException {
-      return terms.size();
-    }
-
-    @Override
-    public long getSumTotalTermFreq() throws IOException {
-      return -1;
-    }
-
-    @Override
-    public long getSumDocFreq() throws IOException {
-      return terms.size();
-    }
-
-    @Override
-    public int getDocCount() throws IOException {
-      return 1;
-    }
-
-    @Override
-    public boolean hasOffsets() {
-      return hasOffsets;
-    }
-
-    @Override
-    public boolean hasPositions() {
-      return hasPositions;
-    }
-    
-    @Override
-    public boolean hasPayloads() {
-      return hasPayloads;
-    }
-  }
-  
-  private static class SimpleTVPostings {
-    private int freq;
-    private int positions[];
-    private int startOffsets[];
-    private int endOffsets[];
-    private BytesRef payloads[];
-  }
-  
-  private static class SimpleTVTermsEnum extends TermsEnum {
-    SortedMap<BytesRef,SimpleTVPostings> terms;
-    Iterator<Map.Entry<BytesRef,SimpleTextTermVectorsReader.SimpleTVPostings>> iterator;
-    Map.Entry<BytesRef,SimpleTextTermVectorsReader.SimpleTVPostings> current;
-    
-    SimpleTVTermsEnum(SortedMap<BytesRef,SimpleTVPostings> terms) {
-      this.terms = terms;
-      this.iterator = terms.entrySet().iterator();
-    }
-    
-    @Override
-    public SeekStatus seekCeil(BytesRef text, boolean useCache) throws IOException {
-      iterator = terms.tailMap(text).entrySet().iterator();
-      if (!iterator.hasNext()) {
-        return SeekStatus.END;
-      } else {
-        return next().equals(text) ? SeekStatus.FOUND : SeekStatus.NOT_FOUND;
-      }
-    }
-
-    @Override
-    public void seekExact(long ord) throws IOException {
-      throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public BytesRef next() throws IOException {
-      if (!iterator.hasNext()) {
-        return null;
-      } else {
-        current = iterator.next();
-        return current.getKey();
-      }
-    }
-
-    @Override
-    public BytesRef term() throws IOException {
-      return current.getKey();
-    }
-
-    @Override
-    public long ord() throws IOException {
-      throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public int docFreq() throws IOException {
-      return 1;
-    }
-
-    @Override
-    public long totalTermFreq() throws IOException {
-      return current.getValue().freq;
-    }
-
-    @Override
-    public DocsEnum docs(Bits liveDocs, DocsEnum reuse, int flags) throws IOException {
-      // TODO: reuse
-      SimpleTVDocsEnum e = new SimpleTVDocsEnum();
-      e.reset(liveDocs, (flags & DocsEnum.FLAG_FREQS) == 0 ? 1 : current.getValue().freq);
-      return e;
-    }
-
-    @Override
-    public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum reuse, int flags) throws IOException {
-      SimpleTVPostings postings = current.getValue();
-      if (postings.positions == null && postings.startOffsets == null) {
-        return null;
-      }
-      // TODO: reuse
-      SimpleTVDocsAndPositionsEnum e = new SimpleTVDocsAndPositionsEnum();
-      e.reset(liveDocs, postings.positions, postings.startOffsets, postings.endOffsets, postings.payloads);
-      return e;
-    }
-
-    @Override
-    public Comparator<BytesRef> getComparator() {
-      return BytesRef.getUTF8SortedAsUnicodeComparator();
-    }
-  }
-  
-  // note: these two enum classes are exactly like the Default impl...
-  private static class SimpleTVDocsEnum extends DocsEnum {
-    private boolean didNext;
-    private int doc = -1;
-    private int freq;
-    private Bits liveDocs;
-
-    @Override
-    public int freq() throws IOException {
-      assert freq != -1;
-      return freq;
-    }
-
-    @Override
-    public int docID() {
-      return doc;
-    }
-
-    @Override
-    public int nextDoc() {
-      if (!didNext && (liveDocs == null || liveDocs.get(0))) {
-        didNext = true;
-        return (doc = 0);
-      } else {
-        return (doc = NO_MORE_DOCS);
-      }
-    }
-
-    @Override
-    public int advance(int target) {
-      if (!didNext && target == 0) {
-        return nextDoc();
-      } else {
-        return (doc = NO_MORE_DOCS);
-      }
-    }
-
-    public void reset(Bits liveDocs, int freq) {
-      this.liveDocs = liveDocs;
-      this.freq = freq;
-      this.doc = -1;
-      didNext = false;
-    }
-  }
-  
-  private static class SimpleTVDocsAndPositionsEnum extends DocsAndPositionsEnum {
-    private boolean didNext;
-    private int doc = -1;
-    private int nextPos;
-    private Bits liveDocs;
-    private int[] positions;
-    private BytesRef[] payloads;
-    private int[] startOffsets;
-    private int[] endOffsets;
-
-    @Override
-    public int freq() throws IOException {
-      if (positions != null) {
-        return positions.length;
-      } else {
-        assert startOffsets != null;
-        return startOffsets.length;
-      }
-    }
-
-    @Override
-    public int docID() {
-      return doc;
-    }
-
-    @Override
-    public int nextDoc() {
-      if (!didNext && (liveDocs == null || liveDocs.get(0))) {
-        didNext = true;
-        return (doc = 0);
-      } else {
-        return (doc = NO_MORE_DOCS);
-      }
-    }
-
-    @Override
-    public int advance(int target) {
-      if (!didNext && target == 0) {
-        return nextDoc();
-      } else {
-        return (doc = NO_MORE_DOCS);
-      }
-    }
-
-    public void reset(Bits liveDocs, int[] positions, int[] startOffsets, int[] endOffsets, BytesRef payloads[]) {
-      this.liveDocs = liveDocs;
-      this.positions = positions;
-      this.startOffsets = startOffsets;
-      this.endOffsets = endOffsets;
-      this.payloads = payloads;
-      this.doc = -1;
-      didNext = false;
-      nextPos = 0;
-    }
-
-    @Override
-    public BytesRef getPayload() {
-      return payloads == null ? null : payloads[nextPos-1];
-    }
-
-    @Override
-    public int nextPosition() {
-      assert (positions != null && nextPos < positions.length) ||
-        startOffsets != null && nextPos < startOffsets.length;
-      if (positions != null) {
-        return positions[nextPos++];
-      } else {
-        nextPos++;
-        return -1;
-      }
-    }
-
-    @Override
-    public int startOffset() {
-      if (startOffsets == null) {
-        return -1;
-      } else {
-        return startOffsets[nextPos-1];
-      }
-    }
-
-    @Override
-    public int endOffset() {
-      if (endOffsets == null) {
-        return -1;
-      } else {
-        return endOffsets[nextPos-1];
-      }
-    }
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextTermVectorsWriter.java b/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextTermVectorsWriter.java
deleted file mode 100644
index 673ecea..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextTermVectorsWriter.java
+++ /dev/null
@@ -1,208 +0,0 @@
-package org.apache.lucene.codecs.simpletext;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Comparator;
-
-import org.apache.lucene.codecs.TermVectorsWriter;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-
-/**
- * Writes plain-text term vectors.
- * <p>
- * <b><font color="red">FOR RECREATIONAL USE ONLY</font></B>
- * @lucene.experimental
- */
-public class SimpleTextTermVectorsWriter extends TermVectorsWriter {
-  
-  static final BytesRef END                = new BytesRef("END");
-  static final BytesRef DOC                = new BytesRef("doc ");
-  static final BytesRef NUMFIELDS          = new BytesRef("  numfields ");
-  static final BytesRef FIELD              = new BytesRef("  field ");
-  static final BytesRef FIELDNAME          = new BytesRef("    name ");
-  static final BytesRef FIELDPOSITIONS     = new BytesRef("    positions ");
-  static final BytesRef FIELDOFFSETS       = new BytesRef("    offsets   ");
-  static final BytesRef FIELDPAYLOADS      = new BytesRef("    payloads  ");
-  static final BytesRef FIELDTERMCOUNT     = new BytesRef("    numterms ");
-  static final BytesRef TERMTEXT           = new BytesRef("    term ");
-  static final BytesRef TERMFREQ           = new BytesRef("      freq ");
-  static final BytesRef POSITION           = new BytesRef("      position ");
-  static final BytesRef PAYLOAD            = new BytesRef("        payload ");
-  static final BytesRef STARTOFFSET        = new BytesRef("        startoffset ");
-  static final BytesRef ENDOFFSET          = new BytesRef("        endoffset ");
-
-  static final String VECTORS_EXTENSION = "vec";
-  
-  private final Directory directory;
-  private final String segment;
-  private IndexOutput out;
-  private int numDocsWritten = 0;
-  private final BytesRef scratch = new BytesRef();
-  private boolean offsets;
-  private boolean positions;
-  private boolean payloads;
-
-  public SimpleTextTermVectorsWriter(Directory directory, String segment, IOContext context) throws IOException {
-    this.directory = directory;
-    this.segment = segment;
-    boolean success = false;
-    try {
-      out = directory.createOutput(IndexFileNames.segmentFileName(segment, "", VECTORS_EXTENSION), context);
-      success = true;
-    } finally {
-      if (!success) {
-        abort();
-      }
-    }
-  }
-  
-  @Override
-  public void startDocument(int numVectorFields) throws IOException {
-    write(DOC);
-    write(Integer.toString(numDocsWritten));
-    newLine();
-    
-    write(NUMFIELDS);
-    write(Integer.toString(numVectorFields));
-    newLine();
-    numDocsWritten++;
-  }
-
-  @Override
-  public void startField(FieldInfo info, int numTerms, boolean positions, boolean offsets, boolean payloads) throws IOException {  
-    write(FIELD);
-    write(Integer.toString(info.number));
-    newLine();
-    
-    write(FIELDNAME);
-    write(info.name);
-    newLine();
-    
-    write(FIELDPOSITIONS);
-    write(Boolean.toString(positions));
-    newLine();
-    
-    write(FIELDOFFSETS);
-    write(Boolean.toString(offsets));
-    newLine();
-    
-    write(FIELDPAYLOADS);
-    write(Boolean.toString(payloads));
-    newLine();
-    
-    write(FIELDTERMCOUNT);
-    write(Integer.toString(numTerms));
-    newLine();
-    
-    this.positions = positions;
-    this.offsets = offsets;
-    this.payloads = payloads;
-  }
-
-  @Override
-  public void startTerm(BytesRef term, int freq) throws IOException {
-    write(TERMTEXT);
-    write(term);
-    newLine();
-    
-    write(TERMFREQ);
-    write(Integer.toString(freq));
-    newLine();
-  }
-
-  @Override
-  public void addPosition(int position, int startOffset, int endOffset, BytesRef payload) throws IOException {
-    assert positions || offsets;
-    
-    if (positions) {
-      write(POSITION);
-      write(Integer.toString(position));
-      newLine();
-      
-      if (payloads) {
-        write(PAYLOAD);
-        if (payload != null) {
-          assert payload.length > 0;
-          write(payload);
-        }
-        newLine();
-      }
-    }
-    
-    if (offsets) {
-      write(STARTOFFSET);
-      write(Integer.toString(startOffset));
-      newLine();
-      
-      write(ENDOFFSET);
-      write(Integer.toString(endOffset));
-      newLine();
-    }
-  }
-
-  @Override
-  public void abort() {
-    try {
-      close();
-    } catch (IOException ignored) {}
-    IOUtils.deleteFilesIgnoringExceptions(directory, IndexFileNames.segmentFileName(segment, "", VECTORS_EXTENSION));
-  }
-
-  @Override
-  public void finish(FieldInfos fis, int numDocs) throws IOException {
-    if (numDocsWritten != numDocs) {
-      throw new RuntimeException("mergeVectors produced an invalid result: mergedDocs is " + numDocs + " but vec numDocs is " + numDocsWritten + " file=" + out.toString() + "; now aborting this merge to prevent index corruption");
-    }
-    write(END);
-    newLine();
-  }
-  
-  @Override
-  public void close() throws IOException {
-    try {
-      IOUtils.close(out);
-    } finally {
-      out = null;
-    }
-  }
-  
-  @Override
-  public Comparator<BytesRef> getComparator() throws IOException {
-    return BytesRef.getUTF8SortedAsUnicodeComparator();
-  }
-  
-  private void write(String s) throws IOException {
-    SimpleTextUtil.write(out, s, scratch);
-  }
-  
-  private void write(BytesRef bytes) throws IOException {
-    SimpleTextUtil.write(out, bytes);
-  }
-  
-  private void newLine() throws IOException {
-    SimpleTextUtil.writeNewline(out);
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextUtil.java b/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextUtil.java
deleted file mode 100644
index c0c7787..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/simpletext/SimpleTextUtil.java
+++ /dev/null
@@ -1,70 +0,0 @@
-package org.apache.lucene.codecs.simpletext;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.store.DataInput;
-import org.apache.lucene.store.DataOutput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.UnicodeUtil;
-
-class SimpleTextUtil {
-  public final static byte NEWLINE = 10;
-  public final static byte ESCAPE = 92;
-  
-  public static void write(DataOutput out, String s, BytesRef scratch) throws IOException {
-    UnicodeUtil.UTF16toUTF8(s, 0, s.length(), scratch);
-    write(out, scratch);
-  }
-
-  public static void write(DataOutput out, BytesRef b) throws IOException {
-    for(int i=0;i<b.length;i++) {
-      final byte bx = b.bytes[b.offset+i];
-      if (bx == NEWLINE || bx == ESCAPE) {
-        out.writeByte(ESCAPE);
-      }
-      out.writeByte(bx);
-    }
-  }
-
-  public static void writeNewline(DataOutput out) throws IOException {
-    out.writeByte(NEWLINE);
-  }
-  
-  public static void readLine(DataInput in, BytesRef scratch) throws IOException {
-    int upto = 0;
-    while(true) {
-      byte b = in.readByte();
-      if (scratch.bytes.length == upto) {
-        scratch.grow(1+upto);
-      }
-      if (b == ESCAPE) {
-        scratch.bytes[upto++] = in.readByte();
-      } else {
-        if (b == NEWLINE) {
-          break;
-        } else {
-          scratch.bytes[upto++] = b;
-        }
-      }
-    }
-    scratch.offset = 0;
-    scratch.length = upto;
-  }
-}
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/simpletext/package.html b/lucene/core/src/java/org/apache/lucene/codecs/simpletext/package.html
deleted file mode 100644
index 88aad68..0000000
--- a/lucene/core/src/java/org/apache/lucene/codecs/simpletext/package.html
+++ /dev/null
@@ -1,25 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<head>
-   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
-</head>
-<body>
-Simpletext Codec: writes human readable postings.
-</body>
-</html>
diff --git a/lucene/core/src/resources/META-INF/services/org.apache.lucene.codecs.Codec b/lucene/core/src/resources/META-INF/services/org.apache.lucene.codecs.Codec
index eaa1d33..82c3e5c 100644
--- a/lucene/core/src/resources/META-INF/services/org.apache.lucene.codecs.Codec
+++ b/lucene/core/src/resources/META-INF/services/org.apache.lucene.codecs.Codec
@@ -14,5 +14,3 @@
 #  limitations under the License.
 
 org.apache.lucene.codecs.lucene40.Lucene40Codec
-org.apache.lucene.codecs.simpletext.SimpleTextCodec
-org.apache.lucene.codecs.appending.AppendingCodec
diff --git a/lucene/core/src/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat b/lucene/core/src/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat
index 7bc89a0..112a169 100644
--- a/lucene/core/src/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat
+++ b/lucene/core/src/resources/META-INF/services/org.apache.lucene.codecs.PostingsFormat
@@ -14,9 +14,3 @@
 #  limitations under the License.
 
 org.apache.lucene.codecs.lucene40.Lucene40PostingsFormat
-org.apache.lucene.codecs.pulsing.Pulsing40PostingsFormat
-org.apache.lucene.codecs.simpletext.SimpleTextPostingsFormat
-org.apache.lucene.codecs.memory.MemoryPostingsFormat
-org.apache.lucene.codecs.bloom.BloomFilteringPostingsFormat
-org.apache.lucene.codecs.memory.DirectPostingsFormat
-org.apache.lucene.codecs.block.BlockPostingsFormat
diff --git a/lucene/core/src/test/org/apache/lucene/codecs/appending/TestAppendingCodec.java b/lucene/core/src/test/org/apache/lucene/codecs/appending/TestAppendingCodec.java
deleted file mode 100644
index d5ccd2e..0000000
--- a/lucene/core/src/test/org/apache/lucene/codecs/appending/TestAppendingCodec.java
+++ /dev/null
@@ -1,169 +0,0 @@
-package org.apache.lucene.codecs.appending;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Random;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.codecs.appending.AppendingCodec;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.FieldType;
-import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.Fields;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexWriter;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.MultiFields;
-import org.apache.lucene.index.StoredDocument;
-import org.apache.lucene.index.Terms;
-import org.apache.lucene.index.TermsEnum.SeekStatus;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.index.TieredMergePolicy;
-import org.apache.lucene.search.DocIdSetIterator;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.store.MockDirectoryWrapper;
-import org.apache.lucene.store.RAMDirectory;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util.Version;
-
-public class TestAppendingCodec extends LuceneTestCase {
-  
-    private static class AppendingIndexOutputWrapper extends IndexOutput {
-    IndexOutput wrapped;
-    
-    public AppendingIndexOutputWrapper(IndexOutput wrapped) {
-      this.wrapped = wrapped;
-    }
-
-    @Override
-    public void close() throws IOException {
-      wrapped.close();
-    }
-
-    @Override
-    public void flush() throws IOException {
-      wrapped.flush();
-    }
-
-    @Override
-    public long getFilePointer() {
-      return wrapped.getFilePointer();
-    }
-
-    @Override
-    public long length() throws IOException {
-      return wrapped.length();
-    }
-
-    @Override
-    public void seek(long pos) throws IOException {
-      throw new UnsupportedOperationException("seek() is unsupported");
-    }
-
-    @Override
-    public void writeByte(byte b) throws IOException {
-      wrapped.writeByte(b);
-    }
-
-    @Override
-    public void writeBytes(byte[] b, int offset, int length) throws IOException {
-      wrapped.writeBytes(b, offset, length);
-    }
-    
-  }
-  
-  @SuppressWarnings("serial")
-  private static class AppendingRAMDirectory extends MockDirectoryWrapper {
-
-    public AppendingRAMDirectory(Random random, Directory delegate) {
-      super(random, delegate);
-    }
-
-    @Override
-    public IndexOutput createOutput(String name, IOContext context) throws IOException {
-      return new AppendingIndexOutputWrapper(super.createOutput(name, context));
-    }
-    
-  }
-  
-  private static final String text = "the quick brown fox jumped over the lazy dog";
-
-  public void testCodec() throws Exception {
-    Directory dir = new AppendingRAMDirectory(random(), new RAMDirectory());
-    IndexWriterConfig cfg = new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer(random()));
-    
-    cfg.setCodec(new AppendingCodec());
-    ((TieredMergePolicy)cfg.getMergePolicy()).setUseCompoundFile(false);
-    IndexWriter writer = new IndexWriter(dir, cfg);
-    Document doc = new Document();
-    FieldType storedTextType = new FieldType(TextField.TYPE_STORED);
-    storedTextType.setStoreTermVectors(true);
-    storedTextType.setStoreTermVectorPositions(true);
-    storedTextType.setStoreTermVectorOffsets(true);
-    doc.add(newField("f", text, storedTextType));
-    writer.addDocument(doc);
-    writer.commit();
-    writer.addDocument(doc);
-    writer.forceMerge(1);
-    writer.close();
-    IndexReader reader = DirectoryReader.open(dir, 1);
-    assertEquals(2, reader.numDocs());
-    StoredDocument doc2 = reader.document(0);
-    assertEquals(text, doc2.get("f"));
-    Fields fields = MultiFields.getFields(reader);
-    Terms terms = fields.terms("f");
-    assertNotNull(terms);
-    TermsEnum te = terms.iterator(null);
-    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef("quick")));
-    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef("brown")));
-    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef("fox")));
-    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef("jumped")));
-    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef("over")));
-    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef("lazy")));
-    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef("dog")));
-    assertEquals(SeekStatus.FOUND, te.seekCeil(new BytesRef("the")));
-    DocsEnum de = te.docs(null, null);
-    assertTrue(de.advance(0) != DocIdSetIterator.NO_MORE_DOCS);
-    assertEquals(2, de.freq());
-    assertTrue(de.advance(1) != DocIdSetIterator.NO_MORE_DOCS);
-    assertTrue(de.advance(2) == DocIdSetIterator.NO_MORE_DOCS);
-    reader.close();
-  }
-  
-  public void testCompoundFile() throws Exception {
-    Directory dir = new AppendingRAMDirectory(random(), new RAMDirectory());
-    IndexWriterConfig cfg = new IndexWriterConfig(Version.LUCENE_40, new MockAnalyzer(random()));
-    TieredMergePolicy mp = new TieredMergePolicy();
-    mp.setUseCompoundFile(true);
-    mp.setNoCFSRatio(1.0);
-    cfg.setMergePolicy(mp);
-    cfg.setCodec(new AppendingCodec());
-    IndexWriter writer = new IndexWriter(dir, cfg);
-    Document doc = new Document();
-    writer.addDocument(doc);
-    writer.close();
-    assertTrue(dir.fileExists("_0.cfs"));
-    dir.close();
-  }
-}
diff --git a/lucene/core/src/test/org/apache/lucene/codecs/block/TestForUtil.java b/lucene/core/src/test/org/apache/lucene/codecs/block/TestForUtil.java
deleted file mode 100644
index 025a634..0000000
--- a/lucene/core/src/test/org/apache/lucene/codecs/block/TestForUtil.java
+++ /dev/null
@@ -1,94 +0,0 @@
-package org.apache.lucene.codecs.block;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import static org.apache.lucene.codecs.block.BlockPostingsFormat.BLOCK_SIZE;
-import static org.apache.lucene.codecs.block.ForUtil.MAX_DATA_SIZE;
-import static org.apache.lucene.codecs.block.ForUtil.MAX_ENCODED_SIZE;
-
-import java.io.IOException;
-import java.util.Arrays;
-
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.store.RAMDirectory;
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util.packed.PackedInts;
-
-import com.carrotsearch.randomizedtesting.generators.RandomInts;
-
-public class TestForUtil extends LuceneTestCase {
-
-  public void testEncodeDecode() throws IOException {
-    final int iterations = RandomInts.randomIntBetween(random(), 1, 1000);
-    final float acceptableOverheadRatio = random().nextFloat();
-    final int[] values = new int[(iterations - 1) * BLOCK_SIZE + ForUtil.MAX_DATA_SIZE];
-    for (int i = 0; i < iterations; ++i) {
-      final int bpv = random().nextInt(32);
-      if (bpv == 0) {
-        final int value = RandomInts.randomIntBetween(random(), 0, Integer.MAX_VALUE);
-        for (int j = 0; j < BLOCK_SIZE; ++j) {
-          values[i * BLOCK_SIZE + j] = value;
-        }
-      } else {
-        for (int j = 0; j < BLOCK_SIZE; ++j) {
-          values[i * BLOCK_SIZE + j] = RandomInts.randomIntBetween(random(),
-              0, (int) PackedInts.maxValue(bpv));
-        }
-      }
-    }
-
-    final Directory d = new RAMDirectory();
-    final long endPointer;
-
-    {
-      // encode
-      IndexOutput out = d.createOutput("test.bin", IOContext.DEFAULT);
-      final ForUtil forUtil = new ForUtil(acceptableOverheadRatio, out);
-      
-      for (int i = 0; i < iterations; ++i) {
-        forUtil.writeBlock(
-            Arrays.copyOfRange(values, i * BLOCK_SIZE, values.length),
-            new byte[MAX_ENCODED_SIZE], out);
-      }
-      endPointer = out.getFilePointer();
-      out.close();
-    }
-
-    {
-      // decode
-      IndexInput in = d.openInput("test.bin", IOContext.READONCE);
-      final ForUtil forUtil = new ForUtil(in);
-      for (int i = 0; i < iterations; ++i) {
-        if (random().nextBoolean()) {
-          forUtil.skipBlock(in);
-          continue;
-        }
-        final int[] restored = new int[MAX_DATA_SIZE];
-        forUtil.readBlock(in, new byte[MAX_ENCODED_SIZE], restored);
-        assertArrayEquals(Arrays.copyOfRange(values, i * BLOCK_SIZE, (i + 1) * BLOCK_SIZE),
-            Arrays.copyOf(restored, BLOCK_SIZE));
-      }
-      assertEquals(endPointer, in.getFilePointer());
-      in.close();
-    }
-  }
-
-}
diff --git a/lucene/core/src/test/org/apache/lucene/codecs/intblock/TestIntBlockCodec.java b/lucene/core/src/test/org/apache/lucene/codecs/intblock/TestIntBlockCodec.java
deleted file mode 100644
index c85662a..0000000
--- a/lucene/core/src/test/org/apache/lucene/codecs/intblock/TestIntBlockCodec.java
+++ /dev/null
@@ -1,64 +0,0 @@
-package org.apache.lucene.codecs.intblock;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.store.*;
-import org.apache.lucene.codecs.mockintblock.*;
-import org.apache.lucene.codecs.sep.*;
-
-public class TestIntBlockCodec extends LuceneTestCase {
-
-  public void testSimpleIntBlocks() throws Exception {
-    Directory dir = newDirectory();
-
-    IntStreamFactory f = new MockFixedIntBlockPostingsFormat(128).getIntFactory();
-
-    IntIndexOutput out = f.createOutput(dir, "test", newIOContext(random()));
-    for(int i=0;i<11777;i++) {
-      out.write(i);
-    }
-    out.close();
-
-    IntIndexInput in = f.openInput(dir, "test", newIOContext(random()));
-    IntIndexInput.Reader r = in.reader();
-
-    for(int i=0;i<11777;i++) {
-      assertEquals(i, r.next());
-    }
-    in.close();
-    
-    dir.close();
-  }
-
-  public void testEmptySimpleIntBlocks() throws Exception {
-    Directory dir = newDirectory();
-
-    IntStreamFactory f = new MockFixedIntBlockPostingsFormat(128).getIntFactory();
-    IntIndexOutput out = f.createOutput(dir, "test", newIOContext(random()));
-
-    // write no ints
-    out.close();
-
-    IntIndexInput in = f.openInput(dir, "test", newIOContext(random()));
-    in.reader();
-    // read no ints
-    in.close();
-    dir.close();
-  }
-}
diff --git a/lucene/core/src/test/org/apache/lucene/codecs/pulsing/Test10KPulsings.java b/lucene/core/src/test/org/apache/lucene/codecs/pulsing/Test10KPulsings.java
deleted file mode 100644
index 3e47dc5..0000000
--- a/lucene/core/src/test/org/apache/lucene/codecs/pulsing/Test10KPulsings.java
+++ /dev/null
@@ -1,158 +0,0 @@
-package org.apache.lucene.codecs.pulsing;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.File;
-import java.text.DecimalFormat;
-import java.text.DecimalFormatSymbols;
-import java.text.NumberFormat;
-import java.util.Locale;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.codecs.pulsing.Pulsing40PostingsFormat;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.FieldType;
-import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.MultiFields;
-import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.search.DocIdSetIterator;
-import org.apache.lucene.store.BaseDirectoryWrapper;
-import org.apache.lucene.store.MockDirectoryWrapper;
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util._TestUtil;
-
-/**
- * Pulses 10k terms/docs, 
- * originally designed to find JRE bugs (https://issues.apache.org/jira/browse/LUCENE-3335)
- * 
- * @lucene.experimental
- */
-@LuceneTestCase.Nightly
-public class Test10KPulsings extends LuceneTestCase {
-  public void test10kPulsed() throws Exception {
-    // we always run this test with pulsing codec.
-    Codec cp = _TestUtil.alwaysPostingsFormat(new Pulsing40PostingsFormat(1));
-    
-    File f = _TestUtil.getTempDir("10kpulsed");
-    BaseDirectoryWrapper dir = newFSDirectory(f);
-    dir.setCheckIndexOnClose(false); // we do this ourselves explicitly
-    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, 
-        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setCodec(cp));
-    
-    Document document = new Document();
-    FieldType ft = new FieldType(TextField.TYPE_STORED);
-    
-    switch(_TestUtil.nextInt(random(), 0, 2)) {
-      case 0: ft.setIndexOptions(IndexOptions.DOCS_ONLY); break;
-      case 1: ft.setIndexOptions(IndexOptions.DOCS_AND_FREQS); break;
-      default: ft.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS); break; 
-    }
-
-    Field field = newField("field", "", ft);
-    document.add(field);
-    
-    NumberFormat df = new DecimalFormat("00000", new DecimalFormatSymbols(Locale.ROOT));
-
-    for (int i = 0; i < 10050; i++) {
-      field.setStringValue(df.format(i));
-      iw.addDocument(document);
-    }
-    
-    IndexReader ir = iw.getReader();
-    iw.close();
-
-    TermsEnum te = MultiFields.getTerms(ir, "field").iterator(null);
-    DocsEnum de = null;
-    
-    for (int i = 0; i < 10050; i++) {
-      String expected = df.format(i);
-      assertEquals(expected, te.next().utf8ToString());
-      de = _TestUtil.docs(random(), te, null, de, 0);
-      assertTrue(de.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);
-      assertEquals(DocIdSetIterator.NO_MORE_DOCS, de.nextDoc());
-    }
-    ir.close();
-
-    _TestUtil.checkIndex(dir);
-    dir.close();
-  }
-  
-  /** a variant, that uses pulsing, but uses a high TF to force pass thru to the underlying codec
-   */
-  public void test10kNotPulsed() throws Exception {
-    // we always run this test with pulsing codec.
-    int freqCutoff = _TestUtil.nextInt(random(), 1, 10);
-    Codec cp = _TestUtil.alwaysPostingsFormat(new Pulsing40PostingsFormat(freqCutoff));
-    
-    File f = _TestUtil.getTempDir("10knotpulsed");
-    BaseDirectoryWrapper dir = newFSDirectory(f);
-    dir.setCheckIndexOnClose(false); // we do this ourselves explicitly
-    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, 
-        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setCodec(cp));
-    
-    Document document = new Document();
-    FieldType ft = new FieldType(TextField.TYPE_STORED);
-    
-    switch(_TestUtil.nextInt(random(), 0, 2)) {
-      case 0: ft.setIndexOptions(IndexOptions.DOCS_ONLY); break;
-      case 1: ft.setIndexOptions(IndexOptions.DOCS_AND_FREQS); break;
-      default: ft.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS); break; 
-    }
-
-    Field field = newField("field", "", ft);
-    document.add(field);
-    
-    NumberFormat df = new DecimalFormat("00000", new DecimalFormatSymbols(Locale.ROOT));
-
-    final int freq = freqCutoff + 1;
-    
-    for (int i = 0; i < 10050; i++) {
-      StringBuilder sb = new StringBuilder();
-      for (int j = 0; j < freq; j++) {
-        sb.append(df.format(i));
-        sb.append(' '); // whitespace
-      }
-      field.setStringValue(sb.toString());
-      iw.addDocument(document);
-    }
-    
-    IndexReader ir = iw.getReader();
-    iw.close();
-
-    TermsEnum te = MultiFields.getTerms(ir, "field").iterator(null);
-    DocsEnum de = null;
-    
-    for (int i = 0; i < 10050; i++) {
-      String expected = df.format(i);
-      assertEquals(expected, te.next().utf8ToString());
-      de = _TestUtil.docs(random(), te, null, de, 0);
-      assertTrue(de.nextDoc() != DocIdSetIterator.NO_MORE_DOCS);
-      assertEquals(DocIdSetIterator.NO_MORE_DOCS, de.nextDoc());
-    }
-    ir.close();
-
-    _TestUtil.checkIndex(dir);
-    dir.close();
-  }
-}
diff --git a/lucene/core/src/test/org/apache/lucene/codecs/pulsing/TestPulsingReuse.java b/lucene/core/src/test/org/apache/lucene/codecs/pulsing/TestPulsingReuse.java
deleted file mode 100644
index a37770d..0000000
--- a/lucene/core/src/test/org/apache/lucene/codecs/pulsing/TestPulsingReuse.java
+++ /dev/null
@@ -1,126 +0,0 @@
-package org.apache.lucene.codecs.pulsing;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.IdentityHashMap;
-import java.util.Map;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.codecs.Codec;
-import org.apache.lucene.codecs.nestedpulsing.NestedPulsingPostingsFormat;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.CheckIndex;
-import org.apache.lucene.index.DirectoryReader;
-import org.apache.lucene.index.DocsAndPositionsEnum;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.store.BaseDirectoryWrapper;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util._TestUtil;
-
-/**
- * Tests that pulsing codec reuses its enums and wrapped enums
- */
-public class TestPulsingReuse extends LuceneTestCase {
-  // TODO: this is a basic test. this thing is complicated, add more
-  public void testSophisticatedReuse() throws Exception {
-    // we always run this test with pulsing codec.
-    Codec cp = _TestUtil.alwaysPostingsFormat(new Pulsing40PostingsFormat(1));
-    Directory dir = newDirectory();
-    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, 
-        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setCodec(cp));
-    Document doc = new Document();
-    doc.add(new TextField("foo", "a b b c c c d e f g g h i i j j k", Field.Store.NO));
-    iw.addDocument(doc);
-    DirectoryReader ir = iw.getReader();
-    iw.close();
-    
-    AtomicReader segment = getOnlySegmentReader(ir);
-    DocsEnum reuse = null;
-    Map<DocsEnum,Boolean> allEnums = new IdentityHashMap<DocsEnum,Boolean>();
-    TermsEnum te = segment.terms("foo").iterator(null);
-    while (te.next() != null) {
-      reuse = te.docs(null, reuse, 0);
-      allEnums.put(reuse, true);
-    }
-    
-    assertEquals(2, allEnums.size());
-    
-    allEnums.clear();
-    DocsAndPositionsEnum posReuse = null;
-    te = segment.terms("foo").iterator(null);
-    while (te.next() != null) {
-      posReuse = te.docsAndPositions(null, posReuse);
-      allEnums.put(posReuse, true);
-    }
-    
-    assertEquals(2, allEnums.size());
-    
-    ir.close();
-    dir.close();
-  }
-  
-  /** tests reuse with Pulsing1(Pulsing2(Standard)) */
-  public void testNestedPulsing() throws Exception {
-    // we always run this test with pulsing codec.
-    Codec cp = _TestUtil.alwaysPostingsFormat(new NestedPulsingPostingsFormat());
-    BaseDirectoryWrapper dir = newDirectory();
-    dir.setCheckIndexOnClose(false); // will do this ourselves, custom codec
-    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, 
-        newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random())).setCodec(cp));
-    Document doc = new Document();
-    doc.add(new TextField("foo", "a b b c c c d e f g g g h i i j j k l l m m m", Field.Store.NO));
-    // note: the reuse is imperfect, here we would have 4 enums (lost reuse when we get an enum for 'm')
-    // this is because we only track the 'last' enum we reused (not all).
-    // but this seems 'good enough' for now.
-    iw.addDocument(doc);
-    DirectoryReader ir = iw.getReader();
-    iw.close();
-    
-    AtomicReader segment = getOnlySegmentReader(ir);
-    DocsEnum reuse = null;
-    Map<DocsEnum,Boolean> allEnums = new IdentityHashMap<DocsEnum,Boolean>();
-    TermsEnum te = segment.terms("foo").iterator(null);
-    while (te.next() != null) {
-      reuse = te.docs(null, reuse, 0);
-      allEnums.put(reuse, true);
-    }
-    
-    assertEquals(4, allEnums.size());
-    
-    allEnums.clear();
-    DocsAndPositionsEnum posReuse = null;
-    te = segment.terms("foo").iterator(null);
-    while (te.next() != null) {
-      posReuse = te.docsAndPositions(null, posReuse);
-      allEnums.put(posReuse, true);
-    }
-    
-    assertEquals(4, allEnums.size());
-    
-    ir.close();
-    CheckIndex ci = new CheckIndex(dir);
-    ci.checkIndex(null);
-    dir.close();
-  }
-}
diff --git a/lucene/module-build.xml b/lucene/module-build.xml
index 233a0ef..fd16a92 100644
--- a/lucene/module-build.xml
+++ b/lucene/module-build.xml
@@ -40,6 +40,7 @@
   <path id="classpath" refid="base.classpath"/>
   
   <path id="test.base.classpath">
+    <pathelement location="${common.dir}/build/codecs/classes/java"/>
     <pathelement location="${common.dir}/build/test-framework/classes/java"/>
     <path refid="classpath"/>
     <path refid="junit-path"/>
@@ -357,6 +358,28 @@
     <property name="analyzers-morfologik-javadocs.uptodate" value="true"/>
   </target>
 
+  <property name="codecs.jar" value="${common.dir}/build/codecs/lucene-codecs-${version}.jar"/>
+  <target name="check-codecs-uptodate" unless="codecs.uptodate">
+    <module-uptodate name="codecs" jarfile="${codecs.jar}" property="codecs.uptodate"/>
+  </target>
+  <target name="jar-codecs" unless="codecs.uptodate" depends="check-codecs-uptodate">
+    <ant dir="${common.dir}/codecs" target="jar-core" inheritall="false">
+      <propertyset refid="uptodate.and.compiled.properties"/>
+    </ant>
+    <property name="codecs.uptodate" value="true"/>
+  </target>
+
+  <property name="codecs-javadoc.jar" value="${common.dir}/build/codecs/lucene-codecs-${version}-javadoc.jar"/>
+  <target name="check-codecs-javadocs-uptodate" unless="codecs-javadocs.uptodate">
+    <module-uptodate name="codecs" jarfile="${codecs-javadoc.jar}" property="codecs-javadocs.uptodate"/>
+  </target>
+  <target name="javadocs-codecs" unless="codecs-javadocs.uptodate" depends="check-codecs-javadocs-uptodate">
+    <ant dir="${common.dir}/codecs" target="javadocs" inheritAll="false">
+      <propertyset refid="uptodate.and.compiled.properties"/>
+    </ant>
+    <property name="codecs-javadocs.uptodate" value="true"/>
+  </target>
+
   <property name="grouping.jar" value="${common.dir}/build/grouping/lucene-grouping-${version}.jar"/>
   <target name="check-grouping-uptodate" unless="grouping.uptodate">
     <module-uptodate name="grouping" jarfile="${grouping.jar}" property="grouping.uptodate"/>
diff --git a/lucene/test-framework/build.xml b/lucene/test-framework/build.xml
index ed624cb..26c9641 100644
--- a/lucene/test-framework/build.xml
+++ b/lucene/test-framework/build.xml
@@ -26,6 +26,7 @@
 
   <path id="classpath">
     <pathelement location="${common.dir}/build/core/classes/java"/>
+    <pathelement location="${common.dir}/build/codecs/classes/java"/>
     <path refid="junit-path"/>
     <path refid="ant-path"/>
   </path>
@@ -35,7 +36,7 @@
       and *not* to depend on clover; clover already includes the
       test-framework sources in each module's test instrumentation.
    -->
-  <target name="compile-core" depends="init,compile-lucene-core"
+  <target name="compile-core" depends="init,compile-lucene-core,compile-codecs"
           description="Compiles test-framework classes">
     <compile srcdir="${src.dir}" destdir="${build.dir}/classes/java">
       <classpath refid="classpath"/>

