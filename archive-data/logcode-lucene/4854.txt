GitDiffStart: d439254a5a66bd58cc4c99efa0b5c5e19737bfc6 | Fri Nov 28 13:38:13 2014 +0000
diff --git a/lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources.java b/lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources.java
index 42f568b..6a3dcf2 100644
--- a/lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources.java
+++ b/lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources.java
@@ -123,11 +123,11 @@ public class TokenSources {
 
     if (!tpv.hasOffsets()) {
       throw new IllegalArgumentException("Highlighting requires offsets from the TokenStream.");
-      //TokenStreamFromTermPositionVector can handle a lack of offsets if there are positions. But
+      //TokenStreamFromTermVector can handle a lack of offsets if there are positions. But
       // highlighters require offsets, so we insist here.
     }
 
-    return new TokenStreamFromTermPositionVector(tpv);
+    return new TokenStreamFromTermVector(tpv);
   }
 
   /**
diff --git a/lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenStreamFromTermPositionVector.java b/lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenStreamFromTermPositionVector.java
deleted file mode 100644
index 982ce4a..0000000
--- a/lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenStreamFromTermPositionVector.java
+++ /dev/null
@@ -1,283 +0,0 @@
-package org.apache.lucene.search.highlight;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-import java.io.IOException;
-
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
-import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
-import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
-import org.apache.lucene.index.DocsAndPositionsEnum;
-import org.apache.lucene.index.Terms;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.UnicodeUtil;
-
-/**
- * TokenStream created from a term vector field. The term vector requires positions and/or offsets (either). If you
- * want payloads add PayloadAttributeImpl (as you would normally) but don't assume the attribute is already added just
- * because you know the term vector has payloads.  This TokenStream supports an efficient {@link #reset()}, so there's
- * no need to wrap with a caching impl.
- * <p />
- * The implementation will create an array of tokens indexed by token position.  As long as there aren't massive jumps
- * in positions, this is fine.  And it assumes there aren't large numbers of tokens at the same position, since it adds
- * them to a linked-list per position in O(N^2) complexity.  When there aren't positions in the term vector, it divides
- * the startOffset by 8 to use as a temporary substitute. In that case, tokens with the same startOffset will occupy
- * the same final position; otherwise tokens become adjacent.
- *
- * @lucene.internal
- */
-//TODO rename to TokenStreamFromTermVector
-public final class TokenStreamFromTermPositionVector extends TokenStream {
-
-  //TODO add a maxStartOffset filter, which highlighters will find handy
-
-  private final Terms vector;
-
-  private final CharTermAttribute termAttribute;
-
-  private final PositionIncrementAttribute positionIncrementAttribute;
-
-  private OffsetAttribute offsetAttribute;//maybe null
-
-  private PayloadAttribute payloadAttribute;//maybe null
-
-  private TokenLL firstToken = null; // the head of a linked-list
-
-  private TokenLL incrementToken = null;
-
-  /**
-   * Constructor.
-   * 
-   * @param vector Terms that contains the data for
-   *        creating the TokenStream. Must have positions and/or offsets.
-   */
-  public TokenStreamFromTermPositionVector(Terms vector) throws IOException {
-    if (!vector.hasPositions() && !vector.hasOffsets()) {
-      throw new IllegalArgumentException("The term vector needs positions and/or offsets.");
-    }
-    assert vector.hasFreqs();
-    this.vector = vector;
-    termAttribute = addAttribute(CharTermAttribute.class);
-    positionIncrementAttribute = addAttribute(PositionIncrementAttribute.class);
-  }
-
-  public Terms getTermVectorTerms() { return vector; }
-
-  @Override
-  public void reset() throws IOException {
-    if (firstToken == null) {//just the first time
-      init();
-    }
-    incrementToken = null;
-    super.reset();
-  }
-
-  //We initialize in reset() because we can see which attributes the consumer wants, particularly payloads
-  private void init() throws IOException {
-    if (vector.hasOffsets()) {
-      offsetAttribute = addAttribute(OffsetAttribute.class);
-    }
-    if (vector.hasPayloads() && hasAttribute(PayloadAttribute.class)) {
-      payloadAttribute = getAttribute(PayloadAttribute.class);
-    }
-
-    // Step 1: iterate termsEnum and create a token, placing into an array of tokens by position
-
-    TokenLL[] positionedTokens = initTokensArray();
-
-    int lastPosition = -1;
-
-    final TermsEnum termsEnum = vector.iterator(null);
-    BytesRef termBytesRef;
-    DocsAndPositionsEnum dpEnum = null;
-    //int sumFreq = 0;
-    while ((termBytesRef = termsEnum.next()) != null) {
-      //Grab the term (in same way as BytesRef.utf8ToString() but we don't want a String obj)
-      // note: if term vectors supported seek by ord then we might just keep an int and seek by ord on-demand
-      final char[] termChars = new char[termBytesRef.length];
-      final int termCharsLen = UnicodeUtil.UTF8toUTF16(termBytesRef, termChars);
-
-      dpEnum = termsEnum.docsAndPositions(null, dpEnum);
-      assert dpEnum != null; // presumably checked by TokenSources.hasPositions earlier
-      dpEnum.nextDoc();
-      final int freq = dpEnum.freq();
-      //sumFreq += freq;
-      for (int j = 0; j < freq; j++) {
-        int pos = dpEnum.nextPosition();
-        TokenLL token = new TokenLL();
-        token.termChars = termChars;
-        token.termCharsLen = termCharsLen;
-        if (offsetAttribute != null) {
-          token.startOffset = dpEnum.startOffset();
-          token.endOffset = dpEnum.endOffset();
-          if (pos == -1) {
-            pos = token.startOffset >> 3;//divide by 8
-          }
-        }
-
-        if (payloadAttribute != null) {
-          // Must make a deep copy of the returned payload,
-          // since D&PEnum API is allowed to re-use on every
-          // call:
-          final BytesRef payload = dpEnum.getPayload();
-          if (payload != null) {
-            token.payload = BytesRef.deepCopyOf(payload);//TODO share a ByteBlockPool & re-use BytesRef
-          }
-        }
-
-        //Add token to an array indexed by position
-        if (positionedTokens.length <= pos) {
-          //grow, but not 2x since we think our original length estimate is close
-          TokenLL[] newPositionedTokens = new TokenLL[(int)((pos + 1) * 1.5f)];
-          System.arraycopy(positionedTokens, 0, newPositionedTokens, 0, lastPosition + 1);
-          positionedTokens = newPositionedTokens;
-        }
-        positionedTokens[pos] = token.insertIntoSortedLinkedList(positionedTokens[pos]);
-
-        lastPosition = Math.max(lastPosition, pos);
-      }
-    }
-
-//    System.out.println(String.format(
-//        "SumFreq: %5d Size: %4d SumFreq/size: %3.3f MaxPos: %4d MaxPos/SumFreq: %3.3f WastePct: %3.3f",
-//        sumFreq, vector.size(), (sumFreq / (float)vector.size()), lastPosition, ((float)lastPosition)/sumFreq,
-//        (originalPositionEstimate/(lastPosition + 1.0f))));
-
-    // Step 2:  Link all Tokens into a linked-list and set position increments as we go
-
-    int prevTokenPos = -1;
-    TokenLL prevToken = null;
-    for (int pos = 0; pos <= lastPosition; pos++) {
-      TokenLL token = positionedTokens[pos];
-      if (token == null) {
-        continue;
-      }
-      //link
-      if (prevToken != null) {
-        assert prevToken.next == null;
-        prevToken.next = token; //concatenate linked-list
-      } else {
-        assert firstToken == null;
-        firstToken = token;
-      }
-      //set increments
-      if (vector.hasPositions()) {
-        token.positionIncrement = pos - prevTokenPos;
-        while (token.next != null) {
-          token = token.next;
-          token.positionIncrement = 0;
-        }
-      } else {
-        token.positionIncrement = 1;
-        while (token.next != null) {
-          prevToken = token;
-          token = token.next;
-          if (prevToken.startOffset == token.startOffset) {
-            token.positionIncrement = 0;
-          } else {
-            token.positionIncrement = 1;
-          }
-        }
-      }
-      prevTokenPos = pos;
-      prevToken = token;
-    }
-  }
-
-  private TokenLL[] initTokensArray() throws IOException {
-    // Estimate the number of position slots we need. We use some estimation factors taken from Wikipedia
-    //  that reduce the likelihood of needing to expand the array.
-    int sumTotalTermFreq = (int) vector.getSumTotalTermFreq();
-    if (sumTotalTermFreq == -1) {//unfortunately term vectors seem to not have this stat
-      int size = (int) vector.size();
-      if (size == -1) {//doesn't happen with term vectors, it seems, but pick a default any way
-        size = 128;
-      }
-      sumTotalTermFreq = (int)(size * 2.4);
-    }
-    final int originalPositionEstimate = (int) (sumTotalTermFreq * 1.5);//less than 1 in 10 docs exceed this
-    return new TokenLL[originalPositionEstimate];
-  }
-
-  @Override
-  public boolean incrementToken() {
-    if (incrementToken == null) {
-      incrementToken = firstToken;
-      if (incrementToken == null) {
-        return false;
-      }
-    } else if (incrementToken.next != null) {
-      incrementToken = incrementToken.next;
-    } else {
-      return false;
-    }
-    clearAttributes();
-    termAttribute.copyBuffer(incrementToken.termChars, 0, incrementToken.termCharsLen);
-    positionIncrementAttribute.setPositionIncrement(incrementToken.positionIncrement);
-    if (offsetAttribute != null) {
-      offsetAttribute.setOffset(incrementToken.startOffset, incrementToken.endOffset);
-    }
-    if (payloadAttribute != null) {
-      payloadAttribute.setPayload(incrementToken.payload);
-    }
-    return true;
-  }
-
-  private static class TokenLL {
-    char[] termChars;
-    int termCharsLen;
-    int positionIncrement;
-    int startOffset;
-    int endOffset;
-    BytesRef payload;
-
-    TokenLL next;
-
-    /** Given the head of a linked-list (possibly null) this inserts the token at the correct
-     * spot to maintain the desired order, and returns the head (which could be this token if it's the smallest).
-     * O(N^2) complexity but N should be a handful at most.
-     */
-    TokenLL insertIntoSortedLinkedList(final TokenLL head) {
-      assert next == null;
-      if (head == null) {
-        return this;
-      } else if (this.compareOffsets(head) <= 0) {
-        this.next = head;
-        return this;
-      }
-      TokenLL prev = head;
-      while (prev.next != null && this.compareOffsets(prev.next) > 0) {
-        prev = prev.next;
-      }
-      this.next = prev.next;
-      prev.next = this;
-      return head;
-    }
-
-    /** by startOffset then endOffset */
-    int compareOffsets(TokenLL tokenB) {
-      int cmp = Integer.compare(this.startOffset, tokenB.startOffset);
-      if (cmp == 0) {
-        cmp = Integer.compare(this.endOffset, tokenB.endOffset);
-      }
-      return cmp;
-    }
-  }
-}
diff --git a/lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenStreamFromTermVector.java b/lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenStreamFromTermVector.java
new file mode 100644
index 0000000..2787dee
--- /dev/null
+++ b/lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenStreamFromTermVector.java
@@ -0,0 +1,282 @@
+package org.apache.lucene.search.highlight;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+import java.io.IOException;
+
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
+import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;
+import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
+import org.apache.lucene.index.DocsAndPositionsEnum;
+import org.apache.lucene.index.Terms;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.UnicodeUtil;
+
+/**
+ * TokenStream created from a term vector field. The term vector requires positions and/or offsets (either). If you
+ * want payloads add PayloadAttributeImpl (as you would normally) but don't assume the attribute is already added just
+ * because you know the term vector has payloads.  This TokenStream supports an efficient {@link #reset()}, so there's
+ * no need to wrap with a caching impl.
+ * <p />
+ * The implementation will create an array of tokens indexed by token position.  As long as there aren't massive jumps
+ * in positions, this is fine.  And it assumes there aren't large numbers of tokens at the same position, since it adds
+ * them to a linked-list per position in O(N^2) complexity.  When there aren't positions in the term vector, it divides
+ * the startOffset by 8 to use as a temporary substitute. In that case, tokens with the same startOffset will occupy
+ * the same final position; otherwise tokens become adjacent.
+ *
+ * @lucene.internal
+ */
+public final class TokenStreamFromTermVector extends TokenStream {
+
+  //TODO add a maxStartOffset filter, which highlighters will find handy
+
+  private final Terms vector;
+
+  private final CharTermAttribute termAttribute;
+
+  private final PositionIncrementAttribute positionIncrementAttribute;
+
+  private OffsetAttribute offsetAttribute;//maybe null
+
+  private PayloadAttribute payloadAttribute;//maybe null
+
+  private TokenLL firstToken = null; // the head of a linked-list
+
+  private TokenLL incrementToken = null;
+
+  /**
+   * Constructor.
+   * 
+   * @param vector Terms that contains the data for
+   *        creating the TokenStream. Must have positions and/or offsets.
+   */
+  public TokenStreamFromTermVector(Terms vector) throws IOException {
+    if (!vector.hasPositions() && !vector.hasOffsets()) {
+      throw new IllegalArgumentException("The term vector needs positions and/or offsets.");
+    }
+    assert vector.hasFreqs();
+    this.vector = vector;
+    termAttribute = addAttribute(CharTermAttribute.class);
+    positionIncrementAttribute = addAttribute(PositionIncrementAttribute.class);
+  }
+
+  public Terms getTermVectorTerms() { return vector; }
+
+  @Override
+  public void reset() throws IOException {
+    if (firstToken == null) {//just the first time
+      init();
+    }
+    incrementToken = null;
+    super.reset();
+  }
+
+  //We initialize in reset() because we can see which attributes the consumer wants, particularly payloads
+  private void init() throws IOException {
+    if (vector.hasOffsets()) {
+      offsetAttribute = addAttribute(OffsetAttribute.class);
+    }
+    if (vector.hasPayloads() && hasAttribute(PayloadAttribute.class)) {
+      payloadAttribute = getAttribute(PayloadAttribute.class);
+    }
+
+    // Step 1: iterate termsEnum and create a token, placing into an array of tokens by position
+
+    TokenLL[] positionedTokens = initTokensArray();
+
+    int lastPosition = -1;
+
+    final TermsEnum termsEnum = vector.iterator(null);
+    BytesRef termBytesRef;
+    DocsAndPositionsEnum dpEnum = null;
+    //int sumFreq = 0;
+    while ((termBytesRef = termsEnum.next()) != null) {
+      //Grab the term (in same way as BytesRef.utf8ToString() but we don't want a String obj)
+      // note: if term vectors supported seek by ord then we might just keep an int and seek by ord on-demand
+      final char[] termChars = new char[termBytesRef.length];
+      final int termCharsLen = UnicodeUtil.UTF8toUTF16(termBytesRef, termChars);
+
+      dpEnum = termsEnum.docsAndPositions(null, dpEnum);
+      assert dpEnum != null; // presumably checked by TokenSources.hasPositions earlier
+      dpEnum.nextDoc();
+      final int freq = dpEnum.freq();
+      //sumFreq += freq;
+      for (int j = 0; j < freq; j++) {
+        int pos = dpEnum.nextPosition();
+        TokenLL token = new TokenLL();
+        token.termChars = termChars;
+        token.termCharsLen = termCharsLen;
+        if (offsetAttribute != null) {
+          token.startOffset = dpEnum.startOffset();
+          token.endOffset = dpEnum.endOffset();
+          if (pos == -1) {
+            pos = token.startOffset >> 3;//divide by 8
+          }
+        }
+
+        if (payloadAttribute != null) {
+          // Must make a deep copy of the returned payload,
+          // since D&PEnum API is allowed to re-use on every
+          // call:
+          final BytesRef payload = dpEnum.getPayload();
+          if (payload != null) {
+            token.payload = BytesRef.deepCopyOf(payload);//TODO share a ByteBlockPool & re-use BytesRef
+          }
+        }
+
+        //Add token to an array indexed by position
+        if (positionedTokens.length <= pos) {
+          //grow, but not 2x since we think our original length estimate is close
+          TokenLL[] newPositionedTokens = new TokenLL[(int)((pos + 1) * 1.5f)];
+          System.arraycopy(positionedTokens, 0, newPositionedTokens, 0, lastPosition + 1);
+          positionedTokens = newPositionedTokens;
+        }
+        positionedTokens[pos] = token.insertIntoSortedLinkedList(positionedTokens[pos]);
+
+        lastPosition = Math.max(lastPosition, pos);
+      }
+    }
+
+//    System.out.println(String.format(
+//        "SumFreq: %5d Size: %4d SumFreq/size: %3.3f MaxPos: %4d MaxPos/SumFreq: %3.3f WastePct: %3.3f",
+//        sumFreq, vector.size(), (sumFreq / (float)vector.size()), lastPosition, ((float)lastPosition)/sumFreq,
+//        (originalPositionEstimate/(lastPosition + 1.0f))));
+
+    // Step 2:  Link all Tokens into a linked-list and set position increments as we go
+
+    int prevTokenPos = -1;
+    TokenLL prevToken = null;
+    for (int pos = 0; pos <= lastPosition; pos++) {
+      TokenLL token = positionedTokens[pos];
+      if (token == null) {
+        continue;
+      }
+      //link
+      if (prevToken != null) {
+        assert prevToken.next == null;
+        prevToken.next = token; //concatenate linked-list
+      } else {
+        assert firstToken == null;
+        firstToken = token;
+      }
+      //set increments
+      if (vector.hasPositions()) {
+        token.positionIncrement = pos - prevTokenPos;
+        while (token.next != null) {
+          token = token.next;
+          token.positionIncrement = 0;
+        }
+      } else {
+        token.positionIncrement = 1;
+        while (token.next != null) {
+          prevToken = token;
+          token = token.next;
+          if (prevToken.startOffset == token.startOffset) {
+            token.positionIncrement = 0;
+          } else {
+            token.positionIncrement = 1;
+          }
+        }
+      }
+      prevTokenPos = pos;
+      prevToken = token;
+    }
+  }
+
+  private TokenLL[] initTokensArray() throws IOException {
+    // Estimate the number of position slots we need. We use some estimation factors taken from Wikipedia
+    //  that reduce the likelihood of needing to expand the array.
+    int sumTotalTermFreq = (int) vector.getSumTotalTermFreq();
+    if (sumTotalTermFreq == -1) {//unfortunately term vectors seem to not have this stat
+      int size = (int) vector.size();
+      if (size == -1) {//doesn't happen with term vectors, it seems, but pick a default any way
+        size = 128;
+      }
+      sumTotalTermFreq = (int)(size * 2.4);
+    }
+    final int originalPositionEstimate = (int) (sumTotalTermFreq * 1.5);//less than 1 in 10 docs exceed this
+    return new TokenLL[originalPositionEstimate];
+  }
+
+  @Override
+  public boolean incrementToken() {
+    if (incrementToken == null) {
+      incrementToken = firstToken;
+      if (incrementToken == null) {
+        return false;
+      }
+    } else if (incrementToken.next != null) {
+      incrementToken = incrementToken.next;
+    } else {
+      return false;
+    }
+    clearAttributes();
+    termAttribute.copyBuffer(incrementToken.termChars, 0, incrementToken.termCharsLen);
+    positionIncrementAttribute.setPositionIncrement(incrementToken.positionIncrement);
+    if (offsetAttribute != null) {
+      offsetAttribute.setOffset(incrementToken.startOffset, incrementToken.endOffset);
+    }
+    if (payloadAttribute != null) {
+      payloadAttribute.setPayload(incrementToken.payload);
+    }
+    return true;
+  }
+
+  private static class TokenLL {
+    char[] termChars;
+    int termCharsLen;
+    int positionIncrement;
+    int startOffset;
+    int endOffset;
+    BytesRef payload;
+
+    TokenLL next;
+
+    /** Given the head of a linked-list (possibly null) this inserts the token at the correct
+     * spot to maintain the desired order, and returns the head (which could be this token if it's the smallest).
+     * O(N^2) complexity but N should be a handful at most.
+     */
+    TokenLL insertIntoSortedLinkedList(final TokenLL head) {
+      assert next == null;
+      if (head == null) {
+        return this;
+      } else if (this.compareOffsets(head) <= 0) {
+        this.next = head;
+        return this;
+      }
+      TokenLL prev = head;
+      while (prev.next != null && this.compareOffsets(prev.next) > 0) {
+        prev = prev.next;
+      }
+      this.next = prev.next;
+      prev.next = this;
+      return head;
+    }
+
+    /** by startOffset then endOffset */
+    int compareOffsets(TokenLL tokenB) {
+      int cmp = Integer.compare(this.startOffset, tokenB.startOffset);
+      if (cmp == 0) {
+        cmp = Integer.compare(this.endOffset, tokenB.endOffset);
+      }
+      return cmp;
+    }
+  }
+}

