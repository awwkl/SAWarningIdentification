GitDiffStart: 6fa480a5edec45a0b398b8fe8649ecaeb1bc2a9d | Tue May 4 09:55:43 2010 +0000
diff --git a/lucene/contrib/CHANGES.txt b/lucene/contrib/CHANGES.txt
index ec3914b..a7df800 100644
--- a/lucene/contrib/CHANGES.txt
+++ b/lucene/contrib/CHANGES.txt
@@ -163,6 +163,8 @@ New features
      constructs.
    - o.a.l.analysis.miscellaneous.WordDelimiterFilter: TokenFilter that splits words 
      into subwords and performs optional transformations on subword groups.
+   - o.a.l.analysis.miscellaneous.RemoveDuplicatesTokenFilter: TokenFilter which 
+     filters out Tokens at the same position and Term text as the previous token.
    (... in progress)
 
 Build
diff --git a/lucene/contrib/analyzers/common/src/java/org/apache/lucene/analysis/miscellaneous/RemoveDuplicatesTokenFilter.java b/lucene/contrib/analyzers/common/src/java/org/apache/lucene/analysis/miscellaneous/RemoveDuplicatesTokenFilter.java
new file mode 100644
index 0000000..200ae0a
--- /dev/null
+++ b/lucene/contrib/analyzers/common/src/java/org/apache/lucene/analysis/miscellaneous/RemoveDuplicatesTokenFilter.java
@@ -0,0 +1,85 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.lucene.analysis.miscellaneous;
+
+import org.apache.lucene.analysis.CharArraySet;
+import org.apache.lucene.analysis.TokenFilter;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
+import org.apache.lucene.util.Version;
+
+import java.io.IOException;
+
+/**
+ * A TokenFilter which filters out Tokens at the same position and Term text as the previous token in the stream.
+ */
+public final class RemoveDuplicatesTokenFilter extends TokenFilter {
+
+  private final CharTermAttribute termAttribute = addAttribute(CharTermAttribute.class);
+  private final PositionIncrementAttribute posIncAttribute =  addAttribute(PositionIncrementAttribute.class);
+  
+  // use a fixed version, as we don't care about case sensitivity.
+  private final CharArraySet previous = new CharArraySet(Version.LUCENE_31, 8, false);
+
+  /**
+   * Creates a new RemoveDuplicatesTokenFilter
+   *
+   * @param in TokenStream that will be filtered
+   */
+  public RemoveDuplicatesTokenFilter(TokenStream in) {
+    super(in);
+  }
+
+  /**
+   * {@inheritDoc}
+   */
+  @Override
+  public boolean incrementToken() throws IOException {
+    while (input.incrementToken()) {
+      final char term[] = termAttribute.buffer();
+      final int length = termAttribute.length();
+      final int posIncrement = posIncAttribute.getPositionIncrement();
+      
+      if (posIncrement > 0) {
+        previous.clear();
+      }
+      
+      boolean duplicate = (posIncrement == 0 && previous.contains(term, 0, length));
+      
+      // clone the term, and add to the set of seen terms.
+      char saved[] = new char[length];
+      System.arraycopy(term, 0, saved, 0, length);
+      previous.add(saved);
+      
+      if (!duplicate) {
+        return true;
+      }
+    }
+    return false;
+  }
+
+  /**
+   * {@inheritDoc}
+   */
+  @Override
+  public void reset() throws IOException {
+    super.reset();
+    previous.clear();
+  }
+} 
diff --git a/lucene/contrib/analyzers/common/src/test/org/apache/lucene/analysis/miscellaneous/TestRemoveDuplicatesTokenFilter.java b/lucene/contrib/analyzers/common/src/test/org/apache/lucene/analysis/miscellaneous/TestRemoveDuplicatesTokenFilter.java
new file mode 100644
index 0000000..75b8b88
--- /dev/null
+++ b/lucene/contrib/analyzers/common/src/test/org/apache/lucene/analysis/miscellaneous/TestRemoveDuplicatesTokenFilter.java
@@ -0,0 +1,120 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.lucene.analysis.miscellaneous;
+
+import org.apache.lucene.analysis.BaseTokenStreamTestCase;
+import org.apache.lucene.analysis.Token;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
+import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
+import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+
+import java.util.Iterator;
+import java.util.Arrays;
+
+public class TestRemoveDuplicatesTokenFilter extends BaseTokenStreamTestCase {
+
+  public static Token tok(int pos, String t, int start, int end) {
+    Token tok = new Token(t,start,end);
+    tok.setPositionIncrement(pos);
+    return tok;
+  }
+  public static Token tok(int pos, String t) {
+    return tok(pos, t, 0,0);
+  }
+
+  public void testDups(final String expected, final Token... tokens)
+    throws Exception {
+
+    final Iterator<Token> toks = Arrays.asList(tokens).iterator();
+    final TokenStream ts = new RemoveDuplicatesTokenFilter(
+      (new TokenStream() {
+          CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
+          OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);
+          PositionIncrementAttribute posIncAtt = addAttribute(PositionIncrementAttribute.class);
+          public boolean incrementToken() {
+            if (toks.hasNext()) {
+              clearAttributes();
+              Token tok = toks.next();
+              termAtt.setEmpty().append(tok.term());
+              offsetAtt.setOffset(tok.startOffset(), tok.endOffset());
+              posIncAtt.setPositionIncrement(tok.getPositionIncrement());
+              return true;
+            } else {
+              return false;
+            }
+          }
+        }));
+    
+    assertTokenStreamContents(ts, expected.split("\\s"));   
+  }
+  
+  public void testNoDups() throws Exception {
+
+    testDups("A B B C D E"
+             ,tok(1,"A", 0,  4)
+             ,tok(1,"B", 5, 10)
+             ,tok(1,"B",11, 15)
+             ,tok(1,"C",16, 20)
+             ,tok(0,"D",16, 20)
+             ,tok(1,"E",21, 25)
+             );
+    
+  }
+  
+
+  public void testSimpleDups() throws Exception {
+
+    testDups("A B C D E"
+             ,tok(1,"A", 0,  4)
+             ,tok(1,"B", 5, 10)
+             ,tok(0,"B",11, 15)
+             ,tok(1,"C",16, 20)
+             ,tok(0,"D",16, 20)
+             ,tok(1,"E",21, 25)
+             );
+    
+  }
+  
+  public void testComplexDups() throws Exception {
+
+    testDups("A B C D E F G H I J K"
+             ,tok(1,"A")
+             ,tok(1,"B")
+             ,tok(0,"B")
+             ,tok(1,"C")
+             ,tok(1,"D")
+             ,tok(0,"D")
+             ,tok(0,"D")
+             ,tok(1,"E")
+             ,tok(1,"F")
+             ,tok(0,"F")
+             ,tok(1,"G")
+             ,tok(0,"H")
+             ,tok(0,"H")
+             ,tok(1,"I")
+             ,tok(1,"J")
+             ,tok(0,"K")
+             ,tok(0,"J")
+             );
+             
+  }
+  
+  
+
+}
diff --git a/solr/src/java/org/apache/solr/analysis/RemoveDuplicatesTokenFilter.java b/solr/src/java/org/apache/solr/analysis/RemoveDuplicatesTokenFilter.java
deleted file mode 100644
index 2978115..0000000
--- a/solr/src/java/org/apache/solr/analysis/RemoveDuplicatesTokenFilter.java
+++ /dev/null
@@ -1,85 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.solr.analysis;
-
-import org.apache.lucene.analysis.CharArraySet;
-import org.apache.lucene.analysis.TokenFilter;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
-import org.apache.lucene.util.Version;
-
-import java.io.IOException;
-
-/**
- * A TokenFilter which filters out Tokens at the same position and Term text as the previous token in the stream.
- */
-public final class RemoveDuplicatesTokenFilter extends TokenFilter {
-
-  private final CharTermAttribute termAttribute = addAttribute(CharTermAttribute.class);
-  private final PositionIncrementAttribute posIncAttribute =  addAttribute(PositionIncrementAttribute.class);
-  
-  // use a fixed version, as we don't care about case sensitivity.
-  private final CharArraySet previous = new CharArraySet(Version.LUCENE_31, 8, false);
-
-  /**
-   * Creates a new RemoveDuplicatesTokenFilter
-   *
-   * @param in TokenStream that will be filtered
-   */
-  public RemoveDuplicatesTokenFilter(TokenStream in) {
-    super(in);
-  }
-
-  /**
-   * {@inheritDoc}
-   */
-  @Override
-  public boolean incrementToken() throws IOException {
-    while (input.incrementToken()) {
-      final char term[] = termAttribute.buffer();
-      final int length = termAttribute.length();
-      final int posIncrement = posIncAttribute.getPositionIncrement();
-      
-      if (posIncrement > 0) {
-        previous.clear();
-      }
-      
-      boolean duplicate = (posIncrement == 0 && previous.contains(term, 0, length));
-      
-      // clone the term, and add to the set of seen terms.
-      char saved[] = new char[length];
-      System.arraycopy(term, 0, saved, 0, length);
-      previous.add(saved);
-      
-      if (!duplicate) {
-        return true;
-      }
-    }
-    return false;
-  }
-
-  /**
-   * {@inheritDoc}
-   */
-  @Override
-  public void reset() throws IOException {
-    super.reset();
-    previous.clear();
-  }
-} 
diff --git a/solr/src/java/org/apache/solr/analysis/RemoveDuplicatesTokenFilterFactory.java b/solr/src/java/org/apache/solr/analysis/RemoveDuplicatesTokenFilterFactory.java
index 202eb0e..a201755 100644
--- a/solr/src/java/org/apache/solr/analysis/RemoveDuplicatesTokenFilterFactory.java
+++ b/solr/src/java/org/apache/solr/analysis/RemoveDuplicatesTokenFilterFactory.java
@@ -18,6 +18,7 @@
 package org.apache.solr.analysis;
 
 import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.miscellaneous.RemoveDuplicatesTokenFilter;
 
 /**
  * @version $Id:$
diff --git a/solr/src/test/org/apache/solr/analysis/TestRemoveDuplicatesTokenFilter.java b/solr/src/test/org/apache/solr/analysis/TestRemoveDuplicatesTokenFilter.java
deleted file mode 100644
index 5a51117..0000000
--- a/solr/src/test/org/apache/solr/analysis/TestRemoveDuplicatesTokenFilter.java
+++ /dev/null
@@ -1,120 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.solr.analysis;
-
-import org.apache.lucene.analysis.Token;
-import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
-import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-
-import java.util.Iterator;
-import java.util.Arrays;
-
-public class TestRemoveDuplicatesTokenFilter extends BaseTokenTestCase {
-
-  public static Token tok(int pos, String t, int start, int end) {
-    Token tok = new Token(t,start,end);
-    tok.setPositionIncrement(pos);
-    return tok;
-  }
-  public static Token tok(int pos, String t) {
-    return tok(pos, t, 0,0);
-  }
-
-  public void testDups(final String expected, final Token... tokens)
-    throws Exception {
-
-    final Iterator<Token> toks = Arrays.asList(tokens).iterator();
-    RemoveDuplicatesTokenFilterFactory factory = new RemoveDuplicatesTokenFilterFactory();
-    final TokenStream ts = factory.create
-      (new TokenStream() {
-          CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
-          OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);
-          PositionIncrementAttribute posIncAtt = addAttribute(PositionIncrementAttribute.class);
-          public boolean incrementToken() {
-            if (toks.hasNext()) {
-              clearAttributes();
-              Token tok = toks.next();
-              termAtt.setEmpty().append(tok.term());
-              offsetAtt.setOffset(tok.startOffset(), tok.endOffset());
-              posIncAtt.setPositionIncrement(tok.getPositionIncrement());
-              return true;
-            } else {
-              return false;
-            }
-          }
-        });
-    
-    assertTokenStreamContents(ts, expected.split("\\s"));   
-  }
-  
-  public void testNoDups() throws Exception {
-
-    testDups("A B B C D E"
-             ,tok(1,"A", 0,  4)
-             ,tok(1,"B", 5, 10)
-             ,tok(1,"B",11, 15)
-             ,tok(1,"C",16, 20)
-             ,tok(0,"D",16, 20)
-             ,tok(1,"E",21, 25)
-             );
-    
-  }
-  
-
-  public void testSimpleDups() throws Exception {
-
-    testDups("A B C D E"
-             ,tok(1,"A", 0,  4)
-             ,tok(1,"B", 5, 10)
-             ,tok(0,"B",11, 15)
-             ,tok(1,"C",16, 20)
-             ,tok(0,"D",16, 20)
-             ,tok(1,"E",21, 25)
-             );
-    
-  }
-  
-  public void testComplexDups() throws Exception {
-
-    testDups("A B C D E F G H I J K"
-             ,tok(1,"A")
-             ,tok(1,"B")
-             ,tok(0,"B")
-             ,tok(1,"C")
-             ,tok(1,"D")
-             ,tok(0,"D")
-             ,tok(0,"D")
-             ,tok(1,"E")
-             ,tok(1,"F")
-             ,tok(0,"F")
-             ,tok(1,"G")
-             ,tok(0,"H")
-             ,tok(0,"H")
-             ,tok(1,"I")
-             ,tok(1,"J")
-             ,tok(0,"K")
-             ,tok(0,"J")
-             );
-             
-  }
-  
-  
-
-}
diff --git a/solr/src/test/org/apache/solr/analysis/TestRemoveDuplicatesTokenFilterFactory.java b/solr/src/test/org/apache/solr/analysis/TestRemoveDuplicatesTokenFilterFactory.java
new file mode 100644
index 0000000..4a62917
--- /dev/null
+++ b/solr/src/test/org/apache/solr/analysis/TestRemoveDuplicatesTokenFilterFactory.java
@@ -0,0 +1,78 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.solr.analysis;
+
+import org.apache.lucene.analysis.Token;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;
+import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;
+import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+
+import java.util.Iterator;
+import java.util.Arrays;
+
+/** Simple tests to ensure this factory is working */
+public class TestRemoveDuplicatesTokenFilterFactory extends BaseTokenTestCase {
+
+  public static Token tok(int pos, String t, int start, int end) {
+    Token tok = new Token(t,start,end);
+    tok.setPositionIncrement(pos);
+    return tok;
+  }
+  public static Token tok(int pos, String t) {
+    return tok(pos, t, 0,0);
+  }
+
+  public void testDups(final String expected, final Token... tokens)
+    throws Exception {
+
+    final Iterator<Token> toks = Arrays.asList(tokens).iterator();
+    RemoveDuplicatesTokenFilterFactory factory = new RemoveDuplicatesTokenFilterFactory();
+    final TokenStream ts = factory.create
+      (new TokenStream() {
+          CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
+          OffsetAttribute offsetAtt = addAttribute(OffsetAttribute.class);
+          PositionIncrementAttribute posIncAtt = addAttribute(PositionIncrementAttribute.class);
+          public boolean incrementToken() {
+            if (toks.hasNext()) {
+              clearAttributes();
+              Token tok = toks.next();
+              termAtt.setEmpty().append(tok.term());
+              offsetAtt.setOffset(tok.startOffset(), tok.endOffset());
+              posIncAtt.setPositionIncrement(tok.getPositionIncrement());
+              return true;
+            } else {
+              return false;
+            }
+          }
+        });
+    
+    assertTokenStreamContents(ts, expected.split("\\s"));   
+  }
+ 
+  public void testSimpleDups() throws Exception {
+    testDups("A B C D E"
+             ,tok(1,"A", 0,  4)
+             ,tok(1,"B", 5, 10)
+             ,tok(0,"B",11, 15)
+             ,tok(1,"C",16, 20)
+             ,tok(0,"D",16, 20)
+             ,tok(1,"E",21, 25)
+             ); 
+  }
+}

