GitDiffStart: 678b1fa018c85491bcf543a0b5b524dfc73d13cf | Fri Nov 15 16:21:24 2013 +0000
diff --git a/lucene/CHANGES.txt b/lucene/CHANGES.txt
index 1aafbf7..ce7c074 100644
--- a/lucene/CHANGES.txt
+++ b/lucene/CHANGES.txt
@@ -216,6 +216,10 @@ Bug Fixes
   deleted at a later point in time. This could cause short-term disk
   pollution or OOM if in-memory directories are used. (Simon Willnauer)
 
+* LUCENE-5342: Fixed bulk-merge issue in CompressingStoredFieldsFormat which
+  created corrupted segments when mixing chunk sizes.
+  Lucene41StoredFieldsFormat is not impacted. (Adrien Grand, Robert Muir)
+
 API Changes
 
 * LUCENE-5222: Add SortField.needsScores(). Previously it was not possible
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsReader.java b/lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsReader.java
index e721efd..588c0f5 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsReader.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsReader.java
@@ -373,6 +373,10 @@ public final class CompressingStoredFieldsReader extends StoredFieldsReader {
     return compressionMode;
   }
 
+  int getChunkSize() {
+    return chunkSize;
+  }
+
   ChunkIterator chunkIterator(int startDocID) throws IOException {
     ensureOpen();
     fieldsStream.seek(indexReader.getStartPointer(startDocID));
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter.java b/lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter.java
index f016050..35f829d 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsWriter.java
@@ -337,7 +337,9 @@ public final class CompressingStoredFieldsWriter extends StoredFieldsWriter {
       final Bits liveDocs = reader.getLiveDocs();
 
       if (matchingFieldsReader == null
-          || matchingFieldsReader.getVersion() != VERSION_CURRENT) { // means reader version is not the same as the writer version
+          || matchingFieldsReader.getVersion() != VERSION_CURRENT // means reader version is not the same as the writer version
+          || matchingFieldsReader.getCompressionMode() != compressionMode
+          || matchingFieldsReader.getChunkSize() != chunkSize) { // the way data is decompressed depends on the chunk size
         // naive merge...
         for (int i = nextLiveDoc(0, liveDocs, maxDoc); i < maxDoc; i = nextLiveDoc(i + 1, liveDocs, maxDoc)) {
           StoredDocument doc = reader.document(i);
@@ -362,8 +364,7 @@ public final class CompressingStoredFieldsWriter extends StoredFieldsWriter {
               startOffsets[i] = startOffsets[i - 1] + it.lengths[i - 1];
             }
 
-            if (compressionMode == matchingFieldsReader.getCompressionMode() // same compression mode
-                && numBufferedDocs == 0 // starting a new chunk
+            if (numBufferedDocs == 0 // starting a new chunk
                 && startOffsets[it.chunkDocs - 1] < chunkSize // chunk is small enough
                 && startOffsets[it.chunkDocs - 1] + it.lengths[it.chunkDocs - 1] >= chunkSize // chunk is large enough
                 && nextDeletedDoc(it.docBase, liveDocs, it.docBase + it.chunkDocs) == it.docBase + it.chunkDocs) { // no deletion in the chunk

