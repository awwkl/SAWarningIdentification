GitDiffStart: e528549a3be113053ee976dbfeeb422f727dc515 | Wed Aug 8 18:01:11 2012 +0000
diff --git a/solr/core/src/test/org/apache/solr/cloud/AbstractDistributedZkTestCase.java b/solr/core/src/test/org/apache/solr/cloud/AbstractDistributedZkTestCase.java
deleted file mode 100644
index bc73ebc..0000000
--- a/solr/core/src/test/org/apache/solr/cloud/AbstractDistributedZkTestCase.java
+++ /dev/null
@@ -1,212 +0,0 @@
-package org.apache.solr.cloud;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.File;
-import java.util.Map;
-import java.util.concurrent.atomic.AtomicInteger;
-
-import org.apache.commons.io.FileUtils;
-import org.apache.solr.BaseDistributedSearchTestCase;
-import org.apache.solr.client.solrj.embedded.JettySolrRunner;
-import org.apache.solr.common.cloud.ClusterState;
-import org.apache.solr.common.cloud.Slice;
-import org.apache.solr.common.cloud.SolrZkClient;
-import org.apache.solr.common.cloud.ZkNodeProps;
-import org.apache.solr.common.cloud.ZkStateReader;
-import org.apache.solr.servlet.SolrDispatchFilter;
-import org.apache.zookeeper.KeeperException;
-import org.junit.After;
-import org.junit.Before;
-import org.junit.BeforeClass;
-
-public abstract class AbstractDistributedZkTestCase extends BaseDistributedSearchTestCase {
-  
-  protected static final String DEFAULT_COLLECTION = "collection1";
-  private static final boolean DEBUG = false;
-  protected ZkTestServer zkServer;
-  private AtomicInteger homeCount = new AtomicInteger();
-
-  @BeforeClass
-  public static void beforeThisClass() throws Exception {
-    useFactory(null);
-  }
-
-
-  @Before
-  @Override
-  public void setUp() throws Exception {
-    super.setUp();
-    createTempDir();
-    
-    String zkDir = testDir.getAbsolutePath() + File.separator
-    + "zookeeper/server1/data";
-    zkServer = new ZkTestServer(zkDir);
-    zkServer.run();
-    
-    System.setProperty("zkHost", zkServer.getZkAddress());
-    System.setProperty("enable.update.log", "true");
-    System.setProperty("remove.version.field", "true");
-
-
-    AbstractZkTestCase.buildZooKeeper(zkServer.getZkHost(), zkServer.getZkAddress(), "solrconfig.xml", "schema.xml");
-
-    // set some system properties for use by tests
-    System.setProperty("solr.test.sys.prop1", "propone");
-    System.setProperty("solr.test.sys.prop2", "proptwo");
-  }
-  
-  @Override
-  protected void createServers(int numShards) throws Exception {
-    // give everyone there own solrhome
-    File controlHome = new File(new File(getSolrHome()).getParentFile(), "control" + homeCount.incrementAndGet());
-    FileUtils.copyDirectory(new File(getSolrHome()), controlHome);
-    
-    System.setProperty("collection", "control_collection");
-    controlJetty = createJetty(controlHome, null, "control_shard");
-    System.clearProperty("collection");
-    controlClient = createNewSolrServer(controlJetty.getLocalPort());
-
-    StringBuilder sb = new StringBuilder();
-    for (int i = 1; i <= numShards; i++) {
-      if (sb.length() > 0) sb.append(',');
-      // give everyone there own solrhome
-      File jettyHome = new File(new File(getSolrHome()).getParentFile(), "jetty" + homeCount.incrementAndGet());
-      FileUtils.copyDirectory(new File(getSolrHome()), jettyHome);
-      JettySolrRunner j = createJetty(jettyHome, null, "shard" + (i + 2));
-      jettys.add(j);
-      clients.add(createNewSolrServer(j.getLocalPort()));
-      sb.append("localhost:").append(j.getLocalPort()).append(context);
-    }
-
-    shards = sb.toString();
-    
-    // now wait till we see the leader for each shard
-    for (int i = 1; i <= numShards; i++) {
-      ZkStateReader zkStateReader = ((SolrDispatchFilter) jettys.get(0)
-          .getDispatchFilter().getFilter()).getCores().getZkController()
-          .getZkStateReader();
-      zkStateReader.getLeaderProps("collection1", "shard" + (i + 2), 15000);
-    }
-  }
-  
-  protected void waitForRecoveriesToFinish(String collection, ZkStateReader zkStateReader, boolean verbose)
-      throws Exception {
-    waitForRecoveriesToFinish(collection, zkStateReader, verbose, true);
-  }
-  
-  protected void waitForRecoveriesToFinish(String collection, ZkStateReader zkStateReader, boolean verbose, boolean failOnTimeout)
-      throws Exception {
-    waitForRecoveriesToFinish(collection, zkStateReader, verbose, failOnTimeout, 120 * (TEST_NIGHTLY ? 2 : 1) * RANDOM_MULTIPLIER);
-  }
-  
-  protected void waitForRecoveriesToFinish(String collection,
-      ZkStateReader zkStateReader, boolean verbose, boolean failOnTimeout, int timeoutSeconds)
-      throws Exception {
-    log.info("Wait for recoveries to finish - collection: " + collection + " failOnTimeout:" + failOnTimeout + " timeout (sec):" + timeoutSeconds);
-    boolean cont = true;
-    int cnt = 0;
-    
-    while (cont) {
-      if (verbose) System.out.println("-");
-      boolean sawLiveRecovering = false;
-      zkStateReader.updateClusterState(true);
-      ClusterState clusterState = zkStateReader.getClusterState();
-      Map<String,Slice> slices = clusterState.getSlices(collection);
-      for (Map.Entry<String,Slice> entry : slices.entrySet()) {
-        Map<String,ZkNodeProps> shards = entry.getValue().getShards();
-        for (Map.Entry<String,ZkNodeProps> shard : shards.entrySet()) {
-          if (verbose) System.out.println("rstate:"
-              + shard.getValue().get(ZkStateReader.STATE_PROP)
-              + " live:"
-              + clusterState.liveNodesContain(shard.getValue().get(
-                  ZkStateReader.NODE_NAME_PROP)));
-          String state = shard.getValue().get(ZkStateReader.STATE_PROP);
-          if ((state.equals(ZkStateReader.RECOVERING) || state
-              .equals(ZkStateReader.SYNC) || state.equals(ZkStateReader.DOWN))
-              && clusterState.liveNodesContain(shard.getValue().get(
-                  ZkStateReader.NODE_NAME_PROP))) {
-            sawLiveRecovering = true;
-          }
-        }
-      }
-      if (!sawLiveRecovering || cnt == timeoutSeconds) {
-        if (!sawLiveRecovering) {
-          if (verbose) System.out.println("no one is recoverying");
-        } else {
-          if (failOnTimeout) {
-            fail("There are still nodes recoverying");
-            printLayout();
-            return;
-          }
-          if (verbose) System.out
-              .println("gave up waiting for recovery to finish..");
-        }
-        cont = false;
-      } else {
-        Thread.sleep(1000);
-      }
-      cnt++;
-    }
-  }
-
-  protected void assertAllActive(String collection,ZkStateReader zkStateReader)
-      throws KeeperException, InterruptedException {
-
-      zkStateReader.updateClusterState(true);
-      ClusterState clusterState = zkStateReader.getClusterState();
-      Map<String,Slice> slices = clusterState.getSlices(collection);
-      if (slices == null) {
-        throw new IllegalArgumentException("Cannot find collection:" + collection);
-      }
-      for (Map.Entry<String,Slice> entry : slices.entrySet()) {
-        Map<String,ZkNodeProps> shards = entry.getValue().getShards();
-        for (Map.Entry<String,ZkNodeProps> shard : shards.entrySet()) {
-
-          String state = shard.getValue().get(ZkStateReader.STATE_PROP);
-          if (!state.equals(ZkStateReader.ACTIVE)) {
-            fail("Not all shards are ACTIVE - found a shard that is: " + state);
-          }
-        }
-      }
-  }
-  
-  @Override
-  @After
-  public void tearDown() throws Exception {
-    if (DEBUG) {
-      printLayout();
-    }
-    zkServer.shutdown();
-    System.clearProperty("zkHost");
-    System.clearProperty("collection");
-    System.clearProperty("enable.update.log");
-    System.clearProperty("remove.version.field");
-    System.clearProperty("solr.directoryFactory");
-    System.clearProperty("solr.test.sys.prop1");
-    System.clearProperty("solr.test.sys.prop2");
-    resetExceptionIgnores();
-    super.tearDown();
-  }
-  
-  protected void printLayout() throws Exception {
-    SolrZkClient zkClient = new SolrZkClient(zkServer.getZkHost(), AbstractZkTestCase.TIMEOUT);
-    zkClient.printLayoutToStdOut();
-    zkClient.close();
-  }
-}
diff --git a/solr/core/src/test/org/apache/solr/cloud/AbstractZkTestCase.java b/solr/core/src/test/org/apache/solr/cloud/AbstractZkTestCase.java
deleted file mode 100644
index 150441f..0000000
--- a/solr/core/src/test/org/apache/solr/cloud/AbstractZkTestCase.java
+++ /dev/null
@@ -1,154 +0,0 @@
-package org.apache.solr.cloud;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.File;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import org.apache.solr.SolrTestCaseJ4;
-import org.apache.solr.common.cloud.SolrZkClient;
-import org.apache.solr.common.cloud.ZkNodeProps;
-import org.apache.solr.common.cloud.ZkStateReader;
-import org.apache.zookeeper.CreateMode;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-/**
- * Base test class for ZooKeeper tests.
- */
-public abstract class AbstractZkTestCase extends SolrTestCaseJ4 {
-
-  static final int TIMEOUT = 10000;
-
-  private static final boolean DEBUG = false;
-
-  protected static Logger log = LoggerFactory
-      .getLogger(AbstractZkTestCase.class);
-
-  protected static ZkTestServer zkServer;
-
-  protected static String zkDir;
-
-
-  @BeforeClass
-  public static void azt_beforeClass() throws Exception {
-    createTempDir();
-    zkDir = dataDir.getAbsolutePath() + File.separator
-        + "zookeeper/server1/data";
-    zkServer = new ZkTestServer(zkDir);
-    zkServer.run();
-    
-    System.setProperty("solrcloud.skip.autorecovery", "true");
-    System.setProperty("zkHost", zkServer.getZkAddress());
-    System.setProperty("jetty.port", "0000");
-    
-    buildZooKeeper(zkServer.getZkHost(), zkServer.getZkAddress(),
-        "solrconfig.xml", "schema.xml");
-    
-    initCore("solrconfig.xml", "schema.xml");
-  }
-
-  // static to share with distrib test
-  static void buildZooKeeper(String zkHost, String zkAddress, String config,
-      String schema) throws Exception {
-    SolrZkClient zkClient = new SolrZkClient(zkHost, AbstractZkTestCase.TIMEOUT);
-    zkClient.makePath("/solr", false, true);
-    zkClient.close();
-
-    zkClient = new SolrZkClient(zkAddress, AbstractZkTestCase.TIMEOUT);
-
-    Map<String,String> props = new HashMap<String,String>();
-    props.put("configName", "conf1");
-    final ZkNodeProps zkProps = new ZkNodeProps(props);
-    
-    zkClient.makePath("/collections/collection1", ZkStateReader.toJSON(zkProps), CreateMode.PERSISTENT, true);
-    zkClient.makePath("/collections/collection1/shards", CreateMode.PERSISTENT, true);
-    zkClient.makePath("/collections/control_collection", ZkStateReader.toJSON(zkProps), CreateMode.PERSISTENT, true);
-    zkClient.makePath("/collections/control_collection/shards", CreateMode.PERSISTENT, true);
-
-    putConfig(zkClient, config);
-    putConfig(zkClient, schema);
-    putConfig(zkClient, "solrconfig.xml");
-    putConfig(zkClient, "stopwords.txt");
-    putConfig(zkClient, "protwords.txt");
-    putConfig(zkClient, "currency.xml");
-    putConfig(zkClient, "open-exchange-rates.json");
-    putConfig(zkClient, "mapping-ISOLatin1Accent.txt");
-    putConfig(zkClient, "old_synonyms.txt");
-    putConfig(zkClient, "synonyms.txt");
-    
-    zkClient.close();
-  }
-
-  private static void putConfig(SolrZkClient zkClient, final String name)
-      throws Exception {
-    zkClient.makePath("/configs/conf1/" + name, getFile("solr" + File.separator + "collection1"
-        + File.separator + "conf" + File.separator + name), false, true);  
-  }
-
-  @Override
-  public void tearDown() throws Exception {
-    if (DEBUG) {
-      printLayout(zkServer.getZkHost());
-    }
-
-    super.tearDown();
-  }
-  
-  @AfterClass
-  public static void azt_afterClass() throws Exception {
-    System.clearProperty("zkHost");
-    System.clearProperty("solr.test.sys.prop1");
-    System.clearProperty("solr.test.sys.prop2");
-    System.clearProperty("solrcloud.skip.autorecovery");
-    System.clearProperty("jetty.port");
-
-    zkServer.shutdown();
-
-    // wait just a bit for any zk client threads to outlast timeout
-    Thread.sleep(2000);
-  }
-
-  protected void printLayout(String zkHost) throws Exception {
-    SolrZkClient zkClient = new SolrZkClient(zkHost, AbstractZkTestCase.TIMEOUT);
-    zkClient.printLayoutToStdOut();
-    zkClient.close();
-  }
-
-  public static void makeSolrZkNode(String zkHost) throws Exception {
-    SolrZkClient zkClient = new SolrZkClient(zkHost, TIMEOUT);
-    zkClient.makePath("/solr", false, true);
-    zkClient.close();
-  }
-  
-  public static void tryCleanSolrZkNode(String zkHost) throws Exception {
-    tryCleanPath(zkHost, "/solr");
-  }
-  
-  static void tryCleanPath(String zkHost, String path) throws Exception {
-    SolrZkClient zkClient = new SolrZkClient(zkHost, TIMEOUT);
-    if (zkClient.exists(path, true)) {
-      zkClient.clean(path);
-    }
-    zkClient.close();
-  }
-}
diff --git a/solr/core/src/test/org/apache/solr/cloud/BasicDistributedZk2Test.java b/solr/core/src/test/org/apache/solr/cloud/BasicDistributedZk2Test.java
new file mode 100644
index 0000000..6d329bc
--- /dev/null
+++ b/solr/core/src/test/org/apache/solr/cloud/BasicDistributedZk2Test.java
@@ -0,0 +1,355 @@
+package org.apache.solr.cloud;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.HashSet;
+import java.util.Set;
+
+import org.apache.solr.client.solrj.SolrQuery;
+import org.apache.solr.client.solrj.SolrServer;
+import org.apache.solr.client.solrj.SolrServerException;
+import org.apache.solr.client.solrj.embedded.JettySolrRunner;
+import org.apache.solr.client.solrj.request.UpdateRequest;
+import org.apache.solr.client.solrj.response.QueryResponse;
+import org.apache.solr.common.SolrException;
+import org.apache.solr.common.SolrInputDocument;
+import org.apache.solr.common.cloud.ZkNodeProps;
+import org.apache.solr.common.cloud.ZkStateReader;
+import org.apache.solr.common.params.CommonParams;
+import org.apache.solr.common.params.ModifiableSolrParams;
+import org.apache.solr.servlet.SolrDispatchFilter;
+
+/**
+ * This test simply does a bunch of basic things in solrcloud mode and asserts things
+ * work as expected.
+ */
+public class BasicDistributedZk2Test extends AbstractFullDistribZkTestBase {
+  
+  public BasicDistributedZk2Test() {
+    super();
+  }
+  
+  /*
+   * (non-Javadoc)
+   * 
+   * @see org.apache.solr.BaseDistributedSearchTestCase#doTest()
+   * 
+   * Create 3 shards, each with one replica
+   */
+  @Override
+  public void doTest() throws Exception {
+    boolean testFinished = false;
+    try {
+      handle.clear();
+      handle.put("QTime", SKIPVAL);
+      handle.put("timestamp", SKIPVAL);
+      
+      indexr(id, 1, i1, 100, tlong, 100, t1,
+          "now is the time for all good men", "foo_f", 1.414f, "foo_b", "true",
+          "foo_d", 1.414d);
+      
+      // make sure we are in a steady state...
+      waitForRecoveriesToFinish(false);
+      
+      commit();
+      
+      assertDocCounts(false);
+      
+      indexAbunchOfDocs();
+      
+      // check again 
+      waitForRecoveriesToFinish(false);
+      
+      commit();
+      
+      assertDocCounts(VERBOSE);
+      checkQueries();
+      
+      assertDocCounts(VERBOSE);
+      
+      query("q", "*:*", "sort", "n_tl1 desc");
+      
+      brindDownShardIndexSomeDocsAndRecover();
+      
+      query("q", "*:*", "sort", "n_tl1 desc");
+      
+      // test adding another replica to a shard - it should do a
+      // recovery/replication to pick up the index from the leader
+      addNewReplica();
+      
+      long docId = testUpdateAndDelete();
+      
+      // index a bad doc...
+      try {
+        indexr(t1, "a doc with no id");
+        fail("this should fail");
+      } catch (SolrException e) {
+        // expected
+      }
+      
+      // TODO: bring this to it's own method?
+      // try indexing to a leader that has no replicas up
+      ZkNodeProps leaderProps = zkStateReader.getLeaderProps(
+          DEFAULT_COLLECTION, SHARD2);
+      
+      String nodeName = leaderProps.get(ZkStateReader.NODE_NAME_PROP);
+      chaosMonkey.stopShardExcept(SHARD2, nodeName);
+      
+      SolrServer client = getClient(nodeName);
+      
+      index_specific(client, "id", docId + 1, t1, "what happens here?");
+      
+      // expire a session...
+      CloudJettyRunner cloudJetty = shardToJetty.get("shard1").get(0);
+      chaosMonkey.expireSession(cloudJetty.jetty);
+      
+      indexr("id", docId + 1, t1, "slip this doc in");
+      
+      waitForRecoveriesToFinish(false);
+      
+      checkShardConsistency("shard1");
+      
+      testFinished = true;
+    } finally {
+      if (!testFinished) {
+        printLayoutOnTearDown = true;
+      }
+    }
+    
+  }
+  
+  private long testUpdateAndDelete() throws Exception {
+    long docId = 99999999L;
+    indexr("id", docId, t1, "originalcontent");
+    
+    commit();
+    
+    ModifiableSolrParams params = new ModifiableSolrParams();
+    params.add("q", t1 + ":originalcontent");
+    QueryResponse results = clients.get(0).query(params);
+    assertEquals(1, results.getResults().getNumFound());
+    
+    // update doc
+    indexr("id", docId, t1, "updatedcontent");
+    
+    commit();
+    
+    results = clients.get(0).query(params);
+    assertEquals(0, results.getResults().getNumFound());
+    
+    params.set("q", t1 + ":updatedcontent");
+    
+    results = clients.get(0).query(params);
+    assertEquals(1, results.getResults().getNumFound());
+    
+    UpdateRequest uReq = new UpdateRequest();
+    // uReq.setParam(UpdateParams.UPDATE_CHAIN, DISTRIB_UPDATE_CHAIN);
+    uReq.deleteById(Long.toString(docId)).process(clients.get(0));
+    
+    commit();
+    
+    results = clients.get(0).query(params);
+    assertEquals(0, results.getResults().getNumFound());
+    return docId;
+  }
+  
+  private void brindDownShardIndexSomeDocsAndRecover() throws Exception {
+    SolrQuery query = new SolrQuery("*:*");
+    query.set("distrib", false);
+    
+    commit();
+    
+    long deadShardCount = shardToJetty.get(SHARD2).get(0).client.solrClient
+        .query(query).getResults().getNumFound();
+
+    query("q", "*:*", "sort", "n_tl1 desc");
+    
+    // kill a shard
+    CloudJettyRunner deadShard = chaosMonkey.stopShard(SHARD2, 0);
+    cloudClient.connect();
+
+    // we are careful to make sure the downed node is no longer in the state,
+    // because on some systems (especially freebsd w/ blackhole enabled), trying
+    // to talk to a downed node causes grief
+    Set<CloudJettyRunner> jetties = new HashSet<CloudJettyRunner>();
+    jetties.addAll(shardToJetty.get(SHARD2));
+    jetties.remove(deadShard);
+    
+    for (CloudJettyRunner cjetty : jetties) {
+      waitToSeeNotLive(((SolrDispatchFilter) cjetty.jetty.getDispatchFilter()
+          .getFilter()).getCores().getZkController().getZkStateReader(),
+          deadShard);
+    }
+    waitToSeeNotLive(cloudClient.getZkStateReader(), deadShard);
+
+    // ensure shard is dead
+    try {
+      index_specific(deadShard.client.solrClient, id, 999, i1, 107, t1,
+          "specific doc!");
+      fail("This server should be down and this update should have failed");
+    } catch (SolrServerException e) {
+      // expected..
+    }
+    
+    commit();
+    query("q", "*:*", "sort", "n_tl1 desc");
+    
+    // long cloudClientDocs = cloudClient.query(new
+    // SolrQuery("*:*")).getResults().getNumFound();
+    // System.out.println("clouddocs:" + cloudClientDocs);
+    
+    // try to index to a living shard at shard2
+
+  
+    long numFound1 = cloudClient.query(new SolrQuery("*:*")).getResults().getNumFound();
+    
+    index_specific(shardToJetty.get(SHARD2).get(1).client.solrClient, id, 1000, i1, 108, t1,
+        "specific doc!");
+    
+    commit();
+    
+    checkShardConsistency(true, false);
+    
+    query("q", "*:*", "sort", "n_tl1 desc");
+    
+    // try adding a doc with CloudSolrServer
+    cloudClient.setDefaultCollection(DEFAULT_COLLECTION);
+
+    long numFound2 = cloudClient.query(new SolrQuery("*:*")).getResults().getNumFound();
+    
+    assertEquals(numFound1 + 1, numFound2);
+    
+    SolrInputDocument doc = new SolrInputDocument();
+    doc.addField("id", 1001);
+    
+    controlClient.add(doc);
+    
+    UpdateRequest ureq = new UpdateRequest();
+    ureq.add(doc);
+    // ureq.setParam("update.chain", DISTRIB_UPDATE_CHAIN);
+    ureq.process(cloudClient);
+    
+    commit();
+    
+    query("q", "*:*", "sort", "n_tl1 desc");
+    
+    long numFound3 = cloudClient.query(new SolrQuery("*:*")).getResults().getNumFound();
+    
+    // lets just check that the one doc since last commit made it in...
+    assertEquals(numFound2 + 1, numFound3);
+    
+    // test debugging
+    testDebugQueries();
+    
+    if (VERBOSE) {
+      System.err.println(controlClient.query(new SolrQuery("*:*")).getResults()
+          .getNumFound());
+      
+      for (SolrServer client : clients) {
+        try {
+          SolrQuery q = new SolrQuery("*:*");
+          q.set("distrib", false);
+          System.err.println(client.query(q).getResults()
+              .getNumFound());
+        } catch (Exception e) {
+          
+        }
+      }
+    }
+    // TODO: This test currently fails because debug info is obtained only
+    // on shards with matches.
+    // query("q","matchesnothing","fl","*,score", "debugQuery", "true");
+    
+    // this should trigger a recovery phase on deadShard
+    ChaosMonkey.start(deadShard.jetty);
+    
+    // make sure we have published we are recovering
+    Thread.sleep(1500);
+    
+    waitForRecoveriesToFinish(false);
+    
+    deadShardCount = shardToJetty.get(SHARD2).get(0).client.solrClient
+        .query(query).getResults().getNumFound();
+    // if we properly recovered, we should now have the couple missing docs that
+    // came in while shard was down
+    checkShardConsistency(true, false);
+    
+    
+    // recover over 100 docs so we do more than just peer sync (replicate recovery)
+    chaosMonkey.stopJetty(deadShard);
+    
+    for (CloudJettyRunner cjetty : jetties) {
+      waitToSeeNotLive(((SolrDispatchFilter) cjetty.jetty.getDispatchFilter()
+          .getFilter()).getCores().getZkController().getZkStateReader(),
+          deadShard);
+    }
+    waitToSeeNotLive(cloudClient.getZkStateReader(), deadShard);
+    
+    for (int i = 0; i < 226; i++) {
+      doc = new SolrInputDocument();
+      doc.addField("id", 2000 + i);
+      controlClient.add(doc);
+      ureq = new UpdateRequest();
+      ureq.add(doc);
+      // ureq.setParam("update.chain", DISTRIB_UPDATE_CHAIN);
+      ureq.process(cloudClient);
+    }
+    commit();
+    
+    Thread.sleep(1500);
+    
+    ChaosMonkey.start(deadShard.jetty);
+    
+    // make sure we have published we are recovering
+    Thread.sleep(1500);
+    
+    waitForRecoveriesToFinish(false);
+    
+    checkShardConsistency(true, false);
+  }
+  
+  private void addNewReplica() throws Exception {
+    JettySolrRunner newReplica = createJettys(1).get(0);
+    
+    waitForRecoveriesToFinish(false);
+    
+    // new server should be part of first shard
+    // how many docs are on the new shard?
+    for (CloudJettyRunner cjetty : shardToJetty.get("shard1")) {
+      if (VERBOSE) System.err.println("total:"
+          + cjetty.client.solrClient.query(new SolrQuery("*:*")).getResults().getNumFound());
+    }
+    
+    checkShardConsistency("shard1");
+    
+    assertDocCounts(VERBOSE);
+  }
+  
+  private void testDebugQueries() throws Exception {
+    handle.put("explain", SKIPVAL);
+    handle.put("debug", UNORDERED);
+    handle.put("time", SKIPVAL);
+    query("q", "now their fox sat had put", "fl", "*,score",
+        CommonParams.DEBUG_QUERY, "true");
+    query("q", "id:[1 TO 5]", CommonParams.DEBUG_QUERY, "true");
+    query("q", "id:[1 TO 5]", CommonParams.DEBUG, CommonParams.TIMING);
+    query("q", "id:[1 TO 5]", CommonParams.DEBUG, CommonParams.RESULTS);
+    query("q", "id:[1 TO 5]", CommonParams.DEBUG, CommonParams.QUERY);
+  }
+  
+}
diff --git a/solr/core/src/test/org/apache/solr/cloud/BasicDistributedZkTest.java b/solr/core/src/test/org/apache/solr/cloud/BasicDistributedZkTest.java
index 821b604..3c0933e 100644
--- a/solr/core/src/test/org/apache/solr/cloud/BasicDistributedZkTest.java
+++ b/solr/core/src/test/org/apache/solr/cloud/BasicDistributedZkTest.java
@@ -71,10 +71,11 @@ import org.apache.solr.update.SolrCmdDistributor.Request;
 import org.apache.solr.util.DefaultSolrThreadFactory;
 
 /**
- *
+ * This test simply does a bunch of basic things in solrcloud mode and asserts things
+ * work as expected.
  */
 @Slow
-public class BasicDistributedZkTest extends AbstractDistributedZkTestCase {
+public class BasicDistributedZkTest extends AbstractDistribZkTestBase {
   
   private static final String DEFAULT_COLLECTION = "collection1";
   private static final boolean DEBUG = false;
diff --git a/solr/core/src/test/org/apache/solr/cloud/ChaosMonkey.java b/solr/core/src/test/org/apache/solr/cloud/ChaosMonkey.java
deleted file mode 100644
index 2c15bdd..0000000
--- a/solr/core/src/test/org/apache/solr/cloud/ChaosMonkey.java
+++ /dev/null
@@ -1,458 +0,0 @@
-package org.apache.solr.cloud;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.net.BindException;
-import java.util.ArrayList;
-import java.util.List;
-import java.util.Map;
-import java.util.Random;
-import java.util.concurrent.atomic.AtomicInteger;
-
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.solr.client.solrj.SolrServer;
-import org.apache.solr.client.solrj.embedded.JettySolrRunner;
-import org.apache.solr.cloud.FullSolrCloudTest.CloudJettyRunner;
-import org.apache.solr.common.cloud.Slice;
-import org.apache.solr.common.cloud.SolrZkClient;
-import org.apache.solr.common.cloud.ZkNodeProps;
-import org.apache.solr.common.cloud.ZkStateReader;
-import org.apache.solr.core.CoreContainer;
-import org.apache.solr.servlet.SolrDispatchFilter;
-import org.apache.zookeeper.KeeperException;
-import org.eclipse.jetty.servlet.FilterHolder;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-/**
- * The monkey can stop random or specific jetties used with SolrCloud.
- * 
- * It can also run in a background thread and start and stop jetties
- * randomly.
- *
- */
-public class ChaosMonkey {
-  private static Logger log = LoggerFactory.getLogger(ChaosMonkey.class);
-  
-  private static final int CONLOSS_PERCENT = 3; //30%
-  private static final int EXPIRE_PERCENT = 4; //40%
-  private Map<String,List<CloudJettyRunner>> shardToJetty;
-  
-  private ZkTestServer zkServer;
-  private ZkStateReader zkStateReader;
-  private String collection;
-  private volatile boolean stop = false;
-  private AtomicInteger stops = new AtomicInteger();
-  private AtomicInteger starts = new AtomicInteger();
-  private AtomicInteger expires = new AtomicInteger();
-  private AtomicInteger connloss = new AtomicInteger();
-  
-  private Map<String,List<SolrServer>> shardToClient;
-  private boolean expireSessions;
-  private boolean causeConnectionLoss;
-  private boolean aggressivelyKillLeaders;
-  private Map<String,CloudJettyRunner> shardToLeaderJetty;
-  private long startTime;
-  
-  public ChaosMonkey(ZkTestServer zkServer, ZkStateReader zkStateReader,
-      String collection, Map<String,List<CloudJettyRunner>> shardToJetty,
-      Map<String,CloudJettyRunner> shardToLeaderJetty) {
-    this.shardToJetty = shardToJetty;
-    this.shardToLeaderJetty = shardToLeaderJetty;
-    this.zkServer = zkServer;
-    this.zkStateReader = zkStateReader;
-    this.collection = collection;
-    Random random = LuceneTestCase.random();
-    expireSessions = random.nextBoolean();
-    causeConnectionLoss = random.nextBoolean();
-    monkeyLog("init - expire sessions:" + expireSessions
-        + " cause connection loss:" + causeConnectionLoss);
-  }
-  
-  public void expireSession(JettySolrRunner jetty) {
-    monkeyLog("expire session for " + jetty.getLocalPort() + " !");
-    SolrDispatchFilter solrDispatchFilter = (SolrDispatchFilter) jetty.getDispatchFilter().getFilter();
-    if (solrDispatchFilter != null) {
-      CoreContainer cores = solrDispatchFilter.getCores();
-      if (cores != null) {
-        long sessionId = cores.getZkController().getZkClient().getSolrZooKeeper().getSessionId();
-        zkServer.expire(sessionId);
-      }
-    }
-  }
-  
-  public void expireRandomSession() throws KeeperException, InterruptedException {
-    String sliceName = getRandomSlice();
-    
-    CloudJettyRunner jetty = getRandomJetty(sliceName, aggressivelyKillLeaders);
-    if (jetty != null) {
-      expireSession(jetty.jetty);
-      expires.incrementAndGet();
-    }
-  }
-  
-  public void randomConnectionLoss() throws KeeperException, InterruptedException {
-    monkeyLog("cause connection loss!");
-    
-    String sliceName = getRandomSlice();
-    CloudJettyRunner jetty = getRandomJetty(sliceName, aggressivelyKillLeaders);
-    if (jetty != null) {
-      causeConnectionLoss(jetty.jetty);
-      connloss.incrementAndGet();
-    }
-  }
-  
-  private void causeConnectionLoss(JettySolrRunner jetty) {
-    SolrDispatchFilter solrDispatchFilter = (SolrDispatchFilter) jetty
-        .getDispatchFilter().getFilter();
-    if (solrDispatchFilter != null) {
-      CoreContainer cores = solrDispatchFilter.getCores();
-      if (cores != null) {
-        SolrZkClient zkClient = cores.getZkController().getZkClient();
-        // must be at least double tick time...
-        zkClient.getSolrZooKeeper().pauseCnxn(ZkTestServer.TICK_TIME * 2);
-      }
-    }
-  }
-
-  public CloudJettyRunner stopShard(String slice, int index) throws Exception {
-    CloudJettyRunner cjetty = shardToJetty.get(slice).get(index);
-    stopJetty(cjetty);
-    return cjetty;
-  }
-
-  public void stopJetty(CloudJettyRunner cjetty) throws Exception {
-    stop(cjetty.jetty);
-    stops.incrementAndGet();
-  }
-
-  public void killJetty(CloudJettyRunner cjetty) throws Exception {
-    kill(cjetty);
-    stops.incrementAndGet();
-  }
-  
-  public void stopJetty(JettySolrRunner jetty) throws Exception {
-    stops.incrementAndGet();
-    stopJettySolrRunner(jetty);
-  }
-  
-  private static void stopJettySolrRunner(JettySolrRunner jetty) throws Exception {
-    
-    monkeyLog("stop shard! " + jetty.getLocalPort());
-    // get a clean shutdown so that no dirs are left open...
-    FilterHolder fh = jetty.getDispatchFilter();
-    if (fh != null) {
-      SolrDispatchFilter sdf = (SolrDispatchFilter) fh.getFilter();
-      if (sdf != null) {
-        sdf.destroy();
-      }
-    }
-    jetty.stop();
-    
-    if (!jetty.isStopped()) {
-      throw new RuntimeException("could not stop jetty");
-    }
-  }
-  
-  public static void kill(CloudJettyRunner cjetty) throws Exception {
-    JettySolrRunner jetty = cjetty.jetty;
-    monkeyLog("kill shard! " + jetty.getLocalPort());
-    FilterHolder fh = jetty.getDispatchFilter();
-    SolrDispatchFilter sdf = null;
-    if (fh != null) {
-      sdf = (SolrDispatchFilter) fh.getFilter();
-    }
-    jetty.stop();
-    
-    if (sdf != null) {
-      sdf.destroy();
-    }
-    
-    if (!jetty.isStopped()) {
-      throw new RuntimeException("could not kill jetty");
-    }
-  }
-  
-  public void stopShard(String slice) throws Exception {
-    List<CloudJettyRunner> jetties = shardToJetty.get(slice);
-    for (CloudJettyRunner jetty : jetties) {
-      stopJetty(jetty);
-    }
-  }
-  
-  public void stopShardExcept(String slice, String shardName) throws Exception {
-    List<CloudJettyRunner> jetties = shardToJetty.get(slice);
-    for (CloudJettyRunner jetty : jetties) {
-      if (!jetty.nodeName.equals(shardName)) {
-        stopJetty(jetty);
-      }
-    }
-  }
-  
-  public JettySolrRunner getShard(String slice, int index) throws Exception {
-    JettySolrRunner jetty = shardToJetty.get(slice).get(index).jetty;
-    return jetty;
-  }
-  
-  public CloudJettyRunner stopRandomShard() throws Exception {
-    String sliceName = getRandomSlice();
-    
-    return stopRandomShard(sliceName);
-  }
-  
-  public CloudJettyRunner stopRandomShard(String slice) throws Exception {
-    CloudJettyRunner cjetty = getRandomJetty(slice, aggressivelyKillLeaders);
-    if (cjetty != null) {
-      stopJetty(cjetty);
-    }
-    return cjetty;
-  }
-  
-  
-  public CloudJettyRunner killRandomShard() throws Exception {
-    // add all the shards to a list
-    String sliceName = getRandomSlice();
-    
-    return killRandomShard(sliceName);
-  }
-
-  private String getRandomSlice() {
-    Map<String,Slice> slices = zkStateReader.getClusterState().getSlices(collection);
-    
-    List<String> sliceKeyList = new ArrayList<String>(slices.size());
-    sliceKeyList.addAll(slices.keySet());
-    String sliceName = sliceKeyList.get(LuceneTestCase.random().nextInt(sliceKeyList.size()));
-    return sliceName;
-  }
-  
-  public CloudJettyRunner killRandomShard(String slice) throws Exception {
-    CloudJettyRunner cjetty = getRandomJetty(slice, aggressivelyKillLeaders);
-    if (cjetty != null) {
-      killJetty(cjetty);
-    }
-    return cjetty;
-  }
-  
-  public CloudJettyRunner getRandomJetty(String slice, boolean aggressivelyKillLeaders) throws KeeperException, InterruptedException {
-    
-
-    int numRunning = 0;
-    int numRecovering = 0;
-    int numActive = 0;
-    
-    for (CloudJettyRunner cloudJetty : shardToJetty.get(slice)) {
-      boolean running = true;
-      
-      // get latest cloud state
-      zkStateReader.updateClusterState(true);
-      
-      Slice theShards = zkStateReader.getClusterState().getSlices(collection)
-          .get(slice);
-      
-      ZkNodeProps props = theShards.getShards().get(cloudJetty.coreNodeName);
-      if (props == null) {
-        throw new RuntimeException("shard name " + cloudJetty.coreNodeName + " not found in " + theShards.getShards().keySet());
-      }
-      
-      String state = props.get(ZkStateReader.STATE_PROP);
-      String nodeName = props.get(ZkStateReader.NODE_NAME_PROP);
-      
-      
-      if (!cloudJetty.jetty.isRunning()
-          || !state.equals(ZkStateReader.ACTIVE)
-          || !zkStateReader.getClusterState().liveNodesContain(nodeName)) {
-        running = false;
-      }
-      
-      if (cloudJetty.jetty.isRunning()
-          && state.equals(ZkStateReader.RECOVERING)
-          && zkStateReader.getClusterState().liveNodesContain(nodeName)) {
-        numRecovering++;
-      }
-      
-      if (cloudJetty.jetty.isRunning()
-          && state.equals(ZkStateReader.ACTIVE)
-          && zkStateReader.getClusterState().liveNodesContain(nodeName)) {
-        numActive++;
-      }
-      
-      if (running) {
-        numRunning++;
-      }
-    }
-    
-    if (numActive < 2) {
-      // we cannot kill anyone
-      monkeyLog("only one active node in shard - monkey cannot kill :(");
-      return null;
-    }
-    Random random = LuceneTestCase.random();
-    int chance = random.nextInt(10);
-    CloudJettyRunner cjetty;
-    if (chance <= 5 && aggressivelyKillLeaders) {
-      // if killLeader, really aggressively go after leaders
-      cjetty = shardToLeaderJetty.get(slice);
-    } else {
-      // get random shard
-      List<CloudJettyRunner> jetties = shardToJetty.get(slice);
-      int index = random.nextInt(jetties.size());
-      cjetty = jetties.get(index);
-      
-      ZkNodeProps leader = zkStateReader.getLeaderProps(collection, slice);
-      boolean isLeader = leader.get(ZkStateReader.NODE_NAME_PROP).equals(jetties.get(index).nodeName);
-      if (!aggressivelyKillLeaders && isLeader) {
-        // we don't kill leaders...
-        monkeyLog("abort! I don't kill leaders");
-        return null;
-      } 
-    }
-
-    if (cjetty.jetty.getLocalPort() == -1) {
-      // we can't kill the dead
-      monkeyLog("abort! This guy is already dead");
-      return null;
-    }
-    
-    //System.out.println("num active:" + numActive + " for " + slice + " sac:" + jetty.getLocalPort());
-    monkeyLog("chose a victim! " + cjetty.jetty.getLocalPort());
-  
-    return cjetty;
-  }
-  
-  public SolrServer getRandomClient(String slice) throws KeeperException, InterruptedException {
-    // get latest cloud state
-    zkStateReader.updateClusterState(true);
-
-    // get random shard
-    List<SolrServer> clients = shardToClient.get(slice);
-    int index = LuceneTestCase.random().nextInt(clients.size() - 1);
-    SolrServer client = clients.get(index);
-
-    return client;
-  }
-  
-  // synchronously starts and stops shards randomly, unless there is only one
-  // active shard up for a slice or if there is one active and others recovering
-  public void startTheMonkey(boolean killLeaders, final int roundPause) {
-    monkeyLog("starting");
-    this.aggressivelyKillLeaders = killLeaders;
-    startTime = System.currentTimeMillis();
-    // TODO: when kill leaders is on, lets kill a higher percentage of leaders
-    
-    stop = false;
-    new Thread() {
-      private List<CloudJettyRunner> deadPool = new ArrayList<CloudJettyRunner>();
-
-      @Override
-      public void run() {
-        while (!stop) {
-          try {
-            Thread.sleep(roundPause);
-            Random random = LuceneTestCase.random();
-            if (random.nextBoolean()) {
-             if (!deadPool.isEmpty()) {
-               int index = random.nextInt(deadPool.size());
-               JettySolrRunner jetty = deadPool.get(index).jetty;
-               if (!ChaosMonkey.start(jetty)) {
-                 continue;
-               }
-               //System.out.println("started on port:" + jetty.getLocalPort());
-               deadPool.remove(index);
-               starts.incrementAndGet();
-               continue;
-             }
-            }
-            
-            int rnd = random.nextInt(10);
-
-            if (expireSessions && rnd < EXPIRE_PERCENT) {
-              expireRandomSession();
-            } 
-            
-            if (causeConnectionLoss && rnd < CONLOSS_PERCENT) {
-              randomConnectionLoss();
-              randomConnectionLoss();
-            }
-            
-            CloudJettyRunner cjetty;
-            if (random.nextBoolean()) {
-              cjetty = stopRandomShard();
-            } else {
-              cjetty = killRandomShard();
-            }
-            if (cjetty == null) {
-              // we cannot kill
-            } else {
-              deadPool.add(cjetty);
-            }
-            
-          } catch (InterruptedException e) {
-            //
-          } catch (Exception e) {
-            // TODO Auto-generated catch block
-            e.printStackTrace();
-          }
-        }
-        monkeyLog("finished");
-        monkeyLog("I ran for " + (System.currentTimeMillis() - startTime)/1000.0f + "sec. I stopped " + stops + " and I started " + starts
-            + ". I also expired " + expires.get() + " and caused " + connloss
-            + " connection losses");
-      }
-    }.start();
-  }
-  
-  public static void monkeyLog(String msg) {
-    log.info("monkey: " + msg);
-  }
-  
-  public void stopTheMonkey() {
-    stop = true;
-  }
-
-  public int getStarts() {
-    return starts.get();
-  }
-
-  public static void stop(JettySolrRunner jetty) throws Exception {
-    stopJettySolrRunner(jetty);
-  }
-  
-  public static boolean start(JettySolrRunner jetty) throws Exception {
-    try {
-      jetty.start();
-    } catch (BindException e) {
-      jetty.stop();
-      Thread.sleep(2000);
-      try {
-        jetty.start();
-      } catch (BindException e2) {
-        jetty.stop();
-        Thread.sleep(5000);
-        try {
-          jetty.start();
-        } catch (BindException e3) {
-          // we coud not get the port
-          jetty.stop();
-          return false;
-        }
-      }
-    }
-    return true;
-  }
-
-}
\ No newline at end of file
diff --git a/solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest.java b/solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest.java
index 2bb89db..7e4f7e3 100644
--- a/solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest.java
+++ b/solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest.java
@@ -39,7 +39,7 @@ import org.slf4j.LoggerFactory;
 
 @Slow
 @Ignore("ignore while investigating jenkins fails")
-public class ChaosMonkeyNothingIsSafeTest extends FullSolrCloudTest {
+public class ChaosMonkeyNothingIsSafeTest extends AbstractFullDistribZkTestBase {
   public static Logger log = LoggerFactory.getLogger(ChaosMonkeyNothingIsSafeTest.class);
   
   private static final int BASE_RUN_LENGTH = 180000;
diff --git a/solr/core/src/test/org/apache/solr/cloud/ChaosMonkeySafeLeaderTest.java b/solr/core/src/test/org/apache/solr/cloud/ChaosMonkeySafeLeaderTest.java
index c38f6361..1ddec4e 100644
--- a/solr/core/src/test/org/apache/solr/cloud/ChaosMonkeySafeLeaderTest.java
+++ b/solr/core/src/test/org/apache/solr/cloud/ChaosMonkeySafeLeaderTest.java
@@ -32,7 +32,7 @@ import org.junit.BeforeClass;
 import org.junit.Ignore;
 
 @Ignore("SOLR-3126")
-public class ChaosMonkeySafeLeaderTest extends FullSolrCloudTest {
+public class ChaosMonkeySafeLeaderTest extends AbstractFullDistribZkTestBase {
   
   private static final int BASE_RUN_LENGTH = 120000;
 
diff --git a/solr/core/src/test/org/apache/solr/cloud/FullSolrCloudDistribCmdsTest.java b/solr/core/src/test/org/apache/solr/cloud/FullSolrCloudDistribCmdsTest.java
index aec1c5a..3bd6c74 100644
--- a/solr/core/src/test/org/apache/solr/cloud/FullSolrCloudDistribCmdsTest.java
+++ b/solr/core/src/test/org/apache/solr/cloud/FullSolrCloudDistribCmdsTest.java
@@ -23,8 +23,8 @@ import org.apache.lucene.util.LuceneTestCase.Slow;
 import org.apache.solr.client.solrj.SolrQuery;
 import org.apache.solr.client.solrj.SolrServer;
 import org.apache.solr.client.solrj.SolrServerException;
-import org.apache.solr.client.solrj.impl.HttpSolrServer;
 import org.apache.solr.client.solrj.impl.ConcurrentUpdateSolrServer;
+import org.apache.solr.client.solrj.impl.HttpSolrServer;
 import org.apache.solr.client.solrj.request.UpdateRequest;
 import org.apache.solr.client.solrj.response.QueryResponse;
 import org.apache.solr.common.SolrDocument;
@@ -37,14 +37,13 @@ import org.apache.solr.common.params.ModifiableSolrParams;
 import org.apache.solr.update.VersionInfo;
 import org.apache.solr.update.processor.DistributedUpdateProcessor;
 import org.apache.zookeeper.CreateMode;
-import org.apache.zookeeper.KeeperException;
 import org.junit.BeforeClass;
 
 /**
  * Super basic testing, no shard restarting or anything.
  */
 @Slow
-public class FullSolrCloudDistribCmdsTest extends FullSolrCloudTest {
+public class FullSolrCloudDistribCmdsTest extends AbstractFullDistribZkTestBase {
   
   
   @BeforeClass
diff --git a/solr/core/src/test/org/apache/solr/cloud/FullSolrCloudTest.java b/solr/core/src/test/org/apache/solr/cloud/FullSolrCloudTest.java
deleted file mode 100644
index 263ce7a..0000000
--- a/solr/core/src/test/org/apache/solr/cloud/FullSolrCloudTest.java
+++ /dev/null
@@ -1,1520 +0,0 @@
-package org.apache.solr.cloud;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.File;
-import java.io.IOException;
-import java.net.MalformedURLException;
-import java.net.URI;
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Map;
-import java.util.Random;
-import java.util.Set;
-import java.util.concurrent.atomic.AtomicInteger;
-
-import org.apache.http.params.CoreConnectionPNames;
-import org.apache.lucene.util.LuceneTestCase.Slow;
-import org.apache.solr.client.solrj.SolrQuery;
-import org.apache.solr.client.solrj.SolrServer;
-import org.apache.solr.client.solrj.SolrServerException;
-import org.apache.solr.client.solrj.embedded.JettySolrRunner;
-import org.apache.solr.client.solrj.impl.CloudSolrServer;
-import org.apache.solr.client.solrj.impl.HttpSolrServer;
-import org.apache.solr.client.solrj.request.UpdateRequest;
-import org.apache.solr.client.solrj.response.QueryResponse;
-import org.apache.solr.common.SolrDocument;
-import org.apache.solr.common.SolrDocumentList;
-import org.apache.solr.common.SolrException;
-import org.apache.solr.common.SolrInputDocument;
-import org.apache.solr.common.cloud.ClusterState;
-import org.apache.solr.common.cloud.Slice;
-import org.apache.solr.common.cloud.ZkCoreNodeProps;
-import org.apache.solr.common.cloud.ZkNodeProps;
-import org.apache.solr.common.cloud.ZkStateReader;
-import org.apache.solr.common.params.CommonParams;
-import org.apache.solr.common.params.ModifiableSolrParams;
-import org.apache.solr.servlet.SolrDispatchFilter;
-import org.junit.After;
-import org.junit.AfterClass;
-import org.junit.Before;
-import org.junit.BeforeClass;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-/**
- * 
- * TODO: we should still test this works as a custom update chain as well as
- * what we test now - the default update chain
- * 
- */
-@Slow
-public class FullSolrCloudTest extends AbstractDistributedZkTestCase {
-  static Logger log = LoggerFactory.getLogger(FullSolrCloudTest.class);
-  
-  @BeforeClass
-  public static void beforeFullSolrCloudTest() {
-    // shorten the log output more for this test type
-    if (formatter != null) formatter.setShorterFormat();
-  }
-
-  private static final String SHARD2 = "shard2";
-  
-  private boolean printLayoutOnTearDown = false;
-  
-  String t1 = "a_t";
-  String i1 = "a_si";
-  String nint = "n_i";
-  String tint = "n_ti";
-  String nfloat = "n_f";
-  String tfloat = "n_tf";
-  String ndouble = "n_d";
-  String tdouble = "n_td";
-  String nlong = "n_l";
-  String tlong = "other_tl1";
-  String ndate = "n_dt";
-  String tdate = "n_tdt";
-  
-  String oddField = "oddField_s";
-  String missingField = "ignore_exception__missing_but_valid_field_t";
-  String invalidField = "ignore_exception__invalid_field_not_in_schema";
-  protected int sliceCount;
-  
-  protected volatile CloudSolrServer cloudClient;
-  
-  protected List<CloudJettyRunner> cloudJettys = new ArrayList<CloudJettyRunner>();
-  protected Map<String,List<CloudJettyRunner>> shardToJetty = new HashMap<String,List<CloudJettyRunner>>();
-  private AtomicInteger jettyIntCntr = new AtomicInteger(0);
-  protected ChaosMonkey chaosMonkey;
-  protected volatile ZkStateReader zkStateReader;
-  
-  protected Map<String,CloudJettyRunner> shardToLeaderJetty = new HashMap<String,CloudJettyRunner>();
-  
-  static class CloudJettyRunner {
-    JettySolrRunner jetty;
-    String nodeName;
-    String coreNodeName;
-    String url;
-    CloudSolrServerClient client;
-    public ZkNodeProps info;
-    @Override
-    public int hashCode() {
-      final int prime = 31;
-      int result = 1;
-      result = prime * result + ((url == null) ? 0 : url.hashCode());
-      return result;
-    }
-    @Override
-    public boolean equals(Object obj) {
-      if (this == obj) return true;
-      if (obj == null) return false;
-      if (getClass() != obj.getClass()) return false;
-      CloudJettyRunner other = (CloudJettyRunner) obj;
-      if (url == null) {
-        if (other.url != null) return false;
-      } else if (!url.equals(other.url)) return false;
-      return true;
-    }
-  }
-  
-  static class CloudSolrServerClient {
-    SolrServer solrClient;
-    String shardName;
-    int port;
-    public ZkNodeProps info;
-    
-    public CloudSolrServerClient() {}
-    
-    public CloudSolrServerClient(SolrServer client) {
-      this.solrClient = client;
-    }
-    
-    @Override
-    public int hashCode() {
-      final int prime = 31;
-      int result = 1;
-      result = prime * result + ((solrClient == null) ? 0 : solrClient.hashCode());
-      return result;
-    }
-    
-    @Override
-    public boolean equals(Object obj) {
-      if (this == obj) return true;
-      if (obj == null) return false;
-      if (getClass() != obj.getClass()) return false;
-      CloudSolrServerClient other = (CloudSolrServerClient) obj;
-      if (solrClient == null) {
-        if (other.solrClient != null) return false;
-      } else if (!solrClient.equals(other.solrClient)) return false;
-      return true;
-    }
-    
-  }
-  
-  @Before
-  @Override
-  public void setUp() throws Exception {
-    super.setUp();
-    // ignoreException(".*");
-    System.setProperty("numShards", Integer.toString(sliceCount));
-  }
-  
-  @BeforeClass
-  public static void beforeClass() {
-    System.setProperty("solrcloud.update.delay", "0");
-  }
-  
-  @AfterClass
-  public static void afterClass() {
-    System.clearProperty("solrcloud.update.delay");
-  }
-  
-  public FullSolrCloudTest() {
-    fixShardCount = true;
-    
-    shardCount = 4;
-    sliceCount = 2;
-    // TODO: for now, turn off stress because it uses regular clients, and we
-    // need the cloud client because we kill servers
-    stress = 0;
-  }
-  
-  protected void initCloud() throws Exception {
-    if (zkStateReader == null) {
-      synchronized (this) {
-        if (zkStateReader != null) {
-          return;
-        }
-        zkStateReader = new ZkStateReader(zkServer.getZkAddress(), 10000,
-            AbstractZkTestCase.TIMEOUT);
-        
-        zkStateReader.createClusterStateWatchersAndUpdate();
-      }
-      
-      chaosMonkey = new ChaosMonkey(zkServer, zkStateReader,
-          DEFAULT_COLLECTION, shardToJetty,
-          shardToLeaderJetty);
-    }
-    
-    // wait until shards have started registering...
-    while (!zkStateReader.getClusterState().getCollections()
-        .contains(DEFAULT_COLLECTION)) {
-      Thread.sleep(500);
-    }
-    while (zkStateReader.getClusterState().getSlices(DEFAULT_COLLECTION).size() != sliceCount) {
-      Thread.sleep(500);
-    }
-    
-    // use the distributed solrj client
-    if (cloudClient == null) {
-      synchronized (this) {
-        if (cloudClient != null) {
-          return;
-        }
-        try {
-          CloudSolrServer server = new CloudSolrServer(zkServer.getZkAddress());
-          server.setDefaultCollection(DEFAULT_COLLECTION);
-          server.getLbServer().getHttpClient().getParams()
-              .setParameter(CoreConnectionPNames.CONNECTION_TIMEOUT, 5000);
-          server.getLbServer().getHttpClient().getParams()
-              .setParameter(CoreConnectionPNames.SO_TIMEOUT, 15000);
-          cloudClient = server;
-        } catch (MalformedURLException e) {
-          throw new RuntimeException(e);
-        }
-      }
-    }
-  }
-  
-  @Override
-  protected void createServers(int numServers) throws Exception {
-    
-    System.setProperty("collection", "control_collection");
-    String numShards = System.getProperty(ZkStateReader.NUM_SHARDS_PROP);
-    System.clearProperty(ZkStateReader.NUM_SHARDS_PROP);
-    controlJetty = createJetty(new File(getSolrHome()), testDir + "/control/data",
-        "control_shard");
-    System.clearProperty("collection");
-    if(numShards != null) {
-      System.setProperty(ZkStateReader.NUM_SHARDS_PROP, numShards);
-    } 
-    controlClient = createNewSolrServer(controlJetty.getLocalPort());
-    
-    createJettys(numServers, true);
-    
-  }
-  
-  private List<JettySolrRunner> createJettys(int numJettys) throws Exception {
-    return createJettys(numJettys, false);
-  }
-  
-
-  /**
-   * @param numJettys
-   * @param checkCreatedVsState
-   *          if true, make sure the number created (numJettys) matches the
-   *          number in the cluster state - if you add more jetties this may not
-   *          be the case
-   * @return
-   * @throws Exception
-   */
-  private List<JettySolrRunner> createJettys(int numJettys, boolean checkCreatedVsState) throws Exception {
-    List<JettySolrRunner> jettys = new ArrayList<JettySolrRunner>();
-    List<SolrServer> clients = new ArrayList<SolrServer>();
-    StringBuilder sb = new StringBuilder();
-    for (int i = 1; i <= numJettys; i++) {
-      if (sb.length() > 0) sb.append(',');
-      JettySolrRunner j = createJetty(new File(getSolrHome()), testDir + "/jetty"
-          + this.jettyIntCntr.incrementAndGet(), null, "solrconfig.xml", null);
-      jettys.add(j);
-      SolrServer client = createNewSolrServer(j.getLocalPort());
-      clients.add(client);
-    }
-    
-    initCloud();
-    
-    this.jettys.addAll(jettys);
-    this.clients.addAll(clients);
-    
-    if (checkCreatedVsState) {
-      // now wait until we see that the number of shards in the cluster state
-      // matches what we expect
-      int numShards = getNumShards(DEFAULT_COLLECTION);
-      int retries = 0;
-      while (numShards != shardCount) {
-        numShards = getNumShards(DEFAULT_COLLECTION);
-        if (numShards == shardCount) break;
-        if (retries++ == 60) {
-          printLayoutOnTearDown = true;
-          fail("Shards in the state does not match what we set:" + numShards
-              + " vs " + shardCount);
-        }
-        Thread.sleep(500);
-      }
-
-      // also make sure we have a leader for each shard
-      for (int i = 1; i <= sliceCount; i++) {
-        zkStateReader.getLeaderProps(DEFAULT_COLLECTION, "shard" + i, 10000);
-      }
-    }
-
-    updateMappingsFromZk(this.jettys, this.clients);
-    
-    // build the shard string
-    for (int i = 1; i <= numJettys / 2; i++) {
-      JettySolrRunner j = this.jettys.get(i);
-      JettySolrRunner j2 = this.jettys.get(i + (numJettys / 2 - 1));
-      if (sb.length() > 0) sb.append(',');
-      sb.append("localhost:").append(j.getLocalPort()).append(context);
-      sb.append("|localhost:").append(j2.getLocalPort()).append(context);
-    }
-    shards = sb.toString();
-    
-    return jettys;
-  }
-
-  private int getNumShards(String defaultCollection) {
-    Map<String,Slice> slices = this.zkStateReader.getClusterState().getSlices(defaultCollection);
-    int cnt = 0;
-    for (Map.Entry<String,Slice> entry : slices.entrySet()) {
-      cnt += entry.getValue().getShards().size();
-    }
-    
-    return cnt;
-  }
-  
-  public JettySolrRunner createJetty(String dataDir, String shardList,
-      String solrConfigOverride) throws Exception {
-    
-    JettySolrRunner jetty = new JettySolrRunner(getSolrHome(), "/solr", 0,
-        solrConfigOverride, null, false);
-    jetty.setShards(shardList);
-    jetty.setDataDir(dataDir);
-    jetty.start();
-    
-    return jetty;
-  }
-  
-  protected void updateMappingsFromZk(List<JettySolrRunner> jettys,
-      List<SolrServer> clients) throws Exception {
-    zkStateReader.updateClusterState(true);
-    cloudJettys.clear();
-    shardToJetty.clear();
-    
-    ClusterState clusterState = zkStateReader.getClusterState();
-    Map<String,Slice> slices = clusterState.getSlices(DEFAULT_COLLECTION);
-    
-    if (slices == null) {
-      throw new RuntimeException("No slices found for collection "
-          + DEFAULT_COLLECTION + " in " + clusterState.getCollections());
-    }
-    
-    List<CloudSolrServerClient> theClients = new ArrayList<CloudSolrServerClient>();
-    for (SolrServer client : clients) {
-      // find info for this client in zk 
-      nextClient:
-      // we find ou state by simply matching ports...
-      for (Map.Entry<String,Slice> slice : slices.entrySet()) {
-        Map<String,ZkNodeProps> theShards = slice.getValue().getShards();
-        for (Map.Entry<String,ZkNodeProps> shard : theShards.entrySet()) {
-          int port = new URI(((HttpSolrServer) client).getBaseURL())
-              .getPort();
-          
-          if (shard.getKey().contains(":" + port + "_")) {
-            CloudSolrServerClient csc = new CloudSolrServerClient();
-            csc.solrClient = client;
-            csc.port = port;
-            csc.shardName = shard.getValue().get(ZkStateReader.NODE_NAME_PROP);
-            csc.info = shard.getValue();
-            
-            theClients .add(csc);
-            
-            break nextClient;
-          }
-        }
-      }
-    }
- 
-    for (JettySolrRunner jetty : jettys) {
-      int port = jetty.getLocalPort();
-      if (port == -1) {
-        throw new RuntimeException("Cannot find the port for jetty");
-      }
-      
-      nextJetty:
-      for (Map.Entry<String,Slice> slice : slices.entrySet()) {
-        Map<String,ZkNodeProps> theShards = slice.getValue().getShards();
-        for (Map.Entry<String,ZkNodeProps> shard : theShards.entrySet()) {
-          if (shard.getKey().contains(":" + port + "_")) {
-            List<CloudJettyRunner> list = shardToJetty.get(slice.getKey());
-            if (list == null) {
-              list = new ArrayList<CloudJettyRunner>();
-              shardToJetty.put(slice.getKey(), list);
-            }
-            boolean isLeader = shard.getValue().containsKey(
-                ZkStateReader.LEADER_PROP);
-            CloudJettyRunner cjr = new CloudJettyRunner();
-            cjr.jetty = jetty;
-            cjr.info = shard.getValue();
-            cjr.nodeName = shard.getValue().get(ZkStateReader.NODE_NAME_PROP);
-            cjr.coreNodeName = shard.getKey();
-            cjr.url = shard.getValue().get(ZkStateReader.BASE_URL_PROP) + "/" + shard.getValue().get(ZkStateReader.CORE_NAME_PROP);
-            cjr.client = findClientByPort(port, theClients);
-            list.add(cjr);
-            if (isLeader) {
-              shardToLeaderJetty.put(slice.getKey(), cjr);
-            }
-            cloudJettys.add(cjr);
-            break nextJetty;
-          }
-        }
-      }
-    }
-    
-    // # of jetties may not match replicas in shard here, because we don't map
-    // jetties that are not running - every shard should have at least one
-    // running jetty though
-    for (Map.Entry<String,Slice> slice : slices.entrySet()) {
-      // check that things look right
-      List<CloudJettyRunner> jetties = shardToJetty.get(slice.getKey());
-      assertNotNull("Test setup problem: We found no jetties for shard: " + slice.getKey()
-          + " just:" + shardToJetty.keySet(), jetties);
-      assertEquals(slice.getValue().getShards().size(), jetties.size());
-    }
-  }
-  
-  private CloudSolrServerClient findClientByPort(int port, List<CloudSolrServerClient> theClients) {
-    for (CloudSolrServerClient client : theClients) {
-      if (client.port == port) {
-        return client;
-      }
-    }
-    throw new IllegalArgumentException("Client with the give port does not exist:" + port);
-  }
-
-  @Override
-  protected void setDistributedParams(ModifiableSolrParams params) {
-    
-    if (r.nextBoolean()) {
-      // don't set shards, let that be figured out from the cloud state
-    } else {
-      // use shard ids rather than physical locations
-      StringBuilder sb = new StringBuilder();
-      for (int i = 0; i < sliceCount; i++) {
-        if (i > 0) sb.append(',');
-        sb.append("shard" + (i + 1));
-      }
-      params.set("shards", sb.toString());
-    }
-  }
-  
-  @Override
-  protected void indexDoc(SolrInputDocument doc) throws IOException,
-      SolrServerException {
-    controlClient.add(doc);
-    
-    // if we wanted to randomly pick a client - but sometimes they may be
-    // down...
-    
-    // boolean pick = random.nextBoolean();
-    //
-    // int which = (doc.getField(id).toString().hashCode() & 0x7fffffff) %
-    // sliceCount;
-    //
-    // if (pick && sliceCount > 1) {
-    // which = which + ((shardCount / sliceCount) *
-    // random.nextInt(sliceCount-1));
-    // }
-    //
-    // CommonsHttpSolrServer client = (CommonsHttpSolrServer)
-    // clients.get(which);
-    
-    UpdateRequest ureq = new UpdateRequest();
-    ureq.add(doc);
-    // ureq.setParam(UpdateParams.UPDATE_CHAIN, DISTRIB_UPDATE_CHAIN);
-    ureq.process(cloudClient);
-  }
-  
-  protected void index_specific(int serverNumber, Object... fields)
-      throws Exception {
-    SolrInputDocument doc = new SolrInputDocument();
-    for (int i = 0; i < fields.length; i += 2) {
-      doc.addField((String) (fields[i]), fields[i + 1]);
-    }
-    controlClient.add(doc);
-    
-    HttpSolrServer client = (HttpSolrServer) clients
-        .get(serverNumber);
-    
-    UpdateRequest ureq = new UpdateRequest();
-    ureq.add(doc);
-    // ureq.setParam("update.chain", DISTRIB_UPDATE_CHAIN);
-    ureq.process(client);
-  }
-  
-  protected void index_specific(SolrServer client, Object... fields)
-      throws Exception {
-    SolrInputDocument doc = new SolrInputDocument();
-    for (int i = 0; i < fields.length; i += 2) {
-      doc.addField((String) (fields[i]), fields[i + 1]);
-    }
-    
-    UpdateRequest ureq = new UpdateRequest();
-    ureq.add(doc);
-    // ureq.setParam("update.chain", DISTRIB_UPDATE_CHAIN);
-    ureq.process(client);
-    
-    // add to control second in case adding to shards fails
-    controlClient.add(doc);
-  }
-  
-  protected void del(String q) throws Exception {
-    controlClient.deleteByQuery(q);
-    cloudClient.deleteByQuery(q);
-
-    /***
-    for (SolrServer client : clients) {
-      UpdateRequest ureq = new UpdateRequest();
-      // ureq.setParam("update.chain", DISTRIB_UPDATE_CHAIN);
-      ureq.deleteByQuery(q).process(client);
-    }
-     ***/
-  }// serial commit...
-  
-  /*
-   * (non-Javadoc)
-   * 
-   * @see org.apache.solr.BaseDistributedSearchTestCase#doTest()
-   * 
-   * Create 3 shards, each with one replica
-   */
-  @Override
-  public void doTest() throws Exception {
-    boolean testFinished = false;
-    try {
-      handle.clear();
-      handle.put("QTime", SKIPVAL);
-      handle.put("timestamp", SKIPVAL);
-      
-      indexr(id, 1, i1, 100, tlong, 100, t1,
-          "now is the time for all good men", "foo_f", 1.414f, "foo_b", "true",
-          "foo_d", 1.414d);
-      
-      // make sure we are in a steady state...
-      waitForRecoveriesToFinish(false);
-      
-      commit();
-      
-      assertDocCounts(false);
-      
-      indexAbunchOfDocs();
-      
-      // check again 
-      waitForRecoveriesToFinish(false);
-      
-      commit();
-      
-      assertDocCounts(VERBOSE);
-      checkQueries();
-      
-      assertDocCounts(VERBOSE);
-      
-      query("q", "*:*", "sort", "n_tl1 desc");
-      
-      brindDownShardIndexSomeDocsAndRecover();
-      
-      query("q", "*:*", "sort", "n_tl1 desc");
-      
-      // test adding another replica to a shard - it should do a
-      // recovery/replication to pick up the index from the leader
-      addNewReplica();
-      
-      long docId = testUpdateAndDelete();
-      
-      // index a bad doc...
-      try {
-        indexr(t1, "a doc with no id");
-        fail("this should fail");
-      } catch (SolrException e) {
-        // expected
-      }
-      
-      // TODO: bring this to it's own method?
-      // try indexing to a leader that has no replicas up
-      ZkNodeProps leaderProps = zkStateReader.getLeaderProps(
-          DEFAULT_COLLECTION, SHARD2);
-      
-      String nodeName = leaderProps.get(ZkStateReader.NODE_NAME_PROP);
-      chaosMonkey.stopShardExcept(SHARD2, nodeName);
-      
-      SolrServer client = getClient(nodeName);
-      
-      index_specific(client, "id", docId + 1, t1, "what happens here?");
-      
-      // expire a session...
-      CloudJettyRunner cloudJetty = shardToJetty.get("shard1").get(0);
-      chaosMonkey.expireSession(cloudJetty.jetty);
-      
-      indexr("id", docId + 1, t1, "slip this doc in");
-      
-      waitForRecoveriesToFinish(false);
-      
-      checkShardConsistency("shard1");
-      
-      testFinished = true;
-    } finally {
-      if (!testFinished) {
-        printLayoutOnTearDown = true;
-      }
-    }
-    
-  }
-  
-  private long testUpdateAndDelete() throws Exception {
-    long docId = 99999999L;
-    indexr("id", docId, t1, "originalcontent");
-    
-    commit();
-    
-    ModifiableSolrParams params = new ModifiableSolrParams();
-    params.add("q", t1 + ":originalcontent");
-    QueryResponse results = clients.get(0).query(params);
-    assertEquals(1, results.getResults().getNumFound());
-    
-    // update doc
-    indexr("id", docId, t1, "updatedcontent");
-    
-    commit();
-    
-    results = clients.get(0).query(params);
-    assertEquals(0, results.getResults().getNumFound());
-    
-    params.set("q", t1 + ":updatedcontent");
-    
-    results = clients.get(0).query(params);
-    assertEquals(1, results.getResults().getNumFound());
-    
-    UpdateRequest uReq = new UpdateRequest();
-    // uReq.setParam(UpdateParams.UPDATE_CHAIN, DISTRIB_UPDATE_CHAIN);
-    uReq.deleteById(Long.toString(docId)).process(clients.get(0));
-    
-    commit();
-    
-    results = clients.get(0).query(params);
-    assertEquals(0, results.getResults().getNumFound());
-    return docId;
-  }
-  
-  private void addNewReplica() throws Exception {
-    JettySolrRunner newReplica = createJettys(1).get(0);
-    
-    waitForRecoveriesToFinish(false);
-    
-    // new server should be part of first shard
-    // how many docs are on the new shard?
-    for (CloudJettyRunner cjetty : shardToJetty.get("shard1")) {
-      if (VERBOSE) System.err.println("total:"
-          + cjetty.client.solrClient.query(new SolrQuery("*:*")).getResults().getNumFound());
-    }
-    
-    checkShardConsistency("shard1");
-    
-    assertDocCounts(VERBOSE);
-  }
-  
-  protected void waitForRecoveriesToFinish(boolean verbose)
-      throws Exception {
-    super.waitForRecoveriesToFinish(DEFAULT_COLLECTION, zkStateReader, verbose);
-  }
-  
-  protected void waitForRecoveriesToFinish(boolean verbose, int timeoutSeconds)
-      throws Exception {
-    super.waitForRecoveriesToFinish(DEFAULT_COLLECTION, zkStateReader, verbose, true, timeoutSeconds);
-  }
-  
-  private void brindDownShardIndexSomeDocsAndRecover() throws Exception {
-    SolrQuery query = new SolrQuery("*:*");
-    query.set("distrib", false);
-    
-    commit();
-    
-    long deadShardCount = shardToJetty.get(SHARD2).get(0).client.solrClient
-        .query(query).getResults().getNumFound();
-
-    query("q", "*:*", "sort", "n_tl1 desc");
-    
-    // kill a shard
-    CloudJettyRunner deadShard = chaosMonkey.stopShard(SHARD2, 0);
-    cloudClient.connect();
-
-    // we are careful to make sure the downed node is no longer in the state,
-    // because on some systems (especially freebsd w/ blackhole enabled), trying
-    // to talk to a downed node causes grief
-    Set<CloudJettyRunner> jetties = new HashSet<CloudJettyRunner>();
-    jetties.addAll(shardToJetty.get(SHARD2));
-    jetties.remove(deadShard);
-    
-    for (CloudJettyRunner cjetty : jetties) {
-      waitToSeeNotLive(((SolrDispatchFilter) cjetty.jetty.getDispatchFilter()
-          .getFilter()).getCores().getZkController().getZkStateReader(),
-          deadShard);
-    }
-    waitToSeeNotLive(cloudClient.getZkStateReader(), deadShard);
-
-    // ensure shard is dead
-    try {
-      index_specific(deadShard.client.solrClient, id, 999, i1, 107, t1,
-          "specific doc!");
-      fail("This server should be down and this update should have failed");
-    } catch (SolrServerException e) {
-      // expected..
-    }
-    
-    commit();
-    query("q", "*:*", "sort", "n_tl1 desc");
-    
-    // long cloudClientDocs = cloudClient.query(new
-    // SolrQuery("*:*")).getResults().getNumFound();
-    // System.out.println("clouddocs:" + cloudClientDocs);
-    
-    // try to index to a living shard at shard2
-
-	
-    long numFound1 = cloudClient.query(new SolrQuery("*:*")).getResults().getNumFound();
-    
-    index_specific(shardToJetty.get(SHARD2).get(1).client.solrClient, id, 1000, i1, 108, t1,
-        "specific doc!");
-    
-    commit();
-    
-    checkShardConsistency(true, false);
-    
-    query("q", "*:*", "sort", "n_tl1 desc");
-    
-    // try adding a doc with CloudSolrServer
-    cloudClient.setDefaultCollection(DEFAULT_COLLECTION);
-
-    long numFound2 = cloudClient.query(new SolrQuery("*:*")).getResults().getNumFound();
-    
-    assertEquals(numFound1 + 1, numFound2);
-    
-    SolrInputDocument doc = new SolrInputDocument();
-    doc.addField("id", 1001);
-    
-    controlClient.add(doc);
-    
-    UpdateRequest ureq = new UpdateRequest();
-    ureq.add(doc);
-    // ureq.setParam("update.chain", DISTRIB_UPDATE_CHAIN);
-    ureq.process(cloudClient);
-    
-    commit();
-    
-    query("q", "*:*", "sort", "n_tl1 desc");
-    
-    long numFound3 = cloudClient.query(new SolrQuery("*:*")).getResults().getNumFound();
-    
-    // lets just check that the one doc since last commit made it in...
-    assertEquals(numFound2 + 1, numFound3);
-    
-    // test debugging
-    testDebugQueries();
-    
-    if (VERBOSE) {
-      System.err.println(controlClient.query(new SolrQuery("*:*")).getResults()
-          .getNumFound());
-      
-      for (SolrServer client : clients) {
-        try {
-          SolrQuery q = new SolrQuery("*:*");
-          q.set("distrib", false);
-          System.err.println(client.query(q).getResults()
-              .getNumFound());
-        } catch (Exception e) {
-          
-        }
-      }
-    }
-    // TODO: This test currently fails because debug info is obtained only
-    // on shards with matches.
-    // query("q","matchesnothing","fl","*,score", "debugQuery", "true");
-    
-    // this should trigger a recovery phase on deadShard
-    ChaosMonkey.start(deadShard.jetty);
-    
-    // make sure we have published we are recovering
-    Thread.sleep(1500);
-    
-    waitForRecoveriesToFinish(false);
-    
-    deadShardCount = shardToJetty.get(SHARD2).get(0).client.solrClient
-        .query(query).getResults().getNumFound();
-    // if we properly recovered, we should now have the couple missing docs that
-    // came in while shard was down
-    checkShardConsistency(true, false);
-    
-    
-    // recover over 100 docs so we do more than just peer sync (replicate recovery)
-    chaosMonkey.stopJetty(deadShard);
-    
-    for (CloudJettyRunner cjetty : jetties) {
-      waitToSeeNotLive(((SolrDispatchFilter) cjetty.jetty.getDispatchFilter()
-          .getFilter()).getCores().getZkController().getZkStateReader(),
-          deadShard);
-    }
-    waitToSeeNotLive(cloudClient.getZkStateReader(), deadShard);
-    
-    for (int i = 0; i < 226; i++) {
-      doc = new SolrInputDocument();
-      doc.addField("id", 2000 + i);
-      controlClient.add(doc);
-      ureq = new UpdateRequest();
-      ureq.add(doc);
-      // ureq.setParam("update.chain", DISTRIB_UPDATE_CHAIN);
-      ureq.process(cloudClient);
-    }
-    commit();
-    
-    Thread.sleep(1500);
-    
-    ChaosMonkey.start(deadShard.jetty);
-    
-    // make sure we have published we are recovering
-    Thread.sleep(1500);
-    
-    waitForRecoveriesToFinish(false);
-    
-    checkShardConsistency(true, false);
-  }
-  
-  private void testDebugQueries() throws Exception {
-    handle.put("explain", SKIPVAL);
-    handle.put("debug", UNORDERED);
-    handle.put("time", SKIPVAL);
-    query("q", "now their fox sat had put", "fl", "*,score",
-        CommonParams.DEBUG_QUERY, "true");
-    query("q", "id:[1 TO 5]", CommonParams.DEBUG_QUERY, "true");
-    query("q", "id:[1 TO 5]", CommonParams.DEBUG, CommonParams.TIMING);
-    query("q", "id:[1 TO 5]", CommonParams.DEBUG, CommonParams.RESULTS);
-    query("q", "id:[1 TO 5]", CommonParams.DEBUG, CommonParams.QUERY);
-  }
-  
-  private void checkQueries() throws Exception {
-
-    handle.put("_version_", SKIPVAL);
-
-    query("q", "*:*", "sort", "n_tl1 desc");
-
-    handle.put("response", UNORDERED);  // get?ids=a,b,c requests are unordered
-    String ids = "987654";
-    for (int i=0; i<20; i++) {
-      query("qt","/get", "id",Integer.toString(i));
-      query("qt","/get", "ids",Integer.toString(i));
-      ids = ids + ',' + Integer.toString(i);
-      query("qt","/get", "ids",ids);
-    }
-    handle.remove("response");
-
-
-
-    // random value sort
-    for (String f : fieldNames) {
-      query("q", "*:*", "sort", f + " desc");
-      query("q", "*:*", "sort", f + " asc");
-    }
-    
-    // these queries should be exactly ordered and scores should exactly match
-    query("q", "*:*", "sort", i1 + " desc");
-    query("q", "*:*", "sort", i1 + " asc");
-    query("q", "*:*", "sort", i1 + " desc", "fl", "*,score");
-    query("q", "*:*", "sort", "n_tl1 asc", "fl", "score"); // test legacy
-                                                           // behavior -
-                                                           // "score"=="*,score"
-    query("q", "*:*", "sort", "n_tl1 desc");
-    handle.put("maxScore", SKIPVAL);
-    query("q", "{!func}" + i1);// does not expect maxScore. So if it comes
-                               // ,ignore it.
-                               // JavaBinCodec.writeSolrDocumentList()
-    // is agnostic of request params.
-    handle.remove("maxScore");
-    query("q", "{!func}" + i1, "fl", "*,score"); // even scores should match
-                                                 // exactly here
-    
-    handle.put("highlighting", UNORDERED);
-    handle.put("response", UNORDERED);
-    
-    handle.put("maxScore", SKIPVAL);
-    query("q", "quick");
-    query("q", "all", "fl", "id", "start", "0");
-    query("q", "all", "fl", "foofoofoo", "start", "0"); // no fields in returned
-                                                        // docs
-    query("q", "all", "fl", "id", "start", "100");
-    
-    handle.put("score", SKIPVAL);
-    query("q", "quick", "fl", "*,score");
-    query("q", "all", "fl", "*,score", "start", "1");
-    query("q", "all", "fl", "*,score", "start", "100");
-    
-    query("q", "now their fox sat had put", "fl", "*,score", "hl", "true",
-        "hl.fl", t1);
-    
-    query("q", "now their fox sat had put", "fl", "foofoofoo", "hl", "true",
-        "hl.fl", t1);
-    
-    query("q", "matchesnothing", "fl", "*,score");
-    
-    query("q", "*:*", "rows", 100, "facet", "true", "facet.field", t1);
-    query("q", "*:*", "rows", 100, "facet", "true", "facet.field", t1,
-        "facet.limit", -1, "facet.sort", "count");
-    query("q", "*:*", "rows", 100, "facet", "true", "facet.field", t1,
-        "facet.limit", -1, "facet.sort", "count", "facet.mincount", 2);
-    query("q", "*:*", "rows", 100, "facet", "true", "facet.field", t1,
-        "facet.limit", -1, "facet.sort", "index");
-    query("q", "*:*", "rows", 100, "facet", "true", "facet.field", t1,
-        "facet.limit", -1, "facet.sort", "index", "facet.mincount", 2);
-    query("q", "*:*", "rows", 100, "facet", "true", "facet.field", t1,
-        "facet.limit", 1);
-    query("q", "*:*", "rows", 100, "facet", "true", "facet.query", "quick",
-        "facet.query", "all", "facet.query", "*:*");
-    query("q", "*:*", "rows", 100, "facet", "true", "facet.field", t1,
-        "facet.offset", 1);
-    query("q", "*:*", "rows", 100, "facet", "true", "facet.field", t1,
-        "facet.mincount", 2);
-    
-    // test faceting multiple things at once
-    query("q", "*:*", "rows", 100, "facet", "true", "facet.query", "quick",
-        "facet.query", "all", "facet.query", "*:*", "facet.field", t1);
-    
-    // test filter tagging, facet exclusion, and naming (multi-select facet
-    // support)
-    query("q", "*:*", "rows", 100, "facet", "true", "facet.query",
-        "{!key=myquick}quick", "facet.query", "{!key=myall ex=a}all",
-        "facet.query", "*:*", "facet.field", "{!key=mykey ex=a}" + t1,
-        "facet.field", "{!key=other ex=b}" + t1, "facet.field",
-        "{!key=again ex=a,b}" + t1, "facet.field", t1, "fq",
-        "{!tag=a}id:[1 TO 7]", "fq", "{!tag=b}id:[3 TO 9]");
-    query("q", "*:*", "facet", "true", "facet.field",
-        "{!ex=t1}SubjectTerms_mfacet", "fq",
-        "{!tag=t1}SubjectTerms_mfacet:(test 1)", "facet.limit", "10",
-        "facet.mincount", "1");
-    
-    // test field that is valid in schema but missing in all shards
-    query("q", "*:*", "rows", 100, "facet", "true", "facet.field",
-        missingField, "facet.mincount", 2);
-    // test field that is valid in schema and missing in some shards
-    query("q", "*:*", "rows", 100, "facet", "true", "facet.field", oddField,
-        "facet.mincount", 2);
-    
-    query("q", "*:*", "sort", i1 + " desc", "stats", "true", "stats.field", i1);
-    
-    // Try to get better coverage for refinement queries by turning off over
-    // requesting.
-    // This makes it much more likely that we may not get the top facet values
-    // and hence
-    // we turn of that checking.
-    handle.put("facet_fields", SKIPVAL);
-    query("q", "*:*", "rows", 0, "facet", "true", "facet.field", t1,
-        "facet.limit", 5, "facet.shard.limit", 5);
-    // check a complex key name
-    query("q", "*:*", "rows", 0, "facet", "true", "facet.field",
-        "{!key='a b/c \\' \\} foo'}" + t1, "facet.limit", 5,
-        "facet.shard.limit", 5);
-    handle.remove("facet_fields");
-    
-    query("q", "*:*", "sort", "n_tl1 desc");
-    
-    // index the same document to two shards and make sure things
-    // don't blow up.
-    // assumes first n clients are first n shards
-    if (clients.size() >= 2) {
-      index(id, 100, i1, 107, t1, "oh no, a duplicate!");
-      for (int i = 0; i < shardCount; i++) {
-        index_specific(i, id, 100, i1, 107, t1, "oh no, a duplicate!");
-      }
-      commit();
-      query("q", "duplicate", "hl", "true", "hl.fl", t1);
-      query("q", "fox duplicate horses", "hl", "true", "hl.fl", t1);
-      query("q", "*:*", "rows", 100);
-    }
-  }
-  
-  private void indexAbunchOfDocs() throws Exception {
-    indexr(id, 2, i1, 50, tlong, 50, t1, "to come to the aid of their country.");
-    indexr(id, 3, i1, 2, tlong, 2, t1, "how now brown cow");
-    indexr(id, 4, i1, -100, tlong, 101, t1,
-        "the quick fox jumped over the lazy dog");
-    indexr(id, 5, i1, 500, tlong, 500, t1,
-        "the quick fox jumped way over the lazy dog");
-    indexr(id, 6, i1, -600, tlong, 600, t1, "humpty dumpy sat on a wall");
-    indexr(id, 7, i1, 123, tlong, 123, t1, "humpty dumpy had a great fall");
-    indexr(id, 8, i1, 876, tlong, 876, t1,
-        "all the kings horses and all the kings men");
-    indexr(id, 9, i1, 7, tlong, 7, t1, "couldn't put humpty together again");
-    indexr(id, 10, i1, 4321, tlong, 4321, t1, "this too shall pass");
-    indexr(id, 11, i1, -987, tlong, 987, t1,
-        "An eye for eye only ends up making the whole world blind.");
-    indexr(id, 12, i1, 379, tlong, 379, t1,
-        "Great works are performed, not by strength, but by perseverance.");
-    indexr(id, 13, i1, 232, tlong, 232, t1, "no eggs on wall, lesson learned",
-        oddField, "odd man out");
-    
-    indexr(id, 14, "SubjectTerms_mfacet", new String[] {"mathematical models",
-        "mathematical analysis"});
-    indexr(id, 15, "SubjectTerms_mfacet", new String[] {"test 1", "test 2",
-        "test3"});
-    indexr(id, 16, "SubjectTerms_mfacet", new String[] {"test 1", "test 2",
-        "test3"});
-    String[] vals = new String[100];
-    for (int i = 0; i < 100; i++) {
-      vals[i] = "test " + i;
-    }
-    indexr(id, 17, "SubjectTerms_mfacet", vals);
-    
-    for (int i = 100; i < 150; i++) {
-      indexr(id, i);
-    }
-  }
-  
-  protected void checkShardConsistency(String shard) throws Exception {
-    checkShardConsistency(shard, false);
-  }
-  
-  protected String checkShardConsistency(String shard, boolean verbose)
-      throws Exception {
-    
-    List<CloudJettyRunner> solrJetties = shardToJetty.get(shard);
-    if (solrJetties == null) {
-      throw new RuntimeException("shard not found:" + shard + " keys:"
-          + shardToJetty.keySet());
-    }
-    long num = -1;
-    long lastNum = -1;
-    String failMessage = null;
-    if (verbose) System.err.println("check const of " + shard);
-    int cnt = 0;
-    
-    assertEquals(
-        "The client count does not match up with the shard count for slice:"
-            + shard,
-        zkStateReader.getClusterState().getSlice(DEFAULT_COLLECTION, shard)
-            .getShards().size(), solrJetties.size());
-
-    SolrServer lastClient = null;
-    for (CloudJettyRunner cjetty : solrJetties) {
-      ZkNodeProps props = cjetty.info;
-      if (verbose) System.err.println("client" + cnt++);
-      if (verbose) System.err.println("PROPS:" + props);
-      
-      try {
-        SolrQuery query = new SolrQuery("*:*");
-        query.set("distrib", false);
-        num = cjetty.client.solrClient.query(query).getResults().getNumFound();
-      } catch (SolrServerException e) {
-        if (verbose) System.err.println("error contacting client: "
-            + e.getMessage() + "\n");
-        continue;
-      } catch (SolrException e) {
-        if (verbose) System.err.println("error contacting client: "
-            + e.getMessage() + "\n");
-        continue;
-      }
-      
-      boolean live = false;
-      String nodeName = props.get(ZkStateReader.NODE_NAME_PROP);
-      if (zkStateReader.getClusterState().liveNodesContain(nodeName)) {
-        live = true;
-      }
-      if (verbose) System.err.println(" live:" + live);
-      
-      if (verbose) System.err.println(" num:" + num + "\n");
-      
-      boolean active = props.get(ZkStateReader.STATE_PROP).equals(
-          ZkStateReader.ACTIVE);
-      if (active && live) {
-        if (lastNum > -1 && lastNum != num && failMessage == null) {
-          failMessage = shard + " is not consistent.  Got " + lastNum + " from " + lastClient + "lastClient"
-              + " and got " + num + " from " + cjetty.url;
-
-          if (verbose || true) {
-            System.err.println("######" + failMessage);
-            SolrQuery query = new SolrQuery("*:*");
-            query.set("distrib", false);
-            query.set("fl","id,_version_");
-            query.set("rows","1000");
-            query.set("sort","id asc");
-
-            SolrDocumentList lst1 = lastClient.query(query).getResults();
-            SolrDocumentList lst2 = cjetty.client.solrClient.query(query).getResults();
-
-            showDiff(lst1, lst2, lastClient.toString(), cjetty.client.solrClient.toString());
-          }
-
-        }
-        lastNum = num;
-        lastClient = cjetty.client.solrClient;
-      }
-    }
-    return failMessage;
-    
-  }
-  
-  void showDiff(SolrDocumentList a, SolrDocumentList b, String aName, String bName) {
-    System.err.println("######"+aName+ ": " + a);
-    System.err.println("######"+bName+ ": " + b);
-    System.err.println("###### sizes=" + a.size() + "," + b.size());
-    
-    Set<Map> setA = new HashSet<Map>();
-    for (SolrDocument sdoc : a) {
-      setA.add(new HashMap(sdoc));
-    }
-
-    Set<Map> setB = new HashSet<Map>();
-    for (SolrDocument sdoc : b) {
-      setB.add(new HashMap(sdoc));
-    }
-
-    Set<Map> onlyInA = new HashSet<Map>(setA);
-    onlyInA.removeAll(setB);
-    Set<Map> onlyInB = new HashSet<Map>(setB);
-    onlyInB.removeAll(setA);
-
-    if (onlyInA.size() > 0) {
-      System.err.println("###### Only in " + aName + ": " + onlyInA);
-    }
-    if (onlyInB.size() > 0) {
-      System.err.println("###### Only in " + bName + ": " + onlyInB);
-    }
-  }
-  
-  protected void checkShardConsistency() throws Exception {
-    checkShardConsistency(true, false);
-  }
-  
-  protected void checkShardConsistency(boolean checkVsControl, boolean verbose)
-      throws Exception {
-    long docs = controlClient.query(new SolrQuery("*:*")).getResults()
-        .getNumFound();
-    if (verbose) System.err.println("Control Docs:" + docs);
-    
-    updateMappingsFromZk(jettys, clients);
-    
-    Set<String> theShards = shardToJetty.keySet();
-    String failMessage = null;
-    for (String shard : theShards) {
-      String shardFailMessage = checkShardConsistency(shard, verbose);
-      if (shardFailMessage != null && failMessage == null) {
-        failMessage = shardFailMessage;
-      }
-    }
-    
-    if (failMessage != null) {
-      fail(failMessage);
-    }
-    
-    if (checkVsControl) {
-      // now check that the right # are on each shard
-      theShards = shardToJetty.keySet();
-      int cnt = 0;
-      for (String s : theShards) {
-        int times = shardToJetty.get(s).size();
-        for (int i = 0; i < times; i++) {
-          try {
-            CloudJettyRunner cjetty = shardToJetty.get(s).get(i);
-            ZkNodeProps props = cjetty.info;
-            SolrServer client = cjetty.client.solrClient;
-            boolean active = props.get(ZkStateReader.STATE_PROP).equals(
-                ZkStateReader.ACTIVE);
-            if (active) {
-              SolrQuery query = new SolrQuery("*:*");
-              query.set("distrib", false);
-              long results = client.query(query).getResults().getNumFound();
-              if (verbose) System.err.println(new ZkCoreNodeProps(props)
-                  .getCoreUrl() + " : " + results);
-              if (verbose) System.err.println("shard:"
-                  + props.get(ZkStateReader.SHARD_ID_PROP));
-              cnt += results;
-              break;
-            }
-          } catch (SolrServerException e) {
-            // if we have a problem, try the next one
-            if (i == times - 1) {
-              throw e;
-            }
-          }
-        }
-      }
-      
-      SolrQuery q = new SolrQuery("*:*");
-      long cloudClientDocs = cloudClient.query(q).getResults().getNumFound();
-      assertEquals(
-          "adding up the # of docs on each shard does not match the control - cloud client returns:"
-              + cloudClientDocs, docs, cnt);
-    }
-  }
-  
-  private SolrServer getClient(String nodeName) {
-    for (CloudJettyRunner cjetty : cloudJettys) {
-      CloudSolrServerClient client = cjetty.client;
-      if (client.shardName.equals(nodeName)) {
-        return client.solrClient;
-      }
-    }
-    return null;
-  }
-  
-  protected void assertDocCounts(boolean verbose) throws Exception {
-    // TODO: as we create the clients, we should build a map from shard to
-    // node/client
-    // and node/client to shard?
-    if (verbose) System.err.println("control docs:"
-        + controlClient.query(new SolrQuery("*:*")).getResults().getNumFound()
-        + "\n\n");
-    long controlCount = controlClient.query(new SolrQuery("*:*")).getResults()
-        .getNumFound();
-    
-    // do some really inefficient mapping...
-    ZkStateReader zk = new ZkStateReader(zkServer.getZkAddress(), 10000,
-        AbstractZkTestCase.TIMEOUT);
-    Map<String,Slice> slices = null;
-    ClusterState clusterState;
-    try {
-      zk.createClusterStateWatchersAndUpdate();
-      clusterState = zk.getClusterState();
-      slices = clusterState.getSlices(DEFAULT_COLLECTION);
-    } finally {
-      zk.close();
-    }
-    
-    if (slices == null) {
-      throw new RuntimeException("Could not find collection "
-          + DEFAULT_COLLECTION + " in " + clusterState.getCollections());
-    }
-    
-    for (CloudJettyRunner cjetty : cloudJettys) {
-      CloudSolrServerClient client = cjetty.client;
-      for (Map.Entry<String,Slice> slice : slices.entrySet()) {
-        Map<String,ZkNodeProps> theShards = slice.getValue().getShards();
-        for (Map.Entry<String,ZkNodeProps> shard : theShards.entrySet()) {
-          String shardName = new URI(
-              ((HttpSolrServer) client.solrClient).getBaseURL()).getPort()
-              + "_solr_";
-          if (verbose && shard.getKey().endsWith(shardName)) {
-            System.err.println("shard:" + slice.getKey());
-            System.err.println(shard.getValue());
-          }
-        }
-      }
-      
-      long count = 0;
-      String currentState = cjetty.info.get(ZkStateReader.STATE_PROP);
-      if (currentState != null
-          && currentState.equals(ZkStateReader.ACTIVE)
-          && zkStateReader.getClusterState().liveNodesContain(
-              cjetty.info.get(ZkStateReader.NODE_NAME_PROP))) {
-        SolrQuery query = new SolrQuery("*:*");
-        query.set("distrib", false);
-        count = client.solrClient.query(query).getResults().getNumFound();
-      }
-      
-      if (verbose) System.err.println("client docs:" + count + "\n\n");
-    }
-    if (verbose) System.err.println("control docs:"
-        + controlClient.query(new SolrQuery("*:*")).getResults().getNumFound()
-        + "\n\n");
-    SolrQuery query = new SolrQuery("*:*");
-    assertEquals("Doc Counts do not add up", controlCount,
-        cloudClient.query(query).getResults().getNumFound());
-  }
-  
-  @Override
-  protected QueryResponse queryServer(ModifiableSolrParams params)
-      throws SolrServerException {
-    
-    if (r.nextBoolean()) params.set("collection", DEFAULT_COLLECTION);
-    
-    QueryResponse rsp = cloudClient.query(params);
-    return rsp;
-  }
-  
-  abstract class StopableThread extends Thread {
-    public StopableThread(String name) {
-      super(name);
-    }
-    public abstract void safeStop();
-  }
-  
-  class StopableIndexingThread extends StopableThread {
-    private volatile boolean stop = false;
-    protected final int startI;
-    protected final List<Integer> deletes = new ArrayList<Integer>();
-    protected final AtomicInteger fails = new AtomicInteger();
-    protected boolean doDeletes;
-    
-    public StopableIndexingThread(int startI, boolean doDeletes) {
-      super("StopableIndexingThread");
-      this.startI = startI;
-      this.doDeletes = doDeletes;
-      setDaemon(true);
-    }
-    
-    @Override
-    public void run() {
-      int i = startI;
-      int numDeletes = 0;
-      int numAdds = 0;
-      
-      while (true && !stop) {
-        ++i;
-        
-        if (doDeletes && random().nextBoolean() && deletes.size() > 0) {
-          Integer delete = deletes.remove(0);
-          try {
-            numDeletes++;
-            controlClient.deleteById(Integer.toString(delete));
-            cloudClient.deleteById(Integer.toString(delete));
-          } catch (Exception e) {
-            System.err.println("REQUEST FAILED:");
-            e.printStackTrace();
-            if (e instanceof SolrServerException) {
-              System.err.println("ROOT CAUSE:");
-              ((SolrServerException) e).getRootCause().printStackTrace();
-            }
-            fails.incrementAndGet();
-          }
-        }
-        
-        try {
-          numAdds++;
-          indexr(id, i, i1, 50, tlong, 50, t1,
-              "to come to the aid of their country.");
-        } catch (Exception e) {
-          System.err.println("REQUEST FAILED:");
-          e.printStackTrace();
-          if (e instanceof SolrServerException) {
-            System.err.println("ROOT CAUSE:");
-            ((SolrServerException) e).getRootCause().printStackTrace();
-          }
-          fails.incrementAndGet();
-        }
-        
-        if (doDeletes && random().nextBoolean()) {
-          deletes.add(i);
-        }
-        
-      }
-      
-      System.err.println("added docs:" + numAdds + " with " + fails + " fails"
-          + " deletes:" + numDeletes);
-    }
-    
-    public void safeStop() {
-      stop = true;
-    }
-    
-    public int getFails() {
-      return fails.get();
-    }
-    
-  };
-  
-  class StopableSearchThread extends StopableThread {
-    private volatile boolean stop = false;
-    protected final AtomicInteger fails = new AtomicInteger();
-    private String[] QUERIES = new String[] {"to come","their country","aid","co*"};
-    
-    public StopableSearchThread() {
-      super("StopableSearchThread");
-      setDaemon(true);
-    }
-    
-    @Override
-    public void run() {
-      Random random = random();
-      int numSearches = 0;
-      
-      while (true && !stop) {
-        numSearches++;
-        try {
-          //to come to the aid of their country.
-          cloudClient.query(new SolrQuery(QUERIES[random.nextInt(QUERIES.length)]));
-        } catch (Exception e) {
-          System.err.println("QUERY REQUEST FAILED:");
-          e.printStackTrace();
-          if (e instanceof SolrServerException) {
-            System.err.println("ROOT CAUSE:");
-            ((SolrServerException) e).getRootCause().printStackTrace();
-          }
-          fails.incrementAndGet();
-        }
-        try {
-          Thread.sleep(random.nextInt(4000) + 300);
-        } catch (InterruptedException e) {
-          Thread.currentThread().interrupt();
-        }
-      }
-      
-      System.err.println("num searches done:" + numSearches + " with " + fails + " fails");
-    }
-    
-    public void safeStop() {
-      stop = true;
-    }
-    
-    public int getFails() {
-      return fails.get();
-    }
-    
-  };
-  
-  protected void waitForThingsToLevelOut(int waitForRecTimeSeconds) throws Exception {
-    log.info("Wait for recoveries to finish - wait " + waitForRecTimeSeconds + " for each attempt");
-    int cnt = 0;
-    boolean retry = false;
-    do {
-      waitForRecoveriesToFinish(VERBOSE, waitForRecTimeSeconds);
-      
-      try {
-        commit();
-      } catch (Exception e) {
-        // we don't care if this commit fails on some nodes
-      }
-      
-      updateMappingsFromZk(jettys, clients);
-      
-      Set<String> theShards = shardToJetty.keySet();
-      String failMessage = null;
-      for (String shard : theShards) {
-        failMessage = checkShardConsistency(shard, false);
-      }
-      
-      if (failMessage != null) {
-        retry  = true;
-      }
-      cnt++;
-      if (cnt > 2) break;
-      Thread.sleep(4000);
-    } while (retry);
-  }
-  
-  @Override
-  @After
-  public void tearDown() throws Exception {
-    if (VERBOSE || printLayoutOnTearDown) {
-      super.printLayout();
-    }
-    ((HttpSolrServer) controlClient).shutdown();
-    if (cloudClient != null) {
-      cloudClient.shutdown();
-    }
-    if (zkStateReader != null) {
-      zkStateReader.close();
-    }
-    super.tearDown();
-    
-    System.clearProperty("zkHost");
-    System.clearProperty("numShards");
-  }
-  
-  protected void commit() throws Exception {
-    controlClient.commit();
-    cloudClient.commit();
-  }
-  
-  protected void destroyServers() throws Exception {
-    ChaosMonkey.stop(controlJetty);
-    for (JettySolrRunner jetty : jettys) {
-      try {
-        ChaosMonkey.stop(jetty);
-      } catch (Exception e) {
-        log.error("", e);
-      }
-    }
-    clients.clear();
-    jettys.clear();
-  }
-  
-  protected SolrServer createNewSolrServer(int port) {
-    try {
-      // setup the server...
-      String url = "http://localhost:" + port + context + "/"
-          + DEFAULT_COLLECTION;
-      HttpSolrServer s = new HttpSolrServer(url);
-      s.setConnectionTimeout(DEFAULT_CONNECTION_TIMEOUT);
-      s.setSoTimeout(20000);
-      s.setDefaultMaxConnectionsPerHost(100);
-      s.setMaxTotalConnections(100);
-      return s;
-    } catch (Exception ex) {
-      throw new RuntimeException(ex);
-    }
-  }
-  
-  protected void waitToSeeNotLive(ZkStateReader zkStateReader,
-      CloudJettyRunner cjetty) throws InterruptedException {
-    int tries = 0;
-    while (zkStateReader.getClusterState()
-        .liveNodesContain(cjetty.info.get(ZkStateReader.NODE_NAME_PROP))) {
-      if (tries++ == 120) {
-        fail("Shard still reported as live in zk");
-      }
-      Thread.sleep(1000);
-    }
-  }
-}
diff --git a/solr/core/src/test/org/apache/solr/cloud/RecoveryZkTest.java b/solr/core/src/test/org/apache/solr/cloud/RecoveryZkTest.java
index dcb90ed..fd74ee7 100644
--- a/solr/core/src/test/org/apache/solr/cloud/RecoveryZkTest.java
+++ b/solr/core/src/test/org/apache/solr/cloud/RecoveryZkTest.java
@@ -26,12 +26,11 @@ import org.apache.solr.client.solrj.embedded.JettySolrRunner;
 import org.apache.solr.common.SolrInputDocument;
 import org.junit.AfterClass;
 import org.junit.BeforeClass;
-import org.junit.Ignore;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
 @Slow
-public class RecoveryZkTest extends FullSolrCloudTest {
+public class RecoveryZkTest extends AbstractFullDistribZkTestBase {
 
   //private static final String DISTRIB_UPDATE_CHAIN = "distrib-update-chain";
   private static Logger log = LoggerFactory.getLogger(RecoveryZkTest.class);
diff --git a/solr/core/src/test/org/apache/solr/cloud/SyncSliceTest.java b/solr/core/src/test/org/apache/solr/cloud/SyncSliceTest.java
index 78e4742..c1811d3 100644
--- a/solr/core/src/test/org/apache/solr/cloud/SyncSliceTest.java
+++ b/solr/core/src/test/org/apache/solr/cloud/SyncSliceTest.java
@@ -38,15 +38,13 @@ import org.junit.After;
 import org.junit.AfterClass;
 import org.junit.Before;
 import org.junit.BeforeClass;
-import org.junit.Ignore;
 
 /**
  * Test sync phase that occurs when Leader goes down and a new Leader is
  * elected.
  */
 @Slow
-@Ignore("ignore while i look into")
-public class SyncSliceTest extends FullSolrCloudTest {
+public class SyncSliceTest extends AbstractFullDistribZkTestBase {
   
   @BeforeClass
   public static void beforeSuperClass() {
@@ -88,7 +86,7 @@ public class SyncSliceTest extends FullSolrCloudTest {
     handle.put("QTime", SKIPVAL);
     handle.put("timestamp", SKIPVAL);
     
-    waitForThingsToLevelOut();
+    waitForThingsToLevelOut(15);
 
     del("*:*");
     List<String> skipServers = new ArrayList<String>();
@@ -131,7 +129,7 @@ public class SyncSliceTest extends FullSolrCloudTest {
     HttpSolrServer baseServer = new HttpSolrServer(baseUrl);
     baseServer.request(request);
     
-    waitForThingsToLevelOut();
+    waitForThingsToLevelOut(15);
     
     checkShardConsistency(false, true);
     
@@ -161,7 +159,7 @@ public class SyncSliceTest extends FullSolrCloudTest {
     // to talk to a downed node causes grief
     waitToSeeDownInClusterState(leaderJetty, jetties);
 
-    waitForThingsToLevelOut();
+    waitForThingsToLevelOut(15);
     
     checkShardConsistency(false, true);
     
@@ -182,7 +180,7 @@ public class SyncSliceTest extends FullSolrCloudTest {
     // give a moment to be sure it has started recovering
     Thread.sleep(2000);
     
-    waitForThingsToLevelOut();
+    waitForThingsToLevelOut(15);
     waitForRecoveriesToFinish(false);
     
     skipServers = getRandomOtherJetty(leaderJetty, null);
@@ -226,6 +224,7 @@ public class SyncSliceTest extends FullSolrCloudTest {
     waitForRecoveriesToFinish(false);
 
     checkShardConsistency(true, true);
+    
   }
 
   private List<String> getRandomJetty() {
@@ -260,34 +259,6 @@ public class SyncSliceTest extends FullSolrCloudTest {
     }
     waitToSeeNotLive(cloudClient.getZkStateReader(), leaderJetty);
   }
-
-  private void waitForThingsToLevelOut() throws Exception {
-    int cnt = 0;
-    boolean retry = false;
-    do {
-      waitForRecoveriesToFinish(false);
-      
-      commit();
-      
-      updateMappingsFromZk(jettys, clients);
-      
-      Set<String> theShards = shardToJetty.keySet();
-      String failMessage = null;
-      for (String shard : theShards) {
-        failMessage = checkShardConsistency(shard, false);
-      }
-      
-      if (failMessage != null) {
-        retry = true;
-      } else {
-        retry = false;
-      }
-      
-      cnt++;
-      if (cnt > 10) break;
-      Thread.sleep(2000);
-    } while (retry);
-  }
   
   protected void indexDoc(List<String> skipServers, Object... fields) throws IOException,
       SolrServerException {
diff --git a/solr/core/src/test/org/apache/solr/cloud/ZkTestServer.java b/solr/core/src/test/org/apache/solr/cloud/ZkTestServer.java
deleted file mode 100644
index 305e96a..0000000
--- a/solr/core/src/test/org/apache/solr/cloud/ZkTestServer.java
+++ /dev/null
@@ -1,345 +0,0 @@
-package org.apache.solr.cloud;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to You under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-
-import java.io.BufferedReader;
-import java.io.File;
-import java.io.IOException;
-import java.io.InputStreamReader;
-import java.io.OutputStream;
-import java.net.InetAddress;
-import java.net.InetSocketAddress;
-import java.net.Socket;
-import java.net.UnknownHostException;
-import java.util.ArrayList;
-import java.util.List;
-
-import javax.management.JMException;
-
-import org.apache.zookeeper.jmx.ManagedUtil;
-import org.apache.zookeeper.server.NIOServerCnxn;
-import org.apache.zookeeper.server.ServerConfig;
-import org.apache.zookeeper.server.SessionTracker.Session;
-import org.apache.zookeeper.server.ZKDatabase;
-import org.apache.zookeeper.server.ZooKeeperServer;
-import org.apache.zookeeper.server.persistence.FileTxnSnapLog;
-import org.apache.zookeeper.server.quorum.QuorumPeerConfig.ConfigException;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-public class ZkTestServer {
-  public static final int TICK_TIME = 3000;
-
-  private static Logger log = LoggerFactory.getLogger(ZkTestServer.class);
-  
-  protected final ZKServerMain zkServer = new ZKServerMain();
-
-  private String zkDir;
-
-  private int clientPort;
-
-  private Thread zooThread;
-  
-  private int theTickTime = TICK_TIME;
-
-  class ZKServerMain {
-
-    private NIOServerCnxn.Factory cnxnFactory;
-    private ZooKeeperServer zooKeeperServer;
-    
-    protected void initializeAndRun(String[] args) throws ConfigException,
-        IOException {
-      try {
-        ManagedUtil.registerLog4jMBeans();
-      } catch (JMException e) {
-
-      }
-
-      ServerConfig config = new ServerConfig();
-      if (args.length == 1) {
-        config.parse(args[0]);
-      } else {
-        config.parse(args);
-      }
-
-      runFromConfig(config);
-    }
-
-    /**
-     * Run from a ServerConfig.
-     * @param config ServerConfig to use.
-     * @throws IOException
-     */
-    public void runFromConfig(ServerConfig config) throws IOException {
-      try {
-        // Note that this thread isn't going to be doing anything else,
-        // so rather than spawning another thread, we will just call
-        // run() in this thread.
-        // create a file logger url from the command line args
-        zooKeeperServer = new ZooKeeperServer();
-
-        FileTxnSnapLog ftxn = new FileTxnSnapLog(new File(config
-            .getDataLogDir()), new File(config.getDataDir()));
-        zooKeeperServer.setTxnLogFactory(ftxn);
-        zooKeeperServer.setTickTime(config.getTickTime());
-        cnxnFactory = new NIOServerCnxn.Factory(config.getClientPortAddress(), config
-            .getMaxClientCnxns());
-        cnxnFactory.startup(zooKeeperServer);
-        cnxnFactory.join();
-        if (zooKeeperServer.isRunning()) {
-          zooKeeperServer.shutdown();
-        }
-      } catch (InterruptedException e) {
-      }
-    }
-
-    /**
-     * Shutdown the serving instance
-     * @throws IOException 
-     */
-    protected void shutdown() throws IOException {
-      zooKeeperServer.shutdown();
-      ZKDatabase zkDb = zooKeeperServer.getZKDatabase();
-      if (zkDb != null) {
-        zkDb.close();
-      }
-      if (cnxnFactory != null && cnxnFactory.getLocalPort() != 0) {
-        waitForServerDown(getZkHost() + ":" + getPort(), 5000);
-      }
-      if (cnxnFactory != null) {
-        cnxnFactory.shutdown();
-      }
-    }
-
-    public int getLocalPort() {
-      if (cnxnFactory == null) {
-        throw new IllegalStateException("A port has not yet been selected");
-      }
-      int port;
-      try {
-        port = cnxnFactory.getLocalPort();
-      } catch (NullPointerException e) {
-        throw new IllegalStateException("A port has not yet been selected");
-      }
-      if (port == 0) {
-        throw new IllegalStateException("A port has not yet been selected");
-      }
-      return port;
-    }
-  }
-
-  public ZkTestServer(String zkDir) {
-    this.zkDir = zkDir;
-  }
-
-  public ZkTestServer(String zkDir, int port) {
-    this.zkDir = zkDir;
-    this.clientPort = port;
-  }
-
-  public String getZkHost() {
-    return "127.0.0.1:" + zkServer.getLocalPort();
-  }
-
-  public String getZkAddress() {
-    return "127.0.0.1:" + zkServer.getLocalPort() + "/solr";
-  }
-
-  public int getPort() {
-    return zkServer.getLocalPort();
-  }
-  
-  public void expire(final long sessionId) {
-    zkServer.zooKeeperServer.expire(new Session() {
-      @Override
-      public long getSessionId() {
-        return sessionId;
-      }
-      @Override
-      public int getTimeout() {
-        return 4000;
-      }
-      @Override
-      public boolean isClosing() {
-        return false;
-      }});
-  }
-
-  public void run() throws InterruptedException {
-    log.info("STARTING ZK TEST SERVER");
-    // we don't call super.setUp
-    zooThread = new Thread() {
-      
-      @Override
-      public void run() {
-        ServerConfig config = new ServerConfig() {
-
-          {
-            setClientPort(ZkTestServer.this.clientPort);
-            this.dataDir = zkDir;
-            this.dataLogDir = zkDir;
-            this.tickTime = theTickTime;
-          }
-          
-          public void setClientPort(int clientPort) {
-            if (clientPortAddress != null) {
-              try {
-                this.clientPortAddress = new InetSocketAddress(
-                        InetAddress.getByName(clientPortAddress.getHostName()), clientPort);
-              } catch (UnknownHostException e) {
-                throw new RuntimeException(e);
-              }
-            } else {
-              this.clientPortAddress = new InetSocketAddress(clientPort);
-            }
-          }
-        };
-
-        try {
-          zkServer.runFromConfig(config);
-        } catch (Throwable e) {
-          throw new RuntimeException(e);
-        }
-      }
-    };
-
-    zooThread.setDaemon(true);
-    zooThread.start();
-
-    int cnt = 0;
-    int port = -1;
-    try {
-       port = getPort();
-    } catch(IllegalStateException e) {
-      
-    }
-    while (port < 1) {
-      Thread.sleep(100);
-      try {
-        port = getPort();
-      } catch(IllegalStateException e) {
-        
-      }
-      if (cnt == 500) {
-        throw new RuntimeException("Could not get the port for ZooKeeper server");
-      }
-      cnt++;
-    }
-  }
-
-  @SuppressWarnings("deprecation")
-  public void shutdown() throws IOException {
-    // TODO: this can log an exception while trying to unregister a JMX MBean
-    zkServer.shutdown();
-  }
- 
-  
-  public static boolean waitForServerDown(String hp, long timeout) {
-    long start = System.currentTimeMillis();
-    while (true) {
-      try {
-        HostPort hpobj = parseHostPortList(hp).get(0);
-        send4LetterWord(hpobj.host, hpobj.port, "stat");
-      } catch (IOException e) {
-        return true;
-      }
-      
-      if (System.currentTimeMillis() > start + timeout) {
-        break;
-      }
-      try {
-        Thread.sleep(250);
-      } catch (InterruptedException e) {
-        // ignore
-      }
-    }
-    return false;
-  }
-  
-  public static class HostPort {
-    String host;
-    int port;
-    
-    HostPort(String host, int port) {
-      this.host = host;
-      this.port = port;
-    }
-  }
-  
-  /**
-   * Send the 4letterword
-   * @param host the destination host
-   * @param port the destination port
-   * @param cmd the 4letterword
-   * @return
-   * @throws IOException
-   */
-  public static String send4LetterWord(String host, int port, String cmd)
-      throws IOException
-  {
-
-      Socket sock = new Socket(host, port);
-      BufferedReader reader = null;
-      try {
-          OutputStream outstream = sock.getOutputStream();
-          outstream.write(cmd.getBytes("US-ASCII"));
-          outstream.flush();
-          // this replicates NC - close the output stream before reading
-          sock.shutdownOutput();
-
-          reader =
-              new BufferedReader(
-                      new InputStreamReader(sock.getInputStream(), "US-ASCII"));
-          StringBuilder sb = new StringBuilder();
-          String line;
-          while((line = reader.readLine()) != null) {
-              sb.append(line + "\n");
-          }
-          return sb.toString();
-      } finally {
-          sock.close();
-          if (reader != null) {
-              reader.close();
-          }
-      }
-  }
-  
-  public static List<HostPort> parseHostPortList(String hplist) {
-    ArrayList<HostPort> alist = new ArrayList<HostPort>();
-    for (String hp : hplist.split(",")) {
-      int idx = hp.lastIndexOf(':');
-      String host = hp.substring(0, idx);
-      int port;
-      try {
-        port = Integer.parseInt(hp.substring(idx + 1));
-      } catch (RuntimeException e) {
-        throw new RuntimeException("Problem parsing " + hp + e.toString());
-      }
-      alist.add(new HostPort(host, port));
-    }
-    return alist;
-  }
-
-  public int getTheTickTime() {
-    return theTickTime;
-  }
-
-  public void setTheTickTime(int theTickTime) {
-    this.theTickTime = theTickTime;
-  }
-}
diff --git a/solr/solrj/src/java/org/apache/solr/client/solrj/impl/CloudSolrServer.java b/solr/solrj/src/java/org/apache/solr/client/solrj/impl/CloudSolrServer.java
index 53783ca..1bfdf74 100644
--- a/solr/solrj/src/java/org/apache/solr/client/solrj/impl/CloudSolrServer.java
+++ b/solr/solrj/src/java/org/apache/solr/client/solrj/impl/CloudSolrServer.java
@@ -65,10 +65,14 @@ public class CloudSolrServer extends SolrServer {
   
   // since the state shouldn't change often, should be very cheap reads
   private volatile List<String> urlList;
+  
   private volatile List<String> leaderUrlList;
+  private volatile List<String> replicasList;
+  
   private volatile int lastClusterStateHashCode;
   
   private final boolean updatesToLeaders;
+
   
   /**
    * @param zkHost The client endpoint of the zookeeper quorum containing the cloud state,
@@ -164,9 +168,11 @@ public class CloudSolrServer extends SolrServer {
     
     ClusterState clusterState = zkStateReader.getClusterState();
     boolean sendToLeaders = false;
+    List<String> replicas = null;
     
     if (request instanceof IsUpdateRequest && updatesToLeaders) {
       sendToLeaders = true;
+      replicas = new ArrayList<String>();
     }
 
     SolrParams reqParams = request.getParams();
@@ -211,17 +217,22 @@ public class CloudSolrServer extends SolrServer {
             if (!sendToLeaders || (sendToLeaders && coreNodeProps.isLeader())) {
               String url = coreNodeProps.getCoreUrl();
               urlList.add(url);
+            } else if (sendToLeaders) {
+              String url = coreNodeProps.getCoreUrl();
+              replicas.add(url);
             }
           }
         }
       }
       if (sendToLeaders) {
         this.leaderUrlList = urlList; 
+        this.replicasList = replicas;
       } else {
         this.urlList = urlList;
       }
       this.lastClusterStateHashCode = clusterState.hashCode();
     }
+    
     List<String> theUrlList;
     if (sendToLeaders) {
       theUrlList = new ArrayList<String>(leaderUrlList.size());
@@ -231,7 +242,14 @@ public class CloudSolrServer extends SolrServer {
       theUrlList.addAll(urlList);
     }
     Collections.shuffle(theUrlList, rand);
-    //System.out.println("########################## MAKING REQUEST TO " + urlList);
+    if (replicas != null) {
+      ArrayList<String> theReplicas = new ArrayList<String>(replicasList.size());
+      theReplicas.addAll(replicasList);
+      Collections.shuffle(theReplicas, rand);
+
+      theUrlList.addAll(theReplicas);
+    }
+    //System.out.println("########################## MAKING REQUEST TO " + theUrlList);
  
     LBHttpSolrServer.Req req = new LBHttpSolrServer.Req(request, theUrlList);
     LBHttpSolrServer.Rsp rsp = lbServer.request(req);
@@ -255,4 +273,16 @@ public class CloudSolrServer extends SolrServer {
   public LBHttpSolrServer getLbServer() {
     return lbServer;
   }
+
+  List<String> getUrlList() {
+    return urlList;
+  }
+
+  List<String> getLeaderUrlList() {
+    return leaderUrlList;
+  }
+
+  List<String> getReplicasList() {
+    return replicasList;
+  }
 }
diff --git a/solr/solrj/src/test-files/solrj/solr/collection1/conf/schema-replication1.xml b/solr/solrj/src/test-files/solrj/solr/collection1/conf/schema-replication1.xml
new file mode 100644
index 0000000..48ecd9f
--- /dev/null
+++ b/solr/solrj/src/test-files/solrj/solr/collection1/conf/schema-replication1.xml
@@ -0,0 +1,49 @@
+<?xml version="1.0" ?>
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+
+<!-- The Solr schema file. This file should be named "schema.xml" and
+     should be located where the classloader for the Solr webapp can find it.
+
+     This schema is used for testing, and as such has everything and the 
+     kitchen sink thrown in. See example/solr/conf/schema.xml for a 
+     more concise example.
+
+     $Id$
+     $Source$
+     $Name$
+  -->
+
+<schema name="test" version="1.2">
+  <types>
+
+    <fieldType name="integer" class="solr.IntField"/>
+    <fieldtype name="string" class="solr.StrField" sortMissingLast="true"/>
+
+
+  </types>
+
+
+  <fields>
+    <field name="id" type="integer" indexed="true" stored="true" multiValued="false" required="false"/>
+    <field name="name" type="string" indexed="true" stored="true"/>
+
+  </fields>
+
+  <uniqueKey>id</uniqueKey>
+
+</schema>
diff --git a/solr/solrj/src/test-files/solrj/solr/collection1/conf/schema.xml b/solr/solrj/src/test-files/solrj/solr/collection1/conf/schema.xml
new file mode 100644
index 0000000..8d69005
--- /dev/null
+++ b/solr/solrj/src/test-files/solrj/solr/collection1/conf/schema.xml
@@ -0,0 +1,627 @@
+<?xml version="1.0" ?>
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+
+<!-- The Solr schema file. This file should be named "schema.xml" and
+     should be located where the classloader for the Solr webapp can find it.
+
+     This schema is used for testing, and as such has everything and the
+     kitchen sink thrown in. See example/solr/conf/schema.xml for a
+     more concise example.
+
+     $Id: schema.xml 382610 2006-03-03 01:43:03Z yonik $
+     $Source: /cvs/main/searching/solr-configs/test/WEB-INF/classes/schema.xml,v $
+     $Name:  $
+  -->
+
+<schema name="test" version="1.5">
+  <types>
+
+    <!-- field type definitions... note that the "name" attribute is
+         just a label to be used by field definitions.  The "class"
+         attribute and any other attributes determine the real type and
+         behavior of the fieldtype.
+      -->
+
+    <!-- numeric field types that store and index the text
+         value verbatim (and hence don't sort correctly or support range queries.)
+         These are provided more for backward compatability, allowing one
+         to create a schema that matches an existing lucene index.
+    -->
+    <fieldType name="pint" class="solr.IntField"/>
+    <fieldType name="plong" class="solr.LongField"/>
+    <fieldtype name="pfloat" class="solr.FloatField"/>
+    <fieldType name="pdouble" class="solr.DoubleField"/>
+
+    <fieldType name="int" class="solr.TrieIntField" precisionStep="0" omitNorms="true" positionIncrementGap="0"/>
+    <fieldType name="float" class="solr.TrieFloatField" precisionStep="0" omitNorms="true" positionIncrementGap="0"/>
+    <fieldType name="long" class="solr.TrieLongField" precisionStep="0" omitNorms="true" positionIncrementGap="0"/>
+    <fieldType name="double" class="solr.TrieDoubleField" precisionStep="0" omitNorms="true" positionIncrementGap="0"/>
+
+    <fieldType name="tint" class="solr.TrieIntField" precisionStep="8" omitNorms="true" positionIncrementGap="0"/>
+    <fieldType name="tfloat" class="solr.TrieFloatField" precisionStep="8" omitNorms="true" positionIncrementGap="0"/>
+    <fieldType name="tlong" class="solr.TrieLongField" precisionStep="8" omitNorms="true" positionIncrementGap="0"/>
+    <fieldType name="tdouble" class="solr.TrieDoubleField" precisionStep="8" omitNorms="true" positionIncrementGap="0"/>
+
+    <!-- numeric field types that manipulate the value into
+       a string value that isn't human readable in it's internal form,
+       but sorts correctly and supports range queries.
+
+         If sortMissingLast="true" then a sort on this field will cause documents
+       without the field to come after documents with the field,
+       regardless of the requested sort order.
+         If sortMissingFirst="true" then a sort on this field will cause documents
+       without the field to come before documents with the field,
+       regardless of the requested sort order.
+         If sortMissingLast="false" and sortMissingFirst="false" (the default),
+       then default lucene sorting will be used which places docs without the field
+       first in an ascending sort and last in a descending sort.
+    -->
+    <fieldtype name="sint" class="solr.SortableIntField" sortMissingLast="true"/>
+    <fieldtype name="slong" class="solr.SortableLongField" sortMissingLast="true"/>
+    <fieldtype name="sfloat" class="solr.SortableFloatField" sortMissingLast="true"/>
+    <fieldtype name="sdouble" class="solr.SortableDoubleField" sortMissingLast="true"/>
+
+    <!-- bcd versions of sortable numeric type may provide smaller
+         storage space and support very large numbers.
+    -->
+    <fieldtype name="bcdint" class="solr.BCDIntField" sortMissingLast="true"/>
+    <fieldtype name="bcdlong" class="solr.BCDLongField" sortMissingLast="true"/>
+    <fieldtype name="bcdstr" class="solr.BCDStrField" sortMissingLast="true"/>
+
+    <!-- Field type demonstrating an Analyzer failure -->
+    <fieldtype name="failtype1" class="solr.TextField">
+      <analyzer type="index">
+          <tokenizer class="solr.MockTokenizerFactory"/>
+          <filter class="solr.WordDelimiterFilterFactory" generateWordParts="1" generateNumberParts="0" catenateWords="0" catenateNumbers="0" catenateAll="0"/>
+          <filter class="solr.LowerCaseFilterFactory"/>
+      </analyzer>
+    </fieldtype>
+
+    <!-- Demonstrating ignoreCaseChange -->
+    <fieldtype name="wdf_nocase" class="solr.TextField">
+      <analyzer>
+          <tokenizer class="solr.MockTokenizerFactory"/>
+          <filter class="solr.WordDelimiterFilterFactory" generateWordParts="1" generateNumberParts="0" catenateWords="0" catenateNumbers="0" catenateAll="0" splitOnCaseChange="0" preserveOriginal="0"/>
+          <filter class="solr.LowerCaseFilterFactory"/>
+      </analyzer>
+    </fieldtype>
+
+     <fieldtype name="wdf_preserve" class="solr.TextField">
+      <analyzer>
+          <tokenizer class="solr.MockTokenizerFactory"/>
+          <filter class="solr.WordDelimiterFilterFactory" generateWordParts="0" generateNumberParts="1" catenateWords="0" catenateNumbers="0" catenateAll="0" splitOnCaseChange="0" preserveOriginal="1"/>
+          <filter class="solr.LowerCaseFilterFactory"/>
+      </analyzer>
+    </fieldtype>
+
+
+    <fieldtype name="boolean" class="solr.BoolField" sortMissingLast="true"/>
+    <fieldtype name="string" class="solr.StrField" sortMissingLast="true"/>
+
+    <!-- format for date is 1995-12-31T23:59:59.999Z and only the fractional
+         seconds part (.999) is optional.
+      -->
+    <fieldtype name="date" class="solr.TrieDateField" precisionStep="0"/>
+    <fieldtype name="tdate" class="solr.TrieDateField" precisionStep="6"/>
+    <fieldtype name="pdate" class="solr.DateField" sortMissingLast="true"/>
+
+
+    <!-- solr.TextField allows the specification of custom
+         text analyzers specified as a tokenizer and a list
+         of token filters.
+      -->
+    <fieldtype name="text" class="solr.TextField">
+      <analyzer>
+        <tokenizer class="solr.StandardTokenizerFactory"/>
+        <filter class="solr.StandardFilterFactory"/>
+        <filter class="solr.LowerCaseFilterFactory"/>
+        <filter class="solr.StopFilterFactory"/>
+        <filter class="solr.PorterStemFilterFactory"/>
+      </analyzer>
+    </fieldtype>
+
+
+    <fieldtype name="nametext" class="solr.TextField">
+      <analyzer class="org.apache.lucene.analysis.core.WhitespaceAnalyzer"/>
+    </fieldtype>
+
+    <fieldtype name="teststop" class="solr.TextField">
+       <analyzer>
+        <tokenizer class="solr.LowerCaseTokenizerFactory"/>
+        <filter class="solr.StandardFilterFactory"/>
+      </analyzer>
+    </fieldtype>
+
+    <!-- fieldtypes in this section isolate tokenizers and tokenfilters for testing -->
+    <fieldtype name="lowertok" class="solr.TextField">
+      <analyzer><tokenizer class="solr.LowerCaseTokenizerFactory"/></analyzer>
+    </fieldtype>
+    <fieldtype name="keywordtok" class="solr.TextField">
+      <analyzer><tokenizer class="solr.MockTokenizerFactory" pattern="keyword"/></analyzer>
+    </fieldtype>
+    <fieldtype name="standardtok" class="solr.TextField">
+      <analyzer><tokenizer class="solr.StandardTokenizerFactory"/></analyzer>
+    </fieldtype>
+    <fieldtype name="lettertok" class="solr.TextField">
+      <analyzer><tokenizer class="solr.LetterTokenizerFactory"/></analyzer>
+    </fieldtype>
+    <fieldtype name="whitetok" class="solr.TextField">
+      <analyzer><tokenizer class="solr.MockTokenizerFactory"/></analyzer>
+    </fieldtype>
+    <fieldtype name="HTMLstandardtok" class="solr.TextField">
+      <analyzer>
+      <charFilter class="solr.HTMLStripCharFilterFactory"/>
+      <tokenizer class="solr.StandardTokenizerFactory"/>
+      </analyzer>
+    </fieldtype>
+    <fieldtype name="HTMLwhitetok" class="solr.TextField">
+      <analyzer>
+      <charFilter class="solr.HTMLStripCharFilterFactory"/>
+      <tokenizer class="solr.MockTokenizerFactory"/>
+      </analyzer>
+    </fieldtype>
+    <fieldtype name="standardtokfilt" class="solr.TextField">
+      <analyzer>
+        <tokenizer class="solr.StandardTokenizerFactory"/>
+        <filter class="solr.StandardFilterFactory"/>
+      </analyzer>
+    </fieldtype>
+    <fieldtype name="standardfilt" class="solr.TextField">
+      <analyzer>
+        <tokenizer class="solr.MockTokenizerFactory"/>
+        <filter class="solr.StandardFilterFactory"/>
+      </analyzer>
+    </fieldtype>
+    <fieldtype name="lowerfilt" class="solr.TextField">
+      <analyzer>
+        <tokenizer class="solr.MockTokenizerFactory"/>
+        <filter class="solr.LowerCaseFilterFactory"/>
+      </analyzer>
+    </fieldtype>
+    <fieldtype name="lowerpunctfilt" class="solr.TextField">
+      <analyzer>
+        <tokenizer class="solr.MockTokenizerFactory"/>
+        <filter class="solr.WordDelimiterFilterFactory" generateWordParts="1" generateNumberParts="1" catenateWords="1" catenateNumbers="1" catenateAll="1" splitOnCaseChange="1"/>
+        <filter class="solr.LowerCaseFilterFactory"/>
+      </analyzer>
+    </fieldtype>
+    <fieldtype name="patternreplacefilt" class="solr.TextField">
+      <analyzer type="index">
+        <tokenizer class="solr.MockTokenizerFactory" pattern="keyword"/>
+        <filter class="solr.PatternReplaceFilterFactory"
+                pattern="([^a-zA-Z])" replacement="_" replace="all"
+        />
+      </analyzer>
+      <analyzer type="query">
+        <tokenizer class="solr.MockTokenizerFactory" pattern="keyword"/>
+      </analyzer>
+    </fieldtype>
+    <fieldtype name="patterntok" class="solr.TextField">
+      <analyzer>
+        <tokenizer class="solr.PatternTokenizerFactory" pattern=","/>
+      </analyzer>
+    </fieldtype>
+    <fieldtype name="porterfilt" class="solr.TextField">
+      <analyzer>
+        <tokenizer class="solr.MockTokenizerFactory"/>
+        <filter class="solr.PorterStemFilterFactory"/>
+      </analyzer>
+    </fieldtype>
+    <!-- fieldtype name="snowballfilt" class="solr.TextField">
+      <analyzer>
+        <tokenizer class="solr.MockTokenizerFactory"/>
+        <filter class="solr.SnowballPorterFilterFactory"/>
+      </analyzer>
+    </fieldtype -->
+    <fieldtype name="engporterfilt" class="solr.TextField">
+      <analyzer>
+        <tokenizer class="solr.MockTokenizerFactory"/>
+        <filter class="solr.PorterStemFilterFactory"/>
+      </analyzer>
+    </fieldtype>
+    <fieldtype name="custengporterfilt" class="solr.TextField">
+      <analyzer>
+        <tokenizer class="solr.MockTokenizerFactory"/>
+        <filter class="solr.PorterStemFilterFactory"/>
+      </analyzer>
+    </fieldtype>
+    <fieldtype name="stopfilt" class="solr.TextField">
+      <analyzer>
+        <tokenizer class="solr.MockTokenizerFactory"/>
+        <filter class="solr.StopFilterFactory" ignoreCase="true"/>
+      </analyzer>
+    </fieldtype>
+    <fieldtype name="custstopfilt" class="solr.TextField">
+      <analyzer>
+        <tokenizer class="solr.MockTokenizerFactory"/>
+      </analyzer>
+    </fieldtype>
+    <fieldtype name="lengthfilt" class="solr.TextField">
+      <analyzer>
+        <tokenizer class="solr.MockTokenizerFactory"/>
+        <filter class="solr.LengthFilterFactory" min="2" max="5"/>
+      </analyzer>
+    </fieldtype>
+    <fieldType name="charfilthtmlmap" class="solr.TextField">
+      <analyzer>
+        <charFilter class="solr.HTMLStripCharFilterFactory"/>
+        <tokenizer class="solr.MockTokenizerFactory"/>
+      </analyzer>
+    </fieldType>
+
+    <fieldtype name="subword" class="solr.TextField" multiValued="true" positionIncrementGap="100">
+      <analyzer type="index">
+          <tokenizer class="solr.MockTokenizerFactory"/>
+          <filter class="solr.WordDelimiterFilterFactory" generateWordParts="1" generateNumberParts="1" catenateWords="1" catenateNumbers="1" catenateAll="0"/>
+          <filter class="solr.LowerCaseFilterFactory"/>
+          <filter class="solr.StopFilterFactory"/>
+          <filter class="solr.PorterStemFilterFactory"/>
+      </analyzer>
+      <analyzer type="query">
+          <tokenizer class="solr.MockTokenizerFactory"/>
+          <filter class="solr.WordDelimiterFilterFactory" generateWordParts="1" generateNumberParts="1" catenateWords="0" catenateNumbers="0" catenateAll="0"/>
+          <filter class="solr.LowerCaseFilterFactory"/>
+          <filter class="solr.StopFilterFactory"/>
+          <filter class="solr.PorterStemFilterFactory"/>
+      </analyzer>
+    </fieldtype>
+
+    <fieldtype name="numericsubword" class="solr.TextField" multiValued="true" positionIncrementGap="100">
+      <analyzer type="index">
+          <tokenizer class="solr.MockTokenizerFactory"/>
+          <filter class="solr.LowerCaseFilterFactory"/>
+          <filter class="solr.WordDelimiterFilterFactory" splitOnNumerics="0" splitOnCaseChange="0" generateWordParts="1" generateNumberParts="0" catenateWords="0" catenateNumbers="0" catenateAll="0"/>
+          <filter class="solr.StopFilterFactory"/>
+          <filter class="solr.PorterStemFilterFactory"/>
+      </analyzer>
+      <analyzer type="query">
+          <tokenizer class="solr.MockTokenizerFactory"/>
+          <filter class="solr.LowerCaseFilterFactory"/>
+          <filter class="solr.WordDelimiterFilterFactory" splitOnNumerics="0" splitOnCaseChange="0" generateWordParts="1" generateNumberParts="1" catenateWords="1" catenateNumbers="1" catenateAll="0"/>
+          <filter class="solr.StopFilterFactory"/>
+          <filter class="solr.PorterStemFilterFactory"/>
+      </analyzer>
+    </fieldtype>
+
+    <fieldtype name="protectedsubword" class="solr.TextField" multiValued="true" positionIncrementGap="100">
+      <analyzer type="index">
+          <tokenizer class="solr.MockTokenizerFactory"/>
+          <filter class="solr.LowerCaseFilterFactory"/>
+          <filter class="solr.WordDelimiterFilterFactory"  splitOnNumerics="0" splitOnCaseChange="0" generateWordParts="1" generateNumberParts="1" catenateWords="0" catenateNumbers="0" catenateAll="0"/>
+      </analyzer>
+      <analyzer type="query">
+          <tokenizer class="solr.MockTokenizerFactory"/>
+          <filter class="solr.LowerCaseFilterFactory"/>
+      </analyzer>
+    </fieldtype>
+
+
+    <!-- more flexible in matching skus, but more chance of a false match -->
+    <fieldtype name="skutype1" class="solr.TextField">
+      <analyzer type="index">
+          <tokenizer class="solr.MockTokenizerFactory"/>
+          <filter class="solr.WordDelimiterFilterFactory" generateWordParts="1" generateNumberParts="1" catenateWords="1" catenateNumbers="1" catenateAll="0"/>
+          <filter class="solr.LowerCaseFilterFactory"/>
+      </analyzer>
+      <analyzer type="query">
+          <tokenizer class="solr.MockTokenizerFactory"/>
+          <filter class="solr.WordDelimiterFilterFactory" generateWordParts="0" generateNumberParts="0" catenateWords="1" catenateNumbers="1" catenateAll="0"/>
+          <filter class="solr.LowerCaseFilterFactory"/>
+      </analyzer>
+    </fieldtype>
+
+    <!-- less flexible in matching skus, but less chance of a false match -->
+    <fieldtype name="skutype2" class="solr.TextField">
+      <analyzer type="index">
+          <tokenizer class="solr.MockTokenizerFactory"/>
+          <filter class="solr.WordDelimiterFilterFactory" generateWordParts="0" generateNumberParts="0" catenateWords="1" catenateNumbers="1" catenateAll="0"/>
+          <filter class="solr.LowerCaseFilterFactory"/>
+      </analyzer>
+      <analyzer type="query">
+          <tokenizer class="solr.MockTokenizerFactory"/>
+          <filter class="solr.WordDelimiterFilterFactory" generateWordParts="0" generateNumberParts="0" catenateWords="1" catenateNumbers="1" catenateAll="0"/>
+          <filter class="solr.LowerCaseFilterFactory"/>
+      </analyzer>
+    </fieldtype>
+
+    <!-- less flexible in matching skus, but less chance of a false match -->
+    <fieldtype name="syn" class="solr.TextField">
+      <analyzer>
+          <tokenizer class="solr.MockTokenizerFactory"/>
+      </analyzer>
+    </fieldtype>
+
+
+    <fieldtype  name="unstored" class="solr.StrField" indexed="true" stored="false"/>
+
+
+  <fieldtype name="textgap" class="solr.TextField" multiValued="true" positionIncrementGap="100">
+      <analyzer>
+          <tokenizer class="solr.MockTokenizerFactory"/>
+          <filter class="solr.LowerCaseFilterFactory"/>
+      </analyzer>
+  </fieldtype>
+
+  <fieldType name="uuid" class="solr.UUIDField" />
+
+    <!-- Try out some point types -->
+  <fieldType name="xy" class="solr.PointType" dimension="2" subFieldType="double"/>
+  <fieldType name="x" class="solr.PointType" dimension="1" subFieldType="double"/>
+  <fieldType name="tenD" class="solr.PointType" dimension="10" subFieldType="double"/>
+    <!-- Use the sub field suffix -->
+  <fieldType name="xyd" class="solr.PointType" dimension="2" subFieldSuffix="_d1"/>
+    <fieldtype name="geohash" class="solr.GeoHashField"/>
+
+
+  <fieldType name="latLon" class="solr.LatLonType" subFieldType="double"/>
+
+  <!--  some per-field similarity examples -->
+  
+  <!--  specify a Similarity classname directly -->
+  <!--
+  <fieldType name="sim1" class="solr.TextField">
+    <analyzer>
+      <tokenizer class="solr.MockTokenizerFactory"/>
+    </analyzer>
+    <similarity class="org.apache.lucene.misc.SweetSpotSimilarity"/>
+  </fieldType>
+  -->
+  <!--  specify a Similarity factory -->  
+  <!--
+  <fieldType name="sim2" class="solr.TextField">
+    <analyzer>
+      <tokenizer class="solr.MockTokenizerFactory"/>
+    </analyzer>
+    <similarity class="org.apache.solr.search.similarities.CustomSimilarityFactory">
+      <str name="echo">is there an echo?</str>
+    </similarity>
+  </fieldType>
+  -->
+  <!-- don't specify any sim at all: get the default  -->
+  <!--
+  <fieldType name="sim3" class="solr.TextField">
+    <analyzer>
+      <tokenizer class="solr.MockTokenizerFactory"/>
+    </analyzer>
+  </fieldType>
+  -->
+ </types>
+
+
+ <fields>
+   <field name="id" type="int" indexed="true" stored="true" multiValued="false" required="false"/>
+   <field name="signatureField" type="string" indexed="true" stored="false"/>
+   <field name="uuid" type="uuid" stored="true" />
+   <field name="name" type="nametext" indexed="true" stored="true"/>
+   <field name="text" type="text" indexed="true" stored="false"/>
+   <field name="subject" type="text" indexed="true" stored="true"/>
+   <field name="title" type="nametext" indexed="true" stored="true"/>
+   <field name="weight" type="float" indexed="true" stored="true" multiValued="false"/>
+   <field name="bday" type="date" indexed="true" stored="true" multiValued="false"/>
+
+   <field name="title_stemmed" type="text" indexed="true" stored="false"/>
+   <field name="title_lettertok" type="lettertok" indexed="true" stored="false"/>
+
+   <field name="syn" type="syn" indexed="true" stored="true"/>
+
+   <!-- to test property inheritance and overriding -->
+   <field name="shouldbeunstored" type="unstored" />
+   <field name="shouldbestored" type="unstored" stored="true"/>
+   <field name="shouldbeunindexed" type="unstored" indexed="false" stored="true"/>
+
+   <!-- Test points -->
+      <!-- Test points -->
+   <field name="home" type="xy" indexed="true" stored="true" multiValued="false"/>
+   <field name="x" type="x" indexed="true" stored="true" multiValued="false"/>
+   <field name="homed" type="xyd" indexed="true" stored="true" multiValued="false"/>
+   <field name="home_ns" type="xy" indexed="true" stored="false" multiValued="false"/>
+   <field name="work" type="xy" indexed="true" stored="true" multiValued="false"/>
+
+   <field name="home_ll" type="latLon" indexed="true" stored="true" multiValued="false"/>
+   <field name="home_gh" type="geohash" indexed="true" stored="true" multiValued="false"/>
+
+
+   <field name="point10" type="tenD" indexed="true" stored="true" multiValued="false"/>
+
+
+   <!-- test different combinations of indexed and stored -->
+   <field name="bind" type="boolean" indexed="true" stored="false"/>
+   <field name="bsto" type="boolean" indexed="false" stored="true"/>
+   <field name="bindsto" type="boolean" indexed="true" stored="true"/>
+   <field name="isto" type="int" indexed="false" stored="true"/>
+   <field name="iind" type="int" indexed="true" stored="false"/>
+   <field name="ssto" type="string" indexed="false" stored="true"/>
+   <field name="sind" type="string" indexed="true" stored="false"/>
+   <field name="sindsto" type="string" indexed="true" stored="true"/>
+
+   <!-- test combinations of term vector settings -->
+   <field name="test_basictv" type="text" termVectors="true"/>
+   <field name="test_notv" type="text" termVectors="false"/>
+   <field name="test_postv" type="text" termVectors="true" termPositions="true"/>
+   <field name="test_offtv" type="text" termVectors="true" termOffsets="true"/>
+   <field name="test_posofftv" type="text" termVectors="true"
+     termPositions="true" termOffsets="true"/>
+
+   <!-- fields to test individual tokenizers and tokenfilters -->
+   <field name="teststop" type="teststop" indexed="true" stored="true"/>
+   <field name="lowertok" type="lowertok" indexed="true" stored="true"/>
+   <field name="keywordtok" type="keywordtok" indexed="true" stored="true"/>
+   <field name="standardtok" type="standardtok" indexed="true" stored="true"/>
+   <field name="HTMLstandardtok" type="HTMLstandardtok" indexed="true" stored="true"/>
+   <field name="lettertok" type="lettertok" indexed="true" stored="true"/>
+   <field name="whitetok" type="whitetok" indexed="true" stored="true"/>
+   <field name="HTMLwhitetok" type="HTMLwhitetok" indexed="true" stored="true"/>
+   <field name="standardtokfilt" type="standardtokfilt" indexed="true" stored="true"/>
+   <field name="standardfilt" type="standardfilt" indexed="true" stored="true"/>
+   <field name="lowerfilt" type="lowerfilt" indexed="true" stored="true"/>
+   <field name="lowerfilt1" type="lowerfilt" indexed="true" stored="true"/>
+	 <field name="lowerfilt1and2" type="lowerfilt" indexed="true" stored="true"/>
+   <field name="patterntok" type="patterntok" indexed="true" stored="true"/>
+   <field name="patternreplacefilt" type="patternreplacefilt" indexed="true" stored="true"/>
+   <field name="porterfilt" type="porterfilt" indexed="true" stored="true"/>
+   <field name="engporterfilt" type="engporterfilt" indexed="true" stored="true"/>
+   <field name="custengporterfilt" type="custengporterfilt" indexed="true" stored="true"/>
+   <field name="stopfilt" type="stopfilt" indexed="true" stored="true"/>
+   <field name="custstopfilt" type="custstopfilt" indexed="true" stored="true"/>
+   <field name="lengthfilt" type="lengthfilt" indexed="true" stored="true"/>
+   <field name="wdf_nocase" type="wdf_nocase" indexed="true" stored="true"/>
+   <field name="wdf_preserve" type="wdf_preserve" indexed="true" stored="true"/>
+
+   <field name="numberpartfail" type="failtype1" indexed="true" stored="true"/>
+
+   <field name="nullfirst" type="string" indexed="true" stored="true" sortMissingFirst="true" multiValued="false"/>
+
+   <field name="subword" type="subword" indexed="true" stored="true"/>
+   <field name="subword_offsets" type="subword" indexed="true" stored="true" termOffsets="true"/>
+   <field name="numericsubword" type="numericsubword" indexed="true" stored="true"/>
+   <field name="protectedsubword" type="protectedsubword" indexed="true" stored="true"/>
+
+   <field name="sku1" type="skutype1" indexed="true" stored="true"/>
+   <field name="sku2" type="skutype2" indexed="true" stored="true"/>
+
+   <field name="textgap" type="textgap" indexed="true" stored="true"/>
+
+   <field name="timestamp" type="date" indexed="true" stored="true" default="NOW" multiValued="false"/>
+   <field name="multiDefault" type="string" indexed="true" stored="true" default="muLti-Default" multiValued="true"/>
+   <field name="intDefault" type="int" indexed="true" stored="true" default="42" multiValued="false"/>
+
+   <!--
+   <field name="sim1text" type="sim1" indexed="true" stored="true"/>
+   <field name="sim2text" type="sim2" indexed="true" stored="true"/>
+   <field name="sim3text" type="sim3" indexed="true" stored="true"/>
+   -->
+   
+   <field name="tlong" type="tlong" indexed="true" stored="true" />
+   
+   <field name="_version_" type="long" indexed="true" stored="true"/>
+
+   <!-- Dynamic field definitions.  If a field name is not found, dynamicFields
+        will be used if the name matches any of the patterns.
+        RESTRICTION: the glob-like pattern in the name attribute must have
+        a "*" only at the start or the end.
+        EXAMPLE:  name="*_i" will match any field ending in _i (like myid_i, z_i)
+        Longer patterns will be matched first.  if equal size patterns
+        both match, the first appearing in the schema will be used.
+   -->
+   <dynamicField name="*_i"  type="int"    indexed="true"  stored="true"/>
+   <dynamicField name="*_i1"  type="int"    indexed="true" stored="true" multiValued="false"/>
+                 
+   <dynamicField name="*_s"  type="string"  indexed="true"  stored="true"/>
+   <dynamicField name="*_s1"  type="string"  indexed="true"  stored="true" multiValued="false"/>
+   <dynamicField name="*_l"  type="long"   indexed="true"  stored="true"/>
+   <dynamicField name="*_l1"  type="long"   indexed="true"  stored="true" multiValued="false"/>
+   <dynamicField name="*_t"  type="text"    indexed="true"  stored="true"/>
+   <dynamicField name="*_b"  type="boolean" indexed="true"  stored="true"/>
+   <dynamicField name="*_f"  type="float"  indexed="true"  stored="true"/>
+   <dynamicField name="*_f1"  type="float"  indexed="true"  stored="true" multiValued="false"/>
+   <dynamicField name="*_d"  type="double" indexed="true"  stored="true"/>
+   <dynamicField name="*_d1"  type="double" indexed="true"  stored="true" multiValued="false"/>
+   <dynamicField name="*_dt" type="date"    indexed="true"  stored="true"/>
+   <dynamicField name="*_dt1" type="date"    indexed="true"  stored="true" multiValued="false"/>
+   <dynamicField name="*_bcd" type="bcdstr" indexed="true"  stored="true"/>
+
+      <!-- some trie-coded dynamic fields for faster range queries -->
+   <dynamicField name="*_ti" type="tint"    indexed="true"  stored="true"/>
+   <dynamicField name="*_ti1" type="tint"    indexed="true"  stored="true" multiValued="false"/>
+   <dynamicField name="*_tl" type="tlong"   indexed="true"  stored="true"/>
+   <dynamicField name="*_tl1" type="tlong"   indexed="true"  stored="true" multiValued="false"/>
+   <dynamicField name="*_tf" type="tfloat"  indexed="true"  stored="true"/>
+   <dynamicField name="*_tf1" type="tfloat"  indexed="true"  stored="true" multiValued="false"/>
+   <dynamicField name="*_td" type="tdouble" indexed="true"  stored="true"/>
+   <dynamicField name="*_td1" type="tdouble" indexed="true" stored="true" multiValued="false"/>
+   <dynamicField name="*_tds" type="tdouble" indexed="true" stored="true" multiValued="false"/>
+   <dynamicField name="*_tdt" type="tdate"  indexed="true"  stored="true"/>
+   <dynamicField name="*_tdt1" type="tdate"  indexed="true"  stored="true" multiValued="false"/>
+
+   <dynamicField name="*_si"  type="sint"  indexed="true"  stored="true" multiValued="false"/>
+   <dynamicField name="*_sl"  type="slong"  indexed="true"  stored="true"/>
+   <dynamicField name="*_sf"  type="sfloat"  indexed="true"  stored="true"/>
+   <dynamicField name="*_sf1"  type="sfloat"  indexed="true"  stored="true" multiValued="false"/>
+   <dynamicField name="*_sd"  type="sdouble"  indexed="true"  stored="true"/>
+   <dynamicField name="*_sd1"  type="sdouble"  indexed="true"  stored="true" multiValued="false"/>
+
+   <dynamicField name="*_pi"  type="pint"    indexed="true"  stored="true"/>
+   <dynamicField name="*_pf"  type="pfloat"  indexed="true"  stored="true"/>
+   <dynamicField name="*_pl"  type="plong"   indexed="true"  stored="true"/>
+   <dynamicField name="*_pd"  type="pdouble" indexed="true"  stored="true"/>
+   <dynamicField name="*_pdt"  type="pdate" indexed="true"  stored="true"/>
+
+
+   <dynamicField name="*_sI" type="string"  indexed="true"  stored="false"/>
+   <dynamicField name="*_sS" type="string"  indexed="false" stored="true"/>
+   <dynamicField name="t_*"  type="text"    indexed="true"  stored="true"/>
+   <dynamicField name="tv_*"  type="text" indexed="true"  stored="true"
+      termVectors="true" termPositions="true" termOffsets="true"/>
+   <dynamicField name="tv_mv_*"  type="text" indexed="true"  stored="true" multiValued="true"
+      termVectors="true" termPositions="true" termOffsets="true"/>
+
+   <dynamicField name="*_p"  type="xyd" indexed="true"  stored="true" multiValued="false"/>
+
+   <!-- special fields for dynamic copyField test -->
+   <dynamicField name="dynamic_*" type="string" indexed="true" stored="true"/>
+   <dynamicField name="*_dynamic" type="string" indexed="true" stored="true"/>
+
+   <!-- for testing to ensure that longer patterns are matched first -->
+   <dynamicField name="*aa"  type="string"  indexed="true" stored="true"/>
+   <dynamicField name="*aaa" type="pint" indexed="false" stored="true"/>
+
+   <!-- ignored becuase not stored or indexed -->
+   <dynamicField name="*_ignored" type="text" indexed="false" stored="false"/>
+
+   <dynamicField name="*_mfacet" type="string" indexed="true" stored="false" multiValued="true" />
+
+   <!-- make sure custom sims work with dynamic fields -->
+   <!--
+   <dynamicField name="*_sim1" type="sim1" indexed="true" stored="true"/>
+   <dynamicField name="*_sim2" type="sim2" indexed="true" stored="true"/>
+   <dynamicField name="*_sim3" type="sim3" indexed="true" stored="true"/>
+   -->
+ </fields>
+
+ <defaultSearchField>text</defaultSearchField>
+ <uniqueKey>id</uniqueKey>
+
+  <!-- copyField commands copy one field to another at the time a document
+        is added to the index.  It's used either to index the same field different
+        ways, or to add multiple fields to the same field for easier/faster searching.
+   -->
+   <copyField source="title" dest="title_stemmed"/>
+   <copyField source="title" dest="title_lettertok"/>
+
+   <copyField source="title" dest="text"/>
+	 <copyField source="subject" dest="text"/>
+
+	 <copyField source="lowerfilt1" dest="lowerfilt1and2"/>
+	 <copyField source="lowerfilt" dest="lowerfilt1and2"/>
+
+	 <copyField source="*_t" dest="text"/>
+
+	 <copyField source="id"            dest="range_facet_si"/>
+	 <copyField source="id"            dest="range_facet_l"/>
+	 <copyField source="id"            dest="range_facet_sl"/>
+	 <copyField source="range_facet_f" dest="range_facet_sf"/>
+	 <copyField source="range_facet_f" dest="range_facet_d"/>
+	 <copyField source="range_facet_f" dest="range_facet_sd"/>
+
+	 <copyField source="bday" dest="bday_pdt"/>
+	 <copyField source="a_tdt" dest="a_pdt"/>
+
+   <!-- dynamic destination -->
+   <copyField source="*_dynamic" dest="dynamic_*"/>
+
+</schema>
diff --git a/solr/solrj/src/test-files/solrj/solr/collection1/conf/solrconfig-slave1.xml b/solr/solrj/src/test-files/solrj/solr/collection1/conf/solrconfig-slave1.xml
new file mode 100644
index 0000000..2fb1db5
--- /dev/null
+++ b/solr/solrj/src/test-files/solrj/solr/collection1/conf/solrconfig-slave1.xml
@@ -0,0 +1,63 @@
+<?xml version="1.0" ?>
+
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+
+<!-- $Id$
+     $Source$
+     $Name$
+  -->
+
+<config>
+  <luceneMatchVersion>${tests.luceneMatchVersion:LUCENE_CURRENT}</luceneMatchVersion>
+  <dataDir>${solr.data.dir:}</dataDir>
+  <directoryFactory name="DirectoryFactory" class="${solr.directoryFactory:solr.RAMDirectoryFactory}"/>
+
+  <!-- <indexConfig> section could go here, but we want the defaults -->
+
+  <updateHandler class="solr.DirectUpdateHandler2">
+  </updateHandler>
+
+  <requestHandler name="standard" class="solr.StandardRequestHandler">
+    <bool name="httpCaching">true</bool>
+  </requestHandler>
+
+  <!-- test query parameter defaults -->
+  <requestHandler name="defaults" class="solr.StandardRequestHandler">
+
+  </requestHandler>
+
+  <!-- test query parameter defaults -->
+  <requestHandler name="lazy" class="solr.StandardRequestHandler" startup="lazy">
+  </requestHandler>
+
+  <requestHandler name="/update" class="solr.UpdateRequestHandler"  />
+
+  <requestHandler name="/replication" class="solr.ReplicationHandler">
+
+  </requestHandler>
+
+
+  <!-- enable streaming for testing... -->
+  <requestDispatcher handleSelect="true">
+    <requestParsers enableRemoteStreaming="true" multipartUploadLimitInKB="2048"/>
+    <httpCaching lastModifiedFrom="openTime" etagSeed="Solr" never304="false">
+      <cacheControl>max-age=30, public</cacheControl>
+    </httpCaching>
+  </requestDispatcher>
+
+</config>
diff --git a/solr/solrj/src/test-files/solrj/solr/collection1/conf/solrconfig.xml b/solr/solrj/src/test-files/solrj/solr/collection1/conf/solrconfig.xml
new file mode 100644
index 0000000..d78ffae
--- /dev/null
+++ b/solr/solrj/src/test-files/solrj/solr/collection1/conf/solrconfig.xml
@@ -0,0 +1,59 @@
+<?xml version="1.0" encoding="UTF-8" ?>
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+
+<!--
+ This is a stripped down config file used for a simple example...  
+ It is *not* a good example to work from. 
+-->
+<config>
+  <luceneMatchVersion>${tests.luceneMatchVersion:LUCENE_CURRENT}</luceneMatchVersion>
+   <dataDir>${solr.data.dir:}</dataDir>
+  <directoryFactory name="DirectoryFactory" class="${solr.directoryFactory:solr.StandardDirectoryFactory}"/>
+
+  <updateHandler class="solr.DirectUpdateHandler2">
+    <updateLog>
+      <str name="dir">${solr.data.dir:}</str>
+    </updateLog>
+  </updateHandler>
+
+  <!-- realtime get handler, guaranteed to return the latest stored fields 
+    of any document, without the need to commit or open a new searcher. The current 
+    implementation relies on the updateLog feature being enabled. -->
+  <requestHandler name="/get" class="solr.RealTimeGetHandler">
+    <lst name="defaults">
+      <str name="omitHeader">true</str>
+    </lst>
+  </requestHandler>  
+
+  <requestDispatcher handleSelect="true" >
+    <requestParsers enableRemoteStreaming="false" multipartUploadLimitInKB="2048" />
+  </requestDispatcher>
+  
+  <requestHandler name="/replication" class="solr.ReplicationHandler" startup="lazy" /> 
+  
+  <requestHandler name="standard" class="solr.StandardRequestHandler" default="true" />
+  <requestHandler name="/update" class="solr.UpdateRequestHandler"  />
+  <requestHandler name="/admin/" class="org.apache.solr.handler.admin.AdminHandlers" />
+      
+  <!-- config for the admin interface --> 
+  <admin>
+    <defaultQuery>solr</defaultQuery>
+  </admin>
+
+</config>
+
diff --git a/solr/solrj/src/test-files/solrj/solr/conf/schema-replication1.xml b/solr/solrj/src/test-files/solrj/solr/conf/schema-replication1.xml
deleted file mode 100644
index 48ecd9f..0000000
--- a/solr/solrj/src/test-files/solrj/solr/conf/schema-replication1.xml
+++ /dev/null
@@ -1,49 +0,0 @@
-<?xml version="1.0" ?>
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-
-<!-- The Solr schema file. This file should be named "schema.xml" and
-     should be located where the classloader for the Solr webapp can find it.
-
-     This schema is used for testing, and as such has everything and the 
-     kitchen sink thrown in. See example/solr/conf/schema.xml for a 
-     more concise example.
-
-     $Id$
-     $Source$
-     $Name$
-  -->
-
-<schema name="test" version="1.2">
-  <types>
-
-    <fieldType name="integer" class="solr.IntField"/>
-    <fieldtype name="string" class="solr.StrField" sortMissingLast="true"/>
-
-
-  </types>
-
-
-  <fields>
-    <field name="id" type="integer" indexed="true" stored="true" multiValued="false" required="false"/>
-    <field name="name" type="string" indexed="true" stored="true"/>
-
-  </fields>
-
-  <uniqueKey>id</uniqueKey>
-
-</schema>
diff --git a/solr/solrj/src/test-files/solrj/solr/conf/schema.xml b/solr/solrj/src/test-files/solrj/solr/conf/schema.xml
deleted file mode 100644
index 0e0e15f..0000000
--- a/solr/solrj/src/test-files/solrj/solr/conf/schema.xml
+++ /dev/null
@@ -1,647 +0,0 @@
-<?xml version="1.0" ?>
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-
-<!-- The Solr schema file. This file should be named "schema.xml" and
-     should be located where the classloader for the Solr webapp can find it.
-
-     This schema is used for testing, and as such has everything and the
-     kitchen sink thrown in. See example/solr/conf/schema.xml for a
-     more concise example.
-
-     $Id: schema.xml 382610 2006-03-03 01:43:03Z yonik $
-     $Source: /cvs/main/searching/solr-configs/test/WEB-INF/classes/schema.xml,v $
-     $Name:  $
-  -->
-
-<schema name="test" version="1.0">
-  <types>
-
-    <!-- field type definitions... note that the "name" attribute is
-         just a label to be used by field definitions.  The "class"
-         attribute and any other attributes determine the real type and
-         behavior of the fieldtype.
-      -->
-
-    <!-- numeric field types that store and index the text
-         value verbatim (and hence don't sort correctly or support range queries.)
-         These are provided more for backward compatability, allowing one
-         to create a schema that matches an existing lucene index.
-    -->
-    <fieldType name="pint" class="solr.IntField"/>
-    <fieldType name="plong" class="solr.LongField"/>
-    <fieldtype name="pfloat" class="solr.FloatField"/>
-    <fieldType name="pdouble" class="solr.DoubleField"/>
-
-    <fieldType name="int" class="solr.TrieIntField" precisionStep="0" omitNorms="true" positionIncrementGap="0"/>
-    <fieldType name="float" class="solr.TrieFloatField" precisionStep="0" omitNorms="true" positionIncrementGap="0"/>
-    <fieldType name="long" class="solr.TrieLongField" precisionStep="0" omitNorms="true" positionIncrementGap="0"/>
-    <fieldType name="double" class="solr.TrieDoubleField" precisionStep="0" omitNorms="true" positionIncrementGap="0"/>
-
-    <fieldType name="tint" class="solr.TrieIntField" precisionStep="8" omitNorms="true" positionIncrementGap="0"/>
-    <fieldType name="tfloat" class="solr.TrieFloatField" precisionStep="8" omitNorms="true" positionIncrementGap="0"/>
-    <fieldType name="tlong" class="solr.TrieLongField" precisionStep="8" omitNorms="true" positionIncrementGap="0"/>
-    <fieldType name="tdouble" class="solr.TrieDoubleField" precisionStep="8" omitNorms="true" positionIncrementGap="0"/>
-
-    <!-- numeric field types that manipulate the value into
-       a string value that isn't human readable in it's internal form,
-       but sorts correctly and supports range queries.
-
-         If sortMissingLast="true" then a sort on this field will cause documents
-       without the field to come after documents with the field,
-       regardless of the requested sort order.
-         If sortMissingFirst="true" then a sort on this field will cause documents
-       without the field to come before documents with the field,
-       regardless of the requested sort order.
-         If sortMissingLast="false" and sortMissingFirst="false" (the default),
-       then default lucene sorting will be used which places docs without the field
-       first in an ascending sort and last in a descending sort.
-    -->
-    <fieldtype name="sint" class="solr.SortableIntField" sortMissingLast="true"/>
-    <fieldtype name="slong" class="solr.SortableLongField" sortMissingLast="true"/>
-    <fieldtype name="sfloat" class="solr.SortableFloatField" sortMissingLast="true"/>
-    <fieldtype name="sdouble" class="solr.SortableDoubleField" sortMissingLast="true"/>
-
-    <!-- bcd versions of sortable numeric type may provide smaller
-         storage space and support very large numbers.
-    -->
-    <fieldtype name="bcdint" class="solr.BCDIntField" sortMissingLast="true"/>
-    <fieldtype name="bcdlong" class="solr.BCDLongField" sortMissingLast="true"/>
-    <fieldtype name="bcdstr" class="solr.BCDStrField" sortMissingLast="true"/>
-
-    <!-- Field type demonstrating an Analyzer failure -->
-    <fieldtype name="failtype1" class="solr.TextField">
-      <analyzer type="index">
-          <tokenizer class="solr.MockTokenizerFactory"/>
-          <filter class="solr.WordDelimiterFilterFactory" generateWordParts="1" generateNumberParts="0" catenateWords="0" catenateNumbers="0" catenateAll="0"/>
-          <filter class="solr.LowerCaseFilterFactory"/>
-      </analyzer>
-    </fieldtype>
-
-    <!-- Demonstrating ignoreCaseChange -->
-    <fieldtype name="wdf_nocase" class="solr.TextField">
-      <analyzer>
-          <tokenizer class="solr.MockTokenizerFactory"/>
-          <filter class="solr.WordDelimiterFilterFactory" generateWordParts="1" generateNumberParts="0" catenateWords="0" catenateNumbers="0" catenateAll="0" splitOnCaseChange="0" preserveOriginal="0"/>
-          <filter class="solr.LowerCaseFilterFactory"/>
-      </analyzer>
-    </fieldtype>
-
-     <fieldtype name="wdf_preserve" class="solr.TextField">
-      <analyzer>
-          <tokenizer class="solr.MockTokenizerFactory"/>
-          <filter class="solr.WordDelimiterFilterFactory" generateWordParts="0" generateNumberParts="1" catenateWords="0" catenateNumbers="0" catenateAll="0" splitOnCaseChange="0" preserveOriginal="1"/>
-          <filter class="solr.LowerCaseFilterFactory"/>
-      </analyzer>
-    </fieldtype>
-
-
-    <!-- HighlitText optimizes storage for (long) columns which will be highlit -->
-    <fieldtype name="highlittext" class="solr.TextField" compressThreshold="345" />
-
-    <fieldtype name="boolean" class="solr.BoolField" sortMissingLast="true"/>
-    <fieldtype name="string" class="solr.StrField" sortMissingLast="true"/>
-
-    <!-- format for date is 1995-12-31T23:59:59.999Z and only the fractional
-         seconds part (.999) is optional.
-      -->
-    <fieldtype name="date" class="solr.TrieDateField" precisionStep="0"/>
-    <fieldtype name="tdate" class="solr.TrieDateField" precisionStep="6"/>
-    <fieldtype name="pdate" class="solr.DateField" sortMissingLast="true"/>
-
-
-    <!-- solr.TextField allows the specification of custom
-         text analyzers specified as a tokenizer and a list
-         of token filters.
-      -->
-    <fieldtype name="text" class="solr.TextField">
-      <analyzer>
-        <tokenizer class="solr.StandardTokenizerFactory"/>
-        <filter class="solr.StandardFilterFactory"/>
-        <filter class="solr.LowerCaseFilterFactory"/>
-        <filter class="solr.StopFilterFactory"/>
-        <filter class="solr.PorterStemFilterFactory"/>
-      </analyzer>
-    </fieldtype>
-
-
-    <fieldtype name="nametext" class="solr.TextField">
-      <analyzer class="org.apache.lucene.analysis.core.WhitespaceAnalyzer"/>
-    </fieldtype>
-
-    <fieldtype name="teststop" class="solr.TextField">
-       <analyzer>
-        <tokenizer class="solr.LowerCaseTokenizerFactory"/>
-        <filter class="solr.StandardFilterFactory"/>
-        <filter class="solr.StopFilterFactory" words="stopwords.txt"/>
-      </analyzer>
-    </fieldtype>
-
-    <!-- fieldtypes in this section isolate tokenizers and tokenfilters for testing -->
-    <fieldtype name="lowertok" class="solr.TextField">
-      <analyzer><tokenizer class="solr.LowerCaseTokenizerFactory"/></analyzer>
-    </fieldtype>
-    <fieldtype name="keywordtok" class="solr.TextField">
-      <analyzer><tokenizer class="solr.MockTokenizerFactory" pattern="keyword"/></analyzer>
-    </fieldtype>
-    <fieldtype name="standardtok" class="solr.TextField">
-      <analyzer><tokenizer class="solr.StandardTokenizerFactory"/></analyzer>
-    </fieldtype>
-    <fieldtype name="lettertok" class="solr.TextField">
-      <analyzer><tokenizer class="solr.LetterTokenizerFactory"/></analyzer>
-    </fieldtype>
-    <fieldtype name="whitetok" class="solr.TextField">
-      <analyzer><tokenizer class="solr.MockTokenizerFactory"/></analyzer>
-    </fieldtype>
-    <fieldtype name="HTMLstandardtok" class="solr.TextField">
-      <analyzer>
-      <charFilter class="solr.HTMLStripCharFilterFactory"/>
-      <tokenizer class="solr.StandardTokenizerFactory"/>
-      </analyzer>
-    </fieldtype>
-    <fieldtype name="HTMLwhitetok" class="solr.TextField">
-      <analyzer>
-      <charFilter class="solr.HTMLStripCharFilterFactory"/>
-      <tokenizer class="solr.MockTokenizerFactory"/>
-      </analyzer>
-    </fieldtype>
-    <fieldtype name="standardtokfilt" class="solr.TextField">
-      <analyzer>
-        <tokenizer class="solr.StandardTokenizerFactory"/>
-        <filter class="solr.StandardFilterFactory"/>
-      </analyzer>
-    </fieldtype>
-    <fieldtype name="standardfilt" class="solr.TextField">
-      <analyzer>
-        <tokenizer class="solr.MockTokenizerFactory"/>
-        <filter class="solr.StandardFilterFactory"/>
-      </analyzer>
-    </fieldtype>
-    <fieldtype name="lowerfilt" class="solr.TextField">
-      <analyzer>
-        <tokenizer class="solr.MockTokenizerFactory"/>
-        <filter class="solr.LowerCaseFilterFactory"/>
-      </analyzer>
-    </fieldtype>
-    <fieldtype name="lowerpunctfilt" class="solr.TextField">
-      <analyzer>
-        <tokenizer class="solr.MockTokenizerFactory"/>
-        <filter name="syn" class="solr.SynonymFilterFactory" synonyms="synonyms.txt" expand="true"/>
-        <filter class="solr.WordDelimiterFilterFactory" generateWordParts="1" generateNumberParts="1" catenateWords="1" catenateNumbers="1" catenateAll="1" splitOnCaseChange="1"/>
-        <filter class="solr.LowerCaseFilterFactory"/>
-      </analyzer>
-    </fieldtype>
-    <fieldtype name="patternreplacefilt" class="solr.TextField">
-      <analyzer type="index">
-        <tokenizer class="solr.MockTokenizerFactory" pattern="keyword"/>
-        <filter class="solr.PatternReplaceFilterFactory"
-                pattern="([^a-zA-Z])" replacement="_" replace="all"
-        />
-      </analyzer>
-      <analyzer type="query">
-        <tokenizer class="solr.MockTokenizerFactory" pattern="keyword"/>
-      </analyzer>
-    </fieldtype>
-    <fieldtype name="patterntok" class="solr.TextField">
-      <analyzer>
-        <tokenizer class="solr.PatternTokenizerFactory" pattern=","/>
-      </analyzer>
-    </fieldtype>
-    <fieldtype name="porterfilt" class="solr.TextField">
-      <analyzer>
-        <tokenizer class="solr.MockTokenizerFactory"/>
-        <filter class="solr.PorterStemFilterFactory"/>
-      </analyzer>
-    </fieldtype>
-    <!-- fieldtype name="snowballfilt" class="solr.TextField">
-      <analyzer>
-        <tokenizer class="solr.MockTokenizerFactory"/>
-        <filter class="solr.SnowballPorterFilterFactory"/>
-      </analyzer>
-    </fieldtype -->
-    <fieldtype name="engporterfilt" class="solr.TextField">
-      <analyzer>
-        <tokenizer class="solr.MockTokenizerFactory"/>
-        <filter class="solr.PorterStemFilterFactory"/>
-      </analyzer>
-    </fieldtype>
-    <fieldtype name="custengporterfilt" class="solr.TextField">
-      <analyzer>
-        <tokenizer class="solr.MockTokenizerFactory"/>
-        <filter class="solr.KeywordMarkerFilterFactory" protected="protwords.txt"/>
-        <filter class="solr.PorterStemFilterFactory"/>
-      </analyzer>
-    </fieldtype>
-    <fieldtype name="stopfilt" class="solr.TextField">
-      <analyzer>
-        <tokenizer class="solr.MockTokenizerFactory"/>
-        <filter class="solr.StopFilterFactory" ignoreCase="true"/>
-      </analyzer>
-    </fieldtype>
-    <fieldtype name="custstopfilt" class="solr.TextField">
-      <analyzer>
-        <tokenizer class="solr.MockTokenizerFactory"/>
-        <filter class="solr.StopFilterFactory" words="stopwords.txt"/>
-      </analyzer>
-    </fieldtype>
-    <fieldtype name="lengthfilt" class="solr.TextField">
-      <analyzer>
-        <tokenizer class="solr.MockTokenizerFactory"/>
-        <filter class="solr.LengthFilterFactory" min="2" max="5"/>
-      </analyzer>
-    </fieldtype>
-    <fieldType name="charfilthtmlmap" class="solr.TextField">
-      <analyzer>
-        <charFilter class="solr.HTMLStripCharFilterFactory"/>
-        <charFilter class="solr.MappingCharFilterFactory" mapping="mapping-ISOLatin1Accent.txt"/>
-        <tokenizer class="solr.MockTokenizerFactory"/>
-      </analyzer>
-    </fieldType>
-
-    <fieldtype name="subword" class="solr.TextField" multiValued="true" positionIncrementGap="100">
-      <analyzer type="index">
-          <tokenizer class="solr.MockTokenizerFactory"/>
-          <filter class="solr.WordDelimiterFilterFactory" generateWordParts="1" generateNumberParts="1" catenateWords="1" catenateNumbers="1" catenateAll="0"/>
-          <filter class="solr.LowerCaseFilterFactory"/>
-          <filter class="solr.StopFilterFactory"/>
-          <filter class="solr.PorterStemFilterFactory"/>
-      </analyzer>
-      <analyzer type="query">
-          <tokenizer class="solr.MockTokenizerFactory"/>
-          <filter class="solr.WordDelimiterFilterFactory" generateWordParts="1" generateNumberParts="1" catenateWords="0" catenateNumbers="0" catenateAll="0"/>
-          <filter class="solr.LowerCaseFilterFactory"/>
-          <filter class="solr.StopFilterFactory"/>
-          <filter class="solr.PorterStemFilterFactory"/>
-      </analyzer>
-    </fieldtype>
-
-    <fieldtype name="numericsubword" class="solr.TextField" multiValued="true" positionIncrementGap="100">
-      <analyzer type="index">
-          <tokenizer class="solr.MockTokenizerFactory"/>
-          <filter class="solr.LowerCaseFilterFactory"/>
-          <filter class="solr.WordDelimiterFilterFactory" protected="protwords.txt" splitOnNumerics="0" splitOnCaseChange="0" generateWordParts="1" generateNumberParts="0" catenateWords="0" catenateNumbers="0" catenateAll="0"/>
-          <filter class="solr.StopFilterFactory"/>
-          <filter class="solr.PorterStemFilterFactory"/>
-      </analyzer>
-      <analyzer type="query">
-          <tokenizer class="solr.MockTokenizerFactory"/>
-          <filter class="solr.LowerCaseFilterFactory"/>
-          <filter class="solr.WordDelimiterFilterFactory" protected="protwords.txt" splitOnNumerics="0" splitOnCaseChange="0" generateWordParts="1" generateNumberParts="1" catenateWords="1" catenateNumbers="1" catenateAll="0"/>
-          <filter class="solr.StopFilterFactory"/>
-          <filter class="solr.PorterStemFilterFactory"/>
-      </analyzer>
-    </fieldtype>
-
-    <fieldtype name="protectedsubword" class="solr.TextField" multiValued="true" positionIncrementGap="100">
-      <analyzer type="index">
-          <tokenizer class="solr.MockTokenizerFactory"/>
-          <filter class="solr.LowerCaseFilterFactory"/>
-          <filter class="solr.WordDelimiterFilterFactory" protected="protwords.txt" splitOnNumerics="0" splitOnCaseChange="0" generateWordParts="1" generateNumberParts="1" catenateWords="0" catenateNumbers="0" catenateAll="0"/>
-      </analyzer>
-      <analyzer type="query">
-          <tokenizer class="solr.MockTokenizerFactory"/>
-          <filter class="solr.LowerCaseFilterFactory"/>
-      </analyzer>
-    </fieldtype>
-
-
-    <!-- more flexible in matching skus, but more chance of a false match -->
-    <fieldtype name="skutype1" class="solr.TextField">
-      <analyzer type="index">
-          <tokenizer class="solr.MockTokenizerFactory"/>
-          <filter class="solr.WordDelimiterFilterFactory" generateWordParts="1" generateNumberParts="1" catenateWords="1" catenateNumbers="1" catenateAll="0"/>
-          <filter class="solr.LowerCaseFilterFactory"/>
-      </analyzer>
-      <analyzer type="query">
-          <tokenizer class="solr.MockTokenizerFactory"/>
-          <filter class="solr.WordDelimiterFilterFactory" generateWordParts="0" generateNumberParts="0" catenateWords="1" catenateNumbers="1" catenateAll="0"/>
-          <filter class="solr.LowerCaseFilterFactory"/>
-      </analyzer>
-    </fieldtype>
-
-    <!-- less flexible in matching skus, but less chance of a false match -->
-    <fieldtype name="skutype2" class="solr.TextField">
-      <analyzer type="index">
-          <tokenizer class="solr.MockTokenizerFactory"/>
-          <filter class="solr.WordDelimiterFilterFactory" generateWordParts="0" generateNumberParts="0" catenateWords="1" catenateNumbers="1" catenateAll="0"/>
-          <filter class="solr.LowerCaseFilterFactory"/>
-      </analyzer>
-      <analyzer type="query">
-          <tokenizer class="solr.MockTokenizerFactory"/>
-          <filter class="solr.WordDelimiterFilterFactory" generateWordParts="0" generateNumberParts="0" catenateWords="1" catenateNumbers="1" catenateAll="0"/>
-          <filter class="solr.LowerCaseFilterFactory"/>
-      </analyzer>
-    </fieldtype>
-
-    <!-- less flexible in matching skus, but less chance of a false match -->
-    <fieldtype name="syn" class="solr.TextField">
-      <analyzer>
-          <tokenizer class="solr.MockTokenizerFactory"/>
-          <filter name="syn" class="solr.SynonymFilterFactory" synonyms="old_synonyms.txt"/>
-      </analyzer>
-    </fieldtype>
-
-    <!-- Demonstrates How RemoveDuplicatesTokenFilter makes stemmed
-         synonyms "better"
-      -->
-    <fieldtype name="dedup" class="solr.TextField">
-      <analyzer>
-          <tokenizer class="solr.MockTokenizerFactory"/>
-          <filter class="solr.SynonymFilterFactory"
-                  synonyms="old_synonyms.txt" expand="true" />
-          <filter class="solr.PorterStemFilterFactory"/>
-          <filter class="solr.RemoveDuplicatesTokenFilterFactory" />
-      </analyzer>
-    </fieldtype>
-
-    <fieldtype  name="unstored" class="solr.StrField" indexed="true" stored="false"/>
-
-
-  <fieldtype name="textgap" class="solr.TextField" multiValued="true" positionIncrementGap="100">
-      <analyzer>
-          <tokenizer class="solr.MockTokenizerFactory"/>
-          <filter class="solr.LowerCaseFilterFactory"/>
-      </analyzer>
-  </fieldtype>
-
-  <fieldType name="uuid" class="solr.UUIDField" />
-
-    <!-- Try out some point types -->
-  <fieldType name="xy" class="solr.PointType" dimension="2" subFieldType="double"/>
-  <fieldType name="x" class="solr.PointType" dimension="1" subFieldType="double"/>
-  <fieldType name="tenD" class="solr.PointType" dimension="10" subFieldType="double"/>
-    <!-- Use the sub field suffix -->
-  <fieldType name="xyd" class="solr.PointType" dimension="2" subFieldSuffix="_d1"/>
-    <fieldtype name="geohash" class="solr.GeoHashField"/>
-
-
-  <fieldType name="latLon" class="solr.LatLonType" subFieldType="double"/>
-
-  <!--  some per-field similarity examples -->
-  
-  <!--  specify a Similarity classname directly -->
-  <fieldType name="sim1" class="solr.TextField">
-    <analyzer>
-      <tokenizer class="solr.MockTokenizerFactory"/>
-    </analyzer>
-    <similarity class="org.apache.lucene.misc.SweetSpotSimilarity"/>
-  </fieldType>
-
-  <!--  specify a Similarity factory -->  
-  <fieldType name="sim2" class="solr.TextField">
-    <analyzer>
-      <tokenizer class="solr.MockTokenizerFactory"/>
-    </analyzer>
-    <similarity class="org.apache.solr.schema.CustomSimilarityFactory">
-      <str name="echo">is there an echo?</str>
-    </similarity>
-  </fieldType>
-  
-  <!-- don't specify any sim at all: get the default  -->
-  <fieldType name="sim3" class="solr.TextField">
-    <analyzer>
-      <tokenizer class="solr.MockTokenizerFactory"/>
-    </analyzer>
-  </fieldType>
- </types>
-
-
- <fields>
-   <field name="id" type="int" indexed="true" stored="true" multiValued="false" required="false"/>
-   <field name="signatureField" type="string" indexed="true" stored="false"/>
-   <field name="uuid" type="uuid" stored="true" />
-   <field name="name" type="nametext" indexed="true" stored="true"/>
-   <field name="text" type="text" indexed="true" stored="false"/>
-   <field name="subject" type="text" indexed="true" stored="true"/>
-   <field name="title" type="nametext" indexed="true" stored="true"/>
-   <field name="weight" type="float" indexed="true" stored="true" multiValued="false"/>
-   <field name="bday" type="date" indexed="true" stored="true" multiValued="false"/>
-
-   <field name="title_stemmed" type="text" indexed="true" stored="false"/>
-   <field name="title_lettertok" type="lettertok" indexed="true" stored="false"/>
-
-   <field name="syn" type="syn" indexed="true" stored="true"/>
-
-   <!-- to test property inheritance and overriding -->
-   <field name="shouldbeunstored" type="unstored" />
-   <field name="shouldbestored" type="unstored" stored="true"/>
-   <field name="shouldbeunindexed" type="unstored" indexed="false" stored="true"/>
-
-   <!-- Test points -->
-      <!-- Test points -->
-   <field name="home" type="xy" indexed="true" stored="true" multiValued="false"/>
-   <field name="x" type="x" indexed="true" stored="true" multiValued="false"/>
-   <field name="homed" type="xyd" indexed="true" stored="true" multiValued="false"/>
-   <field name="home_ns" type="xy" indexed="true" stored="false" multiValued="false"/>
-   <field name="work" type="xy" indexed="true" stored="true" multiValued="false"/>
-
-   <field name="home_ll" type="latLon" indexed="true" stored="true" multiValued="false"/>
-   <field name="home_gh" type="geohash" indexed="true" stored="true" multiValued="false"/>
-
-
-   <field name="point10" type="tenD" indexed="true" stored="true" multiValued="false"/>
-
-
-   <!-- test different combinations of indexed and stored -->
-   <field name="bind" type="boolean" indexed="true" stored="false"/>
-   <field name="bsto" type="boolean" indexed="false" stored="true"/>
-   <field name="bindsto" type="boolean" indexed="true" stored="true"/>
-   <field name="isto" type="int" indexed="false" stored="true"/>
-   <field name="iind" type="int" indexed="true" stored="false"/>
-   <field name="ssto" type="string" indexed="false" stored="true"/>
-   <field name="sind" type="string" indexed="true" stored="false"/>
-   <field name="sindsto" type="string" indexed="true" stored="true"/>
-
-   <!-- test combinations of term vector settings -->
-   <field name="test_basictv" type="text" termVectors="true"/>
-   <field name="test_notv" type="text" termVectors="false"/>
-   <field name="test_postv" type="text" termVectors="true" termPositions="true"/>
-   <field name="test_offtv" type="text" termVectors="true" termOffsets="true"/>
-   <field name="test_posofftv" type="text" termVectors="true"
-     termPositions="true" termOffsets="true"/>
-
-   <!-- test highlit field settings -->
-   <field name="test_hlt" type="highlittext" indexed="true" compressed="true"/>
-   <field name="test_hlt_off" type="highlittext" indexed="true" compressed="false"/>
-
-   <!-- fields to test individual tokenizers and tokenfilters -->
-   <field name="teststop" type="teststop" indexed="true" stored="true"/>
-   <field name="lowertok" type="lowertok" indexed="true" stored="true"/>
-   <field name="keywordtok" type="keywordtok" indexed="true" stored="true"/>
-   <field name="standardtok" type="standardtok" indexed="true" stored="true"/>
-   <field name="HTMLstandardtok" type="HTMLstandardtok" indexed="true" stored="true"/>
-   <field name="lettertok" type="lettertok" indexed="true" stored="true"/>
-   <field name="whitetok" type="whitetok" indexed="true" stored="true"/>
-   <field name="HTMLwhitetok" type="HTMLwhitetok" indexed="true" stored="true"/>
-   <field name="standardtokfilt" type="standardtokfilt" indexed="true" stored="true"/>
-   <field name="standardfilt" type="standardfilt" indexed="true" stored="true"/>
-   <field name="lowerfilt" type="lowerfilt" indexed="true" stored="true"/>
-   <field name="lowerfilt1" type="lowerfilt" indexed="true" stored="true"/>
-	 <field name="lowerfilt1and2" type="lowerfilt" indexed="true" stored="true"/>
-   <field name="patterntok" type="patterntok" indexed="true" stored="true"/>
-   <field name="patternreplacefilt" type="patternreplacefilt" indexed="true" stored="true"/>
-   <field name="porterfilt" type="porterfilt" indexed="true" stored="true"/>
-   <field name="engporterfilt" type="engporterfilt" indexed="true" stored="true"/>
-   <field name="custengporterfilt" type="custengporterfilt" indexed="true" stored="true"/>
-   <field name="stopfilt" type="stopfilt" indexed="true" stored="true"/>
-   <field name="custstopfilt" type="custstopfilt" indexed="true" stored="true"/>
-   <field name="lengthfilt" type="lengthfilt" indexed="true" stored="true"/>
-   <field name="dedup" type="dedup" indexed="true" stored="true"/>
-   <field name="wdf_nocase" type="wdf_nocase" indexed="true" stored="true"/>
-   <field name="wdf_preserve" type="wdf_preserve" indexed="true" stored="true"/>
-
-   <field name="numberpartfail" type="failtype1" indexed="true" stored="true"/>
-
-   <field name="nullfirst" type="string" indexed="true" stored="true" sortMissingFirst="true" multiValued="false"/>
-
-   <field name="subword" type="subword" indexed="true" stored="true"/>
-   <field name="subword_offsets" type="subword" indexed="true" stored="true" termOffsets="true"/>
-   <field name="numericsubword" type="numericsubword" indexed="true" stored="true"/>
-   <field name="protectedsubword" type="protectedsubword" indexed="true" stored="true"/>
-
-   <field name="sku1" type="skutype1" indexed="true" stored="true"/>
-   <field name="sku2" type="skutype2" indexed="true" stored="true"/>
-
-   <field name="textgap" type="textgap" indexed="true" stored="true"/>
-
-   <field name="timestamp" type="date" indexed="true" stored="true" default="NOW" multiValued="false"/>
-   <field name="multiDefault" type="string" indexed="true" stored="true" default="muLti-Default" multiValued="true"/>
-   <field name="intDefault" type="int" indexed="true" stored="true" default="42" multiValued="false"/>
-
-   <field name="sim1text" type="sim1" indexed="true" stored="true"/>
-   <field name="sim2text" type="sim2" indexed="true" stored="true"/>
-   <field name="sim3text" type="sim3" indexed="true" stored="true"/>
-
-   <field name="tlong" type="tlong" indexed="true" stored="true" />
-
-   <!-- Dynamic field definitions.  If a field name is not found, dynamicFields
-        will be used if the name matches any of the patterns.
-        RESTRICTION: the glob-like pattern in the name attribute must have
-        a "*" only at the start or the end.
-        EXAMPLE:  name="*_i" will match any field ending in _i (like myid_i, z_i)
-        Longer patterns will be matched first.  if equal size patterns
-        both match, the first appearing in the schema will be used.
-   -->
-   <dynamicField name="*_i"  type="int"    indexed="true"  stored="true"/>
-   <dynamicField name="*_i1"  type="int"    indexed="true" stored="true" multiValued="false"/>
-                 
-   <dynamicField name="*_s"  type="string"  indexed="true"  stored="true"/>
-   <dynamicField name="*_s1"  type="string"  indexed="true"  stored="true" multiValued="false"/>
-   <dynamicField name="*_l"  type="long"   indexed="true"  stored="true"/>
-   <dynamicField name="*_l1"  type="long"   indexed="true"  stored="true" multiValued="false"/>
-   <dynamicField name="*_t"  type="text"    indexed="true"  stored="true"/>
-   <dynamicField name="*_b"  type="boolean" indexed="true"  stored="true"/>
-   <dynamicField name="*_f"  type="float"  indexed="true"  stored="true"/>
-   <dynamicField name="*_f1"  type="float"  indexed="true"  stored="true" multiValued="false"/>
-   <dynamicField name="*_d"  type="double" indexed="true"  stored="true"/>
-   <dynamicField name="*_d1"  type="double" indexed="true"  stored="true" multiValued="false"/>
-   <dynamicField name="*_dt" type="date"    indexed="true"  stored="true"/>
-   <dynamicField name="*_dt1" type="date"    indexed="true"  stored="true" multiValued="false"/>
-   <dynamicField name="*_bcd" type="bcdstr" indexed="true"  stored="true"/>
-
-      <!-- some trie-coded dynamic fields for faster range queries -->
-   <dynamicField name="*_ti" type="tint"    indexed="true"  stored="true"/>
-   <dynamicField name="*_ti1" type="tint"    indexed="true"  stored="true" multiValued="false"/>
-   <dynamicField name="*_tl" type="tlong"   indexed="true"  stored="true"/>
-   <dynamicField name="*_tl1" type="tlong"   indexed="true"  stored="true" multiValued="false"/>
-   <dynamicField name="*_tf" type="tfloat"  indexed="true"  stored="true"/>
-   <dynamicField name="*_tf1" type="tfloat"  indexed="true"  stored="true" multiValued="false"/>
-   <dynamicField name="*_td" type="tdouble" indexed="true"  stored="true"/>
-   <dynamicField name="*_td1" type="tdouble" indexed="true" stored="true" multiValued="false"/>
-   <dynamicField name="*_tds" type="tdouble" indexed="true" stored="true" multiValued="false"/>
-   <dynamicField name="*_tdt" type="tdate"  indexed="true"  stored="true"/>
-   <dynamicField name="*_tdt1" type="tdate"  indexed="true"  stored="true" multiValued="false"/>
-
-   <dynamicField name="*_si"  type="sint"  indexed="true"  stored="true" multiValued="false"/>
-   <dynamicField name="*_sl"  type="slong"  indexed="true"  stored="true"/>
-   <dynamicField name="*_sf"  type="sfloat"  indexed="true"  stored="true"/>
-   <dynamicField name="*_sf1"  type="sfloat"  indexed="true"  stored="true" multiValued="false"/>
-   <dynamicField name="*_sd"  type="sdouble"  indexed="true"  stored="true"/>
-   <dynamicField name="*_sd1"  type="sdouble"  indexed="true"  stored="true" multiValued="false"/>
-
-   <dynamicField name="*_pi"  type="pint"    indexed="true"  stored="true"/>
-   <dynamicField name="*_pf"  type="pfloat"  indexed="true"  stored="true"/>
-   <dynamicField name="*_pl"  type="plong"   indexed="true"  stored="true"/>
-   <dynamicField name="*_pd"  type="pdouble" indexed="true"  stored="true"/>
-   <dynamicField name="*_pdt"  type="pdate" indexed="true"  stored="true"/>
-
-
-   <dynamicField name="*_sI" type="string"  indexed="true"  stored="false"/>
-   <dynamicField name="*_sS" type="string"  indexed="false" stored="true"/>
-   <dynamicField name="t_*"  type="text"    indexed="true"  stored="true"/>
-   <dynamicField name="tv_*"  type="text" indexed="true"  stored="true"
-      termVectors="true" termPositions="true" termOffsets="true"/>
-   <dynamicField name="tv_mv_*"  type="text" indexed="true"  stored="true" multiValued="true"
-      termVectors="true" termPositions="true" termOffsets="true"/>
-
-   <dynamicField name="*_p"  type="xyd" indexed="true"  stored="true" multiValued="false"/>
-
-   <!-- special fields for dynamic copyField test -->
-   <dynamicField name="dynamic_*" type="string" indexed="true" stored="true"/>
-   <dynamicField name="*_dynamic" type="string" indexed="true" stored="true"/>
-
-   <!-- for testing to ensure that longer patterns are matched first -->
-   <dynamicField name="*aa"  type="string"  indexed="true" stored="true"/>
-   <dynamicField name="*aaa" type="pint" indexed="false" stored="true"/>
-
-   <!-- ignored becuase not stored or indexed -->
-   <dynamicField name="*_ignored" type="text" indexed="false" stored="false"/>
-
-   <dynamicField name="*_mfacet" type="string" indexed="true" stored="false" multiValued="true" />
-
-   <!-- make sure custom sims work with dynamic fields -->
-   <dynamicField name="*_sim1" type="sim1" indexed="true" stored="true"/>
-   <dynamicField name="*_sim2" type="sim2" indexed="true" stored="true"/>
-   <dynamicField name="*_sim3" type="sim3" indexed="true" stored="true"/>
- </fields>
-
- <defaultSearchField>text</defaultSearchField>
- <uniqueKey>id</uniqueKey>
-
-  <!-- copyField commands copy one field to another at the time a document
-        is added to the index.  It's used either to index the same field different
-        ways, or to add multiple fields to the same field for easier/faster searching.
-   -->
-   <copyField source="title" dest="title_stemmed"/>
-   <copyField source="title" dest="title_lettertok"/>
-
-   <copyField source="title" dest="text"/>
-	 <copyField source="subject" dest="text"/>
-
-	 <copyField source="lowerfilt1" dest="lowerfilt1and2"/>
-	 <copyField source="lowerfilt" dest="lowerfilt1and2"/>
-
-	 <copyField source="*_t" dest="text"/>
-
-	 <copyField source="id"            dest="range_facet_si"/>
-	 <copyField source="id"            dest="range_facet_l"/>
-	 <copyField source="id"            dest="range_facet_sl"/>
-	 <copyField source="range_facet_f" dest="range_facet_sf"/>
-	 <copyField source="range_facet_f" dest="range_facet_d"/>
-	 <copyField source="range_facet_f" dest="range_facet_sd"/>
-
-	 <copyField source="bday" dest="bday_pdt"/>
-	 <copyField source="a_tdt" dest="a_pdt"/>
-
-   <!-- dynamic destination -->
-   <copyField source="*_dynamic" dest="dynamic_*"/>
-
- <!-- example of a custom similarity -->
- <similarity class="org.apache.solr.schema.CustomSimilarityFactory">
-   <str name="echo">I am your default sim</str>
- </similarity>
-</schema>
diff --git a/solr/solrj/src/test-files/solrj/solr/conf/solrconfig-slave1.xml b/solr/solrj/src/test-files/solrj/solr/conf/solrconfig-slave1.xml
deleted file mode 100644
index 2fb1db5..0000000
--- a/solr/solrj/src/test-files/solrj/solr/conf/solrconfig-slave1.xml
+++ /dev/null
@@ -1,63 +0,0 @@
-<?xml version="1.0" ?>
-
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-
-<!-- $Id$
-     $Source$
-     $Name$
-  -->
-
-<config>
-  <luceneMatchVersion>${tests.luceneMatchVersion:LUCENE_CURRENT}</luceneMatchVersion>
-  <dataDir>${solr.data.dir:}</dataDir>
-  <directoryFactory name="DirectoryFactory" class="${solr.directoryFactory:solr.RAMDirectoryFactory}"/>
-
-  <!-- <indexConfig> section could go here, but we want the defaults -->
-
-  <updateHandler class="solr.DirectUpdateHandler2">
-  </updateHandler>
-
-  <requestHandler name="standard" class="solr.StandardRequestHandler">
-    <bool name="httpCaching">true</bool>
-  </requestHandler>
-
-  <!-- test query parameter defaults -->
-  <requestHandler name="defaults" class="solr.StandardRequestHandler">
-
-  </requestHandler>
-
-  <!-- test query parameter defaults -->
-  <requestHandler name="lazy" class="solr.StandardRequestHandler" startup="lazy">
-  </requestHandler>
-
-  <requestHandler name="/update" class="solr.UpdateRequestHandler"  />
-
-  <requestHandler name="/replication" class="solr.ReplicationHandler">
-
-  </requestHandler>
-
-
-  <!-- enable streaming for testing... -->
-  <requestDispatcher handleSelect="true">
-    <requestParsers enableRemoteStreaming="true" multipartUploadLimitInKB="2048"/>
-    <httpCaching lastModifiedFrom="openTime" etagSeed="Solr" never304="false">
-      <cacheControl>max-age=30, public</cacheControl>
-    </httpCaching>
-  </requestDispatcher>
-
-</config>
diff --git a/solr/solrj/src/test-files/solrj/solr/solr.xml b/solr/solrj/src/test-files/solrj/solr/solr.xml
new file mode 100644
index 0000000..3045891
--- /dev/null
+++ b/solr/solrj/src/test-files/solrj/solr/solr.xml
@@ -0,0 +1,34 @@
+<?xml version="1.0" encoding="UTF-8" ?>
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+
+<!--
+ All (relative) paths are relative to the installation path
+  
+  persistent: Save changes made via the API to this file
+  sharedLib: path to a lib directory that will be shared across all cores
+-->
+<solr persistent="false">
+
+  <!--
+  adminPath: RequestHandler path to manage cores.  
+    If 'null' (or absent), cores will not be manageable via request handler
+  -->
+  <cores adminPath="/admin/cores" defaultCoreName="collection1" host="127.0.0.1" hostPort="${hostPort:8983}" hostContext="solr" zkClientTimeout="8000" numShards="${numShards:3}">
+    <core name="collection1" instanceDir="collection1" shard="${shard:}" collection="${collection:collection1}" config="${solrconfig:solrconfig.xml}" schema="${schema:schema.xml}"/>
+  </cores>
+</solr>
diff --git a/solr/solrj/src/test/org/apache/solr/client/solrj/TestLBHttpSolrServer.java b/solr/solrj/src/test/org/apache/solr/client/solrj/TestLBHttpSolrServer.java
index d155b78..190b9c3 100644
--- a/solr/solrj/src/test/org/apache/solr/client/solrj/TestLBHttpSolrServer.java
+++ b/solr/solrj/src/test/org/apache/solr/client/solrj/TestLBHttpSolrServer.java
@@ -237,7 +237,7 @@ public class TestLBHttpSolrServer extends LuceneTestCase {
     }
 
     public String getSchemaFile() {
-      return "solrj/solr/conf/schema-replication1.xml";
+      return "solrj/solr/collection1/conf/schema-replication1.xml";
     }
 
     public String getConfDir() {
@@ -249,7 +249,7 @@ public class TestLBHttpSolrServer extends LuceneTestCase {
     }
 
     public String getSolrConfigFile() {
-      return "solrj/solr/conf/solrconfig-slave1.xml";
+      return "solrj/solr/collection1/conf/solrconfig-slave1.xml";
     }
 
     public void setUp() throws Exception {
diff --git a/solr/solrj/src/test/org/apache/solr/client/solrj/impl/CloudSolrServerTest.java b/solr/solrj/src/test/org/apache/solr/client/solrj/impl/CloudSolrServerTest.java
new file mode 100644
index 0000000..9e9ec97
--- /dev/null
+++ b/solr/solrj/src/test/org/apache/solr/client/solrj/impl/CloudSolrServerTest.java
@@ -0,0 +1,139 @@
+package org.apache.solr.client.solrj.impl;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.File;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Set;
+
+import org.apache.lucene.util.LuceneTestCase.Slow;
+import org.apache.solr.cloud.AbstractFullDistribZkTestBase;
+import org.apache.solr.cloud.AbstractZkTestCase;
+import org.apache.solr.util.ExternalPaths;
+import org.junit.After;
+import org.junit.AfterClass;
+import org.junit.Before;
+import org.junit.BeforeClass;
+
+/**
+ * This test would be faster if we simulated the zk state instead.
+ */
+@Slow
+public class CloudSolrServerTest extends AbstractFullDistribZkTestBase {
+  
+  private static final String SOLR_HOME = ExternalPaths.SOURCE_HOME + File.separator + "solrj"
+      + File.separator + "src" + File.separator + "test-files"
+      + File.separator + "solrj" + File.separator + "solr";
+
+  @BeforeClass
+  public static void beforeSuperClass() {
+      AbstractZkTestCase.SOLRHOME = new File(SOLR_HOME());
+  }
+  
+  @AfterClass
+  public static void afterSuperClass() {
+    
+  }
+  
+  @Override
+  public String getSolrHome() {
+    return SOLR_HOME;
+  }
+  
+  public static String SOLR_HOME() {
+    return SOLR_HOME;
+  }
+  
+  @Before
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    // we expect this time of exception as shards go up and down...
+    //ignoreException(".*");
+    
+    System.setProperty("numShards", Integer.toString(sliceCount));
+  }
+  
+  @Override
+  @After
+  public void tearDown() throws Exception {
+    super.tearDown();
+    resetExceptionIgnores();
+  }
+  
+  public CloudSolrServerTest() {
+    super();
+    sliceCount = 2;
+    shardCount = 6;
+  }
+  
+  @Override
+  public void doTest() throws Exception {
+    
+    handle.clear();
+    handle.put("QTime", SKIPVAL);
+    handle.put("timestamp", SKIPVAL);
+    
+    waitForThingsToLevelOut(15);
+
+    del("*:*");
+
+    indexr(id, 0, "a_t", "to come to the aid of their country.");
+    
+    // compare leaders list
+    CloudJettyRunner shard1Leader = shardToLeaderJetty.get("shard1");
+    CloudJettyRunner shard2Leader = shardToLeaderJetty.get("shard2");
+    assertEquals(2, cloudClient.getLeaderUrlList().size());
+    HashSet<String> leaderUrlSet = new HashSet<String>();
+    leaderUrlSet.addAll(cloudClient.getLeaderUrlList());
+    assertTrue("fail check for leader:" + shard1Leader.url + " in "
+        + leaderUrlSet, leaderUrlSet.contains(shard1Leader.url + "/"));
+    assertTrue("fail check for leader:" + shard2Leader.url + " in "
+        + leaderUrlSet, leaderUrlSet.contains(shard2Leader.url + "/"));
+
+    // compare replicas list
+    Set<String> replicas = new HashSet<String>();
+    List<CloudJettyRunner> jetties = shardToJetty.get("shard1");
+    for (CloudJettyRunner cjetty : jetties) {
+      replicas.add(cjetty.url);
+    }
+    jetties = shardToJetty.get("shard2");
+    for (CloudJettyRunner cjetty : jetties) {
+      replicas.add(cjetty.url);
+    }
+    replicas.remove(shard1Leader.url);
+    replicas.remove(shard2Leader.url);
+    
+    assertEquals(replicas.size(), cloudClient.getReplicasList().size());
+    
+    for (String url : cloudClient.getReplicasList()) {
+      assertTrue("fail check for replica:" + url + " in " + replicas,
+          replicas.contains(stripTrailingSlash(url)));
+    }
+    
+  }
+
+  private String stripTrailingSlash(String url) {
+    if (url.endsWith("/")) {
+      return url.substring(0, url.length() - 1);
+    }
+    return url;
+  }
+
+}
diff --git a/solr/test-framework/src/java/org/apache/solr/SolrTestCaseJ4.java b/solr/test-framework/src/java/org/apache/solr/SolrTestCaseJ4.java
index 69a1c3f..60d3b6c 100755
--- a/solr/test-framework/src/java/org/apache/solr/SolrTestCaseJ4.java
+++ b/solr/test-framework/src/java/org/apache/solr/SolrTestCaseJ4.java
@@ -1388,11 +1388,10 @@ public abstract class SolrTestCaseJ4 extends LuceneTestCase {
       return file;
     } catch (Exception e) {
       /* more friendly than NPE */
-      throw new RuntimeException("Cannot find resource: " + name);
+      throw new RuntimeException("Cannot find resource: " + new File(name).getAbsolutePath());
     }
   }
   
-  // TODO: use solr rather than solr/collection1
   public static String TEST_HOME() {
     return getFile("solr/collection1").getParent();
   }
diff --git a/solr/test-framework/src/java/org/apache/solr/cloud/AbstractDistribZkTestBase.java b/solr/test-framework/src/java/org/apache/solr/cloud/AbstractDistribZkTestBase.java
new file mode 100644
index 0000000..6c1a683
--- /dev/null
+++ b/solr/test-framework/src/java/org/apache/solr/cloud/AbstractDistribZkTestBase.java
@@ -0,0 +1,213 @@
+package org.apache.solr.cloud;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.File;
+import java.util.Map;
+import java.util.concurrent.atomic.AtomicInteger;
+
+import org.apache.commons.io.FileUtils;
+import org.apache.solr.BaseDistributedSearchTestCase;
+import org.apache.solr.client.solrj.embedded.JettySolrRunner;
+import org.apache.solr.cloud.ZkTestServer;
+import org.apache.solr.common.cloud.ClusterState;
+import org.apache.solr.common.cloud.Slice;
+import org.apache.solr.common.cloud.SolrZkClient;
+import org.apache.solr.common.cloud.ZkNodeProps;
+import org.apache.solr.common.cloud.ZkStateReader;
+import org.apache.solr.servlet.SolrDispatchFilter;
+import org.apache.zookeeper.KeeperException;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.BeforeClass;
+
+public abstract class AbstractDistribZkTestBase extends BaseDistributedSearchTestCase {
+  
+  protected static final String DEFAULT_COLLECTION = "collection1";
+  private static final boolean DEBUG = false;
+  protected ZkTestServer zkServer;
+  private AtomicInteger homeCount = new AtomicInteger();
+
+  @BeforeClass
+  public static void beforeThisClass() throws Exception {
+    useFactory(null);
+  }
+
+
+  @Before
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    createTempDir();
+    
+    String zkDir = testDir.getAbsolutePath() + File.separator
+    + "zookeeper/server1/data";
+    zkServer = new ZkTestServer(zkDir);
+    zkServer.run();
+    
+    System.setProperty("zkHost", zkServer.getZkAddress());
+    System.setProperty("enable.update.log", "true");
+    System.setProperty("remove.version.field", "true");
+
+
+    AbstractZkTestCase.buildZooKeeper(zkServer.getZkHost(), zkServer.getZkAddress(), "solrconfig.xml", "schema.xml");
+
+    // set some system properties for use by tests
+    System.setProperty("solr.test.sys.prop1", "propone");
+    System.setProperty("solr.test.sys.prop2", "proptwo");
+  }
+  
+  @Override
+  protected void createServers(int numShards) throws Exception {
+    // give everyone there own solrhome
+    File controlHome = new File(new File(getSolrHome()).getParentFile(), "control" + homeCount.incrementAndGet());
+    FileUtils.copyDirectory(new File(getSolrHome()), controlHome);
+    
+    System.setProperty("collection", "control_collection");
+    controlJetty = createJetty(controlHome, null, "control_shard");
+    System.clearProperty("collection");
+    controlClient = createNewSolrServer(controlJetty.getLocalPort());
+
+    StringBuilder sb = new StringBuilder();
+    for (int i = 1; i <= numShards; i++) {
+      if (sb.length() > 0) sb.append(',');
+      // give everyone there own solrhome
+      File jettyHome = new File(new File(getSolrHome()).getParentFile(), "jetty" + homeCount.incrementAndGet());
+      FileUtils.copyDirectory(new File(getSolrHome()), jettyHome);
+      JettySolrRunner j = createJetty(jettyHome, null, "shard" + (i + 2));
+      jettys.add(j);
+      clients.add(createNewSolrServer(j.getLocalPort()));
+      sb.append("localhost:").append(j.getLocalPort()).append(context);
+    }
+
+    shards = sb.toString();
+    
+    // now wait till we see the leader for each shard
+    for (int i = 1; i <= numShards; i++) {
+      ZkStateReader zkStateReader = ((SolrDispatchFilter) jettys.get(0)
+          .getDispatchFilter().getFilter()).getCores().getZkController()
+          .getZkStateReader();
+      zkStateReader.getLeaderProps("collection1", "shard" + (i + 2), 15000);
+    }
+  }
+  
+  protected void waitForRecoveriesToFinish(String collection, ZkStateReader zkStateReader, boolean verbose)
+      throws Exception {
+    waitForRecoveriesToFinish(collection, zkStateReader, verbose, true);
+  }
+  
+  protected void waitForRecoveriesToFinish(String collection, ZkStateReader zkStateReader, boolean verbose, boolean failOnTimeout)
+      throws Exception {
+    waitForRecoveriesToFinish(collection, zkStateReader, verbose, failOnTimeout, 120 * (TEST_NIGHTLY ? 2 : 1) * RANDOM_MULTIPLIER);
+  }
+  
+  protected void waitForRecoveriesToFinish(String collection,
+      ZkStateReader zkStateReader, boolean verbose, boolean failOnTimeout, int timeoutSeconds)
+      throws Exception {
+    log.info("Wait for recoveries to finish - collection: " + collection + " failOnTimeout:" + failOnTimeout + " timeout (sec):" + timeoutSeconds);
+    boolean cont = true;
+    int cnt = 0;
+    
+    while (cont) {
+      if (verbose) System.out.println("-");
+      boolean sawLiveRecovering = false;
+      zkStateReader.updateClusterState(true);
+      ClusterState clusterState = zkStateReader.getClusterState();
+      Map<String,Slice> slices = clusterState.getSlices(collection);
+      for (Map.Entry<String,Slice> entry : slices.entrySet()) {
+        Map<String,ZkNodeProps> shards = entry.getValue().getShards();
+        for (Map.Entry<String,ZkNodeProps> shard : shards.entrySet()) {
+          if (verbose) System.out.println("rstate:"
+              + shard.getValue().get(ZkStateReader.STATE_PROP)
+              + " live:"
+              + clusterState.liveNodesContain(shard.getValue().get(
+                  ZkStateReader.NODE_NAME_PROP)));
+          String state = shard.getValue().get(ZkStateReader.STATE_PROP);
+          if ((state.equals(ZkStateReader.RECOVERING) || state
+              .equals(ZkStateReader.SYNC) || state.equals(ZkStateReader.DOWN))
+              && clusterState.liveNodesContain(shard.getValue().get(
+                  ZkStateReader.NODE_NAME_PROP))) {
+            sawLiveRecovering = true;
+          }
+        }
+      }
+      if (!sawLiveRecovering || cnt == timeoutSeconds) {
+        if (!sawLiveRecovering) {
+          if (verbose) System.out.println("no one is recoverying");
+        } else {
+          if (failOnTimeout) {
+            fail("There are still nodes recoverying");
+            printLayout();
+            return;
+          }
+          if (verbose) System.out
+              .println("gave up waiting for recovery to finish..");
+        }
+        cont = false;
+      } else {
+        Thread.sleep(1000);
+      }
+      cnt++;
+    }
+  }
+
+  protected void assertAllActive(String collection,ZkStateReader zkStateReader)
+      throws KeeperException, InterruptedException {
+
+      zkStateReader.updateClusterState(true);
+      ClusterState clusterState = zkStateReader.getClusterState();
+      Map<String,Slice> slices = clusterState.getSlices(collection);
+      if (slices == null) {
+        throw new IllegalArgumentException("Cannot find collection:" + collection);
+      }
+      for (Map.Entry<String,Slice> entry : slices.entrySet()) {
+        Map<String,ZkNodeProps> shards = entry.getValue().getShards();
+        for (Map.Entry<String,ZkNodeProps> shard : shards.entrySet()) {
+
+          String state = shard.getValue().get(ZkStateReader.STATE_PROP);
+          if (!state.equals(ZkStateReader.ACTIVE)) {
+            fail("Not all shards are ACTIVE - found a shard that is: " + state);
+          }
+        }
+      }
+  }
+  
+  @Override
+  @After
+  public void tearDown() throws Exception {
+    if (DEBUG) {
+      printLayout();
+    }
+    zkServer.shutdown();
+    System.clearProperty("zkHost");
+    System.clearProperty("collection");
+    System.clearProperty("enable.update.log");
+    System.clearProperty("remove.version.field");
+    System.clearProperty("solr.directoryFactory");
+    System.clearProperty("solr.test.sys.prop1");
+    System.clearProperty("solr.test.sys.prop2");
+    resetExceptionIgnores();
+    super.tearDown();
+  }
+  
+  protected void printLayout() throws Exception {
+    SolrZkClient zkClient = new SolrZkClient(zkServer.getZkHost(), AbstractZkTestCase.TIMEOUT);
+    zkClient.printLayoutToStdOut();
+    zkClient.close();
+  }
+}
diff --git a/solr/test-framework/src/java/org/apache/solr/cloud/AbstractFullDistribZkTestBase.java b/solr/test-framework/src/java/org/apache/solr/cloud/AbstractFullDistribZkTestBase.java
new file mode 100644
index 0000000..04083a9
--- /dev/null
+++ b/solr/test-framework/src/java/org/apache/solr/cloud/AbstractFullDistribZkTestBase.java
@@ -0,0 +1,1222 @@
+package org.apache.solr.cloud;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.File;
+import java.io.IOException;
+import java.net.MalformedURLException;
+import java.net.URI;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Map;
+import java.util.Random;
+import java.util.Set;
+import java.util.concurrent.atomic.AtomicInteger;
+
+import org.apache.http.params.CoreConnectionPNames;
+import org.apache.lucene.util.LuceneTestCase.Slow;
+import org.apache.solr.client.solrj.SolrQuery;
+import org.apache.solr.client.solrj.SolrServer;
+import org.apache.solr.client.solrj.SolrServerException;
+import org.apache.solr.client.solrj.embedded.JettySolrRunner;
+import org.apache.solr.client.solrj.impl.CloudSolrServer;
+import org.apache.solr.client.solrj.impl.HttpSolrServer;
+import org.apache.solr.client.solrj.request.UpdateRequest;
+import org.apache.solr.client.solrj.response.QueryResponse;
+import org.apache.solr.cloud.ChaosMonkey;
+import org.apache.solr.common.SolrDocument;
+import org.apache.solr.common.SolrDocumentList;
+import org.apache.solr.common.SolrException;
+import org.apache.solr.common.SolrInputDocument;
+import org.apache.solr.common.cloud.ClusterState;
+import org.apache.solr.common.cloud.Slice;
+import org.apache.solr.common.cloud.ZkCoreNodeProps;
+import org.apache.solr.common.cloud.ZkNodeProps;
+import org.apache.solr.common.cloud.ZkStateReader;
+import org.apache.solr.common.params.ModifiableSolrParams;
+import org.junit.After;
+import org.junit.AfterClass;
+import org.junit.Before;
+import org.junit.BeforeClass;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+/**
+ * 
+ * TODO: we should still test this works as a custom update chain as well as
+ * what we test now - the default update chain
+ * 
+ */
+@Slow
+public abstract class AbstractFullDistribZkTestBase extends AbstractDistribZkTestBase {
+  static Logger log = LoggerFactory.getLogger(AbstractFullDistribZkTestBase.class);
+  
+  @BeforeClass
+  public static void beforeFullSolrCloudTest() {
+    // shorten the log output more for this test type
+    if (formatter != null) formatter.setShorterFormat();
+  }
+  
+  public static final String SHARD1 = "shard1";
+  public static final String SHARD2 = "shard2";
+  
+  protected boolean printLayoutOnTearDown = false;
+  
+  String t1 = "a_t";
+  String i1 = "a_si";
+  String nint = "n_i";
+  String tint = "n_ti";
+  String nfloat = "n_f";
+  String tfloat = "n_tf";
+  String ndouble = "n_d";
+  String tdouble = "n_td";
+  String nlong = "n_l";
+  String tlong = "other_tl1";
+  String ndate = "n_dt";
+  String tdate = "n_tdt";
+  
+  String oddField = "oddField_s";
+  String missingField = "ignore_exception__missing_but_valid_field_t";
+  String invalidField = "ignore_exception__invalid_field_not_in_schema";
+  protected int sliceCount;
+  
+  protected volatile CloudSolrServer cloudClient;
+  
+  protected List<CloudJettyRunner> cloudJettys = new ArrayList<CloudJettyRunner>();
+  protected Map<String,List<CloudJettyRunner>> shardToJetty = new HashMap<String,List<CloudJettyRunner>>();
+  private AtomicInteger jettyIntCntr = new AtomicInteger(0);
+  protected ChaosMonkey chaosMonkey;
+  protected volatile ZkStateReader zkStateReader;
+  
+  protected Map<String,CloudJettyRunner> shardToLeaderJetty = new HashMap<String,CloudJettyRunner>();
+  
+  public static class CloudJettyRunner {
+    public JettySolrRunner jetty;
+    public String nodeName;
+    public String coreNodeName;
+    public String url;
+    public CloudSolrServerClient client;
+    public ZkNodeProps info;
+    @Override
+    public int hashCode() {
+      final int prime = 31;
+      int result = 1;
+      result = prime * result + ((url == null) ? 0 : url.hashCode());
+      return result;
+    }
+    @Override
+    public boolean equals(Object obj) {
+      if (this == obj) return true;
+      if (obj == null) return false;
+      if (getClass() != obj.getClass()) return false;
+      CloudJettyRunner other = (CloudJettyRunner) obj;
+      if (url == null) {
+        if (other.url != null) return false;
+      } else if (!url.equals(other.url)) return false;
+      return true;
+    }
+  }
+  
+  static class CloudSolrServerClient {
+    SolrServer solrClient;
+    String shardName;
+    int port;
+    public ZkNodeProps info;
+    
+    public CloudSolrServerClient() {}
+    
+    public CloudSolrServerClient(SolrServer client) {
+      this.solrClient = client;
+    }
+    
+    @Override
+    public int hashCode() {
+      final int prime = 31;
+      int result = 1;
+      result = prime * result + ((solrClient == null) ? 0 : solrClient.hashCode());
+      return result;
+    }
+    
+    @Override
+    public boolean equals(Object obj) {
+      if (this == obj) return true;
+      if (obj == null) return false;
+      if (getClass() != obj.getClass()) return false;
+      CloudSolrServerClient other = (CloudSolrServerClient) obj;
+      if (solrClient == null) {
+        if (other.solrClient != null) return false;
+      } else if (!solrClient.equals(other.solrClient)) return false;
+      return true;
+    }
+    
+  }
+  
+  @Before
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    // ignoreException(".*");
+    System.setProperty("numShards", Integer.toString(sliceCount));
+  }
+  
+  @BeforeClass
+  public static void beforeClass() {
+    System.setProperty("solrcloud.update.delay", "0");
+  }
+  
+  @AfterClass
+  public static void afterClass() {
+    System.clearProperty("solrcloud.update.delay");
+  }
+  
+  public AbstractFullDistribZkTestBase() {
+    fixShardCount = true;
+    
+    shardCount = 4;
+    sliceCount = 2;
+    // TODO: for now, turn off stress because it uses regular clients, and we
+    // need the cloud client because we kill servers
+    stress = 0;
+  }
+  
+  protected void initCloud() throws Exception {
+    if (zkStateReader == null) {
+      synchronized (this) {
+        if (zkStateReader != null) {
+          return;
+        }
+        zkStateReader = new ZkStateReader(zkServer.getZkAddress(), 10000,
+            AbstractZkTestCase.TIMEOUT);
+        
+        zkStateReader.createClusterStateWatchersAndUpdate();
+      }
+      
+      chaosMonkey = new ChaosMonkey(zkServer, zkStateReader,
+          DEFAULT_COLLECTION, shardToJetty,
+          shardToLeaderJetty);
+    }
+    
+    // wait until shards have started registering...
+    int cnt = 30;
+    while (!zkStateReader.getClusterState().getCollections()
+        .contains(DEFAULT_COLLECTION)) {
+      if (cnt == 0) {
+        throw new RuntimeException("timeout waiting for collection1 in cluster state");
+      }
+      cnt--;
+      Thread.sleep(500);
+    }
+    cnt = 30;
+    while (zkStateReader.getClusterState().getSlices(DEFAULT_COLLECTION).size() != sliceCount) {
+      if (cnt == 0) {
+        throw new RuntimeException("timeout waiting for collection shards to come up");
+      }
+      cnt--;
+      Thread.sleep(500);
+    }
+    
+    // use the distributed solrj client
+    if (cloudClient == null) {
+      synchronized (this) {
+        if (cloudClient != null) {
+          return;
+        }
+        try {
+          CloudSolrServer server = new CloudSolrServer(zkServer.getZkAddress());
+          server.setDefaultCollection(DEFAULT_COLLECTION);
+          server.getLbServer().getHttpClient().getParams()
+              .setParameter(CoreConnectionPNames.CONNECTION_TIMEOUT, 5000);
+          server.getLbServer().getHttpClient().getParams()
+              .setParameter(CoreConnectionPNames.SO_TIMEOUT, 15000);
+          cloudClient = server;
+        } catch (MalformedURLException e) {
+          throw new RuntimeException(e);
+        }
+      }
+    }
+  }
+  
+  @Override
+  protected void createServers(int numServers) throws Exception {
+    
+    System.setProperty("collection", "control_collection");
+    String numShards = System.getProperty(ZkStateReader.NUM_SHARDS_PROP);
+    System.clearProperty(ZkStateReader.NUM_SHARDS_PROP);
+    controlJetty = createJetty(new File(getSolrHome()), testDir + "/control/data",
+        "control_shard");
+    System.clearProperty("collection");
+    if(numShards != null) {
+      System.setProperty(ZkStateReader.NUM_SHARDS_PROP, numShards);
+    } 
+    controlClient = createNewSolrServer(controlJetty.getLocalPort());
+    
+    createJettys(numServers, true);
+    
+  }
+  
+  protected List<JettySolrRunner> createJettys(int numJettys) throws Exception {
+    return createJettys(numJettys, false);
+  }
+  
+
+  /**
+   * @param numJettys
+   * @param checkCreatedVsState
+   *          if true, make sure the number created (numJettys) matches the
+   *          number in the cluster state - if you add more jetties this may not
+   *          be the case
+   * @return
+   * @throws Exception
+   */
+  protected List<JettySolrRunner> createJettys(int numJettys, boolean checkCreatedVsState) throws Exception {
+    List<JettySolrRunner> jettys = new ArrayList<JettySolrRunner>();
+    List<SolrServer> clients = new ArrayList<SolrServer>();
+    StringBuilder sb = new StringBuilder();
+    for (int i = 1; i <= numJettys; i++) {
+      if (sb.length() > 0) sb.append(',');
+      JettySolrRunner j = createJetty(new File(getSolrHome()), testDir + "/jetty"
+          + this.jettyIntCntr.incrementAndGet(), null, "solrconfig.xml", null);
+      jettys.add(j);
+      SolrServer client = createNewSolrServer(j.getLocalPort());
+      clients.add(client);
+    }
+    
+    initCloud();
+    
+    this.jettys.addAll(jettys);
+    this.clients.addAll(clients);
+    
+    if (checkCreatedVsState) {
+      // now wait until we see that the number of shards in the cluster state
+      // matches what we expect
+      int numShards = getNumShards(DEFAULT_COLLECTION);
+      int retries = 0;
+      while (numShards != shardCount) {
+        numShards = getNumShards(DEFAULT_COLLECTION);
+        if (numShards == shardCount) break;
+        if (retries++ == 60) {
+          printLayoutOnTearDown = true;
+          fail("Shards in the state does not match what we set:" + numShards
+              + " vs " + shardCount);
+        }
+        Thread.sleep(500);
+      }
+
+      // also make sure we have a leader for each shard
+      for (int i = 1; i <= sliceCount; i++) {
+        zkStateReader.getLeaderProps(DEFAULT_COLLECTION, "shard" + i, 10000);
+      }
+    }
+
+    updateMappingsFromZk(this.jettys, this.clients);
+    
+    // build the shard string
+    for (int i = 1; i <= numJettys / 2; i++) {
+      JettySolrRunner j = this.jettys.get(i);
+      JettySolrRunner j2 = this.jettys.get(i + (numJettys / 2 - 1));
+      if (sb.length() > 0) sb.append(',');
+      sb.append("localhost:").append(j.getLocalPort()).append(context);
+      sb.append("|localhost:").append(j2.getLocalPort()).append(context);
+    }
+    shards = sb.toString();
+    
+    return jettys;
+  }
+
+  protected int getNumShards(String defaultCollection) {
+    Map<String,Slice> slices = this.zkStateReader.getClusterState().getSlices(defaultCollection);
+    int cnt = 0;
+    for (Map.Entry<String,Slice> entry : slices.entrySet()) {
+      cnt += entry.getValue().getShards().size();
+    }
+    
+    return cnt;
+  }
+  
+  public JettySolrRunner createJetty(String dataDir, String shardList,
+      String solrConfigOverride) throws Exception {
+    
+    JettySolrRunner jetty = new JettySolrRunner(getSolrHome(), "/solr", 0,
+        solrConfigOverride, null, false);
+    jetty.setShards(shardList);
+    jetty.setDataDir(dataDir);
+    jetty.start();
+    
+    return jetty;
+  }
+  
+  protected void updateMappingsFromZk(List<JettySolrRunner> jettys,
+      List<SolrServer> clients) throws Exception {
+    zkStateReader.updateClusterState(true);
+    cloudJettys.clear();
+    shardToJetty.clear();
+    
+    ClusterState clusterState = zkStateReader.getClusterState();
+    Map<String,Slice> slices = clusterState.getSlices(DEFAULT_COLLECTION);
+    
+    if (slices == null) {
+      throw new RuntimeException("No slices found for collection "
+          + DEFAULT_COLLECTION + " in " + clusterState.getCollections());
+    }
+    
+    List<CloudSolrServerClient> theClients = new ArrayList<CloudSolrServerClient>();
+    for (SolrServer client : clients) {
+      // find info for this client in zk 
+      nextClient:
+      // we find ou state by simply matching ports...
+      for (Map.Entry<String,Slice> slice : slices.entrySet()) {
+        Map<String,ZkNodeProps> theShards = slice.getValue().getShards();
+        for (Map.Entry<String,ZkNodeProps> shard : theShards.entrySet()) {
+          int port = new URI(((HttpSolrServer) client).getBaseURL())
+              .getPort();
+          
+          if (shard.getKey().contains(":" + port + "_")) {
+            CloudSolrServerClient csc = new CloudSolrServerClient();
+            csc.solrClient = client;
+            csc.port = port;
+            csc.shardName = shard.getValue().get(ZkStateReader.NODE_NAME_PROP);
+            csc.info = shard.getValue();
+            
+            theClients .add(csc);
+            
+            break nextClient;
+          }
+        }
+      }
+    }
+ 
+    for (JettySolrRunner jetty : jettys) {
+      int port = jetty.getLocalPort();
+      if (port == -1) {
+        throw new RuntimeException("Cannot find the port for jetty");
+      }
+      
+      nextJetty:
+      for (Map.Entry<String,Slice> slice : slices.entrySet()) {
+        Map<String,ZkNodeProps> theShards = slice.getValue().getShards();
+        for (Map.Entry<String,ZkNodeProps> shard : theShards.entrySet()) {
+          if (shard.getKey().contains(":" + port + "_")) {
+            List<CloudJettyRunner> list = shardToJetty.get(slice.getKey());
+            if (list == null) {
+              list = new ArrayList<CloudJettyRunner>();
+              shardToJetty.put(slice.getKey(), list);
+            }
+            boolean isLeader = shard.getValue().containsKey(
+                ZkStateReader.LEADER_PROP);
+            CloudJettyRunner cjr = new CloudJettyRunner();
+            cjr.jetty = jetty;
+            cjr.info = shard.getValue();
+            cjr.nodeName = shard.getValue().get(ZkStateReader.NODE_NAME_PROP);
+            cjr.coreNodeName = shard.getKey();
+            cjr.url = shard.getValue().get(ZkStateReader.BASE_URL_PROP) + "/" + shard.getValue().get(ZkStateReader.CORE_NAME_PROP);
+            cjr.client = findClientByPort(port, theClients);
+            list.add(cjr);
+            if (isLeader) {
+              shardToLeaderJetty.put(slice.getKey(), cjr);
+            }
+            cloudJettys.add(cjr);
+            break nextJetty;
+          }
+        }
+      }
+    }
+    
+    // # of jetties may not match replicas in shard here, because we don't map
+    // jetties that are not running - every shard should have at least one
+    // running jetty though
+    for (Map.Entry<String,Slice> slice : slices.entrySet()) {
+      // check that things look right
+      List<CloudJettyRunner> jetties = shardToJetty.get(slice.getKey());
+      assertNotNull("Test setup problem: We found no jetties for shard: " + slice.getKey()
+          + " just:" + shardToJetty.keySet(), jetties);
+      assertEquals(slice.getValue().getShards().size(), jetties.size());
+    }
+  }
+  
+  private CloudSolrServerClient findClientByPort(int port, List<CloudSolrServerClient> theClients) {
+    for (CloudSolrServerClient client : theClients) {
+      if (client.port == port) {
+        return client;
+      }
+    }
+    throw new IllegalArgumentException("Client with the give port does not exist:" + port);
+  }
+
+  @Override
+  protected void setDistributedParams(ModifiableSolrParams params) {
+    
+    if (r.nextBoolean()) {
+      // don't set shards, let that be figured out from the cloud state
+    } else {
+      // use shard ids rather than physical locations
+      StringBuilder sb = new StringBuilder();
+      for (int i = 0; i < sliceCount; i++) {
+        if (i > 0) sb.append(',');
+        sb.append("shard" + (i + 1));
+      }
+      params.set("shards", sb.toString());
+    }
+  }
+  
+  @Override
+  protected void indexDoc(SolrInputDocument doc) throws IOException,
+      SolrServerException {
+    controlClient.add(doc);
+    
+    // if we wanted to randomly pick a client - but sometimes they may be
+    // down...
+    
+    // boolean pick = random.nextBoolean();
+    //
+    // int which = (doc.getField(id).toString().hashCode() & 0x7fffffff) %
+    // sliceCount;
+    //
+    // if (pick && sliceCount > 1) {
+    // which = which + ((shardCount / sliceCount) *
+    // random.nextInt(sliceCount-1));
+    // }
+    //
+    // CommonsHttpSolrServer client = (CommonsHttpSolrServer)
+    // clients.get(which);
+    
+    UpdateRequest ureq = new UpdateRequest();
+    ureq.add(doc);
+    // ureq.setParam(UpdateParams.UPDATE_CHAIN, DISTRIB_UPDATE_CHAIN);
+    ureq.process(cloudClient);
+  }
+  
+  protected void index_specific(int serverNumber, Object... fields)
+      throws Exception {
+    SolrInputDocument doc = new SolrInputDocument();
+    for (int i = 0; i < fields.length; i += 2) {
+      doc.addField((String) (fields[i]), fields[i + 1]);
+    }
+    controlClient.add(doc);
+    
+    HttpSolrServer client = (HttpSolrServer) clients
+        .get(serverNumber);
+    
+    UpdateRequest ureq = new UpdateRequest();
+    ureq.add(doc);
+    // ureq.setParam("update.chain", DISTRIB_UPDATE_CHAIN);
+    ureq.process(client);
+  }
+  
+  protected void index_specific(SolrServer client, Object... fields)
+      throws Exception {
+    SolrInputDocument doc = new SolrInputDocument();
+    for (int i = 0; i < fields.length; i += 2) {
+      doc.addField((String) (fields[i]), fields[i + 1]);
+    }
+    
+    UpdateRequest ureq = new UpdateRequest();
+    ureq.add(doc);
+    // ureq.setParam("update.chain", DISTRIB_UPDATE_CHAIN);
+    ureq.process(client);
+    
+    // add to control second in case adding to shards fails
+    controlClient.add(doc);
+  }
+  
+  protected void del(String q) throws Exception {
+    controlClient.deleteByQuery(q);
+    cloudClient.deleteByQuery(q);
+
+    /***
+    for (SolrServer client : clients) {
+      UpdateRequest ureq = new UpdateRequest();
+      // ureq.setParam("update.chain", DISTRIB_UPDATE_CHAIN);
+      ureq.deleteByQuery(q).process(client);
+    }
+     ***/
+  }// serial commit...
+  
+  protected void waitForRecoveriesToFinish(boolean verbose)
+      throws Exception {
+    super.waitForRecoveriesToFinish(DEFAULT_COLLECTION, zkStateReader, verbose);
+  }
+  
+  protected void waitForRecoveriesToFinish(boolean verbose, int timeoutSeconds)
+      throws Exception {
+    super.waitForRecoveriesToFinish(DEFAULT_COLLECTION, zkStateReader, verbose, true, timeoutSeconds);
+  }
+  
+  protected void checkQueries() throws Exception {
+
+    handle.put("_version_", SKIPVAL);
+
+    query("q", "*:*", "sort", "n_tl1 desc");
+
+    handle.put("response", UNORDERED);  // get?ids=a,b,c requests are unordered
+    String ids = "987654";
+    for (int i=0; i<20; i++) {
+      query("qt","/get", "id",Integer.toString(i));
+      query("qt","/get", "ids",Integer.toString(i));
+      ids = ids + ',' + Integer.toString(i);
+      query("qt","/get", "ids",ids);
+    }
+    handle.remove("response");
+
+
+
+    // random value sort
+    for (String f : fieldNames) {
+      query("q", "*:*", "sort", f + " desc");
+      query("q", "*:*", "sort", f + " asc");
+    }
+    
+    // these queries should be exactly ordered and scores should exactly match
+    query("q", "*:*", "sort", i1 + " desc");
+    query("q", "*:*", "sort", i1 + " asc");
+    query("q", "*:*", "sort", i1 + " desc", "fl", "*,score");
+    query("q", "*:*", "sort", "n_tl1 asc", "fl", "score"); // test legacy
+                                                           // behavior -
+                                                           // "score"=="*,score"
+    query("q", "*:*", "sort", "n_tl1 desc");
+    handle.put("maxScore", SKIPVAL);
+    query("q", "{!func}" + i1);// does not expect maxScore. So if it comes
+                               // ,ignore it.
+                               // JavaBinCodec.writeSolrDocumentList()
+    // is agnostic of request params.
+    handle.remove("maxScore");
+    query("q", "{!func}" + i1, "fl", "*,score"); // even scores should match
+                                                 // exactly here
+    
+    handle.put("highlighting", UNORDERED);
+    handle.put("response", UNORDERED);
+    
+    handle.put("maxScore", SKIPVAL);
+    query("q", "quick");
+    query("q", "all", "fl", "id", "start", "0");
+    query("q", "all", "fl", "foofoofoo", "start", "0"); // no fields in returned
+                                                        // docs
+    query("q", "all", "fl", "id", "start", "100");
+    
+    handle.put("score", SKIPVAL);
+    query("q", "quick", "fl", "*,score");
+    query("q", "all", "fl", "*,score", "start", "1");
+    query("q", "all", "fl", "*,score", "start", "100");
+    
+    query("q", "now their fox sat had put", "fl", "*,score", "hl", "true",
+        "hl.fl", t1);
+    
+    query("q", "now their fox sat had put", "fl", "foofoofoo", "hl", "true",
+        "hl.fl", t1);
+    
+    query("q", "matchesnothing", "fl", "*,score");
+    
+    query("q", "*:*", "rows", 100, "facet", "true", "facet.field", t1);
+    query("q", "*:*", "rows", 100, "facet", "true", "facet.field", t1,
+        "facet.limit", -1, "facet.sort", "count");
+    query("q", "*:*", "rows", 100, "facet", "true", "facet.field", t1,
+        "facet.limit", -1, "facet.sort", "count", "facet.mincount", 2);
+    query("q", "*:*", "rows", 100, "facet", "true", "facet.field", t1,
+        "facet.limit", -1, "facet.sort", "index");
+    query("q", "*:*", "rows", 100, "facet", "true", "facet.field", t1,
+        "facet.limit", -1, "facet.sort", "index", "facet.mincount", 2);
+    query("q", "*:*", "rows", 100, "facet", "true", "facet.field", t1,
+        "facet.limit", 1);
+    query("q", "*:*", "rows", 100, "facet", "true", "facet.query", "quick",
+        "facet.query", "all", "facet.query", "*:*");
+    query("q", "*:*", "rows", 100, "facet", "true", "facet.field", t1,
+        "facet.offset", 1);
+    query("q", "*:*", "rows", 100, "facet", "true", "facet.field", t1,
+        "facet.mincount", 2);
+    
+    // test faceting multiple things at once
+    query("q", "*:*", "rows", 100, "facet", "true", "facet.query", "quick",
+        "facet.query", "all", "facet.query", "*:*", "facet.field", t1);
+    
+    // test filter tagging, facet exclusion, and naming (multi-select facet
+    // support)
+    query("q", "*:*", "rows", 100, "facet", "true", "facet.query",
+        "{!key=myquick}quick", "facet.query", "{!key=myall ex=a}all",
+        "facet.query", "*:*", "facet.field", "{!key=mykey ex=a}" + t1,
+        "facet.field", "{!key=other ex=b}" + t1, "facet.field",
+        "{!key=again ex=a,b}" + t1, "facet.field", t1, "fq",
+        "{!tag=a}id:[1 TO 7]", "fq", "{!tag=b}id:[3 TO 9]");
+    query("q", "*:*", "facet", "true", "facet.field",
+        "{!ex=t1}SubjectTerms_mfacet", "fq",
+        "{!tag=t1}SubjectTerms_mfacet:(test 1)", "facet.limit", "10",
+        "facet.mincount", "1");
+    
+    // test field that is valid in schema but missing in all shards
+    query("q", "*:*", "rows", 100, "facet", "true", "facet.field",
+        missingField, "facet.mincount", 2);
+    // test field that is valid in schema and missing in some shards
+    query("q", "*:*", "rows", 100, "facet", "true", "facet.field", oddField,
+        "facet.mincount", 2);
+    
+    query("q", "*:*", "sort", i1 + " desc", "stats", "true", "stats.field", i1);
+    
+    // Try to get better coverage for refinement queries by turning off over
+    // requesting.
+    // This makes it much more likely that we may not get the top facet values
+    // and hence
+    // we turn of that checking.
+    handle.put("facet_fields", SKIPVAL);
+    query("q", "*:*", "rows", 0, "facet", "true", "facet.field", t1,
+        "facet.limit", 5, "facet.shard.limit", 5);
+    // check a complex key name
+    query("q", "*:*", "rows", 0, "facet", "true", "facet.field",
+        "{!key='a b/c \\' \\} foo'}" + t1, "facet.limit", 5,
+        "facet.shard.limit", 5);
+    handle.remove("facet_fields");
+    
+    query("q", "*:*", "sort", "n_tl1 desc");
+    
+    // index the same document to two shards and make sure things
+    // don't blow up.
+    // assumes first n clients are first n shards
+    if (clients.size() >= 2) {
+      index(id, 100, i1, 107, t1, "oh no, a duplicate!");
+      for (int i = 0; i < shardCount; i++) {
+        index_specific(i, id, 100, i1, 107, t1, "oh no, a duplicate!");
+      }
+      commit();
+      query("q", "duplicate", "hl", "true", "hl.fl", t1);
+      query("q", "fox duplicate horses", "hl", "true", "hl.fl", t1);
+      query("q", "*:*", "rows", 100);
+    }
+  }
+  
+  protected void indexAbunchOfDocs() throws Exception {
+    indexr(id, 2, i1, 50, tlong, 50, t1, "to come to the aid of their country.");
+    indexr(id, 3, i1, 2, tlong, 2, t1, "how now brown cow");
+    indexr(id, 4, i1, -100, tlong, 101, t1,
+        "the quick fox jumped over the lazy dog");
+    indexr(id, 5, i1, 500, tlong, 500, t1,
+        "the quick fox jumped way over the lazy dog");
+    indexr(id, 6, i1, -600, tlong, 600, t1, "humpty dumpy sat on a wall");
+    indexr(id, 7, i1, 123, tlong, 123, t1, "humpty dumpy had a great fall");
+    indexr(id, 8, i1, 876, tlong, 876, t1,
+        "all the kings horses and all the kings men");
+    indexr(id, 9, i1, 7, tlong, 7, t1, "couldn't put humpty together again");
+    indexr(id, 10, i1, 4321, tlong, 4321, t1, "this too shall pass");
+    indexr(id, 11, i1, -987, tlong, 987, t1,
+        "An eye for eye only ends up making the whole world blind.");
+    indexr(id, 12, i1, 379, tlong, 379, t1,
+        "Great works are performed, not by strength, but by perseverance.");
+    indexr(id, 13, i1, 232, tlong, 232, t1, "no eggs on wall, lesson learned",
+        oddField, "odd man out");
+    
+    indexr(id, 14, "SubjectTerms_mfacet", new String[] {"mathematical models",
+        "mathematical analysis"});
+    indexr(id, 15, "SubjectTerms_mfacet", new String[] {"test 1", "test 2",
+        "test3"});
+    indexr(id, 16, "SubjectTerms_mfacet", new String[] {"test 1", "test 2",
+        "test3"});
+    String[] vals = new String[100];
+    for (int i = 0; i < 100; i++) {
+      vals[i] = "test " + i;
+    }
+    indexr(id, 17, "SubjectTerms_mfacet", vals);
+    
+    for (int i = 100; i < 150; i++) {
+      indexr(id, i);
+    }
+  }
+  
+  protected void checkShardConsistency(String shard) throws Exception {
+    checkShardConsistency(shard, false);
+  }
+  
+  protected String checkShardConsistency(String shard, boolean verbose)
+      throws Exception {
+    
+    List<CloudJettyRunner> solrJetties = shardToJetty.get(shard);
+    if (solrJetties == null) {
+      throw new RuntimeException("shard not found:" + shard + " keys:"
+          + shardToJetty.keySet());
+    }
+    long num = -1;
+    long lastNum = -1;
+    String failMessage = null;
+    if (verbose) System.err.println("check const of " + shard);
+    int cnt = 0;
+    
+    assertEquals(
+        "The client count does not match up with the shard count for slice:"
+            + shard,
+        zkStateReader.getClusterState().getSlice(DEFAULT_COLLECTION, shard)
+            .getShards().size(), solrJetties.size());
+
+    SolrServer lastClient = null;
+    for (CloudJettyRunner cjetty : solrJetties) {
+      ZkNodeProps props = cjetty.info;
+      if (verbose) System.err.println("client" + cnt++);
+      if (verbose) System.err.println("PROPS:" + props);
+      
+      try {
+        SolrQuery query = new SolrQuery("*:*");
+        query.set("distrib", false);
+        num = cjetty.client.solrClient.query(query).getResults().getNumFound();
+      } catch (SolrServerException e) {
+        if (verbose) System.err.println("error contacting client: "
+            + e.getMessage() + "\n");
+        continue;
+      } catch (SolrException e) {
+        if (verbose) System.err.println("error contacting client: "
+            + e.getMessage() + "\n");
+        continue;
+      }
+      
+      boolean live = false;
+      String nodeName = props.get(ZkStateReader.NODE_NAME_PROP);
+      if (zkStateReader.getClusterState().liveNodesContain(nodeName)) {
+        live = true;
+      }
+      if (verbose) System.err.println(" live:" + live);
+      
+      if (verbose) System.err.println(" num:" + num + "\n");
+      
+      boolean active = props.get(ZkStateReader.STATE_PROP).equals(
+          ZkStateReader.ACTIVE);
+      if (active && live) {
+        if (lastNum > -1 && lastNum != num && failMessage == null) {
+          failMessage = shard + " is not consistent.  Got " + lastNum + " from " + lastClient + "lastClient"
+              + " and got " + num + " from " + cjetty.url;
+
+          if (verbose || true) {
+            System.err.println("######" + failMessage);
+            SolrQuery query = new SolrQuery("*:*");
+            query.set("distrib", false);
+            query.set("fl","id,_version_");
+            query.set("rows","1000");
+            query.set("sort","id asc");
+
+            SolrDocumentList lst1 = lastClient.query(query).getResults();
+            SolrDocumentList lst2 = cjetty.client.solrClient.query(query).getResults();
+
+            showDiff(lst1, lst2, lastClient.toString(), cjetty.client.solrClient.toString());
+          }
+
+        }
+        lastNum = num;
+        lastClient = cjetty.client.solrClient;
+      }
+    }
+    return failMessage;
+    
+  }
+  
+  void showDiff(SolrDocumentList a, SolrDocumentList b, String aName, String bName) {
+    System.err.println("######"+aName+ ": " + a);
+    System.err.println("######"+bName+ ": " + b);
+    System.err.println("###### sizes=" + a.size() + "," + b.size());
+    
+    Set<Map> setA = new HashSet<Map>();
+    for (SolrDocument sdoc : a) {
+      setA.add(new HashMap(sdoc));
+    }
+
+    Set<Map> setB = new HashSet<Map>();
+    for (SolrDocument sdoc : b) {
+      setB.add(new HashMap(sdoc));
+    }
+
+    Set<Map> onlyInA = new HashSet<Map>(setA);
+    onlyInA.removeAll(setB);
+    Set<Map> onlyInB = new HashSet<Map>(setB);
+    onlyInB.removeAll(setA);
+
+    if (onlyInA.size() > 0) {
+      System.err.println("###### Only in " + aName + ": " + onlyInA);
+    }
+    if (onlyInB.size() > 0) {
+      System.err.println("###### Only in " + bName + ": " + onlyInB);
+    }
+  }
+  
+  protected void checkShardConsistency() throws Exception {
+    checkShardConsistency(true, false);
+  }
+  
+  protected void checkShardConsistency(boolean checkVsControl, boolean verbose)
+      throws Exception {
+    long docs = controlClient.query(new SolrQuery("*:*")).getResults()
+        .getNumFound();
+    if (verbose) System.err.println("Control Docs:" + docs);
+    
+    updateMappingsFromZk(jettys, clients);
+    
+    Set<String> theShards = shardToJetty.keySet();
+    String failMessage = null;
+    for (String shard : theShards) {
+      String shardFailMessage = checkShardConsistency(shard, verbose);
+      if (shardFailMessage != null && failMessage == null) {
+        failMessage = shardFailMessage;
+      }
+    }
+    
+    if (failMessage != null) {
+      fail(failMessage);
+    }
+    
+    if (checkVsControl) {
+      // now check that the right # are on each shard
+      theShards = shardToJetty.keySet();
+      int cnt = 0;
+      for (String s : theShards) {
+        int times = shardToJetty.get(s).size();
+        for (int i = 0; i < times; i++) {
+          try {
+            CloudJettyRunner cjetty = shardToJetty.get(s).get(i);
+            ZkNodeProps props = cjetty.info;
+            SolrServer client = cjetty.client.solrClient;
+            boolean active = props.get(ZkStateReader.STATE_PROP).equals(
+                ZkStateReader.ACTIVE);
+            if (active) {
+              SolrQuery query = new SolrQuery("*:*");
+              query.set("distrib", false);
+              long results = client.query(query).getResults().getNumFound();
+              if (verbose) System.err.println(new ZkCoreNodeProps(props)
+                  .getCoreUrl() + " : " + results);
+              if (verbose) System.err.println("shard:"
+                  + props.get(ZkStateReader.SHARD_ID_PROP));
+              cnt += results;
+              break;
+            }
+          } catch (SolrServerException e) {
+            // if we have a problem, try the next one
+            if (i == times - 1) {
+              throw e;
+            }
+          }
+        }
+      }
+      
+      SolrQuery q = new SolrQuery("*:*");
+      long cloudClientDocs = cloudClient.query(q).getResults().getNumFound();
+      assertEquals(
+          "adding up the # of docs on each shard does not match the control - cloud client returns:"
+              + cloudClientDocs, docs, cnt);
+    }
+  }
+  
+  protected SolrServer getClient(String nodeName) {
+    for (CloudJettyRunner cjetty : cloudJettys) {
+      CloudSolrServerClient client = cjetty.client;
+      if (client.shardName.equals(nodeName)) {
+        return client.solrClient;
+      }
+    }
+    return null;
+  }
+  
+  protected void assertDocCounts(boolean verbose) throws Exception {
+    // TODO: as we create the clients, we should build a map from shard to
+    // node/client
+    // and node/client to shard?
+    if (verbose) System.err.println("control docs:"
+        + controlClient.query(new SolrQuery("*:*")).getResults().getNumFound()
+        + "\n\n");
+    long controlCount = controlClient.query(new SolrQuery("*:*")).getResults()
+        .getNumFound();
+    
+    // do some really inefficient mapping...
+    ZkStateReader zk = new ZkStateReader(zkServer.getZkAddress(), 10000,
+        AbstractZkTestCase.TIMEOUT);
+    Map<String,Slice> slices = null;
+    ClusterState clusterState;
+    try {
+      zk.createClusterStateWatchersAndUpdate();
+      clusterState = zk.getClusterState();
+      slices = clusterState.getSlices(DEFAULT_COLLECTION);
+    } finally {
+      zk.close();
+    }
+    
+    if (slices == null) {
+      throw new RuntimeException("Could not find collection "
+          + DEFAULT_COLLECTION + " in " + clusterState.getCollections());
+    }
+    
+    for (CloudJettyRunner cjetty : cloudJettys) {
+      CloudSolrServerClient client = cjetty.client;
+      for (Map.Entry<String,Slice> slice : slices.entrySet()) {
+        Map<String,ZkNodeProps> theShards = slice.getValue().getShards();
+        for (Map.Entry<String,ZkNodeProps> shard : theShards.entrySet()) {
+          String shardName = new URI(
+              ((HttpSolrServer) client.solrClient).getBaseURL()).getPort()
+              + "_solr_";
+          if (verbose && shard.getKey().endsWith(shardName)) {
+            System.err.println("shard:" + slice.getKey());
+            System.err.println(shard.getValue());
+          }
+        }
+      }
+      
+      long count = 0;
+      String currentState = cjetty.info.get(ZkStateReader.STATE_PROP);
+      if (currentState != null
+          && currentState.equals(ZkStateReader.ACTIVE)
+          && zkStateReader.getClusterState().liveNodesContain(
+              cjetty.info.get(ZkStateReader.NODE_NAME_PROP))) {
+        SolrQuery query = new SolrQuery("*:*");
+        query.set("distrib", false);
+        count = client.solrClient.query(query).getResults().getNumFound();
+      }
+      
+      if (verbose) System.err.println("client docs:" + count + "\n\n");
+    }
+    if (verbose) System.err.println("control docs:"
+        + controlClient.query(new SolrQuery("*:*")).getResults().getNumFound()
+        + "\n\n");
+    SolrQuery query = new SolrQuery("*:*");
+    assertEquals("Doc Counts do not add up", controlCount,
+        cloudClient.query(query).getResults().getNumFound());
+  }
+  
+  @Override
+  protected QueryResponse queryServer(ModifiableSolrParams params)
+      throws SolrServerException {
+    
+    if (r.nextBoolean()) params.set("collection", DEFAULT_COLLECTION);
+    
+    QueryResponse rsp = cloudClient.query(params);
+    return rsp;
+  }
+  
+  abstract class StopableThread extends Thread {
+    public StopableThread(String name) {
+      super(name);
+    }
+    public abstract void safeStop();
+  }
+  
+  class StopableIndexingThread extends StopableThread {
+    private volatile boolean stop = false;
+    protected final int startI;
+    protected final List<Integer> deletes = new ArrayList<Integer>();
+    protected final AtomicInteger fails = new AtomicInteger();
+    protected boolean doDeletes;
+    
+    public StopableIndexingThread(int startI, boolean doDeletes) {
+      super("StopableIndexingThread");
+      this.startI = startI;
+      this.doDeletes = doDeletes;
+      setDaemon(true);
+    }
+    
+    @Override
+    public void run() {
+      int i = startI;
+      int numDeletes = 0;
+      int numAdds = 0;
+      
+      while (true && !stop) {
+        ++i;
+        
+        if (doDeletes && random().nextBoolean() && deletes.size() > 0) {
+          Integer delete = deletes.remove(0);
+          try {
+            numDeletes++;
+            controlClient.deleteById(Integer.toString(delete));
+            cloudClient.deleteById(Integer.toString(delete));
+          } catch (Exception e) {
+            System.err.println("REQUEST FAILED:");
+            e.printStackTrace();
+            if (e instanceof SolrServerException) {
+              System.err.println("ROOT CAUSE:");
+              ((SolrServerException) e).getRootCause().printStackTrace();
+            }
+            fails.incrementAndGet();
+          }
+        }
+        
+        try {
+          numAdds++;
+          indexr(id, i, i1, 50, tlong, 50, t1,
+              "to come to the aid of their country.");
+        } catch (Exception e) {
+          System.err.println("REQUEST FAILED:");
+          e.printStackTrace();
+          if (e instanceof SolrServerException) {
+            System.err.println("ROOT CAUSE:");
+            ((SolrServerException) e).getRootCause().printStackTrace();
+          }
+          fails.incrementAndGet();
+        }
+        
+        if (doDeletes && random().nextBoolean()) {
+          deletes.add(i);
+        }
+        
+      }
+      
+      System.err.println("added docs:" + numAdds + " with " + fails + " fails"
+          + " deletes:" + numDeletes);
+    }
+    
+    public void safeStop() {
+      stop = true;
+    }
+    
+    public int getFails() {
+      return fails.get();
+    }
+    
+  };
+  
+  class StopableSearchThread extends StopableThread {
+    private volatile boolean stop = false;
+    protected final AtomicInteger fails = new AtomicInteger();
+    private String[] QUERIES = new String[] {"to come","their country","aid","co*"};
+    
+    public StopableSearchThread() {
+      super("StopableSearchThread");
+      setDaemon(true);
+    }
+    
+    @Override
+    public void run() {
+      Random random = random();
+      int numSearches = 0;
+      
+      while (true && !stop) {
+        numSearches++;
+        try {
+          //to come to the aid of their country.
+          cloudClient.query(new SolrQuery(QUERIES[random.nextInt(QUERIES.length)]));
+        } catch (Exception e) {
+          System.err.println("QUERY REQUEST FAILED:");
+          e.printStackTrace();
+          if (e instanceof SolrServerException) {
+            System.err.println("ROOT CAUSE:");
+            ((SolrServerException) e).getRootCause().printStackTrace();
+          }
+          fails.incrementAndGet();
+        }
+        try {
+          Thread.sleep(random.nextInt(4000) + 300);
+        } catch (InterruptedException e) {
+          Thread.currentThread().interrupt();
+        }
+      }
+      
+      System.err.println("num searches done:" + numSearches + " with " + fails + " fails");
+    }
+    
+    public void safeStop() {
+      stop = true;
+    }
+    
+    public int getFails() {
+      return fails.get();
+    }
+    
+  };
+  
+  public void waitForThingsToLevelOut(int waitForRecTimeSeconds) throws Exception {
+    log.info("Wait for recoveries to finish - wait " + waitForRecTimeSeconds + " for each attempt");
+    int cnt = 0;
+    boolean retry = false;
+    do {
+      waitForRecoveriesToFinish(VERBOSE, waitForRecTimeSeconds);
+      
+      try {
+        commit();
+      } catch (Exception e) {
+        // we don't care if this commit fails on some nodes
+      }
+      
+      updateMappingsFromZk(jettys, clients);
+      
+      Set<String> theShards = shardToJetty.keySet();
+      String failMessage = null;
+      for (String shard : theShards) {
+        failMessage = checkShardConsistency(shard, false);
+      }
+      
+      if (failMessage != null) {
+        retry  = true;
+      }
+      cnt++;
+      if (cnt > 2) break;
+      Thread.sleep(4000);
+    } while (retry);
+  }
+  
+  @Override
+  @After
+  public void tearDown() throws Exception {
+    if (VERBOSE || printLayoutOnTearDown) {
+      super.printLayout();
+    }
+    ((HttpSolrServer) controlClient).shutdown();
+    if (cloudClient != null) {
+      cloudClient.shutdown();
+    }
+    if (zkStateReader != null) {
+      zkStateReader.close();
+    }
+    super.tearDown();
+    
+    System.clearProperty("zkHost");
+    System.clearProperty("numShards");
+  }
+  
+  protected void commit() throws Exception {
+    controlClient.commit();
+    cloudClient.commit();
+  }
+  
+  protected void destroyServers() throws Exception {
+    ChaosMonkey.stop(controlJetty);
+    for (JettySolrRunner jetty : jettys) {
+      try {
+        ChaosMonkey.stop(jetty);
+      } catch (Exception e) {
+        log.error("", e);
+      }
+    }
+    clients.clear();
+    jettys.clear();
+  }
+  
+  protected SolrServer createNewSolrServer(int port) {
+    try {
+      // setup the server...
+      String url = "http://localhost:" + port + context + "/"
+          + DEFAULT_COLLECTION;
+      HttpSolrServer s = new HttpSolrServer(url);
+      s.setConnectionTimeout(DEFAULT_CONNECTION_TIMEOUT);
+      s.setSoTimeout(20000);
+      s.setDefaultMaxConnectionsPerHost(100);
+      s.setMaxTotalConnections(100);
+      return s;
+    } catch (Exception ex) {
+      throw new RuntimeException(ex);
+    }
+  }
+  
+  protected void waitToSeeNotLive(ZkStateReader zkStateReader,
+      CloudJettyRunner cjetty) throws InterruptedException {
+    int tries = 0;
+    while (zkStateReader.getClusterState()
+        .liveNodesContain(cjetty.info.get(ZkStateReader.NODE_NAME_PROP))) {
+      if (tries++ == 120) {
+        fail("Shard still reported as live in zk");
+      }
+      Thread.sleep(1000);
+    }
+  }
+}
diff --git a/solr/test-framework/src/java/org/apache/solr/cloud/AbstractZkTestCase.java b/solr/test-framework/src/java/org/apache/solr/cloud/AbstractZkTestCase.java
new file mode 100644
index 0000000..f248ad7
--- /dev/null
+++ b/solr/test-framework/src/java/org/apache/solr/cloud/AbstractZkTestCase.java
@@ -0,0 +1,179 @@
+package org.apache.solr.cloud;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.File;
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.solr.SolrTestCaseJ4;
+import org.apache.solr.common.cloud.SolrZkClient;
+import org.apache.solr.common.cloud.ZkNodeProps;
+import org.apache.solr.common.cloud.ZkStateReader;
+import org.apache.zookeeper.CreateMode;
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+/**
+ * Base test class for ZooKeeper tests.
+ */
+public abstract class AbstractZkTestCase extends SolrTestCaseJ4 {
+
+  static final int TIMEOUT = 10000;
+
+  private static final boolean DEBUG = false;
+
+  protected static Logger log = LoggerFactory
+      .getLogger(AbstractZkTestCase.class);
+
+  
+  public static File SOLRHOME;
+  static {
+    try {
+      SOLRHOME = new File(TEST_HOME());
+    } catch (RuntimeException e) {
+      log.warn("TEST_HOME() does not exist - solrj test?");
+      // solrj tests not working with TEST_HOME()
+      // must override getSolrHome
+    }
+  }
+  
+  protected static ZkTestServer zkServer;
+
+  protected static String zkDir;
+
+
+  @BeforeClass
+  public static void azt_beforeClass() throws Exception {
+    System.out.println("azt beforeclass");
+    createTempDir();
+    zkDir = dataDir.getAbsolutePath() + File.separator
+        + "zookeeper/server1/data";
+    zkServer = new ZkTestServer(zkDir);
+    zkServer.run();
+    
+    System.setProperty("solrcloud.skip.autorecovery", "true");
+    System.setProperty("zkHost", zkServer.getZkAddress());
+    System.setProperty("jetty.port", "0000");
+    
+    buildZooKeeper(zkServer.getZkHost(), zkServer.getZkAddress(), SOLRHOME,
+        "solrconfig.xml", "schema.xml");
+    
+    initCore("solrconfig.xml", "schema.xml");
+  }
+
+  static void buildZooKeeper(String zkHost, String zkAddress, String config,
+      String schema) throws Exception {
+    buildZooKeeper(zkHost, zkAddress, SOLRHOME, config, schema);
+  }
+  
+  // static to share with distrib test
+  static void buildZooKeeper(String zkHost, String zkAddress, File solrhome, String config,
+      String schema) throws Exception {
+    SolrZkClient zkClient = new SolrZkClient(zkHost, AbstractZkTestCase.TIMEOUT);
+    zkClient.makePath("/solr", false, true);
+    zkClient.close();
+
+    zkClient = new SolrZkClient(zkAddress, AbstractZkTestCase.TIMEOUT);
+
+    Map<String,String> props = new HashMap<String,String>();
+    props.put("configName", "conf1");
+    final ZkNodeProps zkProps = new ZkNodeProps(props);
+    
+    zkClient.makePath("/collections/collection1", ZkStateReader.toJSON(zkProps), CreateMode.PERSISTENT, true);
+    zkClient.makePath("/collections/collection1/shards", CreateMode.PERSISTENT, true);
+    zkClient.makePath("/collections/control_collection", ZkStateReader.toJSON(zkProps), CreateMode.PERSISTENT, true);
+    zkClient.makePath("/collections/control_collection/shards", CreateMode.PERSISTENT, true);
+
+    putConfig(zkClient, solrhome, config);
+    putConfig(zkClient, solrhome, schema);
+    putConfig(zkClient, solrhome, "solrconfig.xml");
+    putConfig(zkClient, solrhome, "stopwords.txt");
+    putConfig(zkClient, solrhome, "protwords.txt");
+    putConfig(zkClient, solrhome, "currency.xml");
+    putConfig(zkClient, solrhome, "open-exchange-rates.json");
+    putConfig(zkClient, solrhome, "mapping-ISOLatin1Accent.txt");
+    putConfig(zkClient, solrhome, "old_synonyms.txt");
+    putConfig(zkClient, solrhome, "synonyms.txt");
+    
+    zkClient.close();
+  }
+
+  private static void putConfig(SolrZkClient zkClient, File solrhome, final String name)
+      throws Exception {
+    String path = "/configs/conf1/" + name;
+    File file = new File(solrhome, "collection1"
+        + File.separator + "conf" + File.separator + name);
+    if (!file.exists()) {
+      log.info("skipping " + file.getAbsolutePath() + " because it doesn't exist");
+      return;
+    }
+    
+    log.info("put " + file.getAbsolutePath() + " to " + path);
+    zkClient.makePath(path, file, false, true);  
+  }
+
+  @Override
+  public void tearDown() throws Exception {
+    if (DEBUG) {
+      printLayout(zkServer.getZkHost());
+    }
+
+    super.tearDown();
+  }
+  
+  @AfterClass
+  public static void azt_afterClass() throws Exception {
+    System.clearProperty("zkHost");
+    System.clearProperty("solr.test.sys.prop1");
+    System.clearProperty("solr.test.sys.prop2");
+    System.clearProperty("solrcloud.skip.autorecovery");
+    System.clearProperty("jetty.port");
+
+    zkServer.shutdown();
+
+    // wait just a bit for any zk client threads to outlast timeout
+    Thread.sleep(2000);
+  }
+
+  protected void printLayout(String zkHost) throws Exception {
+    SolrZkClient zkClient = new SolrZkClient(zkHost, AbstractZkTestCase.TIMEOUT);
+    zkClient.printLayoutToStdOut();
+    zkClient.close();
+  }
+
+  public static void makeSolrZkNode(String zkHost) throws Exception {
+    SolrZkClient zkClient = new SolrZkClient(zkHost, TIMEOUT);
+    zkClient.makePath("/solr", false, true);
+    zkClient.close();
+  }
+  
+  public static void tryCleanSolrZkNode(String zkHost) throws Exception {
+    tryCleanPath(zkHost, "/solr");
+  }
+  
+  static void tryCleanPath(String zkHost, String path) throws Exception {
+    SolrZkClient zkClient = new SolrZkClient(zkHost, TIMEOUT);
+    if (zkClient.exists(path, true)) {
+      zkClient.clean(path);
+    }
+    zkClient.close();
+  }
+}
diff --git a/solr/test-framework/src/java/org/apache/solr/cloud/ChaosMonkey.java b/solr/test-framework/src/java/org/apache/solr/cloud/ChaosMonkey.java
new file mode 100644
index 0000000..3dc8292
--- /dev/null
+++ b/solr/test-framework/src/java/org/apache/solr/cloud/ChaosMonkey.java
@@ -0,0 +1,458 @@
+package org.apache.solr.cloud;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.net.BindException;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Map;
+import java.util.Random;
+import java.util.concurrent.atomic.AtomicInteger;
+
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.solr.client.solrj.SolrServer;
+import org.apache.solr.client.solrj.embedded.JettySolrRunner;
+import org.apache.solr.cloud.AbstractFullDistribZkTestBase.CloudJettyRunner;
+import org.apache.solr.common.cloud.Slice;
+import org.apache.solr.common.cloud.SolrZkClient;
+import org.apache.solr.common.cloud.ZkNodeProps;
+import org.apache.solr.common.cloud.ZkStateReader;
+import org.apache.solr.core.CoreContainer;
+import org.apache.solr.servlet.SolrDispatchFilter;
+import org.apache.zookeeper.KeeperException;
+import org.eclipse.jetty.servlet.FilterHolder;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+/**
+ * The monkey can stop random or specific jetties used with SolrCloud.
+ * 
+ * It can also run in a background thread and start and stop jetties
+ * randomly.
+ *
+ */
+public class ChaosMonkey {
+  private static Logger log = LoggerFactory.getLogger(ChaosMonkey.class);
+  
+  private static final int CONLOSS_PERCENT = 3; //30%
+  private static final int EXPIRE_PERCENT = 4; //40%
+  private Map<String,List<CloudJettyRunner>> shardToJetty;
+  
+  private ZkTestServer zkServer;
+  private ZkStateReader zkStateReader;
+  private String collection;
+  private volatile boolean stop = false;
+  private AtomicInteger stops = new AtomicInteger();
+  private AtomicInteger starts = new AtomicInteger();
+  private AtomicInteger expires = new AtomicInteger();
+  private AtomicInteger connloss = new AtomicInteger();
+  
+  private Map<String,List<SolrServer>> shardToClient;
+  private boolean expireSessions;
+  private boolean causeConnectionLoss;
+  private boolean aggressivelyKillLeaders;
+  private Map<String,CloudJettyRunner> shardToLeaderJetty;
+  private long startTime;
+  
+  public ChaosMonkey(ZkTestServer zkServer, ZkStateReader zkStateReader,
+      String collection, Map<String,List<CloudJettyRunner>> shardToJetty,
+      Map<String,CloudJettyRunner> shardToLeaderJetty) {
+    this.shardToJetty = shardToJetty;
+    this.shardToLeaderJetty = shardToLeaderJetty;
+    this.zkServer = zkServer;
+    this.zkStateReader = zkStateReader;
+    this.collection = collection;
+    Random random = LuceneTestCase.random();
+    expireSessions = random.nextBoolean();
+    causeConnectionLoss = random.nextBoolean();
+    monkeyLog("init - expire sessions:" + expireSessions
+        + " cause connection loss:" + causeConnectionLoss);
+  }
+  
+  public void expireSession(JettySolrRunner jetty) {
+    monkeyLog("expire session for " + jetty.getLocalPort() + " !");
+    SolrDispatchFilter solrDispatchFilter = (SolrDispatchFilter) jetty.getDispatchFilter().getFilter();
+    if (solrDispatchFilter != null) {
+      CoreContainer cores = solrDispatchFilter.getCores();
+      if (cores != null) {
+        long sessionId = cores.getZkController().getZkClient().getSolrZooKeeper().getSessionId();
+        zkServer.expire(sessionId);
+      }
+    }
+  }
+  
+  public void expireRandomSession() throws KeeperException, InterruptedException {
+    String sliceName = getRandomSlice();
+    
+    CloudJettyRunner jetty = getRandomJetty(sliceName, aggressivelyKillLeaders);
+    if (jetty != null) {
+      expireSession(jetty.jetty);
+      expires.incrementAndGet();
+    }
+  }
+  
+  public void randomConnectionLoss() throws KeeperException, InterruptedException {
+    monkeyLog("cause connection loss!");
+    
+    String sliceName = getRandomSlice();
+    CloudJettyRunner jetty = getRandomJetty(sliceName, aggressivelyKillLeaders);
+    if (jetty != null) {
+      causeConnectionLoss(jetty.jetty);
+      connloss.incrementAndGet();
+    }
+  }
+  
+  private void causeConnectionLoss(JettySolrRunner jetty) {
+    SolrDispatchFilter solrDispatchFilter = (SolrDispatchFilter) jetty
+        .getDispatchFilter().getFilter();
+    if (solrDispatchFilter != null) {
+      CoreContainer cores = solrDispatchFilter.getCores();
+      if (cores != null) {
+        SolrZkClient zkClient = cores.getZkController().getZkClient();
+        // must be at least double tick time...
+        zkClient.getSolrZooKeeper().pauseCnxn(ZkTestServer.TICK_TIME * 2);
+      }
+    }
+  }
+
+  public CloudJettyRunner stopShard(String slice, int index) throws Exception {
+    CloudJettyRunner cjetty = shardToJetty.get(slice).get(index);
+    stopJetty(cjetty);
+    return cjetty;
+  }
+
+  public void stopJetty(CloudJettyRunner cjetty) throws Exception {
+    stop(cjetty.jetty);
+    stops.incrementAndGet();
+  }
+
+  public void killJetty(CloudJettyRunner cjetty) throws Exception {
+    kill(cjetty);
+    stops.incrementAndGet();
+  }
+  
+  public void stopJetty(JettySolrRunner jetty) throws Exception {
+    stops.incrementAndGet();
+    stopJettySolrRunner(jetty);
+  }
+  
+  private static void stopJettySolrRunner(JettySolrRunner jetty) throws Exception {
+    
+    monkeyLog("stop shard! " + jetty.getLocalPort());
+    // get a clean shutdown so that no dirs are left open...
+    FilterHolder fh = jetty.getDispatchFilter();
+    if (fh != null) {
+      SolrDispatchFilter sdf = (SolrDispatchFilter) fh.getFilter();
+      if (sdf != null) {
+        sdf.destroy();
+      }
+    }
+    jetty.stop();
+    
+    if (!jetty.isStopped()) {
+      throw new RuntimeException("could not stop jetty");
+    }
+  }
+  
+  public static void kill(CloudJettyRunner cjetty) throws Exception {
+    JettySolrRunner jetty = cjetty.jetty;
+    monkeyLog("kill shard! " + jetty.getLocalPort());
+    FilterHolder fh = jetty.getDispatchFilter();
+    SolrDispatchFilter sdf = null;
+    if (fh != null) {
+      sdf = (SolrDispatchFilter) fh.getFilter();
+    }
+    jetty.stop();
+    
+    if (sdf != null) {
+      sdf.destroy();
+    }
+    
+    if (!jetty.isStopped()) {
+      throw new RuntimeException("could not kill jetty");
+    }
+  }
+  
+  public void stopShard(String slice) throws Exception {
+    List<CloudJettyRunner> jetties = shardToJetty.get(slice);
+    for (CloudJettyRunner jetty : jetties) {
+      stopJetty(jetty);
+    }
+  }
+  
+  public void stopShardExcept(String slice, String shardName) throws Exception {
+    List<CloudJettyRunner> jetties = shardToJetty.get(slice);
+    for (CloudJettyRunner jetty : jetties) {
+      if (!jetty.nodeName.equals(shardName)) {
+        stopJetty(jetty);
+      }
+    }
+  }
+  
+  public JettySolrRunner getShard(String slice, int index) throws Exception {
+    JettySolrRunner jetty = shardToJetty.get(slice).get(index).jetty;
+    return jetty;
+  }
+  
+  public CloudJettyRunner stopRandomShard() throws Exception {
+    String sliceName = getRandomSlice();
+    
+    return stopRandomShard(sliceName);
+  }
+  
+  public CloudJettyRunner stopRandomShard(String slice) throws Exception {
+    CloudJettyRunner cjetty = getRandomJetty(slice, aggressivelyKillLeaders);
+    if (cjetty != null) {
+      stopJetty(cjetty);
+    }
+    return cjetty;
+  }
+  
+  
+  public CloudJettyRunner killRandomShard() throws Exception {
+    // add all the shards to a list
+    String sliceName = getRandomSlice();
+    
+    return killRandomShard(sliceName);
+  }
+
+  private String getRandomSlice() {
+    Map<String,Slice> slices = zkStateReader.getClusterState().getSlices(collection);
+    
+    List<String> sliceKeyList = new ArrayList<String>(slices.size());
+    sliceKeyList.addAll(slices.keySet());
+    String sliceName = sliceKeyList.get(LuceneTestCase.random().nextInt(sliceKeyList.size()));
+    return sliceName;
+  }
+  
+  public CloudJettyRunner killRandomShard(String slice) throws Exception {
+    CloudJettyRunner cjetty = getRandomJetty(slice, aggressivelyKillLeaders);
+    if (cjetty != null) {
+      killJetty(cjetty);
+    }
+    return cjetty;
+  }
+  
+  public CloudJettyRunner getRandomJetty(String slice, boolean aggressivelyKillLeaders) throws KeeperException, InterruptedException {
+    
+
+    int numRunning = 0;
+    int numRecovering = 0;
+    int numActive = 0;
+    
+    for (CloudJettyRunner cloudJetty : shardToJetty.get(slice)) {
+      boolean running = true;
+      
+      // get latest cloud state
+      zkStateReader.updateClusterState(true);
+      
+      Slice theShards = zkStateReader.getClusterState().getSlices(collection)
+          .get(slice);
+      
+      ZkNodeProps props = theShards.getShards().get(cloudJetty.coreNodeName);
+      if (props == null) {
+        throw new RuntimeException("shard name " + cloudJetty.coreNodeName + " not found in " + theShards.getShards().keySet());
+      }
+      
+      String state = props.get(ZkStateReader.STATE_PROP);
+      String nodeName = props.get(ZkStateReader.NODE_NAME_PROP);
+      
+      
+      if (!cloudJetty.jetty.isRunning()
+          || !state.equals(ZkStateReader.ACTIVE)
+          || !zkStateReader.getClusterState().liveNodesContain(nodeName)) {
+        running = false;
+      }
+      
+      if (cloudJetty.jetty.isRunning()
+          && state.equals(ZkStateReader.RECOVERING)
+          && zkStateReader.getClusterState().liveNodesContain(nodeName)) {
+        numRecovering++;
+      }
+      
+      if (cloudJetty.jetty.isRunning()
+          && state.equals(ZkStateReader.ACTIVE)
+          && zkStateReader.getClusterState().liveNodesContain(nodeName)) {
+        numActive++;
+      }
+      
+      if (running) {
+        numRunning++;
+      }
+    }
+    
+    if (numActive < 2) {
+      // we cannot kill anyone
+      monkeyLog("only one active node in shard - monkey cannot kill :(");
+      return null;
+    }
+    Random random = LuceneTestCase.random();
+    int chance = random.nextInt(10);
+    CloudJettyRunner cjetty;
+    if (chance <= 5 && aggressivelyKillLeaders) {
+      // if killLeader, really aggressively go after leaders
+      cjetty = shardToLeaderJetty.get(slice);
+    } else {
+      // get random shard
+      List<CloudJettyRunner> jetties = shardToJetty.get(slice);
+      int index = random.nextInt(jetties.size());
+      cjetty = jetties.get(index);
+      
+      ZkNodeProps leader = zkStateReader.getLeaderProps(collection, slice);
+      boolean isLeader = leader.get(ZkStateReader.NODE_NAME_PROP).equals(jetties.get(index).nodeName);
+      if (!aggressivelyKillLeaders && isLeader) {
+        // we don't kill leaders...
+        monkeyLog("abort! I don't kill leaders");
+        return null;
+      } 
+    }
+
+    if (cjetty.jetty.getLocalPort() == -1) {
+      // we can't kill the dead
+      monkeyLog("abort! This guy is already dead");
+      return null;
+    }
+    
+    //System.out.println("num active:" + numActive + " for " + slice + " sac:" + jetty.getLocalPort());
+    monkeyLog("chose a victim! " + cjetty.jetty.getLocalPort());
+  
+    return cjetty;
+  }
+  
+  public SolrServer getRandomClient(String slice) throws KeeperException, InterruptedException {
+    // get latest cloud state
+    zkStateReader.updateClusterState(true);
+
+    // get random shard
+    List<SolrServer> clients = shardToClient.get(slice);
+    int index = LuceneTestCase.random().nextInt(clients.size() - 1);
+    SolrServer client = clients.get(index);
+
+    return client;
+  }
+  
+  // synchronously starts and stops shards randomly, unless there is only one
+  // active shard up for a slice or if there is one active and others recovering
+  public void startTheMonkey(boolean killLeaders, final int roundPause) {
+    monkeyLog("starting");
+    this.aggressivelyKillLeaders = killLeaders;
+    startTime = System.currentTimeMillis();
+    // TODO: when kill leaders is on, lets kill a higher percentage of leaders
+    
+    stop = false;
+    new Thread() {
+      private List<CloudJettyRunner> deadPool = new ArrayList<CloudJettyRunner>();
+
+      @Override
+      public void run() {
+        while (!stop) {
+          try {
+            Thread.sleep(roundPause);
+            Random random = LuceneTestCase.random();
+            if (random.nextBoolean()) {
+             if (!deadPool.isEmpty()) {
+               int index = random.nextInt(deadPool.size());
+               JettySolrRunner jetty = deadPool.get(index).jetty;
+               if (!ChaosMonkey.start(jetty)) {
+                 continue;
+               }
+               //System.out.println("started on port:" + jetty.getLocalPort());
+               deadPool.remove(index);
+               starts.incrementAndGet();
+               continue;
+             }
+            }
+            
+            int rnd = random.nextInt(10);
+
+            if (expireSessions && rnd < EXPIRE_PERCENT) {
+              expireRandomSession();
+            } 
+            
+            if (causeConnectionLoss && rnd < CONLOSS_PERCENT) {
+              randomConnectionLoss();
+              randomConnectionLoss();
+            }
+            
+            CloudJettyRunner cjetty;
+            if (random.nextBoolean()) {
+              cjetty = stopRandomShard();
+            } else {
+              cjetty = killRandomShard();
+            }
+            if (cjetty == null) {
+              // we cannot kill
+            } else {
+              deadPool.add(cjetty);
+            }
+            
+          } catch (InterruptedException e) {
+            //
+          } catch (Exception e) {
+            // TODO Auto-generated catch block
+            e.printStackTrace();
+          }
+        }
+        monkeyLog("finished");
+        monkeyLog("I ran for " + (System.currentTimeMillis() - startTime)/1000.0f + "sec. I stopped " + stops + " and I started " + starts
+            + ". I also expired " + expires.get() + " and caused " + connloss
+            + " connection losses");
+      }
+    }.start();
+  }
+  
+  public static void monkeyLog(String msg) {
+    log.info("monkey: " + msg);
+  }
+  
+  public void stopTheMonkey() {
+    stop = true;
+  }
+
+  public int getStarts() {
+    return starts.get();
+  }
+
+  public static void stop(JettySolrRunner jetty) throws Exception {
+    stopJettySolrRunner(jetty);
+  }
+  
+  public static boolean start(JettySolrRunner jetty) throws Exception {
+    try {
+      jetty.start();
+    } catch (BindException e) {
+      jetty.stop();
+      Thread.sleep(2000);
+      try {
+        jetty.start();
+      } catch (BindException e2) {
+        jetty.stop();
+        Thread.sleep(5000);
+        try {
+          jetty.start();
+        } catch (BindException e3) {
+          // we coud not get the port
+          jetty.stop();
+          return false;
+        }
+      }
+    }
+    return true;
+  }
+
+}
\ No newline at end of file
diff --git a/solr/test-framework/src/java/org/apache/solr/cloud/ZkTestServer.java b/solr/test-framework/src/java/org/apache/solr/cloud/ZkTestServer.java
new file mode 100644
index 0000000..305e96a
--- /dev/null
+++ b/solr/test-framework/src/java/org/apache/solr/cloud/ZkTestServer.java
@@ -0,0 +1,345 @@
+package org.apache.solr.cloud;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with this
+ * work for additional information regarding copyright ownership. The ASF
+ * licenses this file to You under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ * 
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * 
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+ * License for the specific language governing permissions and limitations under
+ * the License.
+ */
+
+import java.io.BufferedReader;
+import java.io.File;
+import java.io.IOException;
+import java.io.InputStreamReader;
+import java.io.OutputStream;
+import java.net.InetAddress;
+import java.net.InetSocketAddress;
+import java.net.Socket;
+import java.net.UnknownHostException;
+import java.util.ArrayList;
+import java.util.List;
+
+import javax.management.JMException;
+
+import org.apache.zookeeper.jmx.ManagedUtil;
+import org.apache.zookeeper.server.NIOServerCnxn;
+import org.apache.zookeeper.server.ServerConfig;
+import org.apache.zookeeper.server.SessionTracker.Session;
+import org.apache.zookeeper.server.ZKDatabase;
+import org.apache.zookeeper.server.ZooKeeperServer;
+import org.apache.zookeeper.server.persistence.FileTxnSnapLog;
+import org.apache.zookeeper.server.quorum.QuorumPeerConfig.ConfigException;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+public class ZkTestServer {
+  public static final int TICK_TIME = 3000;
+
+  private static Logger log = LoggerFactory.getLogger(ZkTestServer.class);
+  
+  protected final ZKServerMain zkServer = new ZKServerMain();
+
+  private String zkDir;
+
+  private int clientPort;
+
+  private Thread zooThread;
+  
+  private int theTickTime = TICK_TIME;
+
+  class ZKServerMain {
+
+    private NIOServerCnxn.Factory cnxnFactory;
+    private ZooKeeperServer zooKeeperServer;
+    
+    protected void initializeAndRun(String[] args) throws ConfigException,
+        IOException {
+      try {
+        ManagedUtil.registerLog4jMBeans();
+      } catch (JMException e) {
+
+      }
+
+      ServerConfig config = new ServerConfig();
+      if (args.length == 1) {
+        config.parse(args[0]);
+      } else {
+        config.parse(args);
+      }
+
+      runFromConfig(config);
+    }
+
+    /**
+     * Run from a ServerConfig.
+     * @param config ServerConfig to use.
+     * @throws IOException
+     */
+    public void runFromConfig(ServerConfig config) throws IOException {
+      try {
+        // Note that this thread isn't going to be doing anything else,
+        // so rather than spawning another thread, we will just call
+        // run() in this thread.
+        // create a file logger url from the command line args
+        zooKeeperServer = new ZooKeeperServer();
+
+        FileTxnSnapLog ftxn = new FileTxnSnapLog(new File(config
+            .getDataLogDir()), new File(config.getDataDir()));
+        zooKeeperServer.setTxnLogFactory(ftxn);
+        zooKeeperServer.setTickTime(config.getTickTime());
+        cnxnFactory = new NIOServerCnxn.Factory(config.getClientPortAddress(), config
+            .getMaxClientCnxns());
+        cnxnFactory.startup(zooKeeperServer);
+        cnxnFactory.join();
+        if (zooKeeperServer.isRunning()) {
+          zooKeeperServer.shutdown();
+        }
+      } catch (InterruptedException e) {
+      }
+    }
+
+    /**
+     * Shutdown the serving instance
+     * @throws IOException 
+     */
+    protected void shutdown() throws IOException {
+      zooKeeperServer.shutdown();
+      ZKDatabase zkDb = zooKeeperServer.getZKDatabase();
+      if (zkDb != null) {
+        zkDb.close();
+      }
+      if (cnxnFactory != null && cnxnFactory.getLocalPort() != 0) {
+        waitForServerDown(getZkHost() + ":" + getPort(), 5000);
+      }
+      if (cnxnFactory != null) {
+        cnxnFactory.shutdown();
+      }
+    }
+
+    public int getLocalPort() {
+      if (cnxnFactory == null) {
+        throw new IllegalStateException("A port has not yet been selected");
+      }
+      int port;
+      try {
+        port = cnxnFactory.getLocalPort();
+      } catch (NullPointerException e) {
+        throw new IllegalStateException("A port has not yet been selected");
+      }
+      if (port == 0) {
+        throw new IllegalStateException("A port has not yet been selected");
+      }
+      return port;
+    }
+  }
+
+  public ZkTestServer(String zkDir) {
+    this.zkDir = zkDir;
+  }
+
+  public ZkTestServer(String zkDir, int port) {
+    this.zkDir = zkDir;
+    this.clientPort = port;
+  }
+
+  public String getZkHost() {
+    return "127.0.0.1:" + zkServer.getLocalPort();
+  }
+
+  public String getZkAddress() {
+    return "127.0.0.1:" + zkServer.getLocalPort() + "/solr";
+  }
+
+  public int getPort() {
+    return zkServer.getLocalPort();
+  }
+  
+  public void expire(final long sessionId) {
+    zkServer.zooKeeperServer.expire(new Session() {
+      @Override
+      public long getSessionId() {
+        return sessionId;
+      }
+      @Override
+      public int getTimeout() {
+        return 4000;
+      }
+      @Override
+      public boolean isClosing() {
+        return false;
+      }});
+  }
+
+  public void run() throws InterruptedException {
+    log.info("STARTING ZK TEST SERVER");
+    // we don't call super.setUp
+    zooThread = new Thread() {
+      
+      @Override
+      public void run() {
+        ServerConfig config = new ServerConfig() {
+
+          {
+            setClientPort(ZkTestServer.this.clientPort);
+            this.dataDir = zkDir;
+            this.dataLogDir = zkDir;
+            this.tickTime = theTickTime;
+          }
+          
+          public void setClientPort(int clientPort) {
+            if (clientPortAddress != null) {
+              try {
+                this.clientPortAddress = new InetSocketAddress(
+                        InetAddress.getByName(clientPortAddress.getHostName()), clientPort);
+              } catch (UnknownHostException e) {
+                throw new RuntimeException(e);
+              }
+            } else {
+              this.clientPortAddress = new InetSocketAddress(clientPort);
+            }
+          }
+        };
+
+        try {
+          zkServer.runFromConfig(config);
+        } catch (Throwable e) {
+          throw new RuntimeException(e);
+        }
+      }
+    };
+
+    zooThread.setDaemon(true);
+    zooThread.start();
+
+    int cnt = 0;
+    int port = -1;
+    try {
+       port = getPort();
+    } catch(IllegalStateException e) {
+      
+    }
+    while (port < 1) {
+      Thread.sleep(100);
+      try {
+        port = getPort();
+      } catch(IllegalStateException e) {
+        
+      }
+      if (cnt == 500) {
+        throw new RuntimeException("Could not get the port for ZooKeeper server");
+      }
+      cnt++;
+    }
+  }
+
+  @SuppressWarnings("deprecation")
+  public void shutdown() throws IOException {
+    // TODO: this can log an exception while trying to unregister a JMX MBean
+    zkServer.shutdown();
+  }
+ 
+  
+  public static boolean waitForServerDown(String hp, long timeout) {
+    long start = System.currentTimeMillis();
+    while (true) {
+      try {
+        HostPort hpobj = parseHostPortList(hp).get(0);
+        send4LetterWord(hpobj.host, hpobj.port, "stat");
+      } catch (IOException e) {
+        return true;
+      }
+      
+      if (System.currentTimeMillis() > start + timeout) {
+        break;
+      }
+      try {
+        Thread.sleep(250);
+      } catch (InterruptedException e) {
+        // ignore
+      }
+    }
+    return false;
+  }
+  
+  public static class HostPort {
+    String host;
+    int port;
+    
+    HostPort(String host, int port) {
+      this.host = host;
+      this.port = port;
+    }
+  }
+  
+  /**
+   * Send the 4letterword
+   * @param host the destination host
+   * @param port the destination port
+   * @param cmd the 4letterword
+   * @return
+   * @throws IOException
+   */
+  public static String send4LetterWord(String host, int port, String cmd)
+      throws IOException
+  {
+
+      Socket sock = new Socket(host, port);
+      BufferedReader reader = null;
+      try {
+          OutputStream outstream = sock.getOutputStream();
+          outstream.write(cmd.getBytes("US-ASCII"));
+          outstream.flush();
+          // this replicates NC - close the output stream before reading
+          sock.shutdownOutput();
+
+          reader =
+              new BufferedReader(
+                      new InputStreamReader(sock.getInputStream(), "US-ASCII"));
+          StringBuilder sb = new StringBuilder();
+          String line;
+          while((line = reader.readLine()) != null) {
+              sb.append(line + "\n");
+          }
+          return sb.toString();
+      } finally {
+          sock.close();
+          if (reader != null) {
+              reader.close();
+          }
+      }
+  }
+  
+  public static List<HostPort> parseHostPortList(String hplist) {
+    ArrayList<HostPort> alist = new ArrayList<HostPort>();
+    for (String hp : hplist.split(",")) {
+      int idx = hp.lastIndexOf(':');
+      String host = hp.substring(0, idx);
+      int port;
+      try {
+        port = Integer.parseInt(hp.substring(idx + 1));
+      } catch (RuntimeException e) {
+        throw new RuntimeException("Problem parsing " + hp + e.toString());
+      }
+      alist.add(new HostPort(host, port));
+    }
+    return alist;
+  }
+
+  public int getTheTickTime() {
+    return theTickTime;
+  }
+
+  public void setTheTickTime(int theTickTime) {
+    this.theTickTime = theTickTime;
+  }
+}
diff --git a/solr/test-framework/src/java/org/apache/solr/util/ExternalPaths.java b/solr/test-framework/src/java/org/apache/solr/util/ExternalPaths.java
index 6c6e10d..79258ea 100644
--- a/solr/test-framework/src/java/org/apache/solr/util/ExternalPaths.java
+++ b/solr/test-framework/src/java/org/apache/solr/util/ExternalPaths.java
@@ -25,7 +25,7 @@ import java.io.File;
  * @lucene.internal
  */
 public class ExternalPaths {
-  private static final String SOURCE_HOME = determineSourceHome();
+  public static final String SOURCE_HOME = determineSourceHome();
   public static String WEBAPP_HOME = new File(SOURCE_HOME, "webapp/web").getAbsolutePath();
   public static String EXAMPLE_HOME = new File(SOURCE_HOME, "example/solr").getAbsolutePath();
   public static String EXAMPLE_MULTICORE_HOME = new File(SOURCE_HOME, "example/multicore").getAbsolutePath();

