GitDiffStart: de3d41432ce203c2d05fd4608783aed526fa54ff | Sat Aug 4 18:32:33 2012 +0000
diff --git a/solr/core/src/java/org/apache/solr/SolrLogFormatter.java b/solr/core/src/java/org/apache/solr/SolrLogFormatter.java
index 399a28e..565d55b 100644
--- a/solr/core/src/java/org/apache/solr/SolrLogFormatter.java
+++ b/solr/core/src/java/org/apache/solr/SolrLogFormatter.java
@@ -250,10 +250,10 @@ sb.append("(group_name=").append(tg.getName()).append(")");
 
     /*** Isn't core specific... prob better logged from zkController
     if (info != null) {
-      CloudState cloudState = zkController.getCloudState();
-      if (info.cloudState != cloudState) {
+      ClusterState clusterState = zkController.getClusterState();
+      if (info.clusterState != clusterState) {
         // something has changed in the matrix...
-        sb.append(zkController.getBaseUrl() + " sees new CloudState:");
+        sb.append(zkController.getBaseUrl() + " sees new ClusterState:");
       }
     }
     ***/
@@ -263,7 +263,7 @@ sb.append("(group_name=").append(tg.getName()).append(")");
 
   private Map<String,String> getCoreProps(ZkController zkController, SolrCore core) {
     final String collection = core.getCoreDescriptor().getCloudDescriptor().getCollectionName();
-    ZkNodeProps props = zkController.getCloudState().getShardProps(collection,  ZkStateReader.getCoreNodeName(zkController.getNodeName(), core.getName()));
+    ZkNodeProps props = zkController.getClusterState().getShardProps(collection,  ZkStateReader.getCoreNodeName(zkController.getNodeName(), core.getName()));
     if(props!=null) {
       return props.getProperties(); 
     }
diff --git a/solr/core/src/java/org/apache/solr/cloud/AssignShard.java b/solr/core/src/java/org/apache/solr/cloud/AssignShard.java
index 9dcd09d..13dc8f4 100644
--- a/solr/core/src/java/org/apache/solr/cloud/AssignShard.java
+++ b/solr/core/src/java/org/apache/solr/cloud/AssignShard.java
@@ -24,7 +24,7 @@ import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 
-import org.apache.solr.common.cloud.CloudState;
+import org.apache.solr.common.cloud.ClusterState;
 import org.apache.solr.common.cloud.Slice;
 
 public class AssignShard {
@@ -36,7 +36,7 @@ public class AssignShard {
    * @param state
    * @return the assigned shard id
    */
-  public static String assignShard(String collection, CloudState state, Integer numShards) {
+  public static String assignShard(String collection, ClusterState state, Integer numShards) {
     if (numShards == null) {
       numShards = 1;
     }
diff --git a/solr/core/src/java/org/apache/solr/cloud/ElectionContext.java b/solr/core/src/java/org/apache/solr/cloud/ElectionContext.java
index b5acd84..fcca293 100644
--- a/solr/core/src/java/org/apache/solr/cloud/ElectionContext.java
+++ b/solr/core/src/java/org/apache/solr/cloud/ElectionContext.java
@@ -5,7 +5,7 @@ import java.util.Map;
 
 import org.apache.solr.common.SolrException;
 import org.apache.solr.common.SolrException.ErrorCode;
-import org.apache.solr.common.cloud.CloudState;
+import org.apache.solr.common.cloud.ClusterState;
 import org.apache.solr.common.cloud.Slice;
 import org.apache.solr.common.cloud.SolrZkClient;
 import org.apache.solr.common.cloud.ZkCoreNodeProps;
@@ -13,7 +13,6 @@ import org.apache.solr.common.cloud.ZkNodeProps;
 import org.apache.solr.common.cloud.ZkStateReader;
 import org.apache.solr.core.CoreContainer;
 import org.apache.solr.core.SolrCore;
-import org.apache.solr.handler.component.ShardHandler;
 import org.apache.zookeeper.CreateMode;
 import org.apache.zookeeper.KeeperException;
 import org.apache.zookeeper.KeeperException.NodeExistsException;
@@ -195,8 +194,8 @@ final class ShardLeaderElectionContext extends ShardLeaderElectionContextBase {
   }
   
   private boolean shouldIBeLeader(ZkNodeProps leaderProps) {
-    CloudState cloudState = zkController.getZkStateReader().getCloudState();
-    Map<String,Slice> slices = cloudState.getSlices(this.collection);
+    ClusterState clusterState = zkController.getZkStateReader().getClusterState();
+    Map<String,Slice> slices = clusterState.getSlices(this.collection);
     Slice slice = slices.get(shardId);
     Map<String,ZkNodeProps> shards = slice.getShards();
     boolean foundSomeoneElseActive = false;
@@ -206,7 +205,7 @@ final class ShardLeaderElectionContext extends ShardLeaderElectionContextBase {
       if (new ZkCoreNodeProps(shard.getValue()).getCoreUrl().equals(
               new ZkCoreNodeProps(leaderProps).getCoreUrl())) {
         if (state.equals(ZkStateReader.ACTIVE)
-          && cloudState.liveNodesContain(shard.getValue().get(
+          && clusterState.liveNodesContain(shard.getValue().get(
               ZkStateReader.NODE_NAME_PROP))) {
           // we are alive
           return true;
@@ -214,7 +213,7 @@ final class ShardLeaderElectionContext extends ShardLeaderElectionContextBase {
       }
       
       if ((state.equals(ZkStateReader.ACTIVE))
-          && cloudState.liveNodesContain(shard.getValue().get(
+          && clusterState.liveNodesContain(shard.getValue().get(
               ZkStateReader.NODE_NAME_PROP))
           && !new ZkCoreNodeProps(shard.getValue()).getCoreUrl().equals(
               new ZkCoreNodeProps(leaderProps).getCoreUrl())) {
@@ -226,8 +225,8 @@ final class ShardLeaderElectionContext extends ShardLeaderElectionContextBase {
   }
   
   private boolean anyoneElseActive() {
-    CloudState cloudState = zkController.getZkStateReader().getCloudState();
-    Map<String,Slice> slices = cloudState.getSlices(this.collection);
+    ClusterState clusterState = zkController.getZkStateReader().getClusterState();
+    Map<String,Slice> slices = clusterState.getSlices(this.collection);
     Slice slice = slices.get(shardId);
     Map<String,ZkNodeProps> shards = slice.getShards();
 
@@ -236,7 +235,7 @@ final class ShardLeaderElectionContext extends ShardLeaderElectionContextBase {
 
       
       if ((state.equals(ZkStateReader.ACTIVE))
-          && cloudState.liveNodesContain(shard.getValue().get(
+          && clusterState.liveNodesContain(shard.getValue().get(
               ZkStateReader.NODE_NAME_PROP))) {
         return true;
       }
diff --git a/solr/core/src/java/org/apache/solr/cloud/Overseer.java b/solr/core/src/java/org/apache/solr/cloud/Overseer.java
index 979cfbe..2231178 100644
--- a/solr/core/src/java/org/apache/solr/cloud/Overseer.java
+++ b/solr/core/src/java/org/apache/solr/cloud/Overseer.java
@@ -24,7 +24,7 @@ import java.util.Map;
 import java.util.Map.Entry;
 
 import org.apache.solr.common.SolrException;
-import org.apache.solr.common.cloud.CloudState;
+import org.apache.solr.common.cloud.ClusterState;
 import org.apache.solr.common.cloud.Slice;
 import org.apache.solr.common.cloud.SolrZkClient;
 import org.apache.solr.common.cloud.ZkCoreNodeProps;
@@ -47,7 +47,7 @@ public class Overseer {
 
   private static Logger log = LoggerFactory.getLogger(Overseer.class);
   
-  private class CloudStateUpdater implements Runnable {
+  private class ClusterStateUpdater implements Runnable {
     
     private static final String DELETECORE = "deletecore";
     private final ZkStateReader reader;
@@ -59,7 +59,7 @@ public class Overseer {
     //If Overseer dies while extracting the main queue a new overseer will start from this queue 
     private final DistributedQueue workQueue;
     
-    public CloudStateUpdater(final ZkStateReader reader, final String myId) {
+    public ClusterStateUpdater(final ZkStateReader reader, final String myId) {
       this.zkClient = reader.getZkClient();
       this.stateUpdateQueue = getInQueue(zkClient);
       this.workQueue = getInternalQueue(zkClient);
@@ -78,17 +78,17 @@ public class Overseer {
               byte[] head = workQueue.peek();
               
               if (head != null) {
-                reader.updateCloudState(true);
-                CloudState cloudState = reader.getCloudState();
+                reader.updateClusterState(true);
+                ClusterState clusterState = reader.getClusterState();
                 log.info("Replaying operations from work queue.");
                 
                 while (head != null && amILeader()) {
                   final ZkNodeProps message = ZkNodeProps.load(head);
                   final String operation = message
                       .get(QUEUE_OPERATION);
-                  cloudState = processMessage(cloudState, message, operation);
+                  clusterState = processMessage(clusterState, message, operation);
                   zkClient.setData(ZkStateReader.CLUSTER_STATE,
-                      ZkStateReader.toJSON(cloudState), true);
+                      ZkStateReader.toJSON(clusterState), true);
                   workQueue.remove();
                   head = workQueue.peek();
                 }
@@ -116,20 +116,20 @@ public class Overseer {
             byte[] head = stateUpdateQueue.peek();
             
             if (head != null) {
-              reader.updateCloudState(true);
-              CloudState cloudState = reader.getCloudState();
+              reader.updateClusterState(true);
+              ClusterState clusterState = reader.getClusterState();
               
               while (head != null) {
                 final ZkNodeProps message = ZkNodeProps.load(head);
                 final String operation = message.get(QUEUE_OPERATION);
                 
-                cloudState = processMessage(cloudState, message, operation);
+                clusterState = processMessage(clusterState, message, operation);
                 byte[] processed = stateUpdateQueue.remove();
                 workQueue.offer(processed);
                 head = stateUpdateQueue.peek();
               }
               zkClient.setData(ZkStateReader.CLUSTER_STATE,
-                  ZkStateReader.toJSON(cloudState), true);
+                  ZkStateReader.toJSON(clusterState), true);
             }
             // clean work queue
             while (workQueue.poll() != null);
@@ -157,12 +157,12 @@ public class Overseer {
       }
     }
 
-    private CloudState processMessage(CloudState cloudState,
+    private ClusterState processMessage(ClusterState clusterState,
         final ZkNodeProps message, final String operation) {
       if ("state".equals(operation)) {
-        cloudState = updateState(cloudState, message);
+        clusterState = updateState(clusterState, message);
       } else if (DELETECORE.equals(operation)) {
-        cloudState = removeCore(cloudState, message);
+        clusterState = removeCore(clusterState, message);
       } else if (ZkStateReader.LEADER_PROP.equals(operation)) {
         StringBuilder sb = new StringBuilder();
         String baseUrl = message.get(ZkStateReader.BASE_URL_PROP);
@@ -172,14 +172,14 @@ public class Overseer {
         sb.append(coreName == null ? "" : coreName);
         if (!(sb.substring(sb.length() - 1).equals("/"))) sb
             .append("/");
-        cloudState = setShardLeader(cloudState,
+        clusterState = setShardLeader(clusterState,
             message.get(ZkStateReader.COLLECTION_PROP),
             message.get(ZkStateReader.SHARD_ID_PROP), sb.toString());
       } else {
         throw new RuntimeException("unknown operation:" + operation
             + " contents:" + message.getProperties());
       }
-      return cloudState;
+      return clusterState;
     }
       
       private boolean amILeader() {
@@ -199,7 +199,7 @@ public class Overseer {
       /**
        * Try to assign core to the cluster. 
        */
-      private CloudState updateState(CloudState state, final ZkNodeProps message) {
+      private ClusterState updateState(ClusterState state, final ZkNodeProps message) {
         final String collection = message.get(ZkStateReader.COLLECTION_PROP);
         final String zkCoreNodeName = message.get(ZkStateReader.NODE_NAME_PROP) + "_" + message.get(ZkStateReader.CORE_NAME_PROP);
         final Integer numShards = message.get(ZkStateReader.NUM_SHARDS_PROP)!=null?Integer.parseInt(message.get(ZkStateReader.NUM_SHARDS_PROP)):null;
@@ -214,7 +214,7 @@ public class Overseer {
         String shardId = message.get(ZkStateReader.SHARD_ID_PROP);
         if (shardId == null) {
           String nodeName = message.get(ZkStateReader.NODE_NAME_PROP);
-          //get shardId from CloudState
+          //get shardId from ClusterState
           shardId = getAssignedId(state, nodeName, message);
         }
         if(shardId == null) {
@@ -242,11 +242,11 @@ public class Overseer {
           shardProps.put(zkCoreNodeName, zkProps);
 
           slice = new Slice(shardId, shardProps);
-          CloudState newCloudState = updateSlice(state, collection, slice);
-          return newCloudState;
+          ClusterState newClusterState = updateSlice(state, collection, slice);
+          return newClusterState;
       }
 
-      private CloudState createCollection(CloudState state, String collectionName, int numShards) {
+      private ClusterState createCollection(ClusterState state, String collectionName, int numShards) {
         Map<String, Map<String, Slice>> newStates = new LinkedHashMap<String,Map<String, Slice>>();
         Map<String, Slice> newSlices = new LinkedHashMap<String,Slice>();
         newStates.putAll(state.getCollectionStates());
@@ -255,14 +255,14 @@ public class Overseer {
           newSlices.put(sliceName, new Slice(sliceName, Collections.EMPTY_MAP));
         }
         newStates.put(collectionName, newSlices);
-        CloudState newCloudState = new CloudState(state.getLiveNodes(), newStates);
-        return newCloudState;
+        ClusterState newClusterState = new ClusterState(state.getLiveNodes(), newStates);
+        return newClusterState;
       }
       
       /*
        * Return an already assigned id or null if not assigned
        */
-      private String getAssignedId(final CloudState state, final String nodeName,
+      private String getAssignedId(final ClusterState state, final String nodeName,
           final ZkNodeProps coreState) {
         final String key = coreState.get(ZkStateReader.NODE_NAME_PROP) + "_" +  coreState.get(ZkStateReader.CORE_NAME_PROP);
         Map<String, Slice> slices = state.getSlices(coreState.get(ZkStateReader.COLLECTION_PROP));
@@ -276,7 +276,7 @@ public class Overseer {
         return null;
       }
       
-      private CloudState updateSlice(CloudState state, String collection, Slice slice) {
+      private ClusterState updateSlice(ClusterState state, String collection, Slice slice) {
         
         final Map<String, Map<String, Slice>> newStates = new LinkedHashMap<String,Map<String,Slice>>();
         newStates.putAll(state.getCollectionStates());
@@ -306,10 +306,10 @@ public class Overseer {
           final Slice updatedSlice = new Slice(slice.getName(), shards);
           slices.put(slice.getName(), updatedSlice);
         }
-        return new CloudState(state.getLiveNodes(), newStates);
+        return new ClusterState(state.getLiveNodes(), newStates);
       }
       
-      private CloudState setShardLeader(CloudState state, String collection, String sliceName, String leaderUrl) {
+      private ClusterState setShardLeader(ClusterState state, String collection, String sliceName, String leaderUrl) {
         
         final Map<String, Map<String, Slice>> newStates = new LinkedHashMap<String,Map<String,Slice>>();
         newStates.putAll(state.getCollectionStates());
@@ -341,21 +341,21 @@ public class Overseer {
           Slice slice = new Slice(sliceName, newShards);
           slices.put(sliceName, slice);
         }
-        return new CloudState(state.getLiveNodes(), newStates);
+        return new ClusterState(state.getLiveNodes(), newStates);
       }
       
       /*
        * Remove core from cloudstate
        */
-      private CloudState removeCore(final CloudState cloudState, ZkNodeProps message) {
+      private ClusterState removeCore(final ClusterState clusterState, ZkNodeProps message) {
         
         final String coreNodeName = message.get(ZkStateReader.NODE_NAME_PROP) + "_" + message.get(ZkStateReader.CORE_NAME_PROP);
         final String collection = message.get(ZkStateReader.COLLECTION_PROP);
 
         final LinkedHashMap<String, Map<String, Slice>> newStates = new LinkedHashMap<String,Map<String,Slice>>();
-        for(String collectionName: cloudState.getCollections()) {
+        for(String collectionName: clusterState.getCollections()) {
           if(collection.equals(collectionName)) {
-            Map<String, Slice> slices = cloudState.getSlices(collection);
+            Map<String, Slice> slices = clusterState.getSlices(collection);
             LinkedHashMap<String, Slice> newSlices = new LinkedHashMap<String, Slice>();
             for(Slice slice: slices.values()) {
               if(slice.getShards().containsKey(coreNodeName)) {
@@ -393,10 +393,10 @@ public class Overseer {
               }
             }
           } else {
-            newStates.put(collectionName, cloudState.getSlices(collectionName));
+            newStates.put(collectionName, clusterState.getSlices(collectionName));
           }
         }
-        CloudState newState = new CloudState(cloudState.getLiveNodes(), newStates);
+        ClusterState newState = new ClusterState(clusterState.getLiveNodes(), newStates);
         return newState;
      }
     
@@ -425,7 +425,7 @@ public class Overseer {
     createOverseerNode(reader.getZkClient());
     //launch cluster state updater thread
     ThreadGroup tg = new ThreadGroup("Overseer state updater.");
-    updaterThread = new Thread(tg, new CloudStateUpdater(reader, id));
+    updaterThread = new Thread(tg, new ClusterStateUpdater(reader, id));
     updaterThread.setDaemon(true);
 
     ThreadGroup ccTg = new ThreadGroup("Overseer collection creation process.");
diff --git a/solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor.java b/solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor.java
index da99535..d4cacf1 100644
--- a/solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor.java
+++ b/solr/core/src/java/org/apache/solr/cloud/OverseerCollectionProcessor.java
@@ -25,7 +25,7 @@ import java.util.Set;
 
 import org.apache.solr.common.SolrException;
 import org.apache.solr.common.SolrException.ErrorCode;
-import org.apache.solr.common.cloud.CloudState;
+import org.apache.solr.common.cloud.ClusterState;
 import org.apache.solr.common.cloud.Slice;
 import org.apache.solr.common.cloud.ZkNodeProps;
 import org.apache.solr.common.cloud.ZkStateReader;
@@ -132,22 +132,22 @@ public class OverseerCollectionProcessor implements Runnable {
   
   private boolean processMessage(ZkNodeProps message, String operation) {
     if (CREATECOLLECTION.equals(operation)) {
-      return createCollection(zkStateReader.getCloudState(), message);
+      return createCollection(zkStateReader.getClusterState(), message);
     } else if (DELETECOLLECTION.equals(operation)) {
       ModifiableSolrParams params = new ModifiableSolrParams();
       params.set(CoreAdminParams.ACTION, CoreAdminAction.UNLOAD.toString());
       params.set(CoreAdminParams.DELETE_INSTANCE_DIR, true);
-      return collectionCmd(zkStateReader.getCloudState(), message, params);
+      return collectionCmd(zkStateReader.getClusterState(), message, params);
     } else if (RELOADCOLLECTION.equals(operation)) {
       ModifiableSolrParams params = new ModifiableSolrParams();
       params.set(CoreAdminParams.ACTION, CoreAdminAction.RELOAD.toString());
-      return collectionCmd(zkStateReader.getCloudState(), message, params);
+      return collectionCmd(zkStateReader.getClusterState(), message, params);
     }
     // unknown command, toss it from our queue
     return true;
   }
 
-  private boolean createCollection(CloudState cloudState, ZkNodeProps message) {
+  private boolean createCollection(ClusterState clusterState, ZkNodeProps message) {
     
     // look at the replication factor and see if it matches reality
     // if it does not, find best nodes to create more cores
@@ -182,7 +182,7 @@ public class OverseerCollectionProcessor implements Runnable {
     
     // TODO: add smarter options that look at the current number of cores per node?
     // for now we just go random
-    Set<String> nodes = cloudState.getLiveNodes();
+    Set<String> nodes = clusterState.getLiveNodes();
     List<String> nodeList = new ArrayList<String>(nodes.size());
     nodeList.addAll(nodes);
     Collections.shuffle(nodeList);
@@ -235,11 +235,11 @@ public class OverseerCollectionProcessor implements Runnable {
     return true;
   }
   
-  private boolean collectionCmd(CloudState cloudState, ZkNodeProps message, ModifiableSolrParams params) {
+  private boolean collectionCmd(ClusterState clusterState, ZkNodeProps message, ModifiableSolrParams params) {
     log.info("Executing Collection Cmd : " + params);
     String name = message.get("name");
     
-    Map<String,Slice> slices = cloudState.getCollectionStates().get(name);
+    Map<String,Slice> slices = clusterState.getCollectionStates().get(name);
     
     if (slices == null) {
       throw new SolrException(ErrorCode.BAD_REQUEST, "Could not find collection:" + name);
@@ -251,7 +251,7 @@ public class OverseerCollectionProcessor implements Runnable {
       Set<Map.Entry<String,ZkNodeProps>> shardEntries = shards.entrySet();
       for (Map.Entry<String,ZkNodeProps> shardEntry : shardEntries) {
         final ZkNodeProps node = shardEntry.getValue();
-        if (cloudState.liveNodesContain(node.get(ZkStateReader.NODE_NAME_PROP))) {
+        if (clusterState.liveNodesContain(node.get(ZkStateReader.NODE_NAME_PROP))) {
           params.set(CoreAdminParams.CORE, node.get(ZkStateReader.CORE_NAME_PROP));
 
           String replica = node.get(ZkStateReader.BASE_URL_PROP);
diff --git a/solr/core/src/java/org/apache/solr/cloud/SyncStrategy.java b/solr/core/src/java/org/apache/solr/cloud/SyncStrategy.java
index c2f60fb..ccf1b43 100644
--- a/solr/core/src/java/org/apache/solr/cloud/SyncStrategy.java
+++ b/solr/core/src/java/org/apache/solr/cloud/SyncStrategy.java
@@ -28,7 +28,7 @@ import org.apache.solr.client.solrj.impl.HttpClientUtil;
 import org.apache.solr.client.solrj.impl.HttpSolrServer;
 import org.apache.solr.client.solrj.request.CoreAdminRequest.RequestRecovery;
 import org.apache.solr.common.SolrException;
-import org.apache.solr.common.cloud.CloudState;
+import org.apache.solr.common.cloud.ClusterState;
 import org.apache.solr.common.cloud.Slice;
 import org.apache.solr.common.cloud.ZkCoreNodeProps;
 import org.apache.solr.common.cloud.ZkNodeProps;
@@ -132,8 +132,8 @@ public class SyncStrategy {
   
   private boolean areAnyOtherReplicasActive(ZkController zkController,
       ZkNodeProps leaderProps, String collection, String shardId) {
-    CloudState cloudState = zkController.getZkStateReader().getCloudState();
-    Map<String,Slice> slices = cloudState.getSlices(collection);
+    ClusterState clusterState = zkController.getZkStateReader().getClusterState();
+    Map<String,Slice> slices = clusterState.getSlices(collection);
     Slice slice = slices.get(shardId);
     Map<String,ZkNodeProps> shards = slice.getShards();
     for (Map.Entry<String,ZkNodeProps> shard : shards.entrySet()) {
@@ -142,10 +142,10 @@ public class SyncStrategy {
 //          + state
 //          + shard.getValue().get(ZkStateReader.NODE_NAME_PROP)
 //          + " live: "
-//          + cloudState.liveNodesContain(shard.getValue().get(
+//          + clusterState.liveNodesContain(shard.getValue().get(
 //              ZkStateReader.NODE_NAME_PROP)));
       if ((state.equals(ZkStateReader.ACTIVE))
-          && cloudState.liveNodesContain(shard.getValue().get(
+          && clusterState.liveNodesContain(shard.getValue().get(
               ZkStateReader.NODE_NAME_PROP))
           && !new ZkCoreNodeProps(shard.getValue()).getCoreUrl().equals(
               new ZkCoreNodeProps(leaderProps).getCoreUrl())) {
diff --git a/solr/core/src/java/org/apache/solr/cloud/ZkController.java b/solr/core/src/java/org/apache/solr/cloud/ZkController.java
index 101a2fa..8a5b3be 100644
--- a/solr/core/src/java/org/apache/solr/cloud/ZkController.java
+++ b/solr/core/src/java/org/apache/solr/cloud/ZkController.java
@@ -38,7 +38,7 @@ import org.apache.solr.client.solrj.impl.HttpSolrServer;
 import org.apache.solr.client.solrj.request.CoreAdminRequest.WaitForState;
 import org.apache.solr.common.SolrException;
 import org.apache.solr.common.SolrException.ErrorCode;
-import org.apache.solr.common.cloud.CloudState;
+import org.apache.solr.common.cloud.ClusterState;
 import org.apache.solr.common.cloud.OnReconnect;
 import org.apache.solr.common.cloud.SolrZkClient;
 import org.apache.solr.common.cloud.ZkCmdExecutor;
@@ -274,8 +274,8 @@ public final class ZkController {
   /**
    * @return information about the cluster from ZooKeeper
    */
-  public CloudState getCloudState() {
-    return zkStateReader.getCloudState();
+  public ClusterState getClusterState() {
+    return zkStateReader.getClusterState();
   }
 
   /**
@@ -545,17 +545,17 @@ public final class ZkController {
     String leaderUrl = getLeaderProps(collection, cloudDesc.getShardId()).getCoreUrl();
     
     // now wait until our currently cloud state contains the latest leader
-    String cloudStateLeader = zkStateReader.getLeaderUrl(collection, shardId, 30000);
+    String clusterStateLeader = zkStateReader.getLeaderUrl(collection, shardId, 30000);
     int tries = 0;
-    while (!leaderUrl.equals(cloudStateLeader)) {
+    while (!leaderUrl.equals(clusterStateLeader)) {
       if (tries == 60) {
         throw new SolrException(ErrorCode.SERVER_ERROR,
             "There is conflicting information about the leader of shard: "
-                + cloudDesc.getShardId() + " our state says:" + cloudStateLeader + " but zookeeper says:" + leaderUrl);
+                + cloudDesc.getShardId() + " our state says:" + clusterStateLeader + " but zookeeper says:" + leaderUrl);
       }
       Thread.sleep(1000);
       tries++;
-      cloudStateLeader = zkStateReader.getLeaderUrl(collection, shardId, 30000);
+      clusterStateLeader = zkStateReader.getLeaderUrl(collection, shardId, 30000);
       leaderUrl = getLeaderProps(collection, cloudDesc.getShardId()).getCoreUrl();
     }
     
@@ -608,7 +608,7 @@ public final class ZkController {
     }
     
     // make sure we have an update cluster state right away
-    zkStateReader.updateCloudState(true);
+    zkStateReader.updateClusterState(true);
 
     return shardId;
   }
@@ -741,7 +741,7 @@ public final class ZkController {
   }
 
   private boolean needsToBeAssignedShardId(final CoreDescriptor desc,
-      final CloudState state, final String shardZkNodeName) {
+      final ClusterState state, final String shardZkNodeName) {
 
     final CloudDescriptor cloudDesc = desc.getCloudDescriptor();
     
@@ -941,7 +941,7 @@ public final class ZkController {
     final String shardZkNodeName = getNodeName() + "_" + coreName;
     int retryCount = 120;
     while (retryCount-- > 0) {
-      final String shardId = zkStateReader.getCloudState().getShardId(
+      final String shardId = zkStateReader.getClusterState().getShardId(
           shardZkNodeName);
       if (shardId != null) {
         return shardId;
@@ -1009,7 +1009,7 @@ public final class ZkController {
     // this also gets us our assigned shard id if it was not specified
     publish(cd, ZkStateReader.DOWN); 
     String shardZkNodeName = getCoreNodeName(cd);
-    if (cd.getCloudDescriptor().getShardId() == null && needsToBeAssignedShardId(cd, zkStateReader.getCloudState(), shardZkNodeName)) {
+    if (cd.getCloudDescriptor().getShardId() == null && needsToBeAssignedShardId(cd, zkStateReader.getClusterState(), shardZkNodeName)) {
       String shardId;
       shardId = doGetShardIdProcess(cd.getName(), cd.getCloudDescriptor());
       cd.getCloudDescriptor().setShardId(shardId);
diff --git a/solr/core/src/java/org/apache/solr/handler/admin/CollectionsHandler.java b/solr/core/src/java/org/apache/solr/handler/admin/CollectionsHandler.java
index 3687db7..25f1d0e 100644
--- a/solr/core/src/java/org/apache/solr/handler/admin/CollectionsHandler.java
+++ b/solr/core/src/java/org/apache/solr/handler/admin/CollectionsHandler.java
@@ -26,7 +26,7 @@ import org.apache.solr.client.solrj.request.CoreAdminRequest.RequestSyncShard;
 import org.apache.solr.cloud.Overseer;
 import org.apache.solr.cloud.OverseerCollectionProcessor;
 import org.apache.solr.common.SolrException;
-import org.apache.solr.common.cloud.CloudState;
+import org.apache.solr.common.cloud.ClusterState;
 import org.apache.solr.common.cloud.ZkCoreNodeProps;
 import org.apache.solr.common.cloud.ZkNodeProps;
 import org.apache.solr.common.cloud.ZkStateReader;
@@ -141,9 +141,9 @@ public class CollectionsHandler extends RequestHandlerBase {
     String collection = req.getParams().required().get("collection");
     String shard = req.getParams().required().get("shard");
     
-    CloudState cloudState = coreContainer.getZkController().getCloudState();
+    ClusterState clusterState = coreContainer.getZkController().getClusterState();
     
-    ZkNodeProps leaderProps = cloudState.getLeader(collection, shard);
+    ZkNodeProps leaderProps = clusterState.getLeader(collection, shard);
     ZkCoreNodeProps nodeProps = new ZkCoreNodeProps(leaderProps);
     
     HttpSolrServer server = new HttpSolrServer(nodeProps.getBaseUrl());
diff --git a/solr/core/src/java/org/apache/solr/handler/admin/CoreAdminHandler.java b/solr/core/src/java/org/apache/solr/handler/admin/CoreAdminHandler.java
index 92f3cfa..aa05e84 100644
--- a/solr/core/src/java/org/apache/solr/handler/admin/CoreAdminHandler.java
+++ b/solr/core/src/java/org/apache/solr/handler/admin/CoreAdminHandler.java
@@ -35,7 +35,7 @@ import org.apache.solr.cloud.SyncStrategy;
 import org.apache.solr.cloud.ZkController;
 import org.apache.solr.common.SolrException;
 import org.apache.solr.common.SolrException.ErrorCode;
-import org.apache.solr.common.cloud.CloudState;
+import org.apache.solr.common.cloud.ClusterState;
 import org.apache.solr.common.cloud.Slice;
 import org.apache.solr.common.cloud.ZkNodeProps;
 import org.apache.solr.common.cloud.ZkStateReader;
@@ -768,16 +768,16 @@ public class CoreAdminHandler extends RequestHandlerBase {
           // to accept updates
           CloudDescriptor cloudDescriptor = core.getCoreDescriptor()
               .getCloudDescriptor();
-          CloudState cloudState = coreContainer.getZkController()
-              .getCloudState();
+          ClusterState clusterState = coreContainer.getZkController()
+              .getClusterState();
           String collection = cloudDescriptor.getCollectionName();
-          Slice slice = cloudState.getSlice(collection,
+          Slice slice = clusterState.getSlice(collection,
               cloudDescriptor.getShardId());
           if (slice != null) {
             ZkNodeProps nodeProps = slice.getShards().get(coreNodeName);
             if (nodeProps != null) {
               state = nodeProps.get(ZkStateReader.STATE_PROP);
-              live = cloudState.liveNodesContain(nodeName);
+              live = clusterState.liveNodesContain(nodeName);
               if (nodeProps != null && state.equals(waitForState)) {
                 if (checkLive == null) {
                   break;
diff --git a/solr/core/src/java/org/apache/solr/handler/component/HttpShardHandler.java b/solr/core/src/java/org/apache/solr/handler/component/HttpShardHandler.java
index d88a683..e38abe8 100644
--- a/solr/core/src/java/org/apache/solr/handler/component/HttpShardHandler.java
+++ b/solr/core/src/java/org/apache/solr/handler/component/HttpShardHandler.java
@@ -41,7 +41,7 @@ import org.apache.solr.cloud.CloudDescriptor;
 import org.apache.solr.cloud.ZkController;
 import org.apache.solr.common.SolrException;
 import org.apache.solr.common.SolrException.ErrorCode;
-import org.apache.solr.common.cloud.CloudState;
+import org.apache.solr.common.cloud.ClusterState;
 import org.apache.solr.common.cloud.Slice;
 import org.apache.solr.common.cloud.ZkCoreNodeProps;
 import org.apache.solr.common.cloud.ZkNodeProps;
@@ -255,7 +255,7 @@ public class HttpShardHandler extends ShardHandler {
     if (rb.isDistrib) {
       // since the cost of grabbing cloud state is still up in the air, we grab it only
       // if we need it.
-      CloudState cloudState = null;
+      ClusterState clusterState = null;
       Map<String,Slice> slices = null;
       CoreDescriptor coreDescriptor = req.getCore().getCoreDescriptor();
       CloudDescriptor cloudDescriptor = coreDescriptor.getCloudDescriptor();
@@ -280,7 +280,7 @@ public class HttpShardHandler extends ShardHandler {
       } else if (zkController != null) {
         // we weren't provided with a list of slices to query, so find the list that will cover the complete index
 
-        cloudState =  zkController.getCloudState();
+        clusterState =  zkController.getClusterState();
 
         // This can be more efficient... we only record the name, even though we
         // have the shard info we need in the next step of mapping slice->shards
@@ -301,12 +301,12 @@ public class HttpShardHandler extends ShardHandler {
           // cloud state and add them to the Map 'slices'.
           for (int i = 0; i < collectionList.size(); i++) {
             String collection = collectionList.get(i);
-            ClientUtils.appendMap(collection, slices, cloudState.getSlices(collection));
+            ClientUtils.appendMap(collection, slices, clusterState.getSlices(collection));
           }
         } else {
           // If no collections were specified, default to the collection for
           // this core.
-          slices = cloudState.getSlices(cloudDescriptor.getCollectionName());
+          slices = clusterState.getSlices(cloudDescriptor.getCollectionName());
           if (slices == null) {
             throw new SolrException(ErrorCode.BAD_REQUEST,
                 "Could not find collection:"
@@ -334,9 +334,9 @@ public class HttpShardHandler extends ShardHandler {
       if (zkController != null) {
         for (int i=0; i<rb.shards.length; i++) {
           if (rb.shards[i] == null) {
-            if (cloudState == null) {
-              cloudState =  zkController.getCloudState();
-              slices = cloudState.getSlices(cloudDescriptor.getCollectionName());
+            if (clusterState == null) {
+              clusterState =  zkController.getClusterState();
+              slices = clusterState.getSlices(cloudDescriptor.getCollectionName());
             }
             String sliceName = rb.slices[i];
 
@@ -353,7 +353,7 @@ public class HttpShardHandler extends ShardHandler {
             Map<String, ZkNodeProps> sliceShards = slice.getShards();
 
             // For now, recreate the | delimited list of equivalent servers
-            Set<String> liveNodes = cloudState.getLiveNodes();
+            Set<String> liveNodes = clusterState.getLiveNodes();
             StringBuilder sliceShardsStr = new StringBuilder();
             boolean first = true;
             for (ZkNodeProps nodeProps : sliceShards.values()) {
diff --git a/solr/core/src/java/org/apache/solr/handler/component/RealTimeGetComponent.java b/solr/core/src/java/org/apache/solr/handler/component/RealTimeGetComponent.java
index 5efeff5..65d17ec 100644
--- a/solr/core/src/java/org/apache/solr/handler/component/RealTimeGetComponent.java
+++ b/solr/core/src/java/org/apache/solr/handler/component/RealTimeGetComponent.java
@@ -352,14 +352,14 @@ public class RealTimeGetComponent extends SearchComponent
 
       String collection = cloudDescriptor.getCollectionName();
 
-      CloudState cloudState = zkController.getCloudState();
+      ClusterState clusterState = zkController.getClusterState();
       
       Map<String, List<String>> shardToId = new HashMap<String, List<String>>();
       for (String id : allIds) {
         BytesRef br = new BytesRef();
         sf.getType().readableToIndexed(id, br);
         int hash = Hash.murmurhash3_x86_32(br.bytes, br.offset, br.length, 0);
-        String shard = cloudState.getShard(hash,  collection);
+        String shard = clusterState.getShard(hash,  collection);
 
         List<String> idsForShard = shardToId.get(shard);
         if (idsForShard == null) {
diff --git a/solr/core/src/java/org/apache/solr/servlet/SolrDispatchFilter.java b/solr/core/src/java/org/apache/solr/servlet/SolrDispatchFilter.java
index 2407084..bd10217 100644
--- a/solr/core/src/java/org/apache/solr/servlet/SolrDispatchFilter.java
+++ b/solr/core/src/java/org/apache/solr/servlet/SolrDispatchFilter.java
@@ -39,7 +39,7 @@ import javax.servlet.http.HttpServletRequest;
 import javax.servlet.http.HttpServletResponse;
 
 import org.apache.solr.common.SolrException;
-import org.apache.solr.common.cloud.CloudState;
+import org.apache.solr.common.cloud.ClusterState;
 import org.apache.solr.common.cloud.Slice;
 import org.apache.solr.common.cloud.ZkNodeProps;
 import org.apache.solr.common.cloud.ZkStateReader;
@@ -315,8 +315,8 @@ public class SolrDispatchFilter implements Filter
     String collection = corename;
     ZkStateReader zkStateReader = cores.getZkController().getZkStateReader();
     
-    CloudState cloudState = zkStateReader.getCloudState();
-    Map<String,Slice> slices = cloudState.getSlices(collection);
+    ClusterState clusterState = zkStateReader.getClusterState();
+    Map<String,Slice> slices = clusterState.getSlices(collection);
     if (slices == null) {
       return null;
     }
@@ -326,7 +326,7 @@ public class SolrDispatchFilter implements Filter
     done:
     for (Entry<String,Slice> entry : entries) {
       // first see if we have the leader
-      ZkNodeProps leaderProps = cloudState.getLeader(collection, entry.getKey());
+      ZkNodeProps leaderProps = clusterState.getLeader(collection, entry.getKey());
       if (leaderProps != null) {
         core = checkProps(cores, path, leaderProps);
       }
diff --git a/solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor.java b/solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor.java
index 434a32a..3ffff9d 100644
--- a/solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor.java
+++ b/solr/core/src/java/org/apache/solr/update/processor/DistributedUpdateProcessor.java
@@ -36,7 +36,7 @@ import org.apache.solr.common.SolrException;
 import org.apache.solr.common.SolrException.ErrorCode;
 import org.apache.solr.common.SolrInputDocument;
 import org.apache.solr.common.SolrInputField;
-import org.apache.solr.common.cloud.CloudState;
+import org.apache.solr.common.cloud.ClusterState;
 import org.apache.solr.common.cloud.Slice;
 import org.apache.solr.common.cloud.ZkCoreNodeProps;
 import org.apache.solr.common.cloud.ZkNodeProps;
@@ -160,7 +160,7 @@ public class DistributedUpdateProcessor extends UpdateRequestProcessor {
     this.zkEnabled  = coreDesc.getCoreContainer().isZooKeeperAware();
     zkController = req.getCore().getCoreDescriptor().getCoreContainer().getZkController();
     if (zkEnabled) {
-      numNodes =  zkController.getZkStateReader().getCloudState().getLiveNodes().size();
+      numNodes =  zkController.getZkStateReader().getClusterState().getLiveNodes().size();
     }
     //this.rsp = reqInfo != null ? reqInfo.getRsp() : null;
 
@@ -181,13 +181,13 @@ public class DistributedUpdateProcessor extends UpdateRequestProcessor {
     // if we are in zk mode...
     if (zkEnabled) {
       // set num nodes
-      numNodes = zkController.getCloudState().getLiveNodes().size();
+      numNodes = zkController.getClusterState().getLiveNodes().size();
       
       // the leader is...
       // TODO: if there is no leader, wait and look again
       // TODO: we are reading the leader from zk every time - we should cache
       // this and watch for changes?? Just pull it from ZkController cluster state probably?
-      String shardId = getShard(hash, collection, zkController.getCloudState()); // get the right shard based on the hash...
+      String shardId = getShard(hash, collection, zkController.getClusterState()); // get the right shard based on the hash...
 
       try {
         // TODO: if we find out we cannot talk to zk anymore, we should probably realize we are not
@@ -252,11 +252,11 @@ public class DistributedUpdateProcessor extends UpdateRequestProcessor {
   }
 
 
-  private String getShard(int hash, String collection, CloudState cloudState) {
+  private String getShard(int hash, String collection, ClusterState clusterState) {
     // ranges should be part of the cloud state and eventually gotten from zk
 
     // get the shard names
-    return cloudState.getShard(hash, collection);
+    return clusterState.getShard(hash, collection);
   }
 
   // used for deleteByQuery to get the list of nodes this leader should forward to
@@ -698,11 +698,11 @@ public class DistributedUpdateProcessor extends UpdateRequestProcessor {
     if (zkEnabled && DistribPhase.NONE == phase) {
       boolean leaderForAnyShard = false;  // start off by assuming we are not a leader for any shard
 
-      Map<String,Slice> slices = zkController.getCloudState().getSlices(collection);
+      Map<String,Slice> slices = zkController.getClusterState().getSlices(collection);
       if (slices == null) {
         throw new SolrException(ErrorCode.BAD_REQUEST,
             "Cannot find collection:" + collection + " in "
-                + zkController.getCloudState().getCollections());
+                + zkController.getClusterState().getCollections());
       }
 
       ModifiableSolrParams params = new ModifiableSolrParams(req.getParams());
@@ -997,13 +997,13 @@ public class DistributedUpdateProcessor extends UpdateRequestProcessor {
 
   
   private List<Node> getCollectionUrls(SolrQueryRequest req, String collection, String shardZkNodeName) {
-    CloudState cloudState = req.getCore().getCoreDescriptor()
-        .getCoreContainer().getZkController().getCloudState();
+    ClusterState clusterState = req.getCore().getCoreDescriptor()
+        .getCoreContainer().getZkController().getClusterState();
     List<Node> urls = new ArrayList<Node>();
-    Map<String,Slice> slices = cloudState.getSlices(collection);
+    Map<String,Slice> slices = clusterState.getSlices(collection);
     if (slices == null) {
       throw new ZooKeeperException(ErrorCode.BAD_REQUEST,
-          "Could not find collection in zk: " + cloudState);
+          "Could not find collection in zk: " + clusterState);
     }
     for (Map.Entry<String,Slice> sliceEntry : slices.entrySet()) {
       Slice replicas = slices.get(sliceEntry.getKey());
@@ -1012,7 +1012,7 @@ public class DistributedUpdateProcessor extends UpdateRequestProcessor {
       
       for (Entry<String,ZkNodeProps> entry : shardMap.entrySet()) {
         ZkCoreNodeProps nodeProps = new ZkCoreNodeProps(entry.getValue());
-        if (cloudState.liveNodesContain(nodeProps.getNodeName()) && !entry.getKey().equals(shardZkNodeName)) {
+        if (clusterState.liveNodesContain(nodeProps.getNodeName()) && !entry.getKey().equals(shardZkNodeName)) {
           urls.add(new StdNode(nodeProps));
         }
       }
diff --git a/solr/core/src/test/org/apache/solr/cloud/AbstractDistributedZkTestCase.java b/solr/core/src/test/org/apache/solr/cloud/AbstractDistributedZkTestCase.java
index 8e2ba33..bc73ebc 100644
--- a/solr/core/src/test/org/apache/solr/cloud/AbstractDistributedZkTestCase.java
+++ b/solr/core/src/test/org/apache/solr/cloud/AbstractDistributedZkTestCase.java
@@ -24,7 +24,7 @@ import java.util.concurrent.atomic.AtomicInteger;
 import org.apache.commons.io.FileUtils;
 import org.apache.solr.BaseDistributedSearchTestCase;
 import org.apache.solr.client.solrj.embedded.JettySolrRunner;
-import org.apache.solr.common.cloud.CloudState;
+import org.apache.solr.common.cloud.ClusterState;
 import org.apache.solr.common.cloud.Slice;
 import org.apache.solr.common.cloud.SolrZkClient;
 import org.apache.solr.common.cloud.ZkNodeProps;
@@ -125,21 +125,21 @@ public abstract class AbstractDistributedZkTestCase extends BaseDistributedSearc
     while (cont) {
       if (verbose) System.out.println("-");
       boolean sawLiveRecovering = false;
-      zkStateReader.updateCloudState(true);
-      CloudState cloudState = zkStateReader.getCloudState();
-      Map<String,Slice> slices = cloudState.getSlices(collection);
+      zkStateReader.updateClusterState(true);
+      ClusterState clusterState = zkStateReader.getClusterState();
+      Map<String,Slice> slices = clusterState.getSlices(collection);
       for (Map.Entry<String,Slice> entry : slices.entrySet()) {
         Map<String,ZkNodeProps> shards = entry.getValue().getShards();
         for (Map.Entry<String,ZkNodeProps> shard : shards.entrySet()) {
           if (verbose) System.out.println("rstate:"
               + shard.getValue().get(ZkStateReader.STATE_PROP)
               + " live:"
-              + cloudState.liveNodesContain(shard.getValue().get(
+              + clusterState.liveNodesContain(shard.getValue().get(
                   ZkStateReader.NODE_NAME_PROP)));
           String state = shard.getValue().get(ZkStateReader.STATE_PROP);
           if ((state.equals(ZkStateReader.RECOVERING) || state
               .equals(ZkStateReader.SYNC) || state.equals(ZkStateReader.DOWN))
-              && cloudState.liveNodesContain(shard.getValue().get(
+              && clusterState.liveNodesContain(shard.getValue().get(
                   ZkStateReader.NODE_NAME_PROP))) {
             sawLiveRecovering = true;
           }
@@ -168,9 +168,9 @@ public abstract class AbstractDistributedZkTestCase extends BaseDistributedSearc
   protected void assertAllActive(String collection,ZkStateReader zkStateReader)
       throws KeeperException, InterruptedException {
 
-      zkStateReader.updateCloudState(true);
-      CloudState cloudState = zkStateReader.getCloudState();
-      Map<String,Slice> slices = cloudState.getSlices(collection);
+      zkStateReader.updateClusterState(true);
+      ClusterState clusterState = zkStateReader.getClusterState();
+      Map<String,Slice> slices = clusterState.getSlices(collection);
       if (slices == null) {
         throw new IllegalArgumentException("Cannot find collection:" + collection);
       }
diff --git a/solr/core/src/test/org/apache/solr/cloud/BasicDistributedZkTest.java b/solr/core/src/test/org/apache/solr/cloud/BasicDistributedZkTest.java
index 83380be..cfc723e 100644
--- a/solr/core/src/test/org/apache/solr/cloud/BasicDistributedZkTest.java
+++ b/solr/core/src/test/org/apache/solr/cloud/BasicDistributedZkTest.java
@@ -57,7 +57,7 @@ import org.apache.solr.common.SolrDocument;
 import org.apache.solr.common.SolrException;
 import org.apache.solr.common.SolrException.ErrorCode;
 import org.apache.solr.common.SolrInputDocument;
-import org.apache.solr.common.cloud.CloudState;
+import org.apache.solr.common.cloud.ClusterState;
 import org.apache.solr.common.cloud.Slice;
 import org.apache.solr.common.cloud.ZkCoreNodeProps;
 import org.apache.solr.common.cloud.ZkNodeProps;
@@ -451,7 +451,7 @@ public class BasicDistributedZkTest extends AbstractDistributedZkTestCase {
   private void collectStartTimes(String collectionName,
       Map<String,Long> urlToTime) throws SolrServerException, IOException {
     Map<String,Map<String,Slice>> collections = solrj.getZkStateReader()
-        .getCloudState().getCollectionStates();
+        .getClusterState().getCollectionStates();
     if (collections.containsKey(collectionName)) {
       Map<String,Slice> slices = collections.get(collectionName);
 
@@ -478,8 +478,8 @@ public class BasicDistributedZkTest extends AbstractDistributedZkTestCase {
   }
 
   private String getUrlFromZk(String collection) {
-    CloudState cloudState = solrj.getZkStateReader().getCloudState();
-    Map<String,Slice> slices = cloudState.getCollectionStates().get(collection);
+    ClusterState clusterState = solrj.getZkStateReader().getClusterState();
+    Map<String,Slice> slices = clusterState.getCollectionStates().get(collection);
     
     if (slices == null) {
       throw new SolrException(ErrorCode.BAD_REQUEST, "Could not find collection:" + collection);
@@ -491,7 +491,7 @@ public class BasicDistributedZkTest extends AbstractDistributedZkTestCase {
       Set<Map.Entry<String,ZkNodeProps>> shardEntries = shards.entrySet();
       for (Map.Entry<String,ZkNodeProps> shardEntry : shardEntries) {
         final ZkNodeProps node = shardEntry.getValue();
-        if (cloudState.liveNodesContain(node.get(ZkStateReader.NODE_NAME_PROP))) {
+        if (clusterState.liveNodesContain(node.get(ZkStateReader.NODE_NAME_PROP))) {
           return new ZkCoreNodeProps(node).getCoreUrl();
         }
       }
@@ -533,9 +533,9 @@ public class BasicDistributedZkTest extends AbstractDistributedZkTestCase {
     boolean found = false;
     boolean sliceMatch = false;
     while (System.currentTimeMillis() < timeoutAt) {
-      solrj.getZkStateReader().updateCloudState(true);
-      CloudState cloudState = solrj.getZkStateReader().getCloudState();
-      Map<String,Map<String,Slice>> collections = cloudState
+      solrj.getZkStateReader().updateClusterState(true);
+      ClusterState clusterState = solrj.getZkStateReader().getClusterState();
+      Map<String,Map<String,Slice>> collections = clusterState
           .getCollectionStates();
       if (collections.containsKey(collectionName)) {
         Map<String,Slice> slices = collections.get(collectionName);
@@ -581,9 +581,9 @@ public class BasicDistributedZkTest extends AbstractDistributedZkTestCase {
     long timeoutAt = System.currentTimeMillis() + 15000;
     boolean found = true;
     while (System.currentTimeMillis() < timeoutAt) {
-      solrj.getZkStateReader().updateCloudState(true);
-      CloudState cloudState = solrj.getZkStateReader().getCloudState();
-      Map<String,Map<String,Slice>> collections = cloudState
+      solrj.getZkStateReader().updateClusterState(true);
+      ClusterState clusterState = solrj.getZkStateReader().getClusterState();
+      Map<String,Map<String,Slice>> collections = clusterState
           .getCollectionStates();
       if (!collections.containsKey(collectionName)) {
         found = false;
@@ -773,8 +773,8 @@ public class BasicDistributedZkTest extends AbstractDistributedZkTestCase {
     
     // we added a role of none on these creates - check for it
     ZkStateReader zkStateReader = solrj.getZkStateReader();
-    zkStateReader.updateCloudState(true);
-    Map<String,Slice> slices = zkStateReader.getCloudState().getSlices(oneInstanceCollection2);
+    zkStateReader.updateClusterState(true);
+    Map<String,Slice> slices = zkStateReader.getClusterState().getSlices(oneInstanceCollection2);
     assertNotNull(slices);
     String roles = slices.get("slice1").getShards().values().iterator().next().get(ZkStateReader.ROLES_PROP);
     assertEquals("none", roles);
diff --git a/solr/core/src/test/org/apache/solr/cloud/BasicZkTest.java b/solr/core/src/test/org/apache/solr/cloud/BasicZkTest.java
index fe73789..8fab323 100644
--- a/solr/core/src/test/org/apache/solr/cloud/BasicZkTest.java
+++ b/solr/core/src/test/org/apache/solr/cloud/BasicZkTest.java
@@ -126,8 +126,8 @@ public class BasicZkTest extends AbstractZkTestCase {
     
     // ensure zk still thinks node is up
     assertTrue(
-        zkController.getCloudState().getLiveNodes().toString(),
-        zkController.getCloudState().liveNodesContain(
+        zkController.getClusterState().getLiveNodes().toString(),
+        zkController.getClusterState().liveNodesContain(
             zkController.getNodeName()));
 
     // test maxint
diff --git a/solr/core/src/test/org/apache/solr/cloud/ChaosMonkey.java b/solr/core/src/test/org/apache/solr/cloud/ChaosMonkey.java
index eb68fdb..2c15bdd 100644
--- a/solr/core/src/test/org/apache/solr/cloud/ChaosMonkey.java
+++ b/solr/core/src/test/org/apache/solr/cloud/ChaosMonkey.java
@@ -232,7 +232,7 @@ public class ChaosMonkey {
   }
 
   private String getRandomSlice() {
-    Map<String,Slice> slices = zkStateReader.getCloudState().getSlices(collection);
+    Map<String,Slice> slices = zkStateReader.getClusterState().getSlices(collection);
     
     List<String> sliceKeyList = new ArrayList<String>(slices.size());
     sliceKeyList.addAll(slices.keySet());
@@ -259,9 +259,9 @@ public class ChaosMonkey {
       boolean running = true;
       
       // get latest cloud state
-      zkStateReader.updateCloudState(true);
+      zkStateReader.updateClusterState(true);
       
-      Slice theShards = zkStateReader.getCloudState().getSlices(collection)
+      Slice theShards = zkStateReader.getClusterState().getSlices(collection)
           .get(slice);
       
       ZkNodeProps props = theShards.getShards().get(cloudJetty.coreNodeName);
@@ -275,19 +275,19 @@ public class ChaosMonkey {
       
       if (!cloudJetty.jetty.isRunning()
           || !state.equals(ZkStateReader.ACTIVE)
-          || !zkStateReader.getCloudState().liveNodesContain(nodeName)) {
+          || !zkStateReader.getClusterState().liveNodesContain(nodeName)) {
         running = false;
       }
       
       if (cloudJetty.jetty.isRunning()
           && state.equals(ZkStateReader.RECOVERING)
-          && zkStateReader.getCloudState().liveNodesContain(nodeName)) {
+          && zkStateReader.getClusterState().liveNodesContain(nodeName)) {
         numRecovering++;
       }
       
       if (cloudJetty.jetty.isRunning()
           && state.equals(ZkStateReader.ACTIVE)
-          && zkStateReader.getCloudState().liveNodesContain(nodeName)) {
+          && zkStateReader.getClusterState().liveNodesContain(nodeName)) {
         numActive++;
       }
       
@@ -336,7 +336,7 @@ public class ChaosMonkey {
   
   public SolrServer getRandomClient(String slice) throws KeeperException, InterruptedException {
     // get latest cloud state
-    zkStateReader.updateCloudState(true);
+    zkStateReader.updateClusterState(true);
 
     // get random shard
     List<SolrServer> clients = shardToClient.get(slice);
diff --git a/solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest.java b/solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest.java
index 664f6d2..2bb89db 100644
--- a/solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest.java
+++ b/solr/core/src/test/org/apache/solr/cloud/ChaosMonkeyNothingIsSafeTest.java
@@ -148,8 +148,8 @@ public class ChaosMonkeyNothingIsSafeTest extends FullSolrCloudTest {
       
       // TODO: assert we didnt kill everyone
       
-      zkStateReader.updateCloudState(true);
-      assertTrue(zkStateReader.getCloudState().getLiveNodes().size() > 0);
+      zkStateReader.updateClusterState(true);
+      assertTrue(zkStateReader.getClusterState().getLiveNodes().size() > 0);
       
       checkShardConsistency(false, true);
       
diff --git a/solr/core/src/test/org/apache/solr/cloud/CloudStateTest.java b/solr/core/src/test/org/apache/solr/cloud/CloudStateTest.java
deleted file mode 100644
index ef413c3..0000000
--- a/solr/core/src/test/org/apache/solr/cloud/CloudStateTest.java
+++ /dev/null
@@ -1,78 +0,0 @@
-package org.apache.solr.cloud;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to You under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * 
- * http://www.apache.org/licenses/LICENSE-2.0
- * 
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.Map;
-import java.util.Set;
-
-import org.apache.solr.SolrTestCaseJ4;
-import org.apache.solr.common.cloud.CloudState;
-import org.apache.solr.common.cloud.Slice;
-import org.apache.solr.common.cloud.ZkNodeProps;
-import org.apache.solr.common.cloud.ZkStateReader;
-import org.junit.Test;
-
-public class CloudStateTest extends SolrTestCaseJ4 {
-  @Test
-  public void testStoreAndRead() throws Exception {
-    Map<String,Map<String,Slice>> collectionStates = new HashMap<String,Map<String,Slice>>();
-    Set<String> liveNodes = new HashSet<String>();
-    liveNodes.add("node1");
-    liveNodes.add("node2");
-    
-    Map<String,Slice> slices = new HashMap<String,Slice>();
-    Map<String,ZkNodeProps> sliceToProps = new HashMap<String,ZkNodeProps>();
-    Map<String,String> props = new HashMap<String,String>();
-
-    props.put("prop1", "value");
-    props.put("prop2", "value2");
-    ZkNodeProps zkNodeProps = new ZkNodeProps(props);
-    sliceToProps.put("node1", zkNodeProps);
-    Slice slice = new Slice("shard1", sliceToProps);
-    slices.put("shard1", slice);
-    Slice slice2 = new Slice("shard2", sliceToProps);
-    slices.put("shard2", slice2);
-    collectionStates.put("collection1", slices);
-    collectionStates.put("collection2", slices);
-    
-    CloudState cloudState = new CloudState(liveNodes, collectionStates);
-    byte[] bytes = ZkStateReader.toJSON(cloudState);
-    
-    CloudState loadedCloudState = CloudState.load(bytes, liveNodes);
-    
-    assertEquals("Provided liveNodes not used properly", 2, loadedCloudState
-        .getLiveNodes().size());
-    assertEquals("No collections found", 2, loadedCloudState.getCollections().size());
-    assertEquals("Poperties not copied properly", zkNodeProps.get("prop1"), loadedCloudState.getSlice("collection1", "shard1").getShards().get("node1").get("prop1"));
-    assertEquals("Poperties not copied properly", zkNodeProps.get("prop2"), loadedCloudState.getSlice("collection1", "shard1").getShards().get("node1").get("prop2"));
-
-    loadedCloudState = CloudState.load(new byte[0], liveNodes);
-    
-    assertEquals("Provided liveNodes not used properly", 2, loadedCloudState
-        .getLiveNodes().size());
-    assertEquals("Should not have collections", 0, loadedCloudState.getCollections().size());
-
-    loadedCloudState = CloudState.load((byte[])null, liveNodes);
-    
-    assertEquals("Provided liveNodes not used properly", 2, loadedCloudState
-        .getLiveNodes().size());
-    assertEquals("Should not have collections", 0, loadedCloudState.getCollections().size());
-  }
-}
diff --git a/solr/core/src/test/org/apache/solr/cloud/CloudStateUpdateTest.java b/solr/core/src/test/org/apache/solr/cloud/CloudStateUpdateTest.java
deleted file mode 100644
index 79dcad0..0000000
--- a/solr/core/src/test/org/apache/solr/cloud/CloudStateUpdateTest.java
+++ /dev/null
@@ -1,262 +0,0 @@
-package org.apache.solr.cloud;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.File;
-import java.util.HashMap;
-import java.util.Map;
-import java.util.Set;
-
-import org.apache.lucene.util.LuceneTestCase.Slow;
-import org.apache.solr.SolrTestCaseJ4;
-import org.apache.solr.common.cloud.CloudState;
-import org.apache.solr.common.cloud.Slice;
-import org.apache.solr.common.cloud.SolrZkClient;
-import org.apache.solr.common.cloud.ZkNodeProps;
-import org.apache.solr.common.cloud.ZkStateReader;
-import org.apache.solr.core.CoreContainer;
-import org.apache.solr.core.CoreContainer.Initializer;
-import org.apache.solr.core.CoreDescriptor;
-import org.apache.solr.core.SolrCore;
-import org.apache.zookeeper.CreateMode;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-@Slow
-public class CloudStateUpdateTest extends SolrTestCaseJ4  {
-  protected static Logger log = LoggerFactory
-      .getLogger(AbstractZkTestCase.class);
-
-  private static final boolean VERBOSE = false;
-
-  protected ZkTestServer zkServer;
-
-  protected String zkDir;
-
-  private CoreContainer container1;
-
-  private CoreContainer container2;
-
-  private CoreContainer container3;
-
-  private File dataDir1;
-
-  private File dataDir2;
-
-  private File dataDir3;
-  
-  private File dataDir4;
-
-  private Initializer init2;
-  
-  @BeforeClass
-  public static void beforeClass() {
-    System.setProperty("solrcloud.skip.autorecovery", "true");
-  }
-  
-  @AfterClass
-  public static void afterClass() throws InterruptedException {
-    System.clearProperty("solrcloud.skip.autorecovery");
-  }
-
-  @Override
-  public void setUp() throws Exception {
-    super.setUp();
-    createTempDir();
-    System.setProperty("zkClientTimeout", "3000");
-
-    zkDir = dataDir.getAbsolutePath() + File.separator
-        + "zookeeper/server1/data";
-    zkServer = new ZkTestServer(zkDir);
-    zkServer.run();
-    System.setProperty("zkHost", zkServer.getZkAddress());
-    AbstractZkTestCase.buildZooKeeper(zkServer.getZkHost(), zkServer
-        .getZkAddress(), "solrconfig.xml", "schema.xml");
-    
-    log.info("####SETUP_START " + getTestName());
-    dataDir1 = new File(dataDir + File.separator + "data1");
-    dataDir1.mkdirs();
-    
-    dataDir2 = new File(dataDir + File.separator + "data2");
-    dataDir2.mkdirs();
-    
-    dataDir3 = new File(dataDir + File.separator + "data3");
-    dataDir3.mkdirs();
-    
-    dataDir4 = new File(dataDir + File.separator + "data4");
-    dataDir4.mkdirs();
-    
-    // set some system properties for use by tests
-    System.setProperty("solr.test.sys.prop1", "propone");
-    System.setProperty("solr.test.sys.prop2", "proptwo");
-    
-    System.setProperty("solr.solr.home", TEST_HOME());
-    System.setProperty("hostPort", "1661");
-    CoreContainer.Initializer init1 = new CoreContainer.Initializer();
-    System.setProperty("solr.data.dir", CloudStateUpdateTest.this.dataDir1.getAbsolutePath());
-    container1 = init1.initialize();
-    System.clearProperty("hostPort");
-    
-    System.setProperty("hostPort", "1662");
-    init2 = new CoreContainer.Initializer();
-    System.setProperty("solr.data.dir", CloudStateUpdateTest.this.dataDir2.getAbsolutePath());
-    container2 = init2.initialize();
-    System.clearProperty("hostPort");
-    
-    System.setProperty("hostPort", "1663");
-    CoreContainer.Initializer init3 = new CoreContainer.Initializer();
-   
-    System.setProperty("solr.data.dir", CloudStateUpdateTest.this.dataDir3.getAbsolutePath());
-    container3 = init3.initialize();
-    System.clearProperty("hostPort");
-    System.clearProperty("solr.solr.home");
-    
-    log.info("####SETUP_END " + getTestName());
-    
-  }
-
-  
-  @Test
-  public void testCoreRegistration() throws Exception {
-    System.setProperty("solrcloud.update.delay", "1");
-    
-   
-    Map<String,String> props2 = new HashMap<String,String>();
-    props2.put("configName", "conf1");
-    ZkNodeProps zkProps2 = new ZkNodeProps(props2);
-    
-    SolrZkClient zkClient = new SolrZkClient(zkServer.getZkAddress(),
-        AbstractZkTestCase.TIMEOUT);
-    zkClient.makePath(ZkStateReader.COLLECTIONS_ZKNODE + "/testcore",
-        ZkStateReader.toJSON(zkProps2), CreateMode.PERSISTENT, true);
-    zkClient.makePath(ZkStateReader.COLLECTIONS_ZKNODE + "/testcore/shards",
-        CreateMode.PERSISTENT, true);
-    zkClient.close();
-
-    CoreDescriptor dcore = new CoreDescriptor(container1, "testcore",
-        "testcore");
-    
-    dcore.setDataDir(dataDir4.getAbsolutePath());
-
-    SolrCore core = container1.create(dcore);
-    
-    container1.register(core, false);
-    
-    ZkController zkController2 = container2.getZkController();
-
-    String host = zkController2.getHostName();
-    
-    // slight pause - TODO: takes an oddly long amount of time to schedule tasks
-    // with almost no delay ...
-    CloudState cloudState2 = null;
-    Map<String,Slice> slices = null;
-    for (int i = 75; i > 0; i--) {
-      cloudState2 = zkController2.getCloudState();
-      slices = cloudState2.getSlices("testcore");
-      
-      if (slices != null && slices.containsKey("shard1")
-          && slices.get("shard1").getShards().size() > 0) {
-        break;
-      }
-      Thread.sleep(500);
-    }
-
-    assertNotNull(slices);
-    assertTrue(slices.containsKey("shard1"));
-
-    Slice slice = slices.get("shard1");
-    assertEquals("shard1", slice.getName());
-
-    Map<String,ZkNodeProps> shards = slice.getShards();
-
-    assertEquals(1, shards.size());
-
-    ZkNodeProps zkProps = shards.get(host + ":1661_solr_testcore");
-
-    assertNotNull(zkProps);
-
-    assertEquals(host + ":1661_solr", zkProps.get(ZkStateReader.NODE_NAME_PROP));
-
-    assertEquals("http://" + host + ":1661/solr", zkProps.get(ZkStateReader.BASE_URL_PROP));
-
-    Set<String> liveNodes = cloudState2.getLiveNodes();
-    assertNotNull(liveNodes);
-    assertEquals(3, liveNodes.size());
-
-    container3.shutdown();
-
-    // slight pause (15s timeout) for watch to trigger
-    for(int i = 0; i < (5 * 15); i++) {
-      if(zkController2.getCloudState().getLiveNodes().size() == 2) {
-        break;
-      }
-      Thread.sleep(200);
-    }
-
-    assertEquals(2, zkController2.getCloudState().getLiveNodes().size());
-
-    // quickly kill / start client
-
-    container2.getZkController().getZkClient().getSolrZooKeeper().getConnection()
-        .disconnect();
-    container2.shutdown();
-
-    container2 = init2.initialize();
-    
-    // pause for watch to trigger
-    for(int i = 0; i < 200; i++) {
-      if (container1.getZkController().getCloudState().liveNodesContain(
-          container2.getZkController().getNodeName())) {
-        break;
-      }
-      Thread.sleep(100);
-    }
-
-    assertTrue(container1.getZkController().getCloudState().liveNodesContain(
-        container2.getZkController().getNodeName()));
-
-    // core.close();  // this core is managed by container1 now
-  }
-
-  @Override
-  public void tearDown() throws Exception {
-    if (VERBOSE) {
-      printLayout(zkServer.getZkHost());
-    }
-    container1.shutdown();
-    container2.shutdown();
-    container3.shutdown();
-
-    zkServer.shutdown();
-    super.tearDown();
-    System.clearProperty("zkClientTimeout");
-    System.clearProperty("zkHost");
-    System.clearProperty("hostPort");
-    System.clearProperty("solrcloud.update.delay");
-  }
-  
-  private void printLayout(String zkHost) throws Exception {
-    SolrZkClient zkClient = new SolrZkClient(
-        zkHost, AbstractZkTestCase.TIMEOUT);
-    zkClient.printLayoutToStdOut();
-    zkClient.close();
-  }
-}
diff --git a/solr/core/src/test/org/apache/solr/cloud/ClusterStateTest.java b/solr/core/src/test/org/apache/solr/cloud/ClusterStateTest.java
new file mode 100644
index 0000000..30f22f1
--- /dev/null
+++ b/solr/core/src/test/org/apache/solr/cloud/ClusterStateTest.java
@@ -0,0 +1,78 @@
+package org.apache.solr.cloud;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with this
+ * work for additional information regarding copyright ownership. The ASF
+ * licenses this file to You under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ * 
+ * http://www.apache.org/licenses/LICENSE-2.0
+ * 
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+ * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+ * License for the specific language governing permissions and limitations under
+ * the License.
+ */
+
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.solr.SolrTestCaseJ4;
+import org.apache.solr.common.cloud.ClusterState;
+import org.apache.solr.common.cloud.Slice;
+import org.apache.solr.common.cloud.ZkNodeProps;
+import org.apache.solr.common.cloud.ZkStateReader;
+import org.junit.Test;
+
+public class ClusterStateTest extends SolrTestCaseJ4 {
+  @Test
+  public void testStoreAndRead() throws Exception {
+    Map<String,Map<String,Slice>> collectionStates = new HashMap<String,Map<String,Slice>>();
+    Set<String> liveNodes = new HashSet<String>();
+    liveNodes.add("node1");
+    liveNodes.add("node2");
+    
+    Map<String,Slice> slices = new HashMap<String,Slice>();
+    Map<String,ZkNodeProps> sliceToProps = new HashMap<String,ZkNodeProps>();
+    Map<String,String> props = new HashMap<String,String>();
+
+    props.put("prop1", "value");
+    props.put("prop2", "value2");
+    ZkNodeProps zkNodeProps = new ZkNodeProps(props);
+    sliceToProps.put("node1", zkNodeProps);
+    Slice slice = new Slice("shard1", sliceToProps);
+    slices.put("shard1", slice);
+    Slice slice2 = new Slice("shard2", sliceToProps);
+    slices.put("shard2", slice2);
+    collectionStates.put("collection1", slices);
+    collectionStates.put("collection2", slices);
+    
+    ClusterState clusterState = new ClusterState(liveNodes, collectionStates);
+    byte[] bytes = ZkStateReader.toJSON(clusterState);
+    
+    ClusterState loadedClusterState = ClusterState.load(bytes, liveNodes);
+    
+    assertEquals("Provided liveNodes not used properly", 2, loadedClusterState
+        .getLiveNodes().size());
+    assertEquals("No collections found", 2, loadedClusterState.getCollections().size());
+    assertEquals("Poperties not copied properly", zkNodeProps.get("prop1"), loadedClusterState.getSlice("collection1", "shard1").getShards().get("node1").get("prop1"));
+    assertEquals("Poperties not copied properly", zkNodeProps.get("prop2"), loadedClusterState.getSlice("collection1", "shard1").getShards().get("node1").get("prop2"));
+
+    loadedClusterState = ClusterState.load(new byte[0], liveNodes);
+    
+    assertEquals("Provided liveNodes not used properly", 2, loadedClusterState
+        .getLiveNodes().size());
+    assertEquals("Should not have collections", 0, loadedClusterState.getCollections().size());
+
+    loadedClusterState = ClusterState.load((byte[])null, liveNodes);
+    
+    assertEquals("Provided liveNodes not used properly", 2, loadedClusterState
+        .getLiveNodes().size());
+    assertEquals("Should not have collections", 0, loadedClusterState.getCollections().size());
+  }
+}
diff --git a/solr/core/src/test/org/apache/solr/cloud/ClusterStateUpdateTest.java b/solr/core/src/test/org/apache/solr/cloud/ClusterStateUpdateTest.java
new file mode 100644
index 0000000..bacbb5c
--- /dev/null
+++ b/solr/core/src/test/org/apache/solr/cloud/ClusterStateUpdateTest.java
@@ -0,0 +1,262 @@
+package org.apache.solr.cloud;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.File;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.lucene.util.LuceneTestCase.Slow;
+import org.apache.solr.SolrTestCaseJ4;
+import org.apache.solr.common.cloud.ClusterState;
+import org.apache.solr.common.cloud.Slice;
+import org.apache.solr.common.cloud.SolrZkClient;
+import org.apache.solr.common.cloud.ZkNodeProps;
+import org.apache.solr.common.cloud.ZkStateReader;
+import org.apache.solr.core.CoreContainer;
+import org.apache.solr.core.CoreContainer.Initializer;
+import org.apache.solr.core.CoreDescriptor;
+import org.apache.solr.core.SolrCore;
+import org.apache.zookeeper.CreateMode;
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+import org.junit.Test;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+@Slow
+public class ClusterStateUpdateTest extends SolrTestCaseJ4  {
+  protected static Logger log = LoggerFactory
+      .getLogger(AbstractZkTestCase.class);
+
+  private static final boolean VERBOSE = false;
+
+  protected ZkTestServer zkServer;
+
+  protected String zkDir;
+
+  private CoreContainer container1;
+
+  private CoreContainer container2;
+
+  private CoreContainer container3;
+
+  private File dataDir1;
+
+  private File dataDir2;
+
+  private File dataDir3;
+  
+  private File dataDir4;
+
+  private Initializer init2;
+  
+  @BeforeClass
+  public static void beforeClass() {
+    System.setProperty("solrcloud.skip.autorecovery", "true");
+  }
+  
+  @AfterClass
+  public static void afterClass() throws InterruptedException {
+    System.clearProperty("solrcloud.skip.autorecovery");
+  }
+
+  @Override
+  public void setUp() throws Exception {
+    super.setUp();
+    createTempDir();
+    System.setProperty("zkClientTimeout", "3000");
+
+    zkDir = dataDir.getAbsolutePath() + File.separator
+        + "zookeeper/server1/data";
+    zkServer = new ZkTestServer(zkDir);
+    zkServer.run();
+    System.setProperty("zkHost", zkServer.getZkAddress());
+    AbstractZkTestCase.buildZooKeeper(zkServer.getZkHost(), zkServer
+        .getZkAddress(), "solrconfig.xml", "schema.xml");
+    
+    log.info("####SETUP_START " + getTestName());
+    dataDir1 = new File(dataDir + File.separator + "data1");
+    dataDir1.mkdirs();
+    
+    dataDir2 = new File(dataDir + File.separator + "data2");
+    dataDir2.mkdirs();
+    
+    dataDir3 = new File(dataDir + File.separator + "data3");
+    dataDir3.mkdirs();
+    
+    dataDir4 = new File(dataDir + File.separator + "data4");
+    dataDir4.mkdirs();
+    
+    // set some system properties for use by tests
+    System.setProperty("solr.test.sys.prop1", "propone");
+    System.setProperty("solr.test.sys.prop2", "proptwo");
+    
+    System.setProperty("solr.solr.home", TEST_HOME());
+    System.setProperty("hostPort", "1661");
+    CoreContainer.Initializer init1 = new CoreContainer.Initializer();
+    System.setProperty("solr.data.dir", ClusterStateUpdateTest.this.dataDir1.getAbsolutePath());
+    container1 = init1.initialize();
+    System.clearProperty("hostPort");
+    
+    System.setProperty("hostPort", "1662");
+    init2 = new CoreContainer.Initializer();
+    System.setProperty("solr.data.dir", ClusterStateUpdateTest.this.dataDir2.getAbsolutePath());
+    container2 = init2.initialize();
+    System.clearProperty("hostPort");
+    
+    System.setProperty("hostPort", "1663");
+    CoreContainer.Initializer init3 = new CoreContainer.Initializer();
+   
+    System.setProperty("solr.data.dir", ClusterStateUpdateTest.this.dataDir3.getAbsolutePath());
+    container3 = init3.initialize();
+    System.clearProperty("hostPort");
+    System.clearProperty("solr.solr.home");
+    
+    log.info("####SETUP_END " + getTestName());
+    
+  }
+
+  
+  @Test
+  public void testCoreRegistration() throws Exception {
+    System.setProperty("solrcloud.update.delay", "1");
+    
+   
+    Map<String,String> props2 = new HashMap<String,String>();
+    props2.put("configName", "conf1");
+    ZkNodeProps zkProps2 = new ZkNodeProps(props2);
+    
+    SolrZkClient zkClient = new SolrZkClient(zkServer.getZkAddress(),
+        AbstractZkTestCase.TIMEOUT);
+    zkClient.makePath(ZkStateReader.COLLECTIONS_ZKNODE + "/testcore",
+        ZkStateReader.toJSON(zkProps2), CreateMode.PERSISTENT, true);
+    zkClient.makePath(ZkStateReader.COLLECTIONS_ZKNODE + "/testcore/shards",
+        CreateMode.PERSISTENT, true);
+    zkClient.close();
+
+    CoreDescriptor dcore = new CoreDescriptor(container1, "testcore",
+        "testcore");
+    
+    dcore.setDataDir(dataDir4.getAbsolutePath());
+
+    SolrCore core = container1.create(dcore);
+    
+    container1.register(core, false);
+    
+    ZkController zkController2 = container2.getZkController();
+
+    String host = zkController2.getHostName();
+    
+    // slight pause - TODO: takes an oddly long amount of time to schedule tasks
+    // with almost no delay ...
+    ClusterState clusterState2 = null;
+    Map<String,Slice> slices = null;
+    for (int i = 75; i > 0; i--) {
+      clusterState2 = zkController2.getClusterState();
+      slices = clusterState2.getSlices("testcore");
+      
+      if (slices != null && slices.containsKey("shard1")
+          && slices.get("shard1").getShards().size() > 0) {
+        break;
+      }
+      Thread.sleep(500);
+    }
+
+    assertNotNull(slices);
+    assertTrue(slices.containsKey("shard1"));
+
+    Slice slice = slices.get("shard1");
+    assertEquals("shard1", slice.getName());
+
+    Map<String,ZkNodeProps> shards = slice.getShards();
+
+    assertEquals(1, shards.size());
+
+    ZkNodeProps zkProps = shards.get(host + ":1661_solr_testcore");
+
+    assertNotNull(zkProps);
+
+    assertEquals(host + ":1661_solr", zkProps.get(ZkStateReader.NODE_NAME_PROP));
+
+    assertEquals("http://" + host + ":1661/solr", zkProps.get(ZkStateReader.BASE_URL_PROP));
+
+    Set<String> liveNodes = clusterState2.getLiveNodes();
+    assertNotNull(liveNodes);
+    assertEquals(3, liveNodes.size());
+
+    container3.shutdown();
+
+    // slight pause (15s timeout) for watch to trigger
+    for(int i = 0; i < (5 * 15); i++) {
+      if(zkController2.getClusterState().getLiveNodes().size() == 2) {
+        break;
+      }
+      Thread.sleep(200);
+    }
+
+    assertEquals(2, zkController2.getClusterState().getLiveNodes().size());
+
+    // quickly kill / start client
+
+    container2.getZkController().getZkClient().getSolrZooKeeper().getConnection()
+        .disconnect();
+    container2.shutdown();
+
+    container2 = init2.initialize();
+    
+    // pause for watch to trigger
+    for(int i = 0; i < 200; i++) {
+      if (container1.getZkController().getClusterState().liveNodesContain(
+          container2.getZkController().getNodeName())) {
+        break;
+      }
+      Thread.sleep(100);
+    }
+
+    assertTrue(container1.getZkController().getClusterState().liveNodesContain(
+        container2.getZkController().getNodeName()));
+
+    // core.close();  // don't close - this core is managed by container1 now
+  }
+
+  @Override
+  public void tearDown() throws Exception {
+    if (VERBOSE) {
+      printLayout(zkServer.getZkHost());
+    }
+    container1.shutdown();
+    container2.shutdown();
+    container3.shutdown();
+
+    zkServer.shutdown();
+    super.tearDown();
+    System.clearProperty("zkClientTimeout");
+    System.clearProperty("zkHost");
+    System.clearProperty("hostPort");
+    System.clearProperty("solrcloud.update.delay");
+  }
+  
+  private void printLayout(String zkHost) throws Exception {
+    SolrZkClient zkClient = new SolrZkClient(
+        zkHost, AbstractZkTestCase.TIMEOUT);
+    zkClient.printLayoutToStdOut();
+    zkClient.close();
+  }
+}
diff --git a/solr/core/src/test/org/apache/solr/cloud/FullSolrCloudTest.java b/solr/core/src/test/org/apache/solr/cloud/FullSolrCloudTest.java
index 23b82b1..ae076bd 100644
--- a/solr/core/src/test/org/apache/solr/cloud/FullSolrCloudTest.java
+++ b/solr/core/src/test/org/apache/solr/cloud/FullSolrCloudTest.java
@@ -44,7 +44,7 @@ import org.apache.solr.common.SolrDocument;
 import org.apache.solr.common.SolrDocumentList;
 import org.apache.solr.common.SolrException;
 import org.apache.solr.common.SolrInputDocument;
-import org.apache.solr.common.cloud.CloudState;
+import org.apache.solr.common.cloud.ClusterState;
 import org.apache.solr.common.cloud.Slice;
 import org.apache.solr.common.cloud.ZkCoreNodeProps;
 import org.apache.solr.common.cloud.ZkNodeProps;
@@ -214,11 +214,11 @@ public class FullSolrCloudTest extends AbstractDistributedZkTestCase {
     }
     
     // wait until shards have started registering...
-    while (!zkStateReader.getCloudState().getCollections()
+    while (!zkStateReader.getClusterState().getCollections()
         .contains(DEFAULT_COLLECTION)) {
       Thread.sleep(500);
     }
-    while (zkStateReader.getCloudState().getSlices(DEFAULT_COLLECTION).size() != sliceCount) {
+    while (zkStateReader.getClusterState().getSlices(DEFAULT_COLLECTION).size() != sliceCount) {
       Thread.sleep(500);
     }
     
@@ -331,7 +331,7 @@ public class FullSolrCloudTest extends AbstractDistributedZkTestCase {
   }
 
   private int getNumShards(String defaultCollection) {
-    Map<String,Slice> slices = this.zkStateReader.getCloudState().getSlices(defaultCollection);
+    Map<String,Slice> slices = this.zkStateReader.getClusterState().getSlices(defaultCollection);
     int cnt = 0;
     for (Map.Entry<String,Slice> entry : slices.entrySet()) {
       cnt += entry.getValue().getShards().size();
@@ -354,16 +354,16 @@ public class FullSolrCloudTest extends AbstractDistributedZkTestCase {
   
   protected void updateMappingsFromZk(List<JettySolrRunner> jettys,
       List<SolrServer> clients) throws Exception {
-    zkStateReader.updateCloudState(true);
+    zkStateReader.updateClusterState(true);
     cloudJettys.clear();
     shardToJetty.clear();
     
-    CloudState cloudState = zkStateReader.getCloudState();
-    Map<String,Slice> slices = cloudState.getSlices(DEFAULT_COLLECTION);
+    ClusterState clusterState = zkStateReader.getClusterState();
+    Map<String,Slice> slices = clusterState.getSlices(DEFAULT_COLLECTION);
     
     if (slices == null) {
       throw new RuntimeException("No slices found for collection "
-          + DEFAULT_COLLECTION + " in " + cloudState.getCollections());
+          + DEFAULT_COLLECTION + " in " + clusterState.getCollections());
     }
     
     List<CloudSolrServerClient> theClients = new ArrayList<CloudSolrServerClient>();
@@ -1053,7 +1053,7 @@ public class FullSolrCloudTest extends AbstractDistributedZkTestCase {
     assertEquals(
         "The client count does not match up with the shard count for slice:"
             + shard,
-        zkStateReader.getCloudState().getSlice(DEFAULT_COLLECTION, shard)
+        zkStateReader.getClusterState().getSlice(DEFAULT_COLLECTION, shard)
             .getShards().size(), solrJetties.size());
 
     SolrServer lastClient = null;
@@ -1078,7 +1078,7 @@ public class FullSolrCloudTest extends AbstractDistributedZkTestCase {
       
       boolean live = false;
       String nodeName = props.get(ZkStateReader.NODE_NAME_PROP);
-      if (zkStateReader.getCloudState().liveNodesContain(nodeName)) {
+      if (zkStateReader.getClusterState().liveNodesContain(nodeName)) {
         live = true;
       }
       if (verbose) System.err.println(" live:" + live);
@@ -1233,18 +1233,18 @@ public class FullSolrCloudTest extends AbstractDistributedZkTestCase {
     ZkStateReader zk = new ZkStateReader(zkServer.getZkAddress(), 10000,
         AbstractZkTestCase.TIMEOUT);
     Map<String,Slice> slices = null;
-    CloudState cloudState;
+    ClusterState clusterState;
     try {
       zk.createClusterStateWatchersAndUpdate();
-      cloudState = zk.getCloudState();
-      slices = cloudState.getSlices(DEFAULT_COLLECTION);
+      clusterState = zk.getClusterState();
+      slices = clusterState.getSlices(DEFAULT_COLLECTION);
     } finally {
       zk.close();
     }
     
     if (slices == null) {
       throw new RuntimeException("Could not find collection "
-          + DEFAULT_COLLECTION + " in " + cloudState.getCollections());
+          + DEFAULT_COLLECTION + " in " + clusterState.getCollections());
     }
     
     for (CloudJettyRunner cjetty : cloudJettys) {
@@ -1266,7 +1266,7 @@ public class FullSolrCloudTest extends AbstractDistributedZkTestCase {
       String currentState = cjetty.info.get(ZkStateReader.STATE_PROP);
       if (currentState != null
           && currentState.equals(ZkStateReader.ACTIVE)
-          && zkStateReader.getCloudState().liveNodesContain(
+          && zkStateReader.getClusterState().liveNodesContain(
               cjetty.info.get(ZkStateReader.NODE_NAME_PROP))) {
         SolrQuery query = new SolrQuery("*:*");
         query.set("distrib", false);
@@ -1509,7 +1509,7 @@ public class FullSolrCloudTest extends AbstractDistributedZkTestCase {
   protected void waitToSeeNotLive(ZkStateReader zkStateReader,
       CloudJettyRunner cjetty) throws InterruptedException {
     int tries = 0;
-    while (zkStateReader.getCloudState()
+    while (zkStateReader.getClusterState()
         .liveNodesContain(cjetty.info.get(ZkStateReader.NODE_NAME_PROP))) {
       if (tries++ == 120) {
         fail("Shard still reported as live in zk");
diff --git a/solr/core/src/test/org/apache/solr/cloud/OverseerTest.java b/solr/core/src/test/org/apache/solr/cloud/OverseerTest.java
index f4a4462..8802f6b 100644
--- a/solr/core/src/test/org/apache/solr/cloud/OverseerTest.java
+++ b/solr/core/src/test/org/apache/solr/cloud/OverseerTest.java
@@ -36,7 +36,7 @@ import javax.xml.parsers.ParserConfigurationException;
 
 import org.apache.lucene.util.LuceneTestCase.Slow;
 import org.apache.solr.SolrTestCaseJ4;
-import org.apache.solr.common.cloud.CloudState;
+import org.apache.solr.common.cloud.ClusterState;
 import org.apache.solr.common.cloud.Slice;
 import org.apache.solr.common.cloud.SolrZkClient;
 import org.apache.solr.common.cloud.ZkNodeProps;
@@ -153,7 +153,7 @@ public class OverseerTest extends SolrTestCaseJ4 {
     }
     
     private String getShardId(final String coreName) {
-      Map<String,Slice> slices = zkStateReader.getCloudState().getSlices(
+      Map<String,Slice> slices = zkStateReader.getClusterState().getSlices(
           collection);
       if (slices != null) {
         for (Slice slice : slices.values()) {
@@ -340,8 +340,8 @@ public class OverseerTest extends SolrTestCaseJ4 {
       
       // make sure all cores have been assigned a id in cloudstate
       for (int i = 0; i < 40; i++) {
-        reader.updateCloudState(true);
-        CloudState state = reader.getCloudState();
+        reader.updateClusterState(true);
+        ClusterState state = reader.getClusterState();
         Map<String,Slice> slices = state.getSlices("collection1");
         int count = 0;
         for (String name : slices.keySet()) {
@@ -419,8 +419,8 @@ public class OverseerTest extends SolrTestCaseJ4 {
   private void waitForCollections(ZkStateReader stateReader, String... collections) throws InterruptedException, KeeperException {
     int maxIterations = 100;
     while (0 < maxIterations--) {
-      stateReader.updateCloudState(true);
-      final CloudState state = stateReader.getCloudState();
+      stateReader.updateClusterState(true);
+      final ClusterState state = stateReader.getClusterState();
       Set<String> availableCollections = state.getCollections();
       int availableCount = 0;
       for(String requiredCollection: collections) {
@@ -431,7 +431,7 @@ public class OverseerTest extends SolrTestCaseJ4 {
         Thread.sleep(50);
       }
     }
-    log.warn("Timeout waiting for collections: " + Arrays.asList(collections) + " state:" + stateReader.getCloudState());
+    log.warn("Timeout waiting for collections: " + Arrays.asList(collections) + " state:" + stateReader.getClusterState());
   }
   
   @Test
@@ -472,8 +472,8 @@ public class OverseerTest extends SolrTestCaseJ4 {
       
       waitForCollections(reader, "collection1");
 
-      assertEquals(reader.getCloudState().toString(), ZkStateReader.RECOVERING,
-          reader.getCloudState().getSlice("collection1", "shard1").getShards()
+      assertEquals(reader.getClusterState().toString(), ZkStateReader.RECOVERING,
+          reader.getClusterState().getSlice("collection1", "shard1").getShards()
               .get("node1_core1").get(ZkStateReader.STATE_PROP));
 
       //publish node state (active)
@@ -503,7 +503,7 @@ public class OverseerTest extends SolrTestCaseJ4 {
     int maxIterations = 100;
     String coreState = null;
     while(maxIterations-->0) {
-      Slice slice = reader.getCloudState().getSlice("collection1", "shard1");
+      Slice slice = reader.getClusterState().getSlice("collection1", "shard1");
       if(slice!=null) {
         coreState = slice.getShards().get("node1_core1").get(ZkStateReader.STATE_PROP);
         if(coreState.equals(expectedState)) {
@@ -512,14 +512,14 @@ public class OverseerTest extends SolrTestCaseJ4 {
       }
       Thread.sleep(50);
     }
-    fail("Illegal state, was:" + coreState + " expected:" + expectedState + "cloudState:" + reader.getCloudState());
+    fail("Illegal state, was:" + coreState + " expected:" + expectedState + "clusterState:" + reader.getClusterState());
   }
   
   private void verifyShardLeader(ZkStateReader reader, String collection, String shard, String expectedCore) throws InterruptedException, KeeperException {
     int maxIterations = 100;
     while(maxIterations-->0) {
-      reader.updateCloudState(true); // poll state
-      ZkNodeProps props =  reader.getCloudState().getLeader(collection, shard);
+      reader.updateClusterState(true); // poll state
+      ZkNodeProps props =  reader.getClusterState().getLeader(collection, shard);
       if(props!=null) {
         if(expectedCore.equals(props.get(ZkStateReader.CORE_NAME_PROP))) {
           return;
@@ -528,7 +528,7 @@ public class OverseerTest extends SolrTestCaseJ4 {
       Thread.sleep(100);
     }
     
-    assertEquals("Unexpected shard leader coll:" + collection + " shard:" + shard, expectedCore, (reader.getCloudState().getLeader(collection, shard)!=null)?reader.getCloudState().getLeader(collection, shard).get(ZkStateReader.CORE_NAME_PROP):null);
+    assertEquals("Unexpected shard leader coll:" + collection + " shard:" + shard, expectedCore, (reader.getClusterState().getLeader(collection, shard)!=null)?reader.getClusterState().getLeader(collection, shard).get(ZkStateReader.CORE_NAME_PROP):null);
   }
 
   @Test
@@ -562,35 +562,35 @@ public class OverseerTest extends SolrTestCaseJ4 {
       waitForCollections(reader, "collection1");
       verifyStatus(reader, ZkStateReader.RECOVERING);
 
-      int version = getCloudStateVersion(controllerClient);
+      int version = getClusterStateVersion(controllerClient);
       
       mockController.publishState("core1", ZkStateReader.ACTIVE, 1);
       
-      while(version == getCloudStateVersion(controllerClient));
+      while(version == getClusterStateVersion(controllerClient));
 
       verifyStatus(reader, ZkStateReader.ACTIVE);
-      version = getCloudStateVersion(controllerClient);
+      version = getClusterStateVersion(controllerClient);
       overseerClient.close();
       Thread.sleep(1000); //wait for overseer to get killed
 
       mockController.publishState("core1", ZkStateReader.RECOVERING, 1);
-      version = getCloudStateVersion(controllerClient);
+      version = getClusterStateVersion(controllerClient);
       
       overseerClient = electNewOverseer(server.getZkAddress());
 
-      while(version == getCloudStateVersion(controllerClient));
+      while(version == getClusterStateVersion(controllerClient));
 
       verifyStatus(reader, ZkStateReader.RECOVERING);
       
-      assertEquals("Live nodes count does not match", 1, reader.getCloudState()
+      assertEquals("Live nodes count does not match", 1, reader.getClusterState()
           .getLiveNodes().size());
-      assertEquals("Shard count does not match", 1, reader.getCloudState()
+      assertEquals("Shard count does not match", 1, reader.getClusterState()
           .getSlice("collection1", "shard1").getShards().size());      
-      version = getCloudStateVersion(controllerClient);
+      version = getClusterStateVersion(controllerClient);
       mockController.publishState("core1", null,1);
-      while(version == getCloudStateVersion(controllerClient));
+      while(version == getClusterStateVersion(controllerClient));
       Thread.sleep(500);
-      assertFalse("collection1 should be gone after publishing the null state", reader.getCloudState().getCollections().contains("collection1"));
+      assertFalse("collection1 should be gone after publishing the null state", reader.getClusterState().getCollections().contains("collection1"));
     } finally {
       
       close(mockController);
@@ -740,15 +740,15 @@ public class OverseerTest extends SolrTestCaseJ4 {
 
       mockController.close();
 
-      int version = getCloudStateVersion(controllerClient);
+      int version = getClusterStateVersion(controllerClient);
       
       mockController = new MockZKController(server.getZkAddress(), "node1", "collection1");
       mockController.publishState("core1", ZkStateReader.RECOVERING, 1);
 
-      while (version == getCloudStateVersion(controllerClient));
+      while (version == getClusterStateVersion(controllerClient));
       
-      reader.updateCloudState(true);
-      CloudState state = reader.getCloudState();
+      reader.updateClusterState(true);
+      ClusterState state = reader.getClusterState();
       
       int numFound = 0;
       for (Map<String,Slice> collection : state.getCollectionStates().values()) {
@@ -758,7 +758,7 @@ public class OverseerTest extends SolrTestCaseJ4 {
           }
         }
       }
-      assertEquals("Shard was found in more than 1 times in CloudState", 1,
+      assertEquals("Shard was found more than once in ClusterState", 1,
           numFound);
     } finally {
       close(overseerClient);
@@ -800,7 +800,7 @@ public class OverseerTest extends SolrTestCaseJ4 {
 
       waitForCollections(reader, "collection1");
       
-      assertEquals("Slicecount does not match", 12, reader.getCloudState().getSlices("collection1").size());
+      assertEquals("Slicecount does not match", 12, reader.getClusterState().getSlices("collection1").size());
       
     } finally {
       close(overseerClient);
@@ -872,12 +872,12 @@ public class OverseerTest extends SolrTestCaseJ4 {
       queue.offer(ZkStateReader.toJSON(m));
       
       for(int i=0;i<100;i++) {
-        Slice s = reader.getCloudState().getSlice("collection1", "s1");
+        Slice s = reader.getClusterState().getSlice("collection1", "s1");
         if(s!=null && s.getShards().size()==3) break;
         Thread.sleep(100);
       }
-      assertNotNull(reader.getCloudState().getSlice("collection1", "s1"));
-      assertEquals(3, reader.getCloudState().getSlice("collection1", "s1").getShards().size());
+      assertNotNull(reader.getClusterState().getSlice("collection1", "s1"));
+      assertEquals(3, reader.getClusterState().getSlice("collection1", "s1").getShards().size());
     } finally {
       close(overseerClient);
       close(zkClient);
@@ -898,7 +898,7 @@ public class OverseerTest extends SolrTestCaseJ4 {
     }
   }
   
-  private int getCloudStateVersion(SolrZkClient controllerClient)
+  private int getClusterStateVersion(SolrZkClient controllerClient)
       throws KeeperException, InterruptedException {
     return controllerClient.exists(ZkStateReader.CLUSTER_STATE, null, false).getVersion();
   }
diff --git a/solr/core/src/test/org/apache/solr/cloud/SyncSliceTest.java b/solr/core/src/test/org/apache/solr/cloud/SyncSliceTest.java
index 650906d..89a16ce 100644
--- a/solr/core/src/test/org/apache/solr/cloud/SyncSliceTest.java
+++ b/solr/core/src/test/org/apache/solr/cloud/SyncSliceTest.java
@@ -157,7 +157,7 @@ public class SyncSliceTest extends FullSolrCloudTest {
     // we are careful to make sure the downed node is no longer in the state,
     // because on some systems (especially freebsd w/ blackhole enabled), trying
     // to talk to a downed node causes grief
-    waitToSeeDownInCloudState(leaderJetty, jetties);
+    waitToSeeDownInClusterState(leaderJetty, jetties);
 
     waitForThingsToLevelOut();
     
@@ -217,7 +217,7 @@ public class SyncSliceTest extends FullSolrCloudTest {
     // kill the current leader
     chaosMonkey.killJetty(leaderJetty);
     
-    waitToSeeDownInCloudState(leaderJetty, jetties);
+    waitToSeeDownInClusterState(leaderJetty, jetties);
     
     Thread.sleep(4000);
     
@@ -248,7 +248,7 @@ public class SyncSliceTest extends FullSolrCloudTest {
     return skipServers;
   }
 
-  private void waitToSeeDownInCloudState(CloudJettyRunner leaderJetty,
+  private void waitToSeeDownInClusterState(CloudJettyRunner leaderJetty,
       Set<CloudJettyRunner> jetties) throws InterruptedException {
 
     for (CloudJettyRunner cjetty : jetties) {
diff --git a/solr/core/src/test/org/apache/solr/cloud/ZkControllerTest.java b/solr/core/src/test/org/apache/solr/cloud/ZkControllerTest.java
index 2129c0b..276704d 100644
--- a/solr/core/src/test/org/apache/solr/cloud/ZkControllerTest.java
+++ b/solr/core/src/test/org/apache/solr/cloud/ZkControllerTest.java
@@ -202,7 +202,7 @@ public class ZkControllerTest extends SolrTestCaseJ4 {
       
       assertNotNull(reader.getLeaderUrl("collection1", "shard1", 15000));
       
-      assertEquals("Shard(s) missing from cloudstate", 2, zkController.getZkStateReader().getCloudState().getSlice("collection1", "shard1").getShards().size());
+      assertEquals("Shard(s) missing from cloudstate", 2, zkController.getZkStateReader().getClusterState().getSlice("collection1", "shard1").getShards().size());
       
       // unregister current leader
       final ZkNodeProps shard1LeaderProps = reader.getLeaderProps(
@@ -224,10 +224,10 @@ public class ZkControllerTest extends SolrTestCaseJ4 {
           reader.getLeaderUrl("collection1", "shard1", 15000));
 
       for(int i=0;i<30;i++) {
-        if(zkController.getZkStateReader().getCloudState().getSlice("collection1", "shard1").getShards().size()==1) break; 
+        if(zkController.getZkStateReader().getClusterState().getSlice("collection1", "shard1").getShards().size()==1) break; 
         Thread.sleep(500);
       }
-      assertEquals("shard was not unregistered", 1, zkController.getZkStateReader().getCloudState().getSlice("collection1", "shard1").getShards().size());
+      assertEquals("shard was not unregistered", 1, zkController.getZkStateReader().getClusterState().getSlice("collection1", "shard1").getShards().size());
     } finally {
       System.clearProperty("solrcloud.skip.autorecovery");
       System.clearProperty(ZkStateReader.NUM_SHARDS_PROP);
diff --git a/solr/solrj/src/java/org/apache/solr/client/solrj/impl/CloudSolrServer.java b/solr/solrj/src/java/org/apache/solr/client/solrj/impl/CloudSolrServer.java
index 903794a..1285571 100644
--- a/solr/solrj/src/java/org/apache/solr/client/solrj/impl/CloudSolrServer.java
+++ b/solr/solrj/src/java/org/apache/solr/client/solrj/impl/CloudSolrServer.java
@@ -34,7 +34,7 @@ import org.apache.solr.client.solrj.SolrServer;
 import org.apache.solr.client.solrj.SolrServerException;
 import org.apache.solr.client.solrj.util.ClientUtils;
 import org.apache.solr.common.SolrException;
-import org.apache.solr.common.cloud.CloudState;
+import org.apache.solr.common.cloud.ClusterState;
 import org.apache.solr.common.cloud.Slice;
 import org.apache.solr.common.cloud.ZkCoreNodeProps;
 import org.apache.solr.common.cloud.ZkNodeProps;
@@ -139,7 +139,7 @@ public class CloudSolrServer extends SolrServer {
 
     // TODO: if you can hash here, you could favor the shard leader
     
-    CloudState cloudState = zkStateReader.getCloudState();
+    ClusterState clusterState = zkStateReader.getClusterState();
 
     SolrParams reqParams = request.getParams();
     if (reqParams == null) {
@@ -159,12 +159,12 @@ public class CloudSolrServer extends SolrServer {
     Map<String,Slice> slices = new HashMap<String,Slice>();
     for (int i = 0; i < collectionList.size(); i++) {
       String coll= collectionList.get(i);
-      ClientUtils.appendMap(coll, slices, cloudState.getSlices(coll));
+      ClientUtils.appendMap(coll, slices, clusterState.getSlices(coll));
     }
 
-    Set<String> liveNodes = cloudState.getLiveNodes();
+    Set<String> liveNodes = clusterState.getLiveNodes();
 
-    // IDEA: have versions on various things... like a global cloudState version
+    // IDEA: have versions on various things... like a global clusterState version
     // or shardAddressVersion (which only changes when the shards change)
     // to allow caching.
 
diff --git a/solr/solrj/src/java/org/apache/solr/common/cloud/CloudState.java b/solr/solrj/src/java/org/apache/solr/common/cloud/CloudState.java
deleted file mode 100644
index bb3364a..0000000
--- a/solr/solrj/src/java/org/apache/solr/common/cloud/CloudState.java
+++ /dev/null
@@ -1,286 +0,0 @@
-package org.apache.solr.common.cloud;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.LinkedHashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.Map.Entry;
-import java.util.Set;
-
-import org.apache.noggit.JSONWriter;
-import org.apache.solr.common.SolrException;
-import org.apache.solr.common.SolrException.ErrorCode;
-import org.apache.solr.common.cloud.HashPartitioner.Range;
-import org.apache.zookeeper.KeeperException;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-/**
- * Immutable state of the cloud. Normally you can get the state by using
- * {@link ZkStateReader#getCloudState()}.
- */
-public class CloudState implements JSONWriter.Writable {
-  private static Logger log = LoggerFactory.getLogger(CloudState.class);
-  
-	private final Map<String, Map<String,Slice>> collectionStates;  // Map<collectionName, Map<sliceName,Slice>>
-	private final Set<String> liveNodes;
-  
-  private final HashPartitioner hp = new HashPartitioner();
-  
-  private final Map<String,RangeInfo> rangeInfos = new HashMap<String,RangeInfo>();
-  private final Map<String,Map<String,ZkNodeProps>> leaders = new HashMap<String,Map<String,ZkNodeProps>>();
-
-	public CloudState(Set<String> liveNodes,
-			Map<String, Map<String,Slice>> collectionStates) {
-		this.liveNodes = new HashSet<String>(liveNodes.size());
-		this.liveNodes.addAll(liveNodes);
-		this.collectionStates = new HashMap<String, Map<String,Slice>>(collectionStates.size());
-		this.collectionStates.putAll(collectionStates);
-		addRangeInfos(collectionStates.keySet());
-		getShardLeaders();
-	}
-
-	private void getShardLeaders() {
-    Set<Entry<String,Map<String,Slice>>> collections = collectionStates.entrySet();
-    for (Entry<String,Map<String,Slice>> collection : collections) {
-      Map<String,Slice> state = collection.getValue();
-      Set<Entry<String,Slice>> slices = state.entrySet();
-      for (Entry<String,Slice> sliceEntry : slices) {
-        Slice slice = sliceEntry.getValue();
-        Map<String,ZkNodeProps> shards = slice.getShards();
-        Set<Entry<String,ZkNodeProps>> shardsEntries = shards.entrySet();
-        for (Entry<String,ZkNodeProps> shardEntry : shardsEntries) {
-          ZkNodeProps props = shardEntry.getValue();
-          if (props.containsKey(ZkStateReader.LEADER_PROP)) {
-            Map<String,ZkNodeProps> leadersForCollection = leaders.get(collection.getKey());
-            if (leadersForCollection == null) {
-              leadersForCollection = new HashMap<String,ZkNodeProps>();
-              leaders.put(collection.getKey(), leadersForCollection);
-            }
-            leadersForCollection.put(sliceEntry.getKey(), props);
-            break; // we found the leader for this shard
-          }
-        }
-      }
-    }
-  }
-
-	/**
-	 * Get properties of a shard leader for specific collection.
-	 */
-	public ZkNodeProps getLeader(String collection, String shard) {
-	  Map<String,ZkNodeProps> collectionLeaders = leaders.get(collection);
-	  if (collectionLeaders == null) return null;
-	  return collectionLeaders.get(shard);
-	}
-	
-	/**
-	 * Get shard properties or null if shard is not found.
-	 */
-	public ZkNodeProps getShardProps(final String collection, final String coreNodeName) {
-	  Map<String, Slice> slices = getSlices(collection);
-	  for(Slice slice: slices.values()) {
-	    if(slice.getShards().get(coreNodeName)!=null) {
-	      return slice.getShards().get(coreNodeName);
-	    }
-	  }
-	  return null;
-	}
-
-  private void addRangeInfos(Set<String> collections) {
-    for (String collection : collections) {
-      addRangeInfo(collection);
-    }
-  }
-
-  /**
-   * Get the index Slice for collection.
-   */
-  public Slice getSlice(String collection, String slice) {
-		if (collectionStates.containsKey(collection)
-				&& collectionStates.get(collection).containsKey(slice))
-			return collectionStates.get(collection).get(slice);
-		return null;
-	}
-
-  /**
-   * Get all slices for collection.
-   */
-	public Map<String, Slice> getSlices(String collection) {
-		if(!collectionStates.containsKey(collection))
-			return null;
-		return Collections.unmodifiableMap(collectionStates.get(collection));
-	}
-
-	/**
-	 * Get collection names.
-	 */
-	public Set<String> getCollections() {
-		return Collections.unmodifiableSet(collectionStates.keySet());
-	}
-
-	/**
-	 * @return Map&lt;collectionName, Map&lt;sliceName,Slice&gt;&gt;
-	 */
-	public Map<String, Map<String, Slice>> getCollectionStates() {
-		return Collections.unmodifiableMap(collectionStates);
-	}
-
-	/**
-	 * Get names of the currently live nodes.
-	 */
-	public Set<String> getLiveNodes() {
-		return Collections.unmodifiableSet(liveNodes);
-	}
-
-	/**
-	 * Get shardId for core.
-	 * @param coreNodeName in the form of nodeName_coreName
-	 */
-	public String getShardId(String coreNodeName) {
-	  for (Entry<String, Map<String, Slice>> states: collectionStates.entrySet()){
-	    for(Entry<String, Slice> slices: states.getValue().entrySet()) {
-	      for(Entry<String, ZkNodeProps> shards: slices.getValue().getShards().entrySet()){
-	        if(coreNodeName.equals(shards.getKey())) {
-	          return slices.getKey();
-	        }
-	      }
-	    }
-	  }
-	  return null;
-	}
-
-	/**
-	 * Check if node is alive. 
-	 */
-	public boolean liveNodesContain(String name) {
-		return liveNodes.contains(name);
-	}
-	
-	public RangeInfo getRanges(String collection) {
-    // TODO: store this in zk
-    RangeInfo rangeInfo = rangeInfos.get(collection);
-
-	  return rangeInfo;
-	}
-
-  private RangeInfo addRangeInfo(String collection) {
-    List<Range> ranges;
-    RangeInfo rangeInfo;
-    rangeInfo = new RangeInfo();
-
-    Map<String,Slice> slices = getSlices(collection);
-    
-    if (slices == null) {
-      throw new SolrException(ErrorCode.BAD_REQUEST, "Can not find collection "
-          + collection + " in " + this);
-    }
-    
-    Set<String> shards = slices.keySet();
-    ArrayList<String> shardList = new ArrayList<String>(shards.size());
-    shardList.addAll(shards);
-    Collections.sort(shardList);
-    
-    ranges = hp.partitionRange(shards.size());
-    
-    rangeInfo.ranges = ranges;
-    rangeInfo.shardList = shardList;
-    rangeInfos.put(collection, rangeInfo);
-    return rangeInfo;
-  }
-
-  /**
-   * Get shard id for hash. This is used when determining which Slice the
-   * document is to be submitted to.
-   */
-  public String getShard(int hash, String collection) {
-    RangeInfo rangInfo = getRanges(collection);
-    
-    int cnt = 0;
-    for (Range range : rangInfo.ranges) {
-      if (hash < range.max) {
-        return rangInfo.shardList.get(cnt);
-      }
-      cnt++;
-    }
-    
-    throw new IllegalStateException("The HashPartitioner failed");
-  }
-
-	@Override
-	public String toString() {
-		StringBuilder sb = new StringBuilder();
-		sb.append("live nodes:" + liveNodes);
-		sb.append(" collections:" + collectionStates);
-		return sb.toString();
-	}
-
-	/**
-	 * Create CloudState by reading the current state from zookeeper. 
-	 */
-	public static CloudState load(SolrZkClient zkClient, Set<String> liveNodes) throws KeeperException, InterruptedException {
-    byte[] state = zkClient.getData(ZkStateReader.CLUSTER_STATE,
-        null, null, true);
-    return load(state, liveNodes);
-	}
-	
-	/**
-	 * Create CloudState from json string that is typically stored in zookeeper.
-	 */
-	public static CloudState load(byte[] bytes, Set<String> liveNodes) {
-    if (bytes == null || bytes.length == 0) {
-      return new CloudState(liveNodes, Collections.<String, Map<String,Slice>>emptyMap());
-    }
-    
-    LinkedHashMap<String, Object> stateMap = (LinkedHashMap<String, Object>) ZkStateReader.fromJSON(bytes);
-    HashMap<String,Map<String, Slice>> state = new HashMap<String,Map<String,Slice>>();
-
-    for(String collectionName: stateMap.keySet()){
-      Map<String, Object> collection = (Map<String, Object>)stateMap.get(collectionName);
-      Map<String, Slice> slices = new LinkedHashMap<String,Slice>();
-      for(String sliceName: collection.keySet()) {
-        Map<String, Map<String, String>> sliceMap = (Map<String, Map<String, String>>)collection.get(sliceName);
-        Map<String, ZkNodeProps> shards = new LinkedHashMap<String,ZkNodeProps>();
-        for(String shardName: sliceMap.keySet()) {
-          shards.put(shardName, new ZkNodeProps(sliceMap.get(shardName)));
-        }
-        Slice slice = new Slice(sliceName, shards);
-        slices.put(sliceName, slice);
-      }
-      state.put(collectionName, slices);
-    }
-    return new CloudState(liveNodes, state);
-	}
-
-  @Override
-  public void write(JSONWriter jsonWriter) {
-    jsonWriter.write(collectionStates);
-  }
-  
-  private class RangeInfo {
-    private List<Range> ranges;
-    private ArrayList<String> shardList;
-  }
-
-
-}
diff --git a/solr/solrj/src/java/org/apache/solr/common/cloud/ClusterState.java b/solr/solrj/src/java/org/apache/solr/common/cloud/ClusterState.java
new file mode 100644
index 0000000..c6598f9
--- /dev/null
+++ b/solr/solrj/src/java/org/apache/solr/common/cloud/ClusterState.java
@@ -0,0 +1,286 @@
+package org.apache.solr.common.cloud;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.LinkedHashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Map.Entry;
+import java.util.Set;
+
+import org.apache.noggit.JSONWriter;
+import org.apache.solr.common.SolrException;
+import org.apache.solr.common.SolrException.ErrorCode;
+import org.apache.solr.common.cloud.HashPartitioner.Range;
+import org.apache.zookeeper.KeeperException;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+/**
+ * Immutable state of the cloud. Normally you can get the state by using
+ * {@link ZkStateReader#getClusterState()}.
+ */
+public class ClusterState implements JSONWriter.Writable {
+  private static Logger log = LoggerFactory.getLogger(ClusterState.class);
+  
+	private final Map<String, Map<String,Slice>> collectionStates;  // Map<collectionName, Map<sliceName,Slice>>
+	private final Set<String> liveNodes;
+  
+  private final HashPartitioner hp = new HashPartitioner();
+  
+  private final Map<String,RangeInfo> rangeInfos = new HashMap<String,RangeInfo>();
+  private final Map<String,Map<String,ZkNodeProps>> leaders = new HashMap<String,Map<String,ZkNodeProps>>();
+
+	public ClusterState(Set<String> liveNodes,
+			Map<String, Map<String,Slice>> collectionStates) {
+		this.liveNodes = new HashSet<String>(liveNodes.size());
+		this.liveNodes.addAll(liveNodes);
+		this.collectionStates = new HashMap<String, Map<String,Slice>>(collectionStates.size());
+		this.collectionStates.putAll(collectionStates);
+		addRangeInfos(collectionStates.keySet());
+		getShardLeaders();
+	}
+
+	private void getShardLeaders() {
+    Set<Entry<String,Map<String,Slice>>> collections = collectionStates.entrySet();
+    for (Entry<String,Map<String,Slice>> collection : collections) {
+      Map<String,Slice> state = collection.getValue();
+      Set<Entry<String,Slice>> slices = state.entrySet();
+      for (Entry<String,Slice> sliceEntry : slices) {
+        Slice slice = sliceEntry.getValue();
+        Map<String,ZkNodeProps> shards = slice.getShards();
+        Set<Entry<String,ZkNodeProps>> shardsEntries = shards.entrySet();
+        for (Entry<String,ZkNodeProps> shardEntry : shardsEntries) {
+          ZkNodeProps props = shardEntry.getValue();
+          if (props.containsKey(ZkStateReader.LEADER_PROP)) {
+            Map<String,ZkNodeProps> leadersForCollection = leaders.get(collection.getKey());
+            if (leadersForCollection == null) {
+              leadersForCollection = new HashMap<String,ZkNodeProps>();
+              leaders.put(collection.getKey(), leadersForCollection);
+            }
+            leadersForCollection.put(sliceEntry.getKey(), props);
+            break; // we found the leader for this shard
+          }
+        }
+      }
+    }
+  }
+
+	/**
+	 * Get properties of a shard leader for specific collection.
+	 */
+	public ZkNodeProps getLeader(String collection, String shard) {
+	  Map<String,ZkNodeProps> collectionLeaders = leaders.get(collection);
+	  if (collectionLeaders == null) return null;
+	  return collectionLeaders.get(shard);
+	}
+	
+	/**
+	 * Get shard properties or null if shard is not found.
+	 */
+	public ZkNodeProps getShardProps(final String collection, final String coreNodeName) {
+	  Map<String, Slice> slices = getSlices(collection);
+	  for(Slice slice: slices.values()) {
+	    if(slice.getShards().get(coreNodeName)!=null) {
+	      return slice.getShards().get(coreNodeName);
+	    }
+	  }
+	  return null;
+	}
+
+  private void addRangeInfos(Set<String> collections) {
+    for (String collection : collections) {
+      addRangeInfo(collection);
+    }
+  }
+
+  /**
+   * Get the index Slice for collection.
+   */
+  public Slice getSlice(String collection, String slice) {
+		if (collectionStates.containsKey(collection)
+				&& collectionStates.get(collection).containsKey(slice))
+			return collectionStates.get(collection).get(slice);
+		return null;
+	}
+
+  /**
+   * Get all slices for collection.
+   */
+	public Map<String, Slice> getSlices(String collection) {
+		if(!collectionStates.containsKey(collection))
+			return null;
+		return Collections.unmodifiableMap(collectionStates.get(collection));
+	}
+
+	/**
+	 * Get collection names.
+	 */
+	public Set<String> getCollections() {
+		return Collections.unmodifiableSet(collectionStates.keySet());
+	}
+
+	/**
+	 * @return Map&lt;collectionName, Map&lt;sliceName,Slice&gt;&gt;
+	 */
+	public Map<String, Map<String, Slice>> getCollectionStates() {
+		return Collections.unmodifiableMap(collectionStates);
+	}
+
+	/**
+	 * Get names of the currently live nodes.
+	 */
+	public Set<String> getLiveNodes() {
+		return Collections.unmodifiableSet(liveNodes);
+	}
+
+	/**
+	 * Get shardId for core.
+	 * @param coreNodeName in the form of nodeName_coreName
+	 */
+	public String getShardId(String coreNodeName) {
+	  for (Entry<String, Map<String, Slice>> states: collectionStates.entrySet()){
+	    for(Entry<String, Slice> slices: states.getValue().entrySet()) {
+	      for(Entry<String, ZkNodeProps> shards: slices.getValue().getShards().entrySet()){
+	        if(coreNodeName.equals(shards.getKey())) {
+	          return slices.getKey();
+	        }
+	      }
+	    }
+	  }
+	  return null;
+	}
+
+	/**
+	 * Check if node is alive. 
+	 */
+	public boolean liveNodesContain(String name) {
+		return liveNodes.contains(name);
+	}
+	
+	public RangeInfo getRanges(String collection) {
+    // TODO: store this in zk
+    RangeInfo rangeInfo = rangeInfos.get(collection);
+
+	  return rangeInfo;
+	}
+
+  private RangeInfo addRangeInfo(String collection) {
+    List<Range> ranges;
+    RangeInfo rangeInfo;
+    rangeInfo = new RangeInfo();
+
+    Map<String,Slice> slices = getSlices(collection);
+    
+    if (slices == null) {
+      throw new SolrException(ErrorCode.BAD_REQUEST, "Can not find collection "
+          + collection + " in " + this);
+    }
+    
+    Set<String> shards = slices.keySet();
+    ArrayList<String> shardList = new ArrayList<String>(shards.size());
+    shardList.addAll(shards);
+    Collections.sort(shardList);
+    
+    ranges = hp.partitionRange(shards.size());
+    
+    rangeInfo.ranges = ranges;
+    rangeInfo.shardList = shardList;
+    rangeInfos.put(collection, rangeInfo);
+    return rangeInfo;
+  }
+
+  /**
+   * Get shard id for hash. This is used when determining which Slice the
+   * document is to be submitted to.
+   */
+  public String getShard(int hash, String collection) {
+    RangeInfo rangInfo = getRanges(collection);
+    
+    int cnt = 0;
+    for (Range range : rangInfo.ranges) {
+      if (hash < range.max) {
+        return rangInfo.shardList.get(cnt);
+      }
+      cnt++;
+    }
+    
+    throw new IllegalStateException("The HashPartitioner failed");
+  }
+
+	@Override
+	public String toString() {
+		StringBuilder sb = new StringBuilder();
+		sb.append("live nodes:" + liveNodes);
+		sb.append(" collections:" + collectionStates);
+		return sb.toString();
+	}
+
+	/**
+	 * Create ClusterState by reading the current state from zookeeper. 
+	 */
+	public static ClusterState load(SolrZkClient zkClient, Set<String> liveNodes) throws KeeperException, InterruptedException {
+    byte[] state = zkClient.getData(ZkStateReader.CLUSTER_STATE,
+        null, null, true);
+    return load(state, liveNodes);
+	}
+	
+	/**
+	 * Create ClusterState from json string that is typically stored in zookeeper.
+	 */
+	public static ClusterState load(byte[] bytes, Set<String> liveNodes) {
+    if (bytes == null || bytes.length == 0) {
+      return new ClusterState(liveNodes, Collections.<String, Map<String,Slice>>emptyMap());
+    }
+    
+    LinkedHashMap<String, Object> stateMap = (LinkedHashMap<String, Object>) ZkStateReader.fromJSON(bytes);
+    HashMap<String,Map<String, Slice>> state = new HashMap<String,Map<String,Slice>>();
+
+    for(String collectionName: stateMap.keySet()){
+      Map<String, Object> collection = (Map<String, Object>)stateMap.get(collectionName);
+      Map<String, Slice> slices = new LinkedHashMap<String,Slice>();
+      for(String sliceName: collection.keySet()) {
+        Map<String, Map<String, String>> sliceMap = (Map<String, Map<String, String>>)collection.get(sliceName);
+        Map<String, ZkNodeProps> shards = new LinkedHashMap<String,ZkNodeProps>();
+        for(String shardName: sliceMap.keySet()) {
+          shards.put(shardName, new ZkNodeProps(sliceMap.get(shardName)));
+        }
+        Slice slice = new Slice(sliceName, shards);
+        slices.put(sliceName, slice);
+      }
+      state.put(collectionName, slices);
+    }
+    return new ClusterState(liveNodes, state);
+	}
+
+  @Override
+  public void write(JSONWriter jsonWriter) {
+    jsonWriter.write(collectionStates);
+  }
+  
+  private class RangeInfo {
+    private List<Range> ranges;
+    private ArrayList<String> shardList;
+  }
+
+
+}
diff --git a/solr/solrj/src/java/org/apache/solr/common/cloud/ZkStateReader.java b/solr/solrj/src/java/org/apache/solr/common/cloud/ZkStateReader.java
index dc7a5c3..a0477cf 100644
--- a/solr/solrj/src/java/org/apache/solr/common/cloud/ZkStateReader.java
+++ b/solr/solrj/src/java/org/apache/solr/common/cloud/ZkStateReader.java
@@ -68,7 +68,7 @@ public class ZkStateReader {
   public static final String DOWN = "down";
   public static final String SYNC = "sync";
   
-  private volatile CloudState cloudState;
+  private volatile ClusterState clusterState;
 
   private static final long SOLRCLOUD_UPDATE_DELAY = Long.parseLong(System.getProperty("solrcloud.update.delay", "5000"));
 
@@ -120,7 +120,7 @@ public class ZkStateReader {
   }
   private ScheduledExecutorService updateCloudExecutor = Executors.newScheduledThreadPool(1, new ZKTF());
 
-  private boolean cloudStateUpdateScheduled;
+  private boolean clusterStateUpdateScheduled;
 
   private SolrZkClient zkClient;
   
@@ -158,13 +158,13 @@ public class ZkStateReader {
   }
   
   // load and publish a new CollectionInfo
-  public void updateCloudState(boolean immediate) throws KeeperException, InterruptedException {
-    updateCloudState(immediate, false);
+  public void updateClusterState(boolean immediate) throws KeeperException, InterruptedException {
+    updateClusterState(immediate, false);
   }
   
   // load and publish a new CollectionInfo
   public void updateLiveNodes() throws KeeperException, InterruptedException {
-    updateCloudState(true, true);
+    updateClusterState(true, true);
   }
   
   public synchronized void createClusterStateWatchersAndUpdate() throws KeeperException,
@@ -189,17 +189,17 @@ public class ZkStateReader {
           try {
             
             // delayed approach
-            // ZkStateReader.this.updateCloudState(false, false);
+            // ZkStateReader.this.updateClusterState(false, false);
             synchronized (ZkStateReader.this.getUpdateLock()) {
               // remake watch
               final Watcher thisWatch = this;
               byte[] data = zkClient.getData(CLUSTER_STATE, thisWatch, null,
                   true);
               
-              CloudState clusterState = CloudState.load(data,
-                  ZkStateReader.this.cloudState.getLiveNodes());
+              ClusterState clusterState = ClusterState.load(data,
+                  ZkStateReader.this.clusterState.getLiveNodes());
               // update volatile
-              cloudState = clusterState;
+              ZkStateReader.this.clusterState = clusterState;
             }
           } catch (KeeperException e) {
             if (e.code() == KeeperException.Code.SESSIONEXPIRED
@@ -236,15 +236,15 @@ public class ZkStateReader {
               log.info("Updating live nodes");
               try {
                 // delayed approach
-                // ZkStateReader.this.updateCloudState(false, true);
+                // ZkStateReader.this.updateClusterState(false, true);
                 synchronized (ZkStateReader.this.getUpdateLock()) {
                   List<String> liveNodes = zkClient.getChildren(
                       LIVE_NODES_ZKNODE, this, true);
                   Set<String> liveNodesSet = new HashSet<String>();
                   liveNodesSet.addAll(liveNodes);
-                  CloudState clusterState = new CloudState(liveNodesSet,
-                      ZkStateReader.this.cloudState.getCollectionStates());
-                  ZkStateReader.this.cloudState = clusterState;
+                  ClusterState clusterState = new ClusterState(liveNodesSet,
+                      ZkStateReader.this.clusterState.getCollectionStates());
+                  ZkStateReader.this.clusterState = clusterState;
                 }
               } catch (KeeperException e) {
                 if (e.code() == KeeperException.Code.SESSIONEXPIRED
@@ -267,51 +267,52 @@ public class ZkStateReader {
     
       Set<String> liveNodeSet = new HashSet<String>();
       liveNodeSet.addAll(liveNodes);
-      CloudState clusterState = CloudState.load(zkClient, liveNodeSet);
-      this.cloudState = clusterState;
+      ClusterState clusterState = ClusterState.load(zkClient, liveNodeSet);
+      this.clusterState = clusterState;
     }
   }
   
   
   // load and publish a new CollectionInfo
-  private synchronized void updateCloudState(boolean immediate,
+  private synchronized void updateClusterState(boolean immediate,
       final boolean onlyLiveNodes) throws KeeperException,
       InterruptedException {
     // build immutable CloudInfo
     
     if (immediate) {
-      CloudState clusterState;
+      ClusterState clusterState;
       synchronized (getUpdateLock()) {
-      List<String> liveNodes = zkClient.getChildren(LIVE_NODES_ZKNODE, null, true);
-      Set<String> liveNodesSet = new HashSet<String>();
-      liveNodesSet.addAll(liveNodes);
-      
+        List<String> liveNodes = zkClient.getChildren(LIVE_NODES_ZKNODE, null,
+            true);
+        Set<String> liveNodesSet = new HashSet<String>();
+        liveNodesSet.addAll(liveNodes);
+        
         if (!onlyLiveNodes) {
           log.info("Updating cloud state from ZooKeeper... ");
           
-          clusterState = CloudState.load(zkClient, liveNodesSet);
+          clusterState = ClusterState.load(zkClient, liveNodesSet);
         } else {
           log.info("Updating live nodes from ZooKeeper... ");
-          clusterState = new CloudState(liveNodesSet,
-              ZkStateReader.this.cloudState.getCollectionStates());
+          clusterState = new ClusterState(liveNodesSet,
+              ZkStateReader.this.clusterState.getCollectionStates());
         }
       }
 
-      this.cloudState = clusterState;
+      this.clusterState = clusterState;
     } else {
-      if (cloudStateUpdateScheduled) {
+      if (clusterStateUpdateScheduled) {
         log.info("Cloud state update for ZooKeeper already scheduled");
         return;
       }
       log.info("Scheduling cloud state update from ZooKeeper...");
-      cloudStateUpdateScheduled = true;
+      clusterStateUpdateScheduled = true;
       updateCloudExecutor.schedule(new Runnable() {
         
         public void run() {
           log.info("Updating cluster state from ZooKeeper...");
           synchronized (getUpdateLock()) {
-            cloudStateUpdateScheduled = false;
-            CloudState clusterState;
+            clusterStateUpdateScheduled = false;
+            ClusterState clusterState;
             try {
               List<String> liveNodes = zkClient.getChildren(LIVE_NODES_ZKNODE,
                   null, true);
@@ -321,13 +322,13 @@ public class ZkStateReader {
               if (!onlyLiveNodes) {
                 log.info("Updating cloud state from ZooKeeper... ");
                 
-                clusterState = CloudState.load(zkClient, liveNodesSet);
+                clusterState = ClusterState.load(zkClient, liveNodesSet);
               } else {
                 log.info("Updating live nodes from ZooKeeper... ");
-                clusterState = new CloudState(liveNodesSet, ZkStateReader.this.cloudState.getCollectionStates());
+                clusterState = new ClusterState(liveNodesSet, ZkStateReader.this.clusterState.getCollectionStates());
               }
               
-              ZkStateReader.this.cloudState = clusterState;
+              ZkStateReader.this.clusterState = clusterState;
               
             } catch (KeeperException e) {
               if (e.code() == KeeperException.Code.SESSIONEXPIRED
@@ -346,7 +347,7 @@ public class ZkStateReader {
                   SolrException.ErrorCode.SERVER_ERROR, "", e);
             } 
             // update volatile
-            ZkStateReader.this.cloudState = cloudState;
+            ZkStateReader.this.clusterState = clusterState;
           }
         }
       }, SOLRCLOUD_UPDATE_DELAY, TimeUnit.MILLISECONDS);
@@ -357,8 +358,8 @@ public class ZkStateReader {
   /**
    * @return information about the cluster from ZooKeeper
    */
-  public CloudState getCloudState() {
-    return cloudState;
+  public ClusterState getClusterState() {
+    return clusterState;
   }
   
   public Object getUpdateLock() {
@@ -411,8 +412,8 @@ public class ZkStateReader {
   public ZkNodeProps getLeaderProps(String collection, String shard, int timeout) throws InterruptedException {
     long timeoutAt = System.currentTimeMillis() + timeout;
     while (System.currentTimeMillis() < timeoutAt) {
-      if (cloudState != null) {    
-        final ZkNodeProps nodeProps = cloudState.getLeader(collection, shard);
+      if (clusterState != null) {    
+        final ZkNodeProps nodeProps = clusterState.getLeader(collection, shard);
         if (nodeProps != null) {
           return nodeProps;
         }
@@ -451,15 +452,15 @@ public class ZkStateReader {
   
   public List<ZkCoreNodeProps> getReplicaProps(String collection,
       String shardId, String thisNodeName, String coreName, String mustMatchStateFilter, String mustNotMatchStateFilter) {
-    CloudState cloudState = this.cloudState;
-    if (cloudState == null) {
+    ClusterState clusterState = this.clusterState;
+    if (clusterState == null) {
       return null;
     }
-    Map<String,Slice> slices = cloudState.getSlices(collection);
+    Map<String,Slice> slices = clusterState.getSlices(collection);
     if (slices == null) {
       throw new ZooKeeperException(ErrorCode.BAD_REQUEST,
           "Could not find collection in zk: " + collection + " "
-              + cloudState.getCollections());
+              + clusterState.getCollections());
     }
     
     Slice replicas = slices.get(shardId);
@@ -473,7 +474,7 @@ public class ZkStateReader {
     for (Entry<String,ZkNodeProps> entry : shardMap.entrySet()) {
       ZkCoreNodeProps nodeProps = new ZkCoreNodeProps(entry.getValue());
       String coreNodeName = nodeProps.getNodeName() + "_" + nodeProps.getCoreName();
-      if (cloudState.liveNodesContain(nodeProps.getNodeName()) && !coreNodeName.equals(filterNodeName)) {
+      if (clusterState.liveNodesContain(nodeProps.getNodeName()) && !coreNodeName.equals(filterNodeName)) {
         if (mustMatchStateFilter == null || mustMatchStateFilter.equals(nodeProps.getState())) {
           if (mustNotMatchStateFilter == null || !mustNotMatchStateFilter.equals(nodeProps.getState())) {
             nodes.add(nodeProps);

