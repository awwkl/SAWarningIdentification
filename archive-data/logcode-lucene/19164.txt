GitDiffStart: f20e419aff4676f7045dda5489d015cd5d8b6157 | Thu Oct 8 20:57:32 2009 +0000
diff --git a/build.xml b/build.xml
index d32ef60..910dd0e 100644
--- a/build.xml
+++ b/build.xml
@@ -108,7 +108,7 @@
 	</sequential>
   </target>
 	
-  <target name="test-tag" depends="download-tag, compile-core, jar-core"
+  <target name="test-tag" depends="compile-core, jar-core"
   	description="Runs tests of a previous Lucene version. Specify tag version like this: -Dtag=branches/lucene_2_9_back_compat_tests">
 	<sequential>
       <available property="tag.available" file="${tags.dir}/${tag}/src/test" />
diff --git a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/shingle/ShingleAnalyzerWrapperTest.java b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/shingle/ShingleAnalyzerWrapperTest.java
index ebe2454..15434ea 100644
--- a/contrib/analyzers/common/src/test/org/apache/lucene/analysis/shingle/ShingleAnalyzerWrapperTest.java
+++ b/contrib/analyzers/common/src/test/org/apache/lucene/analysis/shingle/ShingleAnalyzerWrapperTest.java
@@ -59,7 +59,7 @@ public class ShingleAnalyzerWrapperTest extends BaseTokenStreamTestCase {
    */
   public IndexSearcher setUpSearcher(Analyzer analyzer) throws Exception {
     Directory dir = new RAMDirectory();
-    IndexWriter writer = new IndexWriter(dir, analyzer, true);
+    IndexWriter writer = new IndexWriter(dir, analyzer, true, IndexWriter.MaxFieldLength.UNLIMITED);
 
     Document doc;
     doc = new Document();
diff --git a/contrib/benchmark/conf/autoCommit.alg b/contrib/benchmark/conf/autoCommit.alg
deleted file mode 100644
index 63e0e00..0000000
--- a/contrib/benchmark/conf/autoCommit.alg
+++ /dev/null
@@ -1,70 +0,0 @@
-#/**
-# * Licensed to the Apache Software Foundation (ASF) under one or more
-# * contributor license agreements.  See the NOTICE file distributed with
-# * this work for additional information regarding copyright ownership.
-# * The ASF licenses this file to You under the Apache License, Version 2.0
-# * (the "License"); you may not use this file except in compliance with
-# * the License.  You may obtain a copy of the License at
-# *
-# *     http://www.apache.org/licenses/LICENSE-2.0
-# *
-# * Unless required by applicable law or agreed to in writing, software
-# * distributed under the License is distributed on an "AS IS" BASIS,
-# * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# * See the License for the specific language governing permissions and
-# * limitations under the License.
-# */
-# -------------------------------------------------------------------------------------
-# multi val params are iterated by NewRound's, added to reports, start with column name.
-#
-# based on micro-standard
-#
-# modified to use wikipedia sources and index entire docs
-# currently just used to measure ingest rate
-
-#merge.factor=mrg:10:100:10:100
-#max.buffered=buf:10:10:100:100
-ram.flush.mb=ram:32
-autocommit=acommit:true:false
-
-max.field.length=2147483647
-
-
-compound=true
-
-analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
-directory=FSDirectory
-
-doc.stored=true
-doc.tokenized=true
-doc.term.vector=false
-log.step=5000
-
-docs.file=temp/enwiki-20070527-pages-articles.xml
-
-doc.maker=org.apache.lucene.benchmark.byTask.feeds.EnwikiDocMaker
-
-query.maker=org.apache.lucene.benchmark.byTask.feeds.ReutersQueryMaker
-
-# task at this depth or less would print when they start
-task.max.depth.log=2
-
-log.queries=false
-# -------------------------------------------------------------------------------------
-
-{ "Rounds"
-
-    ResetSystemErase
-
-    { "Populate"
-        CreateIndex
-        { "MAddDocs" AddDoc > : 200000
-        CloseIndex
-    }
-
-    NewRound
-
-} : 4
-
-RepSumByName
-RepSumByPrefRound MAddDocs
diff --git a/contrib/benchmark/conf/deletepercent.alg b/contrib/benchmark/conf/deletepercent.alg
index cdd1f96..01a4988 100644
--- a/contrib/benchmark/conf/deletepercent.alg
+++ b/contrib/benchmark/conf/deletepercent.alg
@@ -17,7 +17,6 @@
 # -------------------------------------------------------------------------------------
 # multi val params are iterated by NewRound's, added to reports, start with column name.
 
-autocommit=false
 analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
 directory=FSDirectory
 #directory=RamDirectory
diff --git a/contrib/benchmark/conf/indexing-flush-by-RAM-multithreaded.alg b/contrib/benchmark/conf/indexing-flush-by-RAM-multithreaded.alg
index 2c16cee..50ee885 100644
--- a/contrib/benchmark/conf/indexing-flush-by-RAM-multithreaded.alg
+++ b/contrib/benchmark/conf/indexing-flush-by-RAM-multithreaded.alg
@@ -22,7 +22,6 @@
 ram.flush.mb=flush:32:40:48:56:32:40:48:56
 compound=cmpnd:true:true:true:true:false:false:false:false
 
-autocommit=false
 analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
 directory=FSDirectory
 #directory=RamDirectory
diff --git a/contrib/benchmark/conf/indexing-flush-by-RAM.alg b/contrib/benchmark/conf/indexing-flush-by-RAM.alg
index c2d3fb4..720bed9 100644
--- a/contrib/benchmark/conf/indexing-flush-by-RAM.alg
+++ b/contrib/benchmark/conf/indexing-flush-by-RAM.alg
@@ -22,7 +22,6 @@
 ram.flush.mb=flush:32:40:48:56:32:40:48:56
 compound=cmpnd:true:true:true:true:false:false:false:false
 
-autocommit=false
 analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
 directory=FSDirectory
 #directory=RamDirectory
diff --git a/contrib/benchmark/conf/indexing-multithreaded.alg b/contrib/benchmark/conf/indexing-multithreaded.alg
index 27adde3..748dbde 100644
--- a/contrib/benchmark/conf/indexing-multithreaded.alg
+++ b/contrib/benchmark/conf/indexing-multithreaded.alg
@@ -22,7 +22,6 @@ max.buffered=buf:10:10:100:100:10:10:100:100
 #ram.flush.mb=flush:32:40:48:56:32:40:48:56
 compound=cmpnd:true:true:true:true:false:false:false:false
 
-autocommit=false
 analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
 directory=FSDirectory
 #directory=RamDirectory
diff --git a/contrib/benchmark/conf/indexing.alg b/contrib/benchmark/conf/indexing.alg
index 9deccdc..5859e9b 100644
--- a/contrib/benchmark/conf/indexing.alg
+++ b/contrib/benchmark/conf/indexing.alg
@@ -22,7 +22,6 @@ max.buffered=buf:10:10:100:100:10:10:100:100
 #ram.flush.mb=flush:32:40:48:56:32:40:48:56
 compound=cmpnd:true:true:true:true:false:false:false:false
 
-autocommit=false
 analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
 directory=FSDirectory
 #directory=RamDirectory
diff --git a/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/package.html b/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/package.html
index a060055..f5440bd 100644
--- a/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/package.html
+++ b/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/package.html
@@ -583,7 +583,6 @@ Here is a list of currently defined properties:
     </li><li>max.buffered
     </li><li>directory
     </li><li>ram.flush.mb
-    </li><li>autocommit
     </li></ul>
   </li>
 
diff --git a/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/CreateIndexTask.java b/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/CreateIndexTask.java
index b174ef3..175d026 100644
--- a/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/CreateIndexTask.java
+++ b/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/CreateIndexTask.java
@@ -34,7 +34,7 @@ import java.io.PrintStream;
  * Create an index. <br>
  * Other side effects: index writer object in perfRunData is set. <br>
  * Relevant properties: <code>merge.factor, max.buffered,
- *  max.field.length, ram.flush.mb [default 0], autocommit
+ *  max.field.length, ram.flush.mb [default 0],
  *  [default true]</code>.
  * <p>
  * This task also supports a "writer.info.stream" property with the following
@@ -129,9 +129,9 @@ public class CreateIndexTask extends PerfTask {
     IndexDeletionPolicy indexDeletionPolicy = getIndexDeletionPolicy(config);
     
     IndexWriter writer = new IndexWriter(runData.getDirectory(),
-                                         runData.getConfig().get("autocommit", OpenIndexTask.DEFAULT_AUTO_COMMIT),
                                          runData.getAnalyzer(),
-                                         true, indexDeletionPolicy);
+                                         true, indexDeletionPolicy,
+                                         IndexWriter.MaxFieldLength.LIMITED);
     setIndexWriterConfig(writer, config);
     runData.setIndexWriter(writer);
     return 1;
diff --git a/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/OpenIndexTask.java b/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/OpenIndexTask.java
index a304d0b..9ae11a2 100644
--- a/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/OpenIndexTask.java
+++ b/contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/OpenIndexTask.java
@@ -29,8 +29,7 @@ import java.io.IOException;
  * Open an index writer.
  * <br>Other side effects: index writer object in perfRunData is set.
  * <br>Relevant properties: <code>merge.factor, max.buffered,
- * max.field.length, ram.flush.mb [default 0], autocommit
- * [default true]</code>.
+ * max.field.length, ram.flush.mb [default 0]</code>.
  */
 public class OpenIndexTask extends PerfTask {
 
@@ -38,7 +37,6 @@ public class OpenIndexTask extends PerfTask {
   public static final int DEFAULT_MAX_FIELD_LENGTH = IndexWriter.DEFAULT_MAX_FIELD_LENGTH;
   public static final int DEFAULT_MERGE_PFACTOR = LogMergePolicy.DEFAULT_MERGE_FACTOR;
   public static final double DEFAULT_RAM_FLUSH_MB = (int) IndexWriter.DEFAULT_RAM_BUFFER_SIZE_MB;
-  public static final boolean DEFAULT_AUTO_COMMIT = false;
 
   public OpenIndexTask(PerfRunData runData) {
     super(runData);
@@ -48,9 +46,9 @@ public class OpenIndexTask extends PerfTask {
     PerfRunData runData = getRunData();
     Config config = runData.getConfig();
     IndexWriter writer = new IndexWriter(runData.getDirectory(),
-                                         config.get("autocommit", DEFAULT_AUTO_COMMIT),
                                          runData.getAnalyzer(),
-                                         false);
+                                         false,
+                                         IndexWriter.MaxFieldLength.UNLIMITED);
     CreateIndexTask.setIndexWriterConfig(writer, config);
     runData.setIndexWriter(writer);
     return 1;
diff --git a/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic.java b/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic.java
index 1218d4a..f8af37a 100755
--- a/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic.java
+++ b/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/TestPerfTasksLogic.java
@@ -338,7 +338,6 @@ public class TestPerfTasksLogic extends TestCase {
       "docs.file=" + lineFile.getAbsolutePath().replace('\\', '/'),
       "content.source.forever=false",
       "doc.reuse.fields=false",
-      "autocommit=false",
       "ram.flush.mb=4",
       "# ----- alg ",
       "ResetSystemErase",
diff --git a/contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest.java b/contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest.java
index 9729de4..cbb3377 100644
--- a/contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest.java
+++ b/contrib/highlighter/src/test/org/apache/lucene/search/highlight/HighlighterTest.java
@@ -1210,7 +1210,7 @@ public class HighlighterTest extends BaseTokenStreamTestCase implements Formatte
   public void testMultiSearcher() throws Exception {
     // setup index 1
     RAMDirectory ramDir1 = new RAMDirectory();
-    IndexWriter writer1 = new IndexWriter(ramDir1, new StandardAnalyzer(), true);
+    IndexWriter writer1 = new IndexWriter(ramDir1, new StandardAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
     Document d = new Document();
     Field f = new Field(FIELD_NAME, "multiOne", Field.Store.YES, Field.Index.ANALYZED);
     d.add(f);
@@ -1221,7 +1221,7 @@ public class HighlighterTest extends BaseTokenStreamTestCase implements Formatte
 
     // setup index 2
     RAMDirectory ramDir2 = new RAMDirectory();
-    IndexWriter writer2 = new IndexWriter(ramDir2, new StandardAnalyzer(), true);
+    IndexWriter writer2 = new IndexWriter(ramDir2, new StandardAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
     d = new Document();
     f = new Field(FIELD_NAME, "multiTwo", Field.Store.YES, Field.Index.ANALYZED);
     d.add(f);
@@ -1601,7 +1601,7 @@ public class HighlighterTest extends BaseTokenStreamTestCase implements Formatte
   protected void setUp() throws Exception {
     super.setUp();
     ramDir = new RAMDirectory();
-    IndexWriter writer = new IndexWriter(ramDir, new StandardAnalyzer(), true);
+    IndexWriter writer = new IndexWriter(ramDir, new StandardAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
     for (int i = 0; i < texts.length; i++) {
       addDoc(writer, texts[i]);
     }
diff --git a/contrib/instantiated/src/test/org/apache/lucene/store/instantiated/TestIndicesEquals.java b/contrib/instantiated/src/test/org/apache/lucene/store/instantiated/TestIndicesEquals.java
index 26c2926..172ea17 100644
--- a/contrib/instantiated/src/test/org/apache/lucene/store/instantiated/TestIndicesEquals.java
+++ b/contrib/instantiated/src/test/org/apache/lucene/store/instantiated/TestIndicesEquals.java
@@ -65,7 +65,7 @@ public class TestIndicesEquals extends TestCase {
     RAMDirectory dir = new RAMDirectory();
 
     // create dir data
-    IndexWriter indexWriter = new IndexWriter(dir, new StandardAnalyzer(), true);
+    IndexWriter indexWriter = new IndexWriter(dir, new StandardAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
     for (int i = 0; i < 20; i++) {
       Document document = new Document();
       assembleDocument(document, i);
@@ -89,7 +89,7 @@ public class TestIndicesEquals extends TestCase {
     InstantiatedIndex ii = new InstantiatedIndex();
 
     // create dir data
-    IndexWriter indexWriter = new IndexWriter(dir, new StandardAnalyzer(), true);
+    IndexWriter indexWriter = new IndexWriter(dir, new StandardAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
     for (int i = 0; i < 500; i++) {
       Document document = new Document();
       assembleDocument(document, i);
diff --git a/contrib/lucli/src/java/lucli/LuceneMethods.java b/contrib/lucli/src/java/lucli/LuceneMethods.java
index d1f6547..169439d 100644
--- a/contrib/lucli/src/java/lucli/LuceneMethods.java
+++ b/contrib/lucli/src/java/lucli/LuceneMethods.java
@@ -174,7 +174,7 @@ class LuceneMethods {
 
     public void optimize() throws IOException {
     //open the index writer. False: don't create a new one
-    IndexWriter indexWriter = new IndexWriter(indexName, createAnalyzer(), false);
+    IndexWriter indexWriter = new IndexWriter(indexName, createAnalyzer(), false, IndexWriter.MaxFieldLength.UNLIMITED);
     message("Starting to optimize index.");
     long start = System.currentTimeMillis();
     indexWriter.optimize();
diff --git a/contrib/misc/src/test/org/apache/lucene/index/TestTermVectorAccessor.java b/contrib/misc/src/test/org/apache/lucene/index/TestTermVectorAccessor.java
index 6cbf3ec..399ad86 100644
--- a/contrib/misc/src/test/org/apache/lucene/index/TestTermVectorAccessor.java
+++ b/contrib/misc/src/test/org/apache/lucene/index/TestTermVectorAccessor.java
@@ -29,7 +29,7 @@ public class TestTermVectorAccessor extends TestCase {
   public void test() throws Exception {
 
     Directory dir = new RAMDirectory();
-    IndexWriter iw = new IndexWriter(dir, new StandardAnalyzer(Collections.EMPTY_SET), true);
+    IndexWriter iw = new IndexWriter(dir, new StandardAnalyzer(Collections.EMPTY_SET), true, IndexWriter.MaxFieldLength.UNLIMITED);
 
     Document doc;
 
diff --git a/contrib/misc/src/test/org/apache/lucene/misc/ChainedFilterTest.java b/contrib/misc/src/test/org/apache/lucene/misc/ChainedFilterTest.java
index 7a68a9e..8a3f987 100644
--- a/contrib/misc/src/test/org/apache/lucene/misc/ChainedFilterTest.java
+++ b/contrib/misc/src/test/org/apache/lucene/misc/ChainedFilterTest.java
@@ -57,7 +57,7 @@ public class ChainedFilterTest extends TestCase {
   public void setUp() throws Exception {
     directory = new RAMDirectory();
     IndexWriter writer =
-       new IndexWriter(directory, new WhitespaceAnalyzer(), true);
+       new IndexWriter(directory, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
 
     Calendar cal = Calendar.getInstance();
     cal.setTimeInMillis(1041397200000L); // 2003 January 01
diff --git a/contrib/queries/src/test/org/apache/lucene/search/BooleanFilterTest.java b/contrib/queries/src/test/org/apache/lucene/search/BooleanFilterTest.java
index f1bff08..7208850 100644
--- a/contrib/queries/src/test/org/apache/lucene/search/BooleanFilterTest.java
+++ b/contrib/queries/src/test/org/apache/lucene/search/BooleanFilterTest.java
@@ -37,7 +37,7 @@ public class BooleanFilterTest extends TestCase
 	protected void setUp() throws Exception
 	{
 		directory = new RAMDirectory();
-		IndexWriter writer = new IndexWriter(directory, new WhitespaceAnalyzer(), true);
+		IndexWriter writer = new IndexWriter(directory, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
 		
 		//Add series of docs with filterable fields : acces rights, prices, dates and "in-stock" flags
 		addDoc(writer, "admin guest", "010", "20040101","Y");
diff --git a/contrib/queries/src/test/org/apache/lucene/search/DuplicateFilterTest.java b/contrib/queries/src/test/org/apache/lucene/search/DuplicateFilterTest.java
index fbc1b7d..cf94737 100644
--- a/contrib/queries/src/test/org/apache/lucene/search/DuplicateFilterTest.java
+++ b/contrib/queries/src/test/org/apache/lucene/search/DuplicateFilterTest.java
@@ -42,7 +42,7 @@ public class DuplicateFilterTest extends TestCase
 	protected void setUp() throws Exception
 	{
 		directory = new RAMDirectory();
-		IndexWriter writer = new IndexWriter(directory, new StandardAnalyzer(), true);
+		IndexWriter writer = new IndexWriter(directory, new StandardAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
 		
 		//Add series of docs with filterable fields : url, text and dates  flags
 		addDoc(writer, "http://lucene.apache.org", "lucene 1.4.3 available", "20040101");
diff --git a/contrib/regex/src/test/org/apache/lucene/search/regex/TestSpanRegexQuery.java b/contrib/regex/src/test/org/apache/lucene/search/regex/TestSpanRegexQuery.java
index 7665713..4f945ac 100644
--- a/contrib/regex/src/test/org/apache/lucene/search/regex/TestSpanRegexQuery.java
+++ b/contrib/regex/src/test/org/apache/lucene/search/regex/TestSpanRegexQuery.java
@@ -44,7 +44,7 @@ public class TestSpanRegexQuery extends TestCase {
 
   public void testSpanRegex() throws Exception {
     RAMDirectory directory = new RAMDirectory();
-    IndexWriter writer = new IndexWriter(directory, new SimpleAnalyzer(), true);
+    IndexWriter writer = new IndexWriter(directory, new SimpleAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
     Document doc = new Document();
     // doc.add(new Field("field", "the quick brown fox jumps over the lazy dog",
     // Field.Store.NO, Field.Index.ANALYZED));
diff --git a/contrib/spatial/src/test/org/apache/lucene/spatial/tier/TestCartesian.java b/contrib/spatial/src/test/org/apache/lucene/spatial/tier/TestCartesian.java
index b64c73d..25813de 100644
--- a/contrib/spatial/src/test/org/apache/lucene/spatial/tier/TestCartesian.java
+++ b/contrib/spatial/src/test/org/apache/lucene/spatial/tier/TestCartesian.java
@@ -75,7 +75,7 @@ public class TestCartesian extends TestCase{
   protected void setUp() throws IOException {
     directory = new RAMDirectory();
 
-    IndexWriter writer = new IndexWriter(directory, new WhitespaceAnalyzer(), true);
+    IndexWriter writer = new IndexWriter(directory, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
     
     setUpPlotter( 2, 15);
     
diff --git a/contrib/spatial/src/test/org/apache/lucene/spatial/tier/TestDistance.java b/contrib/spatial/src/test/org/apache/lucene/spatial/tier/TestDistance.java
index 9200307..5ee87c5 100644
--- a/contrib/spatial/src/test/org/apache/lucene/spatial/tier/TestDistance.java
+++ b/contrib/spatial/src/test/org/apache/lucene/spatial/tier/TestDistance.java
@@ -49,7 +49,7 @@ public class TestDistance extends TestCase{
   @Override
   protected void setUp() throws IOException {
     directory = new RAMDirectory();
-    writer = new IndexWriter(directory, new WhitespaceAnalyzer(), true);
+    writer = new IndexWriter(directory, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
     addData(writer);
     
   }
diff --git a/contrib/spellchecker/src/java/org/apache/lucene/search/spell/SpellChecker.java b/contrib/spellchecker/src/java/org/apache/lucene/search/spell/SpellChecker.java
index 4b8f5a2..b5bb7a2 100755
--- a/contrib/spellchecker/src/java/org/apache/lucene/search/spell/SpellChecker.java
+++ b/contrib/spellchecker/src/java/org/apache/lucene/search/spell/SpellChecker.java
@@ -106,7 +106,7 @@ public class SpellChecker {
   public void setSpellIndex(Directory spellIndex) throws IOException {
     this.spellIndex = spellIndex;
     if (!IndexReader.indexExists(spellIndex)) {
-        IndexWriter writer = new IndexWriter(spellIndex, null, true);
+        IndexWriter writer = new IndexWriter(spellIndex, null, true, IndexWriter.MaxFieldLength.UNLIMITED);
         writer.close();
     }
     // close the old searcher, if there was one
@@ -299,7 +299,7 @@ public class SpellChecker {
    * @throws IOException
    */
   public void clearIndex() throws IOException {
-    IndexWriter writer = new IndexWriter(spellIndex, null, true);
+    IndexWriter writer = new IndexWriter(spellIndex, null, true, IndexWriter.MaxFieldLength.UNLIMITED);
     writer.close();
     
     //close the old searcher
@@ -325,7 +325,7 @@ public class SpellChecker {
    * @throws IOException
    */
   public void indexDictionary(Dictionary dict, int mergeFactor, int ramMB) throws IOException {
-    IndexWriter writer = new IndexWriter(spellIndex, true, new WhitespaceAnalyzer());
+    IndexWriter writer = new IndexWriter(spellIndex, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.UNLIMITED);
     writer.setMergeFactor(mergeFactor);
     writer.setRAMBufferSizeMB(ramMB);
 
diff --git a/contrib/spellchecker/src/test/org/apache/lucene/search/spell/TestLuceneDictionary.java b/contrib/spellchecker/src/test/org/apache/lucene/search/spell/TestLuceneDictionary.java
index 98956da..8dd3525 100644
--- a/contrib/spellchecker/src/test/org/apache/lucene/search/spell/TestLuceneDictionary.java
+++ b/contrib/spellchecker/src/test/org/apache/lucene/search/spell/TestLuceneDictionary.java
@@ -47,7 +47,7 @@ public class TestLuceneDictionary extends TestCase {
 
   public void setUp() throws Exception {
 
-    IndexWriter writer = new IndexWriter(store, new WhitespaceAnalyzer(), true);
+    IndexWriter writer = new IndexWriter(store, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
 
     Document doc;
 
diff --git a/contrib/spellchecker/src/test/org/apache/lucene/search/spell/TestSpellChecker.java b/contrib/spellchecker/src/test/org/apache/lucene/search/spell/TestSpellChecker.java
index a9c71ab..b08512e 100755
--- a/contrib/spellchecker/src/test/org/apache/lucene/search/spell/TestSpellChecker.java
+++ b/contrib/spellchecker/src/test/org/apache/lucene/search/spell/TestSpellChecker.java
@@ -46,7 +46,7 @@ public class TestSpellChecker extends TestCase {
     
     //create a user index
     userindex = new RAMDirectory();
-    IndexWriter writer = new IndexWriter(userindex, new SimpleAnalyzer(), true);
+    IndexWriter writer = new IndexWriter(userindex, new SimpleAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
 
     for (int i = 0; i < 1000; i++) {
       Document doc = new Document();
diff --git a/contrib/swing/src/java/org/apache/lucene/swing/models/TableSearcher.java b/contrib/swing/src/java/org/apache/lucene/swing/models/TableSearcher.java
index bc87e99..483a717 100644
--- a/contrib/swing/src/java/org/apache/lucene/swing/models/TableSearcher.java
+++ b/contrib/swing/src/java/org/apache/lucene/swing/models/TableSearcher.java
@@ -163,7 +163,7 @@ public class TableSearcher extends AbstractTableModel {
         try {
             // recreate the RAMDirectory
             directory = new RAMDirectory();
-            IndexWriter writer = new IndexWriter(directory, analyzer, true);
+            IndexWriter writer = new IndexWriter(directory, analyzer, true, IndexWriter.MaxFieldLength.UNLIMITED);
 
             // iterate through all rows
             for (int row=0; row < tableModel.getRowCount(); row++){
diff --git a/contrib/xml-query-parser/src/test/org/apache/lucene/xmlparser/TestParser.java b/contrib/xml-query-parser/src/test/org/apache/lucene/xmlparser/TestParser.java
index 83be872..f252108 100644
--- a/contrib/xml-query-parser/src/test/org/apache/lucene/xmlparser/TestParser.java
+++ b/contrib/xml-query-parser/src/test/org/apache/lucene/xmlparser/TestParser.java
@@ -61,7 +61,7 @@ public class TestParser extends TestCase {
 		{
 			BufferedReader d = new BufferedReader(new InputStreamReader(TestParser.class.getResourceAsStream("reuters21578.txt"))); 
 			dir=new RAMDirectory();
-			IndexWriter writer=new IndexWriter(dir,analyzer,true);
+			IndexWriter writer=new IndexWriter(dir,analyzer,true, IndexWriter.MaxFieldLength.UNLIMITED);
 			String line = d.readLine();		
 			while(line!=null)
 			{
diff --git a/contrib/xml-query-parser/src/test/org/apache/lucene/xmlparser/TestQueryTemplateManager.java b/contrib/xml-query-parser/src/test/org/apache/lucene/xmlparser/TestQueryTemplateManager.java
index 0270515..ff4aa42 100644
--- a/contrib/xml-query-parser/src/test/org/apache/lucene/xmlparser/TestQueryTemplateManager.java
+++ b/contrib/xml-query-parser/src/test/org/apache/lucene/xmlparser/TestQueryTemplateManager.java
@@ -141,7 +141,7 @@ public class TestQueryTemplateManager extends TestCase {
 		
 		//Create an index
 		RAMDirectory dir=new RAMDirectory();
-		IndexWriter w=new IndexWriter(dir,analyzer,true);
+		IndexWriter w=new IndexWriter(dir,analyzer,true, IndexWriter.MaxFieldLength.UNLIMITED);
 		for (int i = 0; i < docFieldValues.length; i++)
 		{
 			w.addDocument(getDocumentFromString(docFieldValues[i]));
diff --git a/src/java/org/apache/lucene/index/DirectoryReader.java b/src/java/org/apache/lucene/index/DirectoryReader.java
index f502c16..60a3967 100644
--- a/src/java/org/apache/lucene/index/DirectoryReader.java
+++ b/src/java/org/apache/lucene/index/DirectoryReader.java
@@ -785,9 +785,9 @@ class DirectoryReader extends IndexReader implements Cloneable {
   /**
    * Check whether this IndexReader is still using the current (i.e., most recently committed) version of the index.  If
    * a writer has committed any changes to the index since this reader was opened, this will return <code>false</code>,
-   * in which case you must open a new IndexReader in order to see the changes.  See the description of the <a
-   * href="IndexWriter.html#autoCommit"><code>autoCommit</code></a> flag which controls when the {@link IndexWriter}
-   * actually commits changes to the index.
+   * in which case you must open a new IndexReader in order
+   * to see the changes.  Use {@link IndexWriter#commit} to
+   * commit changes to the index.
    *
    * @throws CorruptIndexException if the index is corrupt
    * @throws IOException           if there is a low-level IO error
diff --git a/src/java/org/apache/lucene/index/DocumentsWriter.java b/src/java/org/apache/lucene/index/DocumentsWriter.java
index ad1867d..5eaa372 100644
--- a/src/java/org/apache/lucene/index/DocumentsWriter.java
+++ b/src/java/org/apache/lucene/index/DocumentsWriter.java
@@ -79,9 +79,8 @@ import org.apache.lucene.util.Constants;
  * call).  Finally the synchronized "finishDocument" is
  * called to flush changes to the directory.
  *
- * When flush is called by IndexWriter, or, we flush
- * internally when autoCommit=false, we forcefully idle all
- * threads and flush only once they are all idle.  This
+ * When flush is called by IndexWriter we forcefully idle
+ * all threads and flush only once they are all idle.  This
  * means you can call flush with a given thread even while
  * other threads are actively adding/deleting documents.
  *
@@ -349,8 +348,7 @@ final class DocumentsWriter {
   }
 
   /** Returns the current doc store segment we are writing
-   *  to.  This will be the same as segment when autoCommit
-   *  * is true. */
+   *  to. */
   synchronized String getDocStoreSegment() {
     return docStoreSegment;
   }
@@ -441,8 +439,9 @@ final class DocumentsWriter {
   synchronized void abort() throws IOException {
 
     try {
-      if (infoStream != null)
+      if (infoStream != null) {
         message("docWriter: now abort");
+      }
 
       // Forcefully remove waiting ThreadStates from line
       waitQueue.abort();
@@ -491,6 +490,9 @@ final class DocumentsWriter {
     } finally {
       aborting = false;
       notifyAll();
+      if (infoStream != null) {
+        message("docWriter: done abort");
+      }
     }
   }
 
diff --git a/src/java/org/apache/lucene/index/IndexDeletionPolicy.java b/src/java/org/apache/lucene/index/IndexDeletionPolicy.java
index a754411..3b7d3c5 100644
--- a/src/java/org/apache/lucene/index/IndexDeletionPolicy.java
+++ b/src/java/org/apache/lucene/index/IndexDeletionPolicy.java
@@ -81,13 +81,10 @@ public interface IndexDeletionPolicy {
    * by calling method {@link IndexCommit#delete delete()} 
    * of {@link IndexCommit}.</p>
    * 
-   * <p>If writer has <code>autoCommit = true</code> then
-   * this method will in general be called many times during
-   * one instance of {@link IndexWriter}.  If
-   * <code>autoCommit = false</code> then this method is
-   * only called once when {@link IndexWriter#close} is
-   * called, or not at all if the {@link IndexWriter#abort}
-   * is called. 
+   * <p>This method is only called when {@link
+   * IndexWriter#commit} or {@link IndexWriter#close} is
+   * called, or possibly not at all if the {@link
+   * IndexWriter#abort} is called.
    *
    * <p><u>Note:</u> the last CommitPoint is the most recent one,
    * i.e. the "front index state". Be careful not to delete it,
diff --git a/src/java/org/apache/lucene/index/IndexFileDeleter.java b/src/java/org/apache/lucene/index/IndexFileDeleter.java
index 074facb..d974372 100644
--- a/src/java/org/apache/lucene/index/IndexFileDeleter.java
+++ b/src/java/org/apache/lucene/index/IndexFileDeleter.java
@@ -40,13 +40,6 @@ import java.util.Collection;
  * counting to map the live SegmentInfos instances to
  * individual files in the Directory.
  *
- * When autoCommit=true, IndexWriter currently commits only
- * on completion of a merge (though this may change with
- * time: it is not a guarantee).  When autoCommit=false,
- * IndexWriter only commits when it is closed.  Regardless
- * of autoCommit, the user may call IndexWriter.commit() to
- * force a blocking commit.
- * 
  * The same directory file may be referenced by more than
  * one IndexCommit, i.e. more than one SegmentInfos.
  * Therefore we count how many commits reference each file.
diff --git a/src/java/org/apache/lucene/index/IndexReader.java b/src/java/org/apache/lucene/index/IndexReader.java
index 8b2de92..f70f142 100644
--- a/src/java/org/apache/lucene/index/IndexReader.java
+++ b/src/java/org/apache/lucene/index/IndexReader.java
@@ -526,10 +526,9 @@ public abstract class IndexReader implements Cloneable {
    * index.  If a writer has committed any changes to the
    * index since this reader was opened, this will return
    * <code>false</code>, in which case you must open a new
-   * IndexReader in order to see the changes.  See the
-   * description of the <a href="IndexWriter.html#autoCommit"><code>autoCommit</code></a>
-   * flag which controls when the {@link IndexWriter}
-   * actually commits changes to the index.
+   * IndexReader in order to see the changes.  Changes must
+   * be committed using  {@link IndexWriter#commit} to be
+   * visible to readers.
    * 
    * <p>
    * Not implemented in the IndexReader base class.
diff --git a/src/java/org/apache/lucene/index/IndexWriter.java b/src/java/org/apache/lucene/index/IndexWriter.java
index 39e6dbd..7b037cf 100644
--- a/src/java/org/apache/lucene/index/IndexWriter.java
+++ b/src/java/org/apache/lucene/index/IndexWriter.java
@@ -23,14 +23,12 @@ import org.apache.lucene.index.DocumentsWriter.IndexingChain;
 import org.apache.lucene.search.Similarity;
 import org.apache.lucene.search.Query;
 import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.FSDirectory;
 import org.apache.lucene.store.Lock;
 import org.apache.lucene.store.LockObtainFailedException;
 import org.apache.lucene.store.AlreadyClosedException;
 import org.apache.lucene.store.BufferedIndexInput;
 import org.apache.lucene.util.Constants;
 
-import java.io.File;
 import java.io.IOException;
 import java.io.PrintStream;
 import java.util.List;
@@ -85,55 +83,6 @@ import java.util.Map;
   addDocument calls (see <a href="#mergePolicy">below</a>
   for changing the {@link MergeScheduler}).</p>
 
-  <a name="autoCommit"></a>
-  <p>The optional <code>autoCommit</code> argument to the {@link
-  #IndexWriter(Directory, boolean, Analyzer) constructors}
-  controls visibility of the changes to {@link IndexReader}
-  instances reading the same index.  When this is
-  <code>false</code>, changes are not visible until {@link
-  #close()} or {@link #commit()} is called.  Note that changes will still be
-  flushed to the {@link Directory} as new files, but are 
-  not committed (no new <code>segments_N</code> file is written 
-  referencing the new files, nor are the files sync'd to stable storage)
-  until {@link #close()} or {@link #commit()} is called.  If something
-  goes terribly wrong (for example the JVM crashes), then
-  the index will reflect none of the changes made since the
-  last commit, or the starting state if commit was not called.
-  You can also call {@link #rollback()}, which closes the writer
-  without committing any changes, and removes any index
-  files that had been flushed but are now unreferenced.
-  This mode is useful for preventing readers from refreshing
-  at a bad time (for example after you've done all your
-  deletes but before you've done your adds).  It can also be
-  used to implement simple single-writer transactional
-  semantics ("all or none").  You can do a two-phase commit
-  by calling {@link #prepareCommit()}
-  followed by {@link #commit()}. This is necessary when
-  Lucene is working with an external resource (for example,
-  a database) and both must either commit or rollback the
-  transaction.</p>
-
-  <p>When <code>autoCommit</code> is <code>true</code> then
-  the writer will periodically commit on its own.  [<b>Deprecated</b>: Note that in 3.0, IndexWriter will
-  no longer accept autoCommit=true (it will be hardwired to
-  false).  You can always call {@link #commit()} yourself
-  when needed]. There is
-  no guarantee when exactly an auto commit will occur (it
-  used to be after every flush, but it is now after every
-  completed merge, as of 2.4).  If you want to force a
-  commit, call {@link #commit()}, or, close the writer.  Once
-  a commit has finished, newly opened {@link IndexReader} instances will
-  see the changes to the index as of that commit.  When
-  running in this mode, be careful not to refresh your
-  readers while optimize or segment merges are taking place
-  as this can tie up substantial disk space.</p>
-  
-  <p>Regardless of <code>autoCommit</code>, an {@link
-  IndexReader} or {@link org.apache.lucene.search.IndexSearcher} will only see the
-  index as of the "point in time" that it was opened.  Any
-  changes committed to the index after the reader was opened
-  are not visible until the reader is re-opened.</p>
-
   <p>If an index will not have more documents added for a while and optimal search
   performance is desired, then either the full {@link #optimize() optimize}
   method or partial {@link #optimize(int)} method should be
@@ -183,8 +132,7 @@ import java.util.Map;
   IllegalStateException.  The only course of action is to
   call {@link #close()}, which internally will call {@link
   #rollback()}, to undo any changes to the index since the
-  last commit.  If you opened the writer with autoCommit
-  false you can also just call {@link #rollback()}
+  last commit.  You can also just call {@link #rollback()}
   directly.</p>
 
   <a name="thread-safety"></a><p><b>NOTE</b>: {@link
@@ -199,8 +147,7 @@ import java.util.Map;
 
 /*
  * Clarification: Check Points (and commits)
- * Being able to set autoCommit=false allows IndexWriter to flush and 
- * write new index files to the directory without writing a new segments_N
+ * IndexWriter writes new index files to the directory without writing a new segments_N
  * file which references these new files. It also means that the state of 
  * the in memory SegmentInfos object is different than the most recent
  * segments_N file written to the directory.
@@ -211,9 +158,6 @@ import java.util.Map;
  * (generation of) segments_N file - this check point is also an 
  * IndexCommit.
  * 
- * With autoCommit=true, every checkPoint is also a CommitPoint.
- * With autoCommit=false, some checkPoints may not be commits.
- * 
  * A new checkpoint always replaces the previous checkpoint and 
  * becomes the new "front" of the index. This allows the IndexFileDeleter 
  * to delete files that are referenced only by stale checkpoints.
@@ -917,10 +861,6 @@ public class IndexWriter {
    * is true, then a new, empty index will be created in
    * <code>d</code>, replacing the index already there, if any.
    *
-   * <p><b>NOTE</b>: autoCommit (see <a
-   * href="#autoCommit">above</a>) is set to false with this
-   * constructor.
-   *
    * @param d the index directory
    * @param a the analyzer to use
    * @param create <code>true</code> to create the index or overwrite
@@ -943,43 +883,11 @@ public class IndexWriter {
   }
 
   /**
-   * Constructs an IndexWriter for the index in <code>d</code>.
-   * Text will be analyzed with <code>a</code>.  If <code>create</code>
-   * is true, then a new, empty index will be created in
-   * <code>d</code>, replacing the index already there, if any.
-   *
-   * @param d the index directory
-   * @param a the analyzer to use
-   * @param create <code>true</code> to create the index or overwrite
-   *  the existing one; <code>false</code> to append to the existing
-   *  index
-   * @throws CorruptIndexException if the index is corrupt
-   * @throws LockObtainFailedException if another writer
-   *  has this index open (<code>write.lock</code> could not
-   *  be obtained)
-   * @throws IOException if the directory cannot be read/written to, or
-   *  if it does not exist and <code>create</code> is
-   *  <code>false</code> or if there is any other low-level
-   *  IO error
-   * @deprecated This constructor will be removed in the 3.0
-   *  release, and call {@link #commit()} when needed.
-   *  Use {@link #IndexWriter(Directory,Analyzer,boolean,MaxFieldLength)} instead.
-   */
-  public IndexWriter(Directory d, Analyzer a, boolean create)
-       throws CorruptIndexException, LockObtainFailedException, IOException {
-    init(d, a, create, null, true, DEFAULT_MAX_FIELD_LENGTH, null, null);
-  }
-
-  /**
    * Constructs an IndexWriter for the index in
    * <code>d</code>, first creating it if it does not
    * already exist.  Text will be analyzed with
    * <code>a</code>.
    *
-   * <p><b>NOTE</b>: autoCommit (see <a
-   * href="#autoCommit">above</a>) is set to false with this
-   * constructor.
-   *
    * @param d the index directory
    * @param a the analyzer to use
    * @param mfl Maximum field length in number of terms/tokens: LIMITED, UNLIMITED, or user-specified
@@ -998,96 +906,11 @@ public class IndexWriter {
   }
 
   /**
-   * Constructs an IndexWriter for the index in
-   * <code>d</code>, first creating it if it does not
-   * already exist.  Text will be analyzed with
-   * <code>a</code>.
-   *
-   * @param d the index directory
-   * @param a the analyzer to use
-   * @throws CorruptIndexException if the index is corrupt
-   * @throws LockObtainFailedException if another writer
-   *  has this index open (<code>write.lock</code> could not
-   *  be obtained)
-   * @throws IOException if the directory cannot be
-   *  read/written to or if there is any other low-level
-   *  IO error
-   * @deprecated This constructor will be removed in the 3.0 release.
-   *  Use {@link
-   *  #IndexWriter(Directory,Analyzer,MaxFieldLength)}
-   *  instead, and call {@link #commit()} when needed.
-   */
-  public IndexWriter(Directory d, Analyzer a)
-    throws CorruptIndexException, LockObtainFailedException, IOException {
-    init(d, a, null, true, DEFAULT_MAX_FIELD_LENGTH, null, null);
-  }
-
-  /**
-   * Constructs an IndexWriter for the index in
-   * <code>d</code>, first creating it if it does not
-   * already exist.  Text will be analyzed with
-   * <code>a</code>.
-   *
-   * @param d the index directory
-   * @param autoCommit see <a href="#autoCommit">above</a>
-   * @param a the analyzer to use
-   * @throws CorruptIndexException if the index is corrupt
-   * @throws LockObtainFailedException if another writer
-   *  has this index open (<code>write.lock</code> could not
-   *  be obtained)
-   * @throws IOException if the directory cannot be
-   *  read/written to or if there is any other low-level
-   *  IO error
-   * @deprecated This constructor will be removed in the 3.0 release.
-   *  Use {@link
-   *  #IndexWriter(Directory,Analyzer,MaxFieldLength)}
-   *  instead, and call {@link #commit()} when needed.
-   */
-  public IndexWriter(Directory d, boolean autoCommit, Analyzer a)
-    throws CorruptIndexException, LockObtainFailedException, IOException {
-    init(d, a, null, autoCommit, DEFAULT_MAX_FIELD_LENGTH, null, null);
-  }
-
-  /**
-   * Constructs an IndexWriter for the index in <code>d</code>.
-   * Text will be analyzed with <code>a</code>.  If <code>create</code>
-   * is true, then a new, empty index will be created in
-   * <code>d</code>, replacing the index already there, if any.
-   *
-   * @param d the index directory
-   * @param autoCommit see <a href="#autoCommit">above</a>
-   * @param a the analyzer to use
-   * @param create <code>true</code> to create the index or overwrite
-   *  the existing one; <code>false</code> to append to the existing
-   *  index
-   * @throws CorruptIndexException if the index is corrupt
-   * @throws LockObtainFailedException if another writer
-   *  has this index open (<code>write.lock</code> could not
-   *  be obtained)
-   * @throws IOException if the directory cannot be read/written to, or
-   *  if it does not exist and <code>create</code> is
-   *  <code>false</code> or if there is any other low-level
-   *  IO error
-   * @deprecated This constructor will be removed in the 3.0 release.
-   *  Use {@link
-   *  #IndexWriter(Directory,Analyzer,boolean,MaxFieldLength)}
-   *  instead, and call {@link #commit()} when needed.
-   */
-  public IndexWriter(Directory d, boolean autoCommit, Analyzer a, boolean create)
-       throws CorruptIndexException, LockObtainFailedException, IOException {
-    init(d, a, create, null, autoCommit, DEFAULT_MAX_FIELD_LENGTH, null, null);
-  }
-
-  /**
    * Expert: constructs an IndexWriter with a custom {@link
    * IndexDeletionPolicy}, for the index in <code>d</code>,
    * first creating it if it does not already exist.  Text
    * will be analyzed with <code>a</code>.
    *
-   * <p><b>NOTE</b>: autoCommit (see <a
-   * href="#autoCommit">above</a>) is set to false with this
-   * constructor.
-   *
    * @param d the index directory
    * @param a the analyzer to use
    * @param deletionPolicy see <a href="#deletionPolicy">above</a>
@@ -1107,43 +930,12 @@ public class IndexWriter {
 
   /**
    * Expert: constructs an IndexWriter with a custom {@link
-   * IndexDeletionPolicy}, for the index in <code>d</code>,
-   * first creating it if it does not already exist.  Text
-   * will be analyzed with <code>a</code>.
-   *
-   * @param d the index directory
-   * @param autoCommit see <a href="#autoCommit">above</a>
-   * @param a the analyzer to use
-   * @param deletionPolicy see <a href="#deletionPolicy">above</a>
-   * @throws CorruptIndexException if the index is corrupt
-   * @throws LockObtainFailedException if another writer
-   *  has this index open (<code>write.lock</code> could not
-   *  be obtained)
-   * @throws IOException if the directory cannot be
-   *  read/written to or if there is any other low-level
-   *  IO error
-   * @deprecated This constructor will be removed in the 3.0 release.
-   *  Use {@link
-   *  #IndexWriter(Directory,Analyzer,IndexDeletionPolicy,MaxFieldLength)}
-   *  instead, and call {@link #commit()} when needed.
-   */
-  public IndexWriter(Directory d, boolean autoCommit, Analyzer a, IndexDeletionPolicy deletionPolicy)
-    throws CorruptIndexException, LockObtainFailedException, IOException {
-    init(d, a, deletionPolicy, autoCommit, DEFAULT_MAX_FIELD_LENGTH, null, null);
-  }
-  
-  /**
-   * Expert: constructs an IndexWriter with a custom {@link
    * IndexDeletionPolicy}, for the index in <code>d</code>.
    * Text will be analyzed with <code>a</code>.  If
    * <code>create</code> is true, then a new, empty index
    * will be created in <code>d</code>, replacing the index
    * already there, if any.
    *
-   * <p><b>NOTE</b>: autoCommit (see <a
-   * href="#autoCommit">above</a>) is set to false with this
-   * constructor.
-   *
    * @param d the index directory
    * @param a the analyzer to use
    * @param create <code>true</code> to create the index or overwrite
@@ -1174,10 +966,6 @@ public class IndexWriter {
    * will be created in <code>d</code>, replacing the index
    * already there, if any.
    *
-   * <p><b>NOTE</b>: autoCommit (see <a
-   * href="#autoCommit">above</a>) is set to false with this
-   * constructor.
-   *
    * @param d the index directory
    * @param a the analyzer to use
    * @param create <code>true</code> to create the index or overwrite
@@ -1203,39 +991,6 @@ public class IndexWriter {
   }
   
   /**
-   * Expert: constructs an IndexWriter with a custom {@link
-   * IndexDeletionPolicy}, for the index in <code>d</code>.
-   * Text will be analyzed with <code>a</code>.  If
-   * <code>create</code> is true, then a new, empty index
-   * will be created in <code>d</code>, replacing the index
-   * already there, if any.
-   *
-   * @param d the index directory
-   * @param autoCommit see <a href="#autoCommit">above</a>
-   * @param a the analyzer to use
-   * @param create <code>true</code> to create the index or overwrite
-   *  the existing one; <code>false</code> to append to the existing
-   *  index
-   * @param deletionPolicy see <a href="#deletionPolicy">above</a>
-   * @throws CorruptIndexException if the index is corrupt
-   * @throws LockObtainFailedException if another writer
-   *  has this index open (<code>write.lock</code> could not
-   *  be obtained)
-   * @throws IOException if the directory cannot be read/written to, or
-   *  if it does not exist and <code>create</code> is
-   *  <code>false</code> or if there is any other low-level
-   *  IO error
-   * @deprecated This constructor will be removed in the 3.0 release.
-   *  Use {@link
-   *  #IndexWriter(Directory,Analyzer,boolean,IndexDeletionPolicy,MaxFieldLength)}
-   *  instead, and call {@link #commit()} when needed.
-   */
-  public IndexWriter(Directory d, boolean autoCommit, Analyzer a, boolean create, IndexDeletionPolicy deletionPolicy)
-          throws CorruptIndexException, LockObtainFailedException, IOException {
-    init(d, a, create, deletionPolicy, autoCommit, DEFAULT_MAX_FIELD_LENGTH, null, null);
-  }
-
-  /**
    * Expert: constructs an IndexWriter on specific commit
    * point, with a custom {@link IndexDeletionPolicy}, for
    * the index in <code>d</code>.  Text will be analyzed
@@ -1253,10 +1008,6 @@ public class IndexWriter {
    * {@link IndexDeletionPolicy} has preserved past
    * commits.
    *
-   * <p><b>NOTE</b>: autoCommit (see <a
-   * href="#autoCommit">above</a>) is set to false with this
-   * constructor.
-   *
    * @param d the index directory
    * @param a the analyzer to use
    * @param deletionPolicy see <a href="#deletionPolicy">above</a>
@@ -1290,6 +1041,8 @@ public class IndexWriter {
                     IndexDeletionPolicy deletionPolicy, boolean autoCommit, int maxFieldLength,
                     IndexingChain indexingChain, IndexCommit commit)
     throws CorruptIndexException, LockObtainFailedException, IOException {
+
+    assert !autoCommit;
     directory = d;
     analyzer = a;
     setMessageID(defaultInfoStream);
@@ -1706,31 +1459,6 @@ public class IndexWriter {
     return getLogMergePolicy().getMergeFactor();
   }
 
-  /**
-   * Expert: returns max delay inserted before syncing a
-   * commit point.  On Windows, at least, pausing before
-   * syncing can increase net indexing throughput.  The
-   * delay is variable based on size of the segment's files,
-   * and is only inserted when using
-   * ConcurrentMergeScheduler for merges.
-   * @deprecated This will be removed in 3.0, when
-   * autoCommit=true is removed from IndexWriter.
-   */
-  public double getMaxSyncPauseSeconds() {
-    return maxSyncPauseSeconds;
-  }
-
-  /**
-   * Expert: sets the max delay before syncing a commit
-   * point.
-   * @see #getMaxSyncPauseSeconds
-   * @deprecated This will be removed in 3.0, when
-   * autoCommit=true is removed from IndexWriter.
-   */
-  public void setMaxSyncPauseSeconds(double seconds) {
-    maxSyncPauseSeconds = seconds;
-  }
-
   /** If non-null, this will be the default infoStream used
    * by a newly instantiated IndexWriter.
    * @see #setInfoStream
@@ -1763,7 +1491,6 @@ public class IndexWriter {
 
   private void messageState() {
     message("setInfoStream: dir=" + directory +
-            " autoCommit=" + autoCommit +
             " mergePolicy=" + mergePolicy +
             " mergeScheduler=" + mergeScheduler +
             " ramBufferSizeMB=" + docWriter.getRAMBufferSizeMB() +
@@ -2041,6 +1768,7 @@ public class IndexWriter {
           if (infoStream != null)
             message("hit exception building compound file doc store for segment " + docStoreSegment);
           deleter.deleteFile(compoundFileName);
+          docWriter.abort();
         }
       }
 
@@ -3020,12 +2748,8 @@ public class IndexWriter {
    * This removes any temporary files that had been created,
    * after which the state of the index will be the same as
    * it was when commit() was last called or when this
-   * writer was first opened.  This can only be called when
-   * this IndexWriter was opened with
-   * <code>autoCommit=false</code>.  This also clears a
-   * previous call to {@link #prepareCommit}.
-   * @throws IllegalStateException if this is called when
-   *  the writer was opened with <code>autoCommit=true</code>.
+   * writer was first opened.  This also clears a previous
+   * call to {@link #prepareCommit}.
    * @throws IOException if there is a low-level IO error
    */
   public void rollback() throws IOException {
@@ -3768,12 +3492,11 @@ public class IndexWriter {
 
   /** <p>Expert: prepare for commit, specifying
    *  commitUserData Map (String -> String).  This does the
-   *  first phase of 2-phase commit.  You can only call this
-   *  when autoCommit is false.  This method does all steps
-   *  necessary to commit changes since this writer was
-   *  opened: flushes pending added and deleted docs, syncs
-   *  the index files, writes most of next segments_N file.
-   *  After calling this you must call either {@link
+   *  first phase of 2-phase commit. This method does all
+   *  steps necessary to commit changes since this writer
+   *  was opened: flushes pending added and deleted docs,
+   *  syncs the index files, writes most of next segments_N
+   *  file.  After calling this you must call either {@link
    *  #commit()} to finish the commit, or {@link
    *  #rollback()} to revert the commit and undo all changes
    *  done since the writer was opened.</p>
@@ -3790,14 +3513,12 @@ public class IndexWriter {
    *  that's recorded into the segments file in the index,
    *  and retrievable by {@link
    *  IndexReader#getCommitUserData}.  Note that when
-   *  IndexWriter commits itself, for example if open with
-   *  autoCommit=true, or, during {@link #close}, the
+   *  IndexWriter commits itself during {@link #close}, the
    *  commitUserData is unchanged (just carried over from
    *  the prior commit).  If this is null then the previous
    *  commitUserData is kept.  Also, the commitUserData will
    *  only "stick" if there are actually changes in the
-   *  index to commit.  Therefore it's best to use this
-   *  feature only when autoCommit is false.
+   *  index to commit.
    */
   public final void prepareCommit(Map commitUserData) throws CorruptIndexException, IOException {
     prepareCommit(commitUserData, false);
@@ -3991,7 +3712,7 @@ public class IndexWriter {
       flushDocStores |= autoCommit;
       String docStoreSegment = docWriter.getDocStoreSegment();
 
-      assert docStoreSegment != null || numDocs == 0;
+      assert docStoreSegment != null || numDocs == 0: "dss=" + docStoreSegment + " numDocs=" + numDocs;
 
       if (docStoreSegment == null)
         flushDocStores = false;
diff --git a/src/java/org/apache/lucene/index/SegmentMerger.java b/src/java/org/apache/lucene/index/SegmentMerger.java
index 060ed33..f12643d 100644
--- a/src/java/org/apache/lucene/index/SegmentMerger.java
+++ b/src/java/org/apache/lucene/index/SegmentMerger.java
@@ -276,9 +276,7 @@ final class SegmentMerger {
   private final int mergeFields() throws CorruptIndexException, IOException {
 
     if (!mergeDocStores) {
-      // When we are not merging by doc stores, that means
-      // all segments were written as part of a single
-      // autoCommit=false IndexWriter session, so their field
+      // When we are not merging by doc stores, their field
       // name -> number mapping are the same.  So, we start
       // with the fieldInfos of the last segment in this
       // case, to keep that numbering.
diff --git a/src/test/org/apache/lucene/TestSnapshotDeletionPolicy.java b/src/test/org/apache/lucene/TestSnapshotDeletionPolicy.java
index 0aa58fe..f18f31e 100644
--- a/src/test/org/apache/lucene/TestSnapshotDeletionPolicy.java
+++ b/src/test/org/apache/lucene/TestSnapshotDeletionPolicy.java
@@ -50,10 +50,8 @@ public class TestSnapshotDeletionPolicy extends LuceneTestCase
   public static final String INDEX_PATH = "test.snapshots";
 
   public void testSnapshotDeletionPolicy() throws Exception {
-    File dir = new File(System.getProperty("tempDir"), INDEX_PATH);
+    File dir = _TestUtil.getTempDir(INDEX_PATH);
     try {
-      // Sometimes past test leaves the dir
-      _TestUtil.rmDir(dir);
       Directory fsDir = FSDirectory.open(dir);
       runTest(fsDir);
       fsDir.close();
@@ -70,27 +68,35 @@ public class TestSnapshotDeletionPolicy extends LuceneTestCase
     Directory dir = new MockRAMDirectory();
 
     SnapshotDeletionPolicy dp = new SnapshotDeletionPolicy(new KeepOnlyLastCommitDeletionPolicy());
-    IndexWriter writer = new IndexWriter(dir, true,new StandardAnalyzer(), dp);
-    // Force frequent commits
+    IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(), dp, IndexWriter.MaxFieldLength.UNLIMITED);
+    // Force frequent flushes
     writer.setMaxBufferedDocs(2);
     Document doc = new Document();
     doc.add(new Field("content", "aaa", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
-    for(int i=0;i<7;i++)
+    for(int i=0;i<7;i++) {
       writer.addDocument(doc);
+      if (i % 2 == 0) {
+        writer.commit();
+      }
+    }
     IndexCommit cp = (IndexCommit) dp.snapshot();
     copyFiles(dir, cp);
     writer.close();
     copyFiles(dir, cp);
     
-    writer = new IndexWriter(dir, true, new StandardAnalyzer(), dp);
+    writer = new IndexWriter(dir, new StandardAnalyzer(), dp, IndexWriter.MaxFieldLength.UNLIMITED);
     copyFiles(dir, cp);
-    for(int i=0;i<7;i++)
+    for(int i=0;i<7;i++) {
       writer.addDocument(doc);
+      if (i % 2 == 0) {
+        writer.commit();
+      }
+    }
     copyFiles(dir, cp);
     writer.close();
     copyFiles(dir, cp);
     dp.release();
-    writer = new IndexWriter(dir, true, new StandardAnalyzer(), dp);
+    writer = new IndexWriter(dir, new StandardAnalyzer(), dp, IndexWriter.MaxFieldLength.UNLIMITED);
     writer.close();
     try {
       copyFiles(dir, cp);
@@ -106,9 +112,9 @@ public class TestSnapshotDeletionPolicy extends LuceneTestCase
     final long stopTime = System.currentTimeMillis() + 7000;
 
     SnapshotDeletionPolicy dp = new SnapshotDeletionPolicy(new KeepOnlyLastCommitDeletionPolicy());
-    final IndexWriter writer = new IndexWriter(dir, true, new StandardAnalyzer(), dp);
+    final IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(), dp, IndexWriter.MaxFieldLength.UNLIMITED);
 
-    // Force frequent commits
+    // Force frequent flushes
     writer.setMaxBufferedDocs(2);
 
     final Thread t = new Thread() {
@@ -123,6 +129,13 @@ public class TestSnapshotDeletionPolicy extends LuceneTestCase
                 t.printStackTrace(System.out);
                 fail("addDocument failed");
               }
+              if (i%2 == 0) {
+                try {
+                  writer.commit();
+                } catch (Exception e) {
+                  throw new RuntimeException(e);
+                }
+              }
             }
             try {
               Thread.sleep(1);
diff --git a/src/test/org/apache/lucene/index/TestAddIndexesNoOptimize.java b/src/test/org/apache/lucene/index/TestAddIndexesNoOptimize.java
index ab9601c..6e77800 100755
--- a/src/test/org/apache/lucene/index/TestAddIndexesNoOptimize.java
+++ b/src/test/org/apache/lucene/index/TestAddIndexesNoOptimize.java
@@ -149,6 +149,7 @@ public class TestAddIndexesNoOptimize extends LuceneTestCase {
     writer.deleteDocuments(q);
 
     writer.optimize();
+    writer.commit();
 
     verifyNumDocs(dir, 1039);
     verifyTermDocs(dir, new Term("content", "aaa"), 1030);
@@ -187,6 +188,7 @@ public class TestAddIndexesNoOptimize extends LuceneTestCase {
     writer.deleteDocuments(q);
 
     writer.optimize();
+    writer.commit();
 
     verifyNumDocs(dir, 1039);
     verifyTermDocs(dir, new Term("content", "aaa"), 1030);
@@ -225,6 +227,7 @@ public class TestAddIndexesNoOptimize extends LuceneTestCase {
     writer.addIndexesNoOptimize(new Directory[] {aux});
 
     writer.optimize();
+    writer.commit();
 
     verifyNumDocs(dir, 1039);
     verifyTermDocs(dir, new Term("content", "aaa"), 1030);
@@ -425,7 +428,7 @@ public class TestAddIndexesNoOptimize extends LuceneTestCase {
 
   private IndexWriter newWriter(Directory dir, boolean create)
       throws IOException {
-    final IndexWriter writer = new IndexWriter(dir, true, new WhitespaceAnalyzer(), create);
+    final IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), create, IndexWriter.MaxFieldLength.UNLIMITED);
     writer.setMergePolicy(new LogDocMergePolicy(writer));
     return writer;
   }
diff --git a/src/test/org/apache/lucene/index/TestAtomicUpdate.java b/src/test/org/apache/lucene/index/TestAtomicUpdate.java
index 0187071..7d5fa2a 100644
--- a/src/test/org/apache/lucene/index/TestAtomicUpdate.java
+++ b/src/test/org/apache/lucene/index/TestAtomicUpdate.java
@@ -33,8 +33,8 @@ public class TestAtomicUpdate extends LuceneTestCase {
 
   public class MockIndexWriter extends IndexWriter {
 
-    public MockIndexWriter(Directory dir, boolean autoCommit, Analyzer a, boolean create) throws IOException {
-      super(dir, autoCommit, a, create);
+    public MockIndexWriter(Directory dir, Analyzer a, boolean create, IndexWriter.MaxFieldLength mfl) throws IOException {
+      super(dir, a, create, mfl);
     }
 
     boolean testPoint(String name) {
@@ -125,7 +125,7 @@ public class TestAtomicUpdate extends LuceneTestCase {
 
     TimedThread[] threads = new TimedThread[4];
 
-    IndexWriter writer = new MockIndexWriter(directory, true, ANALYZER, true);
+    IndexWriter writer = new MockIndexWriter(directory, ANALYZER, true, IndexWriter.MaxFieldLength.UNLIMITED);
     writer.setMaxBufferedDocs(7);
     writer.setMergeFactor(3);
 
@@ -134,6 +134,9 @@ public class TestAtomicUpdate extends LuceneTestCase {
       Document d = new Document();
       d.add(new Field("id", Integer.toString(i), Field.Store.YES, Field.Index.NOT_ANALYZED));
       d.add(new Field("contents", English.intToEnglish(i), Field.Store.NO, Field.Index.ANALYZED));
+      if ((i-1)%7 == 0) {
+        writer.commit();
+      }
       writer.addDocument(d);
     }
     writer.commit();
diff --git a/src/test/org/apache/lucene/index/TestConcurrentMergeScheduler.java b/src/test/org/apache/lucene/index/TestConcurrentMergeScheduler.java
index 5b3005c..113b596 100644
--- a/src/test/org/apache/lucene/index/TestConcurrentMergeScheduler.java
+++ b/src/test/org/apache/lucene/index/TestConcurrentMergeScheduler.java
@@ -33,10 +33,12 @@ public class TestConcurrentMergeScheduler extends LuceneTestCase {
   private static final Analyzer ANALYZER = new SimpleAnalyzer();
 
   private static class FailOnlyOnFlush extends MockRAMDirectory.Failure {
-    boolean doFail = false;
+    boolean doFail;
+    boolean hitExc;
 
     public void setDoFail() {
       this.doFail = true;
+      hitExc = false;
     }
     public void clearDoFail() {
       this.doFail = false;
@@ -47,6 +49,7 @@ public class TestConcurrentMergeScheduler extends LuceneTestCase {
         StackTraceElement[] trace = new Exception().getStackTrace();
         for (int i = 0; i < trace.length; i++) {
           if ("doFlush".equals(trace[i].getMethodName())) {
+            hitExc = true;
             //new RuntimeException().printStackTrace(System.out);
             throw new IOException("now failing during flush");
           }
@@ -63,33 +66,43 @@ public class TestConcurrentMergeScheduler extends LuceneTestCase {
     FailOnlyOnFlush failure = new FailOnlyOnFlush();
     directory.failOn(failure);
 
-    IndexWriter writer = new IndexWriter(directory, true, ANALYZER, true);
+    IndexWriter writer = new IndexWriter(directory, ANALYZER, true, IndexWriter.MaxFieldLength.UNLIMITED);
     ConcurrentMergeScheduler cms = new ConcurrentMergeScheduler();
     writer.setMergeScheduler(cms);
     writer.setMaxBufferedDocs(2);
     Document doc = new Document();
     Field idField = new Field("id", "", Field.Store.YES, Field.Index.NOT_ANALYZED);
     doc.add(idField);
+    int extraCount = 0;
+
     for(int i=0;i<10;i++) {
       for(int j=0;j<20;j++) {
         idField.setValue(Integer.toString(i*20+j));
         writer.addDocument(doc);
       }
 
-      writer.addDocument(doc);
-
-      failure.setDoFail();
-      try {
-        writer.flush();
-        fail("failed to hit IOException");
-      } catch (IOException ioe) {
-        failure.clearDoFail();
+      // must cycle here because sometimes the merge flushes
+      // the doc we just added and so there's nothing to
+      // flush, and we don't hit the exception
+      while(true) {
+        writer.addDocument(doc);
+        failure.setDoFail();
+        try {
+          writer.flush();
+          if (failure.hitExc) {
+            fail("failed to hit IOException");
+          }
+          extraCount++;
+        } catch (IOException ioe) {
+          failure.clearDoFail();
+          break;
+        }
       }
     }
 
     writer.close();
     IndexReader reader = IndexReader.open(directory, true);
-    assertEquals(200, reader.numDocs());
+    assertEquals(200+extraCount, reader.numDocs());
     reader.close();
     directory.close();
   }
@@ -100,7 +113,7 @@ public class TestConcurrentMergeScheduler extends LuceneTestCase {
 
     RAMDirectory directory = new MockRAMDirectory();
 
-    IndexWriter writer = new IndexWriter(directory, true, ANALYZER, true);
+    IndexWriter writer = new IndexWriter(directory, ANALYZER, true, IndexWriter.MaxFieldLength.UNLIMITED);
     ConcurrentMergeScheduler cms = new ConcurrentMergeScheduler();
     writer.setMergeScheduler(cms);
 
@@ -142,32 +155,28 @@ public class TestConcurrentMergeScheduler extends LuceneTestCase {
 
     RAMDirectory directory = new MockRAMDirectory();
 
-    for(int pass=0;pass<2;pass++) {
-
-      boolean autoCommit = pass==0;
-      IndexWriter writer = new IndexWriter(directory, autoCommit, ANALYZER, true);
+    IndexWriter writer = new IndexWriter(directory, ANALYZER, true, IndexWriter.MaxFieldLength.UNLIMITED);
 
-      for(int iter=0;iter<7;iter++) {
-        ConcurrentMergeScheduler cms = new ConcurrentMergeScheduler();
-        writer.setMergeScheduler(cms);
-        writer.setMaxBufferedDocs(2);
+    for(int iter=0;iter<7;iter++) {
+      ConcurrentMergeScheduler cms = new ConcurrentMergeScheduler();
+      writer.setMergeScheduler(cms);
+      writer.setMaxBufferedDocs(2);
 
-        for(int j=0;j<21;j++) {
-          Document doc = new Document();
-          doc.add(new Field("content", "a b c", Field.Store.NO, Field.Index.ANALYZED));
-          writer.addDocument(doc);
-        }
-        
-        writer.close();
-        TestIndexWriter.assertNoUnreferencedFiles(directory, "testNoExtraFiles autoCommit=" + autoCommit);
-
-        // Reopen
-        writer = new IndexWriter(directory, autoCommit, ANALYZER, false);
+      for(int j=0;j<21;j++) {
+        Document doc = new Document();
+        doc.add(new Field("content", "a b c", Field.Store.NO, Field.Index.ANALYZED));
+        writer.addDocument(doc);
       }
-
+        
       writer.close();
+      TestIndexWriter.assertNoUnreferencedFiles(directory, "testNoExtraFiles");
+
+      // Reopen
+      writer = new IndexWriter(directory, ANALYZER, false, IndexWriter.MaxFieldLength.UNLIMITED);
     }
 
+    writer.close();
+
     directory.close();
   }
 
@@ -178,44 +187,41 @@ public class TestConcurrentMergeScheduler extends LuceneTestCase {
     Field idField = new Field("id", "", Field.Store.YES, Field.Index.NOT_ANALYZED);
     doc.add(idField);
 
-    for(int pass=0;pass<2;pass++) {
-      boolean autoCommit = pass==0;
-      IndexWriter writer = new IndexWriter(directory, autoCommit, ANALYZER, true);
+    IndexWriter writer = new IndexWriter(directory, ANALYZER, true, IndexWriter.MaxFieldLength.UNLIMITED);
 
-      for(int iter=0;iter<10;iter++) {
-        ConcurrentMergeScheduler cms = new ConcurrentMergeScheduler();
-        writer.setMergeScheduler(cms);
-        writer.setMaxBufferedDocs(2);
-        writer.setMergeFactor(100);
+    for(int iter=0;iter<10;iter++) {
+      ConcurrentMergeScheduler cms = new ConcurrentMergeScheduler();
+      writer.setMergeScheduler(cms);
+      writer.setMaxBufferedDocs(2);
+      writer.setMergeFactor(100);
 
-        for(int j=0;j<201;j++) {
-          idField.setValue(Integer.toString(iter*201+j));
-          writer.addDocument(doc);
-        }
+      for(int j=0;j<201;j++) {
+        idField.setValue(Integer.toString(iter*201+j));
+        writer.addDocument(doc);
+      }
 
-        int delID = iter*201;
-        for(int j=0;j<20;j++) {
-          writer.deleteDocuments(new Term("id", Integer.toString(delID)));
-          delID += 5;
-        }
+      int delID = iter*201;
+      for(int j=0;j<20;j++) {
+        writer.deleteDocuments(new Term("id", Integer.toString(delID)));
+        delID += 5;
+      }
 
-        // Force a bunch of merge threads to kick off so we
-        // stress out aborting them on close:
-        writer.setMergeFactor(3);
-        writer.addDocument(doc);
-        writer.flush();
+      // Force a bunch of merge threads to kick off so we
+      // stress out aborting them on close:
+      writer.setMergeFactor(3);
+      writer.addDocument(doc);
+      writer.flush();
 
-        writer.close(false);
+      writer.close(false);
 
-        IndexReader reader = IndexReader.open(directory, true);
-        assertEquals((1+iter)*182, reader.numDocs());
-        reader.close();
+      IndexReader reader = IndexReader.open(directory, true);
+      assertEquals((1+iter)*182, reader.numDocs());
+      reader.close();
 
-        // Reopen
-        writer = new IndexWriter(directory, autoCommit, ANALYZER, false);
-      }
-      writer.close();
+      // Reopen
+      writer = new IndexWriter(directory, ANALYZER, false, IndexWriter.MaxFieldLength.UNLIMITED);
     }
+    writer.close();
 
     directory.close();
   }
diff --git a/src/test/org/apache/lucene/index/TestCrash.java b/src/test/org/apache/lucene/index/TestCrash.java
index 9a56729..f415563 100644
--- a/src/test/org/apache/lucene/index/TestCrash.java
+++ b/src/test/org/apache/lucene/index/TestCrash.java
@@ -35,7 +35,7 @@ public class TestCrash extends LuceneTestCase {
   private IndexWriter initIndex(MockRAMDirectory dir) throws IOException {
     dir.setLockFactory(NoLockFactory.getNoLockFactory());
 
-    IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer());
+    IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.UNLIMITED);
     //writer.setMaxBufferedDocs(2);
     writer.setMaxBufferedDocs(10);
     ((ConcurrentMergeScheduler) writer.getMergeScheduler()).setSuppressExceptions();
diff --git a/src/test/org/apache/lucene/index/TestDeletionPolicy.java b/src/test/org/apache/lucene/index/TestDeletionPolicy.java
index c3b3031..c4751ff 100644
--- a/src/test/org/apache/lucene/index/TestDeletionPolicy.java
+++ b/src/test/org/apache/lucene/index/TestDeletionPolicy.java
@@ -83,8 +83,8 @@ public class TestDeletionPolicy extends LuceneTestCase
   }
 
   /**
-   * This is useful for adding to a big index w/ autoCommit
-   * false when you know readers are not using it.
+   * This is useful for adding to a big index when you know
+   * readers are not using it.
    */
   class KeepNoneOnInitDeletionPolicy implements IndexDeletionPolicy {
     int numOnInit;
@@ -202,12 +202,11 @@ public class TestDeletionPolicy extends LuceneTestCase
 
     final double SECONDS = 2.0;
 
-    boolean autoCommit = false;
     boolean useCompoundFile = true;
 
     Directory dir = new RAMDirectory();
     ExpirationTimeDeletionPolicy policy = new ExpirationTimeDeletionPolicy(dir, SECONDS);
-    IndexWriter writer = new IndexWriter(dir, autoCommit, new WhitespaceAnalyzer(), true, policy);
+    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true, policy, IndexWriter.MaxFieldLength.UNLIMITED);
     writer.setUseCompoundFile(useCompoundFile);
     writer.close();
 
@@ -216,7 +215,7 @@ public class TestDeletionPolicy extends LuceneTestCase
       // Record last time when writer performed deletes of
       // past commits
       lastDeleteTime = System.currentTimeMillis();
-      writer = new IndexWriter(dir, autoCommit, new WhitespaceAnalyzer(), false, policy);
+      writer = new IndexWriter(dir, new WhitespaceAnalyzer(), false, policy, IndexWriter.MaxFieldLength.UNLIMITED);
       writer.setUseCompoundFile(useCompoundFile);
       for(int j=0;j<17;j++) {
         addDoc(writer);
@@ -267,10 +266,9 @@ public class TestDeletionPolicy extends LuceneTestCase
    */
   public void testKeepAllDeletionPolicy() throws IOException {
 
-    for(int pass=0;pass<4;pass++) {
+    for(int pass=0;pass<2;pass++) {
 
-      boolean autoCommit = pass < 2;
-      boolean useCompoundFile = (pass % 2) > 0;
+      boolean useCompoundFile = (pass % 2) != 0;
 
       // Never deletes a commit
       KeepAllDeletionPolicy policy = new KeepAllDeletionPolicy();
@@ -278,37 +276,30 @@ public class TestDeletionPolicy extends LuceneTestCase
       Directory dir = new RAMDirectory();
       policy.dir = dir;
 
-      IndexWriter writer = new IndexWriter(dir, autoCommit, new WhitespaceAnalyzer(), true, policy);
+      IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true, policy, IndexWriter.MaxFieldLength.UNLIMITED);
       writer.setMaxBufferedDocs(10);
       writer.setUseCompoundFile(useCompoundFile);
       writer.setMergeScheduler(new SerialMergeScheduler());
       for(int i=0;i<107;i++) {
         addDoc(writer);
-        if (autoCommit && i%10 == 0)
-          writer.commit();
       }
       writer.close();
 
-      writer = new IndexWriter(dir, autoCommit, new WhitespaceAnalyzer(), false, policy);
+      writer = new IndexWriter(dir, new WhitespaceAnalyzer(), false, policy, IndexWriter.MaxFieldLength.UNLIMITED);
       writer.setUseCompoundFile(useCompoundFile);
       writer.optimize();
       writer.close();
 
       assertEquals(2, policy.numOnInit);
-      if (!autoCommit)
-        // If we are not auto committing then there should
-        // be exactly 2 commits (one per close above):
-        assertEquals(2, policy.numOnCommit);
+
+      // If we are not auto committing then there should
+      // be exactly 2 commits (one per close above):
+      assertEquals(2, policy.numOnCommit);
 
       // Test listCommits
       Collection commits = IndexReader.listCommits(dir);
-      if (!autoCommit)
-        // 1 from opening writer + 2 from closing writer
-        assertEquals(3, commits.size());
-      else
-        // 1 from opening writer + 2 from closing writer +
-        // 11 from calling writer.commit() explicitly above
-        assertEquals(14, commits.size());
+      // 1 from opening writer + 2 from closing writer
+      assertEquals(3, commits.size());
 
       Iterator it = commits.iterator();
       // Make sure we can open a reader on each commit:
@@ -448,21 +439,20 @@ public class TestDeletionPolicy extends LuceneTestCase
 
 
   /* Test keeping NO commit points.  This is a viable and
-   * useful case eg where you want to build a big index with
-   * autoCommit false and you know there are no readers.
+   * useful case eg where you want to build a big index and
+   * you know there are no readers.
    */
   public void testKeepNoneOnInitDeletionPolicy() throws IOException {
 
-    for(int pass=0;pass<4;pass++) {
+    for(int pass=0;pass<2;pass++) {
 
-      boolean autoCommit = pass < 2;
-      boolean useCompoundFile = (pass % 2) > 0;
+      boolean useCompoundFile = (pass % 2) != 0;
 
       KeepNoneOnInitDeletionPolicy policy = new KeepNoneOnInitDeletionPolicy();
 
       Directory dir = new RAMDirectory();
 
-      IndexWriter writer = new IndexWriter(dir, autoCommit, new WhitespaceAnalyzer(), true, policy);
+      IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true, policy, IndexWriter.MaxFieldLength.UNLIMITED);
       writer.setMaxBufferedDocs(10);
       writer.setUseCompoundFile(useCompoundFile);
       for(int i=0;i<107;i++) {
@@ -470,16 +460,15 @@ public class TestDeletionPolicy extends LuceneTestCase
       }
       writer.close();
 
-      writer = new IndexWriter(dir, autoCommit, new WhitespaceAnalyzer(), false, policy);
+      writer = new IndexWriter(dir, new WhitespaceAnalyzer(), false, policy, IndexWriter.MaxFieldLength.UNLIMITED);
       writer.setUseCompoundFile(useCompoundFile);
       writer.optimize();
       writer.close();
 
       assertEquals(2, policy.numOnInit);
-      if (!autoCommit)
-        // If we are not auto committing then there should
-        // be exactly 2 commits (one per close above):
-        assertEquals(2, policy.numOnCommit);
+      // If we are not auto committing then there should
+      // be exactly 2 commits (one per close above):
+      assertEquals(2, policy.numOnCommit);
 
       // Simplistic check: just verify the index is in fact
       // readable:
@@ -497,17 +486,16 @@ public class TestDeletionPolicy extends LuceneTestCase
 
     final int N = 5;
 
-    for(int pass=0;pass<4;pass++) {
+    for(int pass=0;pass<2;pass++) {
 
-      boolean autoCommit = pass < 2;
-      boolean useCompoundFile = (pass % 2) > 0;
+      boolean useCompoundFile = (pass % 2) != 0;
 
       Directory dir = new RAMDirectory();
 
       KeepLastNDeletionPolicy policy = new KeepLastNDeletionPolicy(N);
 
       for(int j=0;j<N+1;j++) {
-        IndexWriter writer = new IndexWriter(dir, autoCommit, new WhitespaceAnalyzer(), true, policy);
+        IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true, policy, IndexWriter.MaxFieldLength.UNLIMITED);
         writer.setMaxBufferedDocs(10);
         writer.setUseCompoundFile(useCompoundFile);
         for(int i=0;i<17;i++) {
@@ -519,11 +507,7 @@ public class TestDeletionPolicy extends LuceneTestCase
 
       assertTrue(policy.numDelete > 0);
       assertEquals(N+1, policy.numOnInit);
-      if (autoCommit) {
-        assertTrue(policy.numOnCommit > 1);
-      } else {
-        assertEquals(N+1, policy.numOnCommit);
-      }
+      assertEquals(N+1, policy.numOnCommit);
 
       // Simplistic check: just verify only the past N segments_N's still
       // exist, and, I can open a reader on each:
@@ -559,27 +543,26 @@ public class TestDeletionPolicy extends LuceneTestCase
 
     final int N = 10;
 
-    for(int pass=0;pass<4;pass++) {
+    for(int pass=0;pass<2;pass++) {
 
-      boolean autoCommit = pass < 2;
-      boolean useCompoundFile = (pass % 2) > 0;
+      boolean useCompoundFile = (pass % 2) != 0;
 
       KeepLastNDeletionPolicy policy = new KeepLastNDeletionPolicy(N);
 
       Directory dir = new RAMDirectory();
-      IndexWriter writer = new IndexWriter(dir, autoCommit, new WhitespaceAnalyzer(), true, policy);
+      IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true, policy, IndexWriter.MaxFieldLength.UNLIMITED);
       writer.setUseCompoundFile(useCompoundFile);
       writer.close();
       Term searchTerm = new Term("content", "aaa");        
       Query query = new TermQuery(searchTerm);
 
       for(int i=0;i<N+1;i++) {
-        writer = new IndexWriter(dir, autoCommit, new WhitespaceAnalyzer(), false, policy);
+        writer = new IndexWriter(dir, new WhitespaceAnalyzer(), false, policy, IndexWriter.MaxFieldLength.UNLIMITED);
         writer.setUseCompoundFile(useCompoundFile);
         for(int j=0;j<17;j++) {
           addDoc(writer);
         }
-        // this is a commit when autoCommit=false:
+        // this is a commit
         writer.close();
         IndexReader reader = IndexReader.open(dir, policy, false);
         reader.deleteDocument(3*i+1);
@@ -587,19 +570,18 @@ public class TestDeletionPolicy extends LuceneTestCase
         IndexSearcher searcher = new IndexSearcher(reader);
         ScoreDoc[] hits = searcher.search(query, null, 1000).scoreDocs;
         assertEquals(16*(1+i), hits.length);
-        // this is a commit when autoCommit=false:
+        // this is a commit
         reader.close();
         searcher.close();
       }
-      writer = new IndexWriter(dir, autoCommit, new WhitespaceAnalyzer(), false, policy);
+      writer = new IndexWriter(dir, new WhitespaceAnalyzer(), false, policy, IndexWriter.MaxFieldLength.UNLIMITED);
       writer.setUseCompoundFile(useCompoundFile);
       writer.optimize();
-      // this is a commit when autoCommit=false:
+      // this is a commit
       writer.close();
 
       assertEquals(2*(N+2), policy.numOnInit);
-      if (!autoCommit)
-        assertEquals(2*(N+2)-1, policy.numOnCommit);
+      assertEquals(2*(N+2)-1, policy.numOnCommit);
 
       IndexSearcher searcher = new IndexSearcher(dir, false);
       ScoreDoc[] hits = searcher.search(query, null, 1000).scoreDocs;
@@ -617,21 +599,18 @@ public class TestDeletionPolicy extends LuceneTestCase
           IndexReader reader = IndexReader.open(dir, true);
 
           // Work backwards in commits on what the expected
-          // count should be.  Only check this in the
-          // autoCommit false case:
-          if (!autoCommit) {
-            searcher = new IndexSearcher(reader);
-            hits = searcher.search(query, null, 1000).scoreDocs;
-            if (i > 1) {
-              if (i % 2 == 0) {
-                expectedCount += 1;
-              } else {
-                expectedCount -= 17;
-              }
+          // count should be.
+          searcher = new IndexSearcher(reader);
+          hits = searcher.search(query, null, 1000).scoreDocs;
+          if (i > 1) {
+            if (i % 2 == 0) {
+              expectedCount += 1;
+            } else {
+              expectedCount -= 17;
             }
-            assertEquals(expectedCount, hits.length);
-            searcher.close();
           }
+          assertEquals(expectedCount, hits.length);
+          searcher.close();
           reader.close();
           if (i == N) {
             fail("should have failed on commits before last 5");
@@ -659,15 +638,14 @@ public class TestDeletionPolicy extends LuceneTestCase
 
     final int N = 10;
 
-    for(int pass=0;pass<4;pass++) {
+    for(int pass=0;pass<2;pass++) {
 
-      boolean autoCommit = pass < 2;
-      boolean useCompoundFile = (pass % 2) > 0;
+      boolean useCompoundFile = (pass % 2) != 0;
 
       KeepLastNDeletionPolicy policy = new KeepLastNDeletionPolicy(N);
 
       Directory dir = new RAMDirectory();
-      IndexWriter writer = new IndexWriter(dir, autoCommit, new WhitespaceAnalyzer(), true, policy);
+      IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true, policy, IndexWriter.MaxFieldLength.UNLIMITED);
       writer.setMaxBufferedDocs(10);
       writer.setUseCompoundFile(useCompoundFile);
       writer.close();
@@ -676,13 +654,13 @@ public class TestDeletionPolicy extends LuceneTestCase
 
       for(int i=0;i<N+1;i++) {
 
-        writer = new IndexWriter(dir, autoCommit, new WhitespaceAnalyzer(), false, policy);
+        writer = new IndexWriter(dir, new WhitespaceAnalyzer(), false, policy, IndexWriter.MaxFieldLength.UNLIMITED);
         writer.setMaxBufferedDocs(10);
         writer.setUseCompoundFile(useCompoundFile);
         for(int j=0;j<17;j++) {
           addDoc(writer);
         }
-        // this is a commit when autoCommit=false:
+        // this is a commit
         writer.close();
         IndexReader reader = IndexReader.open(dir, policy, false);
         reader.deleteDocument(3);
@@ -690,19 +668,18 @@ public class TestDeletionPolicy extends LuceneTestCase
         IndexSearcher searcher = new IndexSearcher(reader);
         ScoreDoc[] hits = searcher.search(query, null, 1000).scoreDocs;
         assertEquals(16, hits.length);
-        // this is a commit when autoCommit=false:
+        // this is a commit
         reader.close();
         searcher.close();
 
-        writer = new IndexWriter(dir, autoCommit, new WhitespaceAnalyzer(), true, policy);
+        writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true, policy, IndexWriter.MaxFieldLength.UNLIMITED);
         // This will not commit: there are no changes
         // pending because we opened for "create":
         writer.close();
       }
 
       assertEquals(1+3*(N+1), policy.numOnInit);
-      if (!autoCommit)
-        assertEquals(3*(N+1), policy.numOnCommit);
+      assertEquals(3*(N+1), policy.numOnCommit);
 
       IndexSearcher searcher = new IndexSearcher(dir, false);
       ScoreDoc[] hits = searcher.search(query, null, 1000).scoreDocs;
@@ -720,20 +697,17 @@ public class TestDeletionPolicy extends LuceneTestCase
           IndexReader reader = IndexReader.open(dir, true);
 
           // Work backwards in commits on what the expected
-          // count should be.  Only check this in the
-          // autoCommit false case:
-          if (!autoCommit) {
-            searcher = new IndexSearcher(reader);
-            hits = searcher.search(query, null, 1000).scoreDocs;
-            assertEquals(expectedCount, hits.length);
-            searcher.close();
-            if (expectedCount == 0) {
-              expectedCount = 16;
-            } else if (expectedCount == 16) {
-              expectedCount = 17;
-            } else if (expectedCount == 17) {
-              expectedCount = 0;
-            }
+          // count should be.
+          searcher = new IndexSearcher(reader);
+          hits = searcher.search(query, null, 1000).scoreDocs;
+          assertEquals(expectedCount, hits.length);
+          searcher.close();
+          if (expectedCount == 0) {
+            expectedCount = 16;
+          } else if (expectedCount == 16) {
+            expectedCount = 17;
+          } else if (expectedCount == 17) {
+            expectedCount = 0;
           }
           reader.close();
           if (i == N) {
diff --git a/src/test/org/apache/lucene/index/TestIndexWriter.java b/src/test/org/apache/lucene/index/TestIndexWriter.java
index 1a70852..ad7167d 100644
--- a/src/test/org/apache/lucene/index/TestIndexWriter.java
+++ b/src/test/org/apache/lucene/index/TestIndexWriter.java
@@ -117,7 +117,7 @@ public class TestIndexWriter extends BaseTokenStreamTestCase {
         reader.close();
 
         // optimize the index and check that the new doc count is correct
-        writer = new IndexWriter(dir, true, new WhitespaceAnalyzer());
+        writer = new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.UNLIMITED);
         assertEquals(100, writer.maxDoc());
         assertEquals(60, writer.numDocs());
         writer.optimize();
@@ -227,7 +227,7 @@ public class TestIndexWriter extends BaseTokenStreamTestCase {
         startDiskUsage += startDir.fileLength(files[i]);
       }
 
-      for(int iter=0;iter<6;iter++) {
+      for(int iter=0;iter<3;iter++) {
 
         if (debug)
           System.out.println("TEST: iter=" + iter);
@@ -235,8 +235,7 @@ public class TestIndexWriter extends BaseTokenStreamTestCase {
         // Start with 100 bytes more than we are currently using:
         long diskFree = diskUsage+100;
 
-        boolean autoCommit = iter % 2 == 0;
-        int method = iter/2;
+        int method = iter;
 
         boolean success = false;
         boolean done = false;
@@ -254,7 +253,7 @@ public class TestIndexWriter extends BaseTokenStreamTestCase {
 
           // Make a new dir that will enforce disk usage:
           MockRAMDirectory dir = new MockRAMDirectory(startDir);
-          writer = new IndexWriter(dir, autoCommit, new WhitespaceAnalyzer(), false);
+          writer = new IndexWriter(dir, new WhitespaceAnalyzer(), false, IndexWriter.MaxFieldLength.UNLIMITED);
           IOException err = null;
 
           MergeScheduler ms = writer.getMergeScheduler();
@@ -290,12 +289,12 @@ public class TestIndexWriter extends BaseTokenStreamTestCase {
                 rate = 0.0;
               }
               if (debug)
-                testName = "disk full test " + methodName + " with disk full at " + diskFree + " bytes autoCommit=" + autoCommit;
+                testName = "disk full test " + methodName + " with disk full at " + diskFree + " bytes";
             } else {
               thisDiskFree = 0;
               rate = 0.0;
               if (debug)
-                testName = "disk full test " + methodName + " with unlimited disk space autoCommit=" + autoCommit;
+                testName = "disk full test " + methodName + " with unlimited disk space";
             }
 
             if (debug)
@@ -351,29 +350,6 @@ public class TestIndexWriter extends BaseTokenStreamTestCase {
             // ConcurrentMergeScheduler are done
             _TestUtil.syncConcurrentMerges(writer);
 
-            if (autoCommit) {
-
-              // Whether we succeeded or failed, check that
-              // all un-referenced files were in fact
-              // deleted (ie, we did not create garbage).
-              // Only check this when autoCommit is true:
-              // when it's false, it's expected that there
-              // are unreferenced files (ie they won't be
-              // referenced until the "commit on close").
-              // Just create a new IndexFileDeleter, have it
-              // delete unreferenced files, then verify that
-              // in fact no files were deleted:
-
-              String successStr;
-              if (success) {
-                successStr = "success";
-              } else {
-                successStr = "IOException";
-              }
-              String message = methodName + " failed to delete unreferenced files after " + successStr + " (" + diskFree + " bytes)";
-              assertNoUnreferencedFiles(dir, message);
-            }
-
             if (debug) {
               System.out.println("  now test readers");
             }
@@ -390,10 +366,8 @@ public class TestIndexWriter extends BaseTokenStreamTestCase {
             }
             int result = reader.docFreq(searchTerm);
             if (success) {
-              if (autoCommit && result != END_COUNT) {
-                fail(testName + ": method did not throw exception but docFreq('aaa') is " + result + " instead of expected " + END_COUNT);
-              } else if (!autoCommit && result != START_COUNT) {
-                fail(testName + ": method did not throw exception but docFreq('aaa') is " + result + " instead of expected " + START_COUNT + " [autoCommit = false]");
+              if (result != START_COUNT) {
+                fail(testName + ": method did not throw exception but docFreq('aaa') is " + result + " instead of expected " + START_COUNT);
               }
             } else {
               // On hitting exception we still may have added
@@ -480,18 +454,17 @@ public class TestIndexWriter extends BaseTokenStreamTestCase {
 
       boolean debug = false;
 
-      for(int pass=0;pass<3;pass++) {
+      for(int pass=0;pass<2;pass++) {
         if (debug)
           System.out.println("TEST: pass=" + pass);
-        boolean autoCommit = pass == 0;
-        boolean doAbort = pass == 2;
+        boolean doAbort = pass == 1;
         long diskFree = 200;
         while(true) {
           if (debug)
             System.out.println("TEST: cycle: diskFree=" + diskFree);
           MockRAMDirectory dir = new MockRAMDirectory();
           dir.setMaxSizeInBytes(diskFree);
-          IndexWriter writer = new IndexWriter(dir, autoCommit, new WhitespaceAnalyzer(), true);
+          IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
 
           MergeScheduler ms = writer.getMergeScheduler();
           if (ms instanceof ConcurrentMergeScheduler)
@@ -531,7 +504,7 @@ public class TestIndexWriter extends BaseTokenStreamTestCase {
 
             _TestUtil.syncConcurrentMerges(ms);
 
-            assertNoUnreferencedFiles(dir, "after disk full during addDocument with autoCommit=" + autoCommit);
+            assertNoUnreferencedFiles(dir, "after disk full during addDocument");
 
             // Make sure reader can open the index:
             IndexReader.open(dir, true).close();
@@ -947,10 +920,9 @@ public class TestIndexWriter extends BaseTokenStreamTestCase {
     }
 
     /*
-     * Simple test for "commit on close": open writer with
-     * autoCommit=false, so it will only commit on close,
-     * then add a bunch of docs, making sure reader does not
-     * see these docs until writer is closed.
+     * Simple test for "commit on close": open writer then
+     * add a bunch of docs, making sure reader does not see
+     * these docs until writer is closed.
      */
     public void testCommitOnClose() throws IOException {
         Directory dir = new RAMDirectory();      
@@ -975,7 +947,7 @@ public class TestIndexWriter extends BaseTokenStreamTestCase {
           }
           searcher = new IndexSearcher(dir, false);
           hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;
-          assertEquals("reader incorrectly sees changes from writer with autoCommit disabled", 14, hits.length);
+          assertEquals("reader incorrectly sees changes from writer", 14, hits.length);
           searcher.close();
           assertTrue("reader should have still been current", reader.isCurrent());
         }
@@ -991,10 +963,9 @@ public class TestIndexWriter extends BaseTokenStreamTestCase {
     }
 
     /*
-     * Simple test for "commit on close": open writer with
-     * autoCommit=false, so it will only commit on close,
-     * then add a bunch of docs, making sure reader does not
-     * see them until writer has closed.  Then instead of
+     * Simple test for "commit on close": open writer, then
+     * add a bunch of docs, making sure reader does not see
+     * them until writer has closed.  Then instead of
      * closing the writer, call abort and verify reader sees
      * nothing was added.  Then verify we can open the index
      * and add docs to it.
@@ -1024,7 +995,7 @@ public class TestIndexWriter extends BaseTokenStreamTestCase {
 
       searcher = new IndexSearcher(dir, false);
       hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;
-      assertEquals("reader incorrectly sees changes from writer with autoCommit disabled", 14, hits.length);
+      assertEquals("reader incorrectly sees changes from writer", 14, hits.length);
       searcher.close();
 
       // Now, close the writer:
@@ -1052,7 +1023,7 @@ public class TestIndexWriter extends BaseTokenStreamTestCase {
         }
         searcher = new IndexSearcher(dir, false);
         hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;
-        assertEquals("reader incorrectly sees changes from writer with autoCommit disabled", 14, hits.length);
+        assertEquals("reader incorrectly sees changes from writer", 14, hits.length);
         searcher.close();
       }
 
@@ -1103,9 +1074,9 @@ public class TestIndexWriter extends BaseTokenStreamTestCase {
       // and it doesn't delete intermediate segments then it
       // will exceed this 100X:
       // System.out.println("start " + startDiskUsage + "; mid " + midDiskUsage + ";end " + endDiskUsage);
-      assertTrue("writer used too much space while adding documents when autoCommit=false: mid=" + midDiskUsage + " start=" + startDiskUsage + " end=" + endDiskUsage,
+      assertTrue("writer used too much space while adding documents: mid=" + midDiskUsage + " start=" + startDiskUsage + " end=" + endDiskUsage,
                  midDiskUsage < 100*startDiskUsage);
-      assertTrue("writer used too much space after close when autoCommit=false endDiskUsage=" + endDiskUsage + " startDiskUsage=" + startDiskUsage,
+      assertTrue("writer used too much space after close: endDiskUsage=" + endDiskUsage + " startDiskUsage=" + startDiskUsage,
                  endDiskUsage < 100*startDiskUsage);
     }
 
@@ -2116,15 +2087,15 @@ public class TestIndexWriter extends BaseTokenStreamTestCase {
     Field idField = new Field("id", "", Field.Store.YES, Field.Index.NOT_ANALYZED);
     doc.add(idField);
 
-    for(int pass=0;pass<3;pass++) {
-      boolean autoCommit = pass%2 == 0;
-      IndexWriter writer = new IndexWriter(directory, autoCommit, new WhitespaceAnalyzer(), true);
+    for(int pass=0;pass<2;pass++) {
+
+      IndexWriter writer = new IndexWriter(directory, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
 
-      //System.out.println("TEST: pass=" + pass + " ac=" + autoCommit + " cms=" + (pass >= 2));
+      //System.out.println("TEST: pass=" + pass + " cms=" + (pass >= 2));
       for(int iter=0;iter<10;iter++) {
         //System.out.println("TEST: iter=" + iter);
         MergeScheduler ms;
-        if (pass >= 2)
+        if (pass == 1)
           ms = new ConcurrentMergeScheduler();
         else
           ms = new SerialMergeScheduler();
@@ -2189,7 +2160,7 @@ public class TestIndexWriter extends BaseTokenStreamTestCase {
         reader.close();
 
         // Reopen
-        writer = new IndexWriter(directory, autoCommit, new WhitespaceAnalyzer(), false);
+        writer = new IndexWriter(directory, new WhitespaceAnalyzer(), false, IndexWriter.MaxFieldLength.UNLIMITED);
       }
       writer.close();
     }
@@ -2360,7 +2331,7 @@ public class TestIndexWriter extends BaseTokenStreamTestCase {
 
     for(int iter=0;iter<10;iter++) {
       MockRAMDirectory dir = new MockRAMDirectory();
-      IndexWriter writer = new IndexWriter(dir, true, new WhitespaceAnalyzer());
+      IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.UNLIMITED);
       ConcurrentMergeScheduler cms = new ConcurrentMergeScheduler();
       // We expect disk full exceptions in the merge threads
       cms.setSuppressExceptions();
@@ -2421,7 +2392,7 @@ public class TestIndexWriter extends BaseTokenStreamTestCase {
   public void _testSingleThreadFailure(MockRAMDirectory.Failure failure) throws IOException {
     MockRAMDirectory dir = new MockRAMDirectory();
 
-    IndexWriter writer = new IndexWriter(dir, true, new WhitespaceAnalyzer());
+    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.UNLIMITED);
     writer.setMaxBufferedDocs(2);
     final Document doc = new Document();
     doc.add(new Field("field", "aaa bbb ccc ddd eee fff ggg hhh iii jjj", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
@@ -2434,6 +2405,7 @@ public class TestIndexWriter extends BaseTokenStreamTestCase {
     try {
       writer.addDocument(doc);
       writer.addDocument(doc);
+      writer.commit();
       fail("did not hit exception");
     } catch (IOException ioe) {
     }
@@ -2721,7 +2693,7 @@ public class TestIndexWriter extends BaseTokenStreamTestCase {
     FailOnlyInSync failure = new FailOnlyInSync();
     dir.failOn(failure);
 
-    IndexWriter writer  = new IndexWriter(dir, true, new WhitespaceAnalyzer());
+    IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.UNLIMITED);
     failure.setDoFail();
 
     ConcurrentMergeScheduler cms = new ConcurrentMergeScheduler();
@@ -2731,8 +2703,16 @@ public class TestIndexWriter extends BaseTokenStreamTestCase {
     writer.setMaxBufferedDocs(2);
     writer.setMergeFactor(5);
 
-    for (int i = 0; i < 23; i++)
+    for (int i = 0; i < 23; i++) {
       addDoc(writer);
+      if ((i-1)%2 == 0) {
+        try {
+          writer.commit();
+        } catch (IOException ioe) {
+          // expected
+        }
+      }
+    }
 
     cms.sync();
     assertTrue(failure.didFail);
@@ -2749,10 +2729,9 @@ public class TestIndexWriter extends BaseTokenStreamTestCase {
   public void testTermVectorCorruption() throws IOException {
 
     Directory dir = new MockRAMDirectory();
-    for(int iter=0;iter<4;iter++) {
-      final boolean autoCommit = 1==iter/2;
+    for(int iter=0;iter<2;iter++) {
       IndexWriter writer = new IndexWriter(dir,
-                                           autoCommit, new StandardAnalyzer());
+                                           new StandardAnalyzer(), IndexWriter.MaxFieldLength.UNLIMITED);
       writer.setMaxBufferedDocs(2);
       writer.setRAMBufferSizeMB(IndexWriter.DISABLE_AUTO_FLUSH);
       writer.setMergeScheduler(new SerialMergeScheduler());
@@ -2785,7 +2764,7 @@ public class TestIndexWriter extends BaseTokenStreamTestCase {
       reader.close();
 
       writer = new IndexWriter(dir,
-                               autoCommit, new StandardAnalyzer());
+                               new StandardAnalyzer(), IndexWriter.MaxFieldLength.UNLIMITED);
       writer.setMaxBufferedDocs(2);
       writer.setRAMBufferSizeMB(IndexWriter.DISABLE_AUTO_FLUSH);
       writer.setMergeScheduler(new SerialMergeScheduler());
@@ -2801,10 +2780,9 @@ public class TestIndexWriter extends BaseTokenStreamTestCase {
   // LUCENE-1168
   public void testTermVectorCorruption2() throws IOException {
     Directory dir = new MockRAMDirectory();
-    for(int iter=0;iter<4;iter++) {
-      final boolean autoCommit = 1==iter/2;
+    for(int iter=0;iter<2;iter++) {
       IndexWriter writer = new IndexWriter(dir,
-                                           autoCommit, new StandardAnalyzer());
+                                           new StandardAnalyzer(), IndexWriter.MaxFieldLength.UNLIMITED);
       writer.setMaxBufferedDocs(2);
       writer.setRAMBufferSizeMB(IndexWriter.DISABLE_AUTO_FLUSH);
       writer.setMergeScheduler(new SerialMergeScheduler());
@@ -3049,7 +3027,7 @@ public class TestIndexWriter extends BaseTokenStreamTestCase {
   // LUCENE-1179
   public void testEmptyFieldName() throws IOException {
     MockRAMDirectory dir = new MockRAMDirectory();
-    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer());
+    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.UNLIMITED);
     Document doc = new Document();
     doc.add(new Field("", "a b c", Field.Store.NO, Field.Index.ANALYZED));
     writer.addDocument(doc);
@@ -4034,7 +4012,7 @@ public class TestIndexWriter extends BaseTokenStreamTestCase {
 
     final List thrown = new ArrayList();
 
-    final IndexWriter writer = new IndexWriter(new MockRAMDirectory(), new StandardAnalyzer()) {
+    final IndexWriter writer = new IndexWriter(new MockRAMDirectory(), new StandardAnalyzer(), IndexWriter.MaxFieldLength.UNLIMITED) {
         public void message(final String message) {
           if (message.startsWith("now flush at close") && 0 == thrown.size()) {
             thrown.add(null);
@@ -4324,7 +4302,7 @@ public class TestIndexWriter extends BaseTokenStreamTestCase {
 
   public void testDeadlock() throws Exception {
     MockRAMDirectory dir = new MockRAMDirectory();
-    IndexWriter writer = new IndexWriter(dir, true, new WhitespaceAnalyzer());
+    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.UNLIMITED);
     writer.setMaxBufferedDocs(2);
     Document doc = new Document();
     doc.add(new Field("content", "aaa bbb ccc ddd eee fff ggg hhh iii", Field.Store.YES,
diff --git a/src/test/org/apache/lucene/index/TestIndexWriterDelete.java b/src/test/org/apache/lucene/index/TestIndexWriterDelete.java
index fecad0f..9fb5795 100644
--- a/src/test/org/apache/lucene/index/TestIndexWriterDelete.java
+++ b/src/test/org/apache/lucene/index/TestIndexWriterDelete.java
@@ -40,285 +40,262 @@ public class TestIndexWriterDelete extends LuceneTestCase {
         "Venice has lots of canals" };
     String[] text = { "Amsterdam", "Venice" };
 
-    for(int pass=0;pass<2;pass++) {
-      boolean autoCommit = (0==pass);
+    Directory dir = new MockRAMDirectory();
+    IndexWriter modifier = new IndexWriter(dir,
+                                           new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
+    modifier.setUseCompoundFile(true);
+    modifier.setMaxBufferedDeleteTerms(1);
 
-      Directory dir = new MockRAMDirectory();
-      IndexWriter modifier = new IndexWriter(dir, autoCommit,
-                                             new WhitespaceAnalyzer(), true);
-      modifier.setUseCompoundFile(true);
-      modifier.setMaxBufferedDeleteTerms(1);
-
-      for (int i = 0; i < keywords.length; i++) {
-        Document doc = new Document();
-        doc.add(new Field("id", keywords[i], Field.Store.YES,
-                          Field.Index.NOT_ANALYZED));
-        doc.add(new Field("country", unindexed[i], Field.Store.YES,
-                          Field.Index.NO));
-        doc.add(new Field("contents", unstored[i], Field.Store.NO,
-                          Field.Index.ANALYZED));
-        doc
-          .add(new Field("city", text[i], Field.Store.YES,
-                         Field.Index.ANALYZED));
-        modifier.addDocument(doc);
-      }
-      modifier.optimize();
-      modifier.commit();
+    for (int i = 0; i < keywords.length; i++) {
+      Document doc = new Document();
+      doc.add(new Field("id", keywords[i], Field.Store.YES,
+                        Field.Index.NOT_ANALYZED));
+      doc.add(new Field("country", unindexed[i], Field.Store.YES,
+                        Field.Index.NO));
+      doc.add(new Field("contents", unstored[i], Field.Store.NO,
+                        Field.Index.ANALYZED));
+      doc
+        .add(new Field("city", text[i], Field.Store.YES,
+                       Field.Index.ANALYZED));
+      modifier.addDocument(doc);
+    }
+    modifier.optimize();
+    modifier.commit();
 
-      Term term = new Term("city", "Amsterdam");
-      int hitCount = getHitCount(dir, term);
-      assertEquals(1, hitCount);
-      modifier.deleteDocuments(term);
-      modifier.commit();
-      hitCount = getHitCount(dir, term);
-      assertEquals(0, hitCount);
+    Term term = new Term("city", "Amsterdam");
+    int hitCount = getHitCount(dir, term);
+    assertEquals(1, hitCount);
+    modifier.deleteDocuments(term);
+    modifier.commit();
+    hitCount = getHitCount(dir, term);
+    assertEquals(0, hitCount);
 
-      modifier.close();
-      dir.close();
-    }
+    modifier.close();
+    dir.close();
   }
 
   // test when delete terms only apply to disk segments
   public void testNonRAMDelete() throws IOException {
-    for(int pass=0;pass<2;pass++) {
-      boolean autoCommit = (0==pass);
 
-      Directory dir = new MockRAMDirectory();
-      IndexWriter modifier = new IndexWriter(dir, autoCommit,
-                                             new WhitespaceAnalyzer(), true);
-      modifier.setMaxBufferedDocs(2);
-      modifier.setMaxBufferedDeleteTerms(2);
+    Directory dir = new MockRAMDirectory();
+    IndexWriter modifier = new IndexWriter(dir,
+                                           new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
+    modifier.setMaxBufferedDocs(2);
+    modifier.setMaxBufferedDeleteTerms(2);
 
-      int id = 0;
-      int value = 100;
+    int id = 0;
+    int value = 100;
 
-      for (int i = 0; i < 7; i++) {
-        addDoc(modifier, ++id, value);
-      }
-      modifier.commit();
+    for (int i = 0; i < 7; i++) {
+      addDoc(modifier, ++id, value);
+    }
+    modifier.commit();
 
-      assertEquals(0, modifier.getNumBufferedDocuments());
-      assertTrue(0 < modifier.getSegmentCount());
+    assertEquals(0, modifier.getNumBufferedDocuments());
+    assertTrue(0 < modifier.getSegmentCount());
 
-      modifier.commit();
+    modifier.commit();
 
-      IndexReader reader = IndexReader.open(dir, true);
-      assertEquals(7, reader.numDocs());
-      reader.close();
+    IndexReader reader = IndexReader.open(dir, true);
+    assertEquals(7, reader.numDocs());
+    reader.close();
 
-      modifier.deleteDocuments(new Term("value", String.valueOf(value)));
+    modifier.deleteDocuments(new Term("value", String.valueOf(value)));
 
-      modifier.commit();
+    modifier.commit();
 
-      reader = IndexReader.open(dir, true);
-      assertEquals(0, reader.numDocs());
-      reader.close();
-      modifier.close();
-      dir.close();
-    }
+    reader = IndexReader.open(dir, true);
+    assertEquals(0, reader.numDocs());
+    reader.close();
+    modifier.close();
+    dir.close();
   }
 
   public void testMaxBufferedDeletes() throws IOException {
-    for(int pass=0;pass<2;pass++) {
-      boolean autoCommit = (0==pass);
-      Directory dir = new MockRAMDirectory();
-      IndexWriter writer = new IndexWriter(dir, autoCommit,
-                                           new WhitespaceAnalyzer(), true);
-      writer.setMaxBufferedDeleteTerms(1);
-      writer.deleteDocuments(new Term("foobar", "1"));
-      writer.deleteDocuments(new Term("foobar", "1"));
-      writer.deleteDocuments(new Term("foobar", "1"));
-      assertEquals(3, writer.getFlushDeletesCount());
-      writer.close();
-      dir.close();
-    }
+    Directory dir = new MockRAMDirectory();
+    IndexWriter writer = new IndexWriter(dir,
+                                         new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
+    writer.setMaxBufferedDeleteTerms(1);
+    writer.deleteDocuments(new Term("foobar", "1"));
+    writer.deleteDocuments(new Term("foobar", "1"));
+    writer.deleteDocuments(new Term("foobar", "1"));
+    assertEquals(3, writer.getFlushDeletesCount());
+    writer.close();
+    dir.close();
   }
 
   // test when delete terms only apply to ram segments
   public void testRAMDeletes() throws IOException {
-    for(int pass=0;pass<2;pass++) {
-      for(int t=0;t<2;t++) {
-        boolean autoCommit = (0==pass);
-        Directory dir = new MockRAMDirectory();
-        IndexWriter modifier = new IndexWriter(dir, autoCommit,
-                                               new WhitespaceAnalyzer(), true);
-        modifier.setMaxBufferedDocs(4);
-        modifier.setMaxBufferedDeleteTerms(4);
-
-        int id = 0;
-        int value = 100;
-
-        addDoc(modifier, ++id, value);
-        if (0 == t)
-          modifier.deleteDocuments(new Term("value", String.valueOf(value)));
-        else
-          modifier.deleteDocuments(new TermQuery(new Term("value", String.valueOf(value))));
-        addDoc(modifier, ++id, value);
-        if (0 == t) {
-          modifier.deleteDocuments(new Term("value", String.valueOf(value)));
-          assertEquals(2, modifier.getNumBufferedDeleteTerms());
-          assertEquals(1, modifier.getBufferedDeleteTermsSize());
-        }
-        else
-          modifier.deleteDocuments(new TermQuery(new Term("value", String.valueOf(value))));
+    for(int t=0;t<2;t++) {
+      Directory dir = new MockRAMDirectory();
+      IndexWriter modifier = new IndexWriter(dir,
+                                             new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
+      modifier.setMaxBufferedDocs(4);
+      modifier.setMaxBufferedDeleteTerms(4);
 
-        addDoc(modifier, ++id, value);
-        assertEquals(0, modifier.getSegmentCount());
-        modifier.flush();
+      int id = 0;
+      int value = 100;
 
-        modifier.commit();
+      addDoc(modifier, ++id, value);
+      if (0 == t)
+        modifier.deleteDocuments(new Term("value", String.valueOf(value)));
+      else
+        modifier.deleteDocuments(new TermQuery(new Term("value", String.valueOf(value))));
+      addDoc(modifier, ++id, value);
+      if (0 == t) {
+        modifier.deleteDocuments(new Term("value", String.valueOf(value)));
+        assertEquals(2, modifier.getNumBufferedDeleteTerms());
+        assertEquals(1, modifier.getBufferedDeleteTermsSize());
+      }
+      else
+        modifier.deleteDocuments(new TermQuery(new Term("value", String.valueOf(value))));
 
-        IndexReader reader = IndexReader.open(dir, true);
-        assertEquals(1, reader.numDocs());
+      addDoc(modifier, ++id, value);
+      assertEquals(0, modifier.getSegmentCount());
+      modifier.flush();
 
-        int hitCount = getHitCount(dir, new Term("id", String.valueOf(id)));
-        assertEquals(1, hitCount);
-        reader.close();
-        modifier.close();
-        dir.close();
-      }
+      modifier.commit();
+
+      IndexReader reader = IndexReader.open(dir, true);
+      assertEquals(1, reader.numDocs());
+
+      int hitCount = getHitCount(dir, new Term("id", String.valueOf(id)));
+      assertEquals(1, hitCount);
+      reader.close();
+      modifier.close();
+      dir.close();
     }
   }
 
   // test when delete terms apply to both disk and ram segments
   public void testBothDeletes() throws IOException {
-    for(int pass=0;pass<2;pass++) {
-      boolean autoCommit = (0==pass);
-
-      Directory dir = new MockRAMDirectory();
-      IndexWriter modifier = new IndexWriter(dir, autoCommit,
-                                             new WhitespaceAnalyzer(), true);
-      modifier.setMaxBufferedDocs(100);
-      modifier.setMaxBufferedDeleteTerms(100);
+    Directory dir = new MockRAMDirectory();
+    IndexWriter modifier = new IndexWriter(dir,
+                                           new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
+    modifier.setMaxBufferedDocs(100);
+    modifier.setMaxBufferedDeleteTerms(100);
 
-      int id = 0;
-      int value = 100;
+    int id = 0;
+    int value = 100;
 
-      for (int i = 0; i < 5; i++) {
-        addDoc(modifier, ++id, value);
-      }
+    for (int i = 0; i < 5; i++) {
+      addDoc(modifier, ++id, value);
+    }
 
-      value = 200;
-      for (int i = 0; i < 5; i++) {
-        addDoc(modifier, ++id, value);
-      }
-      modifier.commit();
+    value = 200;
+    for (int i = 0; i < 5; i++) {
+      addDoc(modifier, ++id, value);
+    }
+    modifier.commit();
 
-      for (int i = 0; i < 5; i++) {
-        addDoc(modifier, ++id, value);
-      }
-      modifier.deleteDocuments(new Term("value", String.valueOf(value)));
+    for (int i = 0; i < 5; i++) {
+      addDoc(modifier, ++id, value);
+    }
+    modifier.deleteDocuments(new Term("value", String.valueOf(value)));
 
-      modifier.commit();
+    modifier.commit();
 
-      IndexReader reader = IndexReader.open(dir, true);
-      assertEquals(5, reader.numDocs());
-      modifier.close();
-    }
+    IndexReader reader = IndexReader.open(dir, true);
+    assertEquals(5, reader.numDocs());
+    modifier.close();
   }
 
   // test that batched delete terms are flushed together
   public void testBatchDeletes() throws IOException {
-    for(int pass=0;pass<2;pass++) {
-      boolean autoCommit = (0==pass);
-      Directory dir = new MockRAMDirectory();
-      IndexWriter modifier = new IndexWriter(dir, autoCommit,
-                                             new WhitespaceAnalyzer(), true);
-      modifier.setMaxBufferedDocs(2);
-      modifier.setMaxBufferedDeleteTerms(2);
+    Directory dir = new MockRAMDirectory();
+    IndexWriter modifier = new IndexWriter(dir,
+                                           new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
+    modifier.setMaxBufferedDocs(2);
+    modifier.setMaxBufferedDeleteTerms(2);
 
-      int id = 0;
-      int value = 100;
+    int id = 0;
+    int value = 100;
 
-      for (int i = 0; i < 7; i++) {
-        addDoc(modifier, ++id, value);
-      }
-      modifier.commit();
+    for (int i = 0; i < 7; i++) {
+      addDoc(modifier, ++id, value);
+    }
+    modifier.commit();
 
-      IndexReader reader = IndexReader.open(dir, true);
-      assertEquals(7, reader.numDocs());
-      reader.close();
+    IndexReader reader = IndexReader.open(dir, true);
+    assertEquals(7, reader.numDocs());
+    reader.close();
       
-      id = 0;
-      modifier.deleteDocuments(new Term("id", String.valueOf(++id)));
-      modifier.deleteDocuments(new Term("id", String.valueOf(++id)));
-
-      modifier.commit();
+    id = 0;
+    modifier.deleteDocuments(new Term("id", String.valueOf(++id)));
+    modifier.deleteDocuments(new Term("id", String.valueOf(++id)));
 
-      reader = IndexReader.open(dir, true);
-      assertEquals(5, reader.numDocs());
-      reader.close();
+    modifier.commit();
 
-      Term[] terms = new Term[3];
-      for (int i = 0; i < terms.length; i++) {
-        terms[i] = new Term("id", String.valueOf(++id));
-      }
-      modifier.deleteDocuments(terms);
-      modifier.commit();
-      reader = IndexReader.open(dir, true);
-      assertEquals(2, reader.numDocs());
-      reader.close();
+    reader = IndexReader.open(dir, true);
+    assertEquals(5, reader.numDocs());
+    reader.close();
 
-      modifier.close();
-      dir.close();
+    Term[] terms = new Term[3];
+    for (int i = 0; i < terms.length; i++) {
+      terms[i] = new Term("id", String.valueOf(++id));
     }
+    modifier.deleteDocuments(terms);
+    modifier.commit();
+    reader = IndexReader.open(dir, true);
+    assertEquals(2, reader.numDocs());
+    reader.close();
+
+    modifier.close();
+    dir.close();
   }
 
   // test deleteAll()
   public void testDeleteAll() throws IOException {
-    for (int pass=0;pass<2;pass++) {
-      boolean autoCommit = (0==pass);
-      Directory dir = new MockRAMDirectory();
-      IndexWriter modifier = new IndexWriter(dir, autoCommit,
-                                             new WhitespaceAnalyzer(), true);
-      modifier.setMaxBufferedDocs(2);
-      modifier.setMaxBufferedDeleteTerms(2);
+    Directory dir = new MockRAMDirectory();
+    IndexWriter modifier = new IndexWriter(dir,
+                                           new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
+    modifier.setMaxBufferedDocs(2);
+    modifier.setMaxBufferedDeleteTerms(2);
 
-      int id = 0;
-      int value = 100;
+    int id = 0;
+    int value = 100;
 
-      for (int i = 0; i < 7; i++) {
-        addDoc(modifier, ++id, value);
-      }
-      modifier.commit();
+    for (int i = 0; i < 7; i++) {
+      addDoc(modifier, ++id, value);
+    }
+    modifier.commit();
 
-      IndexReader reader = IndexReader.open(dir, true);
-      assertEquals(7, reader.numDocs());
-      reader.close();
+    IndexReader reader = IndexReader.open(dir, true);
+    assertEquals(7, reader.numDocs());
+    reader.close();
 
-      // Add 1 doc (so we will have something buffered)
-      addDoc(modifier, 99, value);
+    // Add 1 doc (so we will have something buffered)
+    addDoc(modifier, 99, value);
 
-      // Delete all
-      modifier.deleteAll();
+    // Delete all
+    modifier.deleteAll();
 
-      // Delete all shouldn't be on disk yet
-      reader = IndexReader.open(dir, true);
-      assertEquals(7, reader.numDocs());
-      reader.close();
+    // Delete all shouldn't be on disk yet
+    reader = IndexReader.open(dir, true);
+    assertEquals(7, reader.numDocs());
+    reader.close();
 
-      // Add a doc and update a doc (after the deleteAll, before the commit)
-      addDoc(modifier, 101, value);
-      updateDoc(modifier, 102, value);
+    // Add a doc and update a doc (after the deleteAll, before the commit)
+    addDoc(modifier, 101, value);
+    updateDoc(modifier, 102, value);
 
-      // commit the delete all
-      modifier.commit();
+    // commit the delete all
+    modifier.commit();
 
-      // Validate there are no docs left
-      reader = IndexReader.open(dir, true);
-      assertEquals(2, reader.numDocs());
-      reader.close();
+    // Validate there are no docs left
+    reader = IndexReader.open(dir, true);
+    assertEquals(2, reader.numDocs());
+    reader.close();
 
-      modifier.close();
-      dir.close();
-    }
+    modifier.close();
+    dir.close();
   }
 
   // test rollback of deleteAll()
   public void testDeleteAllRollback() throws IOException {
     Directory dir = new MockRAMDirectory();
-    IndexWriter modifier = new IndexWriter(dir, false,
-                                           new WhitespaceAnalyzer(), true);
+    IndexWriter modifier = new IndexWriter(dir,
+                                           new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
     modifier.setMaxBufferedDocs(2);
     modifier.setMaxBufferedDeleteTerms(2);
     
@@ -355,8 +332,8 @@ public class TestIndexWriterDelete extends LuceneTestCase {
   // test deleteAll() w/ near real-time reader
   public void testDeleteAllNRT() throws IOException {
     Directory dir = new MockRAMDirectory();
-    IndexWriter modifier = new IndexWriter(dir, false,
-                                           new WhitespaceAnalyzer(), true);
+    IndexWriter modifier = new IndexWriter(dir,
+                                           new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
     modifier.setMaxBufferedDocs(2);
     modifier.setMaxBufferedDeleteTerms(2);
     
@@ -445,187 +422,183 @@ public class TestIndexWriterDelete extends LuceneTestCase {
     int START_COUNT = 157;
     int END_COUNT = 144;
 
-    for(int pass=0;pass<2;pass++) {
-      boolean autoCommit = (0==pass);
-
-      // First build up a starting index:
-      MockRAMDirectory startDir = new MockRAMDirectory();
-      IndexWriter writer = new IndexWriter(startDir, autoCommit,
-                                           new WhitespaceAnalyzer(), true);
-      for (int i = 0; i < 157; i++) {
-        Document d = new Document();
-        d.add(new Field("id", Integer.toString(i), Field.Store.YES,
-                        Field.Index.NOT_ANALYZED));
-        d.add(new Field("content", "aaa " + i, Field.Store.NO,
-                        Field.Index.ANALYZED));
-        writer.addDocument(d);
-      }
-      writer.close();
+    // First build up a starting index:
+    MockRAMDirectory startDir = new MockRAMDirectory();
+    IndexWriter writer = new IndexWriter(startDir,
+                                         new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
+    for (int i = 0; i < 157; i++) {
+      Document d = new Document();
+      d.add(new Field("id", Integer.toString(i), Field.Store.YES,
+                      Field.Index.NOT_ANALYZED));
+      d.add(new Field("content", "aaa " + i, Field.Store.NO,
+                      Field.Index.ANALYZED));
+      writer.addDocument(d);
+    }
+    writer.close();
 
-      long diskUsage = startDir.sizeInBytes();
-      long diskFree = diskUsage + 10;
+    long diskUsage = startDir.sizeInBytes();
+    long diskFree = diskUsage + 10;
 
-      IOException err = null;
+    IOException err = null;
 
-      boolean done = false;
+    boolean done = false;
 
-      // Iterate w/ ever increasing free disk space:
-      while (!done) {
-        MockRAMDirectory dir = new MockRAMDirectory(startDir);
-        dir.setPreventDoubleWrite(false);
-        IndexWriter modifier = new IndexWriter(dir, autoCommit,
-                                               new WhitespaceAnalyzer());
+    // Iterate w/ ever increasing free disk space:
+    while (!done) {
+      MockRAMDirectory dir = new MockRAMDirectory(startDir);
+      dir.setPreventDoubleWrite(false);
+      IndexWriter modifier = new IndexWriter(dir,
+                                             new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.UNLIMITED);
 
-        modifier.setMaxBufferedDocs(1000); // use flush or close
-        modifier.setMaxBufferedDeleteTerms(1000); // use flush or close
+      modifier.setMaxBufferedDocs(1000); // use flush or close
+      modifier.setMaxBufferedDeleteTerms(1000); // use flush or close
 
-        // For each disk size, first try to commit against
-        // dir that will hit random IOExceptions & disk
-        // full; after, give it infinite disk space & turn
-        // off random IOExceptions & retry w/ same reader:
-        boolean success = false;
+      // For each disk size, first try to commit against
+      // dir that will hit random IOExceptions & disk
+      // full; after, give it infinite disk space & turn
+      // off random IOExceptions & retry w/ same reader:
+      boolean success = false;
 
-        for (int x = 0; x < 2; x++) {
+      for (int x = 0; x < 2; x++) {
 
-          double rate = 0.1;
-          double diskRatio = ((double)diskFree) / diskUsage;
-          long thisDiskFree;
-          String testName;
+        double rate = 0.1;
+        double diskRatio = ((double)diskFree) / diskUsage;
+        long thisDiskFree;
+        String testName;
 
-          if (0 == x) {
-            thisDiskFree = diskFree;
-            if (diskRatio >= 2.0) {
-              rate /= 2;
-            }
-            if (diskRatio >= 4.0) {
-              rate /= 2;
-            }
-            if (diskRatio >= 6.0) {
-              rate = 0.0;
-            }
-            if (debug) {
-              System.out.println("\ncycle: " + diskFree + " bytes");
-            }
-            testName = "disk full during reader.close() @ " + thisDiskFree
-              + " bytes";
-          } else {
-            thisDiskFree = 0;
+        if (0 == x) {
+          thisDiskFree = diskFree;
+          if (diskRatio >= 2.0) {
+            rate /= 2;
+          }
+          if (diskRatio >= 4.0) {
+            rate /= 2;
+          }
+          if (diskRatio >= 6.0) {
             rate = 0.0;
-            if (debug) {
-              System.out.println("\ncycle: same writer: unlimited disk space");
-            }
-            testName = "reader re-use after disk full";
           }
+          if (debug) {
+            System.out.println("\ncycle: " + diskFree + " bytes");
+          }
+          testName = "disk full during reader.close() @ " + thisDiskFree
+            + " bytes";
+        } else {
+          thisDiskFree = 0;
+          rate = 0.0;
+          if (debug) {
+            System.out.println("\ncycle: same writer: unlimited disk space");
+          }
+          testName = "reader re-use after disk full";
+        }
+
+        dir.setMaxSizeInBytes(thisDiskFree);
+        dir.setRandomIOExceptionRate(rate, diskFree);
 
-          dir.setMaxSizeInBytes(thisDiskFree);
-          dir.setRandomIOExceptionRate(rate, diskFree);
-
-          try {
-            if (0 == x) {
-              int docId = 12;
-              for (int i = 0; i < 13; i++) {
-                if (updates) {
-                  Document d = new Document();
-                  d.add(new Field("id", Integer.toString(i), Field.Store.YES,
-                                  Field.Index.NOT_ANALYZED));
-                  d.add(new Field("content", "bbb " + i, Field.Store.NO,
-                                  Field.Index.ANALYZED));
-                  modifier.updateDocument(new Term("id", Integer.toString(docId)), d);
-                } else { // deletes
-                  modifier.deleteDocuments(new Term("id", Integer.toString(docId)));
-                  // modifier.setNorm(docId, "contents", (float)2.0);
-                }
-                docId += 12;
+        try {
+          if (0 == x) {
+            int docId = 12;
+            for (int i = 0; i < 13; i++) {
+              if (updates) {
+                Document d = new Document();
+                d.add(new Field("id", Integer.toString(i), Field.Store.YES,
+                                Field.Index.NOT_ANALYZED));
+                d.add(new Field("content", "bbb " + i, Field.Store.NO,
+                                Field.Index.ANALYZED));
+                modifier.updateDocument(new Term("id", Integer.toString(docId)), d);
+              } else { // deletes
+                modifier.deleteDocuments(new Term("id", Integer.toString(docId)));
+                // modifier.setNorm(docId, "contents", (float)2.0);
               }
-            }
-            modifier.close();
-            success = true;
-            if (0 == x) {
-              done = true;
+              docId += 12;
             }
           }
-          catch (IOException e) {
-            if (debug) {
-              System.out.println("  hit IOException: " + e);
-              e.printStackTrace(System.out);
-            }
-            err = e;
-            if (1 == x) {
-              e.printStackTrace();
-              fail(testName + " hit IOException after disk space was freed up");
-            }
+          modifier.close();
+          success = true;
+          if (0 == x) {
+            done = true;
           }
-
-          // If the close() succeeded, make sure there are
-          // no unreferenced files.
-          if (success)
-            TestIndexWriter.assertNoUnreferencedFiles(dir, "after writer.close");
-
-          // Finally, verify index is not corrupt, and, if
-          // we succeeded, we see all docs changed, and if
-          // we failed, we see either all docs or no docs
-          // changed (transactional semantics):
-          IndexReader newReader = null;
-          try {
-            newReader = IndexReader.open(dir, true);
+        }
+        catch (IOException e) {
+          if (debug) {
+            System.out.println("  hit IOException: " + e);
+            e.printStackTrace(System.out);
           }
-          catch (IOException e) {
+          err = e;
+          if (1 == x) {
             e.printStackTrace();
-            fail(testName
-                 + ":exception when creating IndexReader after disk full during close: "
-                 + e);
+            fail(testName + " hit IOException after disk space was freed up");
           }
+        }
 
-          IndexSearcher searcher = new IndexSearcher(newReader);
-          ScoreDoc[] hits = null;
-          try {
-            hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;
-          }
-          catch (IOException e) {
-            e.printStackTrace();
-            fail(testName + ": exception when searching: " + e);
-          }
-          int result2 = hits.length;
-          if (success) {
-            if (x == 0 && result2 != END_COUNT) {
-              fail(testName
-                   + ": method did not throw exception but hits.length for search on term 'aaa' is "
-                   + result2 + " instead of expected " + END_COUNT);
-            } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {
-              // It's possible that the first exception was
-              // "recoverable" wrt pending deletes, in which
-              // case the pending deletes are retained and
-              // then re-flushing (with plenty of disk
-              // space) will succeed in flushing the
-              // deletes:
-              fail(testName
-                   + ": method did not throw exception but hits.length for search on term 'aaa' is "
-                   + result2 + " instead of expected " + START_COUNT + " or " + END_COUNT);
-            }
-          } else {
-            // On hitting exception we still may have added
-            // all docs:
-            if (result2 != START_COUNT && result2 != END_COUNT) {
-              err.printStackTrace();
-              fail(testName
-                   + ": method did throw exception but hits.length for search on term 'aaa' is "
-                   + result2 + " instead of expected " + START_COUNT + " or " + END_COUNT);
-            }
-          }
+        // If the close() succeeded, make sure there are
+        // no unreferenced files.
+        if (success)
+          TestIndexWriter.assertNoUnreferencedFiles(dir, "after writer.close");
 
-          searcher.close();
-          newReader.close();
+        // Finally, verify index is not corrupt, and, if
+        // we succeeded, we see all docs changed, and if
+        // we failed, we see either all docs or no docs
+        // changed (transactional semantics):
+        IndexReader newReader = null;
+        try {
+          newReader = IndexReader.open(dir, true);
+        }
+        catch (IOException e) {
+          e.printStackTrace();
+          fail(testName
+               + ":exception when creating IndexReader after disk full during close: "
+               + e);
+        }
 
-          if (result2 == END_COUNT) {
-            break;
+        IndexSearcher searcher = new IndexSearcher(newReader);
+        ScoreDoc[] hits = null;
+        try {
+          hits = searcher.search(new TermQuery(searchTerm), null, 1000).scoreDocs;
+        }
+        catch (IOException e) {
+          e.printStackTrace();
+          fail(testName + ": exception when searching: " + e);
+        }
+        int result2 = hits.length;
+        if (success) {
+          if (x == 0 && result2 != END_COUNT) {
+            fail(testName
+                 + ": method did not throw exception but hits.length for search on term 'aaa' is "
+                 + result2 + " instead of expected " + END_COUNT);
+          } else if (x == 1 && result2 != START_COUNT && result2 != END_COUNT) {
+            // It's possible that the first exception was
+            // "recoverable" wrt pending deletes, in which
+            // case the pending deletes are retained and
+            // then re-flushing (with plenty of disk
+            // space) will succeed in flushing the
+            // deletes:
+            fail(testName
+                 + ": method did not throw exception but hits.length for search on term 'aaa' is "
+                 + result2 + " instead of expected " + START_COUNT + " or " + END_COUNT);
+          }
+        } else {
+          // On hitting exception we still may have added
+          // all docs:
+          if (result2 != START_COUNT && result2 != END_COUNT) {
+            err.printStackTrace();
+            fail(testName
+                 + ": method did throw exception but hits.length for search on term 'aaa' is "
+                 + result2 + " instead of expected " + START_COUNT + " or " + END_COUNT);
           }
         }
 
-        dir.close();
+        searcher.close();
+        newReader.close();
 
-        // Try again with 10 more bytes of free space:
-        diskFree += 10;
+        if (result2 == END_COUNT) {
+          break;
+        }
       }
+
+      dir.close();
+
+      // Try again with 10 more bytes of free space:
+      diskFree += 10;
     }
   }
 
@@ -677,87 +650,84 @@ public class TestIndexWriterDelete extends LuceneTestCase {
         "Venice has lots of canals" };
     String[] text = { "Amsterdam", "Venice" };
 
-    for(int pass=0;pass<2;pass++) {
-      boolean autoCommit = (0==pass);
-      MockRAMDirectory dir = new MockRAMDirectory();
-      IndexWriter modifier = new IndexWriter(dir, autoCommit,
-                                             new WhitespaceAnalyzer(), true);
-      modifier.setUseCompoundFile(true);
-      modifier.setMaxBufferedDeleteTerms(2);
-
-      dir.failOn(failure.reset());
-
-      for (int i = 0; i < keywords.length; i++) {
-        Document doc = new Document();
-        doc.add(new Field("id", keywords[i], Field.Store.YES,
-                          Field.Index.NOT_ANALYZED));
-        doc.add(new Field("country", unindexed[i], Field.Store.YES,
-                          Field.Index.NO));
-        doc.add(new Field("contents", unstored[i], Field.Store.NO,
-                          Field.Index.ANALYZED));
-        doc.add(new Field("city", text[i], Field.Store.YES,
-                          Field.Index.ANALYZED));
-        modifier.addDocument(doc);
-      }
-      // flush (and commit if ac)
-
-      modifier.optimize();
-      modifier.commit();
+    MockRAMDirectory dir = new MockRAMDirectory();
+    IndexWriter modifier = new IndexWriter(dir,
+                                           new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
+    modifier.setUseCompoundFile(true);
+    modifier.setMaxBufferedDeleteTerms(2);
 
-      // one of the two files hits
+    dir.failOn(failure.reset());
 
-      Term term = new Term("city", "Amsterdam");
-      int hitCount = getHitCount(dir, term);
-      assertEquals(1, hitCount);
+    for (int i = 0; i < keywords.length; i++) {
+      Document doc = new Document();
+      doc.add(new Field("id", keywords[i], Field.Store.YES,
+                        Field.Index.NOT_ANALYZED));
+      doc.add(new Field("country", unindexed[i], Field.Store.YES,
+                        Field.Index.NO));
+      doc.add(new Field("contents", unstored[i], Field.Store.NO,
+                        Field.Index.ANALYZED));
+      doc.add(new Field("city", text[i], Field.Store.YES,
+                        Field.Index.ANALYZED));
+      modifier.addDocument(doc);
+    }
+    // flush (and commit if ac)
 
-      // open the writer again (closed above)
+    modifier.optimize();
+    modifier.commit();
 
-      // delete the doc
-      // max buf del terms is two, so this is buffered
+    // one of the two files hits
 
-      modifier.deleteDocuments(term);
+    Term term = new Term("city", "Amsterdam");
+    int hitCount = getHitCount(dir, term);
+    assertEquals(1, hitCount);
 
-      // add a doc (needed for the !ac case; see below)
-      // doc remains buffered
+    // open the writer again (closed above)
 
-      Document doc = new Document();
-      modifier.addDocument(doc);
+    // delete the doc
+    // max buf del terms is two, so this is buffered
 
-      // commit the changes, the buffered deletes, and the new doc
+    modifier.deleteDocuments(term);
 
-      // The failure object will fail on the first write after the del
-      // file gets created when processing the buffered delete
+    // add a doc (needed for the !ac case; see below)
+    // doc remains buffered
 
-      // in the ac case, this will be when writing the new segments
-      // files so we really don't need the new doc, but it's harmless
+    Document doc = new Document();
+    modifier.addDocument(doc);
 
-      // in the !ac case, a new segments file won't be created but in
-      // this case, creation of the cfs file happens next so we need
-      // the doc (to test that it's okay that we don't lose deletes if
-      // failing while creating the cfs file)
+    // commit the changes, the buffered deletes, and the new doc
 
-      boolean failed = false;
-      try {
-        modifier.commit();
-      } catch (IOException ioe) {
-        failed = true;
-      }
+    // The failure object will fail on the first write after the del
+    // file gets created when processing the buffered delete
 
-      assertTrue(failed);
+    // in the ac case, this will be when writing the new segments
+    // files so we really don't need the new doc, but it's harmless
 
-      // The commit above failed, so we need to retry it (which will
-      // succeed, because the failure is a one-shot)
+    // in the !ac case, a new segments file won't be created but in
+    // this case, creation of the cfs file happens next so we need
+    // the doc (to test that it's okay that we don't lose deletes if
+    // failing while creating the cfs file)
 
+    boolean failed = false;
+    try {
       modifier.commit();
+    } catch (IOException ioe) {
+      failed = true;
+    }
 
-      hitCount = getHitCount(dir, term);
+    assertTrue(failed);
 
-      // Make sure the delete was successfully flushed:
-      assertEquals(0, hitCount);
+    // The commit above failed, so we need to retry it (which will
+    // succeed, because the failure is a one-shot)
 
-      modifier.close();
-      dir.close();
-    }
+    modifier.commit();
+
+    hitCount = getHitCount(dir, term);
+
+    // Make sure the delete was successfully flushed:
+    assertEquals(0, hitCount);
+
+    modifier.close();
+    dir.close();
   }
 
   // This test tests that the files created by the docs writer before
@@ -787,47 +757,43 @@ public class TestIndexWriterDelete extends LuceneTestCase {
         "Venice has lots of canals" };
     String[] text = { "Amsterdam", "Venice" };
 
-    for(int pass=0;pass<2;pass++) {
-      boolean autoCommit = (0==pass);
-      MockRAMDirectory dir = new MockRAMDirectory();
-      IndexWriter modifier = new IndexWriter(dir, autoCommit,
-                                             new WhitespaceAnalyzer(), true);
-
-      dir.failOn(failure.reset());
-
-      for (int i = 0; i < keywords.length; i++) {
-        Document doc = new Document();
-        doc.add(new Field("id", keywords[i], Field.Store.YES,
-                          Field.Index.NOT_ANALYZED));
-        doc.add(new Field("country", unindexed[i], Field.Store.YES,
-                          Field.Index.NO));
-        doc.add(new Field("contents", unstored[i], Field.Store.NO,
-                          Field.Index.ANALYZED));
-        doc.add(new Field("city", text[i], Field.Store.YES,
-                          Field.Index.ANALYZED));
-        try {
-          modifier.addDocument(doc);
-        } catch (IOException io) {
-          break;
-        }
-      }
+    MockRAMDirectory dir = new MockRAMDirectory();
+    IndexWriter modifier = new IndexWriter(dir,
+                                           new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
 
-      String[] startFiles = dir.listAll();
-      SegmentInfos infos = new SegmentInfos();
-      infos.read(dir);
-      new IndexFileDeleter(dir, new KeepOnlyLastCommitDeletionPolicy(), infos, null, null);
-      String[] endFiles = dir.listAll();
+    dir.failOn(failure.reset());
 
-      if (!Arrays.equals(startFiles, endFiles)) {
-        fail("docswriter abort() failed to delete unreferenced files:\n  before delete:\n    "
-             + arrayToString(startFiles) + "\n  after delete:\n    "
-             + arrayToString(endFiles));
+    for (int i = 0; i < keywords.length; i++) {
+      Document doc = new Document();
+      doc.add(new Field("id", keywords[i], Field.Store.YES,
+                        Field.Index.NOT_ANALYZED));
+      doc.add(new Field("country", unindexed[i], Field.Store.YES,
+                        Field.Index.NO));
+      doc.add(new Field("contents", unstored[i], Field.Store.NO,
+                        Field.Index.ANALYZED));
+      doc.add(new Field("city", text[i], Field.Store.YES,
+                        Field.Index.ANALYZED));
+      try {
+        modifier.addDocument(doc);
+      } catch (IOException io) {
+        break;
       }
+    }
 
-      modifier.close();
+    String[] startFiles = dir.listAll();
+    SegmentInfos infos = new SegmentInfos();
+    infos.read(dir);
+    new IndexFileDeleter(dir, new KeepOnlyLastCommitDeletionPolicy(), infos, null, null);
+    String[] endFiles = dir.listAll();
 
+    if (!Arrays.equals(startFiles, endFiles)) {
+      fail("docswriter abort() failed to delete unreferenced files:\n  before delete:\n    "
+           + arrayToString(startFiles) + "\n  after delete:\n    "
+           + arrayToString(endFiles));
     }
 
+    modifier.close();
+
   }
 
   private String arrayToString(String[] l) {
diff --git a/src/test/org/apache/lucene/index/TestIndexWriterMergePolicy.java b/src/test/org/apache/lucene/index/TestIndexWriterMergePolicy.java
index 0c7ae70..13905d8 100755
--- a/src/test/org/apache/lucene/index/TestIndexWriterMergePolicy.java
+++ b/src/test/org/apache/lucene/index/TestIndexWriterMergePolicy.java
@@ -125,7 +125,7 @@ public class TestIndexWriterMergePolicy extends LuceneTestCase {
   public void testMaxBufferedDocsChange() throws IOException {
     Directory dir = new RAMDirectory();
 
-    IndexWriter writer = new IndexWriter(dir, true, new WhitespaceAnalyzer(), true);
+    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
     writer.setMaxBufferedDocs(101);
     writer.setMergeFactor(101);
     writer.setMergePolicy(new LogDocMergePolicy(writer));
@@ -139,7 +139,7 @@ public class TestIndexWriterMergePolicy extends LuceneTestCase {
       }
       writer.close();
 
-      writer = new IndexWriter(dir, true, new WhitespaceAnalyzer(), false);
+      writer = new IndexWriter(dir, new WhitespaceAnalyzer(), false, IndexWriter.MaxFieldLength.UNLIMITED);
       writer.setMaxBufferedDocs(101);
       writer.setMergeFactor(101);
       writer.setMergePolicy(new LogDocMergePolicy(writer));
@@ -158,6 +158,9 @@ public class TestIndexWriterMergePolicy extends LuceneTestCase {
     for (int i = 100; i < 1000; i++) {
       addDoc(writer);
     }
+    writer.commit();
+    ((ConcurrentMergeScheduler) writer.getMergeScheduler()).sync();
+    writer.commit();
     checkInvariants(writer);
 
     writer.close();
@@ -167,7 +170,7 @@ public class TestIndexWriterMergePolicy extends LuceneTestCase {
   public void testMergeDocCount0() throws IOException {
     Directory dir = new RAMDirectory();
 
-    IndexWriter writer = new IndexWriter(dir, true, new WhitespaceAnalyzer(), true);
+    IndexWriter writer = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
     writer.setMergePolicy(new LogDocMergePolicy(writer));
     writer.setMaxBufferedDocs(10);
     writer.setMergeFactor(100);
@@ -182,7 +185,7 @@ public class TestIndexWriterMergePolicy extends LuceneTestCase {
     reader.deleteDocuments(new Term("content", "aaa"));
     reader.close();
 
-    writer = new IndexWriter(dir, true, new WhitespaceAnalyzer(), false);
+    writer = new IndexWriter(dir, new WhitespaceAnalyzer(), false, IndexWriter.MaxFieldLength.UNLIMITED);
     writer.setMergePolicy(new LogDocMergePolicy(writer));
     writer.setMaxBufferedDocs(10);
     writer.setMergeFactor(5);
@@ -191,6 +194,9 @@ public class TestIndexWriterMergePolicy extends LuceneTestCase {
     for (int i = 0; i < 10; i++) {
       addDoc(writer);
     }
+    writer.commit();
+    ((ConcurrentMergeScheduler) writer.getMergeScheduler()).sync();
+    writer.commit();
     checkInvariants(writer);
     assertEquals(10, writer.docCount());
 
diff --git a/src/test/org/apache/lucene/index/TestStressIndexing.java b/src/test/org/apache/lucene/index/TestStressIndexing.java
index f8687d7..de8d762 100644
--- a/src/test/org/apache/lucene/index/TestStressIndexing.java
+++ b/src/test/org/apache/lucene/index/TestStressIndexing.java
@@ -115,8 +115,8 @@ public class TestStressIndexing extends LuceneTestCase {
     Run one indexer and 2 searchers against single index as
     stress test.
   */
-  public void runStressTest(Directory directory, boolean autoCommit, MergeScheduler mergeScheduler) throws Exception {
-    IndexWriter modifier = new IndexWriter(directory, autoCommit, ANALYZER, true);
+  public void runStressTest(Directory directory, MergeScheduler mergeScheduler) throws Exception {
+    IndexWriter modifier = new IndexWriter(directory, ANALYZER, true, IndexWriter.MaxFieldLength.UNLIMITED);
 
     modifier.setMaxBufferedDocs(10);
 
@@ -166,35 +166,15 @@ public class TestStressIndexing extends LuceneTestCase {
   public void testStressIndexAndSearching() throws Exception {
     RANDOM = newRandom();
 
-    // RAMDir
-    Directory directory = new MockRAMDirectory();
-    runStressTest(directory, true, null);
-    directory.close();
-
-    // FSDir
-    File dirPath = _TestUtil.getTempDir("lucene.test.stress");
-    directory = FSDirectory.open(dirPath);
-    runStressTest(directory, true, null);
-    directory.close();
-
     // With ConcurrentMergeScheduler, in RAMDir
-    directory = new MockRAMDirectory();
-    runStressTest(directory, true, new ConcurrentMergeScheduler());
+    Directory directory = new MockRAMDirectory();
+    runStressTest(directory, new ConcurrentMergeScheduler());
     directory.close();
 
     // With ConcurrentMergeScheduler, in FSDir
+    File dirPath = _TestUtil.getTempDir("lucene.test.stress");
     directory = FSDirectory.open(dirPath);
-    runStressTest(directory, true, new ConcurrentMergeScheduler());
-    directory.close();
-
-    // With ConcurrentMergeScheduler and autoCommit=false, in RAMDir
-    directory = new MockRAMDirectory();
-    runStressTest(directory, false, new ConcurrentMergeScheduler());
-    directory.close();
-
-    // With ConcurrentMergeScheduler and autoCommit=false, in FSDir
-    directory = FSDirectory.open(dirPath);
-    runStressTest(directory, false, new ConcurrentMergeScheduler());
+    runStressTest(directory, new ConcurrentMergeScheduler());
     directory.close();
 
     _TestUtil.rmDir(dirPath);
diff --git a/src/test/org/apache/lucene/index/TestStressIndexing2.java b/src/test/org/apache/lucene/index/TestStressIndexing2.java
index eb0b57b..2106017 100644
--- a/src/test/org/apache/lucene/index/TestStressIndexing2.java
+++ b/src/test/org/apache/lucene/index/TestStressIndexing2.java
@@ -32,7 +32,6 @@ public class TestStressIndexing2 extends LuceneTestCase {
   static int maxFields=4;
   static int bigFieldSize=10;
   static boolean sameFieldOrder=false;
-  static boolean autoCommit=false;
   static int mergeFactor=3;
   static int maxBufferedDocs=3;
   static int seed=0;
@@ -41,8 +40,8 @@ public class TestStressIndexing2 extends LuceneTestCase {
 
   public class MockIndexWriter extends IndexWriter {
 
-    public MockIndexWriter(Directory dir, boolean autoCommit, Analyzer a, boolean create) throws IOException {
-      super(dir, autoCommit, a, create);
+    public MockIndexWriter(Directory dir, Analyzer a, boolean create, IndexWriter.MaxFieldLength mfl) throws IOException {
+      super(dir, a, create, mfl);
     }
 
     boolean testPoint(String name) {
@@ -88,7 +87,6 @@ public class TestStressIndexing2 extends LuceneTestCase {
     r = newRandom();
     for (int i=0; i<100; i++) {  // increase iterations for better testing
       sameFieldOrder=r.nextBoolean();
-      autoCommit=r.nextBoolean();
       mergeFactor=r.nextInt(3)+2;
       maxBufferedDocs=r.nextInt(3)+2;
       seed++;
@@ -124,7 +122,7 @@ public class TestStressIndexing2 extends LuceneTestCase {
   
   public DocsAndWriter indexRandomIWReader(int nThreads, int iterations, int range, Directory dir) throws IOException, InterruptedException {
     Map docs = new HashMap();
-    IndexWriter w = new MockIndexWriter(dir, autoCommit, new WhitespaceAnalyzer(), true);
+    IndexWriter w = new MockIndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
     w.setUseCompoundFile(false);
 
     /***
@@ -176,7 +174,7 @@ public class TestStressIndexing2 extends LuceneTestCase {
   public Map indexRandom(int nThreads, int iterations, int range, Directory dir) throws IOException, InterruptedException {
     Map docs = new HashMap();
     for(int iter=0;iter<3;iter++) {
-      IndexWriter w = new MockIndexWriter(dir, autoCommit, new WhitespaceAnalyzer(), true);
+      IndexWriter w = new MockIndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.UNLIMITED);
       w.setUseCompoundFile(false);
 
       // force many merges
diff --git a/src/test/org/apache/lucene/index/TestThreadedOptimize.java b/src/test/org/apache/lucene/index/TestThreadedOptimize.java
index 60a655f..eac3057 100644
--- a/src/test/org/apache/lucene/index/TestThreadedOptimize.java
+++ b/src/test/org/apache/lucene/index/TestThreadedOptimize.java
@@ -51,9 +51,9 @@ public class TestThreadedOptimize extends LuceneTestCase {
     failed = true;
   }
 
-  public void runTest(Directory directory, boolean autoCommit, MergeScheduler merger) throws Exception {
+  public void runTest(Directory directory, MergeScheduler merger) throws Exception {
 
-    IndexWriter writer = new IndexWriter(directory, autoCommit, ANALYZER, true);
+    IndexWriter writer = new IndexWriter(directory, ANALYZER, true, IndexWriter.MaxFieldLength.UNLIMITED);
     writer.setMaxBufferedDocs(2);
     if (merger != null)
       writer.setMergeScheduler(merger);
@@ -73,8 +73,6 @@ public class TestThreadedOptimize extends LuceneTestCase {
       writer.setMergeFactor(4);
       //writer.setInfoStream(System.out);
 
-      final int docCount = writer.docCount();
-
       Thread[] threads = new Thread[NUM_THREADS];
       
       for(int i=0;i<NUM_THREADS;i++) {
@@ -118,11 +116,9 @@ public class TestThreadedOptimize extends LuceneTestCase {
 
       assertEquals(expectedDocCount, writer.docCount());
 
-      if (!autoCommit) {
-        writer.close();
-        writer = new IndexWriter(directory, autoCommit, ANALYZER, false);
-        writer.setMaxBufferedDocs(2);
-      }
+      writer.close();
+      writer = new IndexWriter(directory, ANALYZER, false, IndexWriter.MaxFieldLength.UNLIMITED);
+      writer.setMaxBufferedDocs(2);
 
       IndexReader reader = IndexReader.open(directory, true);
       assertTrue(reader.isOptimized());
@@ -138,10 +134,8 @@ public class TestThreadedOptimize extends LuceneTestCase {
   */
   public void testThreadedOptimize() throws Exception {
     Directory directory = new MockRAMDirectory();
-    runTest(directory, false, new SerialMergeScheduler());
-    runTest(directory, true, new SerialMergeScheduler());
-    runTest(directory, false, new ConcurrentMergeScheduler());
-    runTest(directory, true, new ConcurrentMergeScheduler());
+    runTest(directory, new SerialMergeScheduler());
+    runTest(directory, new ConcurrentMergeScheduler());
     directory.close();
 
     String tempDir = System.getProperty("tempDir");
@@ -150,10 +144,8 @@ public class TestThreadedOptimize extends LuceneTestCase {
 
     String dirName = tempDir + "/luceneTestThreadedOptimize";
     directory = FSDirectory.open(new File(dirName));
-    runTest(directory, false, new SerialMergeScheduler());
-    runTest(directory, true, new SerialMergeScheduler());
-    runTest(directory, false, new ConcurrentMergeScheduler());
-    runTest(directory, true, new ConcurrentMergeScheduler());
+    runTest(directory, new SerialMergeScheduler());
+    runTest(directory, new ConcurrentMergeScheduler());
     directory.close();
     _TestUtil.rmDir(dirName);
   }
diff --git a/src/test/org/apache/lucene/search/payloads/PayloadHelper.java b/src/test/org/apache/lucene/search/payloads/PayloadHelper.java
index 0a473a0..68ab661 100644
--- a/src/test/org/apache/lucene/search/payloads/PayloadHelper.java
+++ b/src/test/org/apache/lucene/search/payloads/PayloadHelper.java
@@ -103,7 +103,7 @@ public class PayloadHelper {
     RAMDirectory directory = new RAMDirectory();
     PayloadAnalyzer analyzer = new PayloadAnalyzer();
     IndexWriter writer
-            = new IndexWriter(directory, analyzer, true);
+            = new IndexWriter(directory, analyzer, true, IndexWriter.MaxFieldLength.UNLIMITED);
     writer.setSimilarity(similarity);
     //writer.infoStream = System.out;
     for (int i = 0; i < numDocs; i++) {
diff --git a/src/test/org/apache/lucene/search/spans/TestPayloadSpans.java b/src/test/org/apache/lucene/search/spans/TestPayloadSpans.java
index ef364fc..a6fe12a 100644
--- a/src/test/org/apache/lucene/search/spans/TestPayloadSpans.java
+++ b/src/test/org/apache/lucene/search/spans/TestPayloadSpans.java
@@ -115,7 +115,7 @@ public class TestPayloadSpans extends LuceneTestCase {
       throws IOException {
     RAMDirectory directory = new RAMDirectory();
     PayloadAnalyzer analyzer = new PayloadAnalyzer();
-    IndexWriter writer = new IndexWriter(directory, analyzer, true);
+    IndexWriter writer = new IndexWriter(directory, analyzer, true, IndexWriter.MaxFieldLength.UNLIMITED);
     writer.setSimilarity(similarity);
 
     Document doc = new Document();
@@ -308,7 +308,6 @@ public class TestPayloadSpans extends LuceneTestCase {
     for (int i = 0; i < topDocs.scoreDocs.length; i++) {
       while (spans.next()) {
         Collection payloads = spans.getPayload();
-        int cnt = 0;
         for (Iterator it = payloads.iterator(); it.hasNext();) {
           payloadSet.add(new String((byte[]) it.next()));
         }
@@ -362,7 +361,7 @@ public class TestPayloadSpans extends LuceneTestCase {
   public void testPayloadSpanUtil() throws Exception {
     RAMDirectory directory = new RAMDirectory();
     PayloadAnalyzer analyzer = new PayloadAnalyzer();
-    IndexWriter writer = new IndexWriter(directory, analyzer, true);
+    IndexWriter writer = new IndexWriter(directory, analyzer, true, IndexWriter.MaxFieldLength.UNLIMITED);
     writer.setSimilarity(similarity);
     Document doc = new Document();
     doc.add(new Field(PayloadHelper.FIELD,"xx rr yy mm  pp", Field.Store.YES, Field.Index.ANALYZED));
@@ -425,7 +424,7 @@ public class TestPayloadSpans extends LuceneTestCase {
     RAMDirectory directory = new RAMDirectory();
     PayloadAnalyzer analyzer = new PayloadAnalyzer();
     String[] docs = new String[]{"xx rr yy mm  pp","xx yy mm rr pp", "nopayload qq ss pp np", "one two three four five six seven eight nine ten eleven", "nine one two three four five six seven eight eleven ten"};
-    IndexWriter writer = new IndexWriter(directory, analyzer, true);
+    IndexWriter writer = new IndexWriter(directory, analyzer, true, IndexWriter.MaxFieldLength.UNLIMITED);
 
     writer.setSimilarity(similarity);
 

