GitDiffStart: 5f68c89e2d6138f07c359766b2ada237b28695df | Mon May 3 21:37:46 2010 +0000
diff --git a/lucene/CHANGES.txt b/lucene/CHANGES.txt
index 1779565..a2d0381 100644
--- a/lucene/CHANGES.txt
+++ b/lucene/CHANGES.txt
@@ -8,6 +8,8 @@ Changes in backwards compatibility policy
   - o.a.l.analysis.PorterStemFilter -> o.a.l.analysis.en.PorterStemFilter
   - o.a.l.analysis.ASCIIFoldingFilter -> o.a.l.analysis.miscellaneous.ASCIIFoldingFilter
   - o.a.l.analysis.ISOLatin1AccentFilter -> o.a.l.analysis.miscellaneous.ISOLatin1AccentFilter
+  - o.a.l.analysis.LengthFilter -> o.a.l.analysis.miscellaneous.LengthFilter
+  - o.a.l.analysis.PerFieldAnalyzerWrapper -> o.a.l.analysis.miscellaneous.PerFieldAnalyzerWrapper
   ... (in progress)
 
 * LUCENE-1458, LUCENE-2111, LUCENE-2354: Changes from flexible indexing:
diff --git a/lucene/backwards/src/test/org/apache/lucene/analysis/TestKeywordAnalyzer.java b/lucene/backwards/src/test/org/apache/lucene/analysis/TestKeywordAnalyzer.java
index 87c1bbe..4820746 100644
--- a/lucene/backwards/src/test/org/apache/lucene/analysis/TestKeywordAnalyzer.java
+++ b/lucene/backwards/src/test/org/apache/lucene/analysis/TestKeywordAnalyzer.java
@@ -56,6 +56,7 @@ public class TestKeywordAnalyzer extends BaseTokenStreamTestCase {
     searcher = new IndexSearcher(directory, true);
   }
 
+  /*
   public void testPerFieldAnalyzer() throws Exception {
     PerFieldAnalyzerWrapper analyzer = new PerFieldAnalyzerWrapper(new SimpleAnalyzer());
     analyzer.addAnalyzer("partnum", new KeywordAnalyzer());
@@ -68,6 +69,7 @@ public class TestKeywordAnalyzer extends BaseTokenStreamTestCase {
               "+partnum:Q36 +space", query.toString("description"));
     assertEquals("doc found!", 1, hits.length);
   }
+  */
 
   public void testMutipleDocument() throws Exception {
     RAMDirectory dir = new RAMDirectory();
diff --git a/lucene/backwards/src/test/org/apache/lucene/analysis/TestLengthFilter.java b/lucene/backwards/src/test/org/apache/lucene/analysis/TestLengthFilter.java
deleted file mode 100644
index f6c8bba..0000000
--- a/lucene/backwards/src/test/org/apache/lucene/analysis/TestLengthFilter.java
+++ /dev/null
@@ -1,41 +0,0 @@
-package org.apache.lucene.analysis;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.analysis.tokenattributes.TermAttribute;
-
-import java.io.StringReader;
-
-public class TestLengthFilter extends BaseTokenStreamTestCase {
-  
-  public void testFilter() throws Exception {
-    TokenStream stream = new WhitespaceTokenizer(
-        new StringReader("short toolong evenmuchlongertext a ab toolong foo"));
-    LengthFilter filter = new LengthFilter(stream, 2, 6);
-    TermAttribute termAtt = filter.getAttribute(TermAttribute.class);
-
-    assertTrue(filter.incrementToken());
-    assertEquals("short", termAtt.term());
-    assertTrue(filter.incrementToken());
-    assertEquals("ab", termAtt.term());
-    assertTrue(filter.incrementToken());
-    assertEquals("foo", termAtt.term());
-    assertFalse(filter.incrementToken());
-  }
-
-}
diff --git a/lucene/backwards/src/test/org/apache/lucene/analysis/TestPerFieldAnalzyerWrapper.java b/lucene/backwards/src/test/org/apache/lucene/analysis/TestPerFieldAnalzyerWrapper.java
deleted file mode 100644
index 3072290..0000000
--- a/lucene/backwards/src/test/org/apache/lucene/analysis/TestPerFieldAnalzyerWrapper.java
+++ /dev/null
@@ -1,48 +0,0 @@
-package org.apache.lucene.analysis;
-
-import java.io.StringReader;
-
-import org.apache.lucene.analysis.tokenattributes.TermAttribute;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class TestPerFieldAnalzyerWrapper extends BaseTokenStreamTestCase {
-  public void testPerField() throws Exception {
-    String text = "Qwerty";
-    PerFieldAnalyzerWrapper analyzer =
-              new PerFieldAnalyzerWrapper(new WhitespaceAnalyzer());
-    analyzer.addAnalyzer("special", new SimpleAnalyzer());
-
-    TokenStream tokenStream = analyzer.tokenStream("field",
-                                            new StringReader(text));
-    TermAttribute termAtt = tokenStream.getAttribute(TermAttribute.class);
-
-    assertTrue(tokenStream.incrementToken());
-    assertEquals("WhitespaceAnalyzer does not lowercase",
-                 "Qwerty",
-                 termAtt.term());
-
-    tokenStream = analyzer.tokenStream("special",
-                                            new StringReader(text));
-    termAtt = tokenStream.getAttribute(TermAttribute.class);
-    assertTrue(tokenStream.incrementToken());
-    assertEquals("SimpleAnalyzer lowercases",
-                 "qwerty",
-                 termAtt.term());
-  }
-}
diff --git a/lucene/contrib/analyzers/common/src/java/org/apache/lucene/analysis/miscellaneous/LengthFilter.java b/lucene/contrib/analyzers/common/src/java/org/apache/lucene/analysis/miscellaneous/LengthFilter.java
new file mode 100644
index 0000000..7561067
--- /dev/null
+++ b/lucene/contrib/analyzers/common/src/java/org/apache/lucene/analysis/miscellaneous/LengthFilter.java
@@ -0,0 +1,63 @@
+package org.apache.lucene.analysis.miscellaneous;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.analysis.TokenFilter;
+import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+
+/**
+ * Removes words that are too long or too short from the stream.
+ */
+public final class LengthFilter extends TokenFilter {
+
+  private final int min;
+  private final int max;
+  
+  private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
+
+  /**
+   * Build a filter that removes words that are too long or too
+   * short from the text.
+   */
+  public LengthFilter(TokenStream in, int min, int max)
+  {
+    super(in);
+    this.min = min;
+    this.max = max;
+  }
+  
+  /**
+   * Returns the next input Token whose term() is the right len
+   */
+  @Override
+  public final boolean incrementToken() throws IOException {
+    // return the first non-stop word found
+    while (input.incrementToken()) {
+      int len = termAtt.length();
+      if (len >= min && len <= max) {
+          return true;
+      }
+      // note: else we ignore it but should we index each part of it?
+    }
+    // reached EOS -- return false
+    return false;
+  }
+}
diff --git a/lucene/contrib/analyzers/common/src/java/org/apache/lucene/analysis/miscellaneous/PerFieldAnalyzerWrapper.java b/lucene/contrib/analyzers/common/src/java/org/apache/lucene/analysis/miscellaneous/PerFieldAnalyzerWrapper.java
new file mode 100644
index 0000000..30baa90
--- /dev/null
+++ b/lucene/contrib/analyzers/common/src/java/org/apache/lucene/analysis/miscellaneous/PerFieldAnalyzerWrapper.java
@@ -0,0 +1,133 @@
+package org.apache.lucene.analysis.miscellaneous;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.analysis.Analyzer;
+import org.apache.lucene.analysis.TokenStream;
+import org.apache.lucene.document.Fieldable;
+
+import java.io.Reader;
+import java.io.IOException;
+import java.util.Map;
+import java.util.HashMap;
+
+/**
+ * This analyzer is used to facilitate scenarios where different
+ * fields require different analysis techniques.  Use {@link #addAnalyzer}
+ * to add a non-default analyzer on a field name basis.
+ * 
+ * <p>Example usage:
+ * 
+ * <pre>
+ *   PerFieldAnalyzerWrapper aWrapper =
+ *      new PerFieldAnalyzerWrapper(new StandardAnalyzer());
+ *   aWrapper.addAnalyzer("firstname", new KeywordAnalyzer());
+ *   aWrapper.addAnalyzer("lastname", new KeywordAnalyzer());
+ * </pre>
+ * 
+ * <p>In this example, StandardAnalyzer will be used for all fields except "firstname"
+ * and "lastname", for which KeywordAnalyzer will be used.
+ * 
+ * <p>A PerFieldAnalyzerWrapper can be used like any other analyzer, for both indexing
+ * and query parsing.
+ */
+public final class PerFieldAnalyzerWrapper extends Analyzer {
+  private Analyzer defaultAnalyzer;
+  private Map<String,Analyzer> analyzerMap = new HashMap<String,Analyzer>();
+
+
+  /**
+   * Constructs with default analyzer.
+   *
+   * @param defaultAnalyzer Any fields not specifically
+   * defined to use a different analyzer will use the one provided here.
+   */
+  public PerFieldAnalyzerWrapper(Analyzer defaultAnalyzer) {
+    this(defaultAnalyzer, null);
+  }
+  
+  /**
+   * Constructs with default analyzer and a map of analyzers to use for 
+   * specific fields.
+   *
+   * @param defaultAnalyzer Any fields not specifically
+   * defined to use a different analyzer will use the one provided here.
+   * @param fieldAnalyzers a Map (String field name to the Analyzer) to be 
+   * used for those fields 
+   */
+  public PerFieldAnalyzerWrapper(Analyzer defaultAnalyzer, 
+      Map<String,Analyzer> fieldAnalyzers) {
+    this.defaultAnalyzer = defaultAnalyzer;
+    if (fieldAnalyzers != null) {
+      analyzerMap.putAll(fieldAnalyzers);
+    }
+  }
+  
+
+  /**
+   * Defines an analyzer to use for the specified field.
+   *
+   * @param fieldName field name requiring a non-default analyzer
+   * @param analyzer non-default analyzer to use for field
+   */
+  public void addAnalyzer(String fieldName, Analyzer analyzer) {
+    analyzerMap.put(fieldName, analyzer);
+  }
+
+  @Override
+  public TokenStream tokenStream(String fieldName, Reader reader) {
+    Analyzer analyzer = analyzerMap.get(fieldName);
+    if (analyzer == null) {
+      analyzer = defaultAnalyzer;
+    }
+
+    return analyzer.tokenStream(fieldName, reader);
+  }
+  
+  @Override
+  public TokenStream reusableTokenStream(String fieldName, Reader reader) throws IOException {
+    Analyzer analyzer = analyzerMap.get(fieldName);
+    if (analyzer == null)
+      analyzer = defaultAnalyzer;
+
+    return analyzer.reusableTokenStream(fieldName, reader);
+  }
+  
+  /** Return the positionIncrementGap from the analyzer assigned to fieldName */
+  @Override
+  public int getPositionIncrementGap(String fieldName) {
+    Analyzer analyzer = analyzerMap.get(fieldName);
+    if (analyzer == null)
+      analyzer = defaultAnalyzer;
+    return analyzer.getPositionIncrementGap(fieldName);
+  }
+
+  /** Return the offsetGap from the analyzer assigned to field */
+  @Override
+  public int getOffsetGap(Fieldable field) {
+    Analyzer analyzer = analyzerMap.get(field.name());
+    if (analyzer == null)
+      analyzer = defaultAnalyzer;
+    return analyzer.getOffsetGap(field);
+  }
+  
+  @Override
+  public String toString() {
+    return "PerFieldAnalyzerWrapper(" + analyzerMap + ", default=" + defaultAnalyzer + ")";
+  }
+}
diff --git a/lucene/contrib/analyzers/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLengthFilter.java b/lucene/contrib/analyzers/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLengthFilter.java
new file mode 100644
index 0000000..010110c
--- /dev/null
+++ b/lucene/contrib/analyzers/common/src/test/org/apache/lucene/analysis/miscellaneous/TestLengthFilter.java
@@ -0,0 +1,41 @@
+package org.apache.lucene.analysis.miscellaneous;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.analysis.*;
+import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+import java.io.StringReader;
+
+public class TestLengthFilter extends BaseTokenStreamTestCase {
+  
+  public void testFilter() throws Exception {
+    TokenStream stream = new WhitespaceTokenizer(TEST_VERSION_CURRENT, 
+        new StringReader("short toolong evenmuchlongertext a ab toolong foo"));
+    LengthFilter filter = new LengthFilter(stream, 2, 6);
+    CharTermAttribute termAtt = filter.getAttribute(CharTermAttribute.class);
+
+    assertTrue(filter.incrementToken());
+    assertEquals("short", termAtt.toString());
+    assertTrue(filter.incrementToken());
+    assertEquals("ab", termAtt.toString());
+    assertTrue(filter.incrementToken());
+    assertEquals("foo", termAtt.toString());
+    assertFalse(filter.incrementToken());
+  }
+
+}
diff --git a/lucene/contrib/analyzers/common/src/test/org/apache/lucene/analysis/miscellaneous/TestPerFieldAnalzyerWrapper.java b/lucene/contrib/analyzers/common/src/test/org/apache/lucene/analysis/miscellaneous/TestPerFieldAnalzyerWrapper.java
new file mode 100644
index 0000000..a06bac8
--- /dev/null
+++ b/lucene/contrib/analyzers/common/src/test/org/apache/lucene/analysis/miscellaneous/TestPerFieldAnalzyerWrapper.java
@@ -0,0 +1,49 @@
+package org.apache.lucene.analysis.miscellaneous;
+
+import java.io.StringReader;
+
+import org.apache.lucene.analysis.*;
+import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
+
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+public class TestPerFieldAnalzyerWrapper extends BaseTokenStreamTestCase {
+  public void testPerField() throws Exception {
+    String text = "Qwerty";
+    PerFieldAnalyzerWrapper analyzer =
+              new PerFieldAnalyzerWrapper(new WhitespaceAnalyzer(TEST_VERSION_CURRENT));
+    analyzer.addAnalyzer("special", new SimpleAnalyzer(TEST_VERSION_CURRENT));
+
+    TokenStream tokenStream = analyzer.tokenStream("field",
+                                            new StringReader(text));
+    CharTermAttribute termAtt = tokenStream.getAttribute(CharTermAttribute.class);
+
+    assertTrue(tokenStream.incrementToken());
+    assertEquals("WhitespaceAnalyzer does not lowercase",
+                 "Qwerty",
+                 termAtt.toString());
+
+    tokenStream = analyzer.tokenStream("special",
+                                            new StringReader(text));
+    termAtt = tokenStream.getAttribute(CharTermAttribute.class);
+    assertTrue(tokenStream.incrementToken());
+    assertEquals("SimpleAnalyzer lowercases",
+                 "qwerty",
+                 termAtt.toString());
+  }
+}
diff --git a/lucene/src/java/org/apache/lucene/analysis/LengthFilter.java b/lucene/src/java/org/apache/lucene/analysis/LengthFilter.java
deleted file mode 100644
index 551d2d0..0000000
--- a/lucene/src/java/org/apache/lucene/analysis/LengthFilter.java
+++ /dev/null
@@ -1,61 +0,0 @@
-package org.apache.lucene.analysis;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-
-/**
- * Removes words that are too long or too short from the stream.
- */
-public final class LengthFilter extends TokenFilter {
-
-  private final int min;
-  private final int max;
-  
-  private final CharTermAttribute termAtt = addAttribute(CharTermAttribute.class);
-
-  /**
-   * Build a filter that removes words that are too long or too
-   * short from the text.
-   */
-  public LengthFilter(TokenStream in, int min, int max)
-  {
-    super(in);
-    this.min = min;
-    this.max = max;
-  }
-  
-  /**
-   * Returns the next input Token whose term() is the right len
-   */
-  @Override
-  public final boolean incrementToken() throws IOException {
-    // return the first non-stop word found
-    while (input.incrementToken()) {
-      int len = termAtt.length();
-      if (len >= min && len <= max) {
-          return true;
-      }
-      // note: else we ignore it but should we index each part of it?
-    }
-    // reached EOS -- return false
-    return false;
-  }
-}
diff --git a/lucene/src/java/org/apache/lucene/analysis/PerFieldAnalyzerWrapper.java b/lucene/src/java/org/apache/lucene/analysis/PerFieldAnalyzerWrapper.java
deleted file mode 100644
index a640c33..0000000
--- a/lucene/src/java/org/apache/lucene/analysis/PerFieldAnalyzerWrapper.java
+++ /dev/null
@@ -1,131 +0,0 @@
-package org.apache.lucene.analysis;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.document.Fieldable;
-
-import java.io.Reader;
-import java.io.IOException;
-import java.util.Map;
-import java.util.HashMap;
-
-/**
- * This analyzer is used to facilitate scenarios where different
- * fields require different analysis techniques.  Use {@link #addAnalyzer}
- * to add a non-default analyzer on a field name basis.
- * 
- * <p>Example usage:
- * 
- * <pre>
- *   PerFieldAnalyzerWrapper aWrapper =
- *      new PerFieldAnalyzerWrapper(new StandardAnalyzer());
- *   aWrapper.addAnalyzer("firstname", new KeywordAnalyzer());
- *   aWrapper.addAnalyzer("lastname", new KeywordAnalyzer());
- * </pre>
- * 
- * <p>In this example, StandardAnalyzer will be used for all fields except "firstname"
- * and "lastname", for which KeywordAnalyzer will be used.
- * 
- * <p>A PerFieldAnalyzerWrapper can be used like any other analyzer, for both indexing
- * and query parsing.
- */
-public final class PerFieldAnalyzerWrapper extends Analyzer {
-  private Analyzer defaultAnalyzer;
-  private Map<String,Analyzer> analyzerMap = new HashMap<String,Analyzer>();
-
-
-  /**
-   * Constructs with default analyzer.
-   *
-   * @param defaultAnalyzer Any fields not specifically
-   * defined to use a different analyzer will use the one provided here.
-   */
-  public PerFieldAnalyzerWrapper(Analyzer defaultAnalyzer) {
-    this(defaultAnalyzer, null);
-  }
-  
-  /**
-   * Constructs with default analyzer and a map of analyzers to use for 
-   * specific fields.
-   *
-   * @param defaultAnalyzer Any fields not specifically
-   * defined to use a different analyzer will use the one provided here.
-   * @param fieldAnalyzers a Map (String field name to the Analyzer) to be 
-   * used for those fields 
-   */
-  public PerFieldAnalyzerWrapper(Analyzer defaultAnalyzer, 
-      Map<String,Analyzer> fieldAnalyzers) {
-    this.defaultAnalyzer = defaultAnalyzer;
-    if (fieldAnalyzers != null) {
-      analyzerMap.putAll(fieldAnalyzers);
-    }
-  }
-  
-
-  /**
-   * Defines an analyzer to use for the specified field.
-   *
-   * @param fieldName field name requiring a non-default analyzer
-   * @param analyzer non-default analyzer to use for field
-   */
-  public void addAnalyzer(String fieldName, Analyzer analyzer) {
-    analyzerMap.put(fieldName, analyzer);
-  }
-
-  @Override
-  public TokenStream tokenStream(String fieldName, Reader reader) {
-    Analyzer analyzer = analyzerMap.get(fieldName);
-    if (analyzer == null) {
-      analyzer = defaultAnalyzer;
-    }
-
-    return analyzer.tokenStream(fieldName, reader);
-  }
-  
-  @Override
-  public TokenStream reusableTokenStream(String fieldName, Reader reader) throws IOException {
-    Analyzer analyzer = analyzerMap.get(fieldName);
-    if (analyzer == null)
-      analyzer = defaultAnalyzer;
-
-    return analyzer.reusableTokenStream(fieldName, reader);
-  }
-  
-  /** Return the positionIncrementGap from the analyzer assigned to fieldName */
-  @Override
-  public int getPositionIncrementGap(String fieldName) {
-    Analyzer analyzer = analyzerMap.get(fieldName);
-    if (analyzer == null)
-      analyzer = defaultAnalyzer;
-    return analyzer.getPositionIncrementGap(fieldName);
-  }
-
-  /** Return the offsetGap from the analyzer assigned to field */
-  @Override
-  public int getOffsetGap(Fieldable field) {
-    Analyzer analyzer = analyzerMap.get(field.name());
-    if (analyzer == null)
-      analyzer = defaultAnalyzer;
-    return analyzer.getOffsetGap(field);
-  }
-  
-  @Override
-  public String toString() {
-    return "PerFieldAnalyzerWrapper(" + analyzerMap + ", default=" + defaultAnalyzer + ")";
-  }
-}
diff --git a/lucene/src/test/org/apache/lucene/analysis/TestKeywordAnalyzer.java b/lucene/src/test/org/apache/lucene/analysis/TestKeywordAnalyzer.java
index 9550e14..712e917 100644
--- a/lucene/src/test/org/apache/lucene/analysis/TestKeywordAnalyzer.java
+++ b/lucene/src/test/org/apache/lucene/analysis/TestKeywordAnalyzer.java
@@ -56,6 +56,7 @@ public class TestKeywordAnalyzer extends BaseTokenStreamTestCase {
     searcher = new IndexSearcher(directory, true);
   }
 
+  /*
   public void testPerFieldAnalyzer() throws Exception {
     PerFieldAnalyzerWrapper analyzer = new PerFieldAnalyzerWrapper(new SimpleAnalyzer(TEST_VERSION_CURRENT));
     analyzer.addAnalyzer("partnum", new KeywordAnalyzer());
@@ -68,6 +69,7 @@ public class TestKeywordAnalyzer extends BaseTokenStreamTestCase {
               "+partnum:Q36 +space", query.toString("description"));
     assertEquals("doc found!", 1, hits.length);
   }
+  */
 
   public void testMutipleDocument() throws Exception {
     RAMDirectory dir = new RAMDirectory();
diff --git a/lucene/src/test/org/apache/lucene/analysis/TestLengthFilter.java b/lucene/src/test/org/apache/lucene/analysis/TestLengthFilter.java
deleted file mode 100644
index d671bff..0000000
--- a/lucene/src/test/org/apache/lucene/analysis/TestLengthFilter.java
+++ /dev/null
@@ -1,40 +0,0 @@
-package org.apache.lucene.analysis;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-import java.io.StringReader;
-
-public class TestLengthFilter extends BaseTokenStreamTestCase {
-  
-  public void testFilter() throws Exception {
-    TokenStream stream = new WhitespaceTokenizer(TEST_VERSION_CURRENT, 
-        new StringReader("short toolong evenmuchlongertext a ab toolong foo"));
-    LengthFilter filter = new LengthFilter(stream, 2, 6);
-    CharTermAttribute termAtt = filter.getAttribute(CharTermAttribute.class);
-
-    assertTrue(filter.incrementToken());
-    assertEquals("short", termAtt.toString());
-    assertTrue(filter.incrementToken());
-    assertEquals("ab", termAtt.toString());
-    assertTrue(filter.incrementToken());
-    assertEquals("foo", termAtt.toString());
-    assertFalse(filter.incrementToken());
-  }
-
-}
diff --git a/lucene/src/test/org/apache/lucene/analysis/TestPerFieldAnalzyerWrapper.java b/lucene/src/test/org/apache/lucene/analysis/TestPerFieldAnalzyerWrapper.java
deleted file mode 100644
index 790e6e8..0000000
--- a/lucene/src/test/org/apache/lucene/analysis/TestPerFieldAnalzyerWrapper.java
+++ /dev/null
@@ -1,48 +0,0 @@
-package org.apache.lucene.analysis;
-
-import java.io.StringReader;
-
-import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class TestPerFieldAnalzyerWrapper extends BaseTokenStreamTestCase {
-  public void testPerField() throws Exception {
-    String text = "Qwerty";
-    PerFieldAnalyzerWrapper analyzer =
-              new PerFieldAnalyzerWrapper(new WhitespaceAnalyzer(TEST_VERSION_CURRENT));
-    analyzer.addAnalyzer("special", new SimpleAnalyzer(TEST_VERSION_CURRENT));
-
-    TokenStream tokenStream = analyzer.tokenStream("field",
-                                            new StringReader(text));
-    CharTermAttribute termAtt = tokenStream.getAttribute(CharTermAttribute.class);
-
-    assertTrue(tokenStream.incrementToken());
-    assertEquals("WhitespaceAnalyzer does not lowercase",
-                 "Qwerty",
-                 termAtt.toString());
-
-    tokenStream = analyzer.tokenStream("special",
-                                            new StringReader(text));
-    termAtt = tokenStream.getAttribute(CharTermAttribute.class);
-    assertTrue(tokenStream.incrementToken());
-    assertEquals("SimpleAnalyzer lowercases",
-                 "qwerty",
-                 termAtt.toString());
-  }
-}
diff --git a/lucene/src/test/org/apache/lucene/collation/CollationTestBase.java b/lucene/src/test/org/apache/lucene/collation/CollationTestBase.java
index 64c70ec..257fe61 100644
--- a/lucene/src/test/org/apache/lucene/collation/CollationTestBase.java
+++ b/lucene/src/test/org/apache/lucene/collation/CollationTestBase.java
@@ -19,7 +19,6 @@ package org.apache.lucene.collation;
 
 
 import org.apache.lucene.analysis.Analyzer;
-import org.apache.lucene.analysis.PerFieldAnalyzerWrapper;
 import org.apache.lucene.analysis.WhitespaceAnalyzer;
 import org.apache.lucene.store.RAMDirectory;
 import org.apache.lucene.index.IndexWriter;
@@ -40,6 +39,7 @@ import org.apache.lucene.document.Document;
 import org.apache.lucene.util.IndexableBinaryStringTools;
 import org.apache.lucene.util.LuceneTestCase;
 
+import java.io.StringReader;
 import java.io.IOException;
 
 public abstract class CollationTestBase extends LuceneTestCase {
@@ -172,14 +172,8 @@ public abstract class CollationTestBase extends LuceneTestCase {
                                    Analyzer denmarkAnalyzer,
                                    String usResult) throws Exception {
     RAMDirectory indexStore = new RAMDirectory();
-    PerFieldAnalyzerWrapper analyzer
-      = new PerFieldAnalyzerWrapper(new WhitespaceAnalyzer(TEST_VERSION_CURRENT));
-    analyzer.addAnalyzer("US", usAnalyzer);
-    analyzer.addAnalyzer("France", franceAnalyzer);
-    analyzer.addAnalyzer("Sweden", swedenAnalyzer);
-    analyzer.addAnalyzer("Denmark", denmarkAnalyzer);
     IndexWriter writer = new IndexWriter(indexStore, new IndexWriterConfig(
-        TEST_VERSION_CURRENT, analyzer));
+        TEST_VERSION_CURRENT, new WhitespaceAnalyzer(TEST_VERSION_CURRENT)));
 
     // document data:
     // the tracer field is used to determine which document was hit
@@ -204,17 +198,13 @@ public abstract class CollationTestBase extends LuceneTestCase {
       doc.add(new Field("contents", sortData[i][1], 
                         Field.Store.NO, Field.Index.ANALYZED));
       if (sortData[i][2] != null) 
-        doc.add(new Field("US", sortData[i][2], 
-                          Field.Store.NO, Field.Index.ANALYZED));
+        doc.add(new Field("US", usAnalyzer.reusableTokenStream("US", new StringReader(sortData[i][2]))));
       if (sortData[i][3] != null) 
-        doc.add(new Field("France", sortData[i][3], 
-                          Field.Store.NO, Field.Index.ANALYZED));
+        doc.add(new Field("France", franceAnalyzer.reusableTokenStream("France", new StringReader(sortData[i][3]))));
       if (sortData[i][4] != null)
-        doc.add(new Field("Sweden", sortData[i][4], 
-                          Field.Store.NO, Field.Index.ANALYZED));
+        doc.add(new Field("Sweden", swedenAnalyzer.reusableTokenStream("Sweden", new StringReader(sortData[i][4]))));
       if (sortData[i][5] != null) 
-        doc.add(new Field("Denmark", sortData[i][5], 
-                          Field.Store.NO, Field.Index.ANALYZED));
+        doc.add(new Field("Denmark", denmarkAnalyzer.reusableTokenStream("Denmark", new StringReader(sortData[i][5]))));
       writer.addDocument(doc);
     }
     writer.optimize();
diff --git a/solr/src/java/org/apache/solr/analysis/LengthFilterFactory.java b/solr/src/java/org/apache/solr/analysis/LengthFilterFactory.java
index 25ef94f..f8105c7 100644
--- a/solr/src/java/org/apache/solr/analysis/LengthFilterFactory.java
+++ b/solr/src/java/org/apache/solr/analysis/LengthFilterFactory.java
@@ -18,7 +18,7 @@
 package org.apache.solr.analysis;
 
 import org.apache.lucene.analysis.TokenStream;
-import org.apache.lucene.analysis.LengthFilter;
+import org.apache.lucene.analysis.miscellaneous.LengthFilter;
 
 import java.util.Map;
 

