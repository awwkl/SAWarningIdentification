GitDiffStart: 376256316b016f4971bfa86517c750fae42158c7 | Sat Feb 7 10:10:34 2015 +0000
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/bg/BulgarianAnalyzer.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/bg/BulgarianAnalyzer.java
index 76e6ca0..d461dd0 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/bg/BulgarianAnalyzer.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/bg/BulgarianAnalyzer.java
@@ -37,7 +37,6 @@ import org.apache.lucene.analysis.util.StopwordAnalyzerBase;
  * This analyzer implements light-stemming as specified by: <i> Searching
  * Strategies for the Bulgarian Language </i>
  * http://members.unine.ch/jacques.savoy/Papers/BUIR.pdf
- * <p>
  */
 public final class BulgarianAnalyzer extends StopwordAnalyzerBase {
 
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/package-info.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/package-info.java
index 3c416b5..12a43a5 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/package-info.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/charfilter/package-info.java
@@ -17,7 +17,6 @@
 
 /**
  * Normalization of text before the tokenizer.
- * </p>
  * <p>
  *   CharFilters are chainable filters that normalize text before tokenization 
  *   and provide mappings between normalized text offsets and the corresponding 
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/cjk/CJKAnalyzer.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/cjk/CJKAnalyzer.java
index dda8e93..ddbf0cd 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/cjk/CJKAnalyzer.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/cjk/CJKAnalyzer.java
@@ -38,7 +38,7 @@ public final class CJKAnalyzer extends StopwordAnalyzerBase {
 
   /**
    * File containing default CJK stopwords.
-   * <p/>
+   * <p>
    * Currently it contains some common English words that are not usually
    * useful for searching and some double-byte interpunctions.
    */
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/cjk/package-info.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/cjk/package-info.java
index 4649fb9..2fd4ad4 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/cjk/package-info.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/cjk/package-info.java
@@ -32,6 +32,5 @@
  *  <li>CJKAnalyzer: ???ï¼??ä¸??ä¸??ï¼??äº?</li>
  *  <li>SmartChineseAnalyzer: ??????ä¸??ï¼?ºº</li>
  * </ol>
- * </p>
  */
 package org.apache.lucene.analysis.cjk;
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/DictionaryCompoundWordTokenFilter.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/DictionaryCompoundWordTokenFilter.java
index 34e19b7..c94950e 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/DictionaryCompoundWordTokenFilter.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/DictionaryCompoundWordTokenFilter.java
@@ -27,7 +27,6 @@ import org.apache.lucene.analysis.util.CharArraySet;
  * "Donaudampfschiff" becomes Donau, dampf, schiff so that you can find
  * "Donaudampfschiff" even when you only enter "schiff".
  *  It uses a brute-force algorithm to achieve this.
- * <p>
  */
 public class DictionaryCompoundWordTokenFilter extends CompoundWordTokenFilterBase {
 
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/HyphenationCompoundWordTokenFilterFactory.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/HyphenationCompoundWordTokenFilterFactory.java
index 8479182..0fe31d0 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/HyphenationCompoundWordTokenFilterFactory.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/HyphenationCompoundWordTokenFilterFactory.java
@@ -48,7 +48,7 @@ import org.xml.sax.InputSource;
  *  <li><code>onlyLongestMatch</code> (optional): if true, adds only the longest matching subword 
  *    to the stream. defaults to false.
  * </ul>
- * <p>
+ * <br>
  * <pre class="prettyprint">
  * &lt;fieldType name="text_hyphncomp" class="solr.TextField" positionIncrementGap="100"&gt;
  *   &lt;analyzer&gt;
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/package-info.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/package-info.java
index 96d2183..126f887 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/package-info.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/compound/package-info.java
@@ -92,7 +92,7 @@
  * ). The files you need are in the subfolder
  * <i>offo-hyphenation/hyph/</i>
  * .
- * <br />
+ * <br>
  * Credits for the hyphenation code go to the
  * <a href="http://xmlgraphics.apache.org/fop/">Apache FOP project</a>
  * .
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/de/GermanNormalizationFilter.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/de/GermanNormalizationFilter.java
index 9c50b31..45b1e73 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/de/GermanNormalizationFilter.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/de/GermanNormalizationFilter.java
@@ -29,14 +29,12 @@ import org.apache.lucene.analysis.util.StemmerUtil;
  * of the <a href="http://snowball.tartarus.org/algorithms/german2/stemmer.html">
  * German2 snowball algorithm</a>.
  * It allows for the fact that Ã¤, Ã¶ and Ã¼ are sometimes written as ae, oe and ue.
- * <p>
  * <ul>
  *   <li> '?' is replaced by 'ss'
  *   <li> 'Ã¤', 'Ã¶', 'Ã¼' are replaced by 'a', 'o', 'u', respectively.
  *   <li> 'ae' and 'oe' are replaced by 'a', and 'o', respectively.
  *   <li> 'ue' is replaced by 'u', when not following a vowel or q.
  * </ul>
- * <p>
  * This is useful if you want this normalization without using
  * the German2 stemmer, or perhaps no stemming at all.
  */
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/en/KStemFilter.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/en/KStemFilter.java
index e0e705f..f5d7904 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/en/KStemFilter.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/en/KStemFilter.java
@@ -25,12 +25,12 @@ import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 import org.apache.lucene.analysis.tokenattributes.KeywordAttribute;
 
 /** A high-performance kstem filter for english.
- * <p/>
+ * <p>
  * See <a href="http://ciir.cs.umass.edu/pubfiles/ir-35.pdf">
  * "Viewing Morphology as an Inference Process"</a>
  * (Krovetz, R., Proceedings of the Sixteenth Annual International ACM SIGIR
  * Conference on Research and Development in Information Retrieval, 191-203, 1993).
- * <p/>
+ * <p>
  * All terms must already be lowercased for this filter to work correctly.
  *
  * <p>
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/en/PorterStemFilter.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/en/PorterStemFilter.java
index 449ebd5..f61f61c 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/en/PorterStemFilter.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/en/PorterStemFilter.java
@@ -33,7 +33,7 @@ import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
     Analyzer class that sets up the TokenStream chain as you want it.
     To use this with LowerCaseTokenizer, for example, you'd write an
     analyzer like this:
-    <P>
+    <br>
     <PRE class="prettyprint">
     class MyAnalyzer extends Analyzer {
       {@literal @Override}
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/CapitalizationFilter.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/CapitalizationFilter.java
index af2a2f3..76ce695 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/CapitalizationFilter.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/CapitalizationFilter.java
@@ -28,7 +28,7 @@ import org.apache.lucene.analysis.util.CharArraySet;
 /** 
  * A filter to apply normal capitalization rules to Tokens.  It will make the first letter
  * capital and the rest lower case.
- * <p/>
+ * <p>
  * This filter is particularly useful to build nice looking facet parameters.  This filter
  * is not appropriate if you intend to use a prefix query.
  */
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/CapitalizationFilterFactory.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/CapitalizationFilterFactory.java
index cc34391..f65c66d 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/CapitalizationFilterFactory.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/CapitalizationFilterFactory.java
@@ -29,19 +29,21 @@ import java.util.Set;
 
 /**
  * Factory for {@link CapitalizationFilter}.
- * <p/>
- * The factory takes parameters:<br/>
- * "onlyFirstWord" - should each word be capitalized or all of the words?<br/>
- * "keep" - a keep word list.  Each word that should be kept separated by whitespace.<br/>
- * "keepIgnoreCase - true or false.  If true, the keep list will be considered case-insensitive.<br/>
- * "forceFirstLetter" - Force the first letter to be capitalized even if it is in the keep list<br/>
- * "okPrefix" - do not change word capitalization if a word begins with something in this list.
+ * <p>
+ * The factory takes parameters:
+ * <ul>
+ * <li> "onlyFirstWord" - should each word be capitalized or all of the words?
+ * <li> "keep" - a keep word list.  Each word that should be kept separated by whitespace.
+ * <li> "keepIgnoreCase - true or false.  If true, the keep list will be considered case-insensitive.
+ * <li> "forceFirstLetter" - Force the first letter to be capitalized even if it is in the keep list
+ * <li> "okPrefix" - do not change word capitalization if a word begins with something in this list.
  * for example if "McK" is on the okPrefix list, the word "McKinley" should not be changed to
- * "Mckinley"<br/>
- * "minWordLength" - how long the word needs to be to get capitalization applied.  If the
- * minWordLength is 3, "and" &gt; "And" but "or" stays "or"<br/>
- * "maxWordCount" - if the token contains more then maxWordCount words, the capitalization is
- * assumed to be correct.<br/>
+ * "Mckinley"
+ * <li> "minWordLength" - how long the word needs to be to get capitalization applied.  If the
+ * minWordLength is 3, "and" &gt; "And" but "or" stays "or"
+ * <li>"maxWordCount" - if the token contains more then maxWordCount words, the capitalization is
+ * assumed to be correct.
+ * </ul>
  *
  * <pre class="prettyprint">
  * &lt;fieldType name="text_cptlztn" class="solr.TextField" positionIncrementGap="100"&gt;
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/PrefixAndSuffixAwareTokenFilter.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/PrefixAndSuffixAwareTokenFilter.java
index 48dd355..84fda6d 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/PrefixAndSuffixAwareTokenFilter.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/PrefixAndSuffixAwareTokenFilter.java
@@ -24,7 +24,7 @@ import java.io.IOException;
 
 /**
  * Links two {@link PrefixAwareTokenFilter}.
- * <p/>
+ * <p>
  * <b>NOTE:</b> This filter might not behave correctly if used with custom Attributes, i.e. Attributes other than
  * the ones located in org.apache.lucene.analysis.tokenattributes. 
  */
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/PrefixAwareTokenFilter.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/PrefixAwareTokenFilter.java
index 7ec6dd2..99fa87e 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/PrefixAwareTokenFilter.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/PrefixAwareTokenFilter.java
@@ -35,7 +35,7 @@ import java.io.IOException;
  * to be used when updating the token values in the second stream based on that token.
  *
  * The default implementation adds last prefix token end offset to the suffix token start and end offsets.
- * <p/>
+ * <p>
  * <b>NOTE:</b> This filter might not behave correctly if used with custom Attributes, i.e. Attributes other than
  * the ones located in org.apache.lucene.analysis.tokenattributes. 
  */
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/ScandinavianFoldingFilter.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/ScandinavianFoldingFilter.java
index ce7746d..9d4cfaf 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/ScandinavianFoldingFilter.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/ScandinavianFoldingFilter.java
@@ -27,25 +27,24 @@ import java.io.IOException;
 /**
  * This filter folds Scandinavian characters Ã¥?Ã¤Ã¦??-&gt;a and Ã¶?Ã¸?-&gt;o.
  * It also discriminate against use of double vowels aa, ae, ao, oe and oo, leaving just the first one.
- * <p/>
+ * <p>
  * It's a semantically more destructive solution than {@link ScandinavianNormalizationFilter} but
  * can in addition help with matching raksmorgas as rÃ¤ksmÃ¶rgÃ¥s.
- * <p/>
+ * <p>
  * blÃ¥bÃ¦rsyltetÃ¸j == blÃ¥bÃ¤rsyltetÃ¶j == blaabaarsyltetoej == blabarsyltetoj
  * rÃ¤ksmÃ¶rgÃ¥s == rÃ¦ksmÃ¸rgÃ¥s == rÃ¦ksmÃ¶rgaos == raeksmoergaas == raksmorgas
- * <p/>
+ * <p>
  * Background:
  * Swedish Ã¥Ã¤Ã¶ are in fact the same letters as Norwegian and Danish Ã¥Ã¦Ã¸ and thus interchangeable
  * when used between these languages. They are however folded differently when people type
  * them on a keyboard lacking these characters.
- * <p/>
+ * <p>
  * In that situation almost all Swedish people use a, a, o instead of Ã¥, Ã¤, Ã¶.
- * <p/>
+ * <p>
  * Norwegians and Danes on the other hand usually type aa, ae and oe instead of Ã¥, Ã¦ and Ã¸.
  * Some do however use a, a, o, oo, ao and sometimes permutations of everything above.
- * <p/>
+ * <p>
  * This filter solves that mismatch problem, but might also cause new.
- * <p/>
  * @see ScandinavianNormalizationFilter
  */
 public final class ScandinavianFoldingFilter extends TokenFilter {
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/ScandinavianNormalizationFilter.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/ScandinavianNormalizationFilter.java
index 3da0034..137d280 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/ScandinavianNormalizationFilter.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/ScandinavianNormalizationFilter.java
@@ -27,14 +27,13 @@ import java.io.IOException;
 /**
  * This filter normalize use of the interchangeable Scandinavian characters Ã¦?Ã¤?Ã¶?Ã¸?
  * and folded variants (aa, ao, ae, oe and oo) by transforming them to Ã¥?Ã¦?Ã¸?.
- * <p/>
+ * <p>
  * It's a semantically less destructive solution than {@link ScandinavianFoldingFilter},
  * most useful when a person with a Norwegian or Danish keyboard queries a Swedish index
  * and vice versa. This filter does <b>not</b>  the common Swedish folds of Ã¥ and Ã¤ to a nor Ã¶ to o.
- * <p/>
+ * <p>
  * blÃ¥bÃ¦rsyltetÃ¸j == blÃ¥bÃ¤rsyltetÃ¶j == blaabaarsyltetoej but not blabarsyltetoj
  * rÃ¤ksmÃ¶rgÃ¥s == rÃ¦ksmÃ¸rgÃ¥s == rÃ¦ksmÃ¶rgaos == raeksmoergaas but not raksmorgas
- * <p/>
  * @see ScandinavianFoldingFilter
  */
 public final class ScandinavianNormalizationFilter extends TokenFilter {
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/WordDelimiterFilter.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/WordDelimiterFilter.java
index aa69863..4fa2640 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/WordDelimiterFilter.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/miscellaneous/WordDelimiterFilter.java
@@ -96,42 +96,42 @@ public final class WordDelimiterFilter extends TokenFilter {
 
   /**
    * Causes parts of words to be generated:
-   * <p/>
+   * <p>
    * "PowerShot" =&gt; "Power" "Shot"
    */
   public static final int GENERATE_WORD_PARTS = 1;
 
   /**
    * Causes number subwords to be generated:
-   * <p/>
+   * <p>
    * "500-42" =&gt; "500" "42"
    */
   public static final int GENERATE_NUMBER_PARTS = 2;
 
   /**
    * Causes maximum runs of word parts to be catenated:
-   * <p/>
+   * <p>
    * "wi-fi" =&gt; "wifi"
    */
   public static final int CATENATE_WORDS = 4;
 
   /**
    * Causes maximum runs of word parts to be catenated:
-   * <p/>
+   * <p>
    * "wi-fi" =&gt; "wifi"
    */
   public static final int CATENATE_NUMBERS = 8;
 
   /**
    * Causes all subword parts to be catenated:
-   * <p/>
+   * <p>
    * "wi-fi-4000" =&gt; "wifi4000"
    */
   public static final int CATENATE_ALL = 16;
 
   /**
    * Causes original words are preserved and added to the subword list (Defaults to false)
-   * <p/>
+   * <p>
    * "500-42" =&gt; "500" "42" "500-42"
    */
   public static final int PRESERVE_ORIGINAL = 32;
@@ -150,7 +150,7 @@ public final class WordDelimiterFilter extends TokenFilter {
 
   /**
    * Causes trailing "'s" to be removed for each subword
-   * <p/>
+   * <p>
    * "O'Neil's" =&gt; "O", "Neil"
    */
   public static final int STEM_ENGLISH_POSSESSIVE = 256;
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter.java
index 219d4ca..ed6db2e 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter.java
@@ -31,7 +31,7 @@ import org.apache.lucene.analysis.util.CharacterUtils;
  * Tokenizes the given token into n-grams of given size(s).
  * <p>
  * This {@link TokenFilter} create n-grams from the beginning edge of a input token.
- * <p><a name="match_version" />As of Lucene 4.4, this filter handles correctly
+ * <p><a name="match_version"></a>As of Lucene 4.4, this filter handles correctly
  * supplementary characters.
  */
 public final class EdgeNGramTokenFilter extends TokenFilter {
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenizer.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenizer.java
index 9e277ab..796aca3 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenizer.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenizer.java
@@ -25,7 +25,7 @@ import org.apache.lucene.util.Version;
  * Tokenizes the input from an edge into n-grams of given size(s).
  * <p>
  * This {@link Tokenizer} create n-grams from the beginning edge of a input token.
- * <p><a name="match_version" />As of Lucene 4.4, this class supports
+ * <p><a name="match_version"></a>As of Lucene 4.4, this class supports
  * {@link #isTokenChar(int) pre-tokenization} and correctly handles
  * supplementary characters.
  */
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/NGramTokenizer.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/NGramTokenizer.java
index 40ba69e..0c6668cde5 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/NGramTokenizer.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/ngram/NGramTokenizer.java
@@ -39,7 +39,7 @@ import org.apache.lucene.util.AttributeFactory;
  * <tr><th>Position length</th><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td></tr>
  * <tr><th>Offsets</th><td>[0,2[</td><td>[0,3[</td><td>[1,3[</td><td>[1,4[</td><td>[2,4[</td><td>[2,5[</td><td>[3,5[</td></tr>
  * </table>
- * <a name="version"/>
+ * <a name="version"></a>
  * <p>This tokenizer changed a lot in Lucene 4.4 in order to:<ul>
  * <li>tokenize in a streaming fashion to support streams which are larger
  * than 1024 chars (limit of the previous version),
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/pattern/PatternCaptureGroupTokenFilter.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/pattern/PatternCaptureGroupTokenFilter.java
index 3410027..0a397fb 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/pattern/PatternCaptureGroupTokenFilter.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/pattern/PatternCaptureGroupTokenFilter.java
@@ -58,9 +58,9 @@ import org.apache.lucene.util.CharsRefBuilder;
  * </p>
  * <p>
  * <code>
- *   "([A-Z]{2,})",                                 <br />
- *   "(?&lt;![A-Z])([A-Z][a-z]+)",                     <br />
- *   "(?:^|\\b|(?&lt;=[0-9_])|(?&lt;=[A-Z]{2}))([a-z]+)", <br />
+ *   "([A-Z]{2,})",                                 
+ *   "(?&lt;![A-Z])([A-Z][a-z]+)",                     
+ *   "(?:^|\\b|(?&lt;=[0-9_])|(?&lt;=[A-Z]{2}))([a-z]+)",
  *   "([0-9]+)"
  * </code>
  * </p>
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/pattern/PatternReplaceCharFilter.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/pattern/PatternReplaceCharFilter.java
index ecfc370..398f67b 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/pattern/PatternReplaceCharFilter.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/pattern/PatternReplaceCharFilter.java
@@ -30,7 +30,7 @@ import org.apache.lucene.analysis.charfilter.BaseCharFilter;
  * The pattern match will be done in each "block" in char stream.
  * 
  * <p>
- * ex1) source="aa&nbsp;&nbsp;bb&nbsp;aa&nbsp;bb", pattern="(aa)\\s+(bb)" replacement="$1#$2"<br/>
+ * ex1) source="aa&nbsp;&nbsp;bb&nbsp;aa&nbsp;bb", pattern="(aa)\\s+(bb)" replacement="$1#$2"<br>
  * output="aa#bb&nbsp;aa#bb"
  * </p>
  * 
@@ -39,9 +39,9 @@ import org.apache.lucene.analysis.charfilter.BaseCharFilter;
  * face a trouble.
  * 
  * <p>
- * ex2) source="aa123bb", pattern="(aa)\\d+(bb)" replacement="$1&nbsp;$2"<br/>
- * output="aa&nbsp;bb"<br/>
- * and you want to search bb and highlight it, you will get<br/>
+ * ex2) source="aa123bb", pattern="(aa)\\d+(bb)" replacement="$1&nbsp;$2"<br>
+ * output="aa&nbsp;bb"<br>
+ * and you want to search bb and highlight it, you will get<br>
  * highlight snippet="aa1&lt;em&gt;23bb&lt;/em&gt;"
  * </p>
  * 
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/pattern/PatternTokenizer.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/pattern/PatternTokenizer.java
index 038a99e..faa7e91 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/pattern/PatternTokenizer.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/pattern/PatternTokenizer.java
@@ -30,7 +30,6 @@ import org.apache.lucene.util.AttributeFactory;
 /**
  * This tokenizer uses regex pattern matching to construct distinct tokens
  * for the input stream.  It takes two arguments:  "pattern" and "group".
- * <p/>
  * <ul>
  * <li>"pattern" is the regular expression.</li>
  * <li>"group" says which group to extract into tokens.</li>
@@ -41,7 +40,7 @@ import org.apache.lucene.util.AttributeFactory;
  * {@link String#split(java.lang.String)}
  * </p>
  * <p>
- * Using group &gt;= 0 selects the matching group as the token.  For example, if you have:<br/>
+ * Using group &gt;= 0 selects the matching group as the token.  For example, if you have:<br>
  * <pre>
  *  pattern = \'([^\']+)\'
  *  group = 0
@@ -49,7 +48,6 @@ import org.apache.lucene.util.AttributeFactory;
  *</pre>
  * the output will be two tokens: 'bbb' and 'ccc' (including the ' marks).  With the same input
  * but using group=1, the output would be: bbb and ccc (no ' marks)
- * </p>
  * <p>NOTE: This Tokenizer does not output tokens that are of zero length.</p>
  *
  * @see Pattern
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/pattern/PatternTokenizerFactory.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/pattern/PatternTokenizerFactory.java
index f48d4c8..42ea022 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/pattern/PatternTokenizerFactory.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/pattern/PatternTokenizerFactory.java
@@ -27,7 +27,7 @@ import org.apache.lucene.util.AttributeFactory;
  * Factory for {@link PatternTokenizer}.
  * This tokenizer uses regex pattern matching to construct distinct tokens
  * for the input stream.  It takes two arguments:  "pattern" and "group".
- * <p/>
+ * <br>
  * <ul>
  * <li>"pattern" is the regular expression.</li>
  * <li>"group" says which group to extract into tokens.</li>
@@ -38,7 +38,7 @@ import org.apache.lucene.util.AttributeFactory;
  * {@link String#split(java.lang.String)}
  * </p>
  * <p>
- * Using group &gt;= 0 selects the matching group as the token.  For example, if you have:<br/>
+ * Using group &gt;= 0 selects the matching group as the token.  For example, if you have:<br>
  * <pre>
  *  pattern = \'([^\']+)\'
  *  group = 0
@@ -46,7 +46,6 @@ import org.apache.lucene.util.AttributeFactory;
  * </pre>
  * the output will be two tokens: 'bbb' and 'ccc' (including the ' marks).  With the same input
  * but using group=1, the output would be: bbb and ccc (no ' marks)
- * </p>
  * <p>NOTE: This Tokenizer does not output tokens that are of zero length.</p>
  *
  * <pre class="prettyprint">
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/payloads/DelimitedPayloadTokenFilter.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/payloads/DelimitedPayloadTokenFilter.java
index 7f0fe7c..cf59717 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/payloads/DelimitedPayloadTokenFilter.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/payloads/DelimitedPayloadTokenFilter.java
@@ -26,12 +26,12 @@ import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
 
 /**
  * Characters before the delimiter are the "token", those after are the payload.
- * <p/>
+ * <p>
  * For example, if the delimiter is '|', then for the string "foo|bar", foo is the token
  * and "bar" is a payload.
- * <p/>
+ * <p>
  * Note, you can also include a {@link org.apache.lucene.analysis.payloads.PayloadEncoder} to convert the payload in an appropriate way (from characters to bytes).
- * <p/>
+ * <p>
  * Note make sure your Tokenizer doesn't split on the delimiter, or this won't work
  *
  * @see PayloadEncoder
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/payloads/FloatEncoder.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/payloads/FloatEncoder.java
index 5319a5e..41ced04 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/payloads/FloatEncoder.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/payloads/FloatEncoder.java
@@ -20,8 +20,7 @@ import org.apache.lucene.util.BytesRef;
  */
 
 /**
- *  Encode a character array Float as a {@link BytesRef}.
- * <p/>
+ * Encode a character array Float as a {@link BytesRef}.
  * @see org.apache.lucene.analysis.payloads.PayloadHelper#encodeFloat(float, byte[], int)
  *
  **/
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/payloads/IntegerEncoder.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/payloads/IntegerEncoder.java
index 2f1de16..a6228f4 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/payloads/IntegerEncoder.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/payloads/IntegerEncoder.java
@@ -22,7 +22,7 @@ import org.apache.lucene.util.BytesRef;
 
 /**
  *  Encode a character array Integer as a {@link BytesRef}.
- * <p/>
+ * <p>
  * See {@link org.apache.lucene.analysis.payloads.PayloadHelper#encodeInt(int, byte[], int)}.
  *
  **/
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/payloads/PayloadEncoder.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/payloads/PayloadEncoder.java
index cc30f47..11eb2a9 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/payloads/PayloadEncoder.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/payloads/PayloadEncoder.java
@@ -23,7 +23,7 @@ import org.apache.lucene.util.BytesRef;
 /**
  * Mainly for use with the DelimitedPayloadTokenFilter, converts char buffers to
  * {@link BytesRef}.
- * <p/>
+ * <p>
  * NOTE: This interface is subject to change 
  *
  **/
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/sinks/DateRecognizerSinkFilter.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/sinks/DateRecognizerSinkFilter.java
index 301615e..d73f866 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/sinks/DateRecognizerSinkFilter.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/sinks/DateRecognizerSinkFilter.java
@@ -28,7 +28,6 @@ import org.apache.lucene.util.AttributeSource;
 /**
  * Attempts to parse the {@link CharTermAttribute#buffer()} as a Date using a {@link java.text.DateFormat}.
  * If the value is a Date, it will add it to the sink.
- * <p/> 
  *
  **/
 public class DateRecognizerSinkFilter extends TeeSinkTokenFilter.SinkFilter {
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/sinks/TeeSinkTokenFilter.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/sinks/TeeSinkTokenFilter.java
index 69e90a9..a6a9b57 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/sinks/TeeSinkTokenFilter.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/sinks/TeeSinkTokenFilter.java
@@ -32,7 +32,7 @@ import org.apache.lucene.util.AttributeSource;
  * This TokenFilter provides the ability to set aside attribute states
  * that have already been analyzed.  This is useful in situations where multiple fields share
  * many common analysis steps and then go their separate ways.
- * <p/>
+ * <p>
  * It is also useful for doing things like entity extraction or proper noun analysis as
  * part of the analysis workflow and saving off those tokens for use in another field.
  *
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/snowball/SnowballFilter.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/snowball/SnowballFilter.java
index 9218c44..eacb43e 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/snowball/SnowballFilter.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/snowball/SnowballFilter.java
@@ -36,7 +36,6 @@ import org.tartarus.snowball.SnowballProgram;
  *  <li>For the Turkish language, see {@link TurkishLowerCaseFilter}.
  *  <li>For other languages, see {@link LowerCaseFilter}.
  * </ul>
- * </p>
  *
  * <p>
  * Note: This filter is aware of the {@link KeywordAttribute}. To prevent
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizer.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizer.java
index 2585e92..05d0605 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizer.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizer.java
@@ -31,7 +31,6 @@ import org.apache.lucene.util.AttributeFactory;
  * This class implements the Word Break rules from the
  * Unicode Text Segmentation algorithm, as specified in 
  * <a href="http://unicode.org/reports/tr29/">Unicode Standard Annex #29</a>.
- * <p/>
  * <p>Many applications have specific tokenizer needs.  If this tokenizer does
  * not suit your application, please consider copying this source code
  * directory to your project and maintaining your own grammar-based tokenizer.
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImpl.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImpl.java
index 3fa948e..81805cb 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImpl.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImpl.java
@@ -25,7 +25,7 @@ import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
  * This class implements Word Break rules from the Unicode Text Segmentation 
  * algorithm, as specified in 
  * <a href="http://unicode.org/reports/tr29/">Unicode Standard Annex #29</a>. 
- * <p/>
+ * <p>
  * Tokens produced are of the following types:
  * <ul>
  *   <li>&lt;ALPHANUM&gt;: A sequence of alphabetic and numeric characters</li>
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImpl.jflex b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImpl.jflex
index d99b17d..090822c 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImpl.jflex
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImpl.jflex
@@ -23,7 +23,7 @@ import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
  * This class implements Word Break rules from the Unicode Text Segmentation 
  * algorithm, as specified in 
  * <a href="http://unicode.org/reports/tr29/">Unicode Standard Annex #29</a>. 
- * <p/>
+ * <p>
  * Tokens produced are of the following types:
  * <ul>
  *   <li>&lt;ALPHANUM&gt;: A sequence of alphabetic and numeric characters</li>
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizer.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizer.java
index 1d1c944..741a7d6 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizer.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizer.java
@@ -31,7 +31,7 @@ import org.apache.lucene.util.AttributeFactory;
  * algorithm, as specified in 
  * <a href="http://unicode.org/reports/tr29/">Unicode Standard Annex #29</a> 
  * URLs and email addresses are also tokenized according to the relevant RFCs.
- * <p/>
+ * <p>
  * Tokens produced are of the following types:
  * <ul>
  *   <li>&lt;ALPHANUM&gt;: A sequence of alphabetic and numeric characters</li>
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizerImpl.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizerImpl.java
index e0a43fb..81621e2 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizerImpl.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizerImpl.java
@@ -26,7 +26,7 @@ import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
  * algorithm, as specified in 
  * <a href="http://unicode.org/reports/tr29/">Unicode Standard Annex #29</a> 
  * URLs and email addresses are also tokenized according to the relevant RFCs.
- * <p/>
+ * <p>
  * Tokens produced are of the following types:
  * <ul>
  *   <li>&lt;ALPHANUM&gt;: A sequence of alphabetic and numeric characters</li>
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizerImpl.jflex b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizerImpl.jflex
index 2aef724..7af02ac 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizerImpl.jflex
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/standard/UAX29URLEmailTokenizerImpl.jflex
@@ -24,7 +24,7 @@ import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;
  * algorithm, as specified in 
  * <a href="http://unicode.org/reports/tr29/">Unicode Standard Annex #29</a> 
  * URLs and email addresses are also tokenized according to the relevant RFCs.
- * <p/>
+ * <p>
  * Tokens produced are of the following types:
  * <ul>
  *   <li>&lt;ALPHANUM&gt;: A sequence of alphabetic and numeric characters</li>
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SynonymFilter.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SynonymFilter.java
index f659c0e..af656b6 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SynonymFilter.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SynonymFilter.java
@@ -58,7 +58,7 @@ import org.apache.lucene.util.fst.FST;
  * Then input <code>a b c d e</code> parses to <code>y b c
  * d</code>, ie the 2nd rule "wins" because it started
  * earliest and matched the most input tokens of other rules
- * starting at that point.</p>
+ * starting at that point.
  *
  * <p>A future improvement to this filter could allow
  * non-greedy parsing, such that the 3rd rule would win, and
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SynonymFilterFactory.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SynonymFilterFactory.java
index c8b01c5..4896fa8 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SynonymFilterFactory.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/synonym/SynonymFilterFactory.java
@@ -73,7 +73,6 @@ import org.apache.lucene.util.Version;
  *   <li><code>boolean expand</code> - true if conflation groups should be expanded, false if they are one-directional</li>
  *   <li><code>{@link Analyzer} analyzer</code> - an analyzer used for each raw synonym</li>
  * </ul>
- * </p>
  * @see SolrSynonymParser SolrSynonymParser: default format
  */
 public class SynonymFilterFactory extends TokenFilterFactory implements ResourceLoaderAware {
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/util/WordlistLoader.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/util/WordlistLoader.java
index 26f03b8..91f28f4 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/util/WordlistLoader.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/util/WordlistLoader.java
@@ -130,7 +130,6 @@ public class WordlistLoader {
    * <li>The comment character is the vertical line (&#124;).
    * <li>Lines may contain trailing comments.
    * </ul>
-   * </p>
    * 
    * @param reader Reader containing a Snowball stopword list
    * @param result the {@link CharArraySet} to fill with the readers words
@@ -164,7 +163,6 @@ public class WordlistLoader {
    * <li>The comment character is the vertical line (&#124;).
    * <li>Lines may contain trailing comments.
    * </ul>
-   * </p>
    * 
    * @param reader Reader containing a Snowball stopword list
    * @return A {@link CharArraySet} with the reader's words
diff --git a/lucene/analysis/common/src/java/org/apache/lucene/analysis/wikipedia/WikipediaTokenizer.java b/lucene/analysis/common/src/java/org/apache/lucene/analysis/wikipedia/WikipediaTokenizer.java
index cd96cd2..0779380 100644
--- a/lucene/analysis/common/src/java/org/apache/lucene/analysis/wikipedia/WikipediaTokenizer.java
+++ b/lucene/analysis/common/src/java/org/apache/lucene/analysis/wikipedia/WikipediaTokenizer.java
@@ -34,8 +34,6 @@ import java.util.*;
 /**
  * Extension of StandardTokenizer that is aware of Wikipedia syntax.  It is based off of the
  * Wikipedia tutorial available at http://en.wikipedia.org/wiki/Wikipedia:Tutorial, but it may not be complete.
- * <p/>
- * <p/>
  * @lucene.experimental
  */
 public final class WikipediaTokenizer extends Tokenizer {
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/standard/TestClassicAnalyzer.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/standard/TestClassicAnalyzer.java
index 67cab60..1180007 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/standard/TestClassicAnalyzer.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/standard/TestClassicAnalyzer.java
@@ -1,5 +1,22 @@
 package org.apache.lucene.analysis.standard;
 
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
 import org.apache.lucene.analysis.Analyzer;
 import org.apache.lucene.analysis.BaseTokenStreamTestCase;
 import org.apache.lucene.document.Document;
@@ -20,23 +37,7 @@ import java.io.IOException;
 import java.util.Arrays;
 import java.util.Random;
 
-
-/**
- * Copyright 2004 The Apache Software Foundation
- * <p/>
- * Licensed under the Apache License, Version 2.0 (the "License");
- * you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- * <p/>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p/>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
+/** tests for classicanalyzer */
 public class TestClassicAnalyzer extends BaseTokenStreamTestCase {
 
   private Analyzer  a = new ClassicAnalyzer();
diff --git a/lucene/analysis/common/src/test/org/apache/lucene/analysis/util/BaseTokenStreamFactoryTestCase.java b/lucene/analysis/common/src/test/org/apache/lucene/analysis/util/BaseTokenStreamFactoryTestCase.java
index 0b26c5b..08c2382 100644
--- a/lucene/analysis/common/src/test/org/apache/lucene/analysis/util/BaseTokenStreamFactoryTestCase.java
+++ b/lucene/analysis/common/src/test/org/apache/lucene/analysis/util/BaseTokenStreamFactoryTestCase.java
@@ -28,14 +28,14 @@ import org.apache.lucene.util.Version;
  * Base class for testing tokenstream factories. 
  * <p>
  * Example usage:
- * <code><pre>
+ * <pre class="prettyprint">
  *   Reader reader = new StringReader("Some Text to Analyze");
  *   reader = charFilterFactory("htmlstrip").create(reader);
  *   TokenStream stream = tokenizerFactory("standard").create(reader);
  *   stream = tokenFilterFactory("lowercase").create(stream);
  *   stream = tokenFilterFactory("asciifolding").create(stream);
  *   assertTokenStreamContents(stream, new String[] { "some", "text", "to", "analyze" });
- * </pre></code>
+ * </pre>
  */
 // TODO: this has to be here, since the abstract factories are not in lucene-core,
 // so test-framework doesnt know about them...
diff --git a/lucene/analysis/common/src/tools/java/org/apache/lucene/analysis/standard/GenerateJflexTLDMacros.java b/lucene/analysis/common/src/tools/java/org/apache/lucene/analysis/standard/GenerateJflexTLDMacros.java
index a96e41d..6722622 100644
--- a/lucene/analysis/common/src/tools/java/org/apache/lucene/analysis/standard/GenerateJflexTLDMacros.java
+++ b/lucene/analysis/common/src/tools/java/org/apache/lucene/analysis/standard/GenerateJflexTLDMacros.java
@@ -39,7 +39,7 @@ import java.util.regex.Pattern;
  * Generates a file containing JFlex macros to accept valid ASCII TLDs 
  * (top level domains), for inclusion in JFlex grammars that can accept 
  * domain names.
- * <p/> 
+ * <p> 
  * The IANA Root Zone Database is queried via HTTP from URL cmdline arg #0, the
  * response is parsed, and the results are written out to a file containing 
  * a JFlex macro that will accept all valid ASCII-only TLDs, including punycode 
diff --git a/lucene/analysis/icu/src/java/org/apache/lucene/analysis/icu/ICUTransformFilter.java b/lucene/analysis/icu/src/java/org/apache/lucene/analysis/icu/ICUTransformFilter.java
index 4256672..52493eb 100644
--- a/lucene/analysis/icu/src/java/org/apache/lucene/analysis/icu/ICUTransformFilter.java
+++ b/lucene/analysis/icu/src/java/org/apache/lucene/analysis/icu/ICUTransformFilter.java
@@ -46,11 +46,10 @@ import com.ibm.icu.text.UnicodeSet;
  * <li>Conversion from Fullwidth to Halfwidth forms.
  * <li>Script conversions, for example Serbian Cyrillic to Latin
  * </ul>
- * </p>
  * <p>
  * Example usage: <blockquote>stream = new ICUTransformFilter(stream,
  * Transliterator.getInstance("Traditional-Simplified"));</blockquote>
- * </p>
+ * <br>
  * For more details, see the <a
  * href="http://userguide.icu-project.org/transforms/general">ICU User
  * Guide</a>.
diff --git a/lucene/analysis/icu/src/java/org/apache/lucene/analysis/icu/segmentation/ICUTokenizerFactory.java b/lucene/analysis/icu/src/java/org/apache/lucene/analysis/icu/segmentation/ICUTokenizerFactory.java
index 0b3ad83..26c6fcf 100644
--- a/lucene/analysis/icu/src/java/org/apache/lucene/analysis/icu/segmentation/ICUTokenizerFactory.java
+++ b/lucene/analysis/icu/src/java/org/apache/lucene/analysis/icu/segmentation/ICUTokenizerFactory.java
@@ -42,8 +42,7 @@ import com.ibm.icu.text.RuleBasedBreakIterator;
  * Words are broken across script boundaries, then segmented according to
  * the BreakIterator and typing provided by the {@link DefaultICUTokenizerConfig}.
  *
- * <p/>
- *
+ * <p>
  * To use the default set of per-script rules:
  *
  * <pre class="prettyprint" >
@@ -53,13 +52,13 @@ import com.ibm.icu.text.RuleBasedBreakIterator;
  *   &lt;/analyzer&gt;
  * &lt;/fieldType&gt;</pre>
  *
- * <p/>
- *
+ * <p>
  * You can customize this tokenizer's behavior by specifying per-script rule files,
  * which are compiled by the ICU RuleBasedBreakIterator.  See the
  * <a href="http://userguide.icu-project.org/boundaryanalysis#TOC-RBBI-Rules"
  * >ICU RuleBasedBreakIterator syntax reference</a>.
  *
+ * <p>
  * To add per-script rules, add a "rulefiles" argument, which should contain a
  * comma-separated list of <tt>code:rulefile</tt> pairs in the following format:
  * <a href="http://unicode.org/iso15924/iso15924-codes.html"
diff --git a/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/JapaneseIterationMarkCharFilter.java b/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/JapaneseIterationMarkCharFilter.java
index 222b5ce..d4d7b54 100644
--- a/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/JapaneseIterationMarkCharFilter.java
+++ b/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/JapaneseIterationMarkCharFilter.java
@@ -28,11 +28,11 @@ import java.io.Reader;
  * <p>
  * Sequences of iteration marks are supported.  In case an illegal sequence of iteration
  * marks is encountered, the implementation emits the illegal source character as-is
- * without considering its script.  For example, with input "&#x003f;&#x309d;", we get
- * "&#x003f;&#x003f;" even though "&#x003f;" isn't hiragana.
+ * without considering its script.  For example, with input "???", we get
+ * "??" even though the question mark isn't hiragana.
  * </p>
  * <p>
- * Note that a full stop punctuation character "&#x3002;" (U+3002) can not be iterated
+ * Note that a full stop punctuation character "??" (U+3002) can not be iterated
  * (see below). Iteration marks themselves can be emitted in case they are illegal,
  * i.e. if they go back past the beginning of the character stream.
  * </p>
diff --git a/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/JapaneseNumberFilterFactory.java b/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/JapaneseNumberFilterFactory.java
index 9443768..8cb3ac1 100644
--- a/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/JapaneseNumberFilterFactory.java
+++ b/lucene/analysis/kuromoji/src/java/org/apache/lucene/analysis/ja/JapaneseNumberFilterFactory.java
@@ -24,7 +24,7 @@ import org.apache.lucene.analysis.util.TokenFilterFactory;
 
 /**
  * Factory for {@link JapaneseNumberFilter}.
- * <p>
+ * <br>
  * <pre class="prettyprint">
  * &lt;fieldType name="text_ja" class="solr.TextField"&gt;
  *   &lt;analyzer&gt;
diff --git a/lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseKatakanaStemFilter.java b/lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseKatakanaStemFilter.java
index a439b85..c7014fd 100644
--- a/lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseKatakanaStemFilter.java
+++ b/lucene/analysis/kuromoji/src/test/org/apache/lucene/analysis/ja/TestJapaneseKatakanaStemFilter.java
@@ -54,7 +54,6 @@ public class TestJapaneseKatakanaStemFilter extends BaseTokenStreamTestCase {
    *   <li>center</li>
    * </ul>
    * Note that we remove a long sound in the case of "coffee" that is required.
-   * </p>
    */
   public void testStemVariants() throws IOException {
     assertAnalyzesTo(analyzer, "?³ã??? ?³ã???? ?¿ã??·ã? ???????? ?????? ?»ã??¿ã?",
diff --git a/lucene/analysis/phonetic/src/java/org/apache/lucene/analysis/phonetic/PhoneticFilterFactory.java b/lucene/analysis/phonetic/src/java/org/apache/lucene/analysis/phonetic/PhoneticFilterFactory.java
index c357f72..8bf80b8 100644
--- a/lucene/analysis/phonetic/src/java/org/apache/lucene/analysis/phonetic/PhoneticFilterFactory.java
+++ b/lucene/analysis/phonetic/src/java/org/apache/lucene/analysis/phonetic/PhoneticFilterFactory.java
@@ -40,9 +40,8 @@ import org.apache.lucene.analysis.util.TokenFilterFactory;
 /**
  * Factory for {@link PhoneticFilter}.
  * 
- * Create tokens based on phonetic encoders from <a href="
- * http://commons.apache.org/codec/api-release/org/apache/commons/codec/language/package-summary.html
- * ">Apache Commons Codec</a>.
+ * Create tokens based on phonetic encoders from 
+ * <a href="http://commons.apache.org/codec/api-release/org/apache/commons/codec/language/package-summary.html">Apache Commons Codec</a>.
  * <p>
  * This takes one required argument, "encoder", and the rest are optional:
  * <dl>
diff --git a/lucene/analysis/uima/src/java/org/apache/lucene/analysis/uima/BaseUIMATokenizer.java b/lucene/analysis/uima/src/java/org/apache/lucene/analysis/uima/BaseUIMATokenizer.java
index f150306..cd1d376 100644
--- a/lucene/analysis/uima/src/java/org/apache/lucene/analysis/uima/BaseUIMATokenizer.java
+++ b/lucene/analysis/uima/src/java/org/apache/lucene/analysis/uima/BaseUIMATokenizer.java
@@ -54,7 +54,7 @@ public abstract class BaseUIMATokenizer extends Tokenizer {
 
   /**
    * analyzes the tokenizer input using the given analysis engine
-   * <p/>
+   * <p>
    * {@link #cas} will be filled with  extracted metadata (UIMA annotations, feature structures)
    *
    * @throws IOException If there is a low-level I/O error.
diff --git a/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/FileBasedQueryMaker.java b/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/FileBasedQueryMaker.java
index 622018b..9b653ea 100644
--- a/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/FileBasedQueryMaker.java
+++ b/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/FileBasedQueryMaker.java
@@ -38,7 +38,7 @@ import java.util.List;
  * File can be specified as a absolute, relative or resource.
  * Two properties can be set:
  * file.query.maker.file=&lt;Full path to file containing queries&gt;
- * <br/>
+ * <br>
  * file.query.maker.default.field=&lt;Name of default field - Default value is "body"&gt;
  *
  * Example:
diff --git a/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/AnalyzerFactoryTask.java b/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/AnalyzerFactoryTask.java
index b96b33e..ef31fbf 100644
--- a/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/AnalyzerFactoryTask.java
+++ b/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/AnalyzerFactoryTask.java
@@ -63,7 +63,7 @@ import java.util.regex.Pattern;
  * Each component analysis factory may specify <tt>luceneMatchVersion</tt> (defaults to
  * {@link Version#LATEST}) and any of the args understood by the specified
  * *Factory class, in the above-describe param format.
- * <p/>
+ * <p>
  * Example:
  * <pre>
  *     -AnalyzerFactory(name:'strip html, fold to ascii, whitespace tokenize, max 10k tokens',
@@ -75,7 +75,7 @@ import java.util.regex.Pattern;
  *     [...]
  *     -NewAnalyzer('strip html, fold to ascii, whitespace tokenize, max 10k tokens')
  * </pre>
- * <p/>
+ * <p>
  * AnalyzerFactory will direct analysis component factories to look for resources
  * under the directory specified in the "work.dir" property.
  */
diff --git a/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/NewAnalyzerTask.java b/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/NewAnalyzerTask.java
index b83aa9a..9205092 100644
--- a/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/NewAnalyzerTask.java
+++ b/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/NewAnalyzerTask.java
@@ -96,11 +96,11 @@ public class NewAnalyzerTask extends PerfTask {
   /**
    * Set the params (analyzerName only),  Comma-separate list of Analyzer class names.  If the Analyzer lives in
    * org.apache.lucene.analysis, the name can be shortened by dropping the o.a.l.a part of the Fully Qualified Class Name.
-   * <p/>
+   * <p>
    * Analyzer names may also refer to previously defined AnalyzerFactory's.
-   * <p/>
+   * <p>
    * Example Declaration: {"NewAnalyzer" NewAnalyzer(WhitespaceAnalyzer, SimpleAnalyzer, StopAnalyzer, standard.StandardAnalyzer) &gt;
-   * <p/>
+   * <p>
    * Example AnalyzerFactory usage:
    * <pre>
    * -AnalyzerFactory(name:'whitespace tokenized',WhitespaceTokenizer)
diff --git a/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/NewCollationAnalyzerTask.java b/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/NewCollationAnalyzerTask.java
index 3697e0a..ab06044 100644
--- a/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/NewCollationAnalyzerTask.java
+++ b/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/NewCollationAnalyzerTask.java
@@ -27,12 +27,11 @@ import org.apache.lucene.benchmark.byTask.PerfRunData;
 
 /**
  * Task to support benchmarking collation.
- * <p>
+ * <br>
  * <ul>
  *  <li> <code>NewCollationAnalyzer</code> with the default jdk impl
  *  <li> <code>NewCollationAnalyzer(impl:icu)</code> specify an impl (jdk,icu)
  * </ul>
- * </p>
  */
 public class NewCollationAnalyzerTask extends PerfTask {
   /**
diff --git a/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/NewLocaleTask.java b/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/NewLocaleTask.java
index c1c2282..6cdcadf 100644
--- a/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/NewLocaleTask.java
+++ b/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/NewLocaleTask.java
@@ -33,7 +33,6 @@ import org.apache.lucene.benchmark.byTask.PerfRunData;
  *  <li><code>ROOT</code>: The root (language-agnostic) Locale
  *  <li>&lt;empty string&gt;: Erase the Locale (null)
  * </ul>
- * </p>
  */
 public class NewLocaleTask extends PerfTask {
   private String language;
diff --git a/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTask.java b/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTask.java
index b940ac4..7d8dbb5 100644
--- a/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTask.java
+++ b/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTask.java
@@ -51,7 +51,6 @@ import org.apache.lucene.util.Bits;
  * Read index (abstract) task.
  * Sub classes implement withSearch(), withWarm(), withTraverse() and withRetrieve()
  * methods to configure the actual action.
- * <p/>
  * <p>Note: All ReadTasks reuse the reader if it is already open.
  * Otherwise a reader is opened at start and closed at the end.
  * <p>
@@ -238,7 +237,7 @@ public abstract class ReadTask extends PerfTask {
   /**
    * Specify the number of hits to traverse.  Tasks should override this if they want to restrict the number
    * of hits that are traversed when {@link #withTraverse()} is true. Must be greater than 0.
-   * <p/>
+   * <p>
    * Read task calculates the traversal as: Math.min(hits.length(), traversalSize())
    *
    * @return Integer.MAX_VALUE
diff --git a/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchTravTask.java b/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchTravTask.java
index faa5f43..535e946 100644
--- a/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchTravTask.java
+++ b/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/SearchTravTask.java
@@ -25,7 +25,6 @@ import org.apache.lucene.benchmark.byTask.feeds.QueryMaker;
  * 
  * <p>Note: This task reuses the reader if it is already open. 
  * Otherwise a reader is opened at start and closed at the end.
- * <p/>
  * 
  * <p>Takes optional param: traversal size (otherwise all results are traversed).</p>
  * 
diff --git a/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/utils/Config.java b/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/utils/Config.java
index f9456a3..b684158 100644
--- a/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/utils/Config.java
+++ b/lucene/benchmark/src/java/org/apache/lucene/benchmark/byTask/utils/Config.java
@@ -30,11 +30,11 @@ import java.util.StringTokenizer;
 
 /**
  * Perf run configuration properties.
- * <p/>
+ * <p>
  * Numeric property containing ":", e.g. "10:100:5" is interpreted
  * as array of numeric values. It is extracted once, on first use, and
  * maintain a round number to return the appropriate value.
- * <p/>
+ * <p>
  * The config property "work.dir" tells where is the root of
  * docs data dirs and indexes dirs. It is set to either of: <ul>
  * <li>value supplied for it in the alg file;</li>
diff --git a/lucene/classification/src/java/org/apache/lucene/classification/CachingNaiveBayesClassifier.java b/lucene/classification/src/java/org/apache/lucene/classification/CachingNaiveBayesClassifier.java
index e413c9f..d442856 100644
--- a/lucene/classification/src/java/org/apache/lucene/classification/CachingNaiveBayesClassifier.java
+++ b/lucene/classification/src/java/org/apache/lucene/classification/CachingNaiveBayesClassifier.java
@@ -42,7 +42,7 @@ import org.apache.lucene.util.BytesRef;
 /**
  * A simplistic Lucene based NaiveBayes classifier, with caching feature, see
  * <code>http://en.wikipedia.org/wiki/Naive_Bayes_classifier</code>
- * <p/>
+ * <p>
  * This is NOT an online classifier.
  *
  * @lucene.experimental
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/bloom/FuzzySet.java b/lucene/codecs/src/java/org/apache/lucene/codecs/bloom/FuzzySet.java
index 636ee2f..424d32e 100644
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/bloom/FuzzySet.java
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/bloom/FuzzySet.java
@@ -38,7 +38,6 @@ import org.apache.lucene.util.RamUsageEstimator;
  * </p> 
  * Another application of the set is that it can be used to perform fuzzy counting because
  * it can estimate reasonably accurately how many unique values are contained in the set. 
- * </p>
  * <p>This class is NOT threadsafe.</p>
  * <p>
  * Internally a Bitset is used to record values and once a client has finished recording
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/memory/FSTOrdTermsWriter.java b/lucene/codecs/src/java/org/apache/lucene/codecs/memory/FSTOrdTermsWriter.java
index 20d8306..33a306c 100644
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/memory/FSTOrdTermsWriter.java
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/memory/FSTOrdTermsWriter.java
@@ -63,9 +63,8 @@ import org.apache.lucene.util.fst.Util;
  *  <li><tt>.tix</tt>: <a href="#Termindex">Term Index</a></li>
  *  <li><tt>.tbk</tt>: <a href="#Termblock">Term Block</a></li>
  * </ul>
- * </p>
  *
- * <a name="Termindex" id="Termindex"></a>
+ * <a name="Termindex"></a>
  * <h3>Term Index</h3>
  * <p>
  *  The .tix contains a list of FSTs, one for each field.
@@ -87,7 +86,7 @@ import org.apache.lucene.util.fst.Util;
  *  </li>
  * </ul>
  *
- * <a name="Termblock" id="Termblock"></a>
+ * <a name="Termblock"></a>
  * <h3>Term Block</h3>
  * <p>
  *  The .tbk contains all the statistics and metadata for terms, along with field summary (e.g. 
@@ -98,7 +97,6 @@ import org.apache.lucene.util.fst.Util;
  *   <li>metadata bytes block: encodes other parts of metadata; </li>
  *   <li>skip block: contains skip data, to speed up metadata seeking and decoding</li>
  *  </ul>
- * </p>
  *
  * <p>File Format:</p>
  * <ul>
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/memory/FSTTermsWriter.java b/lucene/codecs/src/java/org/apache/lucene/codecs/memory/FSTTermsWriter.java
index 7faeade..a6bff29 100644
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/memory/FSTTermsWriter.java
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/memory/FSTTermsWriter.java
@@ -61,7 +61,7 @@ import org.apache.lucene.util.fst.Util;
  * </ul>
  * <p>
  *
- * <a name="Termdictionary" id="Termdictionary"></a>
+ * <a name="Termdictionary"></a>
  * <h3>Term Dictionary</h3>
  * <p>
  *  The .tst contains a list of FSTs, one for each field.
@@ -80,7 +80,6 @@ import org.apache.lucene.util.fst.Util;
  *    Generic byte array: Used to store non-monotonic metadata.
  *   </li>
  *  </ul>
- * </p>
  *
  * File format:
  * <ul>
diff --git a/lucene/common-build.xml b/lucene/common-build.xml
index 4ff842c..6d3ef6a 100644
--- a/lucene/common-build.xml
+++ b/lucene/common-build.xml
@@ -164,7 +164,7 @@
   <property name="javac.debug" value="on"/>
   <property name="javac.source" value="1.8"/>
   <property name="javac.target" value="1.8"/>
-  <property name="javac.args" value="-Xlint -Xlint:-deprecation -Xlint:-serial -Xlint:-options -Xdoclint:all/protected -Xdoclint:-html -Xdoclint:-missing"/>
+  <property name="javac.args" value="-Xlint -Xlint:-deprecation -Xlint:-serial -Xlint:-options -Xdoclint:all/protected -Xdoclint:-missing"/>
   <property name="javadoc.link" value="http://download.oracle.com/javase/8/docs/api/"/>
   <property name="javadoc.link.junit" value="http://junit.sourceforge.net/javadoc/"/>
   <property name="javadoc.packagelist.dir" location="${common.dir}/tools/javadoc"/>
@@ -373,7 +373,7 @@
   </target>
 
   <!-- for now enable only some doclint: -->
-  <property name="javadoc.args" value="-Xdoclint:all -Xdoclint:-html -Xdoclint:-missing"/>
+  <property name="javadoc.args" value="-Xdoclint:all -Xdoclint:-missing"/>
 
   <!-- Import custom ANT tasks. -->
   <import file="${common.dir}/tools/custom-tasks.xml" />
diff --git a/lucene/core/src/java/org/apache/lucene/analysis/CachingTokenFilter.java b/lucene/core/src/java/org/apache/lucene/analysis/CachingTokenFilter.java
index 47b6140..a8be808 100644
--- a/lucene/core/src/java/org/apache/lucene/analysis/CachingTokenFilter.java
+++ b/lucene/core/src/java/org/apache/lucene/analysis/CachingTokenFilter.java
@@ -29,7 +29,7 @@ import org.apache.lucene.util.AttributeSource;
  * are intended to be consumed more than once. It caches
  * all token attribute states locally in a List when the first call to
  * {@link #incrementToken()} is called. Subsequent calls will used the cache.
- * <p/>
+ * <p>
  * <em>Important:</em> Like any proper TokenFilter, {@link #reset()} propagates
  * to the input, although only before {@link #incrementToken()} is called the
  * first time. Prior to  Lucene 5, it was never propagated.
diff --git a/lucene/core/src/java/org/apache/lucene/analysis/Token.java b/lucene/core/src/java/org/apache/lucene/analysis/Token.java
index cdb8482..1cdd789 100644
--- a/lucene/core/src/java/org/apache/lucene/analysis/Token.java
+++ b/lucene/core/src/java/org/apache/lucene/analysis/Token.java
@@ -34,7 +34,7 @@ import org.apache.lucene.util.BytesRef;
   <p>
   The start and end offsets permit applications to re-associate a token with
   its source text, e.g., to display highlighted query terms in a document
-  browser, or to show matching text fragments in a <abbr title="KeyWord In Context">KWIC</abbr>
+  browser, or to show matching text fragments in a <a href="http://en.wikipedia.org/wiki/Key_Word_in_Context">KWIC</a>
   display, etc.
   <p>
   The type is a string, assigned by a lexical analyzer
@@ -61,12 +61,10 @@ import org.apache.lucene.util.BytesRef;
   <li>The startOffset and endOffset represent the start and offset in the source text, so be careful in adjusting them.</li>
   <li>When caching a reusable token, clone it. When injecting a cached token into a stream that can be reset, clone it again.</li>
   </ul>
-  </p>
   <p>
   <b>Please note:</b> With Lucene 3.1, the <code>{@linkplain #toString toString()}</code> method had to be changed to match the
   {@link CharSequence} interface introduced by the interface {@link org.apache.lucene.analysis.tokenattributes.CharTermAttribute}.
   This method now only prints the term text, no additional information anymore.
-  </p>
   @deprecated This class is outdated and no longer used since Lucene 2.9. Nuke it finally!
 */
 @Deprecated
diff --git a/lucene/core/src/java/org/apache/lucene/analysis/TokenStream.java b/lucene/core/src/java/org/apache/lucene/analysis/TokenStream.java
index ea6d696..81178254 100644
--- a/lucene/core/src/java/org/apache/lucene/analysis/TokenStream.java
+++ b/lucene/core/src/java/org/apache/lucene/analysis/TokenStream.java
@@ -161,7 +161,7 @@ public abstract class TokenStream extends AttributeSource implements Closeable {
    * consumed, after {@link #incrementToken()} returned <code>false</code>
    * (using the new <code>TokenStream</code> API). Streams implementing the old API
    * should upgrade to use this feature.
-   * <p/>
+   * <p>
    * This method can be used to perform any end-of-stream operations, such as
    * setting the final offset of a stream. The final offset of a stream might
    * differ from the offset of the last token eg in case one or more whitespaces
diff --git a/lucene/core/src/java/org/apache/lucene/analysis/package-info.java b/lucene/core/src/java/org/apache/lucene/analysis/package-info.java
index 396aced..1b228cc 100644
--- a/lucene/core/src/java/org/apache/lucene/analysis/package-info.java
+++ b/lucene/core/src/java/org/apache/lucene/analysis/package-info.java
@@ -21,13 +21,11 @@
  * <h2>Parsing? Tokenization? Analysis!</h2>
  * <p>
  * Lucene, an indexing and search library, accepts only plain text input.
- * <p>
  * <h2>Parsing</h2>
  * <p>
  * Applications that build their search capabilities upon Lucene may support documents in various formats &ndash; HTML, XML, PDF, Word &ndash; just to name a few.
  * Lucene does not care about the <i>Parsing</i> of these and other document formats, and it is the responsibility of the 
  * application using Lucene to use an appropriate <i>Parser</i> to convert the original format into plain text before passing that plain text to Lucene.
- * <p>
  * <h2>Tokenization</h2>
  * <p>
  * Plain text passed to Lucene for indexing goes through a process generally called tokenization. Tokenization is the process
@@ -67,8 +65,7 @@
  *       Adding in synonyms at the same token position as the current word can mean better 
  *       matching when users search with words in the synonym set.
  *   </li>
- * </ul> 
- * <p>
+ * </ul>
  * <h2>Core Analysis</h2>
  * <p>
  *   The analysis package provides the mechanism to convert Strings and Readers
@@ -249,7 +246,6 @@
  *         This might sometimes require a modified analyzer &ndash; see the next section on how to do that.
  *     </li>
  *   </ol>
- * </p>
  * <h2>Implementing your own Analyzer and Analysis Components</h2>
  * <p>
  *   Creating your own Analyzer is straightforward. Your Analyzer should subclass {@link org.apache.lucene.analysis.Analyzer}. It can use
@@ -416,7 +412,7 @@
  *    This new attribute makes clear that "IBM" and "International Business Machines" start and end
  *    at the same positions.
  * </p>
- * <a name="corrupt" />
+ * <a name="corrupt"></a>
  * <h3>How to not write corrupt token streams</h3>
  * <p>
  *    There are a few rules to observe when writing custom Tokenizers and TokenFilters:
@@ -586,7 +582,6 @@
  * a chain of a TokenStream and multiple TokenFilters is used, then all TokenFilters in that chain share the Attributes
  * with the TokenStream.
  * </li>
- * <br>
  * <li>
  * Attribute instances are reused for all tokens of a document. Thus, a TokenStream/-Filter needs to update
  * the appropriate Attribute(s) in incrementToken(). The consumer, commonly the Lucene indexer, consumes the data in the
@@ -594,13 +589,11 @@
  * was reached. This means that in each call of incrementToken() a TokenStream/-Filter can safely overwrite the data in
  * the Attribute instances.
  * </li>
- * <br>
  * <li>
  * For performance reasons a TokenStream/-Filter should add/get Attributes during instantiation; i.e., create an attribute in the
  * constructor and store references to it in an instance variable.  Using an instance variable instead of calling addAttribute()/getAttribute() 
  * in incrementToken() will avoid attribute lookups for every token in the document.
  * </li>
- * <br>
  * <li>
  * All methods in AttributeSource are idempotent, which means calling them multiple times always yields the same
  * result. This is especially important to know for addAttribute(). The method takes the <b>type</b> (<code>Class</code>)
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/SegmentInfoFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/SegmentInfoFormat.java
index b2e6fa9..e1892c7 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/SegmentInfoFormat.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/SegmentInfoFormat.java
@@ -26,8 +26,6 @@ import org.apache.lucene.store.IOContext;
 /**
  * Expert: Controls the format of the 
  * {@link SegmentInfo} (segment metadata file).
- * <p>
- * 
  * @see SegmentInfo
  * @lucene.experimental
  */
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/StoredFieldsWriter.java b/lucene/core/src/java/org/apache/lucene/codecs/StoredFieldsWriter.java
index 9d85f12..00a7d52 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/StoredFieldsWriter.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/StoredFieldsWriter.java
@@ -32,7 +32,6 @@ import org.apache.lucene.util.BytesRef;
 
 /**
  * Codec API for writing stored fields:
- * <p>
  * <ol>
  *   <li>For every document, {@link #startDocument()} is called,
  *       informing the Codec that a new document has started.
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/TermVectorsWriter.java b/lucene/core/src/java/org/apache/lucene/codecs/TermVectorsWriter.java
index e0a5a1a..0038e61 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/TermVectorsWriter.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/TermVectorsWriter.java
@@ -36,7 +36,6 @@ import org.apache.lucene.util.BytesRefBuilder;
 
 /**
  * Codec API for writing term vectors:
- * <p>
  * <ol>
  *   <li>For every document, {@link #startDocument(int)} is called,
  *       informing the Codec how many fields will be written.
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsWriter.java b/lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsWriter.java
index 5ddd6ff..2d96f85 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsWriter.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/blocktree/BlockTreeTermsWriter.java
@@ -93,7 +93,7 @@ import org.apache.lucene.util.packed.PackedInts;
  *   <li><tt>.tip</tt>: <a href="#Termindex">Term Index</a></li>
  * </ul>
  * <p>
- * <a name="Termdictionary" id="Termdictionary"></a>
+ * <a name="Termdictionary"></a>
  * <h3>Term Dictionary</h3>
  *
  * <p>The .tim file contains the list of terms in each
@@ -152,7 +152,7 @@ import org.apache.lucene.util.packed.PackedInts;
  *    <li>For inner nodes of the tree, every entry will steal one bit to mark whether it points
  *        to child nodes(sub-block). If so, the corresponding TermStats and TermMetaData are omitted </li>
  * </ul>
- * <a name="Termindex" id="Termindex"></a>
+ * <a name="Termindex"></a>
  * <h3>Term Index</h3>
  * <p>The .tip file contains an index into the term dictionary, so that it can be 
  * accessed randomly.  The index is also used to determine
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsIndexWriter.java b/lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsIndexWriter.java
index d61b058..9f76607 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsIndexWriter.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/compressing/CompressingStoredFieldsIndexWriter.java
@@ -37,8 +37,8 @@ import org.apache.lucene.util.packed.PackedInts;
  * 1024 chunks, this index computes the average number of bytes per
  * chunk and for every chunk, only stores the difference between<ul>
  * <li>${chunk number} * ${average length of a chunk}</li>
- * <li>and the actual start offset of the chunk</li></ul></p>
- * <p>Data is written as follows:</p>
+ * <li>and the actual start offset of the chunk</li></ul>
+ * <p>Data is written as follows:
  * <ul>
  * <li>PackedIntsVersion, &lt;Block&gt;<sup>BlockCount</sup>, BlocksEndMarker</li>
  * <li>PackedIntsVersion --&gt; {@link PackedInts#VERSION_CURRENT} as a {@link DataOutput#writeVInt VInt}</li>
@@ -57,7 +57,7 @@ import org.apache.lucene.util.packed.PackedInts;
  * <li>StartPointerDeltas --&gt; {@link PackedInts packed} array of BlockChunks elements of BitsPerStartPointerDelta bits each, representing the deltas from the average start pointer using <a href="https://developers.google.com/protocol-buffers/docs/encoding#types">ZigZag encoding</a></li>
  * <li>Footer --&gt; {@link CodecUtil#writeFooter CodecFooter}</li>
  * </ul>
- * <p>Notes</p>
+ * <p>Notes
  * <ul>
  * <li>For any block, the doc base of the n-th chunk can be restored with
  * <code>DocBase + AvgChunkDocs * n + DocBaseDeltas[n]</code>.</li>
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50DocValuesFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50DocValuesFormat.java
index 86bbac1..14fcdd9 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50DocValuesFormat.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50DocValuesFormat.java
@@ -89,7 +89,7 @@ import org.apache.lucene.util.packed.MonotonicBlockPackedWriter;
  *   <li><tt>.dvm</tt>: DocValues metadata</li>
  * </ol>
  * <ol>
- *   <li><a name="dvm" id="dvm"></a>
+ *   <li><a name="dvm"></a>
  *   <p>The DocValues metadata or .dvm file.</p>
  *   <p>For DocValues field, this stores metadata, such as the offset into the 
  *      DocValues data (.dvd)</p>
@@ -150,7 +150,7 @@ import org.apache.lucene.util.packed.MonotonicBlockPackedWriter;
  *      is written for the addresses.
  *   <p>MissingOffset points to a byte[] containing a bitset of all documents that had a value for the field.
  *      If it's -1, then there are no missing values. If it's -2, all values are missing.
- *   <li><a name="dvd" id="dvd"></a>
+ *   <li><a name="dvd"></a>
  *   <p>The DocValues data or .dvd file.</p>
  *   <p>For DocValues field, this stores the actual per-document data (the heavy-lifting)</p>
  *   <p>DocValues data (.dvd) --&gt; Header,&lt;NumericData | BinaryData | SortedData&gt;<sup>NumFields</sup>,Footer</p>
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50FieldInfosFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50FieldInfosFormat.java
index c8bd6d9..977aaaa 100755
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50FieldInfosFormat.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50FieldInfosFormat.java
@@ -40,10 +40,9 @@ import org.apache.lucene.store.IndexOutput;
 
 /**
  * Lucene 5.0 Field Infos format.
- * <p>
- * <p>Field names are stored in the field info file, with suffix <tt>.fnm</tt>.</p>
+ * <p>Field names are stored in the field info file, with suffix <tt>.fnm</tt>.
  * <p>FieldInfos (.fnm) --&gt; Header,FieldsCount, &lt;FieldName,FieldNumber,
- * FieldBits,DocValuesBits,DocValuesGen,Attributes&gt; <sup>FieldsCount</sup>,Footer</p>
+ * FieldBits,DocValuesBits,DocValuesGen,Attributes&gt; <sup>FieldsCount</sup>,Footer
  * <p>Data types:
  * <ul>
  *   <li>Header --&gt; {@link CodecUtil#checkIndexHeader IndexHeader}</li>
@@ -55,7 +54,6 @@ import org.apache.lucene.store.IndexOutput;
  *   <li>DocValuesGen --&gt; {@link DataOutput#writeLong(long) Int64}</li>
  *   <li>Footer --&gt; {@link CodecUtil#writeFooter CodecFooter}</li>
  * </ul>
- * </p>
  * Field Descriptions:
  * <ul>
  *   <li>FieldsCount: the number of fields in this file.</li>
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50LiveDocsFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50LiveDocsFormat.java
index 0dd4189..a9dda88 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50LiveDocsFormat.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50LiveDocsFormat.java
@@ -36,12 +36,11 @@ import org.apache.lucene.util.MutableBits;
 
 /** 
  * Lucene 5.0 live docs format 
- * <p>
  * <p>The .liv file is optional, and only exists when a segment contains
- * deletions.</p>
+ * deletions.
  * <p>Although per-segment, this file is maintained exterior to compound segment
- * files.</p>
- * <p>Deletions (.liv) --&gt; IndexHeader,Generation,Bits</p>
+ * files.
+ * <p>Deletions (.liv) --&gt; IndexHeader,Generation,Bits
  * <ul>
  *   <li>SegmentHeader --&gt; {@link CodecUtil#writeIndexHeader IndexHeader}</li>
  *   <li>Bits --&gt; &lt;{@link DataOutput#writeLong Int64}&gt; <sup>LongCount</sup></li>
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50NormsFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50NormsFormat.java
index 0f63f7a..d3c6c18 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50NormsFormat.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50NormsFormat.java
@@ -35,7 +35,6 @@ import org.apache.lucene.util.packed.PackedInts;
  * Lucene 5.0 Score normalization format.
  * <p>
  * Encodes normalization values with these strategies:
- * <p>
  * <ul>
  *    <li>Uncompressed: when values fit into a single byte and would require more than 4 bits
  *        per value, they are just encoded as an uncompressed byte array.
@@ -65,7 +64,7 @@ import org.apache.lucene.util.packed.PackedInts;
  *   <li><tt>.nvm</tt>: Norms metadata</li>
  * </ol>
  * <ol>
- *   <li><a name="nvm" id="nvm"></a>
+ *   <li><a name="nvm"></a>
  *   <p>The Norms metadata or .nvm file.</p>
  *   <p>For each norms field, this stores metadata, such as the offset into the 
  *      Norms data (.nvd)</p>
@@ -94,7 +93,7 @@ import org.apache.lucene.util.packed.PackedInts;
  *         <li>6 --&gt; patched table. Documents with very common values are written with a lookup table.
  *             Other values are written using a nested indirect.
  *      </ul>
- *   <li><a name="nvd" id="nvd"></a>
+ *   <li><a name="nvd"></a>
  *   <p>The Norms data or .nvd file.</p>
  *   <p>For each Norms field, this stores the actual per-document data (the heavy-lifting)</p>
  *   <p>Norms data (.nvd) --&gt; Header,&lt;Uncompressed | TableCompressed | DeltaCompressed | MonotonicCompressed &gt;<sup>NumFields</sup>,Footer</p>
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50PostingsFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50PostingsFormat.java
index b4337e3..a2bd71d 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50PostingsFormat.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50PostingsFormat.java
@@ -95,7 +95,6 @@ import org.apache.lucene.util.packed.PackedInts;
  *      this reduces disk pre-fetches.</p>
  *   </li>
  * </ul>
- * </p>
  *
  * <p>
  * Files and detailed format:
@@ -106,9 +105,8 @@ import org.apache.lucene.util.packed.PackedInts;
  *   <li><tt>.pos</tt>: <a href="#Positions">Positions</a></li>
  *   <li><tt>.pay</tt>: <a href="#Payloads">Payloads and Offsets</a></li>
  * </ul>
- * </p>
  *
- * <a name="Termdictionary" id="Termdictionary"></a>
+ * <a name="Termdictionary"></a>
  * <dl>
  * <dd>
  * <b>Term Dictionary</b>
@@ -118,11 +116,10 @@ import org.apache.lucene.util.packed.PackedInts;
  * and pointers to the frequencies, positions, payload and
  * skip data in the .doc, .pos, and .pay files.
  * See {@link BlockTreeTermsWriter} for more details on the format.
- * </p>
  *
  * <p>NOTE: The term dictionary can plug into different postings implementations:
  * the postings writer/reader are actually responsible for encoding 
- * and decoding the PostingsHeader and TermMetadata sections described here:</p>
+ * and decoding the PostingsHeader and TermMetadata sections described here:
  *
  * <ul>
  *   <li>PostingsHeader --&gt; Header, PackedBlockSize</li>
@@ -133,7 +130,7 @@ import org.apache.lucene.util.packed.PackedInts;
  *   <li>DocFPDelta, PosFPDelta, PayFPDelta, PosVIntBlockFPDelta, SkipFPDelta --&gt; {@link DataOutput#writeVLong VLong}</li>
  *   <li>Footer --&gt; {@link CodecUtil#writeFooter CodecFooter}</li>
  * </ul>
- * <p>Notes:</p>
+ * <p>Notes:
  * <ul>
  *    <li>Header is a {@link CodecUtil#writeIndexHeader IndexHeader} storing the version information
  *        for the postings.</li>
@@ -169,17 +166,17 @@ import org.apache.lucene.util.packed.PackedInts;
  * </dd>
  * </dl>
  *
- * <a name="Termindex" id="Termindex"></a>
+ * <a name="Termindex"></a>
  * <dl>
  * <dd>
  * <b>Term Index</b>
  * <p>The .tip file contains an index into the term dictionary, so that it can be 
- * accessed randomly.  See {@link BlockTreeTermsWriter} for more details on the format.</p>
+ * accessed randomly.  See {@link BlockTreeTermsWriter} for more details on the format.
  * </dd>
  * </dl>
  *
  *
- * <a name="Frequencies" id="Frequencies"></a>
+ * <a name="Frequencies"></a>
  * <dl>
  * <dd>
  * <b>Frequencies and Skip Data</b>
@@ -208,7 +205,7 @@ import org.apache.lucene.util.packed.PackedInts;
  *   <li>SkipChildLevelPointer --&gt; {@link DataOutput#writeVLong VLong}</li>
  *   <li>Footer --&gt; {@link CodecUtil#writeFooter CodecFooter}</li>
  * </ul>
- * <p>Notes:</p>
+ * <p>Notes:
  * <ul>
  *   <li>PackedDocDeltaBlock is theoretically generated from two steps: 
  *     <ol>
@@ -267,7 +264,7 @@ import org.apache.lucene.util.packed.PackedInts;
  * </dd>
  * </dl>
  *
- * <a name="Positions" id="Positions"></a>
+ * <a name="Positions"></a>
  * <dl>
  * <dd>
  * <b>Positions</b>
@@ -286,7 +283,7 @@ import org.apache.lucene.util.packed.PackedInts;
  *   <li>PayloadData --&gt; {@link DataOutput#writeByte byte}<sup>PayLength</sup></li>
  *   <li>Footer --&gt; {@link CodecUtil#writeFooter CodecFooter}</li>
  * </ul>
- * <p>Notes:</p>
+ * <p>Notes:
  * <ul>
  *   <li>TermPositions are order by term (terms are implicit, from the term dictionary), and position 
  *       values for each term document pair are incremental, and ordered by document number.</li>
@@ -320,12 +317,12 @@ import org.apache.lucene.util.packed.PackedInts;
  * </dd>
  * </dl>
  *
- * <a name="Payloads" id="Payloads"></a>
+ * <a name="Payloads"></a>
  * <dl>
  * <dd>
  * <b>Payloads and Offsets</b>
  * <p>The .pay file will store payloads and offsets associated with certain term-document positions. 
- *    Some payloads and offsets will be separated out into .pos file, for performance reasons.</p>
+ *    Some payloads and offsets will be separated out into .pos file, for performance reasons.
  * <ul>
  *   <li>PayFile(.pay): --&gt; Header, &lt;TermPayloads, TermOffsets?&gt; <sup>TermCount</sup>, Footer</li>
  *   <li>Header --&gt; {@link CodecUtil#writeIndexHeader IndexHeader}</li>
@@ -336,7 +333,7 @@ import org.apache.lucene.util.packed.PackedInts;
  *   <li>PayData --&gt; {@link DataOutput#writeByte byte}<sup>SumPayLength</sup></li>
  *   <li>Footer --&gt; {@link CodecUtil#writeFooter CodecFooter}</li>
  * </ul>
- * <p>Notes:</p>
+ * <p>Notes:
  * <ul>
  *   <li>The order of TermPayloads/TermOffsets will be the same as TermPositions, note that part of 
  *       payload/offsets are stored in .pos.</li>
@@ -352,7 +349,6 @@ import org.apache.lucene.util.packed.PackedInts;
  * </ul>
  * </dd>
  * </dl>
- * </p>
  *
  * @lucene.experimental
  */
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50SegmentInfoFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50SegmentInfoFormat.java
index 6b673fb..7198657 100755
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50SegmentInfoFormat.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50SegmentInfoFormat.java
@@ -43,9 +43,7 @@ import org.apache.lucene.util.Version;
  * <ul>
  *   <li><tt>.si</tt>: Header, SegVersion, SegSize, IsCompoundFile, Diagnostics, Files, Attributes, Footer
  * </ul>
- * </p>
  * Data types:
- * <p>
  * <ul>
  *   <li>Header --&gt; {@link CodecUtil#writeIndexHeader IndexHeader}</li>
  *   <li>SegSize --&gt; {@link DataOutput#writeInt Int32}</li>
@@ -55,9 +53,7 @@ import org.apache.lucene.util.Version;
  *   <li>IsCompoundFile --&gt; {@link DataOutput#writeByte Int8}</li>
  *   <li>Footer --&gt; {@link CodecUtil#writeFooter CodecFooter}</li>
  * </ul>
- * </p>
  * Field Descriptions:
- * <p>
  * <ul>
  *   <li>SegVersion is the code version that created the segment.</li>
  *   <li>SegSize is the number of documents contained in the segment index.</li>
@@ -70,7 +66,6 @@ import org.apache.lucene.util.Version;
  *       addIndexes), etc.</li>
  *   <li>Files is a list of files referred to by this segment.</li>
  * </ul>
- * </p>
  * 
  * @see SegmentInfos
  * @lucene.experimental
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50StoredFieldsFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50StoredFieldsFormat.java
index 2774e01..16bc2d5 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50StoredFieldsFormat.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50StoredFieldsFormat.java
@@ -38,7 +38,7 @@ import org.apache.lucene.util.packed.PackedInts;
 /**
  * Lucene 5.0 stored fields format.
  *
- * <p><b>Principle</b></p>
+ * <p><b>Principle</b>
  * <p>This {@link StoredFieldsFormat} compresses blocks of documents in
  * order to improve the compression ratio compared to document-level
  * compression. It uses the <a href="http://code.google.com/p/lz4/">LZ4</a>
@@ -50,17 +50,17 @@ import org.apache.lucene.util.packed.PackedInts;
  * compression, you can choose ({@link Mode#BEST_COMPRESSION BEST_COMPRESSION}), which uses 
  * the <a href="http://en.wikipedia.org/wiki/DEFLATE">DEFLATE</a> algorithm with 60KB blocks 
  * for a better ratio at the expense of slower performance. 
- * These two options can be configured like this: </p>
+ * These two options can be configured like this:
  * <pre class="prettyprint">
  *   // the default: for high performance
  *   indexWriterConfig.setCodec(new Lucene50Codec(Mode.BEST_SPEED));
  *   // instead for higher performance (but slower):
  *   // indexWriterConfig.setCodec(new Lucene50Codec(Mode.BEST_COMPRESSION));
  * </pre>
- * <p><b>File formats</b></p>
- * <p>Stored fields are represented by two files:</p>
+ * <p><b>File formats</b>
+ * <p>Stored fields are represented by two files:
  * <ol>
- * <li><a name="field_data" id="field_data"></a>
+ * <li><a name="field_data"></a>
  * <p>A fields data file (extension <tt>.fdt</tt>). This file stores a compact
  * representation of documents in compressed blocks of 16KB or more. When
  * writing a segment, documents are appended to an in-memory <tt>byte[]</tt>
@@ -106,7 +106,7 @@ import org.apache.lucene.util.packed.PackedInts;
  * <li>DirtyChunkCount --&gt; the number of prematurely flushed chunks in this file</li>
  * <li>Footer --&gt; {@link CodecUtil#writeFooter CodecFooter}</li>
  * </ul>
- * <p>Notes</p>
+ * <p>Notes
  * <ul>
  * <li>If documents are larger than 16KB then chunks will likely contain only
  * one document. However, documents can never spread across several chunks (all
@@ -123,7 +123,7 @@ import org.apache.lucene.util.packed.PackedInts;
  * 0.5% larger than Docs.</li>
  * </ul>
  * </li>
- * <li><a name="field_index" id="field_index"></a>
+ * <li><a name="field_index"></a>
  * <p>A fields index file (extension <tt>.fdx</tt>).</p>
  * <ul>
  * <li>FieldsIndex (.fdx) --&gt; &lt;Header&gt;, &lt;ChunkIndex&gt;, Footer</li>
@@ -133,9 +133,9 @@ import org.apache.lucene.util.packed.PackedInts;
  * </ul>
  * </li>
  * </ol>
- * <p><b>Known limitations</b></p>
+ * <p><b>Known limitations</b>
  * <p>This {@link StoredFieldsFormat} does not support individual documents
- * larger than (<tt>2<sup>31</sup> - 2<sup>14</sup></tt>) bytes.</p>
+ * larger than (<tt>2<sup>31</sup> - 2<sup>14</sup></tt>) bytes.
  * @lucene.experimental
  */
 public final class Lucene50StoredFieldsFormat extends StoredFieldsFormat {
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50TermVectorsFormat.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50TermVectorsFormat.java
index ca62754..719da84 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50TermVectorsFormat.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene50/Lucene50TermVectorsFormat.java
@@ -48,7 +48,7 @@ import org.apache.lucene.util.packed.PackedInts;
  * Looking up term vectors for any document requires at most 1 disk seek.
  * <p><b>File formats</b>
  * <ol>
- * <li><a name="vector_data" id="vector_data"></a>
+ * <li><a name="vector_data"></a>
  * <p>A vector data file (extension <tt>.tvd</tt>). This file stores terms,
  * frequencies, positions, offsets and payloads for every document. Upon writing
  * a new segment, it accumulates data into memory until the buffer used to store
@@ -111,8 +111,8 @@ import org.apache.lucene.util.packed.PackedInts;
  * <li>Footer --&gt; {@link CodecUtil#writeFooter CodecFooter}</li>
  * </ul>
  * </li>
- * <li><a name="vector_index" id="vector_index"></a>
- * <p>An index file (extension <tt>.tvx</tt>).</p>
+ * <li><a name="vector_index"></a>
+ * <p>An index file (extension <tt>.tvx</tt>).
  * <ul>
  * <li>VectorIndex (.tvx) --&gt; &lt;Header&gt;, &lt;ChunkIndex&gt;, Footer</li>
  * <li>Header --&gt; {@link CodecUtil#writeIndexHeader IndexHeader}</li>
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/lucene50/package-info.java b/lucene/core/src/java/org/apache/lucene/codecs/lucene50/package-info.java
index c142d91..144f01c 100755
--- a/lucene/core/src/java/org/apache/lucene/codecs/lucene50/package-info.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/lucene50/package-info.java
@@ -32,12 +32,13 @@
  * </li>
  * <li><a href="#Overview">Index Structure Overview</a></li>
  * <li><a href="#File_Naming">File Naming</a></li>
- * <li><a href="#file-names">Summary of File Extensions</a></li>
+ * <li><a href="#file-names">Summary of File Extensions</a>
  *   <ul>
  *   <li><a href="#Lock_File">Lock File</a></li>
  *   <li><a href="#History">History</a></li>
  *   <li><a href="#Limitations">Limitations</a></li>
  *   </ul>
+ * </li>
  * </ul>
  * </div>
  * <a name="Introduction"></a>
@@ -57,7 +58,7 @@
  * different programming languages should endeavor to agree on file formats, and
  * generate new versions of this document.</p>
  * </div>
- * <a name="Definitions" id="Definitions"></a>
+ * <a name="Definitions"></a>
  * <h2>Definitions</h2>
  * <div>
  * <p>The fundamental concepts in Lucene are index, document, field and term.</p>
@@ -88,7 +89,7 @@
  * indexed literally.</p>
  * <p>See the {@link org.apache.lucene.document.Field Field}
  * java docs for more information on Fields.</p>
- * <a name="Segments" id="Segments"></a>
+ * <a name="Segments"></a>
  * <h3>Segments</h3>
  * <p>Lucene indexes may be composed of multiple sub-indexes, or <i>segments</i>.
  * Each segment is a fully independent index, which could be searched separately.
@@ -128,7 +129,7 @@
  * </li>
  * </ul>
  * </div>
- * <a name="Overview" id="Overview"></a>
+ * <a name="Overview"></a>
  * <h2>Index Structure Overview</h2>
  * <div>
  * <p>Each segment index maintains the following:</p>
@@ -211,7 +212,7 @@
  * segments_1, then segments_2, etc. The generation is a sequential long integer
  * represented in alpha-numeric (base 36) form.</p>
  * </div>
- * <a name="file-names" id="file-names"></a>
+ * <a name="file-names"></a>
  * <h2>Summary of File Extensions</h2>
  * <div>
  * <p>The following table summarizes the names and extensions of the files in
@@ -316,14 +317,14 @@
  * </tr>
  * </table>
  * </div>
- * <a name="Lock_File" id="Lock_File"></a>
+ * <a name="Lock_File"></a>
  * <h2>Lock File</h2>
  * The write lock, which is stored in the index directory by default, is named
  * "write.lock". If the lock directory is different from the index directory then
  * the write lock will be named "XXXX-write.lock" where XXXX is a unique prefix
  * derived from the full path to the index directory. When this file is present, a
  * writer is currently modifying the index (adding or removing documents). This
- * lock file ensures that only one writer is modifying the index at a time.</p>
+ * lock file ensures that only one writer is modifying the index at a time.
  * <a name="History"></a>
  * <h2>History</h2>
  * <p>Compatibility notes are provided in this document, describing how file
@@ -386,7 +387,7 @@
  * that is suitable for faceting/sorting/analytics.
  * </li>
  * </ul>
- * <a name="Limitations" id="Limitations"></a>
+ * <a name="Limitations"></a>
  * <h2>Limitations</h2>
  * <div>
  * <p>Lucene uses a Java <code>int</code> to refer to
diff --git a/lucene/core/src/java/org/apache/lucene/codecs/package-info.java b/lucene/core/src/java/org/apache/lucene/codecs/package-info.java
index 43faa3a..28b260d 100644
--- a/lucene/core/src/java/org/apache/lucene/codecs/package-info.java
+++ b/lucene/core/src/java/org/apache/lucene/codecs/package-info.java
@@ -30,7 +30,6 @@
  *   <li>Norms - see {@link org.apache.lucene.codecs.NormsFormat}</li>
  *   <li>Live documents - see {@link org.apache.lucene.codecs.LiveDocsFormat}</li>
  * </ul>
- * </p>
  *  
  *   For some concrete implementations beyond Lucene's official index format, see
  *   the <a href="{@docRoot}/../codecs/overview-summary.html">Codecs module</a>.
@@ -51,7 +50,6 @@
  * You will need to register the Codec class so that the {@link java.util.ServiceLoader ServiceLoader} can find it, by including a
  * META-INF/services/org.apache.lucene.codecs.Codec file on your classpath that contains the package-qualified
  * name of your codec.
- * </p>
  * 
  * <p>
  *   If you just want to customise the {@link org.apache.lucene.codecs.PostingsFormat}, or use different postings
diff --git a/lucene/core/src/java/org/apache/lucene/document/Field.java b/lucene/core/src/java/org/apache/lucene/document/Field.java
index aa0aecf..6ee7243 100644
--- a/lucene/core/src/java/org/apache/lucene/document/Field.java
+++ b/lucene/core/src/java/org/apache/lucene/document/Field.java
@@ -42,13 +42,13 @@ import org.apache.lucene.util.BytesRef;
  * NumericDocValuesField}, {@link SortedDocValuesField}, {@link
  * StringField}, {@link TextField}, {@link StoredField}.
  *
- * <p/> A field is a section of a Document. Each field has three
+ * <p> A field is a section of a Document. Each field has three
  * parts: name, type and value. Values may be text
  * (String, Reader or pre-analyzed TokenStream), binary
  * (byte[]), or numeric (a Number).  Fields are optionally stored in the
  * index, so that they may be returned with hits on the document.
  *
- * <p/>
+ * <p>
  * NOTE: the field type is an {@link IndexableFieldType}.  Making changes
  * to the state of the IndexableFieldType will impact any
  * Field it is used in.  It is strongly recommended that no
diff --git a/lucene/core/src/java/org/apache/lucene/document/LongField.java b/lucene/core/src/java/org/apache/lucene/document/LongField.java
index 9dad385..e7d530f 100644
--- a/lucene/core/src/java/org/apache/lucene/document/LongField.java
+++ b/lucene/core/src/java/org/apache/lucene/document/LongField.java
@@ -59,21 +59,21 @@ import org.apache.lucene.util.NumericUtils;
  * value, either by dividing the result of
  * {@link java.util.Date#getTime} or using the separate getters
  * (for year, month, etc.) to construct an <code>int</code> or
- * <code>long</code> value.</p>
+ * <code>long</code> value.
  *
  * <p>To perform range querying or filtering against a
  * <code>LongField</code>, use {@link NumericRangeQuery} or {@link
  * NumericRangeFilter}.  To sort according to a
  * <code>LongField</code>, use the normal numeric sort types, eg
  * {@link org.apache.lucene.search.SortField.Type#LONG}. <code>LongField</code> 
- * values can also be loaded directly from {@link org.apache.lucene.index.LeafReader#getNumericDocValues}.</p>
+ * values can also be loaded directly from {@link org.apache.lucene.index.LeafReader#getNumericDocValues}.
  *
  * <p>You may add the same field name as an <code>LongField</code> to
  * the same document more than once.  Range querying and
  * filtering will be the logical OR of all values; so a range query
  * will hit all documents that have at least one value in
  * the range. However sort behavior is not defined.  If you need to sort,
- * you should separately index a single-valued <code>LongField</code>.</p>
+ * you should separately index a single-valued <code>LongField</code>.
  *
  * <p>A <code>LongField</code> will consume somewhat more disk space
  * in the index than an ordinary single-valued field.
@@ -111,7 +111,7 @@ import org.apache.lucene.util.NumericUtils;
  * <p>If you only need to sort by numeric value, and never
  * run range querying/filtering, you can index using a
  * <code>precisionStep</code> of {@link Integer#MAX_VALUE}.
- * This will minimize disk space consumed. </p>
+ * This will minimize disk space consumed.
  *
  * <p>More advanced users can instead use {@link
  * NumericTokenStream} directly, when indexing numbers. This
diff --git a/lucene/core/src/java/org/apache/lucene/index/DirectoryReader.java b/lucene/core/src/java/org/apache/lucene/index/DirectoryReader.java
index c636444..7ab0399 100644
--- a/lucene/core/src/java/org/apache/lucene/index/DirectoryReader.java
+++ b/lucene/core/src/java/org/apache/lucene/index/DirectoryReader.java
@@ -384,7 +384,6 @@ public abstract class DirectoryReader extends BaseCompositeReader<LeafReader> {
 
   /**
    * Expert: return the IndexCommit that this reader has opened.
-   * <p/>
    * @lucene.experimental
    */
   public abstract IndexCommit getIndexCommit() throws IOException;
diff --git a/lucene/core/src/java/org/apache/lucene/index/SegmentInfos.java b/lucene/core/src/java/org/apache/lucene/index/SegmentInfos.java
index 7549be2..b92bfe9 100644
--- a/lucene/core/src/java/org/apache/lucene/index/SegmentInfos.java
+++ b/lucene/core/src/java/org/apache/lucene/index/SegmentInfos.java
@@ -63,9 +63,7 @@ import org.apache.lucene.util.StringHelper;
  * HasSegID, SegID, SegCodec, DelGen, DeletionCount, FieldInfosGen, DocValuesGen,
  * UpdatesFiles&gt;<sup>SegCount</sup>, CommitUserData, Footer
  * </ul>
- * </p>
  * Data types:
- * <p>
  * <ul>
  * <li>Header --&gt; {@link CodecUtil#writeIndexHeader IndexHeader}</li>
  * <li>NameCounter, SegCount, DeletionCount --&gt;
@@ -81,9 +79,7 @@ import org.apache.lucene.util.StringHelper;
  * {@link DataOutput#writeStringSet(Set) Set&lt;String&gt;}&gt;</li>
  * <li>Footer --&gt; {@link CodecUtil#writeFooter CodecFooter}</li>
  * </ul>
- * </p>
  * Field Descriptions:
- * <p>
  * <ul>
  * <li>Version counts how often the index has been changed by adding or deleting
  * documents.</li>
@@ -113,7 +109,6 @@ import org.apache.lucene.util.StringHelper;
  * <li>UpdatesFiles stores the set of files that were updated in that segment
  * per field.</li>
  * </ul>
- * </p>
  * 
  * @lucene.experimental
  */
diff --git a/lucene/core/src/java/org/apache/lucene/index/package-info.java b/lucene/core/src/java/org/apache/lucene/index/package-info.java
index e81409a..a82ac2c 100644
--- a/lucene/core/src/java/org/apache/lucene/index/package-info.java
+++ b/lucene/core/src/java/org/apache/lucene/index/package-info.java
@@ -19,7 +19,6 @@
  * Code to maintain and access indices.
  * <!-- TODO: add IndexWriter, IndexWriterConfig, DocValues, etc etc -->
  * <h2>Table Of Contents</h2>
- * <p>
  *     <ol>
  *         <li><a href="#postings">Postings APIs</a>
  *             <ul>
@@ -38,7 +37,6 @@
  *             </ul>
  *         </li>
  *     </ol>
- * </p>
  * <a name="postings"></a>
  * <h2>Postings APIs</h2>
  * <a name="fields"></a>
@@ -63,7 +61,6 @@
  *   Terms terms = fields.terms(field);
  * }
  * </pre>
- * </p>
  * <a name="terms"></a>
  * <h3>
  *     Terms
@@ -100,7 +97,6 @@
  *   DocsAndPositionsEnum docsAndPositions = termsEnum.docsAndPositions(null, null);
  * }
  * </pre>
- * </p>
  * <a name="documents"></a>
  * <h3>
  *   Documents
@@ -116,7 +112,6 @@
  *   System.out.println(docsEnum.freq());
  *  }
  * </pre>
- * </p>
  * <a name="positions"></a>
  * <h3>
  *   Positions
@@ -140,14 +135,12 @@
  *   }
  * }
  * </pre>
- * </p>
  * <a name="stats"></a>
  * <h2>Index Statistics</h2>
  * <a name="termstats"></a>
  * <h3>
  *     Term statistics
  * </h3>
- * <p>
  *     <ul>
  *        <li>{@link org.apache.lucene.index.TermsEnum#docFreq}: Returns the number of 
  *            documents that contain at least one occurrence of the term. This statistic 
@@ -162,12 +155,10 @@
  *            for the field. Like docFreq(), it will also count occurrences that appear in 
  *            deleted documents.
  *     </ul>
- * </p>
  * <a name="fieldstats"></a>
  * <h3>
  *     Field statistics
  * </h3>
- * <p>
  *     <ul>
  *        <li>{@link org.apache.lucene.index.Terms#size}: Returns the number of 
  *            unique terms in the field. This statistic may be unavailable 
@@ -194,12 +185,10 @@
  *            ({@link org.apache.lucene.index.IndexOptions#DOCS DOCS}) 
  *            for the field.
  *     </ul>
- * </p>
  * <a name="segmentstats"></a>
  * <h3>
  *     Segment statistics
  * </h3>
- * <p>
  *     <ul>
  *        <li>{@link org.apache.lucene.index.IndexReader#maxDoc}: Returns the number of 
  *            documents (including deleted documents) in the index. 
@@ -210,7 +199,6 @@
  *        <li>{@link org.apache.lucene.index.Fields#size}: Returns the number of indexed
  *            fields.
  *     </ul>
- * </p>
  * <a name="documentstats"></a>
  * <h3>
  *     Document statistics
@@ -220,8 +208,6 @@
  * a {@link org.apache.lucene.search.similarities.Similarity} implementation will store some
  * of these values (possibly in a lossy way), into the normalization value for the document in
  * its {@link org.apache.lucene.search.similarities.Similarity#computeNorm} method.
- * </p>
- * <p>
  *     <ul>
  *        <li>{@link org.apache.lucene.index.FieldInvertState#getLength}: Returns the number of 
  *            tokens for this field in the document. Note that this is just the number
@@ -248,11 +234,8 @@
  *        <li>{@link org.apache.lucene.index.FieldInvertState#getMaxTermFrequency}: Returns the maximum
  *            frequency across all unique terms encountered for this field in the document. 
  *     </ul>
- * </p>
  * <p>
  * Additional user-supplied statistics can be added to the document as DocValues fields and
  * accessed via {@link org.apache.lucene.index.LeafReader#getNumericDocValues}.
- * </p>
- * <p>
  */
 package org.apache.lucene.index;
diff --git a/lucene/core/src/java/org/apache/lucene/search/DocValuesTermsFilter.java b/lucene/core/src/java/org/apache/lucene/search/DocValuesTermsFilter.java
index f4bc6e1..b182712 100644
--- a/lucene/core/src/java/org/apache/lucene/search/DocValuesTermsFilter.java
+++ b/lucene/core/src/java/org/apache/lucene/search/DocValuesTermsFilter.java
@@ -32,8 +32,7 @@ import org.apache.lucene.util.FixedBitSet;
  * term value in the specified field is contained in the
  * provided set of allowed terms.
  * 
- * <p/>
- * 
+ * <p>
  * This is the same functionality as TermsFilter (from
  * queries/), except this filter requires that the
  * field contains only a single term for all documents.
@@ -41,9 +40,7 @@ import org.apache.lucene.util.FixedBitSet;
  * also have different performance characteristics, as
  * described below.
  * 
- * 
- * <p/>
- * 
+ * <p>
  * With each search, this filter translates the specified
  * set of Terms into a private {@link FixedBitSet} keyed by
  * term number per unique {@link IndexReader} (normally one
@@ -58,8 +55,7 @@ import org.apache.lucene.util.FixedBitSet;
  * index with a great many small documents may find this
  * linear scan too costly.
  * 
- * <p/>
- * 
+ * <p>
  * In contrast, TermsFilter builds up an {@link FixedBitSet},
  * keyed by docID, every time it's created, by enumerating
  * through all matching docs using {@link org.apache.lucene.index.PostingsEnum} to seek
@@ -70,8 +66,7 @@ import org.apache.lucene.util.FixedBitSet;
  * to the number of terms, which can be exceptionally costly
  * when there are cache misses in the OS's IO cache.
  * 
- * <p/>
- * 
+ * <p>
  * Generally, this filter will be slower on the first
  * invocation for a given field, but subsequent invocations,
  * even if you change the allowed set of Terms, should be
@@ -81,8 +76,7 @@ import org.apache.lucene.util.FixedBitSet;
  * match a very small number of documents, TermsFilter may
  * perform faster.
  *
- * <p/>
- *
+ * <p>
  * Which filter is best is very application dependent.
  */
 
diff --git a/lucene/core/src/java/org/apache/lucene/search/FilteredDocIdSet.java b/lucene/core/src/java/org/apache/lucene/search/FilteredDocIdSet.java
index 1703d3b..8f079f4 100644
--- a/lucene/core/src/java/org/apache/lucene/search/FilteredDocIdSet.java
+++ b/lucene/core/src/java/org/apache/lucene/search/FilteredDocIdSet.java
@@ -29,8 +29,7 @@ import org.apache.lucene.util.RamUsageEstimator;
  * that provides on-demand filtering/validation
  * mechanism on a given DocIdSet.
  *
- * <p/>
- *
+ * <p>
  * Technically, this same functionality could be achieved
  * with ChainedFilter (under queries/), however the
  * benefit of this class is it never materializes the full
diff --git a/lucene/core/src/java/org/apache/lucene/search/NumericRangeQuery.java b/lucene/core/src/java/org/apache/lucene/search/NumericRangeQuery.java
index 6736516..922bfbb 100644
--- a/lucene/core/src/java/org/apache/lucene/search/NumericRangeQuery.java
+++ b/lucene/core/src/java/org/apache/lucene/search/NumericRangeQuery.java
@@ -109,7 +109,7 @@ import org.apache.lucene.index.Term; // for javadocs
  * In practice, we have seen up to 300 terms in most cases (index with 500,000 metadata records
  * and a uniform value distribution).</p>
  *
- * <a name="precisionStepDesc"><h3>Precision Step</h3>
+ * <h3><a name="precisionStepDesc">Precision Step</a></h3>
  * <p>You can choose any <code>precisionStep</code> when encoding values.
  * Lower step values mean more precisions and so more terms in index (and index gets larger). The number
  * of indexed terms per value is (those are generated by {@link NumericTokenStream}):
@@ -123,14 +123,14 @@ import org.apache.lucene.index.Term; // for javadocs
  * of the term dictionary in comparison to one term per value:
  * <p>
  * <!-- the formula in the alt attribute was transformed from latex to PNG with http://1.618034.com/latex.php (with 110 dpi): -->
- * &nbsp;&nbsp;<img src="doc-files/nrq-formula-1.png" alt="\mathrm{termDictOverhead} = \sum\limits_{i=0}^{\mathrm{indexedTermsPerValue}-1} \frac{1}{2^{\mathrm{precisionStep}\cdot i}}" />
+ * &nbsp;&nbsp;<img src="doc-files/nrq-formula-1.png" alt="\mathrm{termDictOverhead} = \sum\limits_{i=0}^{\mathrm{indexedTermsPerValue}-1} \frac{1}{2^{\mathrm{precisionStep}\cdot i}}">
  * </p>
  * <p>On the other hand, if the <code>precisionStep</code> is smaller, the maximum number of terms to match reduces,
  * which optimizes query speed. The formula to calculate the maximum number of terms that will be visited while
  * executing the query is:
  * <p>
  * <!-- the formula in the alt attribute was transformed from latex to PNG with http://1.618034.com/latex.php (with 110 dpi): -->
- * &nbsp;&nbsp;<img src="doc-files/nrq-formula-2.png" alt="\mathrm{maxQueryTerms} = \left[ \left( \mathrm{indexedTermsPerValue} - 1 \right) \cdot \left(2^\mathrm{precisionStep} - 1 \right) \cdot 2 \right] + \left( 2^\mathrm{precisionStep} - 1 \right)" />
+ * &nbsp;&nbsp;<img src="doc-files/nrq-formula-2.png" alt="\mathrm{maxQueryTerms} = \left[ \left( \mathrm{indexedTermsPerValue} - 1 \right) \cdot \left(2^\mathrm{precisionStep} - 1 \right) \cdot 2 \right] + \left( 2^\mathrm{precisionStep} - 1 \right)">
  * </p>
  * <p>For longs stored using a precision step of 4, <code>maxQueryTerms = 15*15*2 + 15 = 465</code>, and for a precision
  * step of 2, <code>maxQueryTerms = 31*3*2 + 3 = 189</code>. But the faster search speed is reduced by more seeking
diff --git a/lucene/core/src/java/org/apache/lucene/search/Sort.java b/lucene/core/src/java/org/apache/lucene/search/Sort.java
index eee5a6e..4340cec 100644
--- a/lucene/core/src/java/org/apache/lucene/search/Sort.java
+++ b/lucene/core/src/java/org/apache/lucene/search/Sort.java
@@ -34,7 +34,7 @@ import java.util.Arrays;
  * <p><code>document.add (new Field ("byNumber", Integer.toString(x), Field.Store.NO, Field.Index.NOT_ANALYZED));</code></p>
  * 
  *
- * <p><h3>Valid Types of Values</h3>
+ * <h3>Valid Types of Values</h3>
  *
  * <p>There are four possible kinds of term values which may be put into
  * sorting fields: Integers, Longs, Floats, or Strings.  Unless
@@ -67,14 +67,14 @@ import java.util.Arrays;
  * of term value has higher memory requirements than the other
  * two types.
  *
- * <p><h3>Object Reuse</h3>
+ * <h3>Object Reuse</h3>
  *
  * <p>One of these objects can be
  * used multiple times and the sort order changed between usages.
  *
  * <p>This class is thread safe.
  *
- * <p><h3>Memory Usage</h3>
+ * <h3>Memory Usage</h3>
  *
  * <p>Sorting uses of caches of term values maintained by the
  * internal HitQueue(s).  The cache is static and contains an integer
diff --git a/lucene/core/src/java/org/apache/lucene/search/SortedNumericSortField.java b/lucene/core/src/java/org/apache/lucene/search/SortedNumericSortField.java
index eca8ab6..9531706 100644
--- a/lucene/core/src/java/org/apache/lucene/search/SortedNumericSortField.java
+++ b/lucene/core/src/java/org/apache/lucene/search/SortedNumericSortField.java
@@ -35,7 +35,6 @@ import org.apache.lucene.index.SortedNumericDocValues;
  * <p>
  * Like sorting by string, this also supports sorting missing values as first or last,
  * via {@link #setMissingValue(Object)}.
- * <p>
  * @see SortedNumericSelector
  */
 public class SortedNumericSortField extends SortField {
diff --git a/lucene/core/src/java/org/apache/lucene/search/SortedSetSortField.java b/lucene/core/src/java/org/apache/lucene/search/SortedSetSortField.java
index 402d99e..d204819 100644
--- a/lucene/core/src/java/org/apache/lucene/search/SortedSetSortField.java
+++ b/lucene/core/src/java/org/apache/lucene/search/SortedSetSortField.java
@@ -36,7 +36,6 @@ import org.apache.lucene.index.SortedSetDocValues;
  * <p>
  * Like sorting by string, this also supports sorting missing values as first or last,
  * via {@link #setMissingValue(Object)}.
- * <p>
  * @see SortedSetSelector
  */
 public class SortedSetSortField extends SortField {
diff --git a/lucene/core/src/java/org/apache/lucene/search/TermRangeTermsEnum.java b/lucene/core/src/java/org/apache/lucene/search/TermRangeTermsEnum.java
index 184413f..e76fe62 100644
--- a/lucene/core/src/java/org/apache/lucene/search/TermRangeTermsEnum.java
+++ b/lucene/core/src/java/org/apache/lucene/search/TermRangeTermsEnum.java
@@ -24,7 +24,7 @@ import org.apache.lucene.util.BytesRef;
 /**
  * Subclass of FilteredTermEnum for enumerating all terms that match the
  * specified range parameters.  Each term in the enumeration is
- * greater than all that precede it.</p>
+ * greater than all that precede it.
  */
 public class TermRangeTermsEnum extends FilteredTermsEnum {
 
diff --git a/lucene/core/src/java/org/apache/lucene/search/TimeLimitingCollector.java b/lucene/core/src/java/org/apache/lucene/search/TimeLimitingCollector.java
index 2d4e026..2aa67d1 100644
--- a/lucene/core/src/java/org/apache/lucene/search/TimeLimitingCollector.java
+++ b/lucene/core/src/java/org/apache/lucene/search/TimeLimitingCollector.java
@@ -94,7 +94,6 @@ public class TimeLimitingCollector implements Collector {
    *   collector.setBaseline(baseline);
    *   indexSearcher.search(query, collector);
    * </pre>
-   * </p>
    * @see #setBaseline() 
    */
   public void setBaseline(long clockTime) {
diff --git a/lucene/core/src/java/org/apache/lucene/search/TopFieldCollector.java b/lucene/core/src/java/org/apache/lucene/search/TopFieldCollector.java
index f153c79..3bf860a 100644
--- a/lucene/core/src/java/org/apache/lucene/search/TopFieldCollector.java
+++ b/lucene/core/src/java/org/apache/lucene/search/TopFieldCollector.java
@@ -26,7 +26,7 @@ import org.apache.lucene.util.PriorityQueue;
 /**
  * A {@link Collector} that sorts by {@link SortField} using
  * {@link FieldComparator}s.
- * <p/>
+ * <p>
  * See the {@link #create(org.apache.lucene.search.Sort, int, boolean, boolean, boolean)} method
  * for instantiating a TopFieldCollector.
  *
diff --git a/lucene/core/src/java/org/apache/lucene/search/package-info.java b/lucene/core/src/java/org/apache/lucene/search/package-info.java
index dba7d0b..eba4bdd 100644
--- a/lucene/core/src/java/org/apache/lucene/search/package-info.java
+++ b/lucene/core/src/java/org/apache/lucene/search/package-info.java
@@ -19,7 +19,6 @@
  * Code to search indices.
  * 
  * <h2>Table Of Contents</h2>
- * <p>
  *     <ol>
  *         <li><a href="#search">Search Basics</a></li>
  *         <li><a href="#query">The Query Classes</a></li>
@@ -28,7 +27,6 @@
  *         <li><a href="#changingScoring">Changing the Scoring</a></li>
  *         <li><a href="#algorithm">Appendix: Search Algorithm</a></li>
  *     </ol>
- * </p>
  * 
  * 
  * <a name="search"></a>
@@ -40,21 +38,17 @@
  * variety of ways to provide complex querying capabilities along with information about where matches took place in the document 
  * collection. The <a href="#query">Query Classes</a> section below highlights some of the more important Query classes. For details 
  * on implementing your own Query class, see <a href="#customQueriesExpert">Custom Queries -- Expert Level</a> below.
- * </p>
  * <p>
  * To perform a search, applications usually call {@link
  * org.apache.lucene.search.IndexSearcher#search(Query,int)} or {@link
  * org.apache.lucene.search.IndexSearcher#search(Query,Filter,int)}.
- * </p>
  * <p>
  * Once a Query has been created and submitted to the {@link org.apache.lucene.search.IndexSearcher IndexSearcher}, the scoring
  * process begins. After some infrastructure setup, control finally passes to the {@link org.apache.lucene.search.Weight Weight}
  * implementation and its {@link org.apache.lucene.search.Scorer Scorer} or {@link org.apache.lucene.search.BulkScorer BulkScore}
  * instances. See the <a href="#algorithm">Algorithm</a> section for more notes on the process.
- * </p>
  *     <!-- FILL IN MORE HERE -->   
  *     <!-- TODO: this page over-links the same things too many times -->
- * </p>
  * 
  * 
  * <a name="query"></a>
@@ -83,7 +77,6 @@
  *         {@link org.apache.lucene.document.Document Document}s that have the 
  *         {@link org.apache.lucene.document.Field Field} named <tt>"fieldName"</tt>
  *     containing the word <tt>"term"</tt>.
- * </p>
  * <h3>
  *     {@link org.apache.lucene.search.BooleanQuery BooleanQuery}
  * </h3>
@@ -123,7 +116,6 @@
  *     The default setting for the maximum number
  *     of clauses 1024, but this can be changed via the
  *     static method {@link org.apache.lucene.search.BooleanQuery#setMaxClauseCount(int)}.
- * </p>
  * 
  * <h3>Phrases</h3>
  * 
@@ -156,7 +148,6 @@
  *                 instances.</p>
  *         </li>
  *     </ol>
- * </p>
  * 
  * <h3>
  *     {@link org.apache.lucene.search.TermRangeQuery TermRangeQuery}
@@ -174,7 +165,6 @@
  * 
  *     For example, one could find all documents
  *     that have terms beginning with the letters <tt>a</tt> through <tt>c</tt>.
- * </p>
  * 
  * <h3>
  *     {@link org.apache.lucene.search.NumericRangeQuery NumericRangeQuery}
@@ -187,7 +177,6 @@
  *     using a one of the numeric fields ({@link org.apache.lucene.document.IntField IntField},
  *     {@link org.apache.lucene.document.LongField LongField}, {@link org.apache.lucene.document.FloatField FloatField},
  *     or {@link org.apache.lucene.document.DoubleField DoubleField}).
- * </p>
  * 
  * <h3>
  *     {@link org.apache.lucene.search.PrefixQuery PrefixQuery},
@@ -211,7 +200,6 @@
  *     to remove that protection.
  *     The {@link org.apache.lucene.search.RegexpQuery RegexpQuery} is even more general than WildcardQuery,
  *     allowing an application to identify all documents with terms that match a regular expression pattern.
- * </p>
  * <h3>
  *     {@link org.apache.lucene.search.FuzzyQuery FuzzyQuery}
  * </h3>
@@ -222,7 +210,6 @@
  *     determined using
  *     <a href="http://en.wikipedia.org/wiki/Levenshtein">Levenshtein (edit) distance</a>.
  *     This type of query can be useful when accounting for spelling variations in the collection.
- * </p>
  * 
  * 
  * <a name="scoring"></a>
@@ -234,10 +221,8 @@
  *    <a href="mailto:java-user@lucene.apache.org">java-user@lucene.apache.org</a> to figure out 
  *    why a document with five of our query terms scores lower than a different document with 
  *    only one of the query terms. 
- * </p>
  * <p>While this document won't answer your specific scoring issues, it will, hopefully, point you 
  *   to the places that can help you figure out the <i>what</i> and <i>why</i> of Lucene scoring.
- * </p>
  * <p>Lucene scoring supports a number of pluggable information retrieval 
  *    <a href="http://en.wikipedia.org/wiki/Information_retrieval#Model_types">models</a>, including:
  *    <ul>
@@ -252,14 +237,12 @@
  *    that need to be scored based on boolean logic in the Query specification, and then ranks this subset of
  *    matching documents via the retrieval model. For some valuable references on VSM and IR in general refer to
  *    <a href="http://wiki.apache.org/lucene-java/InformationRetrieval">Lucene Wiki IR references</a>.
- * </p>
  * <p>The rest of this document will cover <a href="#scoringBasics">Scoring basics</a> and explain how to 
  *    change your {@link org.apache.lucene.search.similarities.Similarity Similarity}. Next, it will cover
  *    ways you can customize the lucene internals in 
  *    <a href="#customQueriesExpert">Custom Queries -- Expert Level</a>, which gives details on 
  *    implementing your own {@link org.apache.lucene.search.Query Query} class and related functionality.
  *    Finally, we will finish up with some reference material in the <a href="#algorithm">Appendix</a>.
- * </p>
  * 
  * 
  * <a name="scoringBasics"></a>
@@ -286,7 +269,6 @@
  *    important because two Documents with the exact same content, but one having the content in two
  *    Fields and the other in one Field may return different scores for the same query due to length
  *    normalization.
- * </p>
  * <h3>Score Boosting</h3>
  * <p>Lucene allows influencing search results by "boosting" at different times:
  *    <ul>                   
@@ -296,7 +278,6 @@
  *       <li><b>Query-time boost</b> by setting a boost on a query clause, calling
  *        {@link org.apache.lucene.search.Query#setBoost(float) Query.setBoost()}.</li>
  *    </ul>    
- * </p>
  * <p>Indexing time boosts are pre-processed for storage efficiency and written to
  *    storage for a field as follows:
  *    <ul>
@@ -310,8 +291,6 @@
  *        <li>Decoding of any index-time normalization values and integration into the document's score is also performed 
  *            at search time by the Similarity.</li>
  *     </ul>
- * </p>
- * 
  * 
  * <a name="changingScoring"></a>
  * <h2>Changing Scoring &mdash; Similarity</h2>
@@ -324,22 +303,18 @@
  *  IndexSearcher.setSimilarity(Similarity)}.  Be sure to use the same
  * Similarity at query-time as at index-time (so that norms are
  * encoded/decoded correctly); Lucene makes no effort to verify this.
- * </p>
  * <p>
  * You can influence scoring by configuring a different built-in Similarity implementation, or by tweaking its
  * parameters, subclassing it to override behavior. Some implementations also offer a modular API which you can
  * extend by plugging in a different component (e.g. term frequency normalizer).
- * </p>
  * <p>
  * Finally, you can extend the low level {@link org.apache.lucene.search.similarities.Similarity Similarity} directly
  * to implement a new retrieval model, or to use external scoring factors particular to your application. For example,
  * a custom Similarity can access per-document values via {@link org.apache.lucene.index.NumericDocValues} and 
  * integrate them into the score.
- * </p>
  * <p>
  * See the {@link org.apache.lucene.search.similarities} package documentation for information
  * on the built-in available scoring models and extending or changing Similarity.
- * </p>
  * 
  * 
  * <a name="customQueriesExpert"></a>
@@ -347,7 +322,6 @@
  * 
  * <p>Custom queries are an expert level task, so tread carefully and be prepared to share your code if
  *     you want help.
- * </p>
  * 
  * <p>With the warning out of the way, it is possible to change a lot more than just the Similarity
  *     when it comes to matching and scoring in Lucene. Lucene's search is a complex mechanism that is grounded by
@@ -374,7 +348,6 @@
  *       implementations.</li>
  *     </ol>
  *     Details on each of these classes, and their children, can be found in the subsections below.
- * </p>
  * <h3>The Query Class</h3>
  *     <p>In some sense, the
  *         {@link org.apache.lucene.search.Query Query}
@@ -396,7 +369,6 @@
  *                 {@link org.apache.lucene.search.BooleanQuery BooleanQuery}, <span
  *                     >and other queries that implement {@link org.apache.lucene.search.Query#createWeight(IndexSearcher,boolean) createWeight(IndexSearcher searcher,boolean)}</span></li>
  *         </ol>
- *     </p>
  * <a name="weightClass"></a>
  * <h3>The Weight Interface</h3>
  *     <p>The
@@ -449,10 +421,8 @@
  *                 Typically a weight such as TermWeight
  *                 that scores via a {@link org.apache.lucene.search.similarities.Similarity Similarity} will make use of the Similarity's implementation:
  *                 {@link org.apache.lucene.search.similarities.Similarity.SimScorer#explain(int, Explanation) SimScorer#explain(int doc, Explanation freq)}.
- *                 </li>
- *              </li>
+ *             </li>
  *         </ol>
- *     </p>
  * <a name="scorerClass"></a>
  * <h3>The Scorer Class</h3>
  *     <p>The
@@ -494,7 +464,6 @@
  *                 details on the scoring process.
  *             </li>
  *         </ol>
- *     </p>
  * <a name="bulkScorerClass"></a>
  * <h3>The BulkScorer Class</h3>
  *     <p>The
@@ -506,14 +475,13 @@
  *     Score all documents up to but not including the specified max document.
  *       </li>
  *         </ol>
- *     </p>
  * <h3>Why would I want to add my own Query?</h3>
  * 
  *     <p>In a nutshell, you want to add your own custom Query implementation when you think that Lucene's
  *         aren't appropriate for the
  *         task that you want to do. You might be doing some cutting edge research or you need more information
  *         back
- *         out of Lucene (similar to Doug adding SpanQuery functionality).</p>
+ *         out of Lucene (similar to Doug adding SpanQuery functionality).
  * 
  * <!-- TODO: integrate this better, it's better served as an intro than an appendix -->
  * 
@@ -521,10 +489,10 @@
  * <a name="algorithm"></a>
  * <h2>Appendix: Search Algorithm</h2>
  * <p>This section is mostly notes on stepping through the Scoring process and serves as
- *    fertilizer for the earlier sections.</p>
+ *    fertilizer for the earlier sections.
  * <p>In the typical search application, a {@link org.apache.lucene.search.Query Query}
  *    is passed to the {@link org.apache.lucene.search.IndexSearcher IndexSearcher},
- *    beginning the scoring process.</p>
+ *    beginning the scoring process.
  * <p>Once inside the IndexSearcher, a {@link org.apache.lucene.search.Collector Collector}
  *    is used for the scoring and sorting of the search results.
  *    These important objects are involved in a search:
@@ -538,7 +506,6 @@
  *       <li>A {@link org.apache.lucene.search.Sort Sort} object for specifying how to sort
  *           the results if the standard score-based sort method is not desired.</li>                   
  *   </ol>       
- * </p>
  * <p>Assuming we are not sorting (since sorting doesn't affect the raw Lucene score),
  *    we call one of the search methods of the IndexSearcher, passing in the
  *    {@link org.apache.lucene.search.Weight Weight} object created by
@@ -553,12 +520,10 @@
  *    see {@link org.apache.lucene.search.IndexSearcher IndexSearcher}). The TopScoreDocCollector
  *    uses a {@link org.apache.lucene.util.PriorityQueue PriorityQueue} to collect the
  *    top results for the search.
- * </p> 
  * <p>If a Filter is being used, some initial setup is done to determine which docs to include. 
  *    Otherwise, we ask the Weight for a {@link org.apache.lucene.search.Scorer Scorer} for each
  *    {@link org.apache.lucene.index.IndexReader IndexReader} segment and proceed by calling
  *    {@link org.apache.lucene.search.BulkScorer#score(org.apache.lucene.search.LeafCollector) BulkScorer.score(LeafCollector)}.
- * </p>
  * <p>At last, we are actually going to score some documents. The score method takes in the Collector
  *    (most likely the TopScoreDocCollector or TopFieldCollector) and does its business.Of course, here 
  *    is where things get involved. The {@link org.apache.lucene.search.Scorer Scorer} that is returned
@@ -567,13 +532,12 @@
  *    {@link org.apache.lucene.search.Scorer Scorer} is going to be a <code>BooleanScorer2</code> created
  *    from {@link org.apache.lucene.search.BooleanWeight BooleanWeight} (see the section on
  *    <a href="#customQueriesExpert">custom queries</a> for info on changing this).
- * </p>
  * <p>Assuming a BooleanScorer2, we first initialize the Coordinator, which is used to apply the coord() 
  *   factor. We then get a internal Scorer based on the required, optional and prohibited parts of the query.
  *   Using this internal Scorer, the BooleanScorer2 then proceeds into a while loop based on the 
  *   {@link org.apache.lucene.search.Scorer#nextDoc Scorer.nextDoc()} method. The nextDoc() method advances 
  *   to the next document matching the query. This is an abstract method in the Scorer class and is thus 
  *   overridden by all derived  implementations. If you have a simple OR query your internal Scorer is most 
- *   likely a DisjunctionSumScorer, which essentially combines the scorers from the sub scorers of the OR'd terms.</p>
+ *   likely a DisjunctionSumScorer, which essentially combines the scorers from the sub scorers of the OR'd terms.
  */
 package org.apache.lucene.search;
diff --git a/lucene/core/src/java/org/apache/lucene/search/payloads/AveragePayloadFunction.java b/lucene/core/src/java/org/apache/lucene/search/payloads/AveragePayloadFunction.java
index 808c3c2..79cded7 100644
--- a/lucene/core/src/java/org/apache/lucene/search/payloads/AveragePayloadFunction.java
+++ b/lucene/core/src/java/org/apache/lucene/search/payloads/AveragePayloadFunction.java
@@ -20,7 +20,7 @@ package org.apache.lucene.search.payloads;
 
 /**
  * Calculate the final score as the average score of all payloads seen.
- * <p/>
+ * <p>
  * Is thread safe and completely reusable. 
  *
  **/
diff --git a/lucene/core/src/java/org/apache/lucene/search/payloads/MaxPayloadFunction.java b/lucene/core/src/java/org/apache/lucene/search/payloads/MaxPayloadFunction.java
index b868e3b..a13b18a 100644
--- a/lucene/core/src/java/org/apache/lucene/search/payloads/MaxPayloadFunction.java
+++ b/lucene/core/src/java/org/apache/lucene/search/payloads/MaxPayloadFunction.java
@@ -20,7 +20,7 @@ package org.apache.lucene.search.payloads;
 
 /**
  * Returns the maximum payload score seen, else 1 if there are no payloads on the doc.
- * <p/>
+ * <p>
  * Is thread safe and completely reusable.
  *
  **/
diff --git a/lucene/core/src/java/org/apache/lucene/search/payloads/PayloadNearQuery.java b/lucene/core/src/java/org/apache/lucene/search/payloads/PayloadNearQuery.java
index 924e6b9..e46bb45 100644
--- a/lucene/core/src/java/org/apache/lucene/search/payloads/PayloadNearQuery.java
+++ b/lucene/core/src/java/org/apache/lucene/search/payloads/PayloadNearQuery.java
@@ -46,11 +46,11 @@ import org.apache.lucene.util.ToStringUtils;
  * {@link org.apache.lucene.search.spans.SpanNearQuery} except that it factors
  * in the value of the payloads located at each of the positions where the
  * {@link org.apache.lucene.search.spans.TermSpans} occurs.
- * <p/>
+ * <p>
  * NOTE: In order to take advantage of this with the default scoring implementation
  * ({@link DefaultSimilarity}), you must override {@link DefaultSimilarity#scorePayload(int, int, int, BytesRef)},
  * which returns 1 by default.
- * <p/>
+ * <p>
  * Payload scores are aggregated using a pluggable {@link PayloadFunction}.
  * 
  * @see org.apache.lucene.search.similarities.Similarity.SimScorer#computePayloadFactor(int, int, int, BytesRef)
diff --git a/lucene/core/src/java/org/apache/lucene/search/payloads/PayloadTermQuery.java b/lucene/core/src/java/org/apache/lucene/search/payloads/PayloadTermQuery.java
index a9a2655..463a6a0 100644
--- a/lucene/core/src/java/org/apache/lucene/search/payloads/PayloadTermQuery.java
+++ b/lucene/core/src/java/org/apache/lucene/search/payloads/PayloadTermQuery.java
@@ -43,11 +43,11 @@ import org.apache.lucene.util.BytesRef;
  * {@link org.apache.lucene.search.spans.SpanTermQuery} except that it factors
  * in the value of the payload located at each of the positions where the
  * {@link org.apache.lucene.index.Term} occurs.
- * <p/>
+ * <p>
  * NOTE: In order to take advantage of this with the default scoring implementation
  * ({@link DefaultSimilarity}), you must override {@link DefaultSimilarity#scorePayload(int, int, int, BytesRef)},
  * which returns 1 by default.
- * <p/>
+ * <p>
  * Payload scores are aggregated using a pluggable {@link PayloadFunction}.
  * @see org.apache.lucene.search.similarities.Similarity.SimScorer#computePayloadFactor(int, int, int, BytesRef)
  **/
@@ -151,7 +151,7 @@ public class PayloadTermQuery extends SpanTermQuery {
 
       /**
        * Returns the SpanScorer score only.
-       * <p/>
+       * <p>
        * Should not be overridden without good cause!
        * 
        * @return the score for just the Span part w/o the payload
diff --git a/lucene/core/src/java/org/apache/lucene/search/payloads/package-info.java b/lucene/core/src/java/org/apache/lucene/search/payloads/package-info.java
index da40d88..51bd982 100644
--- a/lucene/core/src/java/org/apache/lucene/search/payloads/package-info.java
+++ b/lucene/core/src/java/org/apache/lucene/search/payloads/package-info.java
@@ -24,6 +24,5 @@
  *    <li>{@link org.apache.lucene.search.payloads.PayloadNearQuery PayloadNearQuery} -- A {@link org.apache.lucene.search.spans.SpanNearQuery SpanNearQuery} that factors in the value of the payloads located 
  *        at each of the positions where the spans occur.</li>
  *   </ol>
- * </p>
  */
 package org.apache.lucene.search.payloads;
diff --git a/lucene/core/src/java/org/apache/lucene/search/similarities/IBSimilarity.java b/lucene/core/src/java/org/apache/lucene/search/similarities/IBSimilarity.java
index f09549f..fb7eae3 100644
--- a/lucene/core/src/java/org/apache/lucene/search/similarities/IBSimilarity.java
+++ b/lucene/core/src/java/org/apache/lucene/search/similarities/IBSimilarity.java
@@ -36,7 +36,6 @@ import org.apache.lucene.search.similarities.Normalization.NoNormalization;
  *   <li><em>t<sup>d</sup><sub>w</sub></em> is the normalized term frequency;</li>
  *   <li><em>&lambda;<sub>w</sub></em> is a parameter.</li>
  * </ul>
- * </p>
  * <p>The framework described in the paper has many similarities to the DFR
  * framework (see {@link DFRSimilarity}). It is possible that the two
  * Similarities will be merged at one point.</p>
@@ -64,7 +63,6 @@ import org.apache.lucene.search.similarities.Normalization.NoNormalization;
  *                      {@link DFRSimilarity})</blockquote>
  *     </li>
  * </ol>
- * <p>
  * @see DFRSimilarity
  * @lucene.experimental 
  */
diff --git a/lucene/core/src/java/org/apache/lucene/search/similarities/Similarity.java b/lucene/core/src/java/org/apache/lucene/search/similarities/Similarity.java
index beb7b90..65f0529 100644
--- a/lucene/core/src/java/org/apache/lucene/search/similarities/Similarity.java
+++ b/lucene/core/src/java/org/apache/lucene/search/similarities/Similarity.java
@@ -47,7 +47,7 @@ import java.io.IOException;
  * this class at both <a href="#indextime">index-time</a> and 
  * <a href="#querytime">query-time</a>.
  * <p>
- * <a name="indextime"/>
+ * <a name="indextime">Indexing Time</a>
  * At indexing time, the indexer calls {@link #computeNorm(FieldInvertState)}, allowing
  * the Similarity implementation to set a per-document value for the field that will 
  * be later accessible via {@link org.apache.lucene.index.LeafReader#getNormValues(String)}.  Lucene makes no assumption
@@ -74,7 +74,7 @@ import java.io.IOException;
  * boost parameter <i>C</i>, and {@link PerFieldSimilarityWrapper} can return different 
  * instances with different boosts depending upon field name.
  * <p>
- * <a name="querytime"/>
+ * <a name="querytime">Query time</a>
  * At query-time, Queries interact with the Similarity via these steps:
  * <ol>
  *   <li>The {@link #computeWeight(float, CollectionStatistics, TermStatistics...)} method is called a single time,
@@ -91,7 +91,7 @@ import java.io.IOException;
  *       The score() method is called for each matching document.
  * </ol>
  * <p>
- * <a name="explaintime"/>
+ * <a name="explaintime">Explanations</a>
  * When {@link IndexSearcher#explain(org.apache.lucene.search.Query, int)} is called, queries consult the Similarity's DocScorer for an 
  * explanation of how it computed its score. The query passes in a the document id and an explanation of how the frequency
  * was computed.
diff --git a/lucene/core/src/java/org/apache/lucene/search/similarities/TFIDFSimilarity.java b/lucene/core/src/java/org/apache/lucene/search/similarities/TFIDFSimilarity.java
index 90ea47d..63cd8be 100644
--- a/lucene/core/src/java/org/apache/lucene/search/similarities/TFIDFSimilarity.java
+++ b/lucene/core/src/java/org/apache/lucene/search/similarities/TFIDFSimilarity.java
@@ -256,7 +256,6 @@ import org.apache.lucene.util.BytesRef;
  * The color codes demonstrate how it relates
  * to those of the <i>conceptual</i> formula:
  *
- * <P>
  * <table cellpadding="2" cellspacing="2" border="0" style="width:auto; margin-left:auto; margin-right:auto" summary="formatting only">
  *  <tr><td>
  *  <table cellpadding="" cellspacing="2" border="2" style="margin-left:auto; margin-right:auto" summary="formatting only">
@@ -280,7 +279,7 @@ import org.apache.lucene.util.BytesRef;
  *       <big><big>)</big></big>
  *     </td>
  *   </tr>
- *   <tr valigh="top">
+ *   <tr valign="top">
  *    <td></td>
  *    <td align="center" style="text-align: center"><small>t in q</small></td>
  *    <td></td>
@@ -426,7 +425,7 @@ import org.apache.lucene.util.BytesRef;
  *            <big><big>) <sup>2</sup> </big></big>
  *          </td>
  *        </tr>
- *        <tr valigh="top">
+ *        <tr valign="top">
  *          <td></td>
  *          <td align="center" style="text-align: center"><small>t in q</small></td>
  *          <td></td>
@@ -489,7 +488,7 @@ import org.apache.lucene.util.BytesRef;
  *            {@link org.apache.lucene.index.IndexableField#boost() f.boost}()
  *          </td>
  *        </tr>
- *        <tr valigh="top">
+ *        <tr valign="top">
  *          <td></td>
  *          <td align="center" style="text-align: center"><small>field <i><b>f</b></i> in <i>d</i> named as <i><b>t</b></i></small></td>
  *          <td></td>
diff --git a/lucene/core/src/java/org/apache/lucene/search/similarities/package-info.java b/lucene/core/src/java/org/apache/lucene/search/similarities/package-info.java
index 6e7d99e..086c82a 100644
--- a/lucene/core/src/java/org/apache/lucene/search/similarities/package-info.java
+++ b/lucene/core/src/java/org/apache/lucene/search/similarities/package-info.java
@@ -23,12 +23,10 @@
  * package.
  * 
  * <h2>Table Of Contents</h2>
- * <p>
  *     <ol>
  *         <li><a href="#sims">Summary of the Ranking Methods</a></li>
  *         <li><a href="#changingSimilarity">Changing the Similarity</a></li>
  *     </ol>
- * </p>
  * 
  * 
  * <a name="sims"></a>
@@ -37,10 +35,10 @@
  * <p>{@link org.apache.lucene.search.similarities.DefaultSimilarity} is the original Lucene
  * scoring function. It is based on a highly optimized 
  * <a href="http://en.wikipedia.org/wiki/Vector_Space_Model">Vector Space Model</a>. For more
- * information, see {@link org.apache.lucene.search.similarities.TFIDFSimilarity}.</p>
+ * information, see {@link org.apache.lucene.search.similarities.TFIDFSimilarity}.
  * 
  * <p>{@link org.apache.lucene.search.similarities.BM25Similarity} is an optimized
- * implementation of the successful Okapi BM25 model.</p>
+ * implementation of the successful Okapi BM25 model.
  * 
  * <p>{@link org.apache.lucene.search.similarities.SimilarityBase} provides a basic
  * implementation of the Similarity contract and exposes a highly simplified
@@ -63,7 +61,7 @@
  * {@link org.apache.lucene.search.similarities.BM25Similarity}, a difference in
  * performance is to be expected when using the methods listed above. However,
  * optimizations can always be implemented in subclasses; see
- * <a href="#changingSimilarity">below</a>.</p>
+ * <a href="#changingSimilarity">below</a>.
  * 
  * <a name="changingSimilarity"></a>
  * <h2>Changing Similarity</h2>
@@ -74,13 +72,12 @@
  *         href="Similarity.html">Similarity</a> implementation. For instance, some
  *     applications do not need to
  *     distinguish between shorter and longer documents (see <a
- *         href="http://www.gossamer-threads.com/lists/lucene/java-user/38967#38967">a "fair" similarity</a>).</p>
+ *         href="http://www.gossamer-threads.com/lists/lucene/java-user/38967#38967">a "fair" similarity</a>).
  * 
  * <p>To change {@link org.apache.lucene.search.similarities.Similarity}, one must do so for both indexing and
  *     searching, and the changes must happen before
  *     either of these actions take place. Although in theory there is nothing stopping you from changing mid-stream, it
  *     just isn't well-defined what is going to happen.
- * </p>
  * 
  * <p>To make this change, implement your own {@link org.apache.lucene.search.similarities.Similarity} (likely
  *     you'll want to simply subclass an existing method, be it
@@ -91,7 +88,6 @@
  *     before indexing and
  *     {@link org.apache.lucene.search.IndexSearcher#setSimilarity(Similarity)}
  *     before searching.
- * </p>
  * 
  * <h3>Extending {@linkplain org.apache.lucene.search.similarities.SimilarityBase}</h3>
  * <p>
@@ -100,7 +96,7 @@
  * basic implementations for the low level . Subclasses are only required to
  * implement the {@link org.apache.lucene.search.similarities.SimilarityBase#score(BasicStats, float, float)}
  * and {@link org.apache.lucene.search.similarities.SimilarityBase#toString()}
- * methods.</p>
+ * methods.
  * 
  * <p>Another option is to extend one of the <a href="#framework">frameworks</a>
  * based on {@link org.apache.lucene.search.similarities.SimilarityBase}. These
@@ -111,7 +107,7 @@
  * {@link org.apache.lucene.search.similarities.AfterEffect} and
  * {@link org.apache.lucene.search.similarities.Normalization}. Instead of
  * subclassing the Similarity, one can simply introduce a new basic model and tell
- * {@link org.apache.lucene.search.similarities.DFRSimilarity} to use it.</p>
+ * {@link org.apache.lucene.search.similarities.DFRSimilarity} to use it.
  * 
  * <h3>Changing {@linkplain org.apache.lucene.search.similarities.DefaultSimilarity}</h3>
  * <p>
@@ -123,17 +119,17 @@
  *             <code>org.apache.lucene.misc</code> gives small
  *             increases as the frequency increases a small amount
  *             and then greater increases when you hit the "sweet spot", i.e. where
- *             you think the frequency of terms is more significant.</p></li>
+ *             you think the frequency of terms is more significant.</li>
  *         <li><p>Overriding tf &mdash; In some applications, it doesn't matter what the score of a document is as long as a
  *             matching term occurs. In these
- *             cases people have overridden Similarity to return 1 from the tf() method.</p></li>
+ *             cases people have overridden Similarity to return 1 from the tf() method.</li>
  *         <li><p>Changing Length Normalization &mdash; By overriding
  *             {@link org.apache.lucene.search.similarities.Similarity#computeNorm(org.apache.lucene.index.FieldInvertState state)},
  *             it is possible to discount how the length of a field contributes
  *             to a score. In {@link org.apache.lucene.search.similarities.DefaultSimilarity},
  *             lengthNorm = 1 / (numTerms in field)^0.5, but if one changes this to be
  *             1 / (numTerms in field), all fields will be treated
- *             <a href="http://www.gossamer-threads.com/lists/lucene/java-user/38967#38967">"fairly"</a>.</p></li>
+ *             <a href="http://www.gossamer-threads.com/lists/lucene/java-user/38967#38967">"fairly"</a>.</li>
  *     </ol>
  *     In general, Chris Hostetter sums it up best in saying (from <a
  *         href="http://www.gossamer-threads.com/lists/lucene/java-user/39125#39125">the Lucene users's mailing list</a>):
@@ -141,6 +137,5 @@
  *         that
  *         it's "text" is a situation where it *might* make sense to to override your
  *         Similarity method.</blockquote>
- * </p>
  */
 package org.apache.lucene.search.similarities;
diff --git a/lucene/core/src/java/org/apache/lucene/search/spans/FieldMaskingSpanQuery.java b/lucene/core/src/java/org/apache/lucene/search/spans/FieldMaskingSpanQuery.java
index ed21306..9b740f6 100644
--- a/lucene/core/src/java/org/apache/lucene/search/spans/FieldMaskingSpanQuery.java
+++ b/lucene/core/src/java/org/apache/lucene/search/spans/FieldMaskingSpanQuery.java
@@ -67,12 +67,12 @@ import org.apache.lucene.util.ToStringUtils;
  * </pre>
  * to search for 'studentfirstname:james studentsurname:jones' and find 
  * teacherid 1 without matching teacherid 2 (which has a 'james' in position 0 
- * and 'jones' in position 1). </p>
+ * and 'jones' in position 1).
  * 
  * <p>Note: as {@link #getField()} returns the masked field, scoring will be 
  * done using the Similarity and collection statistics of the field name supplied,
  * but with the term statistics of the real field. This may lead to exceptions,
- * poor performance, and unexpected scoring behaviour.</p>
+ * poor performance, and unexpected scoring behaviour.
  */
 public class FieldMaskingSpanQuery extends SpanQuery {
   private SpanQuery maskedQuery;
diff --git a/lucene/core/src/java/org/apache/lucene/search/spans/SpanFirstQuery.java b/lucene/core/src/java/org/apache/lucene/search/spans/SpanFirstQuery.java
index 62f98a2..7bcaa2c 100644
--- a/lucene/core/src/java/org/apache/lucene/search/spans/SpanFirstQuery.java
+++ b/lucene/core/src/java/org/apache/lucene/search/spans/SpanFirstQuery.java
@@ -21,13 +21,12 @@ import org.apache.lucene.util.ToStringUtils;
 
 import java.io.IOException;
 
-/** Matches spans near the beginning of a field.
- * <p/> 
+/** 
+ * Matches spans near the beginning of a field.
+ * <p> 
  * This class is a simple extension of {@link SpanPositionRangeQuery} in that it assumes the
  * start to be zero and only checks the end boundary.
- *
- *
- *  */
+ */
 public class SpanFirstQuery extends SpanPositionRangeQuery {
 
   /** Construct a SpanFirstQuery matching spans in <code>match</code> whose end
diff --git a/lucene/core/src/java/org/apache/lucene/search/spans/SpanNearPayloadCheckQuery.java b/lucene/core/src/java/org/apache/lucene/search/spans/SpanNearPayloadCheckQuery.java
index ff761e8..aa69146 100644
--- a/lucene/core/src/java/org/apache/lucene/search/spans/SpanNearPayloadCheckQuery.java
+++ b/lucene/core/src/java/org/apache/lucene/search/spans/SpanNearPayloadCheckQuery.java
@@ -26,8 +26,6 @@ import java.util.Collection;
 /**
  * Only return those matches that have a specific payload at
  * the given position.
- * <p/>
- * 
  */
 public class SpanNearPayloadCheckQuery extends SpanPositionCheckQuery {
   protected final Collection<byte[]> payloadToMatch;
diff --git a/lucene/core/src/java/org/apache/lucene/search/spans/SpanPayloadCheckQuery.java b/lucene/core/src/java/org/apache/lucene/search/spans/SpanPayloadCheckQuery.java
index 9abd62e..dda6009 100644
--- a/lucene/core/src/java/org/apache/lucene/search/spans/SpanPayloadCheckQuery.java
+++ b/lucene/core/src/java/org/apache/lucene/search/spans/SpanPayloadCheckQuery.java
@@ -25,14 +25,13 @@ import java.util.Iterator;
 
 
 /**
- *   Only return those matches that have a specific payload at
- *  the given position.
- *<p/>
+ * Only return those matches that have a specific payload at
+ * the given position.
+ * <p>
  * Do not use this with an SpanQuery that contains a {@link org.apache.lucene.search.spans.SpanNearQuery}.  Instead, use
  * {@link SpanNearPayloadCheckQuery} since it properly handles the fact that payloads
  * aren't ordered by {@link org.apache.lucene.search.spans.SpanNearQuery}.
- *
- **/
+ */
 public class SpanPayloadCheckQuery extends SpanPositionCheckQuery{
   protected final Collection<byte[]> payloadToMatch;
 
diff --git a/lucene/core/src/java/org/apache/lucene/search/spans/Spans.java b/lucene/core/src/java/org/apache/lucene/search/spans/Spans.java
index 8bb6ed9..32aff3b 100644
--- a/lucene/core/src/java/org/apache/lucene/search/spans/Spans.java
+++ b/lucene/core/src/java/org/apache/lucene/search/spans/Spans.java
@@ -69,19 +69,19 @@ public abstract class Spans {
    * you do not want ordered SpanNearQuerys to collect payloads, you can
    * disable collection with a constructor option.<br>
    * <br>
-    * Note that the return type is a collection, thus the ordering should not be relied upon.
-    * <br/>
+   * Note that the return type is a collection, thus the ordering should not be relied upon.
+   * <br>
    * @lucene.experimental
    *
    * @return a List of byte arrays containing the data of this payload, otherwise null if isPayloadAvailable is false
    * @throws IOException if there is a low-level I/O error
-    */
+   */
   // TODO: Remove warning after API has been finalized
   public abstract Collection<byte[]> getPayload() throws IOException;
 
   /**
    * Checks if a payload can be loaded at this position.
-   * <p/>
+   * <p>
    * Payloads can only be loaded once per call to
    * {@link #next()}.
    *
diff --git a/lucene/core/src/java/org/apache/lucene/search/spans/package-info.java b/lucene/core/src/java/org/apache/lucene/search/spans/package-info.java
index 58b3265..20f20b0 100644
--- a/lucene/core/src/java/org/apache/lucene/search/spans/package-info.java
+++ b/lucene/core/src/java/org/apache/lucene/search/spans/package-info.java
@@ -53,7 +53,6 @@
  * In all cases, output spans are minimally inclusive.  In other words, a
  * span formed by matching a span in x and y starts at the lesser of the
  * two starts and ends at the greater of the two ends.
- * </p>
  * 
  * <p>For example, a span query which matches "John Kerry" within ten
  * words of "George Bush" within the first 100 words of the document
diff --git a/lucene/core/src/java/org/apache/lucene/store/DataOutput.java b/lucene/core/src/java/org/apache/lucene/store/DataOutput.java
index 755b568..c18d1cf 100644
--- a/lucene/core/src/java/org/apache/lucene/store/DataOutput.java
+++ b/lucene/core/src/java/org/apache/lucene/store/DataOutput.java
@@ -23,7 +23,6 @@ import java.util.Set;
 
 import org.apache.lucene.util.BitUtil;
 import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.UnicodeUtil;
 
 /**
  * Abstract base class for performing write operations of Lucene's low-level
@@ -92,10 +91,6 @@ public abstract class DataOutput {
    * byte, values from 128 to 16,383 may be stored in two bytes, and so on.</p>
    * <p>VByte Encoding Example</p>
    * <table cellspacing="0" cellpadding="2" border="0" summary="variable length encoding examples">
-   * <col width="64*">
-   * <col width="64*">
-   * <col width="64*">
-   * <col width="64*">
    * <tr valign="top">
    *   <th align="left">Value</th>
    *   <th align="left">Byte 1</th>
@@ -104,19 +99,19 @@ public abstract class DataOutput {
    * </tr>
    * <tr valign="bottom">
    *   <td>0</td>
-   *   <td><kbd>00000000</kbd></td>
+   *   <td><code>00000000</code></td>
    *   <td></td>
    *   <td></td>
    * </tr>
    * <tr valign="bottom">
    *   <td>1</td>
-   *   <td><kbd>00000001</kbd></td>
+   *   <td><code>00000001</code></td>
    *   <td></td>
    *   <td></td>
    * </tr>
    * <tr valign="bottom">
    *   <td>2</td>
-   *   <td><kbd>00000010</kbd></td>
+   *   <td><code>00000010</code></td>
    *   <td></td>
    *   <td></td>
    * </tr>
@@ -128,26 +123,26 @@ public abstract class DataOutput {
    * </tr>
    * <tr valign="bottom">
    *   <td>127</td>
-   *   <td><kbd>01111111</kbd></td>
+   *   <td><code>01111111</code></td>
    *   <td></td>
    *   <td></td>
    * </tr>
    * <tr valign="bottom">
    *   <td>128</td>
-   *   <td><kbd>10000000</kbd></td>
-   *   <td><kbd>00000001</kbd></td>
+   *   <td><code>10000000</code></td>
+   *   <td><code>00000001</code></td>
    *   <td></td>
    * </tr>
    * <tr valign="bottom">
    *   <td>129</td>
-   *   <td><kbd>10000001</kbd></td>
-   *   <td><kbd>00000001</kbd></td>
+   *   <td><code>10000001</code></td>
+   *   <td><code>00000001</code></td>
    *   <td></td>
    * </tr>
    * <tr valign="bottom">
    *   <td>130</td>
-   *   <td><kbd>10000010</kbd></td>
-   *   <td><kbd>00000001</kbd></td>
+   *   <td><code>10000010</code></td>
+   *   <td><code>00000001</code></td>
    *   <td></td>
    * </tr>
    * <tr>
@@ -158,21 +153,21 @@ public abstract class DataOutput {
    * </tr>
    * <tr valign="bottom">
    *   <td>16,383</td>
-   *   <td><kbd>11111111</kbd></td>
-   *   <td><kbd>01111111</kbd></td>
+   *   <td><code>11111111</code></td>
+   *   <td><code>01111111</code></td>
    *   <td></td>
    * </tr>
    * <tr valign="bottom">
    *   <td>16,384</td>
-   *   <td><kbd>10000000</kbd></td>
-   *   <td><kbd>10000000</kbd></td>
-   *   <td><kbd>00000001</kbd></td>
+   *   <td><code>10000000</code></td>
+   *   <td><code>10000000</code></td>
+   *   <td><code>00000001</code></td>
    * </tr>
    * <tr valign="bottom">
    *   <td>16,385</td>
-   *   <td><kbd>10000001</kbd></td>
-   *   <td><kbd>10000000</kbd></td>
-   *   <td><kbd>00000001</kbd></td>
+   *   <td><code>10000001</code></td>
+   *   <td><code>10000000</code></td>
+   *   <td><code>00000001</code></td>
    * </tr>
    * <tr>
    *   <td valign="top">...</td>
diff --git a/lucene/core/src/java/org/apache/lucene/store/Directory.java b/lucene/core/src/java/org/apache/lucene/store/Directory.java
index 69323ab..2060e17 100644
--- a/lucene/core/src/java/org/apache/lucene/store/Directory.java
+++ b/lucene/core/src/java/org/apache/lucene/store/Directory.java
@@ -78,8 +78,8 @@ public abstract class Directory implements Closeable {
    * Ensure that any writes to these files are moved to
    * stable storage.  Lucene uses this to properly commit
    * changes to the index, to prevent a machine/OS crash
-   * from corrupting the index.<br/>
-   * <br/>
+   * from corrupting the index.
+   * <br>
    * NOTE: Clients may call this method for same files over
    * and over again, so some impls might optimize for that.
    * For other impls the operation can be a noop, for various
diff --git a/lucene/core/src/java/org/apache/lucene/store/FSDirectory.java b/lucene/core/src/java/org/apache/lucene/store/FSDirectory.java
index d412f50..4b7a96a 100644
--- a/lucene/core/src/java/org/apache/lucene/store/FSDirectory.java
+++ b/lucene/core/src/java/org/apache/lucene/store/FSDirectory.java
@@ -36,7 +36,7 @@ import org.apache.lucene.util.IOUtils;
 /**
  * Base class for Directory implementations that store index
  * files in the file system.  
- * <a name="subclasses"/>
+ * <a name="subclasses"></a>
  * There are currently three core
  * subclasses:
  *
diff --git a/lucene/core/src/java/org/apache/lucene/util/SentinelIntSet.java b/lucene/core/src/java/org/apache/lucene/util/SentinelIntSet.java
index 3be7890..2bb4bde 100644
--- a/lucene/core/src/java/org/apache/lucene/util/SentinelIntSet.java
+++ b/lucene/core/src/java/org/apache/lucene/util/SentinelIntSet.java
@@ -25,7 +25,7 @@ import java.util.Arrays;
  * would make it &gt;= 75% full.  Consider extending and over-riding {@link #hash(int)} if the values might be poor
  * hash keys; Lucene docids should be fine.
  * The internal fields are exposed publicly to enable more efficient use at the expense of better O-O principles.
- * <p/>
+ * <p>
  * To iterate over the integers held in this set, simply use code like this:
  * <pre class="prettyprint">
  * SentinelIntSet set = ...
diff --git a/lucene/core/src/java/org/apache/lucene/util/TimSorter.java b/lucene/core/src/java/org/apache/lucene/util/TimSorter.java
index d8b40be..44267dc 100644
--- a/lucene/core/src/java/org/apache/lucene/util/TimSorter.java
+++ b/lucene/core/src/java/org/apache/lucene/util/TimSorter.java
@@ -26,7 +26,7 @@ import java.util.Arrays;
  * <p>This implementation is especially good at sorting partially-sorted
  * arrays and sorts small arrays with binary sort.
  * <p><b>NOTE</b>:There are a few differences with the original implementation:<ul>
- * <li><a name="maxTempSlots"/>The extra amount of memory to perform merges is
+ * <li><a name="maxTempSlots"></a>The extra amount of memory to perform merges is
  * configurable. This allows small merges to be very fast while large merges
  * will be performed in-place (slightly slower). You can make sure that the
  * fast merge routine will always be used by having <code>maxTempSlots</code>
diff --git a/lucene/core/src/java/org/apache/lucene/util/WeakIdentityMap.java b/lucene/core/src/java/org/apache/lucene/util/WeakIdentityMap.java
index 3ac4297..2784615 100644
--- a/lucene/core/src/java/org/apache/lucene/util/WeakIdentityMap.java
+++ b/lucene/core/src/java/org/apache/lucene/util/WeakIdentityMap.java
@@ -45,7 +45,7 @@ import java.util.concurrent.ConcurrentHashMap;
  * on the values and not-GCed keys. Lucene's implementation also supports {@code null}
  * keys, but those are never weak!
  * 
- * <p><a name="reapInfo" />The map supports two modes of operation:
+ * <p><a name="reapInfo"></a>The map supports two modes of operation:
  * <ul>
  *  <li>{@code reapOnRead = true}: This behaves identical to a {@link java.util.WeakHashMap}
  *  where it also cleans up the reference queue on every read operation ({@link #get(Object)},
diff --git a/lucene/core/src/java/org/apache/lucene/util/automaton/LevenshteinAutomata.java b/lucene/core/src/java/org/apache/lucene/util/automaton/LevenshteinAutomata.java
index 01badf0..7fc3af4 100644
--- a/lucene/core/src/java/org/apache/lucene/util/automaton/LevenshteinAutomata.java
+++ b/lucene/core/src/java/org/apache/lucene/util/automaton/LevenshteinAutomata.java
@@ -28,7 +28,6 @@ import org.apache.lucene.util.UnicodeUtil;
  * <p>
  * Implements the algorithm described in:
  * Schulz and Mihov: Fast String Correction with Levenshtein Automata
- * <p>
  * @lucene.experimental
  */
 public class LevenshteinAutomata {
@@ -125,7 +124,6 @@ public class LevenshteinAutomata {
    * <li>There are no transitions to dead states.
    * <li>They are not minimal (some transitions could be combined).
    * </ul>
-   * </p>
    */
   public Automaton toAutomaton(int n) {
     return toAutomaton(n, "");
@@ -141,7 +139,6 @@ public class LevenshteinAutomata {
    * <li>There are no transitions to dead states.
    * <li>They are not minimal (some transitions could be combined).
    * </ul>
-   * </p>
    */
   public Automaton toAutomaton(int n, String prefix) {
     assert prefix != null;
diff --git a/lucene/core/src/java/org/apache/lucene/util/automaton/RegExp.java b/lucene/core/src/java/org/apache/lucene/util/automaton/RegExp.java
index ebea907..fa630d0 100644
--- a/lucene/core/src/java/org/apache/lucene/util/automaton/RegExp.java
+++ b/lucene/core/src/java/org/apache/lucene/util/automaton/RegExp.java
@@ -40,7 +40,6 @@ import java.util.Set;
  * Regular Expression extension to <code>Automaton</code>.
  * <p>
  * Regular expressions are built from the following abstract syntax:
- * <p>
  * <table border=0 summary="description of regular expression grammar">
  * <tr>
  * <td><i>regexp</i></td>
diff --git a/lucene/core/src/java/org/apache/lucene/util/packed/PackedInts.java b/lucene/core/src/java/org/apache/lucene/util/packed/PackedInts.java
index c8f7891..86a991b 100644
--- a/lucene/core/src/java/org/apache/lucene/util/packed/PackedInts.java
+++ b/lucene/core/src/java/org/apache/lucene/util/packed/PackedInts.java
@@ -224,12 +224,12 @@ public class PackedInts {
    * Try to find the {@link Format} and number of bits per value that would
    * restore from disk the fastest reader whose overhead is less than
    * <code>acceptableOverheadRatio</code>.
-   * </p><p>
+   * <p>
    * The <code>acceptableOverheadRatio</code> parameter makes sense for
    * random-access {@link Reader}s. In case you only plan to perform
    * sequential access on this stream later on, you should probably use
    * {@link PackedInts#COMPACT}.
-   * </p><p>
+   * <p>
    * If you don't know how many values you are going to write, use
    * <code>valueCount = -1</code>.
    */
@@ -867,7 +867,7 @@ public class PackedInts {
    * metadata at the beginning of the stream. This method is useful to restore
    * data from streams which have been created using
    * {@link PackedInts#getWriterNoHeader(DataOutput, Format, int, int, int)}.
-   * </p><p>
+   * <p>
    * The returned reader will have very little memory overhead, but every call
    * to {@link Reader#get(int)} is likely to perform a disk seek.
    *
@@ -896,7 +896,7 @@ public class PackedInts {
    * Construct a direct {@link Reader} from an {@link IndexInput}. This method
    * is useful to restore data from streams which have been created using
    * {@link PackedInts#getWriter(DataOutput, int, int, float)}.
-   * </p><p>
+   * <p>
    * The returned reader will have very little memory overhead, but every call
    * to {@link Reader#get(int)} is likely to perform a disk seek.
    *
@@ -918,7 +918,7 @@ public class PackedInts {
    * Create a packed integer array with the given amount of values initialized
    * to 0. the valueCount and the bitsPerValue cannot be changed after creation.
    * All Mutables known by this factory are kept fully in RAM.
-   * </p><p>
+   * <p>
    * Positive values of <code>acceptableOverheadRatio</code> will trade space
    * for speed by selecting a faster but potentially less memory-efficient
    * implementation. An <code>acceptableOverheadRatio</code> of
@@ -978,12 +978,12 @@ public class PackedInts {
   /**
    * Expert: Create a packed integer array writer for the given output, format,
    * value count, and number of bits per value.
-   * </p><p>
+   * <p>
    * The resulting stream will be long-aligned. This means that depending on
    * the format which is used, up to 63 bits will be wasted. An easy way to
    * make sure that no space is lost is to always use a <code>valueCount</code>
    * that is a multiple of 64.
-   * </p><p>
+   * <p>
    * This method does not write any metadata to the stream, meaning that it is
    * your responsibility to store it somewhere else in order to be able to
    * recover data from the stream later on:
@@ -993,7 +993,7 @@ public class PackedInts {
    *   <li><code>bitsPerValue</code>,</li>
    *   <li>{@link #VERSION_CURRENT}.</li>
    * </ul>
-   * </p><p>
+   * <p>
    * It is possible to start writing values without knowing how many of them you
    * are actually going to write. To do this, just pass <code>-1</code> as
    * <code>valueCount</code>. On the other hand, for any positive value of
@@ -1001,7 +1001,7 @@ public class PackedInts {
    * write more values than expected and pad the end of stream with zeros in
    * case you have written less than <code>valueCount</code> when calling
    * {@link Writer#finish()}.
-   * </p><p>
+   * <p>
    * The <code>mem</code> parameter lets you control how much memory can be used
    * to buffer changes in memory before flushing to disk. High values of
    * <code>mem</code> are likely to improve throughput. On the other hand, if
@@ -1026,18 +1026,18 @@ public class PackedInts {
   /**
    * Create a packed integer array writer for the given output, format, value
    * count, and number of bits per value.
-   * </p><p>
+   * <p>
    * The resulting stream will be long-aligned. This means that depending on
    * the format which is used under the hoods, up to 63 bits will be wasted.
    * An easy way to make sure that no space is lost is to always use a
    * <code>valueCount</code> that is a multiple of 64.
-   * </p><p>
+   * <p>
    * This method writes metadata to the stream, so that the resulting stream is
    * sufficient to restore a {@link Reader} from it. You don't need to track
    * <code>valueCount</code> or <code>bitsPerValue</code> by yourself. In case
    * this is a problem, you should probably look at
    * {@link #getWriterNoHeader(DataOutput, Format, int, int, int)}.
-   * </p><p>
+   * <p>
    * The <code>acceptableOverheadRatio</code> parameter controls how
    * readers that will be restored from this stream trade space
    * for speed by selecting a faster but potentially less memory-efficient
diff --git a/lucene/core/src/java/org/apache/lucene/util/packed/package-info.java b/lucene/core/src/java/org/apache/lucene/util/packed/package-info.java
index f05a143..0365f77 100644
--- a/lucene/core/src/java/org/apache/lucene/util/packed/package-info.java
+++ b/lucene/core/src/java/org/apache/lucene/util/packed/package-info.java
@@ -28,9 +28,8 @@
  *     The implementations provide different trade-offs between memory usage and
  *     access speed. The standard usage scenario is replacing large int or long
  *     arrays in order to reduce the memory footprint.
- * </p><p>
+ * <p>
  *     The main access point is the {@link org.apache.lucene.util.packed.PackedInts} factory.
- * </p>
  * 
  * <h3>In-memory structures</h3>
  * 
diff --git a/lucene/grouping/src/java/org/apache/lucene/search/grouping/AbstractAllGroupHeadsCollector.java b/lucene/grouping/src/java/org/apache/lucene/search/grouping/AbstractAllGroupHeadsCollector.java
index 2d35145..2916c17 100644
--- a/lucene/grouping/src/java/org/apache/lucene/search/grouping/AbstractAllGroupHeadsCollector.java
+++ b/lucene/grouping/src/java/org/apache/lucene/search/grouping/AbstractAllGroupHeadsCollector.java
@@ -82,7 +82,7 @@ public abstract class AbstractAllGroupHeadsCollector<GH extends AbstractAllGroup
   /**
    * Returns the group head and puts it into {@link #temporalResult}.
    * If the group head wasn't encountered before then it will be added to the collected group heads.
-   * <p/>
+   * <p>
    * The {@link TemporalResult#stop} property will be <code>true</code> if the group head wasn't encountered before
    * otherwise <code>false</code>.
    *
diff --git a/lucene/grouping/src/java/org/apache/lucene/search/grouping/AbstractAllGroupsCollector.java b/lucene/grouping/src/java/org/apache/lucene/search/grouping/AbstractAllGroupsCollector.java
index be9ed38..6582ef3 100644
--- a/lucene/grouping/src/java/org/apache/lucene/search/grouping/AbstractAllGroupsCollector.java
+++ b/lucene/grouping/src/java/org/apache/lucene/search/grouping/AbstractAllGroupsCollector.java
@@ -29,8 +29,7 @@ import org.apache.lucene.util.BytesRef;
  * query. Only the group value is collected, and the order
  * is undefined.  This collector does not determine
  * the most relevant document of a group.
- *
- * <p/>
+ * <p>
  * This is an abstract version. Concrete implementations define
  * what a group actually is and how it is internally collected.
  *
@@ -50,7 +49,7 @@ public abstract class AbstractAllGroupsCollector<GROUP_VALUE_TYPE> extends Simpl
 
   /**
    * Returns the group values
-   * <p/>
+   * <p>
    * This is an unordered collections of group values. For each group that matched the query there is a {@link BytesRef}
    * representing a group value.
    *
diff --git a/lucene/grouping/src/java/org/apache/lucene/search/grouping/GroupingSearch.java b/lucene/grouping/src/java/org/apache/lucene/search/grouping/GroupingSearch.java
index 6515caa..5c367ac 100644
--- a/lucene/grouping/src/java/org/apache/lucene/search/grouping/GroupingSearch.java
+++ b/lucene/grouping/src/java/org/apache/lucene/search/grouping/GroupingSearch.java
@@ -379,7 +379,7 @@ public class GroupingSearch {
   /**
    * Whether to also compute all groups matching the query.
    * This can be used to determine the number of groups, which can be used for accurate pagination.
-   * <p/>
+   * <p>
    * When grouping by doc block the number of groups are automatically included in the {@link TopGroups} and this
    * option doesn't have any influence.
    *
@@ -406,7 +406,7 @@ public class GroupingSearch {
 
   /**
    * Whether to compute all group heads (most relevant document per group) matching the query.
-   * <p/>
+   * <p>
    * This feature isn't enabled when grouping by doc block.
    *
    * @param allGroupHeads Whether to compute all group heads (most relevant document per group) matching the query
@@ -430,7 +430,7 @@ public class GroupingSearch {
    * Sets the initial size of some internal used data structures.
    * This prevents growing data structures many times. This can improve the performance of the grouping at the cost of
    * more initial RAM.
-   * <p/>
+   * <p>
    * The {@link #setAllGroups} and {@link #setAllGroupHeads} features use this option.
    * Defaults to 128.
    *
diff --git a/lucene/grouping/src/java/org/apache/lucene/search/grouping/function/FunctionAllGroupsCollector.java b/lucene/grouping/src/java/org/apache/lucene/search/grouping/function/FunctionAllGroupsCollector.java
index ddb1b32..c6f1ad9 100644
--- a/lucene/grouping/src/java/org/apache/lucene/search/grouping/function/FunctionAllGroupsCollector.java
+++ b/lucene/grouping/src/java/org/apache/lucene/search/grouping/function/FunctionAllGroupsCollector.java
@@ -34,8 +34,7 @@ import java.util.TreeSet;
  * query. Only the group value is collected, and the order
  * is undefined.  This collector does not determine
  * the most relevant document of a group.
- *
- * <p/>
+ * <p>
  * Implementation detail: Uses {@link ValueSource} and {@link FunctionValues} to retrieve the
  * field values to group by.
  *
diff --git a/lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermAllGroupsCollector.java b/lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermAllGroupsCollector.java
index 8c4f104..568789b 100644
--- a/lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermAllGroupsCollector.java
+++ b/lucene/grouping/src/java/org/apache/lucene/search/grouping/term/TermAllGroupsCollector.java
@@ -34,8 +34,7 @@ import java.util.List;
  * query. Only the group value is collected, and the order
  * is undefined.  This collector does not determine
  * the most relevant document of a group.
- *
- * <p/>
+ * <p>
  * Implementation detail: an int hash set (SentinelIntSet)
  * is used to detect if a group is already added to the
  * total count.  For each segment the int set is cleared and filled
diff --git a/lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenStreamFromTermVector.java b/lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenStreamFromTermVector.java
index c381fad..efa3dee 100644
--- a/lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenStreamFromTermVector.java
+++ b/lucene/highlighter/src/java/org/apache/lucene/search/highlight/TokenStreamFromTermVector.java
@@ -41,7 +41,7 @@ import org.apache.lucene.util.UnicodeUtil;
  * because you know the term vector has payloads, since the first call to incrementToken() will observe if you asked
  * for them and if not then won't get them.  This TokenStream supports an efficient {@link #reset()}, so there's
  * no need to wrap with a caching impl.
- * <p />
+ * <p>
  * The implementation will create an array of tokens indexed by token position.  As long as there aren't massive jumps
  * in positions, this is fine.  And it assumes there aren't large numbers of tokens at the same position, since it adds
  * them to a linked-list per position in O(N^2) complexity.  When there aren't positions in the term vector, it divides
diff --git a/lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/SingleFragListBuilder.java b/lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/SingleFragListBuilder.java
index 2051860..ebe6fad 100644
--- a/lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/SingleFragListBuilder.java
+++ b/lucene/highlighter/src/java/org/apache/lucene/search/vectorhighlight/SingleFragListBuilder.java
@@ -27,7 +27,7 @@ import org.apache.lucene.search.vectorhighlight.FieldPhraseList.WeightedPhraseIn
 /**
  * An implementation class of {@link FragListBuilder} that generates one {@link WeightedFragInfo} object.
  * Typical use case of this class is that you can get an entire field contents
- * by using both of this class and {@link SimpleFragmentsBuilder}.<br/>
+ * by using both of this class and {@link SimpleFragmentsBuilder}.<br>
  * <pre class="prettyprint">
  * FastVectorHighlighter h = new FastVectorHighlighter( true, true,
  *   new SingleFragListBuilder(), new SimpleFragmentsBuilder() );
diff --git a/lucene/join/src/java/org/apache/lucene/search/join/JoinUtil.java b/lucene/join/src/java/org/apache/lucene/search/join/JoinUtil.java
index 8649f31..44abe7b 100644
--- a/lucene/join/src/java/org/apache/lucene/search/join/JoinUtil.java
+++ b/lucene/join/src/java/org/apache/lucene/search/join/JoinUtil.java
@@ -36,16 +36,16 @@ public final class JoinUtil {
 
   /**
    * Method for query time joining.
-   * <p/>
+   * <p>
    * Execute the returned query with a {@link IndexSearcher} to retrieve all documents that have the same terms in the
    * to field that match with documents matching the specified fromQuery and have the same terms in the from field.
-   * <p/>
+   * <p>
    * In the case a single document relates to more than one document the <code>multipleValuesPerDocument</code> option
    * should be set to true. When the <code>multipleValuesPerDocument</code> is set to <code>true</code> only the
    * the score from the first encountered join value originating from the 'from' side is mapped into the 'to' side.
    * Even in the case when a second join value related to a specific document yields a higher score. Obviously this
    * doesn't apply in the case that {@link ScoreMode#None} is used, since no scores are computed at all.
-   * </p>
+   * <p>
    * Memory considerations: During joining all unique join values are kept in memory. On top of that when the scoreMode
    * isn't set to {@link ScoreMode#None} a float value per unique join value is kept in memory for computing scores.
    * When scoreMode is set to {@link ScoreMode#Avg} also an additional integer value is kept in memory per unique
diff --git a/lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java b/lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java
index c7758d2..deee844 100644
--- a/lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java
+++ b/lucene/memory/src/java/org/apache/lucene/index/memory/MemoryIndex.java
@@ -123,7 +123,7 @@ import org.apache.lucene.util.RecyclingIntBlockAllocator;
  * 
  * <p>
  * <b>Example Usage</b> 
- * <p>
+ * <br>
  * <pre class="prettyprint">
  * Analyzer analyzer = new SimpleAnalyzer(version);
  * MemoryIndex index = new MemoryIndex();
diff --git a/lucene/misc/src/java/org/apache/lucene/store/WindowsDirectory.java b/lucene/misc/src/java/org/apache/lucene/store/WindowsDirectory.java
index 46f762d..dd8cb6c 100644
--- a/lucene/misc/src/java/org/apache/lucene/store/WindowsDirectory.java
+++ b/lucene/misc/src/java/org/apache/lucene/store/WindowsDirectory.java
@@ -38,7 +38,6 @@ import org.apache.lucene.store.Directory; // javadoc
  *   <li>Put WindowsDirectory.dll into some directory in your windows PATH
  *   <li>Open indexes with WindowsDirectory and use it.
  * </ol>
- * </p>
  * @lucene.experimental
  */
 public class WindowsDirectory extends FSDirectory {
diff --git a/lucene/queries/src/java/org/apache/lucene/queries/mlt/MoreLikeThis.java b/lucene/queries/src/java/org/apache/lucene/queries/mlt/MoreLikeThis.java
index 4f07b22..f750620 100644
--- a/lucene/queries/src/java/org/apache/lucene/queries/mlt/MoreLikeThis.java
+++ b/lucene/queries/src/java/org/apache/lucene/queries/mlt/MoreLikeThis.java
@@ -49,12 +49,12 @@ import java.util.Set;
 /**
  * Generate "more like this" similarity queries.
  * Based on this mail:
- * <code><pre>
+ * <pre><code>
  * Lucene does let you access the document frequency of terms, with IndexReader.docFreq().
  * Term frequencies can be computed by re-tokenizing the text, which, for a single document,
  * is usually fast enough.  But looking up the docFreq() of every term in the document is
  * probably too slow.
- * <p/>
+ * 
  * You can use some heuristics to prune the set of terms, to avoid calling docFreq() too much,
  * or at all.  Since you're trying to maximize a tf*idf score, you're probably most interested
  * in terms with a high tf. Choosing a tf threshold even as low as two or three will radically
@@ -63,44 +63,40 @@ import java.util.Set;
  * number of characters, not selecting anything less than, e.g., six or seven characters.
  * With these sorts of heuristics you can usually find small set of, e.g., ten or fewer terms
  * that do a pretty good job of characterizing a document.
- * <p/>
+ * 
  * It all depends on what you're trying to do.  If you're trying to eek out that last percent
  * of precision and recall regardless of computational difficulty so that you can win a TREC
  * competition, then the techniques I mention above are useless.  But if you're trying to
  * provide a "more like this" button on a search results page that does a decent job and has
  * good performance, such techniques might be useful.
- * <p/>
+ * 
  * An efficient, effective "more-like-this" query generator would be a great contribution, if
  * anyone's interested.  I'd imagine that it would take a Reader or a String (the document's
  * text), analyzer Analyzer, and return a set of representative terms using heuristics like those
  * above.  The frequency and length thresholds could be parameters, etc.
- * <p/>
+ * 
  * Doug
- * </pre></code>
- * <p/>
- * <p/>
- * <p/>
+ * </code></pre>
  * <h3>Initial Usage</h3>
- * <p/>
+ * <p>
  * This class has lots of options to try to make it efficient and flexible.
  * The simplest possible usage is as follows. The bold
  * fragment is specific to this class.
- * <p/>
+ * <br>
  * <pre class="prettyprint">
- * <p/>
  * IndexReader ir = ...
  * IndexSearcher is = ...
- * <p/>
+ *
  * MoreLikeThis mlt = new MoreLikeThis(ir);
  * Reader target = ... // orig source of doc you want to find similarities to
  * Query query = mlt.like( target);
- * <p/>
+ * 
  * Hits hits = is.search(query);
  * // now the usual iteration thru 'hits' - the only thing to watch for is to make sure
  * //you ignore the doc if it matches your 'target' document, as it should be similar to itself
- * <p/>
+ *
  * </pre>
- * <p/>
+ * <p>
  * Thus you:
  * <ol>
  * <li> do your normal, Lucene setup for searching,
@@ -109,13 +105,12 @@ import java.util.Set;
  * <li> then call one of the like() calls to generate a similarity query
  * <li> call the searcher to find the similar docs
  * </ol>
- * <p/>
+ * <br>
  * <h3>More Advanced Usage</h3>
- * <p/>
+ * <p>
  * You may want to use {@link #setFieldNames setFieldNames(...)} so you can examine
  * multiple fields (e.g. body and title) for similarity.
- * <p/>
- * <p/>
+ * <p>
  * Depending on the size of your index and the size and makeup of your documents you
  * may want to call the other set methods to control how the similarity queries are
  * generated:
@@ -130,7 +125,7 @@ import java.util.Set;
  * <li> {@link #setMaxNumTokensParsed setMaxNumTokensParsed(...)}
  * <li> {@link #setStopWords setStopWord(...)}
  * </ul>
- * <p/>
+ * <br>
  * <hr>
  * <pre>
  * Changes: Mark Harwood 29/02/04
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParserBase.java b/lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParserBase.java
index a4d7797..bb6f0c2 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParserBase.java
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/classic/QueryParserBase.java
@@ -236,7 +236,7 @@ public abstract class QueryParserBase extends QueryBuilder implements CommonQuer
    * Sets the boolean operator of the QueryParser.
    * In default mode (<code>OR_OPERATOR</code>) terms without any modifiers
    * are considered optional: for example <code>capital of Hungary</code> is equal to
-   * <code>capital OR of OR Hungary</code>.<br/>
+   * <code>capital OR of OR Hungary</code>.<br>
    * In <code>AND_OPERATOR</code> mode terms are considered to be in conjunction: the
    * above mentioned query is parsed as <code>capital AND of AND Hungary</code>
    */
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/ext/ExtendableQueryParser.java b/lucene/queryparser/src/java/org/apache/lucene/queryparser/ext/ExtendableQueryParser.java
index 37398da..19560b0 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/ext/ExtendableQueryParser.java
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/ext/ExtendableQueryParser.java
@@ -55,7 +55,6 @@ import org.apache.lucene.util.Version;
  * <pre>
  *   _customExt:"Apache Lucene\?" OR _customExt:prefix\*
  * </pre>
- * </p>
  * <p>
  * The {@link ExtendableQueryParser} itself does not implement the logic how
  * field and extension key are separated or ordered. All logic regarding the
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/core/QueryParserHelper.java b/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/core/QueryParserHelper.java
index e48f9ab..ef00f76 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/core/QueryParserHelper.java
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/core/QueryParserHelper.java
@@ -25,15 +25,12 @@ import org.apache.lucene.queryparser.flexible.core.processors.QueryNodeProcessor
 
   
 /**
- * <p>
  * This class is a helper for the query parser framework, it does all the three
  * query parser phrases at once: text parsing, query processing and query
  * building.
- * </p>
  * <p>
  * It contains methods that allows the user to change the implementation used on
  * the three phases.
- * </p>
  * 
  * @see QueryNodeProcessor
  * @see SyntaxParser
@@ -221,17 +218,17 @@ public class QueryParserHelper {
   }
 
   /**
-   * Parses a query string to an object, usually some query object. <br/>
-   * <br/>
-   * In this method the three phases are executed: <br/>
-   * <br/>
+   * Parses a query string to an object, usually some query object.<br>
+   * <br>
+   * In this method the three phases are executed: <br>
+   * <br>
    * &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1st - the query string is parsed using the
    * text parser returned by {@link #getSyntaxParser()}, the result is a query
-   * node tree <br/>
-   * <br/>
+   * node tree <br>
+   * <br>
    * &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2nd - the query node tree is processed by the
-   * processor returned by {@link #getQueryNodeProcessor()} <br/>
-   * <br/>
+   * processor returned by {@link #getQueryNodeProcessor()} <br>
+   * <br>
    * &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3th - a object is built from the query node
    * tree using the builder returned by {@link #getQueryBuilder()}
    * 
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/core/processors/QueryNodeProcessor.java b/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/core/processors/QueryNodeProcessor.java
index 2e8d7da..93591f5 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/core/processors/QueryNodeProcessor.java
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/core/processors/QueryNodeProcessor.java
@@ -22,20 +22,16 @@ import org.apache.lucene.queryparser.flexible.core.config.QueryConfigHandler;
 import org.apache.lucene.queryparser.flexible.core.nodes.QueryNode;
 
 /**
- * <p>
  * A {@link QueryNodeProcessor} is an interface for classes that process a
  * {@link QueryNode} tree.
  * <p>
- * </p>
  * The implementor of this class should perform some operation on a query node
  * tree and return the same or another query node tree.
  * <p>
- * </p>
  * It also may carry a {@link QueryConfigHandler} object that contains
  * configuration about the query represented by the query tree or the
  * collection/index where it's intended to be executed.
  * <p>
- * </p>
  * In case there is any {@link QueryConfigHandler} associated to the query tree
  * to be processed, it should be set using
  * {@link QueryNodeProcessor#setQueryConfigHandler(QueryConfigHandler)} before
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/StandardQueryParser.java b/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/StandardQueryParser.java
index bb6a7b8..384e44c 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/StandardQueryParser.java
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/StandardQueryParser.java
@@ -41,30 +41,25 @@ import org.apache.lucene.search.MultiTermQuery;
 import org.apache.lucene.search.Query;
 
 /**
- * <p>
  * This class is a helper that enables users to easily use the Lucene query
  * parser.
- * </p>
  * <p>
  * To construct a Query object from a query string, use the
  * {@link #parse(String, String)} method:
- * <ul>
- * StandardQueryParser queryParserHelper = new StandardQueryParser(); <br/>
+ * <pre class="prettyprint">
+ * StandardQueryParser queryParserHelper = new StandardQueryParser();
  * Query query = queryParserHelper.parse("a AND b", "defaultField");
- * </ul>
+ * </pre>
  * <p>
  * To change any configuration before parsing the query string do, for example:
- * <p/>
- * <ul>
- * // the query config handler returned by {@link StandardQueryParser} is a
- * {@link StandardQueryConfigHandler} <br/>
- * queryParserHelper.getQueryConfigHandler().setAnalyzer(new
- * WhitespaceAnalyzer());
- * </ul>
+ * <br>
+ * <pre class="prettyprint">
+ * // the query config handler returned by {@link StandardQueryParser} is a {@link StandardQueryConfigHandler}
+ * queryParserHelper.getQueryConfigHandler().setAnalyzer(new WhitespaceAnalyzer());
+ * </pre>
  * <p>
  * The syntax for query strings is as follows (copied from the old QueryParser
  * javadoc):
- * <ul>
  * A Query is a series of clauses. A clause may be prefixed by:
  * <ul>
  * <li>a plus (<code>+</code>) or a minus (<code>-</code>) sign, indicating that
@@ -92,17 +87,13 @@ import org.apache.lucene.search.Query;
  * href="{@docRoot}/org/apache/lucene/queryparser/classic/package-summary.html#package_description">
  * query syntax documentation</a>.
  * </p>
- * </ul>
  * <p>
  * The text parser used by this helper is a {@link StandardSyntaxParser}.
- * <p/>
  * <p>
  * The query node processor used by this helper is a
  * {@link StandardQueryNodeProcessorPipeline}.
- * <p/>
  * <p>
  * The builder used by this helper is a {@link StandardQueryTreeBuilder}.
- * <p/>
  * 
  * @see StandardQueryParser
  * @see StandardQueryConfigHandler
@@ -126,10 +117,10 @@ public class StandardQueryParser extends QueryParserHelper implements CommonQuer
    * Constructs a {@link StandardQueryParser} object and sets an
    * {@link Analyzer} to it. The same as:
    * 
-   * <ul>
+   * <pre class="prettyprint">
    * StandardQueryParser qp = new StandardQueryParser();
    * qp.getQueryConfigHandler().setAnalyzer(analyzer);
-   * </ul>
+   * </pre>
    * 
    * @param analyzer
    *          the analyzer to be used by this query parser helper
@@ -181,7 +172,7 @@ public class StandardQueryParser extends QueryParserHelper implements CommonQuer
    * Sets the boolean operator of the QueryParser. In default mode (
    * {@link Operator#OR}) terms without any modifiers are considered optional:
    * for example <code>capital of Hungary</code> is equal to
-   * <code>capital OR of OR Hungary</code>.<br/>
+   * <code>capital OR of OR Hungary</code>.<br>
    * In {@link Operator#AND} mode terms are considered to be in conjunction: the
    * above mentioned query is parsed as <code>capital AND of AND Hungary</code>
    */
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/builders/BooleanQueryNodeBuilder.java b/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/builders/BooleanQueryNodeBuilder.java
index 5712edc..f64ee4e 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/builders/BooleanQueryNodeBuilder.java
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/builders/BooleanQueryNodeBuilder.java
@@ -36,8 +36,8 @@ import org.apache.lucene.search.BooleanQuery.TooManyClauses;
  * Builds a {@link BooleanQuery} object from a {@link BooleanQueryNode} object.
  * Every children in the {@link BooleanQueryNode} object must be already tagged
  * using {@link QueryTreeBuilder#QUERY_TREE_BUILDER_TAGID} with a {@link Query}
- * object. <br/>
- * <br/>
+ * object. <br>
+ * <br>
  * It takes in consideration if the children is a {@link ModifierQueryNode} to
  * define the {@link BooleanClause}.
  */
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/builders/StandardBooleanQueryNodeBuilder.java b/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/builders/StandardBooleanQueryNodeBuilder.java
index 6551dab..9bc5f05 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/builders/StandardBooleanQueryNodeBuilder.java
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/builders/StandardBooleanQueryNodeBuilder.java
@@ -37,7 +37,7 @@ import org.apache.lucene.search.similarities.Similarity;
 /**
  * This builder does the same as the {@link BooleanQueryNodeBuilder}, but this
  * considers if the built {@link BooleanQuery} should have its coord disabled or
- * not. <br/>
+ * not. <br>
  * 
  * @see BooleanQueryNodeBuilder
  * @see BooleanQuery
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/builders/StandardQueryBuilder.java b/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/builders/StandardQueryBuilder.java
index e9d641a..6ca88dc 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/builders/StandardQueryBuilder.java
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/builders/StandardQueryBuilder.java
@@ -25,7 +25,7 @@ import org.apache.lucene.search.Query;
 
 /**
  * This interface should be implemented by every class that wants to build
- * {@link Query} objects from {@link QueryNode} objects. <br/>
+ * {@link Query} objects from {@link QueryNode} objects.
  * 
  * @see QueryBuilder
  * @see QueryTreeBuilder
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/builders/StandardQueryTreeBuilder.java b/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/builders/StandardQueryTreeBuilder.java
index c0cd179..af672b3 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/builders/StandardQueryTreeBuilder.java
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/builders/StandardQueryTreeBuilder.java
@@ -45,7 +45,7 @@ import org.apache.lucene.search.Query;
  * This query tree builder only defines the necessary map to build a
  * {@link Query} tree object. It should be used to generate a {@link Query} tree
  * object from a query node tree processed by a
- * {@link StandardQueryNodeProcessorPipeline}. <br/>
+ * {@link StandardQueryNodeProcessorPipeline}.
  * 
  * @see QueryTreeBuilder
  * @see StandardQueryNodeProcessorPipeline
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/config/StandardQueryConfigHandler.java b/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/config/StandardQueryConfigHandler.java
index 81ef7c1..0912111 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/config/StandardQueryConfigHandler.java
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/config/StandardQueryConfigHandler.java
@@ -38,7 +38,7 @@ import org.apache.lucene.search.MultiTermQuery.RewriteMethod;
  * This query configuration handler is used for almost every processor defined
  * in the {@link StandardQueryNodeProcessorPipeline} processor pipeline. It holds
  * configuration methods that reproduce the configuration methods that could be set on the old
- * lucene 2.4 QueryParser class. <br/>
+ * lucene 2.4 QueryParser class.
  * 
  * @see StandardQueryNodeProcessorPipeline
  */
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/nodes/BooleanModifierNode.java b/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/nodes/BooleanModifierNode.java
index 1decaaf..fc46a9c 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/nodes/BooleanModifierNode.java
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/nodes/BooleanModifierNode.java
@@ -24,7 +24,7 @@ import org.apache.lucene.queryparser.flexible.standard.processors.BooleanQuery2M
 /**
  * A {@link BooleanModifierNode} has the same behaviour as
  * {@link ModifierQueryNode}, it only indicates that this modifier was added by
- * {@link BooleanQuery2ModifierNodeProcessor} and not by the user. <br/>
+ * {@link BooleanQuery2ModifierNodeProcessor} and not by the user.
  * 
  * @see ModifierQueryNode
  */
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/nodes/StandardBooleanQueryNode.java b/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/nodes/StandardBooleanQueryNode.java
index 2d65360..241dd20 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/nodes/StandardBooleanQueryNode.java
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/nodes/StandardBooleanQueryNode.java
@@ -27,7 +27,7 @@ import org.apache.lucene.search.similarities.Similarity;
 /**
  * A {@link StandardBooleanQueryNode} has the same behavior as
  * {@link BooleanQueryNode}. It only indicates if the coord should be enabled or
- * not for this boolean query. <br/>
+ * not for this boolean query.
  * 
  * @see Similarity#coord(int, int)
  * @see BooleanQuery
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/AllowLeadingWildcardProcessor.java b/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/AllowLeadingWildcardProcessor.java
index c2009ab..85afb5c 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/AllowLeadingWildcardProcessor.java
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/AllowLeadingWildcardProcessor.java
@@ -35,7 +35,7 @@ import org.apache.lucene.queryparser.flexible.standard.parser.EscapeQuerySyntaxI
  * {@link ConfigurationKeys#ALLOW_LEADING_WILDCARD} is defined in the
  * {@link QueryConfigHandler}. If it is and leading wildcard is not allowed, it
  * looks for every {@link WildcardQueryNode} contained in the query node tree
- * and throws an exception if any of them has a leading wildcard ('*' or '?'). <br/>
+ * and throws an exception if any of them has a leading wildcard ('*' or '?').
  * 
  * @see ConfigurationKeys#ALLOW_LEADING_WILDCARD
  */
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/AnalyzerQueryNodeProcessor.java b/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/AnalyzerQueryNodeProcessor.java
index bfe0b9b..83e9734 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/AnalyzerQueryNodeProcessor.java
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/AnalyzerQueryNodeProcessor.java
@@ -57,17 +57,17 @@ import org.apache.lucene.queryparser.flexible.standard.nodes.WildcardQueryNode;
  * not <code>null</code>, it looks for every {@link FieldQueryNode} that is not
  * {@link WildcardQueryNode}, {@link FuzzyQueryNode} or
  * {@link RangeQueryNode} contained in the query node tree, then it applies
- * the analyzer to that {@link FieldQueryNode} object. <br/>
- * <br/>
+ * the analyzer to that {@link FieldQueryNode} object. <br>
+ * <br>
  * If the analyzer return only one term, the returned term is set to the
- * {@link FieldQueryNode} and it's returned. <br/>
- * <br/>
+ * {@link FieldQueryNode} and it's returned. <br>
+ * <br>
  * If the analyzer return more than one term, a {@link TokenizedPhraseQueryNode}
  * or {@link MultiPhraseQueryNode} is created, whether there is one or more
- * terms at the same position, and it's returned. <br/>
- * <br/>
+ * terms at the same position, and it's returned. <br>
+ * <br>
  * If no term is returned by the analyzer a {@link NoTokenFoundQueryNode} object
- * is returned. <br/>
+ * is returned.
  * 
  * @see ConfigurationKeys#ANALYZER
  * @see Analyzer
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/BooleanSingleChildOptimizationQueryNodeProcessor.java b/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/BooleanSingleChildOptimizationQueryNodeProcessor.java
index f6887cb..65c9939 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/BooleanSingleChildOptimizationQueryNodeProcessor.java
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/BooleanSingleChildOptimizationQueryNodeProcessor.java
@@ -31,7 +31,7 @@ import org.apache.lucene.queryparser.flexible.standard.nodes.BooleanModifierNode
  * This processor removes every {@link BooleanQueryNode} that contains only one
  * child and returns this child. If this child is {@link ModifierQueryNode} that
  * was defined by the user. A modifier is not defined by the user when it's a
- * {@link BooleanModifierNode} <br/>
+ * {@link BooleanModifierNode}
  * 
  * @see ModifierQueryNode
  */
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/BoostQueryNodeProcessor.java b/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/BoostQueryNodeProcessor.java
index 02387fd..b0c455c 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/BoostQueryNodeProcessor.java
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/BoostQueryNodeProcessor.java
@@ -32,7 +32,7 @@ import org.apache.lucene.queryparser.flexible.standard.config.StandardQueryConfi
 /**
  * This processor iterates the query node tree looking for every
  * {@link FieldableNode} that has {@link ConfigurationKeys#BOOST} in its
- * config. If there is, the boost is applied to that {@link FieldableNode}. <br/>
+ * config. If there is, the boost is applied to that {@link FieldableNode}.
  * 
  * @see ConfigurationKeys#BOOST
  * @see QueryConfigHandler
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/DefaultPhraseSlopQueryNodeProcessor.java b/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/DefaultPhraseSlopQueryNodeProcessor.java
index bea4675..f2fc891 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/DefaultPhraseSlopQueryNodeProcessor.java
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/DefaultPhraseSlopQueryNodeProcessor.java
@@ -34,7 +34,7 @@ import org.apache.lucene.queryparser.flexible.standard.nodes.MultiPhraseQueryNod
  * {@link TokenizedPhraseQueryNode} and {@link MultiPhraseQueryNode} that does
  * not have any {@link SlopQueryNode} applied to it and creates an
  * {@link SlopQueryNode} and apply to it. The new {@link SlopQueryNode} has the
- * same slop value defined in the configuration. <br/>
+ * same slop value defined in the configuration.
  * 
  * @see SlopQueryNode
  * @see ConfigurationKeys#PHRASE_SLOP
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/FuzzyQueryNodeProcessor.java b/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/FuzzyQueryNodeProcessor.java
index 5b20742..a2ffae4 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/FuzzyQueryNodeProcessor.java
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/FuzzyQueryNodeProcessor.java
@@ -34,7 +34,7 @@ import org.apache.lucene.search.FuzzyQuery;
  * query configuration for
  * {@link ConfigurationKeys#FUZZY_CONFIG}, gets the
  * fuzzy prefix length and default similarity from it and set to the fuzzy node.
- * For more information about fuzzy prefix length check: {@link FuzzyQuery}. <br/>
+ * For more information about fuzzy prefix length check: {@link FuzzyQuery}.
  * 
  * @see ConfigurationKeys#FUZZY_CONFIG
  * @see FuzzyQuery
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/LowercaseExpandedTermsQueryNodeProcessor.java b/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/LowercaseExpandedTermsQueryNodeProcessor.java
index f4b5fb3..f81a40a 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/LowercaseExpandedTermsQueryNodeProcessor.java
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/LowercaseExpandedTermsQueryNodeProcessor.java
@@ -39,7 +39,7 @@ import org.apache.lucene.queryparser.flexible.standard.nodes.WildcardQueryNode;
  * {@link QueryConfigHandler}. If it is and the expanded terms should be
  * lower-cased, it looks for every {@link WildcardQueryNode},
  * {@link FuzzyQueryNode} and children of a {@link RangeQueryNode} and lower-case its
- * term. <br/>
+ * term.
  * 
  * @see ConfigurationKeys#LOWERCASE_EXPANDED_TERMS
  */
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/MultiFieldQueryNodeProcessor.java b/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/MultiFieldQueryNodeProcessor.java
index 44f74e8..20708a4 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/MultiFieldQueryNodeProcessor.java
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/MultiFieldQueryNodeProcessor.java
@@ -32,15 +32,15 @@ import org.apache.lucene.queryparser.flexible.standard.config.StandardQueryConfi
 
 /**
  * This processor is used to expand terms so the query looks for the same term
- * in different fields. It also boosts a query based on its field. <br/>
- * <br/>
+ * in different fields. It also boosts a query based on its field. <br>
+ * <br>
  * This processor looks for every {@link FieldableNode} contained in the query
  * node tree. If a {@link FieldableNode} is found, it checks if there is a
  * {@link ConfigurationKeys#MULTI_FIELDS} defined in the {@link QueryConfigHandler}. If
  * there is, the {@link FieldableNode} is cloned N times and the clones are
  * added to a {@link BooleanQueryNode} together with the original node. N is
  * defined by the number of fields that it will be expanded to. The
- * {@link BooleanQueryNode} is returned. <br/>
+ * {@link BooleanQueryNode} is returned.
  * 
  * @see ConfigurationKeys#MULTI_FIELDS
  */
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/NumericQueryNodeProcessor.java b/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/NumericQueryNodeProcessor.java
index 7c18eb1..443a3e0 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/NumericQueryNodeProcessor.java
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/NumericQueryNodeProcessor.java
@@ -46,8 +46,8 @@ import org.apache.lucene.queryparser.flexible.standard.nodes.NumericRangeQueryNo
  * {@link NumericRangeQueryNode} with upper and lower inclusive and lower and
  * upper equals to the value represented by the {@link FieldQueryNode} converted
  * to {@link Number}. It means that <b>field:1</b> is converted to <b>field:[1
- * TO 1]</b>. <br/>
- * <br/>
+ * TO 1]</b>. <br>
+ * <br>
  * Note that {@link FieldQueryNode}s children of a
  * {@link RangeQueryNode} are ignored.
  * 
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/PhraseSlopQueryNodeProcessor.java b/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/PhraseSlopQueryNodeProcessor.java
index 5d62fea..518e2ca 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/PhraseSlopQueryNodeProcessor.java
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/PhraseSlopQueryNodeProcessor.java
@@ -29,7 +29,7 @@ import org.apache.lucene.queryparser.flexible.standard.nodes.MultiPhraseQueryNod
 /**
  * This processor removes invalid {@link SlopQueryNode} objects in the query
  * node tree. A {@link SlopQueryNode} is invalid if its child is neither a
- * {@link TokenizedPhraseQueryNode} nor a {@link MultiPhraseQueryNode}. <br/>
+ * {@link TokenizedPhraseQueryNode} nor a {@link MultiPhraseQueryNode}.
  * 
  * @see SlopQueryNode
  */
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/RemoveEmptyNonLeafQueryNodeProcessor.java b/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/RemoveEmptyNonLeafQueryNodeProcessor.java
index 0365d35..ed7d403 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/RemoveEmptyNonLeafQueryNodeProcessor.java
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/RemoveEmptyNonLeafQueryNodeProcessor.java
@@ -30,11 +30,11 @@ import org.apache.lucene.queryparser.flexible.core.processors.QueryNodeProcessor
 /**
  * This processor removes every {@link QueryNode} that is not a leaf and has not
  * children. If after processing the entire tree the root node is not a leaf and
- * has no children, a {@link MatchNoDocsQueryNode} object is returned. <br/>
- * <br/>
+ * has no children, a {@link MatchNoDocsQueryNode} object is returned.
+ * <br>
  * This processor is used at the end of a pipeline to avoid invalid query node
  * tree structures like a {@link GroupQueryNode} or {@link ModifierQueryNode}
- * with no children. <br/>
+ * with no children.
  * 
  * @see QueryNode
  * @see MatchNoDocsQueryNode
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/StandardQueryNodeProcessorPipeline.java b/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/StandardQueryNodeProcessorPipeline.java
index 0cbcd73..c0cdfc5 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/StandardQueryNodeProcessorPipeline.java
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/StandardQueryNodeProcessorPipeline.java
@@ -31,15 +31,15 @@ import org.apache.lucene.search.Query;
 
 /**
  * This pipeline has all the processors needed to process a query node tree,
- * generated by {@link StandardSyntaxParser}, already assembled. <br/>
- * <br/>
- * The order they are assembled affects the results. <br/>
- * <br/>
+ * generated by {@link StandardSyntaxParser}, already assembled. <br>
+ * <br>
+ * The order they are assembled affects the results. <br>
+ * <br>
  * This processor pipeline was designed to work with
- * {@link StandardQueryConfigHandler}. <br/>
- * <br/>
+ * {@link StandardQueryConfigHandler}. <br>
+ * <br>
  * The result query node tree can be used to build a {@link Query} object using
- * {@link StandardQueryTreeBuilder}. <br/>
+ * {@link StandardQueryTreeBuilder}.
  * 
  * @see StandardQueryTreeBuilder
  * @see StandardQueryConfigHandler
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/TermRangeQueryNodeProcessor.java b/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/TermRangeQueryNodeProcessor.java
index 48ce640..5cb0b8a 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/TermRangeQueryNodeProcessor.java
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/TermRangeQueryNodeProcessor.java
@@ -40,16 +40,15 @@ import org.apache.lucene.queryparser.flexible.standard.nodes.TermRangeQueryNode;
  * upper bounds value from the {@link TermRangeQueryNode} object and try
  * to parse their values using a {@link DateFormat}. If the values cannot be
  * parsed to a date value, it will only create the {@link TermRangeQueryNode}
- * using the non-parsed values. <br/>
- * <br/>
+ * using the non-parsed values. <br>
+ * <br>
  * If a {@link ConfigurationKeys#LOCALE} is defined in the
  * {@link QueryConfigHandler} it will be used to parse the date, otherwise
- * {@link Locale#getDefault()} will be used. <br/>
- * <br/>
+ * {@link Locale#getDefault()} will be used. <br>
+ * <br>
  * If a {@link ConfigurationKeys#DATE_RESOLUTION} is defined and the
  * {@link Resolution} is not <code>null</code> it will also be used to parse the
- * date value. <br/>
- * <br/>
+ * date value.
  * 
  * @see ConfigurationKeys#DATE_RESOLUTION
  * @see ConfigurationKeys#LOCALE
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/WildcardQueryNodeProcessor.java b/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/WildcardQueryNodeProcessor.java
index b9fe6c0..f5a0a61 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/WildcardQueryNodeProcessor.java
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/flexible/standard/processors/WildcardQueryNodeProcessor.java
@@ -37,7 +37,7 @@ import org.apache.lucene.search.PrefixQuery;
  * have values containing the prefixed wildcard. However, Lucene
  * {@link PrefixQuery} cannot contain the prefixed wildcard. So, this processor
  * basically removed the prefixed wildcard from the
- * {@link PrefixWildcardQueryNode} value. <br/>
+ * {@link PrefixWildcardQueryNode} value.
  * 
  * @see PrefixQuery
  * @see PrefixWildcardQueryNode
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/CachedFilterBuilder.java b/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/CachedFilterBuilder.java
index da01dc9..24d7060 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/CachedFilterBuilder.java
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/CachedFilterBuilder.java
@@ -36,10 +36,10 @@ import java.util.Map;
  * A good example of this might be a term query on a field with only 2 possible  values -
  * "true" or "false". In a large index, querying or filtering on this field requires reading
  * millions  of document ids from disk which can more usefully be cached as a filter bitset.
- * <p/>
+ * <p>
  * For Queries/Filters to be cached and reused the object must implement hashcode and
  * equals methods correctly so that duplicate queries/filters can be detected in the cache.
- * <p/>
+ * <p>
  * The CoreParser.maxNumCachedFilters property can be used to control the size of the LRU
  * Cache established during the construction of CoreParser instances.
  */
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/NumericRangeFilterBuilder.java b/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/NumericRangeFilterBuilder.java
index 8c70b9d..fa3ae83 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/NumericRangeFilterBuilder.java
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/NumericRangeFilterBuilder.java
@@ -85,7 +85,7 @@ import java.io.IOException;
  * <td>4</td>
  * </tr>
  * </table>
- * <p/>
+ * <p>
  * If an error occurs parsing the supplied <tt>lowerTerm</tt> or
  * <tt>upperTerm</tt> into the numeric type specified by <tt>type</tt>, then the
  * error will be silently ignored and the resulting filter will not match any
@@ -99,13 +99,13 @@ public class NumericRangeFilterBuilder implements FilterBuilder {
 
   /**
    * Specifies how this {@link NumericRangeFilterBuilder} will handle errors.
-   * <p/>
+   * <p>
    * If this is set to true, {@link #getFilter(Element)} will throw a
    * {@link ParserException} if it is unable to parse the lowerTerm or upperTerm
    * into the appropriate numeric type. If this is set to false, then this
    * exception will be silently ignored and the resulting filter will not match
    * any documents.
-   * <p/>
+   * <p>
    * Defaults to false.
    */
   public void setStrictMode(boolean strictMode) {
diff --git a/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/NumericRangeQueryBuilder.java b/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/NumericRangeQueryBuilder.java
index 877e4d9..b40a6c1 100644
--- a/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/NumericRangeQueryBuilder.java
+++ b/lucene/queryparser/src/java/org/apache/lucene/queryparser/xml/builders/NumericRangeQueryBuilder.java
@@ -80,7 +80,7 @@ import org.w3c.dom.Element;
  * <td>4</td>
  * </tr>
  * </table>
- * <p/>
+ * <p>
  * A {@link ParserException} will be thrown if an error occurs parsing the
  * supplied <tt>lowerTerm</tt> or <tt>upperTerm</tt> into the numeric type
  * specified by <tt>type</tt>.
diff --git a/lucene/queryparser/src/test/org/apache/lucene/queryparser/flexible/spans/SpanOrQueryNodeBuilder.java b/lucene/queryparser/src/test/org/apache/lucene/queryparser/flexible/spans/SpanOrQueryNodeBuilder.java
index edfd7a2..8eac6e4 100644
--- a/lucene/queryparser/src/test/org/apache/lucene/queryparser/flexible/spans/SpanOrQueryNodeBuilder.java
+++ b/lucene/queryparser/src/test/org/apache/lucene/queryparser/flexible/spans/SpanOrQueryNodeBuilder.java
@@ -28,8 +28,8 @@ import org.apache.lucene.search.spans.SpanOrQuery;
 import org.apache.lucene.search.spans.SpanQuery;
 
 /**
- * This builder creates {@link SpanOrQuery}s from a {@link BooleanQueryNode}.<br/>
- * <br/>
+ * This builder creates {@link SpanOrQuery}s from a {@link BooleanQueryNode}.<br>
+ * <br>
  * 
  * It assumes that the {@link BooleanQueryNode} instance has at least one child.
  */
diff --git a/lucene/queryparser/src/test/org/apache/lucene/queryparser/flexible/spans/SpansQueryConfigHandler.java b/lucene/queryparser/src/test/org/apache/lucene/queryparser/flexible/spans/SpansQueryConfigHandler.java
index 853c429..a04bb16 100644
--- a/lucene/queryparser/src/test/org/apache/lucene/queryparser/flexible/spans/SpansQueryConfigHandler.java
+++ b/lucene/queryparser/src/test/org/apache/lucene/queryparser/flexible/spans/SpansQueryConfigHandler.java
@@ -22,8 +22,8 @@ import org.apache.lucene.queryparser.flexible.core.config.FieldConfig;
 import org.apache.lucene.queryparser.flexible.core.config.QueryConfigHandler;
 
 /**
- * This query config handler only adds the {@link UniqueFieldAttribute} to it.<br/>
- * <br/>
+ * This query config handler only adds the {@link UniqueFieldAttribute} to it.<br>
+ * <br>
  * 
  * It does not return any configuration for a field in specific.
  */
diff --git a/lucene/queryparser/src/test/org/apache/lucene/queryparser/flexible/spans/SpansQueryTreeBuilder.java b/lucene/queryparser/src/test/org/apache/lucene/queryparser/flexible/spans/SpansQueryTreeBuilder.java
index e344c86..4093819 100644
--- a/lucene/queryparser/src/test/org/apache/lucene/queryparser/flexible/spans/SpansQueryTreeBuilder.java
+++ b/lucene/queryparser/src/test/org/apache/lucene/queryparser/flexible/spans/SpansQueryTreeBuilder.java
@@ -27,12 +27,12 @@ import org.apache.lucene.search.spans.SpanQuery;
 
 /**
  * Sets up a query tree builder to build a span query tree from a query node
- * tree.<br/>
- * <br/>
+ * tree.<br>
+ * <br>
  * 
- * The defined map is:<br/>
- * - every BooleanQueryNode instance is delegated to the SpanOrQueryNodeBuilder<br/>
- * - every FieldQueryNode instance is delegated to the SpanTermQueryNodeBuilder <br/>
+ * The defined map is:<br>
+ * - every BooleanQueryNode instance is delegated to the SpanOrQueryNodeBuilder<br>
+ * - every FieldQueryNode instance is delegated to the SpanTermQueryNodeBuilder <br>
  * 
  */
 public class SpansQueryTreeBuilder extends QueryTreeBuilder implements
diff --git a/lucene/queryparser/src/test/org/apache/lucene/queryparser/flexible/spans/SpansValidatorQueryNodeProcessor.java b/lucene/queryparser/src/test/org/apache/lucene/queryparser/flexible/spans/SpansValidatorQueryNodeProcessor.java
index 501dc22..66827e8 100644
--- a/lucene/queryparser/src/test/org/apache/lucene/queryparser/flexible/spans/SpansValidatorQueryNodeProcessor.java
+++ b/lucene/queryparser/src/test/org/apache/lucene/queryparser/flexible/spans/SpansValidatorQueryNodeProcessor.java
@@ -33,8 +33,8 @@ import org.apache.lucene.queryparser.flexible.core.processors.QueryNodeProcessor
  * Validates every query node in a query node tree. This processor will pass
  * fine if the query nodes are only {@link BooleanQueryNode}s,
  * {@link OrQueryNode}s or {@link FieldQueryNode}s, otherwise an exception will
- * be thrown. <br/>
- * <br/>
+ * be thrown. <br>
+ * <br>
  * 
  * If they are {@link AndQueryNode} or an instance of anything else that
  * implements {@link FieldQueryNode} the exception will also be thrown.
diff --git a/lucene/queryparser/src/test/org/apache/lucene/queryparser/flexible/spans/TestSpanQueryParser.java b/lucene/queryparser/src/test/org/apache/lucene/queryparser/flexible/spans/TestSpanQueryParser.java
index 57d0534..7197e9d 100644
--- a/lucene/queryparser/src/test/org/apache/lucene/queryparser/flexible/spans/TestSpanQueryParser.java
+++ b/lucene/queryparser/src/test/org/apache/lucene/queryparser/flexible/spans/TestSpanQueryParser.java
@@ -31,33 +31,33 @@ import org.apache.lucene.search.Query;
 import org.apache.lucene.util.LuceneTestCase;
 
 /**
- * This test case demonstrates how the new query parser can be used.<br/>
- * <br/>
+ * This test case demonstrates how the new query parser can be used.<br>
+ * <br>
  * 
  * It tests queries likes "term", "field:term" "term1 term2" "term1 OR term2",
  * which are all already supported by the current syntax parser (
- * {@link StandardSyntaxParser}).<br/>
- * <br/>
+ * {@link StandardSyntaxParser}).<br>
+ * <br>
  * 
  * The goals is to create a new query parser that supports only the pair
  * "field:term" or a list of pairs separated or not by an OR operator, and from
  * this query generate {@link SpanQuery} objects instead of the regular
  * {@link Query} objects. Basically, every pair will be converted to a
  * {@link SpanTermQuery} object and if there are more than one pair they will be
- * grouped by an {@link OrQueryNode}.<br/>
- * <br/>
+ * grouped by an {@link OrQueryNode}.<br>
+ * <br>
  * 
  * Another functionality that will be added is the ability to convert every
- * field defined in the query to an unique specific field.<br/>
- * <br/>
+ * field defined in the query to an unique specific field.<br>
+ * <br>
  * 
  * The query generation is divided in three different steps: parsing (syntax),
- * processing (semantic) and building.<br/>
- * <br/>
+ * processing (semantic) and building.<br>
+ * <br>
  * 
  * The parsing phase, as already mentioned will be performed by the current
- * query parser: {@link StandardSyntaxParser}.<br/>
- * <br/>
+ * query parser: {@link StandardSyntaxParser}.<br>
+ * <br>
  * 
  * The processing phase will be performed by a processor pipeline which is
  * compound by 2 processors: {@link SpansValidatorQueryNodeProcessor} and
@@ -79,8 +79,8 @@ import org.apache.lucene.util.LuceneTestCase;
  * 
  * The building phase is performed by the {@link SpansQueryTreeBuilder}, which
  * basically contains a map that defines which builder will be used to generate
- * {@link SpanQuery} objects from {@link QueryNode} objects.<br/>
- * <br/>
+ * {@link SpanQuery} objects from {@link QueryNode} objects.<br>
+ * <br>
  * 
  * @see SpansQueryConfigHandler
  * @see SpansQueryTreeBuilder
diff --git a/lucene/queryparser/src/test/org/apache/lucene/queryparser/flexible/spans/TestSpanQueryParserSimpleSample.java b/lucene/queryparser/src/test/org/apache/lucene/queryparser/flexible/spans/TestSpanQueryParserSimpleSample.java
index 08706fc..4d64b2b 100644
--- a/lucene/queryparser/src/test/org/apache/lucene/queryparser/flexible/spans/TestSpanQueryParserSimpleSample.java
+++ b/lucene/queryparser/src/test/org/apache/lucene/queryparser/flexible/spans/TestSpanQueryParserSimpleSample.java
@@ -30,33 +30,33 @@ import org.apache.lucene.search.spans.SpanTermQuery;
 import org.apache.lucene.util.LuceneTestCase;
 
 /**
- * This test case demonstrates how the new query parser can be used.<br/>
- * <br/>
+ * This test case demonstrates how the new query parser can be used.<br>
+ * <br>
  * 
  * It tests queries likes "term", "field:term" "term1 term2" "term1 OR term2",
  * which are all already supported by the current syntax parser (
- * {@link StandardSyntaxParser}).<br/>
- * <br/>
+ * {@link StandardSyntaxParser}).<br>
+ * <br>
  * 
  * The goals is to create a new query parser that supports only the pair
  * "field:term" or a list of pairs separated or not by an OR operator, and from
  * this query generate {@link SpanQuery} objects instead of the regular
  * {@link Query} objects. Basically, every pair will be converted to a
  * {@link SpanTermQuery} object and if there are more than one pair they will be
- * grouped by an {@link OrQueryNode}.<br/>
- * <br/>
+ * grouped by an {@link OrQueryNode}.<br>
+ * <br>
  * 
  * Another functionality that will be added is the ability to convert every
- * field defined in the query to an unique specific field.<br/>
- * <br/>
+ * field defined in the query to an unique specific field.<br>
+ * <br>
  * 
  * The query generation is divided in three different steps: parsing (syntax),
- * processing (semantic) and building.<br/>
- * <br/>
+ * processing (semantic) and building.<br>
+ * <br>
  * 
  * The parsing phase, as already mentioned will be performed by the current
- * query parser: {@link StandardSyntaxParser}.<br/>
- * <br/>
+ * query parser: {@link StandardSyntaxParser}.<br>
+ * <br>
  * 
  * The processing phase will be performed by a processor pipeline which is
  * compound by 2 processors: {@link SpansValidatorQueryNodeProcessor} and
@@ -78,8 +78,8 @@ import org.apache.lucene.util.LuceneTestCase;
  * 
  * The building phase is performed by the {@link SpansQueryTreeBuilder}, which
  * basically contains a map that defines which builder will be used to generate
- * {@link SpanQuery} objects from {@link QueryNode} objects.<br/>
- * <br/>
+ * {@link SpanQuery} objects from {@link QueryNode} objects.<br>
+ * <br>
  * 
  * @see TestSpanQueryParser for a more advanced example
  * 
diff --git a/lucene/queryparser/src/test/org/apache/lucene/queryparser/flexible/spans/UniqueFieldAttribute.java b/lucene/queryparser/src/test/org/apache/lucene/queryparser/flexible/spans/UniqueFieldAttribute.java
index 45225a7..c078c4d 100644
--- a/lucene/queryparser/src/test/org/apache/lucene/queryparser/flexible/spans/UniqueFieldAttribute.java
+++ b/lucene/queryparser/src/test/org/apache/lucene/queryparser/flexible/spans/UniqueFieldAttribute.java
@@ -23,8 +23,7 @@ import org.apache.lucene.util.Attribute;
 /**
  * This attribute is used by the {@link UniqueFieldQueryNodeProcessor}
  * processor. It holds a value that defines which is the unique field name that
- * should be set in every {@link FieldableNode}.<br/>
- * <br/>
+ * should be set in every {@link FieldableNode}.
  * 
  * @see UniqueFieldQueryNodeProcessor
  */
diff --git a/lucene/queryparser/src/test/org/apache/lucene/queryparser/flexible/spans/UniqueFieldAttributeImpl.java b/lucene/queryparser/src/test/org/apache/lucene/queryparser/flexible/spans/UniqueFieldAttributeImpl.java
index 6f7c25b..cb40297 100644
--- a/lucene/queryparser/src/test/org/apache/lucene/queryparser/flexible/spans/UniqueFieldAttributeImpl.java
+++ b/lucene/queryparser/src/test/org/apache/lucene/queryparser/flexible/spans/UniqueFieldAttributeImpl.java
@@ -23,8 +23,7 @@ import org.apache.lucene.util.AttributeImpl;
 /**
  * This attribute is used by the {@link UniqueFieldQueryNodeProcessor}
  * processor. It holds a value that defines which is the unique field name that
- * should be set in every {@link FieldableNode}.<br/>
- * <br/>
+ * should be set in every {@link FieldableNode}.
  * 
  * @see UniqueFieldQueryNodeProcessor
  */
diff --git a/lucene/queryparser/src/test/org/apache/lucene/queryparser/flexible/spans/UniqueFieldQueryNodeProcessor.java b/lucene/queryparser/src/test/org/apache/lucene/queryparser/flexible/spans/UniqueFieldQueryNodeProcessor.java
index 10563ea..8595400 100644
--- a/lucene/queryparser/src/test/org/apache/lucene/queryparser/flexible/spans/UniqueFieldQueryNodeProcessor.java
+++ b/lucene/queryparser/src/test/org/apache/lucene/queryparser/flexible/spans/UniqueFieldQueryNodeProcessor.java
@@ -30,8 +30,7 @@ import org.apache.lucene.queryparser.flexible.core.processors.QueryNodeProcessor
  * node contained in the query tree to the field name defined in the
  * {@link UniqueFieldAttribute}. So, the {@link UniqueFieldAttribute} must be
  * defined in the {@link QueryConfigHandler} object set in this processor,
- * otherwise it throws an exception.<br/>
- * <br/>
+ * otherwise it throws an exception.
  * 
  * @see UniqueFieldAttribute
  */
diff --git a/lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/DuplicateFilter.java b/lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/DuplicateFilter.java
index 3648003..d83fc84 100644
--- a/lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/DuplicateFilter.java
+++ b/lucene/sandbox/src/java/org/apache/lucene/sandbox/queries/DuplicateFilter.java
@@ -59,7 +59,7 @@ public class DuplicateFilter extends Filter {
   /**
    * "Full" processing mode starts by setting all bits to false and only setting bits
    * for documents that contain the given field and are identified as none-duplicates.
-   * <p/>
+   * <p>
    * "Fast" processing sets all bits to true then unsets all duplicate docs found for the
    * given field. This approach avoids the need to read DocsEnum for terms that are seen
    * to have a document frequency of exactly "1" (i.e. no duplicates). While a potentially
diff --git a/lucene/spatial/src/java/org/apache/lucene/spatial/SpatialStrategy.java b/lucene/spatial/src/java/org/apache/lucene/spatial/SpatialStrategy.java
index c20fb8f..7141500 100644
--- a/lucene/spatial/src/java/org/apache/lucene/spatial/SpatialStrategy.java
+++ b/lucene/spatial/src/java/org/apache/lucene/spatial/SpatialStrategy.java
@@ -32,7 +32,7 @@ import org.apache.lucene.spatial.query.SpatialArgs;
 /**
  * The SpatialStrategy encapsulates an approach to indexing and searching based
  * on shapes.
- * <p/>
+ * <p>
  * Different implementations will support different features. A strategy should
  * document these common elements:
  * <ul>
@@ -46,10 +46,10 @@ import org.apache.lucene.spatial.query.SpatialArgs;
  * If a strategy only supports certain shapes at index or query time, then in
  * general it will throw an exception if given an incompatible one.  It will not
  * be coerced into compatibility.
- * <p/>
+ * <p>
  * Note that a SpatialStrategy is not involved with the Lucene stored field
  * values of shapes, which is immaterial to indexing and search.
- * <p/>
+ * <p>
  * Thread-safe.
  *
  * @lucene.experimental
@@ -88,7 +88,7 @@ public abstract class SpatialStrategy {
    * Returns the IndexableField(s) from the {@code shape} that are to be
    * added to the {@link org.apache.lucene.document.Document}.  These fields
    * are expected to be marked as indexed and not stored.
-   * <p/>
+   * <p>
    * Note: If you want to <i>store</i> the shape as a string for retrieval in
    * search results, you could add it like this:
    * <pre>document.add(new StoredField(fieldName,ctx.toString(shape)));</pre>
@@ -133,7 +133,7 @@ public abstract class SpatialStrategy {
   /**
    * Make a Filter based principally on {@link org.apache.lucene.spatial.query.SpatialOperation}
    * and {@link Shape} from the supplied {@code args}.
-   * <p />
+   * <p>
    * If a subclasses implements
    * {@link #makeQuery(org.apache.lucene.spatial.query.SpatialArgs)}
    * then this method could be simply:
diff --git a/lucene/spatial/src/java/org/apache/lucene/spatial/bbox/BBoxOverlapRatioValueSource.java b/lucene/spatial/src/java/org/apache/lucene/spatial/bbox/BBoxOverlapRatioValueSource.java
index 94680e3..70d3ddb 100644
--- a/lucene/spatial/src/java/org/apache/lucene/spatial/bbox/BBoxOverlapRatioValueSource.java
+++ b/lucene/spatial/src/java/org/apache/lucene/spatial/bbox/BBoxOverlapRatioValueSource.java
@@ -23,8 +23,7 @@ import org.apache.lucene.search.Explanation;
 /**
  * The algorithm is implemented as envelope on envelope (rect on rect) overlays rather than
  * complex polygon on complex polygon overlays.
- * <p/>
- * <p/>
+ * <p>
  * Spatial relevance scoring algorithm:
  * <DL>
  *   <DT>queryArea</DT> <DD>the area of the input query envelope</DD>
@@ -43,11 +42,11 @@ import org.apache.lucene.search.Explanation;
  * area is calculated (queryArea, targetArea, intersectionArea). This allows for points or horizontal/vertical lines
  * to be used as the query shape and in such case the descending order should have smallest boxes up front. Without
  * this, a point or line query shape typically scores everything with the same value since there is 0 area.
- * <p />
+ * <p>
  * Note: The actual computation of queryRatio and targetRatio is more complicated so that it considers
  * points and lines. Lines have the ratio of overlap, and points are either 1.0 or 0.0 depending on whether
  * it intersects or not.
- * <p />
+ * <p>
  * Originally based on Geoportal's
  * <a href="http://geoportal.svn.sourceforge.net/svnroot/geoportal/Geoportal/trunk/src/com/esri/gpt/catalog/lucene/SpatialRankingValueSource.java">
  *   SpatialRankingValueSource</a> but modified quite a bit. GeoPortal's algorithm will yield a score of 0
diff --git a/lucene/spatial/src/java/org/apache/lucene/spatial/bbox/BBoxSimilarityValueSource.java b/lucene/spatial/src/java/org/apache/lucene/spatial/bbox/BBoxSimilarityValueSource.java
index ffc379f..3a0bbc7 100644
--- a/lucene/spatial/src/java/org/apache/lucene/spatial/bbox/BBoxSimilarityValueSource.java
+++ b/lucene/spatial/src/java/org/apache/lucene/spatial/bbox/BBoxSimilarityValueSource.java
@@ -32,7 +32,7 @@ import java.util.Map;
  * A base class for calculating a spatial relevance rank per document from a provided
  * {@link ValueSource} in which {@link FunctionValues#objectVal(int)} returns a {@link
  * com.spatial4j.core.shape.Rectangle}.
- * <p/>
+ * <p>
  * Implementers: remember to implement equals and hashCode if you have
  * fields!
  *
diff --git a/lucene/spatial/src/java/org/apache/lucene/spatial/bbox/BBoxStrategy.java b/lucene/spatial/src/java/org/apache/lucene/spatial/bbox/BBoxStrategy.java
index 1ac7037..3991b1c 100644
--- a/lucene/spatial/src/java/org/apache/lucene/spatial/bbox/BBoxStrategy.java
+++ b/lucene/spatial/src/java/org/apache/lucene/spatial/bbox/BBoxStrategy.java
@@ -53,7 +53,7 @@ import com.spatial4j.core.shape.Shape;
  * href="http://geoportal.svn.sourceforge.net/svnroot/geoportal/Geoportal/trunk/src/com/esri/gpt/catalog/lucene/SpatialClauseAdapter.java">SpatialClauseAdapter</a>.
  * <p>
  * <b>Characteristics:</b>
- * <p>
+ * <br>
  * <ul>
  * <li>Only indexes Rectangles; just one per field value. Other shapes can be provided
  * and the bounding box will be used.</li>
diff --git a/lucene/spatial/src/java/org/apache/lucene/spatial/prefix/AbstractVisitingPrefixTreeFilter.java b/lucene/spatial/src/java/org/apache/lucene/spatial/prefix/AbstractVisitingPrefixTreeFilter.java
index 0623ff0..35ce713 100644
--- a/lucene/spatial/src/java/org/apache/lucene/spatial/prefix/AbstractVisitingPrefixTreeFilter.java
+++ b/lucene/spatial/src/java/org/apache/lucene/spatial/prefix/AbstractVisitingPrefixTreeFilter.java
@@ -34,7 +34,7 @@ import org.apache.lucene.util.BytesRef;
  * Traverses a {@link SpatialPrefixTree} indexed field, using the template and
  * visitor design patterns for subclasses to guide the traversal and collect
  * matching documents.
- * <p/>
+ * <p>
  * Subclasses implement {@link #getDocIdSet(org.apache.lucene.index.LeafReaderContext,
  * org.apache.lucene.util.Bits)} by instantiating a custom {@link
  * VisitorTemplate} subclass (i.e. an anonymous inner class) and implement the
diff --git a/lucene/spatial/src/java/org/apache/lucene/spatial/prefix/PrefixTreeStrategy.java b/lucene/spatial/src/java/org/apache/lucene/spatial/prefix/PrefixTreeStrategy.java
index 7ce92fa..9ae31aa 100644
--- a/lucene/spatial/src/java/org/apache/lucene/spatial/prefix/PrefixTreeStrategy.java
+++ b/lucene/spatial/src/java/org/apache/lucene/spatial/prefix/PrefixTreeStrategy.java
@@ -41,7 +41,7 @@ import org.apache.lucene.spatial.util.ShapeFieldCacheDistanceValueSource;
  * approximate spatial search filter.
  * <p>
  * <b>Characteristics:</b>
- * <p>
+ * <br>
  * <ul>
  * <li>Can index any shape; however only {@link RecursivePrefixTreeStrategy}
  * can effectively search non-point shapes.</li>
diff --git a/lucene/spatial/src/java/org/apache/lucene/spatial/prefix/TermQueryPrefixTreeStrategy.java b/lucene/spatial/src/java/org/apache/lucene/spatial/prefix/TermQueryPrefixTreeStrategy.java
index 41bb66b..46b06bb 100644
--- a/lucene/spatial/src/java/org/apache/lucene/spatial/prefix/TermQueryPrefixTreeStrategy.java
+++ b/lucene/spatial/src/java/org/apache/lucene/spatial/prefix/TermQueryPrefixTreeStrategy.java
@@ -39,7 +39,7 @@ import java.util.List;
  * {@link TermsFilter} of all the cells from
  * {@link SpatialPrefixTree#getTreeCellIterator(com.spatial4j.core.shape.Shape, int)}.
  * It only supports the search of indexed Point shapes.
- * <p/>
+ * <p>
  * The precision of query shapes (distErrPct) is an important factor in using
  * this Strategy. If the precision is too precise then it will result in many
  * terms which will amount to a slower query.
diff --git a/lucene/spatial/src/java/org/apache/lucene/spatial/prefix/tree/Cell.java b/lucene/spatial/src/java/org/apache/lucene/spatial/prefix/tree/Cell.java
index 35cad2b..094a56a 100644
--- a/lucene/spatial/src/java/org/apache/lucene/spatial/prefix/tree/Cell.java
+++ b/lucene/spatial/src/java/org/apache/lucene/spatial/prefix/tree/Cell.java
@@ -86,9 +86,9 @@ public interface Cell {
    * {@code shapeFilter}. The returned cells should have {@link #getShapeRel()} set to
    * their relation with {@code shapeFilter}.  In addition, for non-points {@link #isLeaf()}
    * must be true when that relation is WITHIN.
-   * <p/>
+   * <p>
    * IMPORTANT: Cells returned from this iterator can be shared, as well as the bytes.
-   * <p/>
+   * <p>
    * Precondition: Never called when getLevel() == maxLevel.
    *
    * @param shapeFilter an optional filter for the returned cells.
diff --git a/lucene/spatial/src/java/org/apache/lucene/spatial/prefix/tree/DateRangePrefixTree.java b/lucene/spatial/src/java/org/apache/lucene/spatial/prefix/tree/DateRangePrefixTree.java
index b3ecad2..155583e 100644
--- a/lucene/spatial/src/java/org/apache/lucene/spatial/prefix/tree/DateRangePrefixTree.java
+++ b/lucene/spatial/src/java/org/apache/lucene/spatial/prefix/tree/DateRangePrefixTree.java
@@ -32,7 +32,7 @@ import com.spatial4j.core.shape.Shape;
  * months, ...). You pass in {@link Calendar} objects with the desired fields set and the unspecified
  * fields unset, which conveys the precision.  The implementation makes some optimization assumptions about a
  * {@link java.util.GregorianCalendar}; others could probably be supported easily.
- * <p/>
+ * <p>
  * Warning: If you construct a Calendar and then get something from the object like a field (e.g. year) or
  * milliseconds, then every field is fully set by side-effect. So after setting the fields, pass it to this
  * API first.
diff --git a/lucene/spatial/src/java/org/apache/lucene/spatial/prefix/tree/LegacyCell.java b/lucene/spatial/src/java/org/apache/lucene/spatial/prefix/tree/LegacyCell.java
index 03b4c05..5e45796 100644
--- a/lucene/spatial/src/java/org/apache/lucene/spatial/prefix/tree/LegacyCell.java
+++ b/lucene/spatial/src/java/org/apache/lucene/spatial/prefix/tree/LegacyCell.java
@@ -154,7 +154,7 @@ public abstract class LegacyCell implements Cell {
   /**
    * Performant implementations are expected to implement this efficiently by
    * considering the current cell's boundary.
-   * <p/>
+   * <p>
    * Precondition: Never called when getLevel() == maxLevel.
    * Precondition: this.getShape().relate(p) != DISJOINT.
    */
diff --git a/lucene/spatial/src/java/org/apache/lucene/spatial/prefix/tree/NumberRangePrefixTree.java b/lucene/spatial/src/java/org/apache/lucene/spatial/prefix/tree/NumberRangePrefixTree.java
index 3ae5d0c..0e9fd83 100644
--- a/lucene/spatial/src/java/org/apache/lucene/spatial/prefix/tree/NumberRangePrefixTree.java
+++ b/lucene/spatial/src/java/org/apache/lucene/spatial/prefix/tree/NumberRangePrefixTree.java
@@ -32,7 +32,7 @@ import org.apache.lucene.util.StringHelper;
 /**
  * A SpatialPrefixTree for single-dimensional numbers and number ranges of fixed precision values (not floating point).
  * Despite its name, the indexed values (and queries) need not actually be ranges, they can be unit instance/values.
- * <p />
+ * <p>
  * Why might you use this instead of Lucene's built-in integer/long support?  Here are some reasons with features based
  * on code in this class, <em>or are possible based on this class but require a subclass to fully realize it</em>.
  * <ul>
diff --git a/lucene/spatial/src/java/org/apache/lucene/spatial/prefix/tree/SpatialPrefixTree.java b/lucene/spatial/src/java/org/apache/lucene/spatial/prefix/tree/SpatialPrefixTree.java
index f264bb6..d912a66 100644
--- a/lucene/spatial/src/java/org/apache/lucene/spatial/prefix/tree/SpatialPrefixTree.java
+++ b/lucene/spatial/src/java/org/apache/lucene/spatial/prefix/tree/SpatialPrefixTree.java
@@ -26,7 +26,7 @@ import org.apache.lucene.util.BytesRef;
  * at variable lengths corresponding to variable precision.  Each string
  * corresponds to a rectangular spatial region.  This approach is
  * also referred to "Grids", "Tiles", and "Spatial Tiers".
- * <p/>
+ * <p>
  * Implementations of this class should be thread-safe and immutable once
  * initialized.
  *
diff --git a/lucene/spatial/src/java/org/apache/lucene/spatial/query/SpatialArgsParser.java b/lucene/spatial/src/java/org/apache/lucene/spatial/query/SpatialArgsParser.java
index 363ee13..6e20814 100644
--- a/lucene/spatial/src/java/org/apache/lucene/spatial/query/SpatialArgsParser.java
+++ b/lucene/spatial/src/java/org/apache/lucene/spatial/query/SpatialArgsParser.java
@@ -36,7 +36,7 @@ import java.util.StringTokenizer;
  * <pre>
  *   Intersects(ENVELOPE(-10,-8,22,20)) distErrPct=0.025
  * </pre>
- * <p/>
+ * <p>
  * In the future it would be good to support something at least semi-standardized like a
  * variant of <a href="http://docs.geoserver.org/latest/en/user/filter/ecql_reference.html#spatial-predicate">
  *   [E]CQL</a>.
diff --git a/lucene/spatial/src/java/org/apache/lucene/spatial/vector/PointVectorStrategy.java b/lucene/spatial/src/java/org/apache/lucene/spatial/vector/PointVectorStrategy.java
index 5721ed6..3b71402 100644
--- a/lucene/spatial/src/java/org/apache/lucene/spatial/vector/PointVectorStrategy.java
+++ b/lucene/spatial/src/java/org/apache/lucene/spatial/vector/PointVectorStrategy.java
@@ -49,7 +49,7 @@ import org.apache.lucene.spatial.util.ValueSourceFilter;
  *
  * <p>
  * <b>Characteristics:</b>
- * <p>
+ * <br>
  * <ul>
  * <li>Only indexes points; just one per field value.</li>
  * <li>Can query by a rectangle or circle.</li>
@@ -68,7 +68,7 @@ import org.apache.lucene.spatial.util.ValueSourceFilter;
  * an x and y pair of fields.  A Circle query does the same bbox query but adds a
  * ValueSource filter on
  * {@link #makeDistanceValueSource(com.spatial4j.core.shape.Point)}.
- * <p />
+ * <p>
  * One performance shortcoming with this strategy is that a scenario involving
  * both a search using a Circle and sort will result in calculations for the
  * spatial distance being done twice -- once for the filter and second for the
diff --git a/lucene/suggest/src/java/org/apache/lucene/search/spell/PlainTextDictionary.java b/lucene/suggest/src/java/org/apache/lucene/search/spell/PlainTextDictionary.java
index 6ec352d..38b61b5 100644
--- a/lucene/suggest/src/java/org/apache/lucene/search/spell/PlainTextDictionary.java
+++ b/lucene/suggest/src/java/org/apache/lucene/search/spell/PlainTextDictionary.java
@@ -35,10 +35,10 @@ import org.apache.lucene.util.IOUtils;
 /**
  * Dictionary represented by a text file.
  * 
- * <p/>Format allowed: 1 word per line:<br/>
- * word1<br/>
- * word2<br/>
- * word3<br/>
+ * <p>Format allowed: 1 word per line:<br>
+ * word1<br>
+ * word2<br>
+ * word3<br>
  */
 public class PlainTextDictionary implements Dictionary {
 
diff --git a/lucene/suggest/src/java/org/apache/lucene/search/spell/SpellChecker.java b/lucene/suggest/src/java/org/apache/lucene/search/spell/SpellChecker.java
index 5171653..285296b 100644
--- a/lucene/suggest/src/java/org/apache/lucene/search/spell/SpellChecker.java
+++ b/lucene/suggest/src/java/org/apache/lucene/search/spell/SpellChecker.java
@@ -49,7 +49,7 @@ import org.apache.lucene.util.BytesRefIterator;
 
 /**
  * <p>
- *   Spell Checker class  (Main class) <br/>
+ *   Spell Checker class  (Main class).<br>
  *  (initially inspired by the David Spencer code).
  * </p>
  *
diff --git a/lucene/suggest/src/java/org/apache/lucene/search/spell/SuggestWord.java b/lucene/suggest/src/java/org/apache/lucene/search/spell/SuggestWord.java
index 994cd33..bfa0aae 100644
--- a/lucene/suggest/src/java/org/apache/lucene/search/spell/SuggestWord.java
+++ b/lucene/suggest/src/java/org/apache/lucene/search/spell/SuggestWord.java
@@ -19,11 +19,9 @@ package org.apache.lucene.search.spell;
  */
 
 /**
- *  SuggestWord, used in suggestSimilar method in SpellChecker class.
- * <p/>
+ * SuggestWord, used in suggestSimilar method in SpellChecker class.
+ * <p>
  * Default sort is first by score, then by frequency.
- * 
- *
  */
 public final class SuggestWord{
   
diff --git a/lucene/suggest/src/java/org/apache/lucene/search/suggest/FileDictionary.java b/lucene/suggest/src/java/org/apache/lucene/search/suggest/FileDictionary.java
index 01fbdc3..3ee0efe 100644
--- a/lucene/suggest/src/java/org/apache/lucene/search/suggest/FileDictionary.java
+++ b/lucene/suggest/src/java/org/apache/lucene/search/suggest/FileDictionary.java
@@ -31,15 +31,15 @@ import org.apache.lucene.util.IOUtils;
 /**
  * Dictionary represented by a text file.
  * 
- * <p/>Format allowed: 1 entry per line:<br/>
- * An entry can be: <br/>
+ * <p>Format allowed: 1 entry per line:<br>
+ * An entry can be: <br>
  * <ul>
  * <li>suggestion</li>
  * <li>suggestion <code>fieldDelimiter</code> weight</li>
  * <li>suggestion <code>fieldDelimiter</code> weight <code>fieldDelimiter</code> payload</li>
  * </ul>
- * where the default <code>fieldDelimiter</code> is {@value #DEFAULT_FIELD_DELIMITER}<br/>
- * <p/>
+ * where the default <code>fieldDelimiter</code> is {@value #DEFAULT_FIELD_DELIMITER}<br>
+ * <p>
  * <b>NOTE:</b> 
  * <ul>
  * <li>In order to have payload enabled, the first entry has to have a payload</li>
@@ -49,11 +49,11 @@ import org.apache.lucene.util.IOUtils;
  *  then an empty payload is returned</li>
  * <li>An entry cannot have more than two <code>fieldDelimiter</code></li>
  * </ul>
- * <p/>
- * <b>Example:</b><br/>
- * word1 word2 TAB 100 TAB payload1<br/>
- * word3 TAB 101<br/>
- * word4 word3 TAB 102<br/>
+ * <p>
+ * <b>Example:</b><br>
+ * word1 word2 TAB 100 TAB payload1<br>
+ * word3 TAB 101<br>
+ * word4 word3 TAB 102<br>
  */
 public class FileDictionary implements Dictionary {
 
diff --git a/lucene/suggest/src/java/org/apache/lucene/search/suggest/jaspell/JaspellTernarySearchTrie.java b/lucene/suggest/src/java/org/apache/lucene/search/suggest/jaspell/JaspellTernarySearchTrie.java
index ec4bf26..5832816 100644
--- a/lucene/suggest/src/java/org/apache/lucene/search/suggest/jaspell/JaspellTernarySearchTrie.java
+++ b/lucene/suggest/src/java/org/apache/lucene/search/suggest/jaspell/JaspellTernarySearchTrie.java
@@ -47,21 +47,22 @@ import org.apache.lucene.util.RamUsageEstimator;
  * Implementation of a Ternary Search Trie, a data structure for storing
  * <code>String</code> objects that combines the compact size of a binary search
  * tree with the speed of a digital search trie, and is therefore ideal for
- * practical use in sorting and searching data.</p>
- * <p>
+ * practical use in sorting and searching data.
  * 
+ * <p>
  * This data structure is faster than hashing for many typical search problems,
  * and supports a broader range of useful problems and operations. Ternary
  * searches are faster than hashing and more powerful, too.
  * </p>
- * <p>
  * 
+ * <p>
  * The theory of ternary search trees was described at a symposium in 1997 (see
  * "Fast Algorithms for Sorting and Searching Strings," by J.L. Bentley and R.
  * Sedgewick, Proceedings of the 8th Annual ACM-SIAM Symposium on Discrete
  * Algorithms, January 1997). Algorithms in C, Third Edition, by Robert
  * Sedgewick (Addison-Wesley, 1998) provides yet another view of ternary search
  * trees.
+ * </p>
  *
  * @deprecated Migrate to one of the newer suggesters which are much more RAM efficient.
  */
diff --git a/lucene/test-framework/src/java/org/apache/lucene/mockfile/package-info.java b/lucene/test-framework/src/java/org/apache/lucene/mockfile/package-info.java
index a7b731c..b6423fe 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/mockfile/package-info.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/mockfile/package-info.java
@@ -25,6 +25,5 @@
  *   <li>{@link org.apache.lucene.mockfile.WindowsFS}: Acts like windows.
  *   <li>{@link org.apache.lucene.mockfile.DisableFsyncFS}: Makes actual fsync calls a no-op.
  * </ul>
- * </p>
  */
 package org.apache.lucene.mockfile;

