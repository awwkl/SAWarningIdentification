GitDiffStart: 260d8fd3f5de8fe79c937d72a076d54cb84a96c3 | Wed Sep 5 13:16:12 2012 +0000
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/BlockTermsReader.java b/lucene/codecs/src/java/org/apache/lucene/codecs/BlockTermsReader.java
deleted file mode 100644
index 7d1eece..0000000
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/BlockTermsReader.java
+++ /dev/null
@@ -1,870 +0,0 @@
-package org.apache.lucene.codecs;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.Collections;
-import java.util.Comparator;
-import java.util.Iterator;
-import java.util.TreeMap;
-
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.DocsAndPositionsEnum;
-import org.apache.lucene.index.DocsEnum;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentInfo;
-import org.apache.lucene.index.TermState;
-import org.apache.lucene.index.Terms;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.store.ByteArrayDataInput;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.Bits;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.DoubleBarrelLRUCache;
-
-/** Handles a terms dict, but decouples all details of
- *  doc/freqs/positions reading to an instance of {@link
- *  PostingsReaderBase}.  This class is reusable for
- *  codecs that use a different format for
- *  docs/freqs/positions (though codecs are also free to
- *  make their own terms dict impl).
- *
- * <p>This class also interacts with an instance of {@link
- * TermsIndexReaderBase}, to abstract away the specific
- * implementation of the terms dict index. 
- * @lucene.experimental */
-
-public class BlockTermsReader extends FieldsProducer {
-  // Open input to the main terms dict file (_X.tis)
-  private final IndexInput in;
-
-  // Reads the terms dict entries, to gather state to
-  // produce DocsEnum on demand
-  private final PostingsReaderBase postingsReader;
-
-  private final TreeMap<String,FieldReader> fields = new TreeMap<String,FieldReader>();
-
-  // Caches the most recently looked-up field + terms:
-  private final DoubleBarrelLRUCache<FieldAndTerm,BlockTermState> termsCache;
-
-  // Reads the terms index
-  private TermsIndexReaderBase indexReader;
-
-  // keeps the dirStart offset
-  protected long dirOffset;
-
-  // Used as key for the terms cache
-  private static class FieldAndTerm extends DoubleBarrelLRUCache.CloneableKey {
-    String field;
-    BytesRef term;
-
-    public FieldAndTerm() {
-    }
-
-    public FieldAndTerm(FieldAndTerm other) {
-      field = other.field;
-      term = BytesRef.deepCopyOf(other.term);
-    }
-
-    @Override
-    public boolean equals(Object _other) {
-      FieldAndTerm other = (FieldAndTerm) _other;
-      return other.field.equals(field) && term.bytesEquals(other.term);
-    }
-
-    @Override
-    public FieldAndTerm clone() {
-      return new FieldAndTerm(this);
-    }
-
-    @Override
-    public int hashCode() {
-      return field.hashCode() * 31 + term.hashCode();
-    }
-  }
-  
-  // private String segment;
-  
-  public BlockTermsReader(TermsIndexReaderBase indexReader, Directory dir, FieldInfos fieldInfos, SegmentInfo info, PostingsReaderBase postingsReader, IOContext context,
-                          int termsCacheSize, String segmentSuffix)
-    throws IOException {
-    
-    this.postingsReader = postingsReader;
-    termsCache = new DoubleBarrelLRUCache<FieldAndTerm,BlockTermState>(termsCacheSize);
-
-    // this.segment = segment;
-    in = dir.openInput(IndexFileNames.segmentFileName(info.name, segmentSuffix, BlockTermsWriter.TERMS_EXTENSION),
-                       context);
-
-    boolean success = false;
-    try {
-      readHeader(in);
-
-      // Have PostingsReader init itself
-      postingsReader.init(in);
-
-      // Read per-field details
-      seekDir(in, dirOffset);
-
-      final int numFields = in.readVInt();
-      if (numFields < 0) {
-        throw new CorruptIndexException("invalid number of fields: " + numFields + " (resource=" + in + ")");
-      }
-      for(int i=0;i<numFields;i++) {
-        final int field = in.readVInt();
-        final long numTerms = in.readVLong();
-        assert numTerms >= 0;
-        final long termsStartPointer = in.readVLong();
-        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);
-        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();
-        final long sumDocFreq = in.readVLong();
-        final int docCount = in.readVInt();
-        if (docCount < 0 || docCount > info.getDocCount()) { // #docs with field must be <= #docs
-          throw new CorruptIndexException("invalid docCount: " + docCount + " maxDoc: " + info.getDocCount() + " (resource=" + in + ")");
-        }
-        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field
-          throw new CorruptIndexException("invalid sumDocFreq: " + sumDocFreq + " docCount: " + docCount + " (resource=" + in + ")");
-        }
-        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings
-          throw new CorruptIndexException("invalid sumTotalTermFreq: " + sumTotalTermFreq + " sumDocFreq: " + sumDocFreq + " (resource=" + in + ")");
-        }
-        FieldReader previous = fields.put(fieldInfo.name, new FieldReader(fieldInfo, numTerms, termsStartPointer, sumTotalTermFreq, sumDocFreq, docCount));
-        if (previous != null) {
-          throw new CorruptIndexException("duplicate fields: " + fieldInfo.name + " (resource=" + in + ")");
-        }
-      }
-      success = true;
-    } finally {
-      if (!success) {
-        in.close();
-      }
-    }
-
-    this.indexReader = indexReader;
-  }
-
-  protected void readHeader(IndexInput input) throws IOException {
-    CodecUtil.checkHeader(input, BlockTermsWriter.CODEC_NAME,
-                          BlockTermsWriter.VERSION_START,
-                          BlockTermsWriter.VERSION_CURRENT);
-    dirOffset = input.readLong();
-  }
-  
-  protected void seekDir(IndexInput input, long dirOffset)
-      throws IOException {
-    input.seek(dirOffset);
-  }
-  
-  @Override
-  public void close() throws IOException {
-    try {
-      try {
-        if (indexReader != null) {
-          indexReader.close();
-        }
-      } finally {
-        // null so if an app hangs on to us (ie, we are not
-        // GCable, despite being closed) we still free most
-        // ram
-        indexReader = null;
-        if (in != null) {
-          in.close();
-        }
-      }
-    } finally {
-      if (postingsReader != null) {
-        postingsReader.close();
-      }
-    }
-  }
-
-  @Override
-  public Iterator<String> iterator() {
-    return Collections.unmodifiableSet(fields.keySet()).iterator();
-  }
-
-  @Override
-  public Terms terms(String field) throws IOException {
-    assert field != null;
-    return fields.get(field);
-  }
-
-  @Override
-  public int size() {
-    return fields.size();
-  }
-
-  private class FieldReader extends Terms {
-    final long numTerms;
-    final FieldInfo fieldInfo;
-    final long termsStartPointer;
-    final long sumTotalTermFreq;
-    final long sumDocFreq;
-    final int docCount;
-
-    FieldReader(FieldInfo fieldInfo, long numTerms, long termsStartPointer, long sumTotalTermFreq, long sumDocFreq, int docCount) {
-      assert numTerms > 0;
-      this.fieldInfo = fieldInfo;
-      this.numTerms = numTerms;
-      this.termsStartPointer = termsStartPointer;
-      this.sumTotalTermFreq = sumTotalTermFreq;
-      this.sumDocFreq = sumDocFreq;
-      this.docCount = docCount;
-    }
-
-    @Override
-    public Comparator<BytesRef> getComparator() {
-      return BytesRef.getUTF8SortedAsUnicodeComparator();
-    }
-
-    @Override
-    public TermsEnum iterator(TermsEnum reuse) throws IOException {
-      return new SegmentTermsEnum();
-    }
-
-    @Override
-    public boolean hasOffsets() {
-      return fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
-    }
-
-    @Override
-    public boolean hasPositions() {
-      return fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;
-    }
-    
-    @Override
-    public boolean hasPayloads() {
-      return fieldInfo.hasPayloads();
-    }
-
-    @Override
-    public long size() {
-      return numTerms;
-    }
-
-    @Override
-    public long getSumTotalTermFreq() {
-      return sumTotalTermFreq;
-    }
-
-    @Override
-    public long getSumDocFreq() throws IOException {
-      return sumDocFreq;
-    }
-
-    @Override
-    public int getDocCount() throws IOException {
-      return docCount;
-    }
-
-    // Iterates through terms in this field
-    private final class SegmentTermsEnum extends TermsEnum {
-      private final IndexInput in;
-      private final BlockTermState state;
-      private final boolean doOrd;
-      private final FieldAndTerm fieldTerm = new FieldAndTerm();
-      private final TermsIndexReaderBase.FieldIndexEnum indexEnum;
-      private final BytesRef term = new BytesRef();
-
-      /* This is true if indexEnum is "still" seek'd to the index term
-         for the current term. We set it to true on seeking, and then it
-         remains valid until next() is called enough times to load another
-         terms block: */
-      private boolean indexIsCurrent;
-
-      /* True if we've already called .next() on the indexEnum, to "bracket"
-         the current block of terms: */
-      private boolean didIndexNext;
-
-      /* Next index term, bracketing the current block of terms; this is
-         only valid if didIndexNext is true: */
-      private BytesRef nextIndexTerm;
-
-      /* True after seekExact(TermState), do defer seeking.  If the app then
-         calls next() (which is not "typical"), then we'll do the real seek */
-      private boolean seekPending;
-
-      /* How many blocks we've read since last seek.  Once this
-         is >= indexEnum.getDivisor() we set indexIsCurrent to false (since
-         the index can no long bracket seek-within-block). */
-      private int blocksSinceSeek;
-
-      private byte[] termSuffixes;
-      private ByteArrayDataInput termSuffixesReader = new ByteArrayDataInput();
-
-      /* Common prefix used for all terms in this block. */
-      private int termBlockPrefix;
-
-      /* How many terms in current block */
-      private int blockTermCount;
-
-      private byte[] docFreqBytes;
-      private final ByteArrayDataInput freqReader = new ByteArrayDataInput();
-      private int metaDataUpto;
-
-      public SegmentTermsEnum() throws IOException {
-        in = BlockTermsReader.this.in.clone();
-        in.seek(termsStartPointer);
-        indexEnum = indexReader.getFieldEnum(fieldInfo);
-        doOrd = indexReader.supportsOrd();
-        fieldTerm.field = fieldInfo.name;
-        state = postingsReader.newTermState();
-        state.totalTermFreq = -1;
-        state.ord = -1;
-
-        termSuffixes = new byte[128];
-        docFreqBytes = new byte[64];
-        //System.out.println("BTR.enum init this=" + this + " postingsReader=" + postingsReader);
-      }
-
-      @Override
-      public Comparator<BytesRef> getComparator() {
-        return BytesRef.getUTF8SortedAsUnicodeComparator();
-      }
-
-      // TODO: we may want an alternate mode here which is
-      // "if you are about to return NOT_FOUND I won't use
-      // the terms data from that"; eg FuzzyTermsEnum will
-      // (usually) just immediately call seek again if we
-      // return NOT_FOUND so it's a waste for us to fill in
-      // the term that was actually NOT_FOUND
-      @Override
-      public SeekStatus seekCeil(final BytesRef target, final boolean useCache) throws IOException {
-
-        if (indexEnum == null) {
-          throw new IllegalStateException("terms index was not loaded");
-        }
-   
-        //System.out.println("BTR.seek seg=" + segment + " target=" + fieldInfo.name + ":" + target.utf8ToString() + " " + target + " current=" + term().utf8ToString() + " " + term() + " useCache=" + useCache + " indexIsCurrent=" + indexIsCurrent + " didIndexNext=" + didIndexNext + " seekPending=" + seekPending + " divisor=" + indexReader.getDivisor() + " this="  + this);
-        if (didIndexNext) {
-          if (nextIndexTerm == null) {
-            //System.out.println("  nextIndexTerm=null");
-          } else {
-            //System.out.println("  nextIndexTerm=" + nextIndexTerm.utf8ToString());
-          }
-        }
-
-        // Check cache
-        if (useCache) {
-          fieldTerm.term = target;
-          // TODO: should we differentiate "frozen"
-          // TermState (ie one that was cloned and
-          // cached/returned by termState()) from the
-          // malleable (primary) one?
-          final TermState cachedState = termsCache.get(fieldTerm);
-          if (cachedState != null) {
-            seekPending = true;
-            //System.out.println("  cached!");
-            seekExact(target, cachedState);
-            //System.out.println("  term=" + term.utf8ToString());
-            return SeekStatus.FOUND;
-          }
-        }
-
-        boolean doSeek = true;
-
-        // See if we can avoid seeking, because target term
-        // is after current term but before next index term:
-        if (indexIsCurrent) {
-
-          final int cmp = BytesRef.getUTF8SortedAsUnicodeComparator().compare(term, target);
-
-          if (cmp == 0) {
-            // Already at the requested term
-            return SeekStatus.FOUND;
-          } else if (cmp < 0) {
-
-            // Target term is after current term
-            if (!didIndexNext) {
-              if (indexEnum.next() == -1) {
-                nextIndexTerm = null;
-              } else {
-                nextIndexTerm = indexEnum.term();
-              }
-              //System.out.println("  now do index next() nextIndexTerm=" + (nextIndexTerm == null ? "null" : nextIndexTerm.utf8ToString()));
-              didIndexNext = true;
-            }
-
-            if (nextIndexTerm == null || BytesRef.getUTF8SortedAsUnicodeComparator().compare(target, nextIndexTerm) < 0) {
-              // Optimization: requested term is within the
-              // same term block we are now in; skip seeking
-              // (but do scanning):
-              doSeek = false;
-              //System.out.println("  skip seek: nextIndexTerm=" + (nextIndexTerm == null ? "null" : nextIndexTerm.utf8ToString()));
-            }
-          }
-        }
-
-        if (doSeek) {
-          //System.out.println("  seek");
-
-          // Ask terms index to find biggest indexed term (=
-          // first term in a block) that's <= our text:
-          in.seek(indexEnum.seek(target));
-          boolean result = nextBlock();
-
-          // Block must exist since, at least, the indexed term
-          // is in the block:
-          assert result;
-
-          indexIsCurrent = true;
-          didIndexNext = false;
-          blocksSinceSeek = 0;          
-
-          if (doOrd) {
-            state.ord = indexEnum.ord()-1;
-          }
-
-          term.copyBytes(indexEnum.term());
-          //System.out.println("  seek: term=" + term.utf8ToString());
-        } else {
-          //System.out.println("  skip seek");
-          if (state.termBlockOrd == blockTermCount && !nextBlock()) {
-            indexIsCurrent = false;
-            return SeekStatus.END;
-          }
-        }
-
-        seekPending = false;
-
-        int common = 0;
-
-        // Scan within block.  We could do this by calling
-        // _next() and testing the resulting term, but this
-        // is wasteful.  Instead, we first confirm the
-        // target matches the common prefix of this block,
-        // and then we scan the term bytes directly from the
-        // termSuffixesreader's byte[], saving a copy into
-        // the BytesRef term per term.  Only when we return
-        // do we then copy the bytes into the term.
-
-        while(true) {
-
-          // First, see if target term matches common prefix
-          // in this block:
-          if (common < termBlockPrefix) {
-            final int cmp = (term.bytes[common]&0xFF) - (target.bytes[target.offset + common]&0xFF);
-            if (cmp < 0) {
-
-              // TODO: maybe we should store common prefix
-              // in block header?  (instead of relying on
-              // last term of previous block)
-
-              // Target's prefix is after the common block
-              // prefix, so term cannot be in this block
-              // but it could be in next block.  We
-              // must scan to end-of-block to set common
-              // prefix for next block:
-              if (state.termBlockOrd < blockTermCount) {
-                while(state.termBlockOrd < blockTermCount-1) {
-                  state.termBlockOrd++;
-                  state.ord++;
-                  termSuffixesReader.skipBytes(termSuffixesReader.readVInt());
-                }
-                final int suffix = termSuffixesReader.readVInt();
-                term.length = termBlockPrefix + suffix;
-                if (term.bytes.length < term.length) {
-                  term.grow(term.length);
-                }
-                termSuffixesReader.readBytes(term.bytes, termBlockPrefix, suffix);
-              }
-              state.ord++;
-              
-              if (!nextBlock()) {
-                indexIsCurrent = false;
-                return SeekStatus.END;
-              }
-              common = 0;
-
-            } else if (cmp > 0) {
-              // Target's prefix is before the common prefix
-              // of this block, so we position to start of
-              // block and return NOT_FOUND:
-              assert state.termBlockOrd == 0;
-
-              final int suffix = termSuffixesReader.readVInt();
-              term.length = termBlockPrefix + suffix;
-              if (term.bytes.length < term.length) {
-                term.grow(term.length);
-              }
-              termSuffixesReader.readBytes(term.bytes, termBlockPrefix, suffix);
-              return SeekStatus.NOT_FOUND;
-            } else {
-              common++;
-            }
-
-            continue;
-          }
-
-          // Test every term in this block
-          while (true) {
-            state.termBlockOrd++;
-            state.ord++;
-
-            final int suffix = termSuffixesReader.readVInt();
-            
-            // We know the prefix matches, so just compare the new suffix:
-            final int termLen = termBlockPrefix + suffix;
-            int bytePos = termSuffixesReader.getPosition();
-
-            boolean next = false;
-            final int limit = target.offset + (termLen < target.length ? termLen : target.length);
-            int targetPos = target.offset + termBlockPrefix;
-            while(targetPos < limit) {
-              final int cmp = (termSuffixes[bytePos++]&0xFF) - (target.bytes[targetPos++]&0xFF);
-              if (cmp < 0) {
-                // Current term is still before the target;
-                // keep scanning
-                next = true;
-                break;
-              } else if (cmp > 0) {
-                // Done!  Current term is after target. Stop
-                // here, fill in real term, return NOT_FOUND.
-                term.length = termBlockPrefix + suffix;
-                if (term.bytes.length < term.length) {
-                  term.grow(term.length);
-                }
-                termSuffixesReader.readBytes(term.bytes, termBlockPrefix, suffix);
-                //System.out.println("  NOT_FOUND");
-                return SeekStatus.NOT_FOUND;
-              }
-            }
-
-            if (!next && target.length <= termLen) {
-              term.length = termBlockPrefix + suffix;
-              if (term.bytes.length < term.length) {
-                term.grow(term.length);
-              }
-              termSuffixesReader.readBytes(term.bytes, termBlockPrefix, suffix);
-
-              if (target.length == termLen) {
-                // Done!  Exact match.  Stop here, fill in
-                // real term, return FOUND.
-                //System.out.println("  FOUND");
-
-                if (useCache) {
-                  // Store in cache
-                  decodeMetaData();
-                  //System.out.println("  cache! state=" + state);
-                  termsCache.put(new FieldAndTerm(fieldTerm), (BlockTermState) state.clone());
-                }
-
-                return SeekStatus.FOUND;
-              } else {
-                //System.out.println("  NOT_FOUND");
-                return SeekStatus.NOT_FOUND;
-              }
-            }
-
-            if (state.termBlockOrd == blockTermCount) {
-              // Must pre-fill term for next block's common prefix
-              term.length = termBlockPrefix + suffix;
-              if (term.bytes.length < term.length) {
-                term.grow(term.length);
-              }
-              termSuffixesReader.readBytes(term.bytes, termBlockPrefix, suffix);
-              break;
-            } else {
-              termSuffixesReader.skipBytes(suffix);
-            }
-          }
-
-          // The purpose of the terms dict index is to seek
-          // the enum to the closest index term before the
-          // term we are looking for.  So, we should never
-          // cross another index term (besides the first
-          // one) while we are scanning:
-
-          assert indexIsCurrent;
-
-          if (!nextBlock()) {
-            //System.out.println("  END");
-            indexIsCurrent = false;
-            return SeekStatus.END;
-          }
-          common = 0;
-        }
-      }
-
-      @Override
-      public BytesRef next() throws IOException {
-        //System.out.println("BTR.next() seekPending=" + seekPending + " pendingSeekCount=" + state.termBlockOrd);
-
-        // If seek was previously called and the term was cached,
-        // usually caller is just going to pull a D/&PEnum or get
-        // docFreq, etc.  But, if they then call next(),
-        // this method catches up all internal state so next()
-        // works properly:
-        if (seekPending) {
-          assert !indexIsCurrent;
-          in.seek(state.blockFilePointer);
-          final int pendingSeekCount = state.termBlockOrd;
-          boolean result = nextBlock();
-
-          final long savOrd = state.ord;
-
-          // Block must exist since seek(TermState) was called w/ a
-          // TermState previously returned by this enum when positioned
-          // on a real term:
-          assert result;
-
-          while(state.termBlockOrd < pendingSeekCount) {
-            BytesRef nextResult = _next();
-            assert nextResult != null;
-          }
-          seekPending = false;
-          state.ord = savOrd;
-        }
-        return _next();
-      }
-
-      /* Decodes only the term bytes of the next term.  If caller then asks for
-         metadata, ie docFreq, totalTermFreq or pulls a D/&PEnum, we then (lazily)
-         decode all metadata up to the current term. */
-      private BytesRef _next() throws IOException {
-        //System.out.println("BTR._next seg=" + segment + " this=" + this + " termCount=" + state.termBlockOrd + " (vs " + blockTermCount + ")");
-        if (state.termBlockOrd == blockTermCount && !nextBlock()) {
-          //System.out.println("  eof");
-          indexIsCurrent = false;
-          return null;
-        }
-
-        // TODO: cutover to something better for these ints!  simple64?
-        final int suffix = termSuffixesReader.readVInt();
-        //System.out.println("  suffix=" + suffix);
-
-        term.length = termBlockPrefix + suffix;
-        if (term.bytes.length < term.length) {
-          term.grow(term.length);
-        }
-        termSuffixesReader.readBytes(term.bytes, termBlockPrefix, suffix);
-        state.termBlockOrd++;
-
-        // NOTE: meaningless in the non-ord case
-        state.ord++;
-
-        //System.out.println("  return term=" + fieldInfo.name + ":" + term.utf8ToString() + " " + term + " tbOrd=" + state.termBlockOrd);
-        return term;
-      }
-
-      @Override
-      public BytesRef term() {
-        return term;
-      }
-
-      @Override
-      public int docFreq() throws IOException {
-        //System.out.println("BTR.docFreq");
-        decodeMetaData();
-        //System.out.println("  return " + state.docFreq);
-        return state.docFreq;
-      }
-
-      @Override
-      public long totalTermFreq() throws IOException {
-        decodeMetaData();
-        return state.totalTermFreq;
-      }
-
-      @Override
-      public DocsEnum docs(Bits liveDocs, DocsEnum reuse, int flags) throws IOException {
-        //System.out.println("BTR.docs this=" + this);
-        decodeMetaData();
-        //System.out.println("BTR.docs:  state.docFreq=" + state.docFreq);
-        return postingsReader.docs(fieldInfo, state, liveDocs, reuse, flags);
-      }
-
-      @Override
-      public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum reuse, int flags) throws IOException {
-        if (fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) < 0) {
-          // Positions were not indexed:
-          return null;
-        }
-
-        decodeMetaData();
-        return postingsReader.docsAndPositions(fieldInfo, state, liveDocs, reuse, flags);
-      }
-
-      @Override
-      public void seekExact(BytesRef target, TermState otherState) {
-        //System.out.println("BTR.seekExact termState target=" + target.utf8ToString() + " " + target + " this=" + this);
-        assert otherState != null && otherState instanceof BlockTermState;
-        assert !doOrd || ((BlockTermState) otherState).ord < numTerms;
-        state.copyFrom(otherState);
-        seekPending = true;
-        indexIsCurrent = false;
-        term.copyBytes(target);
-      }
-      
-      @Override
-      public TermState termState() throws IOException {
-        //System.out.println("BTR.termState this=" + this);
-        decodeMetaData();
-        TermState ts = state.clone();
-        //System.out.println("  return ts=" + ts);
-        return ts;
-      }
-
-      @Override
-      public void seekExact(long ord) throws IOException {
-        //System.out.println("BTR.seek by ord ord=" + ord);
-        if (indexEnum == null) {
-          throw new IllegalStateException("terms index was not loaded");
-        }
-
-        assert ord < numTerms;
-
-        // TODO: if ord is in same terms block and
-        // after current ord, we should avoid this seek just
-        // like we do in the seek(BytesRef) case
-        in.seek(indexEnum.seek(ord));
-        boolean result = nextBlock();
-
-        // Block must exist since ord < numTerms:
-        assert result;
-
-        indexIsCurrent = true;
-        didIndexNext = false;
-        blocksSinceSeek = 0;
-        seekPending = false;
-
-        state.ord = indexEnum.ord()-1;
-        assert state.ord >= -1: "ord=" + state.ord;
-        term.copyBytes(indexEnum.term());
-
-        // Now, scan:
-        int left = (int) (ord - state.ord);
-        while(left > 0) {
-          final BytesRef term = _next();
-          assert term != null;
-          left--;
-          assert indexIsCurrent;
-        }
-      }
-
-      @Override
-      public long ord() {
-        if (!doOrd) {
-          throw new UnsupportedOperationException();
-        }
-        return state.ord;
-      }
-
-      /* Does initial decode of next block of terms; this
-         doesn't actually decode the docFreq, totalTermFreq,
-         postings details (frq/prx offset, etc.) metadata;
-         it just loads them as byte[] blobs which are then      
-         decoded on-demand if the metadata is ever requested
-         for any term in this block.  This enables terms-only
-         intensive consumes (eg certain MTQs, respelling) to
-         not pay the price of decoding metadata they won't
-         use. */
-      private boolean nextBlock() throws IOException {
-
-        // TODO: we still lazy-decode the byte[] for each
-        // term (the suffix), but, if we decoded
-        // all N terms up front then seeking could do a fast
-        // bsearch w/in the block...
-
-        //System.out.println("BTR.nextBlock() fp=" + in.getFilePointer() + " this=" + this);
-        state.blockFilePointer = in.getFilePointer();
-        blockTermCount = in.readVInt();
-        //System.out.println("  blockTermCount=" + blockTermCount);
-        if (blockTermCount == 0) {
-          return false;
-        }
-        termBlockPrefix = in.readVInt();
-
-        // term suffixes:
-        int len = in.readVInt();
-        if (termSuffixes.length < len) {
-          termSuffixes = new byte[ArrayUtil.oversize(len, 1)];
-        }
-        //System.out.println("  termSuffixes len=" + len);
-        in.readBytes(termSuffixes, 0, len);
-        termSuffixesReader.reset(termSuffixes, 0, len);
-
-        // docFreq, totalTermFreq
-        len = in.readVInt();
-        if (docFreqBytes.length < len) {
-          docFreqBytes = new byte[ArrayUtil.oversize(len, 1)];
-        }
-        //System.out.println("  freq bytes len=" + len);
-        in.readBytes(docFreqBytes, 0, len);
-        freqReader.reset(docFreqBytes, 0, len);
-        metaDataUpto = 0;
-
-        state.termBlockOrd = 0;
-
-        postingsReader.readTermsBlock(in, fieldInfo, state);
-
-        blocksSinceSeek++;
-        indexIsCurrent = indexIsCurrent && (blocksSinceSeek < indexReader.getDivisor());
-        //System.out.println("  indexIsCurrent=" + indexIsCurrent);
-
-        return true;
-      }
-     
-      private void decodeMetaData() throws IOException {
-        //System.out.println("BTR.decodeMetadata mdUpto=" + metaDataUpto + " vs termCount=" + state.termBlockOrd + " state=" + state);
-        if (!seekPending) {
-          // TODO: cutover to random-access API
-          // here.... really stupid that we have to decode N
-          // wasted term metadata just to get to the N+1th
-          // that we really need...
-
-          // lazily catch up on metadata decode:
-          final int limit = state.termBlockOrd;
-          // We must set/incr state.termCount because
-          // postings impl can look at this
-          state.termBlockOrd = metaDataUpto;
-          // TODO: better API would be "jump straight to term=N"???
-          while (metaDataUpto < limit) {
-            //System.out.println("  decode mdUpto=" + metaDataUpto);
-            // TODO: we could make "tiers" of metadata, ie,
-            // decode docFreq/totalTF but don't decode postings
-            // metadata; this way caller could get
-            // docFreq/totalTF w/o paying decode cost for
-            // postings
-
-            // TODO: if docFreq were bulk decoded we could
-            // just skipN here:
-            state.docFreq = freqReader.readVInt();
-            //System.out.println("    dF=" + state.docFreq);
-            if (fieldInfo.getIndexOptions() != IndexOptions.DOCS_ONLY) {
-              state.totalTermFreq = state.docFreq + freqReader.readVLong();
-              //System.out.println("    totTF=" + state.totalTermFreq);
-            }
-
-            postingsReader.nextTerm(fieldInfo, state);
-            metaDataUpto++;
-            state.termBlockOrd++;
-          }
-        } else {
-          //System.out.println("  skip! seekPending");
-        }
-      }
-    }
-  }
-}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/BlockTermsWriter.java b/lucene/codecs/src/java/org/apache/lucene/codecs/BlockTermsWriter.java
deleted file mode 100644
index daf49d4..0000000
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/BlockTermsWriter.java
+++ /dev/null
@@ -1,318 +0,0 @@
-package org.apache.lucene.codecs;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Comparator;
-import java.util.List;
-
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.store.RAMOutputStream;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.RamUsageEstimator;
-
-// TODO: currently we encode all terms between two indexed
-// terms as a block; but, we could decouple the two, ie
-// allow several blocks in between two indexed terms
-
-/**
- * Writes terms dict, block-encoding (column stride) each
- * term's metadata for each set of terms between two
- * index terms.
- *
- * @lucene.experimental
- */
-
-public class BlockTermsWriter extends FieldsConsumer {
-
-  final static String CODEC_NAME = "BLOCK_TERMS_DICT";
-
-  // Initial format
-  public static final int VERSION_START = 0;
-
-  public static final int VERSION_CURRENT = VERSION_START;
-
-  /** Extension of terms file */
-  static final String TERMS_EXTENSION = "tib";
-
-  protected final IndexOutput out;
-  final PostingsWriterBase postingsWriter;
-  final FieldInfos fieldInfos;
-  FieldInfo currentField;
-  private final TermsIndexWriterBase termsIndexWriter;
-  private final List<TermsWriter> fields = new ArrayList<TermsWriter>();
-
-  // private final String segment;
-
-  public BlockTermsWriter(TermsIndexWriterBase termsIndexWriter,
-      SegmentWriteState state, PostingsWriterBase postingsWriter)
-      throws IOException {
-    final String termsFileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, TERMS_EXTENSION);
-    this.termsIndexWriter = termsIndexWriter;
-    out = state.directory.createOutput(termsFileName, state.context);
-    boolean success = false;
-    try {
-      fieldInfos = state.fieldInfos;
-      writeHeader(out);
-      currentField = null;
-      this.postingsWriter = postingsWriter;
-      // segment = state.segmentName;
-      
-      //System.out.println("BTW.init seg=" + state.segmentName);
-      
-      postingsWriter.start(out); // have consumer write its format/header
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(out);
-      }
-    }
-  }
-  
-  protected void writeHeader(IndexOutput out) throws IOException {
-    CodecUtil.writeHeader(out, CODEC_NAME, VERSION_CURRENT); 
-
-    out.writeLong(0);                             // leave space for end index pointer    
-  }
-
-  @Override
-  public TermsConsumer addField(FieldInfo field) throws IOException {
-    //System.out.println("\nBTW.addField seg=" + segment + " field=" + field.name);
-    assert currentField == null || currentField.name.compareTo(field.name) < 0;
-    currentField = field;
-    TermsIndexWriterBase.FieldWriter fieldIndexWriter = termsIndexWriter.addField(field, out.getFilePointer());
-    final TermsWriter terms = new TermsWriter(fieldIndexWriter, field, postingsWriter);
-    fields.add(terms);
-    return terms;
-  }
-
-  @Override
-  public void close() throws IOException {
-
-    try {
-      
-      int nonZeroCount = 0;
-      for(TermsWriter field : fields) {
-        if (field.numTerms > 0) {
-          nonZeroCount++;
-        }
-      }
-
-      final long dirStart = out.getFilePointer();
-
-      out.writeVInt(nonZeroCount);
-      for(TermsWriter field : fields) {
-        if (field.numTerms > 0) {
-          out.writeVInt(field.fieldInfo.number);
-          out.writeVLong(field.numTerms);
-          out.writeVLong(field.termsStartPointer);
-          if (field.fieldInfo.getIndexOptions() != IndexOptions.DOCS_ONLY) {
-            out.writeVLong(field.sumTotalTermFreq);
-          }
-          out.writeVLong(field.sumDocFreq);
-          out.writeVInt(field.docCount);
-        }
-      }
-      writeTrailer(dirStart);
-    } finally {
-      IOUtils.close(out, postingsWriter, termsIndexWriter);
-    }
-  }
-
-  protected void writeTrailer(long dirStart) throws IOException {
-    out.seek(CodecUtil.headerLength(CODEC_NAME));
-    out.writeLong(dirStart);    
-  }
-  
-  private static class TermEntry {
-    public final BytesRef term = new BytesRef();
-    public TermStats stats;
-  }
-
-  class TermsWriter extends TermsConsumer {
-    private final FieldInfo fieldInfo;
-    private final PostingsWriterBase postingsWriter;
-    private final long termsStartPointer;
-    private long numTerms;
-    private final TermsIndexWriterBase.FieldWriter fieldIndexWriter;
-    long sumTotalTermFreq;
-    long sumDocFreq;
-    int docCount;
-
-    private TermEntry[] pendingTerms;
-
-    private int pendingCount;
-
-    TermsWriter(
-        TermsIndexWriterBase.FieldWriter fieldIndexWriter,
-        FieldInfo fieldInfo,
-        PostingsWriterBase postingsWriter) 
-    {
-      this.fieldInfo = fieldInfo;
-      this.fieldIndexWriter = fieldIndexWriter;
-      pendingTerms = new TermEntry[32];
-      for(int i=0;i<pendingTerms.length;i++) {
-        pendingTerms[i] = new TermEntry();
-      }
-      termsStartPointer = out.getFilePointer();
-      postingsWriter.setField(fieldInfo);
-      this.postingsWriter = postingsWriter;
-    }
-    
-    @Override
-    public Comparator<BytesRef> getComparator() {
-      return BytesRef.getUTF8SortedAsUnicodeComparator();
-    }
-
-    @Override
-    public PostingsConsumer startTerm(BytesRef text) throws IOException {
-      //System.out.println("BTW: startTerm term=" + fieldInfo.name + ":" + text.utf8ToString() + " " + text + " seg=" + segment);
-      postingsWriter.startTerm();
-      return postingsWriter;
-    }
-
-    private final BytesRef lastPrevTerm = new BytesRef();
-
-    @Override
-    public void finishTerm(BytesRef text, TermStats stats) throws IOException {
-
-      assert stats.docFreq > 0;
-      //System.out.println("BTW: finishTerm term=" + fieldInfo.name + ":" + text.utf8ToString() + " " + text + " seg=" + segment + " df=" + stats.docFreq);
-
-      final boolean isIndexTerm = fieldIndexWriter.checkIndexTerm(text, stats);
-
-      if (isIndexTerm) {
-        if (pendingCount > 0) {
-          // Instead of writing each term, live, we gather terms
-          // in RAM in a pending buffer, and then write the
-          // entire block in between index terms:
-          flushBlock();
-        }
-        fieldIndexWriter.add(text, stats, out.getFilePointer());
-        //System.out.println("  index term!");
-      }
-
-      if (pendingTerms.length == pendingCount) {
-        final TermEntry[] newArray = new TermEntry[ArrayUtil.oversize(pendingCount+1, RamUsageEstimator.NUM_BYTES_OBJECT_REF)];
-        System.arraycopy(pendingTerms, 0, newArray, 0, pendingCount);
-        for(int i=pendingCount;i<newArray.length;i++) {
-          newArray[i] = new TermEntry();
-        }
-        pendingTerms = newArray;
-      }
-      final TermEntry te = pendingTerms[pendingCount];
-      te.term.copyBytes(text);
-      te.stats = stats;
-
-      pendingCount++;
-
-      postingsWriter.finishTerm(stats);
-      numTerms++;
-    }
-
-    // Finishes all terms in this field
-    @Override
-    public void finish(long sumTotalTermFreq, long sumDocFreq, int docCount) throws IOException {
-      if (pendingCount > 0) {
-        flushBlock();
-      }
-      // EOF marker:
-      out.writeVInt(0);
-
-      this.sumTotalTermFreq = sumTotalTermFreq;
-      this.sumDocFreq = sumDocFreq;
-      this.docCount = docCount;
-      fieldIndexWriter.finish(out.getFilePointer());
-    }
-
-    private int sharedPrefix(BytesRef term1, BytesRef term2) {
-      assert term1.offset == 0;
-      assert term2.offset == 0;
-      int pos1 = 0;
-      int pos1End = pos1 + Math.min(term1.length, term2.length);
-      int pos2 = 0;
-      while(pos1 < pos1End) {
-        if (term1.bytes[pos1] != term2.bytes[pos2]) {
-          return pos1;
-        }
-        pos1++;
-        pos2++;
-      }
-      return pos1;
-    }
-
-    private final RAMOutputStream bytesWriter = new RAMOutputStream();
-
-    private void flushBlock() throws IOException {
-      //System.out.println("BTW.flushBlock seg=" + segment + " pendingCount=" + pendingCount + " fp=" + out.getFilePointer());
-
-      // First pass: compute common prefix for all terms
-      // in the block, against term before first term in
-      // this block:
-      int commonPrefix = sharedPrefix(lastPrevTerm, pendingTerms[0].term);
-      for(int termCount=1;termCount<pendingCount;termCount++) {
-        commonPrefix = Math.min(commonPrefix,
-                                sharedPrefix(lastPrevTerm,
-                                             pendingTerms[termCount].term));
-      }        
-
-      out.writeVInt(pendingCount);
-      out.writeVInt(commonPrefix);
-
-      // 2nd pass: write suffixes, as separate byte[] blob
-      for(int termCount=0;termCount<pendingCount;termCount++) {
-        final int suffix = pendingTerms[termCount].term.length - commonPrefix;
-        // TODO: cutover to better intblock codec, instead
-        // of interleaving here:
-        bytesWriter.writeVInt(suffix);
-        bytesWriter.writeBytes(pendingTerms[termCount].term.bytes, commonPrefix, suffix);
-      }
-      out.writeVInt((int) bytesWriter.getFilePointer());
-      bytesWriter.writeTo(out);
-      bytesWriter.reset();
-
-      // 3rd pass: write the freqs as byte[] blob
-      // TODO: cutover to better intblock codec.  simple64?
-      // write prefix, suffix first:
-      for(int termCount=0;termCount<pendingCount;termCount++) {
-        final TermStats stats = pendingTerms[termCount].stats;
-        assert stats != null;
-        bytesWriter.writeVInt(stats.docFreq);
-        if (fieldInfo.getIndexOptions() != IndexOptions.DOCS_ONLY) {
-          bytesWriter.writeVLong(stats.totalTermFreq-stats.docFreq);
-        }
-      }
-
-      out.writeVInt((int) bytesWriter.getFilePointer());
-      bytesWriter.writeTo(out);
-      bytesWriter.reset();
-
-      postingsWriter.flushTermsBlock(pendingCount, pendingCount);
-      lastPrevTerm.copyBytes(pendingTerms[pendingCount-1].term);
-      pendingCount = 0;
-    }
-  }
-}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/FixedGapTermsIndexReader.java b/lucene/codecs/src/java/org/apache/lucene/codecs/FixedGapTermsIndexReader.java
deleted file mode 100644
index c655b93..0000000
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/FixedGapTermsIndexReader.java
+++ /dev/null
@@ -1,414 +0,0 @@
-package org.apache.lucene.codecs;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.PagedBytes;
-import org.apache.lucene.util.packed.PackedInts;
-
-import java.util.HashMap;
-import java.util.Comparator;
-import java.io.IOException;
-
-import org.apache.lucene.index.IndexFileNames;
-
-/** 
- * TermsIndexReader for simple every Nth terms indexes.
- *
- * @see FixedGapTermsIndexWriter
- * @lucene.experimental 
- */
-public class FixedGapTermsIndexReader extends TermsIndexReaderBase {
-
-  // NOTE: long is overkill here, since this number is 128
-  // by default and only indexDivisor * 128 if you change
-  // the indexDivisor at search time.  But, we use this in a
-  // number of places to multiply out the actual ord, and we
-  // will overflow int during those multiplies.  So to avoid
-  // having to upgrade each multiple to long in multiple
-  // places (error prone), we use long here:
-  private long totalIndexInterval;
-
-  private int indexDivisor;
-  final private int indexInterval;
-
-  // Closed if indexLoaded is true:
-  private IndexInput in;
-  private volatile boolean indexLoaded;
-
-  private final Comparator<BytesRef> termComp;
-
-  private final static int PAGED_BYTES_BITS = 15;
-
-  // all fields share this single logical byte[]
-  private final PagedBytes termBytes = new PagedBytes(PAGED_BYTES_BITS);
-  private PagedBytes.Reader termBytesReader;
-
-  final HashMap<FieldInfo,FieldIndexData> fields = new HashMap<FieldInfo,FieldIndexData>();
-  
-  // start of the field info data
-  protected long dirOffset;
-
-  public FixedGapTermsIndexReader(Directory dir, FieldInfos fieldInfos, String segment, int indexDivisor, Comparator<BytesRef> termComp, String segmentSuffix, IOContext context)
-    throws IOException {
-
-    this.termComp = termComp;
-
-    assert indexDivisor == -1 || indexDivisor > 0;
-
-    in = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, FixedGapTermsIndexWriter.TERMS_INDEX_EXTENSION), context);
-    
-    boolean success = false;
-
-    try {
-      
-      readHeader(in);
-      indexInterval = in.readInt();
-      if (indexInterval < 1) {
-        throw new CorruptIndexException("invalid indexInterval: " + indexInterval + " (resource=" + in + ")");
-      }
-      this.indexDivisor = indexDivisor;
-
-      if (indexDivisor < 0) {
-        totalIndexInterval = indexInterval;
-      } else {
-        // In case terms index gets loaded, later, on demand
-        totalIndexInterval = indexInterval * indexDivisor;
-      }
-      assert totalIndexInterval > 0;
-      
-      seekDir(in, dirOffset);
-
-      // Read directory
-      final int numFields = in.readVInt();     
-      if (numFields < 0) {
-        throw new CorruptIndexException("invalid numFields: " + numFields + " (resource=" + in + ")");
-      }
-      //System.out.println("FGR: init seg=" + segment + " div=" + indexDivisor + " nF=" + numFields);
-      for(int i=0;i<numFields;i++) {
-        final int field = in.readVInt();
-        final int numIndexTerms = in.readVInt();
-        if (numIndexTerms < 0) {
-          throw new CorruptIndexException("invalid numIndexTerms: " + numIndexTerms + " (resource=" + in + ")");
-        }
-        final long termsStart = in.readVLong();
-        final long indexStart = in.readVLong();
-        final long packedIndexStart = in.readVLong();
-        final long packedOffsetsStart = in.readVLong();
-        if (packedIndexStart < indexStart) {
-          throw new CorruptIndexException("invalid packedIndexStart: " + packedIndexStart + " indexStart: " + indexStart + "numIndexTerms: " + numIndexTerms + " (resource=" + in + ")");
-        }
-        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);
-        FieldIndexData previous = fields.put(fieldInfo, new FieldIndexData(fieldInfo, numIndexTerms, indexStart, termsStart, packedIndexStart, packedOffsetsStart));
-        if (previous != null) {
-          throw new CorruptIndexException("duplicate field: " + fieldInfo.name + " (resource=" + in + ")");
-        }
-      }
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(in);
-      }
-      if (indexDivisor > 0) {
-        in.close();
-        in = null;
-        if (success) {
-          indexLoaded = true;
-        }
-        termBytesReader = termBytes.freeze(true);
-      }
-    }
-  }
-  
-  @Override
-  public int getDivisor() {
-    return indexDivisor;
-  }
-
-  protected void readHeader(IndexInput input) throws IOException {
-    CodecUtil.checkHeader(input, FixedGapTermsIndexWriter.CODEC_NAME,
-      FixedGapTermsIndexWriter.VERSION_START, FixedGapTermsIndexWriter.VERSION_START);
-    dirOffset = input.readLong();
-  }
-
-  private class IndexEnum extends FieldIndexEnum {
-    private final FieldIndexData.CoreFieldIndex fieldIndex;
-    private final BytesRef term = new BytesRef();
-    private long ord;
-
-    public IndexEnum(FieldIndexData.CoreFieldIndex fieldIndex) {
-      this.fieldIndex = fieldIndex;
-    }
-
-    @Override
-    public BytesRef term() {
-      return term;
-    }
-
-    @Override
-    public long seek(BytesRef target) {
-      int lo = 0;				  // binary search
-      int hi = fieldIndex.numIndexTerms - 1;
-      assert totalIndexInterval > 0 : "totalIndexInterval=" + totalIndexInterval;
-
-      while (hi >= lo) {
-        int mid = (lo + hi) >>> 1;
-
-        final long offset = fieldIndex.termOffsets.get(mid);
-        final int length = (int) (fieldIndex.termOffsets.get(1+mid) - offset);
-        termBytesReader.fillSlice(term, fieldIndex.termBytesStart + offset, length);
-
-        int delta = termComp.compare(target, term);
-        if (delta < 0) {
-          hi = mid - 1;
-        } else if (delta > 0) {
-          lo = mid + 1;
-        } else {
-          assert mid >= 0;
-          ord = mid*totalIndexInterval;
-          return fieldIndex.termsStart + fieldIndex.termsDictOffsets.get(mid);
-        }
-      }
-
-      if (hi < 0) {
-        assert hi == -1;
-        hi = 0;
-      }
-
-      final long offset = fieldIndex.termOffsets.get(hi);
-      final int length = (int) (fieldIndex.termOffsets.get(1+hi) - offset);
-      termBytesReader.fillSlice(term, fieldIndex.termBytesStart + offset, length);
-
-      ord = hi*totalIndexInterval;
-      return fieldIndex.termsStart + fieldIndex.termsDictOffsets.get(hi);
-    }
-
-    @Override
-    public long next() {
-      final int idx = 1 + (int) (ord / totalIndexInterval);
-      if (idx >= fieldIndex.numIndexTerms) {
-        return -1;
-      }
-      ord += totalIndexInterval;
-
-      final long offset = fieldIndex.termOffsets.get(idx);
-      final int length = (int) (fieldIndex.termOffsets.get(1+idx) - offset);
-      termBytesReader.fillSlice(term, fieldIndex.termBytesStart + offset, length);
-      return fieldIndex.termsStart + fieldIndex.termsDictOffsets.get(idx);
-    }
-
-    @Override
-    public long ord() {
-      return ord;
-    }
-
-    @Override
-    public long seek(long ord) {
-      int idx = (int) (ord / totalIndexInterval);
-      // caller must ensure ord is in bounds
-      assert idx < fieldIndex.numIndexTerms;
-      final long offset = fieldIndex.termOffsets.get(idx);
-      final int length = (int) (fieldIndex.termOffsets.get(1+idx) - offset);
-      termBytesReader.fillSlice(term, fieldIndex.termBytesStart + offset, length);
-      this.ord = idx * totalIndexInterval;
-      return fieldIndex.termsStart + fieldIndex.termsDictOffsets.get(idx);
-    }
-  }
-
-  @Override
-  public boolean supportsOrd() {
-    return true;
-  }
-
-  private final class FieldIndexData {
-
-    volatile CoreFieldIndex coreIndex;
-
-    private final long indexStart;
-    private final long termsStart;
-    private final long packedIndexStart;
-    private final long packedOffsetsStart;
-
-    private final int numIndexTerms;
-
-    public FieldIndexData(FieldInfo fieldInfo, int numIndexTerms, long indexStart, long termsStart, long packedIndexStart,
-                          long packedOffsetsStart) throws IOException {
-
-      this.termsStart = termsStart;
-      this.indexStart = indexStart;
-      this.packedIndexStart = packedIndexStart;
-      this.packedOffsetsStart = packedOffsetsStart;
-      this.numIndexTerms = numIndexTerms;
-
-      if (indexDivisor > 0) {
-        loadTermsIndex();
-      }
-    }
-
-    private void loadTermsIndex() throws IOException {
-      if (coreIndex == null) {
-        coreIndex = new CoreFieldIndex(indexStart, termsStart, packedIndexStart, packedOffsetsStart, numIndexTerms);
-      }
-    }
-
-    private final class CoreFieldIndex {
-
-      // where this field's terms begin in the packed byte[]
-      // data
-      final long termBytesStart;
-
-      // offset into index termBytes
-      final PackedInts.Reader termOffsets;
-
-      // index pointers into main terms dict
-      final PackedInts.Reader termsDictOffsets;
-
-      final int numIndexTerms;
-      final long termsStart;
-
-      public CoreFieldIndex(long indexStart, long termsStart, long packedIndexStart, long packedOffsetsStart, int numIndexTerms) throws IOException {
-
-        this.termsStart = termsStart;
-        termBytesStart = termBytes.getPointer();
-
-        IndexInput clone = in.clone();
-        clone.seek(indexStart);
-
-        // -1 is passed to mean "don't load term index", but
-        // if we are then later loaded it's overwritten with
-        // a real value
-        assert indexDivisor > 0;
-
-        this.numIndexTerms = 1+(numIndexTerms-1) / indexDivisor;
-
-        assert this.numIndexTerms  > 0: "numIndexTerms=" + numIndexTerms + " indexDivisor=" + indexDivisor;
-
-        if (indexDivisor == 1) {
-          // Default (load all index terms) is fast -- slurp in the images from disk:
-          
-          try {
-            final long numTermBytes = packedIndexStart - indexStart;
-            termBytes.copy(clone, numTermBytes);
-
-            // records offsets into main terms dict file
-            termsDictOffsets = PackedInts.getReader(clone);
-            assert termsDictOffsets.size() == numIndexTerms;
-
-            // records offsets into byte[] term data
-            termOffsets = PackedInts.getReader(clone);
-            assert termOffsets.size() == 1+numIndexTerms;
-          } finally {
-            clone.close();
-          }
-        } else {
-          // Get packed iterators
-          final IndexInput clone1 = in.clone();
-          final IndexInput clone2 = in.clone();
-
-          try {
-            // Subsample the index terms
-            clone1.seek(packedIndexStart);
-            final PackedInts.ReaderIterator termsDictOffsetsIter = PackedInts.getReaderIterator(clone1, PackedInts.DEFAULT_BUFFER_SIZE);
-
-            clone2.seek(packedOffsetsStart);
-            final PackedInts.ReaderIterator termOffsetsIter = PackedInts.getReaderIterator(clone2,  PackedInts.DEFAULT_BUFFER_SIZE);
-
-            // TODO: often we can get by w/ fewer bits per
-            // value, below.. .but this'd be more complex:
-            // we'd have to try @ fewer bits and then grow
-            // if we overflowed it.
-
-            PackedInts.Mutable termsDictOffsetsM = PackedInts.getMutable(this.numIndexTerms, termsDictOffsetsIter.getBitsPerValue(), PackedInts.DEFAULT);
-            PackedInts.Mutable termOffsetsM = PackedInts.getMutable(this.numIndexTerms+1, termOffsetsIter.getBitsPerValue(), PackedInts.DEFAULT);
-
-            termsDictOffsets = termsDictOffsetsM;
-            termOffsets = termOffsetsM;
-
-            int upto = 0;
-
-            long termOffsetUpto = 0;
-
-            while(upto < this.numIndexTerms) {
-              // main file offset copies straight over
-              termsDictOffsetsM.set(upto, termsDictOffsetsIter.next());
-
-              termOffsetsM.set(upto, termOffsetUpto);
-
-              long termOffset = termOffsetsIter.next();
-              long nextTermOffset = termOffsetsIter.next();
-              final int numTermBytes = (int) (nextTermOffset - termOffset);
-
-              clone.seek(indexStart + termOffset);
-              assert indexStart + termOffset < clone.length() : "indexStart=" + indexStart + " termOffset=" + termOffset + " len=" + clone.length();
-              assert indexStart + termOffset + numTermBytes < clone.length();
-
-              termBytes.copy(clone, numTermBytes);
-              termOffsetUpto += numTermBytes;
-
-              upto++;
-              if (upto == this.numIndexTerms) {
-                break;
-              }
-
-              // skip terms:
-              termsDictOffsetsIter.next();
-              for(int i=0;i<indexDivisor-2;i++) {
-                termOffsetsIter.next();
-                termsDictOffsetsIter.next();
-              }
-            }
-            termOffsetsM.set(upto, termOffsetUpto);
-
-          } finally {
-            clone1.close();
-            clone2.close();
-            clone.close();
-          }
-        }
-      }
-    }
-  }
-
-  @Override
-  public FieldIndexEnum getFieldEnum(FieldInfo fieldInfo) {
-    final FieldIndexData fieldData = fields.get(fieldInfo);
-    if (fieldData.coreIndex == null) {
-      return null;
-    } else {
-      return new IndexEnum(fieldData.coreIndex);
-    }
-  }
-
-  @Override
-  public void close() throws IOException {
-    if (in != null && !indexLoaded) {
-      in.close();
-    }
-  }
-
-  protected void seekDir(IndexInput input, long dirOffset) throws IOException {
-    input.seek(dirOffset);
-  }
-}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/FixedGapTermsIndexWriter.java b/lucene/codecs/src/java/org/apache/lucene/codecs/FixedGapTermsIndexWriter.java
deleted file mode 100644
index fa82964..0000000
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/FixedGapTermsIndexWriter.java
+++ /dev/null
@@ -1,255 +0,0 @@
-package org.apache.lucene.codecs;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.packed.PackedInts;
-
-import java.util.List;
-import java.util.ArrayList;
-import java.io.IOException;
-
-/**
- * Selects every Nth term as and index term, and hold term
- * bytes (mostly) fully expanded in memory.  This terms index
- * supports seeking by ord.  See {@link
- * VariableGapTermsIndexWriter} for a more memory efficient
- * terms index that does not support seeking by ord.
- *
- * @lucene.experimental */
-public class FixedGapTermsIndexWriter extends TermsIndexWriterBase {
-  protected final IndexOutput out;
-
-  /** Extension of terms index file */
-  static final String TERMS_INDEX_EXTENSION = "tii";
-
-  final static String CODEC_NAME = "SIMPLE_STANDARD_TERMS_INDEX";
-  final static int VERSION_START = 0;
-  final static int VERSION_CURRENT = VERSION_START;
-
-  final private int termIndexInterval;
-
-  private final List<SimpleFieldWriter> fields = new ArrayList<SimpleFieldWriter>();
-  
-  @SuppressWarnings("unused") private final FieldInfos fieldInfos; // unread
-
-  public FixedGapTermsIndexWriter(SegmentWriteState state) throws IOException {
-    final String indexFileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, TERMS_INDEX_EXTENSION);
-    termIndexInterval = state.termIndexInterval;
-    out = state.directory.createOutput(indexFileName, state.context);
-    boolean success = false;
-    try {
-      fieldInfos = state.fieldInfos;
-      writeHeader(out);
-      out.writeInt(termIndexInterval);
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(out);
-      }
-    }
-  }
-  
-  protected void writeHeader(IndexOutput out) throws IOException {
-    CodecUtil.writeHeader(out, CODEC_NAME, VERSION_CURRENT);
-    // Placeholder for dir offset
-    out.writeLong(0);
-  }
-
-  @Override
-  public FieldWriter addField(FieldInfo field, long termsFilePointer) {
-    //System.out.println("FGW: addFfield=" + field.name);
-    SimpleFieldWriter writer = new SimpleFieldWriter(field, termsFilePointer);
-    fields.add(writer);
-    return writer;
-  }
-
-  /** NOTE: if your codec does not sort in unicode code
-   *  point order, you must override this method, to simply
-   *  return indexedTerm.length. */
-  protected int indexedTermPrefixLength(final BytesRef priorTerm, final BytesRef indexedTerm) {
-    // As long as codec sorts terms in unicode codepoint
-    // order, we can safely strip off the non-distinguishing
-    // suffix to save RAM in the loaded terms index.
-    final int idxTermOffset = indexedTerm.offset;
-    final int priorTermOffset = priorTerm.offset;
-    final int limit = Math.min(priorTerm.length, indexedTerm.length);
-    for(int byteIdx=0;byteIdx<limit;byteIdx++) {
-      if (priorTerm.bytes[priorTermOffset+byteIdx] != indexedTerm.bytes[idxTermOffset+byteIdx]) {
-        return byteIdx+1;
-      }
-    }
-    return Math.min(1+priorTerm.length, indexedTerm.length);
-  }
-
-  private class SimpleFieldWriter extends FieldWriter {
-    final FieldInfo fieldInfo;
-    int numIndexTerms;
-    final long indexStart;
-    final long termsStart;
-    long packedIndexStart;
-    long packedOffsetsStart;
-    private long numTerms;
-
-    // TODO: we could conceivably make a PackedInts wrapper
-    // that auto-grows... then we wouldn't force 6 bytes RAM
-    // per index term:
-    private short[] termLengths;
-    private int[] termsPointerDeltas;
-    private long lastTermsPointer;
-    private long totTermLength;
-
-    private final BytesRef lastTerm = new BytesRef();
-
-    SimpleFieldWriter(FieldInfo fieldInfo, long termsFilePointer) {
-      this.fieldInfo = fieldInfo;
-      indexStart = out.getFilePointer();
-      termsStart = lastTermsPointer = termsFilePointer;
-      termLengths = new short[0];
-      termsPointerDeltas = new int[0];
-    }
-
-    @Override
-    public boolean checkIndexTerm(BytesRef text, TermStats stats) throws IOException {
-      // First term is first indexed term:
-      //System.out.println("FGW: checkIndexTerm text=" + text.utf8ToString());
-      if (0 == (numTerms++ % termIndexInterval)) {
-        return true;
-      } else {
-        if (0 == numTerms % termIndexInterval) {
-          // save last term just before next index term so we
-          // can compute wasted suffix
-          lastTerm.copyBytes(text);
-        }
-        return false;
-      }
-    }
-
-    @Override
-    public void add(BytesRef text, TermStats stats, long termsFilePointer) throws IOException {
-      final int indexedTermLength = indexedTermPrefixLength(lastTerm, text);
-      //System.out.println("FGW: add text=" + text.utf8ToString() + " " + text + " fp=" + termsFilePointer);
-
-      // write only the min prefix that shows the diff
-      // against prior term
-      out.writeBytes(text.bytes, text.offset, indexedTermLength);
-
-      if (termLengths.length == numIndexTerms) {
-        termLengths = ArrayUtil.grow(termLengths);
-      }
-      if (termsPointerDeltas.length == numIndexTerms) {
-        termsPointerDeltas = ArrayUtil.grow(termsPointerDeltas);
-      }
-
-      // save delta terms pointer
-      termsPointerDeltas[numIndexTerms] = (int) (termsFilePointer - lastTermsPointer);
-      lastTermsPointer = termsFilePointer;
-
-      // save term length (in bytes)
-      assert indexedTermLength <= Short.MAX_VALUE;
-      termLengths[numIndexTerms] = (short) indexedTermLength;
-      totTermLength += indexedTermLength;
-
-      lastTerm.copyBytes(text);
-      numIndexTerms++;
-    }
-
-    @Override
-    public void finish(long termsFilePointer) throws IOException {
-
-      // write primary terms dict offsets
-      packedIndexStart = out.getFilePointer();
-
-      PackedInts.Writer w = PackedInts.getWriter(out, numIndexTerms, PackedInts.bitsRequired(termsFilePointer), PackedInts.DEFAULT);
-
-      // relative to our indexStart
-      long upto = 0;
-      for(int i=0;i<numIndexTerms;i++) {
-        upto += termsPointerDeltas[i];
-        w.add(upto);
-      }
-      w.finish();
-
-      packedOffsetsStart = out.getFilePointer();
-
-      // write offsets into the byte[] terms
-      w = PackedInts.getWriter(out, 1+numIndexTerms, PackedInts.bitsRequired(totTermLength), PackedInts.DEFAULT);
-      upto = 0;
-      for(int i=0;i<numIndexTerms;i++) {
-        w.add(upto);
-        upto += termLengths[i];
-      }
-      w.add(upto);
-      w.finish();
-
-      // our referrer holds onto us, while other fields are
-      // being written, so don't tie up this RAM:
-      termLengths = null;
-      termsPointerDeltas = null;
-    }
-  }
-
-  public void close() throws IOException {
-    boolean success = false;
-    try {
-      final long dirStart = out.getFilePointer();
-      final int fieldCount = fields.size();
-      
-      int nonNullFieldCount = 0;
-      for(int i=0;i<fieldCount;i++) {
-        SimpleFieldWriter field = fields.get(i);
-        if (field.numIndexTerms > 0) {
-          nonNullFieldCount++;
-        }
-      }
-      
-      out.writeVInt(nonNullFieldCount);
-      for(int i=0;i<fieldCount;i++) {
-        SimpleFieldWriter field = fields.get(i);
-        if (field.numIndexTerms > 0) {
-          out.writeVInt(field.fieldInfo.number);
-          out.writeVInt(field.numIndexTerms);
-          out.writeVLong(field.termsStart);
-          out.writeVLong(field.indexStart);
-          out.writeVLong(field.packedIndexStart);
-          out.writeVLong(field.packedOffsetsStart);
-        }
-      }
-      writeTrailer(dirStart);
-      success = true;
-    } finally {
-      if (success) {
-        IOUtils.close(out);
-      } else {
-        IOUtils.closeWhileHandlingException(out);
-      }
-    }
-  }
-
-  protected void writeTrailer(long dirStart) throws IOException {
-    out.seek(CodecUtil.headerLength(CODEC_NAME));
-    out.writeLong(dirStart);
-  }
-}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/TermsIndexReaderBase.java b/lucene/codecs/src/java/org/apache/lucene/codecs/TermsIndexReaderBase.java
deleted file mode 100644
index 3845305..0000000
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/TermsIndexReaderBase.java
+++ /dev/null
@@ -1,74 +0,0 @@
-package org.apache.lucene.codecs;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.util.BytesRef;
-
-import java.io.IOException;
-import java.io.Closeable;
-
-
-// TODO
-//   - allow for non-regular index intervals?  eg with a
-//     long string of rare terms, you don't need such
-//     frequent indexing
-
-/**
- * {@link BlockTermsReader} interacts with an instance of this class
- * to manage its terms index.  The writer must accept
- * indexed terms (many pairs of BytesRef text + long
- * fileOffset), and then this reader must be able to
- * retrieve the nearest index term to a provided term
- * text. 
- * @lucene.experimental */
-
-public abstract class TermsIndexReaderBase implements Closeable {
-
-  public abstract FieldIndexEnum getFieldEnum(FieldInfo fieldInfo);
-
-  public abstract void close() throws IOException;
-
-  public abstract boolean supportsOrd();
-
-  public abstract int getDivisor();
-
-  /** 
-   * Similar to TermsEnum, except, the only "metadata" it
-   * reports for a given indexed term is the long fileOffset
-   * into the main terms dictionary file.
-   */
-  public static abstract class FieldIndexEnum {
-
-    /** Seeks to "largest" indexed term that's <=
-     *  term; returns file pointer index (into the main
-     *  terms index file) for that term */
-    public abstract long seek(BytesRef term) throws IOException;
-
-    /** Returns -1 at end */
-    public abstract long next() throws IOException;
-
-    public abstract BytesRef term();
-
-    /** Only implemented if {@link TermsIndexReaderBase#supportsOrd()} returns true. */
-    public abstract long seek(long ord) throws IOException;
-    
-    /** Only implemented if {@link TermsIndexReaderBase#supportsOrd()} returns true. */
-    public abstract long ord();
-  }
-}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/TermsIndexWriterBase.java b/lucene/codecs/src/java/org/apache/lucene/codecs/TermsIndexWriterBase.java
deleted file mode 100644
index 477dfde..0000000
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/TermsIndexWriterBase.java
+++ /dev/null
@@ -1,45 +0,0 @@
-package org.apache.lucene.codecs;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.util.BytesRef;
-
-import java.io.Closeable;
-import java.io.IOException;
-
-/** 
- * Base class for terms index implementations to plug
- * into {@link BlockTermsWriter}.
- * 
- * @see TermsIndexReaderBase
- * @lucene.experimental 
- */
-public abstract class TermsIndexWriterBase implements Closeable {
-
-  /**
-   * Terms index API for a single field.
-   */
-  public abstract class FieldWriter {
-    public abstract boolean checkIndexTerm(BytesRef text, TermStats stats) throws IOException;
-    public abstract void add(BytesRef text, TermStats stats, long termsFilePointer) throws IOException;
-    public abstract void finish(long termsFilePointer) throws IOException;
-  }
-
-  public abstract FieldWriter addField(FieldInfo fieldInfo, long termsFilePointer) throws IOException;
-}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/VariableGapTermsIndexReader.java b/lucene/codecs/src/java/org/apache/lucene/codecs/VariableGapTermsIndexReader.java
deleted file mode 100644
index 6a78349..0000000
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/VariableGapTermsIndexReader.java
+++ /dev/null
@@ -1,234 +0,0 @@
-package org.apache.lucene.codecs;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.io.FileOutputStream;   // for toDot
-import java.io.OutputStreamWriter; // for toDot
-import java.io.Writer;             // for toDot
-import java.util.HashMap;
-
-import org.apache.lucene.index.CorruptIndexException;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.store.IOContext;
-import org.apache.lucene.store.IndexInput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IntsRef;
-import org.apache.lucene.util.fst.Builder;
-import org.apache.lucene.util.fst.BytesRefFSTEnum;
-import org.apache.lucene.util.fst.FST;
-import org.apache.lucene.util.fst.PositiveIntOutputs;
-import org.apache.lucene.util.fst.Util; // for toDot
-
-/** See {@link VariableGapTermsIndexWriter}
- * 
- * @lucene.experimental */
-public class VariableGapTermsIndexReader extends TermsIndexReaderBase {
-
-  private final PositiveIntOutputs fstOutputs = PositiveIntOutputs.getSingleton(true);
-  private int indexDivisor;
-
-  // Closed if indexLoaded is true:
-  private IndexInput in;
-  private volatile boolean indexLoaded;
-
-  final HashMap<FieldInfo,FieldIndexData> fields = new HashMap<FieldInfo,FieldIndexData>();
-  
-  // start of the field info data
-  protected long dirOffset;
-
-  final String segment;
-  public VariableGapTermsIndexReader(Directory dir, FieldInfos fieldInfos, String segment, int indexDivisor, String segmentSuffix, IOContext context)
-    throws IOException {
-    in = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, VariableGapTermsIndexWriter.TERMS_INDEX_EXTENSION), new IOContext(context, true));
-    this.segment = segment;
-    boolean success = false;
-    assert indexDivisor == -1 || indexDivisor > 0;
-
-    try {
-      
-      readHeader(in);
-      this.indexDivisor = indexDivisor;
-
-      seekDir(in, dirOffset);
-
-      // Read directory
-      final int numFields = in.readVInt();
-      if (numFields < 0) {
-        throw new CorruptIndexException("invalid numFields: " + numFields + " (resource=" + in + ")");
-      }
-
-      for(int i=0;i<numFields;i++) {
-        final int field = in.readVInt();
-        final long indexStart = in.readVLong();
-        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);
-        FieldIndexData previous = fields.put(fieldInfo, new FieldIndexData(fieldInfo, indexStart));
-        if (previous != null) {
-          throw new CorruptIndexException("duplicate field: " + fieldInfo.name + " (resource=" + in + ")");
-        }
-      }
-      success = true;
-    } finally {
-      if (indexDivisor > 0) {
-        in.close();
-        in = null;
-        if (success) {
-          indexLoaded = true;
-        }
-      }
-    }
-  }
-
-  @Override
-  public int getDivisor() {
-    return indexDivisor;
-  }
-  
-  protected void readHeader(IndexInput input) throws IOException {
-    CodecUtil.checkHeader(input, VariableGapTermsIndexWriter.CODEC_NAME,
-      VariableGapTermsIndexWriter.VERSION_START, VariableGapTermsIndexWriter.VERSION_START);
-    dirOffset = input.readLong();
-  }
-
-  private static class IndexEnum extends FieldIndexEnum {
-    private final BytesRefFSTEnum<Long> fstEnum;
-    private BytesRefFSTEnum.InputOutput<Long> current;
-
-    public IndexEnum(FST<Long> fst) {
-      fstEnum = new BytesRefFSTEnum<Long>(fst);
-    }
-
-    @Override
-    public BytesRef term() {
-      if (current == null) {
-        return null;
-      } else {
-        return current.input;
-      }
-    }
-
-    @Override
-    public long seek(BytesRef target) throws IOException {
-      //System.out.println("VGR: seek field=" + fieldInfo.name + " target=" + target);
-      current = fstEnum.seekFloor(target);
-      //System.out.println("  got input=" + current.input + " output=" + current.output);
-      return current.output;
-    }
-
-    @Override
-    public long next() throws IOException {
-      //System.out.println("VGR: next field=" + fieldInfo.name);
-      current = fstEnum.next();
-      if (current == null) {
-        //System.out.println("  eof");
-        return -1;
-      } else {
-        return current.output;
-      }
-    }
-
-    @Override
-    public long ord() {
-      throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public long seek(long ord) {
-      throw new UnsupportedOperationException();
-    }
-  }
-
-  @Override
-  public boolean supportsOrd() {
-    return false;
-  }
-
-  private final class FieldIndexData {
-
-    private final long indexStart;
-    // Set only if terms index is loaded:
-    private volatile FST<Long> fst;
-
-    public FieldIndexData(FieldInfo fieldInfo, long indexStart) throws IOException {
-      this.indexStart = indexStart;
-
-      if (indexDivisor > 0) {
-        loadTermsIndex();
-      }
-    }
-
-    private void loadTermsIndex() throws IOException {
-      if (fst == null) {
-        IndexInput clone = in.clone();
-        clone.seek(indexStart);
-        fst = new FST<Long>(clone, fstOutputs);
-        clone.close();
-
-        /*
-        final String dotFileName = segment + "_" + fieldInfo.name + ".dot";
-        Writer w = new OutputStreamWriter(new FileOutputStream(dotFileName));
-        Util.toDot(fst, w, false, false);
-        System.out.println("FST INDEX: SAVED to " + dotFileName);
-        w.close();
-        */
-
-        if (indexDivisor > 1) {
-          // subsample
-          final IntsRef scratchIntsRef = new IntsRef();
-          final PositiveIntOutputs outputs = PositiveIntOutputs.getSingleton(true);
-          final Builder<Long> builder = new Builder<Long>(FST.INPUT_TYPE.BYTE1, outputs);
-          final BytesRefFSTEnum<Long> fstEnum = new BytesRefFSTEnum<Long>(fst);
-          BytesRefFSTEnum.InputOutput<Long> result;
-          int count = indexDivisor;
-          while((result = fstEnum.next()) != null) {
-            if (count == indexDivisor) {
-              builder.add(Util.toIntsRef(result.input, scratchIntsRef), result.output);
-              count = 0;
-            }
-            count++;
-          }
-          fst = builder.finish();
-        }
-      }
-    }
-  }
-
-  @Override
-  public FieldIndexEnum getFieldEnum(FieldInfo fieldInfo) {
-    final FieldIndexData fieldData = fields.get(fieldInfo);
-    if (fieldData.fst == null) {
-      return null;
-    } else {
-      return new IndexEnum(fieldData.fst);
-    }
-  }
-
-  @Override
-  public void close() throws IOException {
-    if (in != null && !indexLoaded) {
-      in.close();
-    }
-  }
-
-  protected void seekDir(IndexInput input, long dirOffset) throws IOException {
-    input.seek(dirOffset);
-  }
-}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/VariableGapTermsIndexWriter.java b/lucene/codecs/src/java/org/apache/lucene/codecs/VariableGapTermsIndexWriter.java
deleted file mode 100644
index 2156fbc..0000000
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/VariableGapTermsIndexWriter.java
+++ /dev/null
@@ -1,321 +0,0 @@
-package org.apache.lucene.codecs;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.FieldInfos;
-import org.apache.lucene.index.IndexFileNames;
-import org.apache.lucene.index.SegmentWriteState;
-import org.apache.lucene.store.IndexOutput;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.IOUtils;
-import org.apache.lucene.util.IntsRef;
-import org.apache.lucene.util.fst.Builder;
-import org.apache.lucene.util.fst.FST;
-import org.apache.lucene.util.fst.PositiveIntOutputs;
-import org.apache.lucene.util.fst.Util;
-
-/**
- * Selects index terms according to provided pluggable
- * {@link IndexTermSelector}, and stores them in a prefix trie that's
- * loaded entirely in RAM stored as an FST.  This terms
- * index only supports unsigned byte term sort order
- * (unicode codepoint order when the bytes are UTF8).
- *
- * @lucene.experimental */
-public class VariableGapTermsIndexWriter extends TermsIndexWriterBase {
-  protected final IndexOutput out;
-
-  /** Extension of terms index file */
-  static final String TERMS_INDEX_EXTENSION = "tiv";
-
-  final static String CODEC_NAME = "VARIABLE_GAP_TERMS_INDEX";
-  final static int VERSION_START = 0;
-  final static int VERSION_CURRENT = VERSION_START;
-
-  private final List<FSTFieldWriter> fields = new ArrayList<FSTFieldWriter>();
-  
-  @SuppressWarnings("unused") private final FieldInfos fieldInfos; // unread
-  private final IndexTermSelector policy;
-
-  /** 
-   * Hook for selecting which terms should be placed in the terms index.
-   * <p>
-   * {@link #newField} is called at the start of each new field, and
-   * {@link #isIndexTerm} for each term in that field.
-   * 
-   * @lucene.experimental 
-   */
-  public static abstract class IndexTermSelector {
-    /** 
-     * Called sequentially on every term being written,
-     * returning true if this term should be indexed
-     */
-    public abstract boolean isIndexTerm(BytesRef term, TermStats stats);
-    /**
-     * Called when a new field is started.
-     */
-    public abstract void newField(FieldInfo fieldInfo);
-  }
-
-  /** Same policy as {@link FixedGapTermsIndexWriter} */
-  public static final class EveryNTermSelector extends IndexTermSelector {
-    private int count;
-    private final int interval;
-
-    public EveryNTermSelector(int interval) {
-      this.interval = interval;
-      // First term is first indexed term:
-      count = interval;
-    }
-
-    @Override
-    public boolean isIndexTerm(BytesRef term, TermStats stats) {
-      if (count >= interval) {
-        count = 1;
-        return true;
-      } else {
-        count++;
-        return false;
-      }
-    }
-
-    @Override
-    public void newField(FieldInfo fieldInfo) {
-      count = interval;
-    }
-  }
-
-  /** Sets an index term when docFreq >= docFreqThresh, or
-   *  every interval terms.  This should reduce seek time
-   *  to high docFreq terms.  */
-  public static final class EveryNOrDocFreqTermSelector extends IndexTermSelector {
-    private int count;
-    private final int docFreqThresh;
-    private final int interval;
-
-    public EveryNOrDocFreqTermSelector(int docFreqThresh, int interval) {
-      this.interval = interval;
-      this.docFreqThresh = docFreqThresh;
-
-      // First term is first indexed term:
-      count = interval;
-    }
-
-    @Override
-    public boolean isIndexTerm(BytesRef term, TermStats stats) {
-      if (stats.docFreq >= docFreqThresh || count >= interval) {
-        count = 1;
-        return true;
-      } else {
-        count++;
-        return false;
-      }
-    }
-
-    @Override
-    public void newField(FieldInfo fieldInfo) {
-      count = interval;
-    }
-  }
-
-  // TODO: it'd be nice to let the FST builder prune based
-  // on term count of each node (the prune1/prune2 that it
-  // accepts), and build the index based on that.  This
-  // should result in a more compact terms index, more like
-  // a prefix trie than the other selectors, because it
-  // only stores enough leading bytes to get down to N
-  // terms that may complete that prefix.  It becomes
-  // "deeper" when terms are dense, and "shallow" when they
-  // are less dense.
-  //
-  // However, it's not easy to make that work this this
-  // API, because that pruning doesn't immediately know on
-  // seeing each term whether that term will be a seek point
-  // or not.  It requires some non-causality in the API, ie
-  // only on seeing some number of future terms will the
-  // builder decide which past terms are seek points.
-  // Somehow the API'd need to be able to return a "I don't
-  // know" value, eg like a Future, which only later on is
-  // flipped (frozen) to true or false.
-  //
-  // We could solve this with a 2-pass approach, where the
-  // first pass would build an FSA (no outputs) solely to
-  // determine which prefixes are the 'leaves' in the
-  // pruning. The 2nd pass would then look at this prefix
-  // trie to mark the seek points and build the FST mapping
-  // to the true output.
-  //
-  // But, one downside to this approach is that it'd result
-  // in uneven index term selection.  EG with prune1=10, the
-  // resulting index terms could be as frequent as every 10
-  // terms or as rare as every <maxArcCount> * 10 (eg 2560),
-  // in the extremes.
-
-  public VariableGapTermsIndexWriter(SegmentWriteState state, IndexTermSelector policy) throws IOException {
-    final String indexFileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, TERMS_INDEX_EXTENSION);
-    out = state.directory.createOutput(indexFileName, state.context);
-    boolean success = false;
-    try {
-      fieldInfos = state.fieldInfos;
-      this.policy = policy;
-      writeHeader(out);
-      success = true;
-    } finally {
-      if (!success) {
-        IOUtils.closeWhileHandlingException(out);
-      }
-    }
-  }
-  
-  protected void writeHeader(IndexOutput out) throws IOException {
-    CodecUtil.writeHeader(out, CODEC_NAME, VERSION_CURRENT);
-    // Placeholder for dir offset
-    out.writeLong(0);
-  }
-
-  @Override
-  public FieldWriter addField(FieldInfo field, long termsFilePointer) throws IOException {
-    ////System.out.println("VGW: field=" + field.name);
-    policy.newField(field);
-    FSTFieldWriter writer = new FSTFieldWriter(field, termsFilePointer);
-    fields.add(writer);
-    return writer;
-  }
-
-  /** NOTE: if your codec does not sort in unicode code
-   *  point order, you must override this method, to simply
-   *  return indexedTerm.length. */
-  protected int indexedTermPrefixLength(final BytesRef priorTerm, final BytesRef indexedTerm) {
-    // As long as codec sorts terms in unicode codepoint
-    // order, we can safely strip off the non-distinguishing
-    // suffix to save RAM in the loaded terms index.
-    final int idxTermOffset = indexedTerm.offset;
-    final int priorTermOffset = priorTerm.offset;
-    final int limit = Math.min(priorTerm.length, indexedTerm.length);
-    for(int byteIdx=0;byteIdx<limit;byteIdx++) {
-      if (priorTerm.bytes[priorTermOffset+byteIdx] != indexedTerm.bytes[idxTermOffset+byteIdx]) {
-        return byteIdx+1;
-      }
-    }
-    return Math.min(1+priorTerm.length, indexedTerm.length);
-  }
-
-  private class FSTFieldWriter extends FieldWriter {
-    private final Builder<Long> fstBuilder;
-    private final PositiveIntOutputs fstOutputs;
-    private final long startTermsFilePointer;
-
-    final FieldInfo fieldInfo;
-    FST<Long> fst;
-    final long indexStart;
-
-    private final BytesRef lastTerm = new BytesRef();
-    private boolean first = true;
-
-    public FSTFieldWriter(FieldInfo fieldInfo, long termsFilePointer) throws IOException {
-      this.fieldInfo = fieldInfo;
-      fstOutputs = PositiveIntOutputs.getSingleton(true);
-      fstBuilder = new Builder<Long>(FST.INPUT_TYPE.BYTE1, fstOutputs);
-      indexStart = out.getFilePointer();
-      ////System.out.println("VGW: field=" + fieldInfo.name);
-
-      // Always put empty string in
-      fstBuilder.add(new IntsRef(), termsFilePointer);
-      startTermsFilePointer = termsFilePointer;
-    }
-
-    @Override
-    public boolean checkIndexTerm(BytesRef text, TermStats stats) throws IOException {
-      //System.out.println("VGW: index term=" + text.utf8ToString());
-      // NOTE: we must force the first term per field to be
-      // indexed, in case policy doesn't:
-      if (policy.isIndexTerm(text, stats) || first) {
-        first = false;
-        //System.out.println("  YES");
-        return true;
-      } else {
-        lastTerm.copyBytes(text);
-        return false;
-      }
-    }
-
-    private final IntsRef scratchIntsRef = new IntsRef();
-
-    @Override
-    public void add(BytesRef text, TermStats stats, long termsFilePointer) throws IOException {
-      if (text.length == 0) {
-        // We already added empty string in ctor
-        assert termsFilePointer == startTermsFilePointer;
-        return;
-      }
-      final int lengthSave = text.length;
-      text.length = indexedTermPrefixLength(lastTerm, text);
-      try {
-        fstBuilder.add(Util.toIntsRef(text, scratchIntsRef), termsFilePointer);
-      } finally {
-        text.length = lengthSave;
-      }
-      lastTerm.copyBytes(text);
-    }
-
-    @Override
-    public void finish(long termsFilePointer) throws IOException {
-      fst = fstBuilder.finish();
-      if (fst != null) {
-        fst.save(out);
-      }
-    }
-  }
-
-  public void close() throws IOException {
-    try {
-    final long dirStart = out.getFilePointer();
-    final int fieldCount = fields.size();
-
-    int nonNullFieldCount = 0;
-    for(int i=0;i<fieldCount;i++) {
-      FSTFieldWriter field = fields.get(i);
-      if (field.fst != null) {
-        nonNullFieldCount++;
-      }
-    }
-
-    out.writeVInt(nonNullFieldCount);
-    for(int i=0;i<fieldCount;i++) {
-      FSTFieldWriter field = fields.get(i);
-      if (field.fst != null) {
-        out.writeVInt(field.fieldInfo.number);
-        out.writeVLong(field.indexStart);
-      }
-    }
-    writeTrailer(dirStart);
-    } finally {
-    out.close();
-  }
-  }
-
-  protected void writeTrailer(long dirStart) throws IOException {
-    out.seek(CodecUtil.headerLength(CODEC_NAME));
-    out.writeLong(dirStart);
-  }
-}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/block/BlockPostingsFormat.java b/lucene/codecs/src/java/org/apache/lucene/codecs/block/BlockPostingsFormat.java
index 6fafbd6..b9b22ba 100644
--- a/lucene/codecs/src/java/org/apache/lucene/codecs/block/BlockPostingsFormat.java
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/block/BlockPostingsFormat.java
@@ -140,7 +140,7 @@ import org.apache.lucene.util.packed.PackedInts;
  * <p>Notes:</p>
  * <ul>
  *    <li>Here explains MetadataBlock only, other fields are mentioned in 
- *   <a href="../lucene40/Lucene40PostingsFormat.html#Termdictionary">Lucene40PostingsFormat:TermDictionary</a>
+ *   <a href="{@docRoot}/../core/org/apache/lucene/codecs/lucene40/Lucene40PostingsFormat.html#Termdictionary">Lucene40PostingsFormat:TermDictionary</a>
  *    </li>
  *    <li>PackedBlockSize is the fixed block size for packed blocks. In packed block, bit width is 
  *        determined by the largest integer. Smaller block size result in smaller variance among width 
@@ -176,7 +176,7 @@ import org.apache.lucene.util.packed.PackedInts;
  * <dd>
  * <b>Term Index</b>
  * <p>The .tim file format is mentioned in
- *   <a href="../lucene40/Lucene40PostingsFormat.html#Termindex">Lucene40PostingsFormat:TermIndex</a>
+ *   <a href="{@docRoot}/../core/org/apache/lucene/codecs/lucene40/Lucene40PostingsFormat.html#Termindex">Lucene40PostingsFormat:TermIndex</a>
  * </dd>
  * </dl>
  *
@@ -222,7 +222,7 @@ import org.apache.lucene.util.packed.PackedInts;
  *   </li>
  *   <li>VIntBlock stores remaining d-gaps (along with frequencies when possible) with a format 
  *       mentioned in
- *   <a href="../lucene40/Lucene40PostingsFormat.html#Frequencies">Lucene40PostingsFormat:Frequencies</a>
+ *   <a href="{@docRoot}/../core/org/apache/lucene/codecs/lucene40/Lucene40PostingsFormat.html#Frequencies">Lucene40PostingsFormat:Frequencies</a>
  *   </li>
  *   <li>PackedDocBlockNum is the number of packed blocks for current term's docids or frequencies. 
  *       In particular, PackedDocBlockNum = floor(DocFreq/PackedBlockSize) </li>
@@ -284,7 +284,7 @@ import org.apache.lucene.util.packed.PackedInts;
  *   <li>The procedure how PackedPosDeltaBlock is generated is the same as PackedDocDeltaBlock 
  *       in chapter <a href="#Frequencies">Frequencies and Skip Data</a>.</li>
  *   <li>PosDelta is the same as the format mentioned in 
- *   <a href="../lucene40/Lucene40PostingsFormat.html#Positions">Lucene40PostingsFormat:Positions</a>
+ *   <a href="{@docRoot}/../core/org/apache/lucene/codecs/lucene40/Lucene40PostingsFormat.html#Positions">Lucene40PostingsFormat:Positions</a>
  *   </li>
  *   <li>OffsetStartDelta is the difference between this position's startOffset from the previous 
  *       occurrence (or zero, if this is the first occurrence in this document).</li>
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsReader.java b/lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsReader.java
new file mode 100644
index 0000000..da919a9
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsReader.java
@@ -0,0 +1,874 @@
+package org.apache.lucene.codecs.blockterms;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.Collections;
+import java.util.Comparator;
+import java.util.Iterator;
+import java.util.TreeMap;
+
+import org.apache.lucene.codecs.BlockTermState;
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.FieldsProducer;
+import org.apache.lucene.codecs.PostingsReaderBase;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.DocsAndPositionsEnum;
+import org.apache.lucene.index.DocsEnum;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentInfo;
+import org.apache.lucene.index.TermState;
+import org.apache.lucene.index.Terms;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.store.ByteArrayDataInput;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.Bits;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.DoubleBarrelLRUCache;
+
+/** Handles a terms dict, but decouples all details of
+ *  doc/freqs/positions reading to an instance of {@link
+ *  PostingsReaderBase}.  This class is reusable for
+ *  codecs that use a different format for
+ *  docs/freqs/positions (though codecs are also free to
+ *  make their own terms dict impl).
+ *
+ * <p>This class also interacts with an instance of {@link
+ * TermsIndexReaderBase}, to abstract away the specific
+ * implementation of the terms dict index. 
+ * @lucene.experimental */
+
+public class BlockTermsReader extends FieldsProducer {
+  // Open input to the main terms dict file (_X.tis)
+  private final IndexInput in;
+
+  // Reads the terms dict entries, to gather state to
+  // produce DocsEnum on demand
+  private final PostingsReaderBase postingsReader;
+
+  private final TreeMap<String,FieldReader> fields = new TreeMap<String,FieldReader>();
+
+  // Caches the most recently looked-up field + terms:
+  private final DoubleBarrelLRUCache<FieldAndTerm,BlockTermState> termsCache;
+
+  // Reads the terms index
+  private TermsIndexReaderBase indexReader;
+
+  // keeps the dirStart offset
+  protected long dirOffset;
+
+  // Used as key for the terms cache
+  private static class FieldAndTerm extends DoubleBarrelLRUCache.CloneableKey {
+    String field;
+    BytesRef term;
+
+    public FieldAndTerm() {
+    }
+
+    public FieldAndTerm(FieldAndTerm other) {
+      field = other.field;
+      term = BytesRef.deepCopyOf(other.term);
+    }
+
+    @Override
+    public boolean equals(Object _other) {
+      FieldAndTerm other = (FieldAndTerm) _other;
+      return other.field.equals(field) && term.bytesEquals(other.term);
+    }
+
+    @Override
+    public FieldAndTerm clone() {
+      return new FieldAndTerm(this);
+    }
+
+    @Override
+    public int hashCode() {
+      return field.hashCode() * 31 + term.hashCode();
+    }
+  }
+  
+  // private String segment;
+  
+  public BlockTermsReader(TermsIndexReaderBase indexReader, Directory dir, FieldInfos fieldInfos, SegmentInfo info, PostingsReaderBase postingsReader, IOContext context,
+                          int termsCacheSize, String segmentSuffix)
+    throws IOException {
+    
+    this.postingsReader = postingsReader;
+    termsCache = new DoubleBarrelLRUCache<FieldAndTerm,BlockTermState>(termsCacheSize);
+
+    // this.segment = segment;
+    in = dir.openInput(IndexFileNames.segmentFileName(info.name, segmentSuffix, BlockTermsWriter.TERMS_EXTENSION),
+                       context);
+
+    boolean success = false;
+    try {
+      readHeader(in);
+
+      // Have PostingsReader init itself
+      postingsReader.init(in);
+
+      // Read per-field details
+      seekDir(in, dirOffset);
+
+      final int numFields = in.readVInt();
+      if (numFields < 0) {
+        throw new CorruptIndexException("invalid number of fields: " + numFields + " (resource=" + in + ")");
+      }
+      for(int i=0;i<numFields;i++) {
+        final int field = in.readVInt();
+        final long numTerms = in.readVLong();
+        assert numTerms >= 0;
+        final long termsStartPointer = in.readVLong();
+        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);
+        final long sumTotalTermFreq = fieldInfo.getIndexOptions() == IndexOptions.DOCS_ONLY ? -1 : in.readVLong();
+        final long sumDocFreq = in.readVLong();
+        final int docCount = in.readVInt();
+        if (docCount < 0 || docCount > info.getDocCount()) { // #docs with field must be <= #docs
+          throw new CorruptIndexException("invalid docCount: " + docCount + " maxDoc: " + info.getDocCount() + " (resource=" + in + ")");
+        }
+        if (sumDocFreq < docCount) {  // #postings must be >= #docs with field
+          throw new CorruptIndexException("invalid sumDocFreq: " + sumDocFreq + " docCount: " + docCount + " (resource=" + in + ")");
+        }
+        if (sumTotalTermFreq != -1 && sumTotalTermFreq < sumDocFreq) { // #positions must be >= #postings
+          throw new CorruptIndexException("invalid sumTotalTermFreq: " + sumTotalTermFreq + " sumDocFreq: " + sumDocFreq + " (resource=" + in + ")");
+        }
+        FieldReader previous = fields.put(fieldInfo.name, new FieldReader(fieldInfo, numTerms, termsStartPointer, sumTotalTermFreq, sumDocFreq, docCount));
+        if (previous != null) {
+          throw new CorruptIndexException("duplicate fields: " + fieldInfo.name + " (resource=" + in + ")");
+        }
+      }
+      success = true;
+    } finally {
+      if (!success) {
+        in.close();
+      }
+    }
+
+    this.indexReader = indexReader;
+  }
+
+  protected void readHeader(IndexInput input) throws IOException {
+    CodecUtil.checkHeader(input, BlockTermsWriter.CODEC_NAME,
+                          BlockTermsWriter.VERSION_START,
+                          BlockTermsWriter.VERSION_CURRENT);
+    dirOffset = input.readLong();
+  }
+  
+  protected void seekDir(IndexInput input, long dirOffset)
+      throws IOException {
+    input.seek(dirOffset);
+  }
+  
+  @Override
+  public void close() throws IOException {
+    try {
+      try {
+        if (indexReader != null) {
+          indexReader.close();
+        }
+      } finally {
+        // null so if an app hangs on to us (ie, we are not
+        // GCable, despite being closed) we still free most
+        // ram
+        indexReader = null;
+        if (in != null) {
+          in.close();
+        }
+      }
+    } finally {
+      if (postingsReader != null) {
+        postingsReader.close();
+      }
+    }
+  }
+
+  @Override
+  public Iterator<String> iterator() {
+    return Collections.unmodifiableSet(fields.keySet()).iterator();
+  }
+
+  @Override
+  public Terms terms(String field) throws IOException {
+    assert field != null;
+    return fields.get(field);
+  }
+
+  @Override
+  public int size() {
+    return fields.size();
+  }
+
+  private class FieldReader extends Terms {
+    final long numTerms;
+    final FieldInfo fieldInfo;
+    final long termsStartPointer;
+    final long sumTotalTermFreq;
+    final long sumDocFreq;
+    final int docCount;
+
+    FieldReader(FieldInfo fieldInfo, long numTerms, long termsStartPointer, long sumTotalTermFreq, long sumDocFreq, int docCount) {
+      assert numTerms > 0;
+      this.fieldInfo = fieldInfo;
+      this.numTerms = numTerms;
+      this.termsStartPointer = termsStartPointer;
+      this.sumTotalTermFreq = sumTotalTermFreq;
+      this.sumDocFreq = sumDocFreq;
+      this.docCount = docCount;
+    }
+
+    @Override
+    public Comparator<BytesRef> getComparator() {
+      return BytesRef.getUTF8SortedAsUnicodeComparator();
+    }
+
+    @Override
+    public TermsEnum iterator(TermsEnum reuse) throws IOException {
+      return new SegmentTermsEnum();
+    }
+
+    @Override
+    public boolean hasOffsets() {
+      return fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS) >= 0;
+    }
+
+    @Override
+    public boolean hasPositions() {
+      return fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) >= 0;
+    }
+    
+    @Override
+    public boolean hasPayloads() {
+      return fieldInfo.hasPayloads();
+    }
+
+    @Override
+    public long size() {
+      return numTerms;
+    }
+
+    @Override
+    public long getSumTotalTermFreq() {
+      return sumTotalTermFreq;
+    }
+
+    @Override
+    public long getSumDocFreq() throws IOException {
+      return sumDocFreq;
+    }
+
+    @Override
+    public int getDocCount() throws IOException {
+      return docCount;
+    }
+
+    // Iterates through terms in this field
+    private final class SegmentTermsEnum extends TermsEnum {
+      private final IndexInput in;
+      private final BlockTermState state;
+      private final boolean doOrd;
+      private final FieldAndTerm fieldTerm = new FieldAndTerm();
+      private final TermsIndexReaderBase.FieldIndexEnum indexEnum;
+      private final BytesRef term = new BytesRef();
+
+      /* This is true if indexEnum is "still" seek'd to the index term
+         for the current term. We set it to true on seeking, and then it
+         remains valid until next() is called enough times to load another
+         terms block: */
+      private boolean indexIsCurrent;
+
+      /* True if we've already called .next() on the indexEnum, to "bracket"
+         the current block of terms: */
+      private boolean didIndexNext;
+
+      /* Next index term, bracketing the current block of terms; this is
+         only valid if didIndexNext is true: */
+      private BytesRef nextIndexTerm;
+
+      /* True after seekExact(TermState), do defer seeking.  If the app then
+         calls next() (which is not "typical"), then we'll do the real seek */
+      private boolean seekPending;
+
+      /* How many blocks we've read since last seek.  Once this
+         is >= indexEnum.getDivisor() we set indexIsCurrent to false (since
+         the index can no long bracket seek-within-block). */
+      private int blocksSinceSeek;
+
+      private byte[] termSuffixes;
+      private ByteArrayDataInput termSuffixesReader = new ByteArrayDataInput();
+
+      /* Common prefix used for all terms in this block. */
+      private int termBlockPrefix;
+
+      /* How many terms in current block */
+      private int blockTermCount;
+
+      private byte[] docFreqBytes;
+      private final ByteArrayDataInput freqReader = new ByteArrayDataInput();
+      private int metaDataUpto;
+
+      public SegmentTermsEnum() throws IOException {
+        in = BlockTermsReader.this.in.clone();
+        in.seek(termsStartPointer);
+        indexEnum = indexReader.getFieldEnum(fieldInfo);
+        doOrd = indexReader.supportsOrd();
+        fieldTerm.field = fieldInfo.name;
+        state = postingsReader.newTermState();
+        state.totalTermFreq = -1;
+        state.ord = -1;
+
+        termSuffixes = new byte[128];
+        docFreqBytes = new byte[64];
+        //System.out.println("BTR.enum init this=" + this + " postingsReader=" + postingsReader);
+      }
+
+      @Override
+      public Comparator<BytesRef> getComparator() {
+        return BytesRef.getUTF8SortedAsUnicodeComparator();
+      }
+
+      // TODO: we may want an alternate mode here which is
+      // "if you are about to return NOT_FOUND I won't use
+      // the terms data from that"; eg FuzzyTermsEnum will
+      // (usually) just immediately call seek again if we
+      // return NOT_FOUND so it's a waste for us to fill in
+      // the term that was actually NOT_FOUND
+      @Override
+      public SeekStatus seekCeil(final BytesRef target, final boolean useCache) throws IOException {
+
+        if (indexEnum == null) {
+          throw new IllegalStateException("terms index was not loaded");
+        }
+   
+        //System.out.println("BTR.seek seg=" + segment + " target=" + fieldInfo.name + ":" + target.utf8ToString() + " " + target + " current=" + term().utf8ToString() + " " + term() + " useCache=" + useCache + " indexIsCurrent=" + indexIsCurrent + " didIndexNext=" + didIndexNext + " seekPending=" + seekPending + " divisor=" + indexReader.getDivisor() + " this="  + this);
+        if (didIndexNext) {
+          if (nextIndexTerm == null) {
+            //System.out.println("  nextIndexTerm=null");
+          } else {
+            //System.out.println("  nextIndexTerm=" + nextIndexTerm.utf8ToString());
+          }
+        }
+
+        // Check cache
+        if (useCache) {
+          fieldTerm.term = target;
+          // TODO: should we differentiate "frozen"
+          // TermState (ie one that was cloned and
+          // cached/returned by termState()) from the
+          // malleable (primary) one?
+          final TermState cachedState = termsCache.get(fieldTerm);
+          if (cachedState != null) {
+            seekPending = true;
+            //System.out.println("  cached!");
+            seekExact(target, cachedState);
+            //System.out.println("  term=" + term.utf8ToString());
+            return SeekStatus.FOUND;
+          }
+        }
+
+        boolean doSeek = true;
+
+        // See if we can avoid seeking, because target term
+        // is after current term but before next index term:
+        if (indexIsCurrent) {
+
+          final int cmp = BytesRef.getUTF8SortedAsUnicodeComparator().compare(term, target);
+
+          if (cmp == 0) {
+            // Already at the requested term
+            return SeekStatus.FOUND;
+          } else if (cmp < 0) {
+
+            // Target term is after current term
+            if (!didIndexNext) {
+              if (indexEnum.next() == -1) {
+                nextIndexTerm = null;
+              } else {
+                nextIndexTerm = indexEnum.term();
+              }
+              //System.out.println("  now do index next() nextIndexTerm=" + (nextIndexTerm == null ? "null" : nextIndexTerm.utf8ToString()));
+              didIndexNext = true;
+            }
+
+            if (nextIndexTerm == null || BytesRef.getUTF8SortedAsUnicodeComparator().compare(target, nextIndexTerm) < 0) {
+              // Optimization: requested term is within the
+              // same term block we are now in; skip seeking
+              // (but do scanning):
+              doSeek = false;
+              //System.out.println("  skip seek: nextIndexTerm=" + (nextIndexTerm == null ? "null" : nextIndexTerm.utf8ToString()));
+            }
+          }
+        }
+
+        if (doSeek) {
+          //System.out.println("  seek");
+
+          // Ask terms index to find biggest indexed term (=
+          // first term in a block) that's <= our text:
+          in.seek(indexEnum.seek(target));
+          boolean result = nextBlock();
+
+          // Block must exist since, at least, the indexed term
+          // is in the block:
+          assert result;
+
+          indexIsCurrent = true;
+          didIndexNext = false;
+          blocksSinceSeek = 0;          
+
+          if (doOrd) {
+            state.ord = indexEnum.ord()-1;
+          }
+
+          term.copyBytes(indexEnum.term());
+          //System.out.println("  seek: term=" + term.utf8ToString());
+        } else {
+          //System.out.println("  skip seek");
+          if (state.termBlockOrd == blockTermCount && !nextBlock()) {
+            indexIsCurrent = false;
+            return SeekStatus.END;
+          }
+        }
+
+        seekPending = false;
+
+        int common = 0;
+
+        // Scan within block.  We could do this by calling
+        // _next() and testing the resulting term, but this
+        // is wasteful.  Instead, we first confirm the
+        // target matches the common prefix of this block,
+        // and then we scan the term bytes directly from the
+        // termSuffixesreader's byte[], saving a copy into
+        // the BytesRef term per term.  Only when we return
+        // do we then copy the bytes into the term.
+
+        while(true) {
+
+          // First, see if target term matches common prefix
+          // in this block:
+          if (common < termBlockPrefix) {
+            final int cmp = (term.bytes[common]&0xFF) - (target.bytes[target.offset + common]&0xFF);
+            if (cmp < 0) {
+
+              // TODO: maybe we should store common prefix
+              // in block header?  (instead of relying on
+              // last term of previous block)
+
+              // Target's prefix is after the common block
+              // prefix, so term cannot be in this block
+              // but it could be in next block.  We
+              // must scan to end-of-block to set common
+              // prefix for next block:
+              if (state.termBlockOrd < blockTermCount) {
+                while(state.termBlockOrd < blockTermCount-1) {
+                  state.termBlockOrd++;
+                  state.ord++;
+                  termSuffixesReader.skipBytes(termSuffixesReader.readVInt());
+                }
+                final int suffix = termSuffixesReader.readVInt();
+                term.length = termBlockPrefix + suffix;
+                if (term.bytes.length < term.length) {
+                  term.grow(term.length);
+                }
+                termSuffixesReader.readBytes(term.bytes, termBlockPrefix, suffix);
+              }
+              state.ord++;
+              
+              if (!nextBlock()) {
+                indexIsCurrent = false;
+                return SeekStatus.END;
+              }
+              common = 0;
+
+            } else if (cmp > 0) {
+              // Target's prefix is before the common prefix
+              // of this block, so we position to start of
+              // block and return NOT_FOUND:
+              assert state.termBlockOrd == 0;
+
+              final int suffix = termSuffixesReader.readVInt();
+              term.length = termBlockPrefix + suffix;
+              if (term.bytes.length < term.length) {
+                term.grow(term.length);
+              }
+              termSuffixesReader.readBytes(term.bytes, termBlockPrefix, suffix);
+              return SeekStatus.NOT_FOUND;
+            } else {
+              common++;
+            }
+
+            continue;
+          }
+
+          // Test every term in this block
+          while (true) {
+            state.termBlockOrd++;
+            state.ord++;
+
+            final int suffix = termSuffixesReader.readVInt();
+            
+            // We know the prefix matches, so just compare the new suffix:
+            final int termLen = termBlockPrefix + suffix;
+            int bytePos = termSuffixesReader.getPosition();
+
+            boolean next = false;
+            final int limit = target.offset + (termLen < target.length ? termLen : target.length);
+            int targetPos = target.offset + termBlockPrefix;
+            while(targetPos < limit) {
+              final int cmp = (termSuffixes[bytePos++]&0xFF) - (target.bytes[targetPos++]&0xFF);
+              if (cmp < 0) {
+                // Current term is still before the target;
+                // keep scanning
+                next = true;
+                break;
+              } else if (cmp > 0) {
+                // Done!  Current term is after target. Stop
+                // here, fill in real term, return NOT_FOUND.
+                term.length = termBlockPrefix + suffix;
+                if (term.bytes.length < term.length) {
+                  term.grow(term.length);
+                }
+                termSuffixesReader.readBytes(term.bytes, termBlockPrefix, suffix);
+                //System.out.println("  NOT_FOUND");
+                return SeekStatus.NOT_FOUND;
+              }
+            }
+
+            if (!next && target.length <= termLen) {
+              term.length = termBlockPrefix + suffix;
+              if (term.bytes.length < term.length) {
+                term.grow(term.length);
+              }
+              termSuffixesReader.readBytes(term.bytes, termBlockPrefix, suffix);
+
+              if (target.length == termLen) {
+                // Done!  Exact match.  Stop here, fill in
+                // real term, return FOUND.
+                //System.out.println("  FOUND");
+
+                if (useCache) {
+                  // Store in cache
+                  decodeMetaData();
+                  //System.out.println("  cache! state=" + state);
+                  termsCache.put(new FieldAndTerm(fieldTerm), (BlockTermState) state.clone());
+                }
+
+                return SeekStatus.FOUND;
+              } else {
+                //System.out.println("  NOT_FOUND");
+                return SeekStatus.NOT_FOUND;
+              }
+            }
+
+            if (state.termBlockOrd == blockTermCount) {
+              // Must pre-fill term for next block's common prefix
+              term.length = termBlockPrefix + suffix;
+              if (term.bytes.length < term.length) {
+                term.grow(term.length);
+              }
+              termSuffixesReader.readBytes(term.bytes, termBlockPrefix, suffix);
+              break;
+            } else {
+              termSuffixesReader.skipBytes(suffix);
+            }
+          }
+
+          // The purpose of the terms dict index is to seek
+          // the enum to the closest index term before the
+          // term we are looking for.  So, we should never
+          // cross another index term (besides the first
+          // one) while we are scanning:
+
+          assert indexIsCurrent;
+
+          if (!nextBlock()) {
+            //System.out.println("  END");
+            indexIsCurrent = false;
+            return SeekStatus.END;
+          }
+          common = 0;
+        }
+      }
+
+      @Override
+      public BytesRef next() throws IOException {
+        //System.out.println("BTR.next() seekPending=" + seekPending + " pendingSeekCount=" + state.termBlockOrd);
+
+        // If seek was previously called and the term was cached,
+        // usually caller is just going to pull a D/&PEnum or get
+        // docFreq, etc.  But, if they then call next(),
+        // this method catches up all internal state so next()
+        // works properly:
+        if (seekPending) {
+          assert !indexIsCurrent;
+          in.seek(state.blockFilePointer);
+          final int pendingSeekCount = state.termBlockOrd;
+          boolean result = nextBlock();
+
+          final long savOrd = state.ord;
+
+          // Block must exist since seek(TermState) was called w/ a
+          // TermState previously returned by this enum when positioned
+          // on a real term:
+          assert result;
+
+          while(state.termBlockOrd < pendingSeekCount) {
+            BytesRef nextResult = _next();
+            assert nextResult != null;
+          }
+          seekPending = false;
+          state.ord = savOrd;
+        }
+        return _next();
+      }
+
+      /* Decodes only the term bytes of the next term.  If caller then asks for
+         metadata, ie docFreq, totalTermFreq or pulls a D/&PEnum, we then (lazily)
+         decode all metadata up to the current term. */
+      private BytesRef _next() throws IOException {
+        //System.out.println("BTR._next seg=" + segment + " this=" + this + " termCount=" + state.termBlockOrd + " (vs " + blockTermCount + ")");
+        if (state.termBlockOrd == blockTermCount && !nextBlock()) {
+          //System.out.println("  eof");
+          indexIsCurrent = false;
+          return null;
+        }
+
+        // TODO: cutover to something better for these ints!  simple64?
+        final int suffix = termSuffixesReader.readVInt();
+        //System.out.println("  suffix=" + suffix);
+
+        term.length = termBlockPrefix + suffix;
+        if (term.bytes.length < term.length) {
+          term.grow(term.length);
+        }
+        termSuffixesReader.readBytes(term.bytes, termBlockPrefix, suffix);
+        state.termBlockOrd++;
+
+        // NOTE: meaningless in the non-ord case
+        state.ord++;
+
+        //System.out.println("  return term=" + fieldInfo.name + ":" + term.utf8ToString() + " " + term + " tbOrd=" + state.termBlockOrd);
+        return term;
+      }
+
+      @Override
+      public BytesRef term() {
+        return term;
+      }
+
+      @Override
+      public int docFreq() throws IOException {
+        //System.out.println("BTR.docFreq");
+        decodeMetaData();
+        //System.out.println("  return " + state.docFreq);
+        return state.docFreq;
+      }
+
+      @Override
+      public long totalTermFreq() throws IOException {
+        decodeMetaData();
+        return state.totalTermFreq;
+      }
+
+      @Override
+      public DocsEnum docs(Bits liveDocs, DocsEnum reuse, int flags) throws IOException {
+        //System.out.println("BTR.docs this=" + this);
+        decodeMetaData();
+        //System.out.println("BTR.docs:  state.docFreq=" + state.docFreq);
+        return postingsReader.docs(fieldInfo, state, liveDocs, reuse, flags);
+      }
+
+      @Override
+      public DocsAndPositionsEnum docsAndPositions(Bits liveDocs, DocsAndPositionsEnum reuse, int flags) throws IOException {
+        if (fieldInfo.getIndexOptions().compareTo(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS) < 0) {
+          // Positions were not indexed:
+          return null;
+        }
+
+        decodeMetaData();
+        return postingsReader.docsAndPositions(fieldInfo, state, liveDocs, reuse, flags);
+      }
+
+      @Override
+      public void seekExact(BytesRef target, TermState otherState) {
+        //System.out.println("BTR.seekExact termState target=" + target.utf8ToString() + " " + target + " this=" + this);
+        assert otherState != null && otherState instanceof BlockTermState;
+        assert !doOrd || ((BlockTermState) otherState).ord < numTerms;
+        state.copyFrom(otherState);
+        seekPending = true;
+        indexIsCurrent = false;
+        term.copyBytes(target);
+      }
+      
+      @Override
+      public TermState termState() throws IOException {
+        //System.out.println("BTR.termState this=" + this);
+        decodeMetaData();
+        TermState ts = state.clone();
+        //System.out.println("  return ts=" + ts);
+        return ts;
+      }
+
+      @Override
+      public void seekExact(long ord) throws IOException {
+        //System.out.println("BTR.seek by ord ord=" + ord);
+        if (indexEnum == null) {
+          throw new IllegalStateException("terms index was not loaded");
+        }
+
+        assert ord < numTerms;
+
+        // TODO: if ord is in same terms block and
+        // after current ord, we should avoid this seek just
+        // like we do in the seek(BytesRef) case
+        in.seek(indexEnum.seek(ord));
+        boolean result = nextBlock();
+
+        // Block must exist since ord < numTerms:
+        assert result;
+
+        indexIsCurrent = true;
+        didIndexNext = false;
+        blocksSinceSeek = 0;
+        seekPending = false;
+
+        state.ord = indexEnum.ord()-1;
+        assert state.ord >= -1: "ord=" + state.ord;
+        term.copyBytes(indexEnum.term());
+
+        // Now, scan:
+        int left = (int) (ord - state.ord);
+        while(left > 0) {
+          final BytesRef term = _next();
+          assert term != null;
+          left--;
+          assert indexIsCurrent;
+        }
+      }
+
+      @Override
+      public long ord() {
+        if (!doOrd) {
+          throw new UnsupportedOperationException();
+        }
+        return state.ord;
+      }
+
+      /* Does initial decode of next block of terms; this
+         doesn't actually decode the docFreq, totalTermFreq,
+         postings details (frq/prx offset, etc.) metadata;
+         it just loads them as byte[] blobs which are then      
+         decoded on-demand if the metadata is ever requested
+         for any term in this block.  This enables terms-only
+         intensive consumes (eg certain MTQs, respelling) to
+         not pay the price of decoding metadata they won't
+         use. */
+      private boolean nextBlock() throws IOException {
+
+        // TODO: we still lazy-decode the byte[] for each
+        // term (the suffix), but, if we decoded
+        // all N terms up front then seeking could do a fast
+        // bsearch w/in the block...
+
+        //System.out.println("BTR.nextBlock() fp=" + in.getFilePointer() + " this=" + this);
+        state.blockFilePointer = in.getFilePointer();
+        blockTermCount = in.readVInt();
+        //System.out.println("  blockTermCount=" + blockTermCount);
+        if (blockTermCount == 0) {
+          return false;
+        }
+        termBlockPrefix = in.readVInt();
+
+        // term suffixes:
+        int len = in.readVInt();
+        if (termSuffixes.length < len) {
+          termSuffixes = new byte[ArrayUtil.oversize(len, 1)];
+        }
+        //System.out.println("  termSuffixes len=" + len);
+        in.readBytes(termSuffixes, 0, len);
+        termSuffixesReader.reset(termSuffixes, 0, len);
+
+        // docFreq, totalTermFreq
+        len = in.readVInt();
+        if (docFreqBytes.length < len) {
+          docFreqBytes = new byte[ArrayUtil.oversize(len, 1)];
+        }
+        //System.out.println("  freq bytes len=" + len);
+        in.readBytes(docFreqBytes, 0, len);
+        freqReader.reset(docFreqBytes, 0, len);
+        metaDataUpto = 0;
+
+        state.termBlockOrd = 0;
+
+        postingsReader.readTermsBlock(in, fieldInfo, state);
+
+        blocksSinceSeek++;
+        indexIsCurrent = indexIsCurrent && (blocksSinceSeek < indexReader.getDivisor());
+        //System.out.println("  indexIsCurrent=" + indexIsCurrent);
+
+        return true;
+      }
+     
+      private void decodeMetaData() throws IOException {
+        //System.out.println("BTR.decodeMetadata mdUpto=" + metaDataUpto + " vs termCount=" + state.termBlockOrd + " state=" + state);
+        if (!seekPending) {
+          // TODO: cutover to random-access API
+          // here.... really stupid that we have to decode N
+          // wasted term metadata just to get to the N+1th
+          // that we really need...
+
+          // lazily catch up on metadata decode:
+          final int limit = state.termBlockOrd;
+          // We must set/incr state.termCount because
+          // postings impl can look at this
+          state.termBlockOrd = metaDataUpto;
+          // TODO: better API would be "jump straight to term=N"???
+          while (metaDataUpto < limit) {
+            //System.out.println("  decode mdUpto=" + metaDataUpto);
+            // TODO: we could make "tiers" of metadata, ie,
+            // decode docFreq/totalTF but don't decode postings
+            // metadata; this way caller could get
+            // docFreq/totalTF w/o paying decode cost for
+            // postings
+
+            // TODO: if docFreq were bulk decoded we could
+            // just skipN here:
+            state.docFreq = freqReader.readVInt();
+            //System.out.println("    dF=" + state.docFreq);
+            if (fieldInfo.getIndexOptions() != IndexOptions.DOCS_ONLY) {
+              state.totalTermFreq = state.docFreq + freqReader.readVLong();
+              //System.out.println("    totTF=" + state.totalTermFreq);
+            }
+
+            postingsReader.nextTerm(fieldInfo, state);
+            metaDataUpto++;
+            state.termBlockOrd++;
+          }
+        } else {
+          //System.out.println("  skip! seekPending");
+        }
+      }
+    }
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsWriter.java b/lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsWriter.java
new file mode 100644
index 0000000..aedd848
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/BlockTermsWriter.java
@@ -0,0 +1,324 @@
+package org.apache.lucene.codecs.blockterms;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Comparator;
+import java.util.List;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.FieldsConsumer;
+import org.apache.lucene.codecs.PostingsConsumer;
+import org.apache.lucene.codecs.PostingsWriterBase;
+import org.apache.lucene.codecs.TermStats;
+import org.apache.lucene.codecs.TermsConsumer;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.store.RAMOutputStream;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.RamUsageEstimator;
+
+// TODO: currently we encode all terms between two indexed
+// terms as a block; but, we could decouple the two, ie
+// allow several blocks in between two indexed terms
+
+/**
+ * Writes terms dict, block-encoding (column stride) each
+ * term's metadata for each set of terms between two
+ * index terms.
+ *
+ * @lucene.experimental
+ */
+
+public class BlockTermsWriter extends FieldsConsumer {
+
+  final static String CODEC_NAME = "BLOCK_TERMS_DICT";
+
+  // Initial format
+  public static final int VERSION_START = 0;
+
+  public static final int VERSION_CURRENT = VERSION_START;
+
+  /** Extension of terms file */
+  static final String TERMS_EXTENSION = "tib";
+
+  protected final IndexOutput out;
+  final PostingsWriterBase postingsWriter;
+  final FieldInfos fieldInfos;
+  FieldInfo currentField;
+  private final TermsIndexWriterBase termsIndexWriter;
+  private final List<TermsWriter> fields = new ArrayList<TermsWriter>();
+
+  // private final String segment;
+
+  public BlockTermsWriter(TermsIndexWriterBase termsIndexWriter,
+      SegmentWriteState state, PostingsWriterBase postingsWriter)
+      throws IOException {
+    final String termsFileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, TERMS_EXTENSION);
+    this.termsIndexWriter = termsIndexWriter;
+    out = state.directory.createOutput(termsFileName, state.context);
+    boolean success = false;
+    try {
+      fieldInfos = state.fieldInfos;
+      writeHeader(out);
+      currentField = null;
+      this.postingsWriter = postingsWriter;
+      // segment = state.segmentName;
+      
+      //System.out.println("BTW.init seg=" + state.segmentName);
+      
+      postingsWriter.start(out); // have consumer write its format/header
+      success = true;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(out);
+      }
+    }
+  }
+  
+  protected void writeHeader(IndexOutput out) throws IOException {
+    CodecUtil.writeHeader(out, CODEC_NAME, VERSION_CURRENT); 
+
+    out.writeLong(0);                             // leave space for end index pointer    
+  }
+
+  @Override
+  public TermsConsumer addField(FieldInfo field) throws IOException {
+    //System.out.println("\nBTW.addField seg=" + segment + " field=" + field.name);
+    assert currentField == null || currentField.name.compareTo(field.name) < 0;
+    currentField = field;
+    TermsIndexWriterBase.FieldWriter fieldIndexWriter = termsIndexWriter.addField(field, out.getFilePointer());
+    final TermsWriter terms = new TermsWriter(fieldIndexWriter, field, postingsWriter);
+    fields.add(terms);
+    return terms;
+  }
+
+  @Override
+  public void close() throws IOException {
+
+    try {
+      
+      int nonZeroCount = 0;
+      for(TermsWriter field : fields) {
+        if (field.numTerms > 0) {
+          nonZeroCount++;
+        }
+      }
+
+      final long dirStart = out.getFilePointer();
+
+      out.writeVInt(nonZeroCount);
+      for(TermsWriter field : fields) {
+        if (field.numTerms > 0) {
+          out.writeVInt(field.fieldInfo.number);
+          out.writeVLong(field.numTerms);
+          out.writeVLong(field.termsStartPointer);
+          if (field.fieldInfo.getIndexOptions() != IndexOptions.DOCS_ONLY) {
+            out.writeVLong(field.sumTotalTermFreq);
+          }
+          out.writeVLong(field.sumDocFreq);
+          out.writeVInt(field.docCount);
+        }
+      }
+      writeTrailer(dirStart);
+    } finally {
+      IOUtils.close(out, postingsWriter, termsIndexWriter);
+    }
+  }
+
+  protected void writeTrailer(long dirStart) throws IOException {
+    out.seek(CodecUtil.headerLength(CODEC_NAME));
+    out.writeLong(dirStart);    
+  }
+  
+  private static class TermEntry {
+    public final BytesRef term = new BytesRef();
+    public TermStats stats;
+  }
+
+  class TermsWriter extends TermsConsumer {
+    private final FieldInfo fieldInfo;
+    private final PostingsWriterBase postingsWriter;
+    private final long termsStartPointer;
+    private long numTerms;
+    private final TermsIndexWriterBase.FieldWriter fieldIndexWriter;
+    long sumTotalTermFreq;
+    long sumDocFreq;
+    int docCount;
+
+    private TermEntry[] pendingTerms;
+
+    private int pendingCount;
+
+    TermsWriter(
+        TermsIndexWriterBase.FieldWriter fieldIndexWriter,
+        FieldInfo fieldInfo,
+        PostingsWriterBase postingsWriter) 
+    {
+      this.fieldInfo = fieldInfo;
+      this.fieldIndexWriter = fieldIndexWriter;
+      pendingTerms = new TermEntry[32];
+      for(int i=0;i<pendingTerms.length;i++) {
+        pendingTerms[i] = new TermEntry();
+      }
+      termsStartPointer = out.getFilePointer();
+      postingsWriter.setField(fieldInfo);
+      this.postingsWriter = postingsWriter;
+    }
+    
+    @Override
+    public Comparator<BytesRef> getComparator() {
+      return BytesRef.getUTF8SortedAsUnicodeComparator();
+    }
+
+    @Override
+    public PostingsConsumer startTerm(BytesRef text) throws IOException {
+      //System.out.println("BTW: startTerm term=" + fieldInfo.name + ":" + text.utf8ToString() + " " + text + " seg=" + segment);
+      postingsWriter.startTerm();
+      return postingsWriter;
+    }
+
+    private final BytesRef lastPrevTerm = new BytesRef();
+
+    @Override
+    public void finishTerm(BytesRef text, TermStats stats) throws IOException {
+
+      assert stats.docFreq > 0;
+      //System.out.println("BTW: finishTerm term=" + fieldInfo.name + ":" + text.utf8ToString() + " " + text + " seg=" + segment + " df=" + stats.docFreq);
+
+      final boolean isIndexTerm = fieldIndexWriter.checkIndexTerm(text, stats);
+
+      if (isIndexTerm) {
+        if (pendingCount > 0) {
+          // Instead of writing each term, live, we gather terms
+          // in RAM in a pending buffer, and then write the
+          // entire block in between index terms:
+          flushBlock();
+        }
+        fieldIndexWriter.add(text, stats, out.getFilePointer());
+        //System.out.println("  index term!");
+      }
+
+      if (pendingTerms.length == pendingCount) {
+        final TermEntry[] newArray = new TermEntry[ArrayUtil.oversize(pendingCount+1, RamUsageEstimator.NUM_BYTES_OBJECT_REF)];
+        System.arraycopy(pendingTerms, 0, newArray, 0, pendingCount);
+        for(int i=pendingCount;i<newArray.length;i++) {
+          newArray[i] = new TermEntry();
+        }
+        pendingTerms = newArray;
+      }
+      final TermEntry te = pendingTerms[pendingCount];
+      te.term.copyBytes(text);
+      te.stats = stats;
+
+      pendingCount++;
+
+      postingsWriter.finishTerm(stats);
+      numTerms++;
+    }
+
+    // Finishes all terms in this field
+    @Override
+    public void finish(long sumTotalTermFreq, long sumDocFreq, int docCount) throws IOException {
+      if (pendingCount > 0) {
+        flushBlock();
+      }
+      // EOF marker:
+      out.writeVInt(0);
+
+      this.sumTotalTermFreq = sumTotalTermFreq;
+      this.sumDocFreq = sumDocFreq;
+      this.docCount = docCount;
+      fieldIndexWriter.finish(out.getFilePointer());
+    }
+
+    private int sharedPrefix(BytesRef term1, BytesRef term2) {
+      assert term1.offset == 0;
+      assert term2.offset == 0;
+      int pos1 = 0;
+      int pos1End = pos1 + Math.min(term1.length, term2.length);
+      int pos2 = 0;
+      while(pos1 < pos1End) {
+        if (term1.bytes[pos1] != term2.bytes[pos2]) {
+          return pos1;
+        }
+        pos1++;
+        pos2++;
+      }
+      return pos1;
+    }
+
+    private final RAMOutputStream bytesWriter = new RAMOutputStream();
+
+    private void flushBlock() throws IOException {
+      //System.out.println("BTW.flushBlock seg=" + segment + " pendingCount=" + pendingCount + " fp=" + out.getFilePointer());
+
+      // First pass: compute common prefix for all terms
+      // in the block, against term before first term in
+      // this block:
+      int commonPrefix = sharedPrefix(lastPrevTerm, pendingTerms[0].term);
+      for(int termCount=1;termCount<pendingCount;termCount++) {
+        commonPrefix = Math.min(commonPrefix,
+                                sharedPrefix(lastPrevTerm,
+                                             pendingTerms[termCount].term));
+      }        
+
+      out.writeVInt(pendingCount);
+      out.writeVInt(commonPrefix);
+
+      // 2nd pass: write suffixes, as separate byte[] blob
+      for(int termCount=0;termCount<pendingCount;termCount++) {
+        final int suffix = pendingTerms[termCount].term.length - commonPrefix;
+        // TODO: cutover to better intblock codec, instead
+        // of interleaving here:
+        bytesWriter.writeVInt(suffix);
+        bytesWriter.writeBytes(pendingTerms[termCount].term.bytes, commonPrefix, suffix);
+      }
+      out.writeVInt((int) bytesWriter.getFilePointer());
+      bytesWriter.writeTo(out);
+      bytesWriter.reset();
+
+      // 3rd pass: write the freqs as byte[] blob
+      // TODO: cutover to better intblock codec.  simple64?
+      // write prefix, suffix first:
+      for(int termCount=0;termCount<pendingCount;termCount++) {
+        final TermStats stats = pendingTerms[termCount].stats;
+        assert stats != null;
+        bytesWriter.writeVInt(stats.docFreq);
+        if (fieldInfo.getIndexOptions() != IndexOptions.DOCS_ONLY) {
+          bytesWriter.writeVLong(stats.totalTermFreq-stats.docFreq);
+        }
+      }
+
+      out.writeVInt((int) bytesWriter.getFilePointer());
+      bytesWriter.writeTo(out);
+      bytesWriter.reset();
+
+      postingsWriter.flushTermsBlock(pendingCount, pendingCount);
+      lastPrevTerm.copyBytes(pendingTerms[pendingCount-1].term);
+      pendingCount = 0;
+    }
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/FixedGapTermsIndexReader.java b/lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/FixedGapTermsIndexReader.java
new file mode 100644
index 0000000..e3a3832
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/FixedGapTermsIndexReader.java
@@ -0,0 +1,415 @@
+package org.apache.lucene.codecs.blockterms;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.PagedBytes;
+import org.apache.lucene.util.packed.PackedInts;
+
+import java.util.HashMap;
+import java.util.Comparator;
+import java.io.IOException;
+
+import org.apache.lucene.index.IndexFileNames;
+
+/** 
+ * TermsIndexReader for simple every Nth terms indexes.
+ *
+ * @see FixedGapTermsIndexWriter
+ * @lucene.experimental 
+ */
+public class FixedGapTermsIndexReader extends TermsIndexReaderBase {
+
+  // NOTE: long is overkill here, since this number is 128
+  // by default and only indexDivisor * 128 if you change
+  // the indexDivisor at search time.  But, we use this in a
+  // number of places to multiply out the actual ord, and we
+  // will overflow int during those multiplies.  So to avoid
+  // having to upgrade each multiple to long in multiple
+  // places (error prone), we use long here:
+  private long totalIndexInterval;
+
+  private int indexDivisor;
+  final private int indexInterval;
+
+  // Closed if indexLoaded is true:
+  private IndexInput in;
+  private volatile boolean indexLoaded;
+
+  private final Comparator<BytesRef> termComp;
+
+  private final static int PAGED_BYTES_BITS = 15;
+
+  // all fields share this single logical byte[]
+  private final PagedBytes termBytes = new PagedBytes(PAGED_BYTES_BITS);
+  private PagedBytes.Reader termBytesReader;
+
+  final HashMap<FieldInfo,FieldIndexData> fields = new HashMap<FieldInfo,FieldIndexData>();
+  
+  // start of the field info data
+  protected long dirOffset;
+
+  public FixedGapTermsIndexReader(Directory dir, FieldInfos fieldInfos, String segment, int indexDivisor, Comparator<BytesRef> termComp, String segmentSuffix, IOContext context)
+    throws IOException {
+
+    this.termComp = termComp;
+
+    assert indexDivisor == -1 || indexDivisor > 0;
+
+    in = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, FixedGapTermsIndexWriter.TERMS_INDEX_EXTENSION), context);
+    
+    boolean success = false;
+
+    try {
+      
+      readHeader(in);
+      indexInterval = in.readInt();
+      if (indexInterval < 1) {
+        throw new CorruptIndexException("invalid indexInterval: " + indexInterval + " (resource=" + in + ")");
+      }
+      this.indexDivisor = indexDivisor;
+
+      if (indexDivisor < 0) {
+        totalIndexInterval = indexInterval;
+      } else {
+        // In case terms index gets loaded, later, on demand
+        totalIndexInterval = indexInterval * indexDivisor;
+      }
+      assert totalIndexInterval > 0;
+      
+      seekDir(in, dirOffset);
+
+      // Read directory
+      final int numFields = in.readVInt();     
+      if (numFields < 0) {
+        throw new CorruptIndexException("invalid numFields: " + numFields + " (resource=" + in + ")");
+      }
+      //System.out.println("FGR: init seg=" + segment + " div=" + indexDivisor + " nF=" + numFields);
+      for(int i=0;i<numFields;i++) {
+        final int field = in.readVInt();
+        final int numIndexTerms = in.readVInt();
+        if (numIndexTerms < 0) {
+          throw new CorruptIndexException("invalid numIndexTerms: " + numIndexTerms + " (resource=" + in + ")");
+        }
+        final long termsStart = in.readVLong();
+        final long indexStart = in.readVLong();
+        final long packedIndexStart = in.readVLong();
+        final long packedOffsetsStart = in.readVLong();
+        if (packedIndexStart < indexStart) {
+          throw new CorruptIndexException("invalid packedIndexStart: " + packedIndexStart + " indexStart: " + indexStart + "numIndexTerms: " + numIndexTerms + " (resource=" + in + ")");
+        }
+        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);
+        FieldIndexData previous = fields.put(fieldInfo, new FieldIndexData(fieldInfo, numIndexTerms, indexStart, termsStart, packedIndexStart, packedOffsetsStart));
+        if (previous != null) {
+          throw new CorruptIndexException("duplicate field: " + fieldInfo.name + " (resource=" + in + ")");
+        }
+      }
+      success = true;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(in);
+      }
+      if (indexDivisor > 0) {
+        in.close();
+        in = null;
+        if (success) {
+          indexLoaded = true;
+        }
+        termBytesReader = termBytes.freeze(true);
+      }
+    }
+  }
+  
+  @Override
+  public int getDivisor() {
+    return indexDivisor;
+  }
+
+  protected void readHeader(IndexInput input) throws IOException {
+    CodecUtil.checkHeader(input, FixedGapTermsIndexWriter.CODEC_NAME,
+      FixedGapTermsIndexWriter.VERSION_START, FixedGapTermsIndexWriter.VERSION_START);
+    dirOffset = input.readLong();
+  }
+
+  private class IndexEnum extends FieldIndexEnum {
+    private final FieldIndexData.CoreFieldIndex fieldIndex;
+    private final BytesRef term = new BytesRef();
+    private long ord;
+
+    public IndexEnum(FieldIndexData.CoreFieldIndex fieldIndex) {
+      this.fieldIndex = fieldIndex;
+    }
+
+    @Override
+    public BytesRef term() {
+      return term;
+    }
+
+    @Override
+    public long seek(BytesRef target) {
+      int lo = 0;				  // binary search
+      int hi = fieldIndex.numIndexTerms - 1;
+      assert totalIndexInterval > 0 : "totalIndexInterval=" + totalIndexInterval;
+
+      while (hi >= lo) {
+        int mid = (lo + hi) >>> 1;
+
+        final long offset = fieldIndex.termOffsets.get(mid);
+        final int length = (int) (fieldIndex.termOffsets.get(1+mid) - offset);
+        termBytesReader.fillSlice(term, fieldIndex.termBytesStart + offset, length);
+
+        int delta = termComp.compare(target, term);
+        if (delta < 0) {
+          hi = mid - 1;
+        } else if (delta > 0) {
+          lo = mid + 1;
+        } else {
+          assert mid >= 0;
+          ord = mid*totalIndexInterval;
+          return fieldIndex.termsStart + fieldIndex.termsDictOffsets.get(mid);
+        }
+      }
+
+      if (hi < 0) {
+        assert hi == -1;
+        hi = 0;
+      }
+
+      final long offset = fieldIndex.termOffsets.get(hi);
+      final int length = (int) (fieldIndex.termOffsets.get(1+hi) - offset);
+      termBytesReader.fillSlice(term, fieldIndex.termBytesStart + offset, length);
+
+      ord = hi*totalIndexInterval;
+      return fieldIndex.termsStart + fieldIndex.termsDictOffsets.get(hi);
+    }
+
+    @Override
+    public long next() {
+      final int idx = 1 + (int) (ord / totalIndexInterval);
+      if (idx >= fieldIndex.numIndexTerms) {
+        return -1;
+      }
+      ord += totalIndexInterval;
+
+      final long offset = fieldIndex.termOffsets.get(idx);
+      final int length = (int) (fieldIndex.termOffsets.get(1+idx) - offset);
+      termBytesReader.fillSlice(term, fieldIndex.termBytesStart + offset, length);
+      return fieldIndex.termsStart + fieldIndex.termsDictOffsets.get(idx);
+    }
+
+    @Override
+    public long ord() {
+      return ord;
+    }
+
+    @Override
+    public long seek(long ord) {
+      int idx = (int) (ord / totalIndexInterval);
+      // caller must ensure ord is in bounds
+      assert idx < fieldIndex.numIndexTerms;
+      final long offset = fieldIndex.termOffsets.get(idx);
+      final int length = (int) (fieldIndex.termOffsets.get(1+idx) - offset);
+      termBytesReader.fillSlice(term, fieldIndex.termBytesStart + offset, length);
+      this.ord = idx * totalIndexInterval;
+      return fieldIndex.termsStart + fieldIndex.termsDictOffsets.get(idx);
+    }
+  }
+
+  @Override
+  public boolean supportsOrd() {
+    return true;
+  }
+
+  private final class FieldIndexData {
+
+    volatile CoreFieldIndex coreIndex;
+
+    private final long indexStart;
+    private final long termsStart;
+    private final long packedIndexStart;
+    private final long packedOffsetsStart;
+
+    private final int numIndexTerms;
+
+    public FieldIndexData(FieldInfo fieldInfo, int numIndexTerms, long indexStart, long termsStart, long packedIndexStart,
+                          long packedOffsetsStart) throws IOException {
+
+      this.termsStart = termsStart;
+      this.indexStart = indexStart;
+      this.packedIndexStart = packedIndexStart;
+      this.packedOffsetsStart = packedOffsetsStart;
+      this.numIndexTerms = numIndexTerms;
+
+      if (indexDivisor > 0) {
+        loadTermsIndex();
+      }
+    }
+
+    private void loadTermsIndex() throws IOException {
+      if (coreIndex == null) {
+        coreIndex = new CoreFieldIndex(indexStart, termsStart, packedIndexStart, packedOffsetsStart, numIndexTerms);
+      }
+    }
+
+    private final class CoreFieldIndex {
+
+      // where this field's terms begin in the packed byte[]
+      // data
+      final long termBytesStart;
+
+      // offset into index termBytes
+      final PackedInts.Reader termOffsets;
+
+      // index pointers into main terms dict
+      final PackedInts.Reader termsDictOffsets;
+
+      final int numIndexTerms;
+      final long termsStart;
+
+      public CoreFieldIndex(long indexStart, long termsStart, long packedIndexStart, long packedOffsetsStart, int numIndexTerms) throws IOException {
+
+        this.termsStart = termsStart;
+        termBytesStart = termBytes.getPointer();
+
+        IndexInput clone = in.clone();
+        clone.seek(indexStart);
+
+        // -1 is passed to mean "don't load term index", but
+        // if we are then later loaded it's overwritten with
+        // a real value
+        assert indexDivisor > 0;
+
+        this.numIndexTerms = 1+(numIndexTerms-1) / indexDivisor;
+
+        assert this.numIndexTerms  > 0: "numIndexTerms=" + numIndexTerms + " indexDivisor=" + indexDivisor;
+
+        if (indexDivisor == 1) {
+          // Default (load all index terms) is fast -- slurp in the images from disk:
+          
+          try {
+            final long numTermBytes = packedIndexStart - indexStart;
+            termBytes.copy(clone, numTermBytes);
+
+            // records offsets into main terms dict file
+            termsDictOffsets = PackedInts.getReader(clone);
+            assert termsDictOffsets.size() == numIndexTerms;
+
+            // records offsets into byte[] term data
+            termOffsets = PackedInts.getReader(clone);
+            assert termOffsets.size() == 1+numIndexTerms;
+          } finally {
+            clone.close();
+          }
+        } else {
+          // Get packed iterators
+          final IndexInput clone1 = in.clone();
+          final IndexInput clone2 = in.clone();
+
+          try {
+            // Subsample the index terms
+            clone1.seek(packedIndexStart);
+            final PackedInts.ReaderIterator termsDictOffsetsIter = PackedInts.getReaderIterator(clone1, PackedInts.DEFAULT_BUFFER_SIZE);
+
+            clone2.seek(packedOffsetsStart);
+            final PackedInts.ReaderIterator termOffsetsIter = PackedInts.getReaderIterator(clone2,  PackedInts.DEFAULT_BUFFER_SIZE);
+
+            // TODO: often we can get by w/ fewer bits per
+            // value, below.. .but this'd be more complex:
+            // we'd have to try @ fewer bits and then grow
+            // if we overflowed it.
+
+            PackedInts.Mutable termsDictOffsetsM = PackedInts.getMutable(this.numIndexTerms, termsDictOffsetsIter.getBitsPerValue(), PackedInts.DEFAULT);
+            PackedInts.Mutable termOffsetsM = PackedInts.getMutable(this.numIndexTerms+1, termOffsetsIter.getBitsPerValue(), PackedInts.DEFAULT);
+
+            termsDictOffsets = termsDictOffsetsM;
+            termOffsets = termOffsetsM;
+
+            int upto = 0;
+
+            long termOffsetUpto = 0;
+
+            while(upto < this.numIndexTerms) {
+              // main file offset copies straight over
+              termsDictOffsetsM.set(upto, termsDictOffsetsIter.next());
+
+              termOffsetsM.set(upto, termOffsetUpto);
+
+              long termOffset = termOffsetsIter.next();
+              long nextTermOffset = termOffsetsIter.next();
+              final int numTermBytes = (int) (nextTermOffset - termOffset);
+
+              clone.seek(indexStart + termOffset);
+              assert indexStart + termOffset < clone.length() : "indexStart=" + indexStart + " termOffset=" + termOffset + " len=" + clone.length();
+              assert indexStart + termOffset + numTermBytes < clone.length();
+
+              termBytes.copy(clone, numTermBytes);
+              termOffsetUpto += numTermBytes;
+
+              upto++;
+              if (upto == this.numIndexTerms) {
+                break;
+              }
+
+              // skip terms:
+              termsDictOffsetsIter.next();
+              for(int i=0;i<indexDivisor-2;i++) {
+                termOffsetsIter.next();
+                termsDictOffsetsIter.next();
+              }
+            }
+            termOffsetsM.set(upto, termOffsetUpto);
+
+          } finally {
+            clone1.close();
+            clone2.close();
+            clone.close();
+          }
+        }
+      }
+    }
+  }
+
+  @Override
+  public FieldIndexEnum getFieldEnum(FieldInfo fieldInfo) {
+    final FieldIndexData fieldData = fields.get(fieldInfo);
+    if (fieldData.coreIndex == null) {
+      return null;
+    } else {
+      return new IndexEnum(fieldData.coreIndex);
+    }
+  }
+
+  @Override
+  public void close() throws IOException {
+    if (in != null && !indexLoaded) {
+      in.close();
+    }
+  }
+
+  protected void seekDir(IndexInput input, long dirOffset) throws IOException {
+    input.seek(dirOffset);
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/FixedGapTermsIndexWriter.java b/lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/FixedGapTermsIndexWriter.java
new file mode 100644
index 0000000..9807530
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/FixedGapTermsIndexWriter.java
@@ -0,0 +1,257 @@
+package org.apache.lucene.codecs.blockterms;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.TermStats;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.packed.PackedInts;
+
+import java.util.List;
+import java.util.ArrayList;
+import java.io.IOException;
+
+/**
+ * Selects every Nth term as and index term, and hold term
+ * bytes (mostly) fully expanded in memory.  This terms index
+ * supports seeking by ord.  See {@link
+ * VariableGapTermsIndexWriter} for a more memory efficient
+ * terms index that does not support seeking by ord.
+ *
+ * @lucene.experimental */
+public class FixedGapTermsIndexWriter extends TermsIndexWriterBase {
+  protected final IndexOutput out;
+
+  /** Extension of terms index file */
+  static final String TERMS_INDEX_EXTENSION = "tii";
+
+  final static String CODEC_NAME = "SIMPLE_STANDARD_TERMS_INDEX";
+  final static int VERSION_START = 0;
+  final static int VERSION_CURRENT = VERSION_START;
+
+  final private int termIndexInterval;
+
+  private final List<SimpleFieldWriter> fields = new ArrayList<SimpleFieldWriter>();
+  
+  @SuppressWarnings("unused") private final FieldInfos fieldInfos; // unread
+
+  public FixedGapTermsIndexWriter(SegmentWriteState state) throws IOException {
+    final String indexFileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, TERMS_INDEX_EXTENSION);
+    termIndexInterval = state.termIndexInterval;
+    out = state.directory.createOutput(indexFileName, state.context);
+    boolean success = false;
+    try {
+      fieldInfos = state.fieldInfos;
+      writeHeader(out);
+      out.writeInt(termIndexInterval);
+      success = true;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(out);
+      }
+    }
+  }
+  
+  protected void writeHeader(IndexOutput out) throws IOException {
+    CodecUtil.writeHeader(out, CODEC_NAME, VERSION_CURRENT);
+    // Placeholder for dir offset
+    out.writeLong(0);
+  }
+
+  @Override
+  public FieldWriter addField(FieldInfo field, long termsFilePointer) {
+    //System.out.println("FGW: addFfield=" + field.name);
+    SimpleFieldWriter writer = new SimpleFieldWriter(field, termsFilePointer);
+    fields.add(writer);
+    return writer;
+  }
+
+  /** NOTE: if your codec does not sort in unicode code
+   *  point order, you must override this method, to simply
+   *  return indexedTerm.length. */
+  protected int indexedTermPrefixLength(final BytesRef priorTerm, final BytesRef indexedTerm) {
+    // As long as codec sorts terms in unicode codepoint
+    // order, we can safely strip off the non-distinguishing
+    // suffix to save RAM in the loaded terms index.
+    final int idxTermOffset = indexedTerm.offset;
+    final int priorTermOffset = priorTerm.offset;
+    final int limit = Math.min(priorTerm.length, indexedTerm.length);
+    for(int byteIdx=0;byteIdx<limit;byteIdx++) {
+      if (priorTerm.bytes[priorTermOffset+byteIdx] != indexedTerm.bytes[idxTermOffset+byteIdx]) {
+        return byteIdx+1;
+      }
+    }
+    return Math.min(1+priorTerm.length, indexedTerm.length);
+  }
+
+  private class SimpleFieldWriter extends FieldWriter {
+    final FieldInfo fieldInfo;
+    int numIndexTerms;
+    final long indexStart;
+    final long termsStart;
+    long packedIndexStart;
+    long packedOffsetsStart;
+    private long numTerms;
+
+    // TODO: we could conceivably make a PackedInts wrapper
+    // that auto-grows... then we wouldn't force 6 bytes RAM
+    // per index term:
+    private short[] termLengths;
+    private int[] termsPointerDeltas;
+    private long lastTermsPointer;
+    private long totTermLength;
+
+    private final BytesRef lastTerm = new BytesRef();
+
+    SimpleFieldWriter(FieldInfo fieldInfo, long termsFilePointer) {
+      this.fieldInfo = fieldInfo;
+      indexStart = out.getFilePointer();
+      termsStart = lastTermsPointer = termsFilePointer;
+      termLengths = new short[0];
+      termsPointerDeltas = new int[0];
+    }
+
+    @Override
+    public boolean checkIndexTerm(BytesRef text, TermStats stats) throws IOException {
+      // First term is first indexed term:
+      //System.out.println("FGW: checkIndexTerm text=" + text.utf8ToString());
+      if (0 == (numTerms++ % termIndexInterval)) {
+        return true;
+      } else {
+        if (0 == numTerms % termIndexInterval) {
+          // save last term just before next index term so we
+          // can compute wasted suffix
+          lastTerm.copyBytes(text);
+        }
+        return false;
+      }
+    }
+
+    @Override
+    public void add(BytesRef text, TermStats stats, long termsFilePointer) throws IOException {
+      final int indexedTermLength = indexedTermPrefixLength(lastTerm, text);
+      //System.out.println("FGW: add text=" + text.utf8ToString() + " " + text + " fp=" + termsFilePointer);
+
+      // write only the min prefix that shows the diff
+      // against prior term
+      out.writeBytes(text.bytes, text.offset, indexedTermLength);
+
+      if (termLengths.length == numIndexTerms) {
+        termLengths = ArrayUtil.grow(termLengths);
+      }
+      if (termsPointerDeltas.length == numIndexTerms) {
+        termsPointerDeltas = ArrayUtil.grow(termsPointerDeltas);
+      }
+
+      // save delta terms pointer
+      termsPointerDeltas[numIndexTerms] = (int) (termsFilePointer - lastTermsPointer);
+      lastTermsPointer = termsFilePointer;
+
+      // save term length (in bytes)
+      assert indexedTermLength <= Short.MAX_VALUE;
+      termLengths[numIndexTerms] = (short) indexedTermLength;
+      totTermLength += indexedTermLength;
+
+      lastTerm.copyBytes(text);
+      numIndexTerms++;
+    }
+
+    @Override
+    public void finish(long termsFilePointer) throws IOException {
+
+      // write primary terms dict offsets
+      packedIndexStart = out.getFilePointer();
+
+      PackedInts.Writer w = PackedInts.getWriter(out, numIndexTerms, PackedInts.bitsRequired(termsFilePointer), PackedInts.DEFAULT);
+
+      // relative to our indexStart
+      long upto = 0;
+      for(int i=0;i<numIndexTerms;i++) {
+        upto += termsPointerDeltas[i];
+        w.add(upto);
+      }
+      w.finish();
+
+      packedOffsetsStart = out.getFilePointer();
+
+      // write offsets into the byte[] terms
+      w = PackedInts.getWriter(out, 1+numIndexTerms, PackedInts.bitsRequired(totTermLength), PackedInts.DEFAULT);
+      upto = 0;
+      for(int i=0;i<numIndexTerms;i++) {
+        w.add(upto);
+        upto += termLengths[i];
+      }
+      w.add(upto);
+      w.finish();
+
+      // our referrer holds onto us, while other fields are
+      // being written, so don't tie up this RAM:
+      termLengths = null;
+      termsPointerDeltas = null;
+    }
+  }
+
+  public void close() throws IOException {
+    boolean success = false;
+    try {
+      final long dirStart = out.getFilePointer();
+      final int fieldCount = fields.size();
+      
+      int nonNullFieldCount = 0;
+      for(int i=0;i<fieldCount;i++) {
+        SimpleFieldWriter field = fields.get(i);
+        if (field.numIndexTerms > 0) {
+          nonNullFieldCount++;
+        }
+      }
+      
+      out.writeVInt(nonNullFieldCount);
+      for(int i=0;i<fieldCount;i++) {
+        SimpleFieldWriter field = fields.get(i);
+        if (field.numIndexTerms > 0) {
+          out.writeVInt(field.fieldInfo.number);
+          out.writeVInt(field.numIndexTerms);
+          out.writeVLong(field.termsStart);
+          out.writeVLong(field.indexStart);
+          out.writeVLong(field.packedIndexStart);
+          out.writeVLong(field.packedOffsetsStart);
+        }
+      }
+      writeTrailer(dirStart);
+      success = true;
+    } finally {
+      if (success) {
+        IOUtils.close(out);
+      } else {
+        IOUtils.closeWhileHandlingException(out);
+      }
+    }
+  }
+
+  protected void writeTrailer(long dirStart) throws IOException {
+    out.seek(CodecUtil.headerLength(CODEC_NAME));
+    out.writeLong(dirStart);
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/TermsIndexReaderBase.java b/lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/TermsIndexReaderBase.java
new file mode 100644
index 0000000..81b7c41
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/TermsIndexReaderBase.java
@@ -0,0 +1,74 @@
+package org.apache.lucene.codecs.blockterms;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.util.BytesRef;
+
+import java.io.IOException;
+import java.io.Closeable;
+
+
+// TODO
+//   - allow for non-regular index intervals?  eg with a
+//     long string of rare terms, you don't need such
+//     frequent indexing
+
+/**
+ * {@link BlockTermsReader} interacts with an instance of this class
+ * to manage its terms index.  The writer must accept
+ * indexed terms (many pairs of BytesRef text + long
+ * fileOffset), and then this reader must be able to
+ * retrieve the nearest index term to a provided term
+ * text. 
+ * @lucene.experimental */
+
+public abstract class TermsIndexReaderBase implements Closeable {
+
+  public abstract FieldIndexEnum getFieldEnum(FieldInfo fieldInfo);
+
+  public abstract void close() throws IOException;
+
+  public abstract boolean supportsOrd();
+
+  public abstract int getDivisor();
+
+  /** 
+   * Similar to TermsEnum, except, the only "metadata" it
+   * reports for a given indexed term is the long fileOffset
+   * into the main terms dictionary file.
+   */
+  public static abstract class FieldIndexEnum {
+
+    /** Seeks to "largest" indexed term that's <=
+     *  term; returns file pointer index (into the main
+     *  terms index file) for that term */
+    public abstract long seek(BytesRef term) throws IOException;
+
+    /** Returns -1 at end */
+    public abstract long next() throws IOException;
+
+    public abstract BytesRef term();
+
+    /** Only implemented if {@link TermsIndexReaderBase#supportsOrd()} returns true. */
+    public abstract long seek(long ord) throws IOException;
+    
+    /** Only implemented if {@link TermsIndexReaderBase#supportsOrd()} returns true. */
+    public abstract long ord();
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/TermsIndexWriterBase.java b/lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/TermsIndexWriterBase.java
new file mode 100644
index 0000000..d003ca4
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/TermsIndexWriterBase.java
@@ -0,0 +1,46 @@
+package org.apache.lucene.codecs.blockterms;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.codecs.TermStats;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.util.BytesRef;
+
+import java.io.Closeable;
+import java.io.IOException;
+
+/** 
+ * Base class for terms index implementations to plug
+ * into {@link BlockTermsWriter}.
+ * 
+ * @see TermsIndexReaderBase
+ * @lucene.experimental 
+ */
+public abstract class TermsIndexWriterBase implements Closeable {
+
+  /**
+   * Terms index API for a single field.
+   */
+  public abstract class FieldWriter {
+    public abstract boolean checkIndexTerm(BytesRef text, TermStats stats) throws IOException;
+    public abstract void add(BytesRef text, TermStats stats, long termsFilePointer) throws IOException;
+    public abstract void finish(long termsFilePointer) throws IOException;
+  }
+
+  public abstract FieldWriter addField(FieldInfo fieldInfo, long termsFilePointer) throws IOException;
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/VariableGapTermsIndexReader.java b/lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/VariableGapTermsIndexReader.java
new file mode 100644
index 0000000..c3df6ad
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/VariableGapTermsIndexReader.java
@@ -0,0 +1,235 @@
+package org.apache.lucene.codecs.blockterms;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.io.FileOutputStream;   // for toDot
+import java.io.OutputStreamWriter; // for toDot
+import java.io.Writer;             // for toDot
+import java.util.HashMap;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.index.CorruptIndexException;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.store.IOContext;
+import org.apache.lucene.store.IndexInput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IntsRef;
+import org.apache.lucene.util.fst.Builder;
+import org.apache.lucene.util.fst.BytesRefFSTEnum;
+import org.apache.lucene.util.fst.FST;
+import org.apache.lucene.util.fst.PositiveIntOutputs;
+import org.apache.lucene.util.fst.Util; // for toDot
+
+/** See {@link VariableGapTermsIndexWriter}
+ * 
+ * @lucene.experimental */
+public class VariableGapTermsIndexReader extends TermsIndexReaderBase {
+
+  private final PositiveIntOutputs fstOutputs = PositiveIntOutputs.getSingleton(true);
+  private int indexDivisor;
+
+  // Closed if indexLoaded is true:
+  private IndexInput in;
+  private volatile boolean indexLoaded;
+
+  final HashMap<FieldInfo,FieldIndexData> fields = new HashMap<FieldInfo,FieldIndexData>();
+  
+  // start of the field info data
+  protected long dirOffset;
+
+  final String segment;
+  public VariableGapTermsIndexReader(Directory dir, FieldInfos fieldInfos, String segment, int indexDivisor, String segmentSuffix, IOContext context)
+    throws IOException {
+    in = dir.openInput(IndexFileNames.segmentFileName(segment, segmentSuffix, VariableGapTermsIndexWriter.TERMS_INDEX_EXTENSION), new IOContext(context, true));
+    this.segment = segment;
+    boolean success = false;
+    assert indexDivisor == -1 || indexDivisor > 0;
+
+    try {
+      
+      readHeader(in);
+      this.indexDivisor = indexDivisor;
+
+      seekDir(in, dirOffset);
+
+      // Read directory
+      final int numFields = in.readVInt();
+      if (numFields < 0) {
+        throw new CorruptIndexException("invalid numFields: " + numFields + " (resource=" + in + ")");
+      }
+
+      for(int i=0;i<numFields;i++) {
+        final int field = in.readVInt();
+        final long indexStart = in.readVLong();
+        final FieldInfo fieldInfo = fieldInfos.fieldInfo(field);
+        FieldIndexData previous = fields.put(fieldInfo, new FieldIndexData(fieldInfo, indexStart));
+        if (previous != null) {
+          throw new CorruptIndexException("duplicate field: " + fieldInfo.name + " (resource=" + in + ")");
+        }
+      }
+      success = true;
+    } finally {
+      if (indexDivisor > 0) {
+        in.close();
+        in = null;
+        if (success) {
+          indexLoaded = true;
+        }
+      }
+    }
+  }
+
+  @Override
+  public int getDivisor() {
+    return indexDivisor;
+  }
+  
+  protected void readHeader(IndexInput input) throws IOException {
+    CodecUtil.checkHeader(input, VariableGapTermsIndexWriter.CODEC_NAME,
+      VariableGapTermsIndexWriter.VERSION_START, VariableGapTermsIndexWriter.VERSION_START);
+    dirOffset = input.readLong();
+  }
+
+  private static class IndexEnum extends FieldIndexEnum {
+    private final BytesRefFSTEnum<Long> fstEnum;
+    private BytesRefFSTEnum.InputOutput<Long> current;
+
+    public IndexEnum(FST<Long> fst) {
+      fstEnum = new BytesRefFSTEnum<Long>(fst);
+    }
+
+    @Override
+    public BytesRef term() {
+      if (current == null) {
+        return null;
+      } else {
+        return current.input;
+      }
+    }
+
+    @Override
+    public long seek(BytesRef target) throws IOException {
+      //System.out.println("VGR: seek field=" + fieldInfo.name + " target=" + target);
+      current = fstEnum.seekFloor(target);
+      //System.out.println("  got input=" + current.input + " output=" + current.output);
+      return current.output;
+    }
+
+    @Override
+    public long next() throws IOException {
+      //System.out.println("VGR: next field=" + fieldInfo.name);
+      current = fstEnum.next();
+      if (current == null) {
+        //System.out.println("  eof");
+        return -1;
+      } else {
+        return current.output;
+      }
+    }
+
+    @Override
+    public long ord() {
+      throw new UnsupportedOperationException();
+    }
+
+    @Override
+    public long seek(long ord) {
+      throw new UnsupportedOperationException();
+    }
+  }
+
+  @Override
+  public boolean supportsOrd() {
+    return false;
+  }
+
+  private final class FieldIndexData {
+
+    private final long indexStart;
+    // Set only if terms index is loaded:
+    private volatile FST<Long> fst;
+
+    public FieldIndexData(FieldInfo fieldInfo, long indexStart) throws IOException {
+      this.indexStart = indexStart;
+
+      if (indexDivisor > 0) {
+        loadTermsIndex();
+      }
+    }
+
+    private void loadTermsIndex() throws IOException {
+      if (fst == null) {
+        IndexInput clone = in.clone();
+        clone.seek(indexStart);
+        fst = new FST<Long>(clone, fstOutputs);
+        clone.close();
+
+        /*
+        final String dotFileName = segment + "_" + fieldInfo.name + ".dot";
+        Writer w = new OutputStreamWriter(new FileOutputStream(dotFileName));
+        Util.toDot(fst, w, false, false);
+        System.out.println("FST INDEX: SAVED to " + dotFileName);
+        w.close();
+        */
+
+        if (indexDivisor > 1) {
+          // subsample
+          final IntsRef scratchIntsRef = new IntsRef();
+          final PositiveIntOutputs outputs = PositiveIntOutputs.getSingleton(true);
+          final Builder<Long> builder = new Builder<Long>(FST.INPUT_TYPE.BYTE1, outputs);
+          final BytesRefFSTEnum<Long> fstEnum = new BytesRefFSTEnum<Long>(fst);
+          BytesRefFSTEnum.InputOutput<Long> result;
+          int count = indexDivisor;
+          while((result = fstEnum.next()) != null) {
+            if (count == indexDivisor) {
+              builder.add(Util.toIntsRef(result.input, scratchIntsRef), result.output);
+              count = 0;
+            }
+            count++;
+          }
+          fst = builder.finish();
+        }
+      }
+    }
+  }
+
+  @Override
+  public FieldIndexEnum getFieldEnum(FieldInfo fieldInfo) {
+    final FieldIndexData fieldData = fields.get(fieldInfo);
+    if (fieldData.fst == null) {
+      return null;
+    } else {
+      return new IndexEnum(fieldData.fst);
+    }
+  }
+
+  @Override
+  public void close() throws IOException {
+    if (in != null && !indexLoaded) {
+      in.close();
+    }
+  }
+
+  protected void seekDir(IndexInput input, long dirOffset) throws IOException {
+    input.seek(dirOffset);
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/VariableGapTermsIndexWriter.java b/lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/VariableGapTermsIndexWriter.java
new file mode 100644
index 0000000..da4687c
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/VariableGapTermsIndexWriter.java
@@ -0,0 +1,323 @@
+package org.apache.lucene.codecs.blockterms;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.lucene.codecs.CodecUtil;
+import org.apache.lucene.codecs.TermStats;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.FieldInfos;
+import org.apache.lucene.index.IndexFileNames;
+import org.apache.lucene.index.SegmentWriteState;
+import org.apache.lucene.store.IndexOutput;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.IOUtils;
+import org.apache.lucene.util.IntsRef;
+import org.apache.lucene.util.fst.Builder;
+import org.apache.lucene.util.fst.FST;
+import org.apache.lucene.util.fst.PositiveIntOutputs;
+import org.apache.lucene.util.fst.Util;
+
+/**
+ * Selects index terms according to provided pluggable
+ * {@link IndexTermSelector}, and stores them in a prefix trie that's
+ * loaded entirely in RAM stored as an FST.  This terms
+ * index only supports unsigned byte term sort order
+ * (unicode codepoint order when the bytes are UTF8).
+ *
+ * @lucene.experimental */
+public class VariableGapTermsIndexWriter extends TermsIndexWriterBase {
+  protected final IndexOutput out;
+
+  /** Extension of terms index file */
+  static final String TERMS_INDEX_EXTENSION = "tiv";
+
+  final static String CODEC_NAME = "VARIABLE_GAP_TERMS_INDEX";
+  final static int VERSION_START = 0;
+  final static int VERSION_CURRENT = VERSION_START;
+
+  private final List<FSTFieldWriter> fields = new ArrayList<FSTFieldWriter>();
+  
+  @SuppressWarnings("unused") private final FieldInfos fieldInfos; // unread
+  private final IndexTermSelector policy;
+
+  /** 
+   * Hook for selecting which terms should be placed in the terms index.
+   * <p>
+   * {@link #newField} is called at the start of each new field, and
+   * {@link #isIndexTerm} for each term in that field.
+   * 
+   * @lucene.experimental 
+   */
+  public static abstract class IndexTermSelector {
+    /** 
+     * Called sequentially on every term being written,
+     * returning true if this term should be indexed
+     */
+    public abstract boolean isIndexTerm(BytesRef term, TermStats stats);
+    /**
+     * Called when a new field is started.
+     */
+    public abstract void newField(FieldInfo fieldInfo);
+  }
+
+  /** Same policy as {@link FixedGapTermsIndexWriter} */
+  public static final class EveryNTermSelector extends IndexTermSelector {
+    private int count;
+    private final int interval;
+
+    public EveryNTermSelector(int interval) {
+      this.interval = interval;
+      // First term is first indexed term:
+      count = interval;
+    }
+
+    @Override
+    public boolean isIndexTerm(BytesRef term, TermStats stats) {
+      if (count >= interval) {
+        count = 1;
+        return true;
+      } else {
+        count++;
+        return false;
+      }
+    }
+
+    @Override
+    public void newField(FieldInfo fieldInfo) {
+      count = interval;
+    }
+  }
+
+  /** Sets an index term when docFreq >= docFreqThresh, or
+   *  every interval terms.  This should reduce seek time
+   *  to high docFreq terms.  */
+  public static final class EveryNOrDocFreqTermSelector extends IndexTermSelector {
+    private int count;
+    private final int docFreqThresh;
+    private final int interval;
+
+    public EveryNOrDocFreqTermSelector(int docFreqThresh, int interval) {
+      this.interval = interval;
+      this.docFreqThresh = docFreqThresh;
+
+      // First term is first indexed term:
+      count = interval;
+    }
+
+    @Override
+    public boolean isIndexTerm(BytesRef term, TermStats stats) {
+      if (stats.docFreq >= docFreqThresh || count >= interval) {
+        count = 1;
+        return true;
+      } else {
+        count++;
+        return false;
+      }
+    }
+
+    @Override
+    public void newField(FieldInfo fieldInfo) {
+      count = interval;
+    }
+  }
+
+  // TODO: it'd be nice to let the FST builder prune based
+  // on term count of each node (the prune1/prune2 that it
+  // accepts), and build the index based on that.  This
+  // should result in a more compact terms index, more like
+  // a prefix trie than the other selectors, because it
+  // only stores enough leading bytes to get down to N
+  // terms that may complete that prefix.  It becomes
+  // "deeper" when terms are dense, and "shallow" when they
+  // are less dense.
+  //
+  // However, it's not easy to make that work this this
+  // API, because that pruning doesn't immediately know on
+  // seeing each term whether that term will be a seek point
+  // or not.  It requires some non-causality in the API, ie
+  // only on seeing some number of future terms will the
+  // builder decide which past terms are seek points.
+  // Somehow the API'd need to be able to return a "I don't
+  // know" value, eg like a Future, which only later on is
+  // flipped (frozen) to true or false.
+  //
+  // We could solve this with a 2-pass approach, where the
+  // first pass would build an FSA (no outputs) solely to
+  // determine which prefixes are the 'leaves' in the
+  // pruning. The 2nd pass would then look at this prefix
+  // trie to mark the seek points and build the FST mapping
+  // to the true output.
+  //
+  // But, one downside to this approach is that it'd result
+  // in uneven index term selection.  EG with prune1=10, the
+  // resulting index terms could be as frequent as every 10
+  // terms or as rare as every <maxArcCount> * 10 (eg 2560),
+  // in the extremes.
+
+  public VariableGapTermsIndexWriter(SegmentWriteState state, IndexTermSelector policy) throws IOException {
+    final String indexFileName = IndexFileNames.segmentFileName(state.segmentInfo.name, state.segmentSuffix, TERMS_INDEX_EXTENSION);
+    out = state.directory.createOutput(indexFileName, state.context);
+    boolean success = false;
+    try {
+      fieldInfos = state.fieldInfos;
+      this.policy = policy;
+      writeHeader(out);
+      success = true;
+    } finally {
+      if (!success) {
+        IOUtils.closeWhileHandlingException(out);
+      }
+    }
+  }
+  
+  protected void writeHeader(IndexOutput out) throws IOException {
+    CodecUtil.writeHeader(out, CODEC_NAME, VERSION_CURRENT);
+    // Placeholder for dir offset
+    out.writeLong(0);
+  }
+
+  @Override
+  public FieldWriter addField(FieldInfo field, long termsFilePointer) throws IOException {
+    ////System.out.println("VGW: field=" + field.name);
+    policy.newField(field);
+    FSTFieldWriter writer = new FSTFieldWriter(field, termsFilePointer);
+    fields.add(writer);
+    return writer;
+  }
+
+  /** NOTE: if your codec does not sort in unicode code
+   *  point order, you must override this method, to simply
+   *  return indexedTerm.length. */
+  protected int indexedTermPrefixLength(final BytesRef priorTerm, final BytesRef indexedTerm) {
+    // As long as codec sorts terms in unicode codepoint
+    // order, we can safely strip off the non-distinguishing
+    // suffix to save RAM in the loaded terms index.
+    final int idxTermOffset = indexedTerm.offset;
+    final int priorTermOffset = priorTerm.offset;
+    final int limit = Math.min(priorTerm.length, indexedTerm.length);
+    for(int byteIdx=0;byteIdx<limit;byteIdx++) {
+      if (priorTerm.bytes[priorTermOffset+byteIdx] != indexedTerm.bytes[idxTermOffset+byteIdx]) {
+        return byteIdx+1;
+      }
+    }
+    return Math.min(1+priorTerm.length, indexedTerm.length);
+  }
+
+  private class FSTFieldWriter extends FieldWriter {
+    private final Builder<Long> fstBuilder;
+    private final PositiveIntOutputs fstOutputs;
+    private final long startTermsFilePointer;
+
+    final FieldInfo fieldInfo;
+    FST<Long> fst;
+    final long indexStart;
+
+    private final BytesRef lastTerm = new BytesRef();
+    private boolean first = true;
+
+    public FSTFieldWriter(FieldInfo fieldInfo, long termsFilePointer) throws IOException {
+      this.fieldInfo = fieldInfo;
+      fstOutputs = PositiveIntOutputs.getSingleton(true);
+      fstBuilder = new Builder<Long>(FST.INPUT_TYPE.BYTE1, fstOutputs);
+      indexStart = out.getFilePointer();
+      ////System.out.println("VGW: field=" + fieldInfo.name);
+
+      // Always put empty string in
+      fstBuilder.add(new IntsRef(), termsFilePointer);
+      startTermsFilePointer = termsFilePointer;
+    }
+
+    @Override
+    public boolean checkIndexTerm(BytesRef text, TermStats stats) throws IOException {
+      //System.out.println("VGW: index term=" + text.utf8ToString());
+      // NOTE: we must force the first term per field to be
+      // indexed, in case policy doesn't:
+      if (policy.isIndexTerm(text, stats) || first) {
+        first = false;
+        //System.out.println("  YES");
+        return true;
+      } else {
+        lastTerm.copyBytes(text);
+        return false;
+      }
+    }
+
+    private final IntsRef scratchIntsRef = new IntsRef();
+
+    @Override
+    public void add(BytesRef text, TermStats stats, long termsFilePointer) throws IOException {
+      if (text.length == 0) {
+        // We already added empty string in ctor
+        assert termsFilePointer == startTermsFilePointer;
+        return;
+      }
+      final int lengthSave = text.length;
+      text.length = indexedTermPrefixLength(lastTerm, text);
+      try {
+        fstBuilder.add(Util.toIntsRef(text, scratchIntsRef), termsFilePointer);
+      } finally {
+        text.length = lengthSave;
+      }
+      lastTerm.copyBytes(text);
+    }
+
+    @Override
+    public void finish(long termsFilePointer) throws IOException {
+      fst = fstBuilder.finish();
+      if (fst != null) {
+        fst.save(out);
+      }
+    }
+  }
+
+  public void close() throws IOException {
+    try {
+    final long dirStart = out.getFilePointer();
+    final int fieldCount = fields.size();
+
+    int nonNullFieldCount = 0;
+    for(int i=0;i<fieldCount;i++) {
+      FSTFieldWriter field = fields.get(i);
+      if (field.fst != null) {
+        nonNullFieldCount++;
+      }
+    }
+
+    out.writeVInt(nonNullFieldCount);
+    for(int i=0;i<fieldCount;i++) {
+      FSTFieldWriter field = fields.get(i);
+      if (field.fst != null) {
+        out.writeVInt(field.fieldInfo.number);
+        out.writeVLong(field.indexStart);
+      }
+    }
+    writeTrailer(dirStart);
+    } finally {
+    out.close();
+  }
+  }
+
+  protected void writeTrailer(long dirStart) throws IOException {
+    out.seek(CodecUtil.headerLength(CODEC_NAME));
+    out.writeLong(dirStart);
+  }
+}
diff --git a/lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/package.html b/lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/package.html
new file mode 100644
index 0000000..95ebecf
--- /dev/null
+++ b/lucene/codecs/src/java/org/apache/lucene/codecs/blockterms/package.html
@@ -0,0 +1,25 @@
+<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+<html>
+<head>
+   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
+</head>
+<body>
+Pluggable term index / block terms dictionary implementations.
+</body>
+</html>
diff --git a/lucene/common-build.xml b/lucene/common-build.xml
index 57f2c75..2fdf1ef 100644
--- a/lucene/common-build.xml
+++ b/lucene/common-build.xml
@@ -569,6 +569,12 @@
     </uptodate>
   </target>
 
+  <target name="check-lucene-codecs-javadocs-uptodate" unless="codecs-javadocs.uptodate">
+    <uptodate property="codecs-javadocs.uptodate" targetfile="${common.dir}/build/codecs/lucene-codecs-${version}-javadoc.jar">
+       <srcfiles dir="${common.dir}/codecs/src/java" includes="**/*.java"/>
+    </uptodate>
+  </target>
+
   <target name="javadocs-lucene-core" depends="check-lucene-core-javadocs-uptodate" unless="core-javadocs.uptodate">
     <ant dir="${common.dir}/core" target="javadocs" inheritAll="false">
       <propertyset refid="uptodate.and.compiled.properties"/>
@@ -576,8 +582,18 @@
     <property name="core-javadocs.uptodate" value="true"/>
   </target>
 
-  <target name="compile-codecs">
-    <ant dir="${common.dir}/codecs" target="compile-core" inheritAll="false"/>
+  <target name="compile-codecs" unless="codecs.compiled">
+    <ant dir="${common.dir}/codecs" target="compile-core" inheritAll="false">
+      <propertyset refid="uptodate.and.compiled.properties"/>
+    </ant>
+    <property name="codecs.compiled" value="true"/>
+  </target>
+
+  <target name="javadocs-lucene-codecs" depends="check-lucene-codecs-javadocs-uptodate" unless="codecs-javadocs.uptodate">
+    <ant dir="${common.dir}/codecs" target="javadocs" inheritAll="false">
+      <propertyset refid="uptodate.and.compiled.properties"/>
+    </ant>
+    <property name="codecs-javadocs.uptodate" value="true"/>
   </target>
 
   <target name="compile-test-framework" unless="lucene.test.framework.compiled">
diff --git a/lucene/test-framework/build.xml b/lucene/test-framework/build.xml
index 26c9641..e240a86 100644
--- a/lucene/test-framework/build.xml
+++ b/lucene/test-framework/build.xml
@@ -47,7 +47,7 @@
   </target>
 
   <target name="javadocs-core" depends="javadocs"/>
-  <target name="javadocs" depends="init,javadocs-lucene-core">
+  <target name="javadocs" depends="init,javadocs-lucene-core,javadocs-lucene-codecs">
     <sequential>
       <mkdir dir="${javadoc.dir}/test-framework"/>
       <invoke-javadoc overview="${src.dir}/overview.html"
@@ -58,6 +58,7 @@
           <link offline="true" href="${javadoc.link.junit}"
                 packagelistLoc="${javadoc.packagelist.dir}/junit"/>
           <link href="../core/"/>
+          <link href="../codecs/"/>
           <link href=""/>
         </sources>
       </invoke-javadoc>
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40ords/Lucene40WithOrds.java b/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40ords/Lucene40WithOrds.java
index dbb76eb..ae5da6e 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40ords/Lucene40WithOrds.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/codecs/lucene40ords/Lucene40WithOrds.java
@@ -19,17 +19,17 @@ package org.apache.lucene.codecs.lucene40ords;
 
 import java.io.IOException;
 
-import org.apache.lucene.codecs.BlockTermsReader;
-import org.apache.lucene.codecs.BlockTermsWriter;
 import org.apache.lucene.codecs.FieldsConsumer;
 import org.apache.lucene.codecs.FieldsProducer;
-import org.apache.lucene.codecs.FixedGapTermsIndexReader;
-import org.apache.lucene.codecs.FixedGapTermsIndexWriter;
 import org.apache.lucene.codecs.PostingsFormat;
 import org.apache.lucene.codecs.PostingsReaderBase;
 import org.apache.lucene.codecs.PostingsWriterBase;
-import org.apache.lucene.codecs.TermsIndexReaderBase;
-import org.apache.lucene.codecs.TermsIndexWriterBase;
+import org.apache.lucene.codecs.blockterms.BlockTermsReader;
+import org.apache.lucene.codecs.blockterms.BlockTermsWriter;
+import org.apache.lucene.codecs.blockterms.FixedGapTermsIndexReader;
+import org.apache.lucene.codecs.blockterms.FixedGapTermsIndexWriter;
+import org.apache.lucene.codecs.blockterms.TermsIndexReaderBase;
+import org.apache.lucene.codecs.blockterms.TermsIndexWriterBase;
 import org.apache.lucene.codecs.lucene40.Lucene40Codec; // javadocs
 import org.apache.lucene.codecs.lucene40.Lucene40PostingsReader;
 import org.apache.lucene.codecs.lucene40.Lucene40PostingsWriter;
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/mockintblock/MockFixedIntBlockPostingsFormat.java b/lucene/test-framework/src/java/org/apache/lucene/codecs/mockintblock/MockFixedIntBlockPostingsFormat.java
index f249471..e3eec01 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/codecs/mockintblock/MockFixedIntBlockPostingsFormat.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/codecs/mockintblock/MockFixedIntBlockPostingsFormat.java
@@ -19,17 +19,17 @@ package org.apache.lucene.codecs.mockintblock;
 
 import java.io.IOException;
 
-import org.apache.lucene.codecs.BlockTermsReader;
-import org.apache.lucene.codecs.BlockTermsWriter;
 import org.apache.lucene.codecs.FieldsConsumer;
 import org.apache.lucene.codecs.FieldsProducer;
-import org.apache.lucene.codecs.FixedGapTermsIndexReader;
-import org.apache.lucene.codecs.FixedGapTermsIndexWriter;
 import org.apache.lucene.codecs.PostingsFormat;
 import org.apache.lucene.codecs.PostingsReaderBase;
 import org.apache.lucene.codecs.PostingsWriterBase;
-import org.apache.lucene.codecs.TermsIndexReaderBase;
-import org.apache.lucene.codecs.TermsIndexWriterBase;
+import org.apache.lucene.codecs.blockterms.BlockTermsReader;
+import org.apache.lucene.codecs.blockterms.BlockTermsWriter;
+import org.apache.lucene.codecs.blockterms.FixedGapTermsIndexReader;
+import org.apache.lucene.codecs.blockterms.FixedGapTermsIndexWriter;
+import org.apache.lucene.codecs.blockterms.TermsIndexReaderBase;
+import org.apache.lucene.codecs.blockterms.TermsIndexWriterBase;
 import org.apache.lucene.codecs.intblock.FixedIntBlockIndexInput;
 import org.apache.lucene.codecs.intblock.FixedIntBlockIndexOutput;
 import org.apache.lucene.codecs.sep.IntIndexInput;
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/mockintblock/MockVariableIntBlockPostingsFormat.java b/lucene/test-framework/src/java/org/apache/lucene/codecs/mockintblock/MockVariableIntBlockPostingsFormat.java
index f5679cc..12db9de 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/codecs/mockintblock/MockVariableIntBlockPostingsFormat.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/codecs/mockintblock/MockVariableIntBlockPostingsFormat.java
@@ -19,17 +19,17 @@ package org.apache.lucene.codecs.mockintblock;
 
 import java.io.IOException;
 
-import org.apache.lucene.codecs.BlockTermsReader;
-import org.apache.lucene.codecs.BlockTermsWriter;
 import org.apache.lucene.codecs.FieldsConsumer;
 import org.apache.lucene.codecs.FieldsProducer;
-import org.apache.lucene.codecs.FixedGapTermsIndexReader;
-import org.apache.lucene.codecs.FixedGapTermsIndexWriter;
 import org.apache.lucene.codecs.PostingsFormat;
 import org.apache.lucene.codecs.PostingsReaderBase;
 import org.apache.lucene.codecs.PostingsWriterBase;
-import org.apache.lucene.codecs.TermsIndexReaderBase;
-import org.apache.lucene.codecs.TermsIndexWriterBase;
+import org.apache.lucene.codecs.blockterms.BlockTermsReader;
+import org.apache.lucene.codecs.blockterms.BlockTermsWriter;
+import org.apache.lucene.codecs.blockterms.FixedGapTermsIndexReader;
+import org.apache.lucene.codecs.blockterms.FixedGapTermsIndexWriter;
+import org.apache.lucene.codecs.blockterms.TermsIndexReaderBase;
+import org.apache.lucene.codecs.blockterms.TermsIndexWriterBase;
 import org.apache.lucene.codecs.intblock.VariableIntBlockIndexInput;
 import org.apache.lucene.codecs.intblock.VariableIntBlockIndexOutput;
 import org.apache.lucene.codecs.sep.IntIndexInput;
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/mockrandom/MockRandomPostingsFormat.java b/lucene/test-framework/src/java/org/apache/lucene/codecs/mockrandom/MockRandomPostingsFormat.java
index 2089753..3fe6348 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/codecs/mockrandom/MockRandomPostingsFormat.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/codecs/mockrandom/MockRandomPostingsFormat.java
@@ -22,22 +22,22 @@ import java.util.ArrayList;
 import java.util.List;
 import java.util.Random;
 
-import org.apache.lucene.codecs.BlockTermsReader;
-import org.apache.lucene.codecs.BlockTermsWriter;
 import org.apache.lucene.codecs.BlockTreeTermsReader;
 import org.apache.lucene.codecs.BlockTreeTermsWriter;
 import org.apache.lucene.codecs.FieldsConsumer;
 import org.apache.lucene.codecs.FieldsProducer;
-import org.apache.lucene.codecs.FixedGapTermsIndexReader;
-import org.apache.lucene.codecs.FixedGapTermsIndexWriter;
 import org.apache.lucene.codecs.PostingsFormat;
 import org.apache.lucene.codecs.PostingsReaderBase;
 import org.apache.lucene.codecs.PostingsWriterBase;
 import org.apache.lucene.codecs.TermStats;
-import org.apache.lucene.codecs.TermsIndexReaderBase;
-import org.apache.lucene.codecs.TermsIndexWriterBase;
-import org.apache.lucene.codecs.VariableGapTermsIndexReader;
-import org.apache.lucene.codecs.VariableGapTermsIndexWriter;
+import org.apache.lucene.codecs.blockterms.BlockTermsReader;
+import org.apache.lucene.codecs.blockterms.BlockTermsWriter;
+import org.apache.lucene.codecs.blockterms.FixedGapTermsIndexReader;
+import org.apache.lucene.codecs.blockterms.FixedGapTermsIndexWriter;
+import org.apache.lucene.codecs.blockterms.TermsIndexReaderBase;
+import org.apache.lucene.codecs.blockterms.TermsIndexWriterBase;
+import org.apache.lucene.codecs.blockterms.VariableGapTermsIndexReader;
+import org.apache.lucene.codecs.blockterms.VariableGapTermsIndexWriter;
 import org.apache.lucene.codecs.lucene40.Lucene40PostingsReader;
 import org.apache.lucene.codecs.lucene40.Lucene40PostingsWriter;
 import org.apache.lucene.codecs.mockintblock.MockFixedIntBlockPostingsFormat;
diff --git a/lucene/test-framework/src/java/org/apache/lucene/codecs/mocksep/MockSepPostingsFormat.java b/lucene/test-framework/src/java/org/apache/lucene/codecs/mocksep/MockSepPostingsFormat.java
index 220bd39..c759509 100644
--- a/lucene/test-framework/src/java/org/apache/lucene/codecs/mocksep/MockSepPostingsFormat.java
+++ b/lucene/test-framework/src/java/org/apache/lucene/codecs/mocksep/MockSepPostingsFormat.java
@@ -19,17 +19,17 @@ package org.apache.lucene.codecs.mocksep;
 
 import java.io.IOException;
 
-import org.apache.lucene.codecs.BlockTermsReader;
-import org.apache.lucene.codecs.BlockTermsWriter;
 import org.apache.lucene.codecs.FieldsConsumer;
 import org.apache.lucene.codecs.FieldsProducer;
-import org.apache.lucene.codecs.FixedGapTermsIndexReader;
-import org.apache.lucene.codecs.FixedGapTermsIndexWriter;
 import org.apache.lucene.codecs.PostingsFormat;
 import org.apache.lucene.codecs.PostingsReaderBase;
 import org.apache.lucene.codecs.PostingsWriterBase;
-import org.apache.lucene.codecs.TermsIndexReaderBase;
-import org.apache.lucene.codecs.TermsIndexWriterBase;
+import org.apache.lucene.codecs.blockterms.BlockTermsReader;
+import org.apache.lucene.codecs.blockterms.BlockTermsWriter;
+import org.apache.lucene.codecs.blockterms.FixedGapTermsIndexReader;
+import org.apache.lucene.codecs.blockterms.FixedGapTermsIndexWriter;
+import org.apache.lucene.codecs.blockterms.TermsIndexReaderBase;
+import org.apache.lucene.codecs.blockterms.TermsIndexWriterBase;
 import org.apache.lucene.codecs.sep.SepPostingsReader;
 import org.apache.lucene.codecs.sep.SepPostingsWriter;
 import org.apache.lucene.index.SegmentWriteState;

