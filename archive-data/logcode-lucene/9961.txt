GitDiffStart: 64e18dc0f6beaeeaeb03bc16501b878b4be9fe99 | Sat Jan 5 22:51:30 2013 +0000
diff --git a/lucene/CHANGES.txt b/lucene/CHANGES.txt
index a066fb8..59c99b1 100644
--- a/lucene/CHANGES.txt
+++ b/lucene/CHANGES.txt
@@ -162,7 +162,7 @@ New Features
   extractWikipedia.alg was changed to use this task, so now it creates two
   files. (Doron Cohen)
 
-* LUCENE-4290: Added PostingsHighlighter to the sandbox module. It uses
+* LUCENE-4290: Added PostingsHighlighter to the highlighter module. It uses
   offsets from the postings lists to highlight documents. (Robert Muir)
 
 * LUCENE-4628: Added CommonTermsQuery that executes high-frequency terms
diff --git a/lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/Passage.java b/lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/Passage.java
new file mode 100644
index 0000000..58fe7eb
--- /dev/null
+++ b/lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/Passage.java
@@ -0,0 +1,125 @@
+package org.apache.lucene.search.postingshighlight;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.index.Term;
+import org.apache.lucene.util.ArrayUtil;
+import org.apache.lucene.util.RamUsageEstimator;
+
+/**
+ * Represents a passage (typically a sentence of the document). 
+ * <p>
+ * A passage contains {@link #getNumMatches} highlights from the query,
+ * and the offsets and query terms that correspond with each match.
+ * @lucene.experimental
+ */
+public final class Passage {
+  int startOffset = -1;
+  int endOffset = -1;
+  float score = 0.0f;
+
+  int matchStarts[] = new int[8];
+  int matchEnds[] = new int[8];
+  Term matchTerms[] = new Term[8];
+  int numMatches = 0;
+  
+  void addMatch(int startOffset, int endOffset, Term term) {
+    assert startOffset >= this.startOffset && startOffset <= this.endOffset;
+    if (numMatches == matchStarts.length) {
+      matchStarts = ArrayUtil.grow(matchStarts, numMatches+1);
+      matchEnds = ArrayUtil.grow(matchEnds, numMatches+1);
+      Term newMatchTerms[] = new Term[ArrayUtil.oversize(numMatches+1, RamUsageEstimator.NUM_BYTES_OBJECT_REF)];
+      System.arraycopy(matchTerms, 0, newMatchTerms, 0, numMatches);
+      matchTerms = newMatchTerms;
+    }
+    matchStarts[numMatches] = startOffset;
+    matchEnds[numMatches] = endOffset;
+    matchTerms[numMatches] = term;
+    numMatches++;
+  }
+  
+  void reset() {
+    startOffset = endOffset = -1;
+    score = 0.0f;
+    numMatches = 0;
+  }
+
+  /**
+   * Start offset of this passage.
+   * @return start index (inclusive) of the passage in the 
+   *         original content: always &gt;= 0.
+   */
+  public int getStartOffset() {
+    return startOffset;
+  }
+
+  /**
+   * End offset of this passage.
+   * @return end index (exclusive) of the passage in the 
+   *         original content: always &gt;= {@link #getStartOffset()}
+   */
+  public int getEndOffset() {
+    return endOffset;
+  }
+
+  /**
+   * Passage's score.
+   */
+  public float getScore() {
+    return score;
+  }
+  
+  /**
+   * Number of term matches available in 
+   * {@link #getMatchStarts}, {@link #getMatchEnds}, 
+   * {@link #getMatchTerms}
+   */
+  public int getNumMatches() {
+    return numMatches;
+  }
+
+  /**
+   * Start offsets of the term matches, in increasing order.
+   * <p>
+   * Only {@link #getNumMatches} are valid. Note that these
+   * offsets are absolute (not relative to {@link #getStartOffset()}).
+   */
+  public int[] getMatchStarts() {
+    return matchStarts;
+  }
+
+  /**
+   * End offsets of the term matches, corresponding with {@link #getMatchStarts}. 
+   * <p>
+   * Only {@link #getNumMatches} are valid. Note that its possible that an end offset 
+   * could exceed beyond the bounds of the passage ({@link #getEndOffset()}), if the 
+   * Analyzer produced a term which spans a passage boundary.
+   */
+  public int[] getMatchEnds() {
+    return matchEnds;
+  }
+
+  /**
+   * Term of the matches, corresponding with {@link #getMatchStarts()}.
+   * <p>
+   * Only {@link #getNumMatches()} are valid.
+   */
+  public Term[] getMatchTerms() {
+    return matchTerms;
+  }
+}
diff --git a/lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PassageFormatter.java b/lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PassageFormatter.java
new file mode 100644
index 0000000..041267c
--- /dev/null
+++ b/lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PassageFormatter.java
@@ -0,0 +1,92 @@
+package org.apache.lucene.search.postingshighlight;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Creates a formatted snippet from the top passages.
+ * <p>
+ * The default implementation marks the query terms as bold, and places
+ * ellipses between unconnected passages.
+ * @lucene.experimental
+ */
+public class PassageFormatter {
+  private final String preTag;
+  private final String postTag;
+  private final String ellipsis;
+  
+  /**
+   * Creates a new PassageFormatter with the default tags.
+   */
+  public PassageFormatter() {
+    this("<b>", "</b>", "... ");
+  }
+  
+  /**
+   * Creates a new PassageFormatter with custom tags.
+   * @param preTag text which should appear before a highlighted term.
+   * @param postTag text which should appear after a highlighted term.
+   * @param ellipsis text which should be used to connect two unconnected passages.
+   */
+  public PassageFormatter(String preTag, String postTag, String ellipsis) {
+    if (preTag == null || postTag == null || ellipsis == null) {
+      throw new NullPointerException();
+    }
+    this.preTag = preTag;
+    this.postTag = postTag;
+    this.ellipsis = ellipsis;
+  }
+  
+  /**
+   * Formats the top <code>passages</code> from <code>content</code>
+   * into a human-readable text snippet.
+   * 
+   * @param passages top-N passages for the field. Note these are sorted in
+   *        the order that they appear in the document for convenience.
+   * @param content content for the field.
+   * @return formatted highlight
+   */
+  public String format(Passage passages[], String content) {
+    StringBuilder sb = new StringBuilder();
+    int pos = 0;
+    for (Passage passage : passages) {
+      // don't add ellipsis if its the first one, or if its connected.
+      if (passage.startOffset > pos && pos > 0) {
+        sb.append(ellipsis);
+      }
+      pos = passage.startOffset;
+      for (int i = 0; i < passage.numMatches; i++) {
+        int start = passage.matchStarts[i];
+        int end = passage.matchEnds[i];
+        // its possible to have overlapping terms
+        if (start > pos) {
+          sb.append(content.substring(pos, start));
+        }
+        if (end > pos) {
+          sb.append(preTag);
+          sb.append(content.substring(Math.max(pos, start), end));
+          sb.append(postTag);
+          pos = end;
+        }
+      }
+      // its possible a "term" from the analyzer could span a sentence boundary.
+      sb.append(content.substring(pos, Math.max(pos, passage.endOffset)));
+      pos = passage.endOffset;
+    }
+    return sb.toString();
+  }
+}
diff --git a/lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PassageScorer.java b/lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PassageScorer.java
new file mode 100644
index 0000000..4d36450
--- /dev/null
+++ b/lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PassageScorer.java
@@ -0,0 +1,83 @@
+package org.apache.lucene.search.postingshighlight;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/** 
+ * Ranks passages found by {@link PostingsHighlighter}.
+ * <p>
+ * Each passage is scored as a miniature document within the document.
+ * The final score is computed as {@link #norm} * &sum; ({@link #weight} * {@link #tf}).
+ * The default implementation is {@link #norm} * BM25.
+ * @lucene.experimental
+ */
+public class PassageScorer {
+  
+  // TODO: this formula is completely made up. It might not provide relevant snippets!
+  
+  /** BM25 k1 parameter, controls term frequency normalization */
+  public static final float k1 = 1.2f;
+  /** BM25 b parameter, controls length normalization. */
+  public static final float b = 0.75f;
+  
+  /**
+   * A pivot used for length normalization.
+   * The default value is the typical average English sentence length.
+   */
+  public static final float pivot = 87f;
+    
+  /**
+   * Computes term importance, given its in-document statistics.
+   * 
+   * @param contentLength length of document in characters
+   * @param totalTermFreq number of time term occurs in document
+   * @return term importance
+   */
+  public float weight(int contentLength, int totalTermFreq) {
+    // approximate #docs from content length
+    float numDocs = 1 + contentLength / pivot;
+    // numDocs not numDocs - docFreq (ala DFR), since we approximate numDocs
+    return (k1 + 1) * (float) Math.log(1 + (numDocs + 0.5D)/(totalTermFreq + 0.5D));
+  }
+
+  /**
+   * Computes term weight, given the frequency within the passage
+   * and the passage's length.
+   * 
+   * @param freq number of occurrences of within this passage
+   * @param passageLen length of the passage in characters.
+   * @return term weight
+   */
+  public float tf(int freq, int passageLen) {
+    float norm = k1 * ((1 - b) + b * (passageLen / pivot));
+    return freq / (freq + norm);
+  }
+    
+  /**
+   * Normalize a passage according to its position in the document.
+   * <p>
+   * Typically passages towards the beginning of the document are 
+   * more useful for summarizing the contents.
+   * <p>
+   * The default implementation is <code>1 + 1/log(pivot + passageStart)</code>
+   * @param passageStart start offset of the passage
+   * @return a boost value multiplied into the passage's core.
+   */
+  public float norm(int passageStart) {
+    return 1 + 1/(float)Math.log(pivot + passageStart);
+  }
+}
diff --git a/lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter.java b/lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter.java
new file mode 100644
index 0000000..e39081c
--- /dev/null
+++ b/lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/PostingsHighlighter.java
@@ -0,0 +1,556 @@
+package org.apache.lucene.search.postingshighlight;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.text.BreakIterator;
+import java.util.Arrays;
+import java.util.Comparator;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Locale;
+import java.util.Map;
+import java.util.PriorityQueue;
+import java.util.SortedSet;
+import java.util.TreeSet;
+
+import org.apache.lucene.index.AtomicReader;
+import org.apache.lucene.index.AtomicReaderContext;
+import org.apache.lucene.index.DocsAndPositionsEnum;
+import org.apache.lucene.index.FieldInfo;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexReaderContext;
+import org.apache.lucene.index.MultiReader;
+import org.apache.lucene.index.ReaderUtil;
+import org.apache.lucene.index.StoredFieldVisitor;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.index.Terms;
+import org.apache.lucene.index.TermsEnum;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.ScoreDoc;
+import org.apache.lucene.search.TopDocs;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.UnicodeUtil;
+
+/**
+ * Simple highlighter that does not analyze fields nor use
+ * term vectors. Instead it requires 
+ * {@link IndexOptions#DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS}.
+ * <p>
+ * PostingsHighlighter treats the single original document as the whole corpus, and then scores individual
+ * passages as if they were documents in this corpus. It uses a {@link BreakIterator} to find 
+ * passages in the text; by default it breaks using {@link BreakIterator#getSentenceInstance(Locale) 
+ * getSentenceInstance(Locale.ROOT)}. It then iterates in parallel (merge sorting by offset) through 
+ * the positions of all terms from the query, coalescing those hits that occur in a single passage 
+ * into a {@link Passage}, and then scores each Passage using a separate {@link PassageScorer}. 
+ * Passages are finally formatted into highlighted snippets with a {@link PassageFormatter}.
+ * <p>
+ * <b>WARNING</b>: The code is very new and may still have some exciting bugs!
+ * <p>
+ * Example usage:
+ * <pre class="prettyprint">
+ *   // configure field with offsets at index time
+ *   FieldType offsetsType = new FieldType(TextField.TYPE_STORED);
+ *   offsetsType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);
+ *   Field body = new Field("body", "foobar", offsetsType);
+ *
+ *   // retrieve highlights at query time 
+ *   PostingsHighlighter highlighter = new PostingsHighlighter();
+ *   Query query = new TermQuery(new Term("body", "highlighting"));
+ *   TopDocs topDocs = searcher.search(query, n);
+ *   String highlights[] = highlighter.highlight("body", query, searcher, topDocs);
+ * </pre>
+ * <p>
+ * This is thread-safe, and can be used across different readers.
+ * @lucene.experimental
+ */
+public final class PostingsHighlighter {
+  
+  // TODO: maybe allow re-analysis for tiny fields? currently we require offsets,
+  // but if the analyzer is really fast and the field is tiny, this might really be
+  // unnecessary.
+  
+  /** for rewriting: we don't want slow processing from MTQs */
+  private static final IndexReader EMPTY_INDEXREADER = new MultiReader();
+  
+  /** Default maximum content size to process. Typically snippets
+   *  closer to the beginning of the document better summarize its content */
+  public static final int DEFAULT_MAX_LENGTH = 10000;
+    
+  private final int maxLength;
+  private final BreakIterator breakIterator;
+  private final PassageScorer scorer;
+  private final PassageFormatter formatter;
+  
+  /**
+   * Creates a new highlighter with default parameters.
+   */
+  public PostingsHighlighter() {
+    this(DEFAULT_MAX_LENGTH);
+  }
+  
+  /**
+   * Creates a new highlighter, specifying maximum content length.
+   * @param maxLength maximum content size to process.
+   * @throws IllegalArgumentException if <code>maxLength</code> is negative or <code>Integer.MAX_VALUE</code>
+   */
+  public PostingsHighlighter(int maxLength) {
+    this(maxLength, BreakIterator.getSentenceInstance(Locale.ROOT), new PassageScorer(), new PassageFormatter());
+  }
+  
+  /**
+   * Creates a new highlighter with custom parameters.
+   * @param maxLength maximum content size to process.
+   * @param breakIterator used for finding passage boundaries.
+   * @param scorer used for ranking passages.
+   * @param formatter used for formatting passages into highlighted snippets.
+   * @throws IllegalArgumentException if <code>maxLength</code> is negative or <code>Integer.MAX_VALUE</code>
+   */
+  public PostingsHighlighter(int maxLength, BreakIterator breakIterator, PassageScorer scorer, PassageFormatter formatter) {
+    if (maxLength < 0 || maxLength == Integer.MAX_VALUE) {
+      // two reasons: no overflow problems in BreakIterator.preceding(offset+1),
+      // our sentinel in the offsets queue uses this value to terminate.
+      throw new IllegalArgumentException("maxLength must be < Integer.MAX_VALUE");
+    }
+    if (breakIterator == null || scorer == null || formatter == null) {
+      throw new NullPointerException();
+    }
+    this.maxLength = maxLength;
+    this.breakIterator = breakIterator;
+    this.scorer = scorer;
+    this.formatter = formatter;
+  }
+  
+  /**
+   * Highlights the top passages from a single field.
+   * 
+   * @param field field name to highlight. 
+   *        Must have a stored string value and also be indexed with offsets.
+   * @param query query to highlight.
+   * @param searcher searcher that was previously used to execute the query.
+   * @param topDocs TopDocs containing the summary result documents to highlight.
+   * @return Array of formatted snippets corresponding to the documents in <code>topDocs</code>. 
+   *         If no highlights were found for a document, its value is <code>null</code>.
+   * @throws IOException if an I/O error occurred during processing
+   * @throws IllegalArgumentException if <code>field</code> was indexed without 
+   *         {@link IndexOptions#DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS}
+   */
+  public String[] highlight(String field, Query query, IndexSearcher searcher, TopDocs topDocs) throws IOException {
+    return highlight(field, query, searcher, topDocs, 1);
+  }
+  
+  /**
+   * Highlights the top-N passages from a single field.
+   * 
+   * @param field field name to highlight. 
+   *        Must have a stored string value and also be indexed with offsets.
+   * @param query query to highlight.
+   * @param searcher searcher that was previously used to execute the query.
+   * @param topDocs TopDocs containing the summary result documents to highlight.
+   * @param maxPassages The maximum number of top-N ranked passages used to 
+   *        form the highlighted snippets.
+   * @return Array of formatted snippets corresponding to the documents in <code>topDocs</code>. 
+   *         If no highlights were found for a document, its value is <code>null</code>.
+   * @throws IOException if an I/O error occurred during processing
+   * @throws IllegalArgumentException if <code>field</code> was indexed without 
+   *         {@link IndexOptions#DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS}
+   */
+  public String[] highlight(String field, Query query, IndexSearcher searcher, TopDocs topDocs, int maxPassages) throws IOException {
+    Map<String,String[]> res = highlightFields(new String[] { field }, query, searcher, topDocs, maxPassages);
+    return res.get(field);
+  }
+  
+  /**
+   * Highlights the top passages from multiple fields.
+   * <p>
+   * Conceptually, this behaves as a more efficient form of:
+   * <pre class="prettyprint">
+   * Map m = new HashMap();
+   * for (String field : fields) {
+   *   m.put(field, highlight(field, query, searcher, topDocs));
+   * }
+   * return m;
+   * </pre>
+   * 
+   * @param fields field names to highlight. 
+   *        Must have a stored string value and also be indexed with offsets.
+   * @param query query to highlight.
+   * @param searcher searcher that was previously used to execute the query.
+   * @param topDocs TopDocs containing the summary result documents to highlight.
+   * @return Map keyed on field name, containing the array of formatted snippets 
+   *         corresponding to the documents in <code>topDocs</code>. 
+   *         If no highlights were found for a document, its value is <code>null</code>.
+   * @throws IOException if an I/O error occurred during processing
+   * @throws IllegalArgumentException if <code>field</code> was indexed without 
+   *         {@link IndexOptions#DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS}
+   */
+  public Map<String,String[]> highlightFields(String fields[], Query query, IndexSearcher searcher, TopDocs topDocs) throws IOException {
+    return highlightFields(fields, query, searcher, topDocs, 1);
+  }
+  
+  /**
+   * Highlights the top-N passages from multiple fields.
+   * <p>
+   * Conceptually, this behaves as a more efficient form of:
+   * <pre class="prettyprint">
+   * Map m = new HashMap();
+   * for (String field : fields) {
+   *   m.put(field, highlight(field, query, searcher, topDocs, maxPassages));
+   * }
+   * return m;
+   * </pre>
+   * 
+   * @param fields field names to highlight. 
+   *        Must have a stored string value and also be indexed with offsets.
+   * @param query query to highlight.
+   * @param searcher searcher that was previously used to execute the query.
+   * @param topDocs TopDocs containing the summary result documents to highlight.
+   * @param maxPassages The maximum number of top-N ranked passages per-field used to 
+   *        form the highlighted snippets.
+   * @return Map keyed on field name, containing the array of formatted snippets 
+   *         corresponding to the documents in <code>topDocs</code>. 
+   *         If no highlights were found for a document, its value is <code>null</code>.
+   * @throws IOException if an I/O error occurred during processing
+   * @throws IllegalArgumentException if <code>field</code> was indexed without 
+   *         {@link IndexOptions#DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS}
+   */
+  public Map<String,String[]> highlightFields(String fields[], Query query, IndexSearcher searcher, TopDocs topDocs, int maxPassages) throws IOException {
+    final IndexReader reader = searcher.getIndexReader();
+    final ScoreDoc scoreDocs[] = topDocs.scoreDocs;
+    query = rewrite(query);
+    SortedSet<Term> queryTerms = new TreeSet<Term>();
+    query.extractTerms(queryTerms);
+
+    int docids[] = new int[scoreDocs.length];
+    for (int i = 0; i < docids.length; i++) {
+      docids[i] = scoreDocs[i].doc;
+    }
+    IndexReaderContext readerContext = reader.getContext();
+    List<AtomicReaderContext> leaves = readerContext.leaves();
+
+    BreakIterator bi = (BreakIterator)breakIterator.clone();
+
+    // sort for sequential io
+    Arrays.sort(docids);
+    Arrays.sort(fields);
+    
+    // pull stored data:
+    LimitedStoredFieldVisitor visitor = new LimitedStoredFieldVisitor(fields, maxLength);
+    String contents[][] = new String[fields.length][docids.length];
+    for (int i = 0; i < docids.length; i++) {
+      searcher.doc(docids[i], visitor);
+      for (int j = 0; j < fields.length; j++) {
+        contents[j][i] = visitor.getValue(j).toString();
+      }
+      visitor.reset();
+    }
+    
+    Map<String,String[]> highlights = new HashMap<String,String[]>();
+    for (int i = 0; i < fields.length; i++) {
+      String field = fields[i];
+      Term floor = new Term(field, "");
+      Term ceiling = new Term(field, UnicodeUtil.BIG_TERM);
+      SortedSet<Term> fieldTerms = queryTerms.subSet(floor, ceiling);
+      // TODO: should we have some reasonable defaults for term pruning? (e.g. stopwords)
+      Term terms[] = fieldTerms.toArray(new Term[fieldTerms.size()]);
+      Map<Integer,String> fieldHighlights = highlightField(field, contents[i], bi, terms, docids, leaves, maxPassages);
+        
+      String[] result = new String[scoreDocs.length];
+      for (int j = 0; j < scoreDocs.length; j++) {
+        result[j] = fieldHighlights.get(scoreDocs[j].doc);
+      }
+      highlights.put(field, result);
+    }
+    return highlights;
+  }
+    
+  private Map<Integer,String> highlightField(String field, String contents[], BreakIterator bi, Term terms[], int[] docids, List<AtomicReaderContext> leaves, int maxPassages) throws IOException {  
+    Map<Integer,String> highlights = new HashMap<Integer,String>();
+    
+    // reuse in the real sense... for docs in same segment we just advance our old enum
+    DocsAndPositionsEnum postings[] = null;
+    TermsEnum termsEnum = null;
+    int lastLeaf = -1;
+    
+    for (int i = 0; i < docids.length; i++) {
+      String content = contents[i];
+      if (content.length() == 0) {
+        continue; // nothing to do
+      }
+      bi.setText(content);
+      int doc = docids[i];
+      int leaf = ReaderUtil.subIndex(doc, leaves);
+      AtomicReaderContext subContext = leaves.get(leaf);
+      AtomicReader r = subContext.reader();
+      Terms t = r.terms(field);
+      if (t == null) {
+        continue; // nothing to do
+      }
+      if (leaf != lastLeaf) {
+        termsEnum = t.iterator(null);
+        postings = new DocsAndPositionsEnum[terms.length];
+      }
+      Passage passages[] = highlightDoc(field, terms, content.length(), bi, doc - subContext.docBase, termsEnum, postings, maxPassages);
+      if (passages.length > 0) {
+        // otherwise a null snippet
+        highlights.put(doc, formatter.format(passages, content));
+      }
+      lastLeaf = leaf;
+    }
+    
+    return highlights;
+  }
+  
+  // algorithm: treat sentence snippets as miniature documents
+  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s
+  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))
+  private Passage[] highlightDoc(String field, Term terms[], int contentLength, BreakIterator bi, int doc, 
+      TermsEnum termsEnum, DocsAndPositionsEnum[] postings, int n) throws IOException {
+    PriorityQueue<OffsetsEnum> pq = new PriorityQueue<OffsetsEnum>();
+    float weights[] = new float[terms.length];
+    // initialize postings
+    for (int i = 0; i < terms.length; i++) {
+      DocsAndPositionsEnum de = postings[i];
+      int pDoc;
+      if (de == EMPTY) {
+        continue;
+      } else if (de == null) {
+        postings[i] = EMPTY; // initially
+        if (!termsEnum.seekExact(terms[i].bytes(), true)) {
+          continue; // term not found
+        }
+        de = postings[i] = termsEnum.docsAndPositions(null, null, DocsAndPositionsEnum.FLAG_OFFSETS);
+        if (de == null) {
+          // no positions available
+          throw new IllegalArgumentException("field '" + field + "' was indexed without offsets, cannot highlight");
+        }
+        pDoc = de.advance(doc);
+      } else {
+        pDoc = de.docID();
+        if (pDoc < doc) {
+          pDoc = de.advance(doc);
+        }
+      }
+
+      if (doc == pDoc) {
+        weights[i] = scorer.weight(contentLength, de.freq());
+        de.nextPosition();
+        pq.add(new OffsetsEnum(de, i));
+      }
+    }
+    
+    pq.add(new OffsetsEnum(EMPTY, Integer.MAX_VALUE)); // a sentinel for termination
+    
+    PriorityQueue<Passage> passageQueue = new PriorityQueue<Passage>(n, new Comparator<Passage>() {
+      @Override
+      public int compare(Passage left, Passage right) {
+        if (right.score == left.score) {
+          return right.startOffset - left.endOffset;
+        } else {
+          return right.score > left.score ? 1 : -1;
+        }
+      }
+    });
+    Passage current = new Passage();
+    
+    OffsetsEnum off;
+    while ((off = pq.poll()) != null) {
+      final DocsAndPositionsEnum dp = off.dp;
+      int start = dp.startOffset();
+      if (start == -1) {
+        throw new IllegalArgumentException("field '" + field + "' was indexed without offsets, cannot highlight");
+      }
+      int end = dp.endOffset();
+      if (start > current.endOffset) {
+        if (current.startOffset >= 0) {
+          // finalize current
+          current.score *= scorer.norm(current.startOffset);
+          // new sentence: first add 'current' to queue 
+          if (passageQueue.size() == n && current.score < passageQueue.peek().score) {
+            current.reset(); // can't compete, just reset it
+          } else {
+            passageQueue.offer(current);
+            if (passageQueue.size() > n) {
+              current = passageQueue.poll();
+              current.reset();
+            } else {
+              current = new Passage();
+            }
+          }
+        }
+        // if we exceed limit, we are done
+        if (start >= contentLength) {
+          Passage passages[] = new Passage[passageQueue.size()];
+          passageQueue.toArray(passages);
+          // sort in ascending order
+          Arrays.sort(passages, new Comparator<Passage>() {
+            @Override
+            public int compare(Passage left, Passage right) {
+              return left.startOffset - right.startOffset;
+            }
+          });
+          return passages;
+        }
+        // advance breakiterator
+        assert BreakIterator.DONE < 0;
+        current.startOffset = Math.max(bi.preceding(start+1), 0);
+        current.endOffset = Math.min(bi.next(), contentLength);
+      }
+      int tf = 0;
+      while (true) {
+        tf++;
+        current.addMatch(start, end, terms[off.id]);
+        if (off.pos == dp.freq()) {
+          break; // removed from pq
+        } else {
+          off.pos++;
+          dp.nextPosition();
+          start = dp.startOffset();
+          end = dp.endOffset();
+        }
+        if (start >= current.endOffset) {
+          pq.offer(off);
+          break;
+        }
+      }
+      current.score += weights[off.id] * scorer.tf(tf, current.endOffset - current.startOffset);
+    }
+    return new Passage[0];
+  }
+  
+  private static class OffsetsEnum implements Comparable<OffsetsEnum> {
+    DocsAndPositionsEnum dp;
+    int pos;
+    int id;
+    
+    OffsetsEnum(DocsAndPositionsEnum dp, int id) throws IOException {
+      this.dp = dp;
+      this.id = id;
+      this.pos = 1;
+    }
+
+    @Override
+    public int compareTo(OffsetsEnum other) {
+      try {
+        int off = dp.startOffset();
+        int otherOff = other.dp.startOffset();
+        if (off == otherOff) {
+          return id - other.id;
+        } else {
+          return Long.signum(((long)off) - otherOff);
+        }
+      } catch (IOException e) {
+        throw new RuntimeException(e);
+      }
+    }
+  }
+  
+  private static final DocsAndPositionsEnum EMPTY = new DocsAndPositionsEnum() {
+
+    @Override
+    public int nextPosition() throws IOException { return 0; }
+
+    @Override
+    public int startOffset() throws IOException { return Integer.MAX_VALUE; }
+
+    @Override
+    public int endOffset() throws IOException { return Integer.MAX_VALUE; }
+
+    @Override
+    public BytesRef getPayload() throws IOException { return null; }
+
+    @Override
+    public int freq() throws IOException { return 0; }
+
+    @Override
+    public int docID() { return NO_MORE_DOCS; }
+
+    @Override
+    public int nextDoc() throws IOException { return NO_MORE_DOCS; }
+
+    @Override
+    public int advance(int target) throws IOException { return NO_MORE_DOCS; }
+  };
+  
+  /** 
+   * we rewrite against an empty indexreader: as we don't want things like
+   * rangeQueries that don't summarize the document
+   */
+  private static Query rewrite(Query original) throws IOException {
+    Query query = original;
+    for (Query rewrittenQuery = query.rewrite(EMPTY_INDEXREADER); rewrittenQuery != query;
+         rewrittenQuery = query.rewrite(EMPTY_INDEXREADER)) {
+      query = rewrittenQuery;
+    }
+    return query;
+  }
+  
+  private static class LimitedStoredFieldVisitor extends StoredFieldVisitor {
+    private final String fields[];
+    private final int maxLength;
+    private final StringBuilder builders[];
+    private int currentField = -1;
+    
+    public LimitedStoredFieldVisitor(String fields[], int maxLength) {
+      this.fields = fields;
+      this.maxLength = maxLength;
+      builders = new StringBuilder[fields.length];
+      for (int i = 0; i < builders.length; i++) {
+        builders[i] = new StringBuilder();
+      }
+    }
+    
+    @Override
+    public void stringField(FieldInfo fieldInfo, String value) throws IOException {
+      assert currentField >= 0;
+      StringBuilder builder = builders[currentField];
+      if (builder.length() > 0) {
+        builder.append(' '); // for the offset gap, TODO: make this configurable
+      }
+      if (builder.length() + value.length() > maxLength) {
+        builder.append(value, 0, maxLength - builder.length());
+      } else {
+        builder.append(value);
+      }
+    }
+
+    @Override
+    public Status needsField(FieldInfo fieldInfo) throws IOException {
+      currentField = Arrays.binarySearch(fields, fieldInfo.name);
+      if (currentField < 0) {
+        return Status.NO;
+      } else if (builders[currentField].length() > maxLength) {
+        return fields.length == 1 ? Status.STOP : Status.NO;
+      }
+      return Status.YES;
+    }
+    
+    String getValue(int i) {
+      return builders[i].toString();
+    }
+    
+    void reset() {
+      currentField = -1;
+      for (int i = 0; i < fields.length; i++) {
+        builders[i].setLength(0);
+      }
+    }
+  }
+}
diff --git a/lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/package.html b/lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/package.html
new file mode 100644
index 0000000..d6b4663
--- /dev/null
+++ b/lucene/highlighter/src/java/org/apache/lucene/search/postingshighlight/package.html
@@ -0,0 +1,22 @@
+<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+<html>
+<body>
+Highlighter implementation that uses offsets from postings lists.
+</body>
+</html>
\ No newline at end of file
diff --git a/lucene/highlighter/src/test/org/apache/lucene/search/postingshighlight/TestPostingsHighlighter.java b/lucene/highlighter/src/test/org/apache/lucene/search/postingshighlight/TestPostingsHighlighter.java
new file mode 100644
index 0000000..a5dda65
--- /dev/null
+++ b/lucene/highlighter/src/test/org/apache/lucene/search/postingshighlight/TestPostingsHighlighter.java
@@ -0,0 +1,275 @@
+package org.apache.lucene.search.postingshighlight;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.util.Map;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.analysis.MockTokenizer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.FieldType;
+import org.apache.lucene.document.StringField;
+import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.IndexWriterConfig;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.BooleanClause;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.Sort;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.TopDocs;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util.LuceneTestCase.SuppressCodecs;
+
+@SuppressCodecs({"MockFixedIntBlock", "MockVariableIntBlock", "MockSep", "MockRandom"})
+public class TestPostingsHighlighter extends LuceneTestCase {
+  
+  public void testBasics() throws Exception {
+    Directory dir = newDirectory();
+    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
+    iwc.setMergePolicy(newLogMergePolicy());
+    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);
+    
+    FieldType offsetsType = new FieldType(TextField.TYPE_STORED);
+    offsetsType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);
+    Field body = new Field("body", "", offsetsType);
+    Document doc = new Document();
+    doc.add(body);
+    
+    body.setStringValue("This is a test. Just a test highlighting from postings. Feel free to ignore.");
+    iw.addDocument(doc);
+    body.setStringValue("Highlighting the first term. Hope it works.");
+    iw.addDocument(doc);
+    
+    IndexReader ir = iw.getReader();
+    iw.close();
+    
+    IndexSearcher searcher = newSearcher(ir);
+    PostingsHighlighter highlighter = new PostingsHighlighter();
+    Query query = new TermQuery(new Term("body", "highlighting"));
+    TopDocs topDocs = searcher.search(query, null, 10, Sort.INDEXORDER);
+    assertEquals(2, topDocs.totalHits);
+    String snippets[] = highlighter.highlight("body", query, searcher, topDocs);
+    assertEquals(2, snippets.length);
+    assertEquals("Just a test <b>highlighting</b> from postings. ", snippets[0]);
+    assertEquals("<b>Highlighting</b> the first term. ", snippets[1]);
+    
+    ir.close();
+    dir.close();
+  }
+  
+  // simple test with one sentence documents.
+  public void testOneSentence() throws Exception {
+    Directory dir = newDirectory();
+    // use simpleanalyzer for more natural tokenization (else "test." is a token)
+    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random(), MockTokenizer.SIMPLE, true));
+    iwc.setMergePolicy(newLogMergePolicy());
+    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);
+    
+    FieldType offsetsType = new FieldType(TextField.TYPE_STORED);
+    offsetsType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);
+    Field body = new Field("body", "", offsetsType);
+    Document doc = new Document();
+    doc.add(body);
+    
+    body.setStringValue("This is a test.");
+    iw.addDocument(doc);
+    body.setStringValue("Test a one sentence document.");
+    iw.addDocument(doc);
+    
+    IndexReader ir = iw.getReader();
+    iw.close();
+    
+    IndexSearcher searcher = newSearcher(ir);
+    PostingsHighlighter highlighter = new PostingsHighlighter();
+    Query query = new TermQuery(new Term("body", "test"));
+    TopDocs topDocs = searcher.search(query, null, 10, Sort.INDEXORDER);
+    assertEquals(2, topDocs.totalHits);
+    String snippets[] = highlighter.highlight("body", query, searcher, topDocs);
+    assertEquals(2, snippets.length);
+    assertEquals("This is a <b>test</b>.", snippets[0]);
+    assertEquals("<b>Test</b> a one sentence document.", snippets[1]);
+    
+    ir.close();
+    dir.close();
+  }
+  
+  public void testMultipleFields() throws Exception {
+    Directory dir = newDirectory();
+    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random(), MockTokenizer.SIMPLE, true));
+    iwc.setMergePolicy(newLogMergePolicy());
+    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);
+    
+    FieldType offsetsType = new FieldType(TextField.TYPE_STORED);
+    offsetsType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);
+    Field body = new Field("body", "", offsetsType);
+    Field title = new Field("title", "", offsetsType);
+    Document doc = new Document();
+    doc.add(body);
+    doc.add(title);
+    
+    body.setStringValue("This is a test. Just a test highlighting from postings. Feel free to ignore.");
+    title.setStringValue("I am hoping for the best.");
+    iw.addDocument(doc);
+    body.setStringValue("Highlighting the first term. Hope it works.");
+    title.setStringValue("But best may not be good enough.");
+    iw.addDocument(doc);
+    
+    IndexReader ir = iw.getReader();
+    iw.close();
+    
+    IndexSearcher searcher = newSearcher(ir);
+    PostingsHighlighter highlighter = new PostingsHighlighter();
+    BooleanQuery query = new BooleanQuery();
+    query.add(new TermQuery(new Term("body", "highlighting")), BooleanClause.Occur.SHOULD);
+    query.add(new TermQuery(new Term("title", "best")), BooleanClause.Occur.SHOULD);
+    TopDocs topDocs = searcher.search(query, null, 10, Sort.INDEXORDER);
+    assertEquals(2, topDocs.totalHits);
+    Map<String,String[]> snippets = highlighter.highlightFields(new String [] { "body", "title" }, query, searcher, topDocs);
+    assertEquals(2, snippets.size());
+    assertEquals("Just a test <b>highlighting</b> from postings. ", snippets.get("body")[0]);
+    assertEquals("<b>Highlighting</b> the first term. ", snippets.get("body")[1]);
+    assertEquals("I am hoping for the <b>best</b>.", snippets.get("title")[0]);
+    assertEquals("But <b>best</b> may not be good enough.", snippets.get("title")[1]);
+    ir.close();
+    dir.close();
+  }
+  
+  public void testMultipleTerms() throws Exception {
+    Directory dir = newDirectory();
+    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
+    iwc.setMergePolicy(newLogMergePolicy());
+    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);
+    
+    FieldType offsetsType = new FieldType(TextField.TYPE_STORED);
+    offsetsType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);
+    Field body = new Field("body", "", offsetsType);
+    Document doc = new Document();
+    doc.add(body);
+    
+    body.setStringValue("This is a test. Just a test highlighting from postings. Feel free to ignore.");
+    iw.addDocument(doc);
+    body.setStringValue("Highlighting the first term. Hope it works.");
+    iw.addDocument(doc);
+    
+    IndexReader ir = iw.getReader();
+    iw.close();
+    
+    IndexSearcher searcher = newSearcher(ir);
+    PostingsHighlighter highlighter = new PostingsHighlighter();
+    BooleanQuery query = new BooleanQuery();
+    query.add(new TermQuery(new Term("body", "highlighting")), BooleanClause.Occur.SHOULD);
+    query.add(new TermQuery(new Term("body", "just")), BooleanClause.Occur.SHOULD);
+    query.add(new TermQuery(new Term("body", "first")), BooleanClause.Occur.SHOULD);
+    TopDocs topDocs = searcher.search(query, null, 10, Sort.INDEXORDER);
+    assertEquals(2, topDocs.totalHits);
+    String snippets[] = highlighter.highlight("body", query, searcher, topDocs);
+    assertEquals(2, snippets.length);
+    assertEquals("<b>Just</b> a test <b>highlighting</b> from postings. ", snippets[0]);
+    assertEquals("<b>Highlighting</b> the <b>first</b> term. ", snippets[1]);
+    
+    ir.close();
+    dir.close();
+  }
+  
+  public void testMultiplePassages() throws Exception {
+    Directory dir = newDirectory();
+    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random(), MockTokenizer.SIMPLE, true));
+    iwc.setMergePolicy(newLogMergePolicy());
+    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);
+    
+    FieldType offsetsType = new FieldType(TextField.TYPE_STORED);
+    offsetsType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);
+    Field body = new Field("body", "", offsetsType);
+    Document doc = new Document();
+    doc.add(body);
+    
+    body.setStringValue("This is a test. Just a test highlighting from postings. Feel free to ignore.");
+    iw.addDocument(doc);
+    body.setStringValue("This test is another test. Not a good sentence. Test test test test.");
+    iw.addDocument(doc);
+    
+    IndexReader ir = iw.getReader();
+    iw.close();
+    
+    IndexSearcher searcher = newSearcher(ir);
+    PostingsHighlighter highlighter = new PostingsHighlighter();
+    Query query = new TermQuery(new Term("body", "test"));
+    TopDocs topDocs = searcher.search(query, null, 10, Sort.INDEXORDER);
+    assertEquals(2, topDocs.totalHits);
+    String snippets[] = highlighter.highlight("body", query, searcher, topDocs, 2);
+    assertEquals(2, snippets.length);
+    assertEquals("This is a <b>test</b>. Just a <b>test</b> highlighting from postings. ", snippets[0]);
+    assertEquals("This <b>test</b> is another <b>test</b>. ... <b>Test</b> <b>test</b> <b>test</b> <b>test</b>.", snippets[1]);
+    
+    ir.close();
+    dir.close();
+  }
+
+  public void testUserFailedToIndexOffsets() throws Exception {
+    Directory dir = newDirectory();
+    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random(), MockTokenizer.SIMPLE, true));
+    iwc.setMergePolicy(newLogMergePolicy());
+    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);
+    
+    FieldType positionsType = new FieldType(TextField.TYPE_STORED);
+    positionsType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);
+    Field body = new Field("body", "", positionsType);
+    Field title = new StringField("title", "", Field.Store.YES);
+    Document doc = new Document();
+    doc.add(body);
+    doc.add(title);
+    
+    body.setStringValue("This is a test. Just a test highlighting from postings. Feel free to ignore.");
+    title.setStringValue("test");
+    iw.addDocument(doc);
+    body.setStringValue("This test is another test. Not a good sentence. Test test test test.");
+    title.setStringValue("test");
+    iw.addDocument(doc);
+    
+    IndexReader ir = iw.getReader();
+    iw.close();
+    
+    IndexSearcher searcher = newSearcher(ir);
+    PostingsHighlighter highlighter = new PostingsHighlighter();
+    Query query = new TermQuery(new Term("body", "test"));
+    TopDocs topDocs = searcher.search(query, null, 10, Sort.INDEXORDER);
+    assertEquals(2, topDocs.totalHits);
+    try {
+      highlighter.highlight("body", query, searcher, topDocs, 2);
+      fail("did not hit expected exception");
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+    
+    try {
+      highlighter.highlight("title", new TermQuery(new Term("title", "test")), searcher, topDocs, 2);
+      fail("did not hit expected exception");
+    } catch (IllegalArgumentException iae) {
+      // expected
+    }
+    ir.close();
+    dir.close();
+  }
+}
diff --git a/lucene/highlighter/src/test/org/apache/lucene/search/postingshighlight/TestPostingsHighlighterRanking.java b/lucene/highlighter/src/test/org/apache/lucene/search/postingshighlight/TestPostingsHighlighterRanking.java
new file mode 100644
index 0000000..9382ecd
--- /dev/null
+++ b/lucene/highlighter/src/test/org/apache/lucene/search/postingshighlight/TestPostingsHighlighterRanking.java
@@ -0,0 +1,234 @@
+package org.apache.lucene.search.postingshighlight;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.text.BreakIterator;
+import java.util.HashSet;
+import java.util.Locale;
+import java.util.Random;
+
+import org.apache.lucene.analysis.MockAnalyzer;
+import org.apache.lucene.analysis.MockTokenizer;
+import org.apache.lucene.document.Document;
+import org.apache.lucene.document.Field;
+import org.apache.lucene.document.FieldType;
+import org.apache.lucene.document.StringField;
+import org.apache.lucene.document.TextField;
+import org.apache.lucene.index.IndexReader;
+import org.apache.lucene.index.RandomIndexWriter;
+import org.apache.lucene.index.FieldInfo.IndexOptions;
+import org.apache.lucene.index.Term;
+import org.apache.lucene.search.BooleanClause;
+import org.apache.lucene.search.BooleanQuery;
+import org.apache.lucene.search.IndexSearcher;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.TermQuery;
+import org.apache.lucene.search.TopDocs;
+import org.apache.lucene.store.Directory;
+import org.apache.lucene.util.BytesRef;
+import org.apache.lucene.util.LuceneTestCase;
+import org.apache.lucene.util._TestUtil;
+import org.apache.lucene.util.LuceneTestCase.SuppressCodecs;
+
+@SuppressCodecs({"MockFixedIntBlock", "MockVariableIntBlock", "MockSep", "MockRandom"})
+public class TestPostingsHighlighterRanking extends LuceneTestCase {
+  /** 
+   * indexes a bunch of gibberish, and then highlights top(n).
+   * asserts that top(n) highlights is a subset of top(n+1) up to some max N
+   */
+  // TODO: this only tests single-valued fields. we should also index multiple values per field!
+  public void testRanking() throws Exception {
+    // number of documents: we will check each one
+    final int numDocs = atLeast(100);
+    // number of top-N snippets, we will check 1 .. N
+    final int maxTopN = 5;
+    // maximum number of elements to put in a sentence.
+    final int maxSentenceLength = 10;
+    // maximum number of sentences in a document
+    final int maxNumSentences = 20;
+    
+    Directory dir = newDirectory();
+    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, new MockAnalyzer(random(), MockTokenizer.SIMPLE, true));
+    Document document = new Document();
+    Field id = new StringField("id", "", Field.Store.NO);
+    FieldType offsetsType = new FieldType(TextField.TYPE_STORED);
+    offsetsType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);
+    Field body = new Field("body", "", offsetsType);
+    document.add(id);
+    document.add(body);
+    
+    for (int i = 0; i < numDocs; i++) {;
+      StringBuilder bodyText = new StringBuilder();
+      int numSentences = _TestUtil.nextInt(random(), 1, maxNumSentences);
+      for (int j = 0; j < numSentences; j++) {
+        bodyText.append(newSentence(random(), maxSentenceLength));
+      }
+      body.setStringValue(bodyText.toString());
+      id.setStringValue(Integer.toString(i));
+      iw.addDocument(document);
+    }
+    
+    IndexReader ir = iw.getReader();
+    IndexSearcher searcher = newSearcher(ir);
+    for (int i = 0; i < numDocs; i++) {
+      checkDocument(searcher, i, maxTopN);
+    }
+    iw.close();
+    ir.close();
+    dir.close();
+  }
+  
+  private void checkDocument(IndexSearcher is, int doc, int maxTopN) throws IOException {
+    for (int ch = 'a'; ch <= 'z'; ch++) {
+      Term term = new Term("body", "" + (char)ch);
+      // check a simple term query
+      checkQuery(is, new TermQuery(term), doc, maxTopN);
+      // check a boolean query
+      BooleanQuery bq = new BooleanQuery();
+      bq.add(new TermQuery(term), BooleanClause.Occur.SHOULD);
+      Term nextTerm = new Term("body", "" + (char)(ch+1));
+      bq.add(new TermQuery(nextTerm), BooleanClause.Occur.SHOULD);
+      checkQuery(is, bq, doc, maxTopN);
+    }
+  }
+  
+  private void checkQuery(IndexSearcher is, Query query, int doc, int maxTopN) throws IOException {
+    for (int n = 1; n < maxTopN; n++) {
+      FakePassageFormatter f1 = new FakePassageFormatter();
+      PostingsHighlighter p1 = new PostingsHighlighter(Integer.MAX_VALUE-1, 
+                                                       BreakIterator.getSentenceInstance(Locale.ROOT), 
+                                                       new PassageScorer(),
+                                                       f1);
+      FakePassageFormatter f2 = new FakePassageFormatter();
+      PostingsHighlighter p2 = new PostingsHighlighter(Integer.MAX_VALUE-1, 
+                                                       BreakIterator.getSentenceInstance(Locale.ROOT), 
+                                                       new PassageScorer(),
+                                                       f2);
+      BooleanQuery bq = new BooleanQuery(false);
+      bq.add(query, BooleanClause.Occur.MUST);
+      bq.add(new TermQuery(new Term("id", Integer.toString(doc))), BooleanClause.Occur.MUST);
+      TopDocs td = is.search(bq, 1);
+      p1.highlight("body", bq, is, td, n);
+      p2.highlight("body", bq, is, td, n+1);
+      assertTrue(f2.seen.containsAll(f1.seen));
+    }
+  }
+  
+  /** 
+   * returns a new random sentence, up to maxSentenceLength "words" in length.
+   * each word is a single character (a-z). The first one is capitalized.
+   */
+  private String newSentence(Random r, int maxSentenceLength) {
+    StringBuilder sb = new StringBuilder();
+    int numElements = _TestUtil.nextInt(r, 1, maxSentenceLength);
+    for (int i = 0; i < numElements; i++) {
+      if (sb.length() > 0) {
+        sb.append(' ');
+        sb.append((char)_TestUtil.nextInt(r, 'a', 'z'));
+      } else {
+        // capitalize the first word to help breakiterator
+        sb.append((char)_TestUtil.nextInt(r, 'A', 'Z'));
+      }
+    }
+    sb.append(". "); // finalize sentence
+    return sb.toString();
+  }
+  
+  /** 
+   * a fake formatter that doesn't actually format passages.
+   * instead it just collects them for asserts!
+   */
+  static class FakePassageFormatter extends PassageFormatter {
+    HashSet<Pair> seen = new HashSet<Pair>();
+    
+    @Override
+    public String format(Passage passages[], String content) {
+      for (Passage p : passages) {
+        // verify some basics about the passage
+        assertTrue(p.getScore() >= 0);
+        assertTrue(p.getNumMatches() > 0);
+        assertTrue(p.getStartOffset() >= 0);
+        assertTrue(p.getStartOffset() <= content.length());
+        // we use a very simple analyzer. so we can assert the matches are correct
+        for (int i = 0; i < p.getNumMatches(); i++) {
+          Term term = p.getMatchTerms()[i];
+          assertEquals("body", term.field());
+          int matchStart = p.getMatchStarts()[i];
+          assertTrue(matchStart >= 0);
+          int matchEnd = p.getMatchEnds()[i];
+          assertTrue(matchEnd >= 0);
+          // single character terms
+          assertEquals(matchStart+1, matchEnd);
+          // and the offsets must be correct...
+          BytesRef bytes = term.bytes();
+          assertEquals(1, bytes.length);
+          assertEquals((char)bytes.bytes[bytes.offset], Character.toLowerCase(content.charAt(matchStart)));
+        }
+        // record just the start/end offset for simplicity
+        seen.add(new Pair(p.getStartOffset(), p.getEndOffset()));
+      }
+      return "bogus!!!!!!";
+    }
+  }
+  
+  static class Pair {
+    final int start;
+    final int end;
+    
+    Pair(int start, int end) {
+      this.start = start;
+      this.end = end;
+    }
+
+    @Override
+    public int hashCode() {
+      final int prime = 31;
+      int result = 1;
+      result = prime * result + end;
+      result = prime * result + start;
+      return result;
+    }
+
+    @Override
+    public boolean equals(Object obj) {
+      if (this == obj) {
+        return true;
+      }
+      if (obj == null) {
+        return false;
+      }
+      if (getClass() != obj.getClass()) {
+        return false;
+      }
+      Pair other = (Pair) obj;
+      if (end != other.end) {
+        return false;
+      }
+      if (start != other.start) {
+        return false;
+      }
+      return true;
+    }
+
+    @Override
+    public String toString() {
+      return "Pair [start=" + start + ", end=" + end + "]";
+    }
+  }
+}
diff --git a/lucene/sandbox/src/java/org/apache/lucene/sandbox/postingshighlight/Passage.java b/lucene/sandbox/src/java/org/apache/lucene/sandbox/postingshighlight/Passage.java
deleted file mode 100644
index b0145b1..0000000
--- a/lucene/sandbox/src/java/org/apache/lucene/sandbox/postingshighlight/Passage.java
+++ /dev/null
@@ -1,125 +0,0 @@
-package org.apache.lucene.sandbox.postingshighlight;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import org.apache.lucene.index.Term;
-import org.apache.lucene.util.ArrayUtil;
-import org.apache.lucene.util.RamUsageEstimator;
-
-/**
- * Represents a passage (typically a sentence of the document). 
- * <p>
- * A passage contains {@link #getNumMatches} highlights from the query,
- * and the offsets and query terms that correspond with each match.
- * @lucene.experimental
- */
-public final class Passage {
-  int startOffset = -1;
-  int endOffset = -1;
-  float score = 0.0f;
-
-  int matchStarts[] = new int[8];
-  int matchEnds[] = new int[8];
-  Term matchTerms[] = new Term[8];
-  int numMatches = 0;
-  
-  void addMatch(int startOffset, int endOffset, Term term) {
-    assert startOffset >= this.startOffset && startOffset <= this.endOffset;
-    if (numMatches == matchStarts.length) {
-      matchStarts = ArrayUtil.grow(matchStarts, numMatches+1);
-      matchEnds = ArrayUtil.grow(matchEnds, numMatches+1);
-      Term newMatchTerms[] = new Term[ArrayUtil.oversize(numMatches+1, RamUsageEstimator.NUM_BYTES_OBJECT_REF)];
-      System.arraycopy(matchTerms, 0, newMatchTerms, 0, numMatches);
-      matchTerms = newMatchTerms;
-    }
-    matchStarts[numMatches] = startOffset;
-    matchEnds[numMatches] = endOffset;
-    matchTerms[numMatches] = term;
-    numMatches++;
-  }
-  
-  void reset() {
-    startOffset = endOffset = -1;
-    score = 0.0f;
-    numMatches = 0;
-  }
-
-  /**
-   * Start offset of this passage.
-   * @return start index (inclusive) of the passage in the 
-   *         original content: always &gt;= 0.
-   */
-  public int getStartOffset() {
-    return startOffset;
-  }
-
-  /**
-   * End offset of this passage.
-   * @return end index (exclusive) of the passage in the 
-   *         original content: always &gt;= {@link #getStartOffset()}
-   */
-  public int getEndOffset() {
-    return endOffset;
-  }
-
-  /**
-   * Passage's score.
-   */
-  public float getScore() {
-    return score;
-  }
-  
-  /**
-   * Number of term matches available in 
-   * {@link #getMatchStarts}, {@link #getMatchEnds}, 
-   * {@link #getMatchTerms}
-   */
-  public int getNumMatches() {
-    return numMatches;
-  }
-
-  /**
-   * Start offsets of the term matches, in increasing order.
-   * <p>
-   * Only {@link #getNumMatches} are valid. Note that these
-   * offsets are absolute (not relative to {@link #getStartOffset()}).
-   */
-  public int[] getMatchStarts() {
-    return matchStarts;
-  }
-
-  /**
-   * End offsets of the term matches, corresponding with {@link #getMatchStarts}. 
-   * <p>
-   * Only {@link #getNumMatches} are valid. Note that its possible that an end offset 
-   * could exceed beyond the bounds of the passage ({@link #getEndOffset()}), if the 
-   * Analyzer produced a term which spans a passage boundary.
-   */
-  public int[] getMatchEnds() {
-    return matchEnds;
-  }
-
-  /**
-   * Term of the matches, corresponding with {@link #getMatchStarts()}.
-   * <p>
-   * Only {@link #getNumMatches()} are valid.
-   */
-  public Term[] getMatchTerms() {
-    return matchTerms;
-  }
-}
diff --git a/lucene/sandbox/src/java/org/apache/lucene/sandbox/postingshighlight/PassageFormatter.java b/lucene/sandbox/src/java/org/apache/lucene/sandbox/postingshighlight/PassageFormatter.java
deleted file mode 100644
index e9b8089..0000000
--- a/lucene/sandbox/src/java/org/apache/lucene/sandbox/postingshighlight/PassageFormatter.java
+++ /dev/null
@@ -1,66 +0,0 @@
-package org.apache.lucene.sandbox.postingshighlight;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Creates a formatted snippet from the top passages.
- * <p>
- * The default implementation marks the query terms as bold, and places
- * ellipses between unconnected passages.
- * @lucene.experimental
- */
-public class PassageFormatter {
-  /**
-   * Formats the top <code>passages</code> from <code>content</code>
-   * into a human-readable text snippet.
-   * 
-   * @param passages top-N passages for the field. Note these are sorted in
-   *        the order that they appear in the document for convenience.
-   * @param content content for the field.
-   * @return formatted highlight
-   */
-  public String format(Passage passages[], String content) {
-    StringBuilder sb = new StringBuilder();
-    int pos = 0;
-    for (Passage passage : passages) {
-      // don't add ellipsis if its the first one, or if its connected.
-      if (passage.startOffset > pos && pos > 0) {
-        sb.append("... ");
-      }
-      pos = passage.startOffset;
-      for (int i = 0; i < passage.numMatches; i++) {
-        int start = passage.matchStarts[i];
-        int end = passage.matchEnds[i];
-        // its possible to have overlapping terms
-        if (start > pos) {
-          sb.append(content.substring(pos, start));
-        }
-        if (end > pos) {
-          sb.append("<b>");
-          sb.append(content.substring(Math.max(pos, start), end));
-          sb.append("</b>");
-          pos = end;
-        }
-      }
-      // its possible a "term" from the analyzer could span a sentence boundary.
-      sb.append(content.substring(pos, Math.max(pos, passage.endOffset)));
-      pos = passage.endOffset;
-    }
-    return sb.toString();
-  }
-}
diff --git a/lucene/sandbox/src/java/org/apache/lucene/sandbox/postingshighlight/PassageScorer.java b/lucene/sandbox/src/java/org/apache/lucene/sandbox/postingshighlight/PassageScorer.java
deleted file mode 100644
index 8609648..0000000
--- a/lucene/sandbox/src/java/org/apache/lucene/sandbox/postingshighlight/PassageScorer.java
+++ /dev/null
@@ -1,83 +0,0 @@
-package org.apache.lucene.sandbox.postingshighlight;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/** 
- * Ranks passages found by {@link PostingsHighlighter}.
- * <p>
- * Each passage is scored as a miniature document within the document.
- * The final score is computed as {@link #norm} * &sum; ({@link #weight} * {@link #tf}).
- * The default implementation is {@link #norm} * BM25.
- * @lucene.experimental
- */
-public class PassageScorer {
-  
-  // TODO: this formula is completely made up. It might not provide relevant snippets!
-  
-  /** BM25 k1 parameter, controls term frequency normalization */
-  public static final float k1 = 1.2f;
-  /** BM25 b parameter, controls length normalization. */
-  public static final float b = 0.75f;
-  
-  /**
-   * A pivot used for length normalization.
-   * The default value is the typical average English sentence length.
-   */
-  public static final float pivot = 87f;
-    
-  /**
-   * Computes term importance, given its in-document statistics.
-   * 
-   * @param contentLength length of document in characters
-   * @param totalTermFreq number of time term occurs in document
-   * @return term importance
-   */
-  public float weight(int contentLength, int totalTermFreq) {
-    // approximate #docs from content length
-    float numDocs = 1 + contentLength / pivot;
-    // numDocs not numDocs - docFreq (ala DFR), since we approximate numDocs
-    return (k1 + 1) * (float) Math.log(1 + (numDocs + 0.5D)/(totalTermFreq + 0.5D));
-  }
-
-  /**
-   * Computes term weight, given the frequency within the passage
-   * and the passage's length.
-   * 
-   * @param freq number of occurrences of within this passage
-   * @param passageLen length of the passage in characters.
-   * @return term weight
-   */
-  public float tf(int freq, int passageLen) {
-    float norm = k1 * ((1 - b) + b * (passageLen / pivot));
-    return freq / (freq + norm);
-  }
-    
-  /**
-   * Normalize a passage according to its position in the document.
-   * <p>
-   * Typically passages towards the beginning of the document are 
-   * more useful for summarizing the contents.
-   * <p>
-   * The default implementation is <code>1 + 1/log(pivot + passageStart)</code>
-   * @param passageStart start offset of the passage
-   * @return a boost value multiplied into the passage's core.
-   */
-  public float norm(int passageStart) {
-    return 1 + 1/(float)Math.log(pivot + passageStart);
-  }
-}
diff --git a/lucene/sandbox/src/java/org/apache/lucene/sandbox/postingshighlight/PostingsHighlighter.java b/lucene/sandbox/src/java/org/apache/lucene/sandbox/postingshighlight/PostingsHighlighter.java
deleted file mode 100644
index 7d46137..0000000
--- a/lucene/sandbox/src/java/org/apache/lucene/sandbox/postingshighlight/PostingsHighlighter.java
+++ /dev/null
@@ -1,557 +0,0 @@
-package org.apache.lucene.sandbox.postingshighlight;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.text.BreakIterator;
-import java.util.Arrays;
-import java.util.Comparator;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Locale;
-import java.util.Map;
-import java.util.PriorityQueue;
-import java.util.SortedSet;
-import java.util.TreeSet;
-
-import org.apache.lucene.index.AtomicReader;
-import org.apache.lucene.index.AtomicReaderContext;
-import org.apache.lucene.index.DocsAndPositionsEnum;
-import org.apache.lucene.index.FieldInfo;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexReaderContext;
-import org.apache.lucene.index.MultiReader;
-import org.apache.lucene.index.ReaderUtil;
-import org.apache.lucene.index.StoredFieldVisitor;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.index.Terms;
-import org.apache.lucene.index.TermsEnum;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.ScoreDoc;
-import org.apache.lucene.search.TopDocs;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.UnicodeUtil;
-
-/**
- * Simple highlighter that does not analyze fields nor use
- * term vectors. Instead it requires 
- * {@link IndexOptions#DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS}.
- * <p>
- * PostingsHighlighter treats the single original document as the whole corpus, and then scores individual
- * passages as if they were documents in this corpus. It uses a {@link BreakIterator} to find 
- * passages in the text; by default it breaks using {@link BreakIterator#getSentenceInstance(Locale) 
- * getSentenceInstance(Locale.ROOT)}. It then iterates in parallel (merge sorting by offset) through 
- * the positions of all terms from the query, coalescing those hits that occur in a single passage 
- * into a {@link Passage}, and then scores each Passage using a separate {@link PassageScorer}. 
- * Passages are finally formatted into highlighted snippets with a {@link PassageFormatter}.
- * <p>
- * <b>WARNING</b>: The code is very new and may still have some exciting bugs! This is why 
- * it's located under Lucene's sandbox module. 
- * <p>
- * Example usage:
- * <pre class="prettyprint">
- *   // configure field with offsets at index time
- *   FieldType offsetsType = new FieldType(TextField.TYPE_STORED);
- *   offsetsType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);
- *   Field body = new Field("body", "foobar", offsetsType);
- *
- *   // retrieve highlights at query time 
- *   PostingsHighlighter highlighter = new PostingsHighlighter();
- *   Query query = new TermQuery(new Term("body", "highlighting"));
- *   TopDocs topDocs = searcher.search(query, n);
- *   String highlights[] = highlighter.highlight("body", query, searcher, topDocs);
- * </pre>
- * <p>
- * This is thread-safe, and can be used across different readers.
- * @lucene.experimental
- */
-public final class PostingsHighlighter {
-  
-  // TODO: maybe allow re-analysis for tiny fields? currently we require offsets,
-  // but if the analyzer is really fast and the field is tiny, this might really be
-  // unnecessary.
-  
-  /** for rewriting: we don't want slow processing from MTQs */
-  private static final IndexReader EMPTY_INDEXREADER = new MultiReader();
-  
-  /** Default maximum content size to process. Typically snippets
-   *  closer to the beginning of the document better summarize its content */
-  public static final int DEFAULT_MAX_LENGTH = 10000;
-    
-  private final int maxLength;
-  private final BreakIterator breakIterator;
-  private final PassageScorer scorer;
-  private final PassageFormatter formatter;
-  
-  /**
-   * Creates a new highlighter with default parameters.
-   */
-  public PostingsHighlighter() {
-    this(DEFAULT_MAX_LENGTH);
-  }
-  
-  /**
-   * Creates a new highlighter, specifying maximum content length.
-   * @param maxLength maximum content size to process.
-   * @throws IllegalArgumentException if <code>maxLength</code> is negative or <code>Integer.MAX_VALUE</code>
-   */
-  public PostingsHighlighter(int maxLength) {
-    this(maxLength, BreakIterator.getSentenceInstance(Locale.ROOT), new PassageScorer(), new PassageFormatter());
-  }
-  
-  /**
-   * Creates a new highlighter with custom parameters.
-   * @param maxLength maximum content size to process.
-   * @param breakIterator used for finding passage boundaries.
-   * @param scorer used for ranking passages.
-   * @param formatter used for formatting passages into highlighted snippets.
-   * @throws IllegalArgumentException if <code>maxLength</code> is negative or <code>Integer.MAX_VALUE</code>
-   */
-  public PostingsHighlighter(int maxLength, BreakIterator breakIterator, PassageScorer scorer, PassageFormatter formatter) {
-    if (maxLength < 0 || maxLength == Integer.MAX_VALUE) {
-      // two reasons: no overflow problems in BreakIterator.preceding(offset+1),
-      // our sentinel in the offsets queue uses this value to terminate.
-      throw new IllegalArgumentException("maxLength must be < Integer.MAX_VALUE");
-    }
-    if (breakIterator == null || scorer == null || formatter == null) {
-      throw new NullPointerException();
-    }
-    this.maxLength = maxLength;
-    this.breakIterator = breakIterator;
-    this.scorer = scorer;
-    this.formatter = formatter;
-  }
-  
-  /**
-   * Highlights the top passages from a single field.
-   * 
-   * @param field field name to highlight. 
-   *        Must have a stored string value and also be indexed with offsets.
-   * @param query query to highlight.
-   * @param searcher searcher that was previously used to execute the query.
-   * @param topDocs TopDocs containing the summary result documents to highlight.
-   * @return Array of formatted snippets corresponding to the documents in <code>topDocs</code>. 
-   *         If no highlights were found for a document, its value is <code>null</code>.
-   * @throws IOException if an I/O error occurred during processing
-   * @throws IllegalArgumentException if <code>field</code> was indexed without 
-   *         {@link IndexOptions#DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS}
-   */
-  public String[] highlight(String field, Query query, IndexSearcher searcher, TopDocs topDocs) throws IOException {
-    return highlight(field, query, searcher, topDocs, 1);
-  }
-  
-  /**
-   * Highlights the top-N passages from a single field.
-   * 
-   * @param field field name to highlight. 
-   *        Must have a stored string value and also be indexed with offsets.
-   * @param query query to highlight.
-   * @param searcher searcher that was previously used to execute the query.
-   * @param topDocs TopDocs containing the summary result documents to highlight.
-   * @param maxPassages The maximum number of top-N ranked passages used to 
-   *        form the highlighted snippets.
-   * @return Array of formatted snippets corresponding to the documents in <code>topDocs</code>. 
-   *         If no highlights were found for a document, its value is <code>null</code>.
-   * @throws IOException if an I/O error occurred during processing
-   * @throws IllegalArgumentException if <code>field</code> was indexed without 
-   *         {@link IndexOptions#DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS}
-   */
-  public String[] highlight(String field, Query query, IndexSearcher searcher, TopDocs topDocs, int maxPassages) throws IOException {
-    Map<String,String[]> res = highlightFields(new String[] { field }, query, searcher, topDocs, maxPassages);
-    return res.get(field);
-  }
-  
-  /**
-   * Highlights the top passages from multiple fields.
-   * <p>
-   * Conceptually, this behaves as a more efficient form of:
-   * <pre class="prettyprint">
-   * Map m = new HashMap();
-   * for (String field : fields) {
-   *   m.put(field, highlight(field, query, searcher, topDocs));
-   * }
-   * return m;
-   * </pre>
-   * 
-   * @param fields field names to highlight. 
-   *        Must have a stored string value and also be indexed with offsets.
-   * @param query query to highlight.
-   * @param searcher searcher that was previously used to execute the query.
-   * @param topDocs TopDocs containing the summary result documents to highlight.
-   * @return Map keyed on field name, containing the array of formatted snippets 
-   *         corresponding to the documents in <code>topDocs</code>. 
-   *         If no highlights were found for a document, its value is <code>null</code>.
-   * @throws IOException if an I/O error occurred during processing
-   * @throws IllegalArgumentException if <code>field</code> was indexed without 
-   *         {@link IndexOptions#DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS}
-   */
-  public Map<String,String[]> highlightFields(String fields[], Query query, IndexSearcher searcher, TopDocs topDocs) throws IOException {
-    return highlightFields(fields, query, searcher, topDocs, 1);
-  }
-  
-  /**
-   * Highlights the top-N passages from multiple fields.
-   * <p>
-   * Conceptually, this behaves as a more efficient form of:
-   * <pre class="prettyprint">
-   * Map m = new HashMap();
-   * for (String field : fields) {
-   *   m.put(field, highlight(field, query, searcher, topDocs, maxPassages));
-   * }
-   * return m;
-   * </pre>
-   * 
-   * @param fields field names to highlight. 
-   *        Must have a stored string value and also be indexed with offsets.
-   * @param query query to highlight.
-   * @param searcher searcher that was previously used to execute the query.
-   * @param topDocs TopDocs containing the summary result documents to highlight.
-   * @param maxPassages The maximum number of top-N ranked passages per-field used to 
-   *        form the highlighted snippets.
-   * @return Map keyed on field name, containing the array of formatted snippets 
-   *         corresponding to the documents in <code>topDocs</code>. 
-   *         If no highlights were found for a document, its value is <code>null</code>.
-   * @throws IOException if an I/O error occurred during processing
-   * @throws IllegalArgumentException if <code>field</code> was indexed without 
-   *         {@link IndexOptions#DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS}
-   */
-  public Map<String,String[]> highlightFields(String fields[], Query query, IndexSearcher searcher, TopDocs topDocs, int maxPassages) throws IOException {
-    final IndexReader reader = searcher.getIndexReader();
-    final ScoreDoc scoreDocs[] = topDocs.scoreDocs;
-    query = rewrite(query);
-    SortedSet<Term> queryTerms = new TreeSet<Term>();
-    query.extractTerms(queryTerms);
-
-    int docids[] = new int[scoreDocs.length];
-    for (int i = 0; i < docids.length; i++) {
-      docids[i] = scoreDocs[i].doc;
-    }
-    IndexReaderContext readerContext = reader.getContext();
-    List<AtomicReaderContext> leaves = readerContext.leaves();
-
-    BreakIterator bi = (BreakIterator)breakIterator.clone();
-
-    // sort for sequential io
-    Arrays.sort(docids);
-    Arrays.sort(fields);
-    
-    // pull stored data:
-    LimitedStoredFieldVisitor visitor = new LimitedStoredFieldVisitor(fields, maxLength);
-    String contents[][] = new String[fields.length][docids.length];
-    for (int i = 0; i < docids.length; i++) {
-      reader.document(docids[i], visitor);
-      for (int j = 0; j < fields.length; j++) {
-        contents[j][i] = visitor.getValue(j).toString();
-      }
-      visitor.reset();
-    }
-    
-    Map<String,String[]> highlights = new HashMap<String,String[]>();
-    for (int i = 0; i < fields.length; i++) {
-      String field = fields[i];
-      Term floor = new Term(field, "");
-      Term ceiling = new Term(field, UnicodeUtil.BIG_TERM);
-      SortedSet<Term> fieldTerms = queryTerms.subSet(floor, ceiling);
-      // TODO: should we have some reasonable defaults for term pruning? (e.g. stopwords)
-      Term terms[] = fieldTerms.toArray(new Term[fieldTerms.size()]);
-      Map<Integer,String> fieldHighlights = highlightField(field, contents[i], bi, terms, docids, leaves, maxPassages);
-        
-      String[] result = new String[scoreDocs.length];
-      for (int j = 0; j < scoreDocs.length; j++) {
-        result[j] = fieldHighlights.get(scoreDocs[j].doc);
-      }
-      highlights.put(field, result);
-    }
-    return highlights;
-  }
-    
-  private Map<Integer,String> highlightField(String field, String contents[], BreakIterator bi, Term terms[], int[] docids, List<AtomicReaderContext> leaves, int maxPassages) throws IOException {  
-    Map<Integer,String> highlights = new HashMap<Integer,String>();
-    
-    // reuse in the real sense... for docs in same segment we just advance our old enum
-    DocsAndPositionsEnum postings[] = null;
-    TermsEnum termsEnum = null;
-    int lastLeaf = -1;
-    
-    for (int i = 0; i < docids.length; i++) {
-      String content = contents[i];
-      if (content.length() == 0) {
-        continue; // nothing to do
-      }
-      bi.setText(content);
-      int doc = docids[i];
-      int leaf = ReaderUtil.subIndex(doc, leaves);
-      AtomicReaderContext subContext = leaves.get(leaf);
-      AtomicReader r = subContext.reader();
-      Terms t = r.terms(field);
-      if (t == null) {
-        continue; // nothing to do
-      }
-      if (leaf != lastLeaf) {
-        termsEnum = t.iterator(null);
-        postings = new DocsAndPositionsEnum[terms.length];
-      }
-      Passage passages[] = highlightDoc(field, terms, content.length(), bi, doc - subContext.docBase, termsEnum, postings, maxPassages);
-      if (passages.length > 0) {
-        // otherwise a null snippet
-        highlights.put(doc, formatter.format(passages, content));
-      }
-      lastLeaf = leaf;
-    }
-    
-    return highlights;
-  }
-  
-  // algorithm: treat sentence snippets as miniature documents
-  // we can intersect these with the postings lists via BreakIterator.preceding(offset),s
-  // score each sentence as norm(sentenceStartOffset) * sum(weight * tf(freq))
-  private Passage[] highlightDoc(String field, Term terms[], int contentLength, BreakIterator bi, int doc, 
-      TermsEnum termsEnum, DocsAndPositionsEnum[] postings, int n) throws IOException {
-    PriorityQueue<OffsetsEnum> pq = new PriorityQueue<OffsetsEnum>();
-    float weights[] = new float[terms.length];
-    // initialize postings
-    for (int i = 0; i < terms.length; i++) {
-      DocsAndPositionsEnum de = postings[i];
-      int pDoc;
-      if (de == EMPTY) {
-        continue;
-      } else if (de == null) {
-        postings[i] = EMPTY; // initially
-        if (!termsEnum.seekExact(terms[i].bytes(), true)) {
-          continue; // term not found
-        }
-        de = postings[i] = termsEnum.docsAndPositions(null, null, DocsAndPositionsEnum.FLAG_OFFSETS);
-        if (de == null) {
-          // no positions available
-          throw new IllegalArgumentException("field '" + field + "' was indexed without offsets, cannot highlight");
-        }
-        pDoc = de.advance(doc);
-      } else {
-        pDoc = de.docID();
-        if (pDoc < doc) {
-          pDoc = de.advance(doc);
-        }
-      }
-
-      if (doc == pDoc) {
-        weights[i] = scorer.weight(contentLength, de.freq());
-        de.nextPosition();
-        pq.add(new OffsetsEnum(de, i));
-      }
-    }
-    
-    pq.add(new OffsetsEnum(EMPTY, Integer.MAX_VALUE)); // a sentinel for termination
-    
-    PriorityQueue<Passage> passageQueue = new PriorityQueue<Passage>(n, new Comparator<Passage>() {
-      @Override
-      public int compare(Passage left, Passage right) {
-        if (right.score == left.score) {
-          return right.startOffset - left.endOffset;
-        } else {
-          return right.score > left.score ? 1 : -1;
-        }
-      }
-    });
-    Passage current = new Passage();
-    
-    OffsetsEnum off;
-    while ((off = pq.poll()) != null) {
-      final DocsAndPositionsEnum dp = off.dp;
-      int start = dp.startOffset();
-      if (start == -1) {
-        throw new IllegalArgumentException("field '" + field + "' was indexed without offsets, cannot highlight");
-      }
-      int end = dp.endOffset();
-      if (start > current.endOffset) {
-        if (current.startOffset >= 0) {
-          // finalize current
-          current.score *= scorer.norm(current.startOffset);
-          // new sentence: first add 'current' to queue 
-          if (passageQueue.size() == n && current.score < passageQueue.peek().score) {
-            current.reset(); // can't compete, just reset it
-          } else {
-            passageQueue.offer(current);
-            if (passageQueue.size() > n) {
-              current = passageQueue.poll();
-              current.reset();
-            } else {
-              current = new Passage();
-            }
-          }
-        }
-        // if we exceed limit, we are done
-        if (start >= contentLength) {
-          Passage passages[] = new Passage[passageQueue.size()];
-          passageQueue.toArray(passages);
-          // sort in ascending order
-          Arrays.sort(passages, new Comparator<Passage>() {
-            @Override
-            public int compare(Passage left, Passage right) {
-              return left.startOffset - right.startOffset;
-            }
-          });
-          return passages;
-        }
-        // advance breakiterator
-        assert BreakIterator.DONE < 0;
-        current.startOffset = Math.max(bi.preceding(start+1), 0);
-        current.endOffset = Math.min(bi.next(), contentLength);
-      }
-      int tf = 0;
-      while (true) {
-        tf++;
-        current.addMatch(start, end, terms[off.id]);
-        if (off.pos == dp.freq()) {
-          break; // removed from pq
-        } else {
-          off.pos++;
-          dp.nextPosition();
-          start = dp.startOffset();
-          end = dp.endOffset();
-        }
-        if (start >= current.endOffset) {
-          pq.offer(off);
-          break;
-        }
-      }
-      current.score += weights[off.id] * scorer.tf(tf, current.endOffset - current.startOffset);
-    }
-    return new Passage[0];
-  }
-  
-  private static class OffsetsEnum implements Comparable<OffsetsEnum> {
-    DocsAndPositionsEnum dp;
-    int pos;
-    int id;
-    
-    OffsetsEnum(DocsAndPositionsEnum dp, int id) throws IOException {
-      this.dp = dp;
-      this.id = id;
-      this.pos = 1;
-    }
-
-    @Override
-    public int compareTo(OffsetsEnum other) {
-      try {
-        int off = dp.startOffset();
-        int otherOff = other.dp.startOffset();
-        if (off == otherOff) {
-          return id - other.id;
-        } else {
-          return Long.signum(((long)off) - otherOff);
-        }
-      } catch (IOException e) {
-        throw new RuntimeException(e);
-      }
-    }
-  }
-  
-  private static final DocsAndPositionsEnum EMPTY = new DocsAndPositionsEnum() {
-
-    @Override
-    public int nextPosition() throws IOException { return 0; }
-
-    @Override
-    public int startOffset() throws IOException { return Integer.MAX_VALUE; }
-
-    @Override
-    public int endOffset() throws IOException { return Integer.MAX_VALUE; }
-
-    @Override
-    public BytesRef getPayload() throws IOException { return null; }
-
-    @Override
-    public int freq() throws IOException { return 0; }
-
-    @Override
-    public int docID() { return NO_MORE_DOCS; }
-
-    @Override
-    public int nextDoc() throws IOException { return NO_MORE_DOCS; }
-
-    @Override
-    public int advance(int target) throws IOException { return NO_MORE_DOCS; }
-  };
-  
-  /** 
-   * we rewrite against an empty indexreader: as we don't want things like
-   * rangeQueries that don't summarize the document
-   */
-  private static Query rewrite(Query original) throws IOException {
-    Query query = original;
-    for (Query rewrittenQuery = query.rewrite(EMPTY_INDEXREADER); rewrittenQuery != query;
-         rewrittenQuery = query.rewrite(EMPTY_INDEXREADER)) {
-      query = rewrittenQuery;
-    }
-    return query;
-  }
-  
-  private static class LimitedStoredFieldVisitor extends StoredFieldVisitor {
-    private final String fields[];
-    private final int maxLength;
-    private final StringBuilder builders[];
-    private int currentField = -1;
-    
-    public LimitedStoredFieldVisitor(String fields[], int maxLength) {
-      this.fields = fields;
-      this.maxLength = maxLength;
-      builders = new StringBuilder[fields.length];
-      for (int i = 0; i < builders.length; i++) {
-        builders[i] = new StringBuilder();
-      }
-    }
-    
-    @Override
-    public void stringField(FieldInfo fieldInfo, String value) throws IOException {
-      assert currentField >= 0;
-      StringBuilder builder = builders[currentField];
-      if (builder.length() > 0) {
-        builder.append(' '); // for the offset gap, TODO: make this configurable
-      }
-      if (builder.length() + value.length() > maxLength) {
-        builder.append(value, 0, maxLength - builder.length());
-      } else {
-        builder.append(value);
-      }
-    }
-
-    @Override
-    public Status needsField(FieldInfo fieldInfo) throws IOException {
-      currentField = Arrays.binarySearch(fields, fieldInfo.name);
-      if (currentField < 0) {
-        return Status.NO;
-      } else if (builders[currentField].length() > maxLength) {
-        return fields.length == 1 ? Status.STOP : Status.NO;
-      }
-      return Status.YES;
-    }
-    
-    String getValue(int i) {
-      return builders[i].toString();
-    }
-    
-    void reset() {
-      currentField = -1;
-      for (int i = 0; i < fields.length; i++) {
-        builders[i].setLength(0);
-      }
-    }
-  }
-}
diff --git a/lucene/sandbox/src/java/org/apache/lucene/sandbox/postingshighlight/package.html b/lucene/sandbox/src/java/org/apache/lucene/sandbox/postingshighlight/package.html
deleted file mode 100644
index d6b4663..0000000
--- a/lucene/sandbox/src/java/org/apache/lucene/sandbox/postingshighlight/package.html
+++ /dev/null
@@ -1,22 +0,0 @@
-<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
-<!--
- Licensed to the Apache Software Foundation (ASF) under one or more
- contributor license agreements.  See the NOTICE file distributed with
- this work for additional information regarding copyright ownership.
- The ASF licenses this file to You under the Apache License, Version 2.0
- (the "License"); you may not use this file except in compliance with
- the License.  You may obtain a copy of the License at
-
-     http://www.apache.org/licenses/LICENSE-2.0
-
- Unless required by applicable law or agreed to in writing, software
- distributed under the License is distributed on an "AS IS" BASIS,
- WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- See the License for the specific language governing permissions and
- limitations under the License.
--->
-<html>
-<body>
-Highlighter implementation that uses offsets from postings lists.
-</body>
-</html>
\ No newline at end of file
diff --git a/lucene/sandbox/src/test/org/apache/lucene/sandbox/postingshighlight/TestPostingsHighlighter.java b/lucene/sandbox/src/test/org/apache/lucene/sandbox/postingshighlight/TestPostingsHighlighter.java
deleted file mode 100644
index e0740fa..0000000
--- a/lucene/sandbox/src/test/org/apache/lucene/sandbox/postingshighlight/TestPostingsHighlighter.java
+++ /dev/null
@@ -1,275 +0,0 @@
-package org.apache.lucene.sandbox.postingshighlight;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.util.Map;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.analysis.MockTokenizer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.FieldType;
-import org.apache.lucene.document.StringField;
-import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.IndexWriterConfig;
-import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.Sort;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.search.TopDocs;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util.LuceneTestCase.SuppressCodecs;
-
-@SuppressCodecs({"MockFixedIntBlock", "MockVariableIntBlock", "MockSep", "MockRandom"})
-public class TestPostingsHighlighter extends LuceneTestCase {
-  
-  public void testBasics() throws Exception {
-    Directory dir = newDirectory();
-    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
-    iwc.setMergePolicy(newLogMergePolicy());
-    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);
-    
-    FieldType offsetsType = new FieldType(TextField.TYPE_STORED);
-    offsetsType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);
-    Field body = new Field("body", "", offsetsType);
-    Document doc = new Document();
-    doc.add(body);
-    
-    body.setStringValue("This is a test. Just a test highlighting from postings. Feel free to ignore.");
-    iw.addDocument(doc);
-    body.setStringValue("Highlighting the first term. Hope it works.");
-    iw.addDocument(doc);
-    
-    IndexReader ir = iw.getReader();
-    iw.close();
-    
-    IndexSearcher searcher = newSearcher(ir);
-    PostingsHighlighter highlighter = new PostingsHighlighter();
-    Query query = new TermQuery(new Term("body", "highlighting"));
-    TopDocs topDocs = searcher.search(query, null, 10, Sort.INDEXORDER);
-    assertEquals(2, topDocs.totalHits);
-    String snippets[] = highlighter.highlight("body", query, searcher, topDocs);
-    assertEquals(2, snippets.length);
-    assertEquals("Just a test <b>highlighting</b> from postings. ", snippets[0]);
-    assertEquals("<b>Highlighting</b> the first term. ", snippets[1]);
-    
-    ir.close();
-    dir.close();
-  }
-  
-  // simple test with one sentence documents.
-  public void testOneSentence() throws Exception {
-    Directory dir = newDirectory();
-    // use simpleanalyzer for more natural tokenization (else "test." is a token)
-    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random(), MockTokenizer.SIMPLE, true));
-    iwc.setMergePolicy(newLogMergePolicy());
-    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);
-    
-    FieldType offsetsType = new FieldType(TextField.TYPE_STORED);
-    offsetsType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);
-    Field body = new Field("body", "", offsetsType);
-    Document doc = new Document();
-    doc.add(body);
-    
-    body.setStringValue("This is a test.");
-    iw.addDocument(doc);
-    body.setStringValue("Test a one sentence document.");
-    iw.addDocument(doc);
-    
-    IndexReader ir = iw.getReader();
-    iw.close();
-    
-    IndexSearcher searcher = newSearcher(ir);
-    PostingsHighlighter highlighter = new PostingsHighlighter();
-    Query query = new TermQuery(new Term("body", "test"));
-    TopDocs topDocs = searcher.search(query, null, 10, Sort.INDEXORDER);
-    assertEquals(2, topDocs.totalHits);
-    String snippets[] = highlighter.highlight("body", query, searcher, topDocs);
-    assertEquals(2, snippets.length);
-    assertEquals("This is a <b>test</b>.", snippets[0]);
-    assertEquals("<b>Test</b> a one sentence document.", snippets[1]);
-    
-    ir.close();
-    dir.close();
-  }
-  
-  public void testMultipleFields() throws Exception {
-    Directory dir = newDirectory();
-    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random(), MockTokenizer.SIMPLE, true));
-    iwc.setMergePolicy(newLogMergePolicy());
-    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);
-    
-    FieldType offsetsType = new FieldType(TextField.TYPE_STORED);
-    offsetsType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);
-    Field body = new Field("body", "", offsetsType);
-    Field title = new Field("title", "", offsetsType);
-    Document doc = new Document();
-    doc.add(body);
-    doc.add(title);
-    
-    body.setStringValue("This is a test. Just a test highlighting from postings. Feel free to ignore.");
-    title.setStringValue("I am hoping for the best.");
-    iw.addDocument(doc);
-    body.setStringValue("Highlighting the first term. Hope it works.");
-    title.setStringValue("But best may not be good enough.");
-    iw.addDocument(doc);
-    
-    IndexReader ir = iw.getReader();
-    iw.close();
-    
-    IndexSearcher searcher = newSearcher(ir);
-    PostingsHighlighter highlighter = new PostingsHighlighter();
-    BooleanQuery query = new BooleanQuery();
-    query.add(new TermQuery(new Term("body", "highlighting")), BooleanClause.Occur.SHOULD);
-    query.add(new TermQuery(new Term("title", "best")), BooleanClause.Occur.SHOULD);
-    TopDocs topDocs = searcher.search(query, null, 10, Sort.INDEXORDER);
-    assertEquals(2, topDocs.totalHits);
-    Map<String,String[]> snippets = highlighter.highlightFields(new String [] { "body", "title" }, query, searcher, topDocs);
-    assertEquals(2, snippets.size());
-    assertEquals("Just a test <b>highlighting</b> from postings. ", snippets.get("body")[0]);
-    assertEquals("<b>Highlighting</b> the first term. ", snippets.get("body")[1]);
-    assertEquals("I am hoping for the <b>best</b>.", snippets.get("title")[0]);
-    assertEquals("But <b>best</b> may not be good enough.", snippets.get("title")[1]);
-    ir.close();
-    dir.close();
-  }
-  
-  public void testMultipleTerms() throws Exception {
-    Directory dir = newDirectory();
-    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random()));
-    iwc.setMergePolicy(newLogMergePolicy());
-    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);
-    
-    FieldType offsetsType = new FieldType(TextField.TYPE_STORED);
-    offsetsType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);
-    Field body = new Field("body", "", offsetsType);
-    Document doc = new Document();
-    doc.add(body);
-    
-    body.setStringValue("This is a test. Just a test highlighting from postings. Feel free to ignore.");
-    iw.addDocument(doc);
-    body.setStringValue("Highlighting the first term. Hope it works.");
-    iw.addDocument(doc);
-    
-    IndexReader ir = iw.getReader();
-    iw.close();
-    
-    IndexSearcher searcher = newSearcher(ir);
-    PostingsHighlighter highlighter = new PostingsHighlighter();
-    BooleanQuery query = new BooleanQuery();
-    query.add(new TermQuery(new Term("body", "highlighting")), BooleanClause.Occur.SHOULD);
-    query.add(new TermQuery(new Term("body", "just")), BooleanClause.Occur.SHOULD);
-    query.add(new TermQuery(new Term("body", "first")), BooleanClause.Occur.SHOULD);
-    TopDocs topDocs = searcher.search(query, null, 10, Sort.INDEXORDER);
-    assertEquals(2, topDocs.totalHits);
-    String snippets[] = highlighter.highlight("body", query, searcher, topDocs);
-    assertEquals(2, snippets.length);
-    assertEquals("<b>Just</b> a test <b>highlighting</b> from postings. ", snippets[0]);
-    assertEquals("<b>Highlighting</b> the <b>first</b> term. ", snippets[1]);
-    
-    ir.close();
-    dir.close();
-  }
-  
-  public void testMultiplePassages() throws Exception {
-    Directory dir = newDirectory();
-    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random(), MockTokenizer.SIMPLE, true));
-    iwc.setMergePolicy(newLogMergePolicy());
-    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);
-    
-    FieldType offsetsType = new FieldType(TextField.TYPE_STORED);
-    offsetsType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);
-    Field body = new Field("body", "", offsetsType);
-    Document doc = new Document();
-    doc.add(body);
-    
-    body.setStringValue("This is a test. Just a test highlighting from postings. Feel free to ignore.");
-    iw.addDocument(doc);
-    body.setStringValue("This test is another test. Not a good sentence. Test test test test.");
-    iw.addDocument(doc);
-    
-    IndexReader ir = iw.getReader();
-    iw.close();
-    
-    IndexSearcher searcher = newSearcher(ir);
-    PostingsHighlighter highlighter = new PostingsHighlighter();
-    Query query = new TermQuery(new Term("body", "test"));
-    TopDocs topDocs = searcher.search(query, null, 10, Sort.INDEXORDER);
-    assertEquals(2, topDocs.totalHits);
-    String snippets[] = highlighter.highlight("body", query, searcher, topDocs, 2);
-    assertEquals(2, snippets.length);
-    assertEquals("This is a <b>test</b>. Just a <b>test</b> highlighting from postings. ", snippets[0]);
-    assertEquals("This <b>test</b> is another <b>test</b>. ... <b>Test</b> <b>test</b> <b>test</b> <b>test</b>.", snippets[1]);
-    
-    ir.close();
-    dir.close();
-  }
-
-  public void testUserFailedToIndexOffsets() throws Exception {
-    Directory dir = newDirectory();
-    IndexWriterConfig iwc = newIndexWriterConfig(TEST_VERSION_CURRENT, new MockAnalyzer(random(), MockTokenizer.SIMPLE, true));
-    iwc.setMergePolicy(newLogMergePolicy());
-    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, iwc);
-    
-    FieldType positionsType = new FieldType(TextField.TYPE_STORED);
-    positionsType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS);
-    Field body = new Field("body", "", positionsType);
-    Field title = new StringField("title", "", Field.Store.YES);
-    Document doc = new Document();
-    doc.add(body);
-    doc.add(title);
-    
-    body.setStringValue("This is a test. Just a test highlighting from postings. Feel free to ignore.");
-    title.setStringValue("test");
-    iw.addDocument(doc);
-    body.setStringValue("This test is another test. Not a good sentence. Test test test test.");
-    title.setStringValue("test");
-    iw.addDocument(doc);
-    
-    IndexReader ir = iw.getReader();
-    iw.close();
-    
-    IndexSearcher searcher = newSearcher(ir);
-    PostingsHighlighter highlighter = new PostingsHighlighter();
-    Query query = new TermQuery(new Term("body", "test"));
-    TopDocs topDocs = searcher.search(query, null, 10, Sort.INDEXORDER);
-    assertEquals(2, topDocs.totalHits);
-    try {
-      highlighter.highlight("body", query, searcher, topDocs, 2);
-      fail("did not hit expected exception");
-    } catch (IllegalArgumentException iae) {
-      // expected
-    }
-    
-    try {
-      highlighter.highlight("title", new TermQuery(new Term("title", "test")), searcher, topDocs, 2);
-      fail("did not hit expected exception");
-    } catch (IllegalArgumentException iae) {
-      // expected
-    }
-    ir.close();
-    dir.close();
-  }
-}
diff --git a/lucene/sandbox/src/test/org/apache/lucene/sandbox/postingshighlight/TestPostingsHighlighterRanking.java b/lucene/sandbox/src/test/org/apache/lucene/sandbox/postingshighlight/TestPostingsHighlighterRanking.java
deleted file mode 100644
index e1db5b2..0000000
--- a/lucene/sandbox/src/test/org/apache/lucene/sandbox/postingshighlight/TestPostingsHighlighterRanking.java
+++ /dev/null
@@ -1,234 +0,0 @@
-package org.apache.lucene.sandbox.postingshighlight;
-
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements.  See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the "License"); you may not use this file except in compliance with
- * the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-import java.io.IOException;
-import java.text.BreakIterator;
-import java.util.HashSet;
-import java.util.Locale;
-import java.util.Random;
-
-import org.apache.lucene.analysis.MockAnalyzer;
-import org.apache.lucene.analysis.MockTokenizer;
-import org.apache.lucene.document.Document;
-import org.apache.lucene.document.Field;
-import org.apache.lucene.document.FieldType;
-import org.apache.lucene.document.StringField;
-import org.apache.lucene.document.TextField;
-import org.apache.lucene.index.IndexReader;
-import org.apache.lucene.index.RandomIndexWriter;
-import org.apache.lucene.index.FieldInfo.IndexOptions;
-import org.apache.lucene.index.Term;
-import org.apache.lucene.search.BooleanClause;
-import org.apache.lucene.search.BooleanQuery;
-import org.apache.lucene.search.IndexSearcher;
-import org.apache.lucene.search.Query;
-import org.apache.lucene.search.TermQuery;
-import org.apache.lucene.search.TopDocs;
-import org.apache.lucene.store.Directory;
-import org.apache.lucene.util.BytesRef;
-import org.apache.lucene.util.LuceneTestCase;
-import org.apache.lucene.util._TestUtil;
-import org.apache.lucene.util.LuceneTestCase.SuppressCodecs;
-
-@SuppressCodecs({"MockFixedIntBlock", "MockVariableIntBlock", "MockSep", "MockRandom"})
-public class TestPostingsHighlighterRanking extends LuceneTestCase {
-  /** 
-   * indexes a bunch of gibberish, and then highlights top(n).
-   * asserts that top(n) highlights is a subset of top(n+1) up to some max N
-   */
-  // TODO: this only tests single-valued fields. we should also index multiple values per field!
-  public void testRanking() throws Exception {
-    // number of documents: we will check each one
-    final int numDocs = atLeast(100);
-    // number of top-N snippets, we will check 1 .. N
-    final int maxTopN = 5;
-    // maximum number of elements to put in a sentence.
-    final int maxSentenceLength = 10;
-    // maximum number of sentences in a document
-    final int maxNumSentences = 20;
-    
-    Directory dir = newDirectory();
-    RandomIndexWriter iw = new RandomIndexWriter(random(), dir, new MockAnalyzer(random(), MockTokenizer.SIMPLE, true));
-    Document document = new Document();
-    Field id = new StringField("id", "", Field.Store.NO);
-    FieldType offsetsType = new FieldType(TextField.TYPE_STORED);
-    offsetsType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);
-    Field body = new Field("body", "", offsetsType);
-    document.add(id);
-    document.add(body);
-    
-    for (int i = 0; i < numDocs; i++) {;
-      StringBuilder bodyText = new StringBuilder();
-      int numSentences = _TestUtil.nextInt(random(), 1, maxNumSentences);
-      for (int j = 0; j < numSentences; j++) {
-        bodyText.append(newSentence(random(), maxSentenceLength));
-      }
-      body.setStringValue(bodyText.toString());
-      id.setStringValue(Integer.toString(i));
-      iw.addDocument(document);
-    }
-    
-    IndexReader ir = iw.getReader();
-    IndexSearcher searcher = newSearcher(ir);
-    for (int i = 0; i < numDocs; i++) {
-      checkDocument(searcher, i, maxTopN);
-    }
-    iw.close();
-    ir.close();
-    dir.close();
-  }
-  
-  private void checkDocument(IndexSearcher is, int doc, int maxTopN) throws IOException {
-    for (int ch = 'a'; ch <= 'z'; ch++) {
-      Term term = new Term("body", "" + (char)ch);
-      // check a simple term query
-      checkQuery(is, new TermQuery(term), doc, maxTopN);
-      // check a boolean query
-      BooleanQuery bq = new BooleanQuery();
-      bq.add(new TermQuery(term), BooleanClause.Occur.SHOULD);
-      Term nextTerm = new Term("body", "" + (char)(ch+1));
-      bq.add(new TermQuery(nextTerm), BooleanClause.Occur.SHOULD);
-      checkQuery(is, bq, doc, maxTopN);
-    }
-  }
-  
-  private void checkQuery(IndexSearcher is, Query query, int doc, int maxTopN) throws IOException {
-    for (int n = 1; n < maxTopN; n++) {
-      FakePassageFormatter f1 = new FakePassageFormatter();
-      PostingsHighlighter p1 = new PostingsHighlighter(Integer.MAX_VALUE-1, 
-                                                       BreakIterator.getSentenceInstance(Locale.ROOT), 
-                                                       new PassageScorer(),
-                                                       f1);
-      FakePassageFormatter f2 = new FakePassageFormatter();
-      PostingsHighlighter p2 = new PostingsHighlighter(Integer.MAX_VALUE-1, 
-                                                       BreakIterator.getSentenceInstance(Locale.ROOT), 
-                                                       new PassageScorer(),
-                                                       f2);
-      BooleanQuery bq = new BooleanQuery(false);
-      bq.add(query, BooleanClause.Occur.MUST);
-      bq.add(new TermQuery(new Term("id", Integer.toString(doc))), BooleanClause.Occur.MUST);
-      TopDocs td = is.search(bq, 1);
-      p1.highlight("body", bq, is, td, n);
-      p2.highlight("body", bq, is, td, n+1);
-      assertTrue(f2.seen.containsAll(f1.seen));
-    }
-  }
-  
-  /** 
-   * returns a new random sentence, up to maxSentenceLength "words" in length.
-   * each word is a single character (a-z). The first one is capitalized.
-   */
-  private String newSentence(Random r, int maxSentenceLength) {
-    StringBuilder sb = new StringBuilder();
-    int numElements = _TestUtil.nextInt(r, 1, maxSentenceLength);
-    for (int i = 0; i < numElements; i++) {
-      if (sb.length() > 0) {
-        sb.append(' ');
-        sb.append((char)_TestUtil.nextInt(r, 'a', 'z'));
-      } else {
-        // capitalize the first word to help breakiterator
-        sb.append((char)_TestUtil.nextInt(r, 'A', 'Z'));
-      }
-    }
-    sb.append(". "); // finalize sentence
-    return sb.toString();
-  }
-  
-  /** 
-   * a fake formatter that doesn't actually format passages.
-   * instead it just collects them for asserts!
-   */
-  static class FakePassageFormatter extends PassageFormatter {
-    HashSet<Pair> seen = new HashSet<Pair>();
-    
-    @Override
-    public String format(Passage passages[], String content) {
-      for (Passage p : passages) {
-        // verify some basics about the passage
-        assertTrue(p.getScore() >= 0);
-        assertTrue(p.getNumMatches() > 0);
-        assertTrue(p.getStartOffset() >= 0);
-        assertTrue(p.getStartOffset() <= content.length());
-        // we use a very simple analyzer. so we can assert the matches are correct
-        for (int i = 0; i < p.getNumMatches(); i++) {
-          Term term = p.getMatchTerms()[i];
-          assertEquals("body", term.field());
-          int matchStart = p.getMatchStarts()[i];
-          assertTrue(matchStart >= 0);
-          int matchEnd = p.getMatchEnds()[i];
-          assertTrue(matchEnd >= 0);
-          // single character terms
-          assertEquals(matchStart+1, matchEnd);
-          // and the offsets must be correct...
-          BytesRef bytes = term.bytes();
-          assertEquals(1, bytes.length);
-          assertEquals((char)bytes.bytes[bytes.offset], Character.toLowerCase(content.charAt(matchStart)));
-        }
-        // record just the start/end offset for simplicity
-        seen.add(new Pair(p.getStartOffset(), p.getEndOffset()));
-      }
-      return "bogus!!!!!!";
-    }
-  }
-  
-  static class Pair {
-    final int start;
-    final int end;
-    
-    Pair(int start, int end) {
-      this.start = start;
-      this.end = end;
-    }
-
-    @Override
-    public int hashCode() {
-      final int prime = 31;
-      int result = 1;
-      result = prime * result + end;
-      result = prime * result + start;
-      return result;
-    }
-
-    @Override
-    public boolean equals(Object obj) {
-      if (this == obj) {
-        return true;
-      }
-      if (obj == null) {
-        return false;
-      }
-      if (getClass() != obj.getClass()) {
-        return false;
-      }
-      Pair other = (Pair) obj;
-      if (end != other.end) {
-        return false;
-      }
-      if (start != other.start) {
-        return false;
-      }
-      return true;
-    }
-
-    @Override
-    public String toString() {
-      return "Pair [start=" + start + ", end=" + end + "]";
-    }
-  }
-}
diff --git a/solr/CHANGES.txt b/solr/CHANGES.txt
index 4a1bfc7..0c535da 100644
--- a/solr/CHANGES.txt
+++ b/solr/CHANGES.txt
@@ -189,6 +189,8 @@ New Features
   rolling averages; median, 75th, 95th, 99th, 99.9th percentile request times
   (Alan Woodward, Shawn Heisey, Adrien Grand, Uwe Schindler)
 
+* SOLR-4271: Add support for PostingsHighlighter.  (Robert Muir)
+
 Optimizations
 ----------------------
 
diff --git a/solr/core/src/java/org/apache/solr/handler/component/HighlightComponent.java b/solr/core/src/java/org/apache/solr/handler/component/HighlightComponent.java
index b26e258..9e17ab5 100644
--- a/solr/core/src/java/org/apache/solr/handler/component/HighlightComponent.java
+++ b/solr/core/src/java/org/apache/solr/handler/component/HighlightComponent.java
@@ -24,6 +24,7 @@ import org.apache.solr.common.params.HighlightParams;
 import org.apache.solr.common.params.SolrParams;
 import org.apache.solr.common.util.NamedList;
 import org.apache.solr.common.util.SimpleOrderedMap;
+import org.apache.solr.highlight.PostingsSolrHighlighter;
 import org.apache.solr.highlight.SolrHighlighter;
 import org.apache.solr.highlight.DefaultSolrHighlighter;
 import org.apache.solr.request.SolrQueryRequest;
@@ -128,7 +129,7 @@ public class HighlightComponent extends SearchComponent implements PluginInfoIni
       }
       
       if(highlightQuery != null) {
-        boolean rewrite = !(Boolean.valueOf(params.get(HighlightParams.USE_PHRASE_HIGHLIGHTER, "true")) &&
+        boolean rewrite = (highlighter instanceof PostingsSolrHighlighter == false) && !(Boolean.valueOf(params.get(HighlightParams.USE_PHRASE_HIGHLIGHTER, "true")) &&
             Boolean.valueOf(params.get(HighlightParams.HIGHLIGHT_MULTI_TERM, "true")));
         highlightQuery = rewrite ?  highlightQuery.rewrite(req.getSearcher().getIndexReader()) : highlightQuery;
       }
diff --git a/solr/core/src/java/org/apache/solr/highlight/PostingsSolrHighlighter.java b/solr/core/src/java/org/apache/solr/highlight/PostingsSolrHighlighter.java
new file mode 100644
index 0000000..7f0d2ed
--- /dev/null
+++ b/solr/core/src/java/org/apache/solr/highlight/PostingsSolrHighlighter.java
@@ -0,0 +1,189 @@
+package org.apache.solr.highlight;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import java.io.IOException;
+import java.text.BreakIterator;
+import java.util.Collections;
+import java.util.Locale;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.lucene.index.StoredDocument;
+import org.apache.lucene.search.Query;
+import org.apache.lucene.search.ScoreDoc;
+import org.apache.lucene.search.TopDocs;
+import org.apache.lucene.search.postingshighlight.PassageFormatter;
+import org.apache.lucene.search.postingshighlight.PassageScorer;
+import org.apache.lucene.search.postingshighlight.PostingsHighlighter;
+import org.apache.solr.common.params.HighlightParams;
+import org.apache.solr.common.params.SolrParams;
+import org.apache.solr.common.util.NamedList;
+import org.apache.solr.common.util.SimpleOrderedMap;
+import org.apache.solr.core.PluginInfo;
+import org.apache.solr.core.SolrConfig;
+import org.apache.solr.request.SolrQueryRequest;
+import org.apache.solr.schema.IndexSchema;
+import org.apache.solr.schema.SchemaField;
+import org.apache.solr.search.DocIterator;
+import org.apache.solr.search.DocList;
+import org.apache.solr.search.SolrIndexSearcher;
+import org.apache.solr.util.plugin.PluginInfoInitialized;
+
+/** 
+ * Highlighter impl that uses {@link PostingsHighlighter}
+ * <p>
+ * Example configuration:
+ * <pre class="prettyprint">
+ *   &lt;searchComponent class="solr.HighlightComponent" name="highlight"&gt;
+ *     &lt;highlighting class="org.apache.solr.highlight.PostingsSolrHighlighter"
+ *                      preTag="&amp;lt;em&amp;gt;"
+ *                      postTag="&amp;lt;/em&amp;gt;"
+ *                      ellipsis="... "
+ *                      maxLength=10000/&gt;
+ *   &lt;/searchComponent&gt;
+ * </pre>
+ * <p>
+ * Notes:
+ *  <ul>
+ *    <li>fields to highlight must be configured with storeOffsetsWithPositions="true"
+ *    <li>hl.fl specifies the field list.
+ *    <li>hl.snippets specifies how many underlying sentence fragments form the resulting snippet.
+ *  </ul>
+ *  
+ * @lucene.experimental 
+ */
+public class PostingsSolrHighlighter extends SolrHighlighter implements PluginInfoInitialized {
+  protected PostingsHighlighter highlighter;
+
+  @Override
+  public void initalize(SolrConfig config) {}
+  
+  @Override
+  public void init(PluginInfo info) {
+    Map<String,String> attributes = info.attributes;
+    BreakIterator breakIterator = BreakIterator.getSentenceInstance(Locale.ROOT);
+    PassageScorer scorer = new PassageScorer();
+    
+    // formatter parameters: preTag/postTag/ellipsis
+    String preTag = attributes.get("preTag");
+    if (preTag == null) {
+      preTag = "<em>";
+    }
+    String postTag = attributes.get("postTag");
+    if (postTag == null) {
+      postTag = "</em>";
+    }
+    String ellipsis = attributes.get("ellipsis");
+    if (ellipsis == null) {
+      ellipsis = "... ";
+    }
+    PassageFormatter formatter = new PassageFormatter(preTag, postTag, ellipsis);
+    
+    // maximum content size to process
+    int maxLength = PostingsHighlighter.DEFAULT_MAX_LENGTH;
+    if (attributes.containsKey("maxLength")) {
+      maxLength = Integer.parseInt(attributes.get("maxLength"));
+    }
+    highlighter = new PostingsHighlighter(maxLength, breakIterator, scorer, formatter);
+  }
+
+  @Override
+  public NamedList<Object> doHighlighting(DocList docs, Query query, SolrQueryRequest req, String[] defaultFields) throws IOException {
+    SolrParams params = req.getParams(); 
+    
+    // if highlighting isnt enabled, then why call doHighlighting?
+    if (isHighlightingEnabled(params)) {
+      SolrIndexSearcher searcher = req.getSearcher();
+      TopDocs topDocs = toTopDocs(docs);
+      
+      // fetch the unique keys
+      String[] keys = getUniqueKeys(searcher, topDocs);
+      
+      // query-time parameters
+      String[] fieldNames = getHighlightFields(query, req, defaultFields);
+      int numSnippets = params.getInt(HighlightParams.SNIPPETS, 1);
+      
+      Map<String,String[]> snippets = highlighter.highlightFields(fieldNames, query, searcher, topDocs, numSnippets);
+      return encodeSnippets(keys, fieldNames, snippets);
+    } else {
+      return null;
+    }
+  }
+  
+  /** 
+   * Encodes the resulting snippets into a namedlist
+   * @param keys the document unique keys
+   * @param fieldNames field names to highlight in the order
+   * @param snippets map from field name to snippet array for the docs
+   * @return encoded namedlist of summaries
+   */
+  protected NamedList<Object> encodeSnippets(String[] keys, String[] fieldNames, Map<String,String[]> snippets) {
+    NamedList<Object> list = new SimpleOrderedMap<Object>();
+    for (int i = 0; i < keys.length; i++) {
+      NamedList<Object> summary = new SimpleOrderedMap<Object>();
+      for (String field : fieldNames) {
+        String snippet = snippets.get(field)[i];
+        // box in an array to match the format of existing highlighters, 
+        // even though its always one element.
+        if (snippet == null) {
+          summary.add(field, new String[0]);
+        } else {
+          summary.add(field, new String[] { snippet });
+        }
+      }
+      list.add(keys[i], summary);
+    }
+    return list;
+  }
+  
+  /** Converts solr's DocList to a lucene TopDocs */
+  protected TopDocs toTopDocs(DocList docs) {
+    ScoreDoc[] scoreDocs = new ScoreDoc[docs.size()];
+    DocIterator iterator = docs.iterator();
+    for (int i = 0; i < scoreDocs.length; i++) {
+      if (!iterator.hasNext()) {
+        throw new AssertionError();
+      }
+      scoreDocs[i] = new ScoreDoc(iterator.nextDoc(), Float.NaN);
+    }
+    if (iterator.hasNext()) {
+      throw new AssertionError();
+    }
+    return new TopDocs(docs.matches(), scoreDocs, Float.NaN);
+  }
+  
+  /** Retrieves the unique keys for the topdocs to key the results */
+  protected String[] getUniqueKeys(SolrIndexSearcher searcher, TopDocs topDocs) throws IOException {
+    IndexSchema schema = searcher.getSchema();
+    SchemaField keyField = schema.getUniqueKeyField();
+    if (keyField != null) {
+      Set<String> selector = Collections.singleton(keyField.getName());
+      String uniqueKeys[] = new String[topDocs.scoreDocs.length];
+      for (int i = 0; i < topDocs.scoreDocs.length; i++) {
+        int docid = topDocs.scoreDocs[i].doc;
+        StoredDocument doc = searcher.doc(docid, selector);
+        String id = schema.printableUniqueKey(doc);
+        uniqueKeys[i] = id;
+      }
+      return uniqueKeys;
+    } else {
+      return new String[topDocs.scoreDocs.length];
+    }
+  }
+}
diff --git a/solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java b/solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java
index 6590645..13f0510 100644
--- a/solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java
+++ b/solr/core/src/java/org/apache/solr/search/SolrIndexSearcher.java
@@ -109,6 +109,8 @@ public class SolrIndexSearcher extends IndexSearcher implements Closeable,SolrIn
   private final SolrCache[] cacheList;
   private static final SolrCache[] noCaches = new SolrCache[0];
   
+  private final FieldInfos fieldInfos;
+  // TODO: do we need this separate set of field names? we can just use the fieldinfos?
   private final Collection<String> fieldNames;
   private Collection<String> storedHighlightFieldNames;
   private DirectoryFactory directoryFactory;
@@ -199,7 +201,8 @@ public class SolrIndexSearcher extends IndexSearcher implements Closeable,SolrIn
     optimizer = null;
     
     fieldNames = new HashSet<String>();
-    for(FieldInfo fieldInfo : atomicReader.getFieldInfos()) {
+    fieldInfos = atomicReader.getFieldInfos();
+    for(FieldInfo fieldInfo : fieldInfos) {
       fieldNames.add(fieldInfo.name);
     }
 
@@ -509,13 +512,56 @@ public class SolrIndexSearcher extends IndexSearcher implements Closeable,SolrIn
   }
 
   /** Visit a document's fields using a {@link StoredFieldVisitor}
-   *  This method does not currently use the Solr document cache.
+   *  This method does not currently add to the Solr document cache.
    * 
    * @see IndexReader#document(int, StoredFieldVisitor) */
   @Override
   public void doc(int n, StoredFieldVisitor visitor) throws IOException {
+    if (documentCache != null) {
+      StoredDocument cached = documentCache.get(n);
+      if (cached != null) {
+        visitFromCached(cached, visitor);
+        return;
+      }
+    }
     getIndexReader().document(n, visitor);
   }
+  
+  /** Executes a stored field visitor against a hit from the document cache */
+  private void visitFromCached(StoredDocument document, StoredFieldVisitor visitor) throws IOException {
+    for (StorableField f : document) {
+      FieldInfo info = fieldInfos.fieldInfo(f.name());
+      switch(visitor.needsField(info)) {
+        case YES:
+          if (f.binaryValue() != null) {
+            BytesRef binaryValue = f.binaryValue();
+            byte copy[] = new byte[binaryValue.length];
+            System.arraycopy(binaryValue.bytes, binaryValue.offset, copy, 0, copy.length);
+            visitor.binaryField(info, copy);
+          } else if (f.numericValue() != null) {
+            Number numericValue = f.numericValue();
+            if (numericValue instanceof Double) {
+              visitor.doubleField(info, numericValue.doubleValue());
+            } else if (numericValue instanceof Integer) {
+              visitor.intField(info, numericValue.intValue());
+            } else if (numericValue instanceof Float) {
+              visitor.floatField(info, numericValue.floatValue());
+            } else if (numericValue instanceof Long) {
+              visitor.longField(info, numericValue.longValue());
+            } else {
+              throw new AssertionError();
+            }
+          } else {
+            visitor.stringField(info, f.stringValue());
+          }
+          break;
+        case NO:
+          break;
+        case STOP:
+          return;
+      }
+    }
+  }
 
   /**
    * Retrieve the {@link Document} instance corresponding to the document id.
diff --git a/solr/core/src/test-files/solr/collection1/conf/schema-postingshighlight.xml b/solr/core/src/test-files/solr/collection1/conf/schema-postingshighlight.xml
new file mode 100644
index 0000000..fedc9fc
--- /dev/null
+++ b/solr/core/src/test-files/solr/collection1/conf/schema-postingshighlight.xml
@@ -0,0 +1,49 @@
+<?xml version="1.0" ?>
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+
+<!-- Test schema file for PostingsHighlighter -->
+
+<schema name="postingshighlight" version="1.0">
+  <types>
+    <fieldType name="int" class="solr.TrieIntField" precisionStep="0" omitNorms="true" positionIncrementGap="0"/>
+
+    <!-- basic text field: no offsets! -->
+    <fieldtype name="text" class="solr.TextField">
+      <analyzer>
+        <tokenizer class="solr.MockTokenizerFactory"/>
+      </analyzer>
+    </fieldtype>
+    
+    <!-- text field with offsets -->
+    <fieldtype name="text_offsets" class="solr.TextField" storeOffsetsWithPositions="true">
+      <analyzer>
+        <tokenizer class="solr.MockTokenizerFactory"/>
+      </analyzer>
+    </fieldtype>
+   </types>
+
+  <fields>
+    <field name="id" type="int" indexed="true" stored="true" multiValued="false" required="false"/>
+    <field name="text" type="text_offsets" indexed="true" stored="true"/>
+    <field name="text2" type="text" indexed="true" stored="true"/>
+    <field name="text3" type="text_offsets" indexed="true" stored="true"/>
+  </fields>
+
+  <defaultSearchField>text</defaultSearchField>
+  <uniqueKey>id</uniqueKey>
+</schema>
diff --git a/solr/core/src/test-files/solr/collection1/conf/solrconfig-postingshighlight.xml b/solr/core/src/test-files/solr/collection1/conf/solrconfig-postingshighlight.xml
new file mode 100644
index 0000000..881c8de
--- /dev/null
+++ b/solr/core/src/test-files/solr/collection1/conf/solrconfig-postingshighlight.xml
@@ -0,0 +1,30 @@
+<?xml version="1.0" ?>
+
+<!--
+ Licensed to the Apache Software Foundation (ASF) under one or more
+ contributor license agreements.  See the NOTICE file distributed with
+ this work for additional information regarding copyright ownership.
+ The ASF licenses this file to You under the Apache License, Version 2.0
+ (the "License"); you may not use this file except in compliance with
+ the License.  You may obtain a copy of the License at
+
+     http://www.apache.org/licenses/LICENSE-2.0
+
+ Unless required by applicable law or agreed to in writing, software
+ distributed under the License is distributed on an "AS IS" BASIS,
+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ See the License for the specific language governing permissions and
+ limitations under the License.
+-->
+
+<!-- a basic solrconfig for postings highlighter -->
+<config>
+  <luceneMatchVersion>${tests.luceneMatchVersion:LUCENE_CURRENT}</luceneMatchVersion>
+  <dataDir>${solr.data.dir:}</dataDir>
+  <directoryFactory name="DirectoryFactory" class="${solr.directoryFactory:solr.RAMDirectoryFactory}"/>
+  <requestHandler name="standard" class="solr.StandardRequestHandler"></requestHandler>
+
+  <searchComponent class="solr.HighlightComponent" name="highlight">
+    <highlighting class="org.apache.solr.highlight.PostingsSolrHighlighter"/>
+  </searchComponent>
+</config>
diff --git a/solr/core/src/test/org/apache/solr/highlight/TestPostingsSolrHighlighter.java b/solr/core/src/test/org/apache/solr/highlight/TestPostingsSolrHighlighter.java
new file mode 100644
index 0000000..d65042d
--- /dev/null
+++ b/solr/core/src/test/org/apache/solr/highlight/TestPostingsSolrHighlighter.java
@@ -0,0 +1,103 @@
+package org.apache.solr.highlight;
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import org.apache.lucene.util.LuceneTestCase.SuppressCodecs;
+import org.apache.solr.SolrTestCaseJ4;
+import org.apache.solr.handler.component.HighlightComponent;
+import org.apache.solr.schema.IndexSchema;
+import org.junit.BeforeClass;
+
+/** simple tests for PostingsSolrHighlighter */
+@SuppressCodecs({"MockFixedIntBlock", "MockVariableIntBlock", "MockSep", "MockRandom"})
+public class TestPostingsSolrHighlighter extends SolrTestCaseJ4 {
+  
+  @BeforeClass
+  public static void beforeClass() throws Exception {
+    initCore("solrconfig-postingshighlight.xml", "schema-postingshighlight.xml");
+    
+    // test our config is sane, just to be sure:
+    
+    // postingshighlighter should be used
+    SolrHighlighter highlighter = HighlightComponent.getHighlighter(h.getCore());
+    assertTrue("wrong highlighter: " + highlighter.getClass(), highlighter instanceof PostingsSolrHighlighter);
+    
+    // 'text' and 'text3' should have offsets, 'text2' should not
+    IndexSchema schema = h.getCore().getSchema();
+    assertTrue(schema.getField("text").storeOffsetsWithPositions());
+    assertTrue(schema.getField("text3").storeOffsetsWithPositions());
+    assertFalse(schema.getField("text2").storeOffsetsWithPositions());
+    
+    assertU(adoc("text", "document one", "text2", "document one", "text3", "crappy document", "id", "101"));
+    assertU(adoc("text", "second document", "text2", "second document", "text3", "crappier document", "id", "102"));
+    assertU(commit());
+  }
+  
+  public void testSimple() {
+    assertQ("simplest test", 
+        req("q", "text:document", "sort", "id asc", "hl", "true"),
+        "count(//lst[@name='highlighting']/*)=2",
+        "//lst[@name='highlighting']/lst[@name='101']/arr[@name='text']/str='<em>document</em> one'",
+        "//lst[@name='highlighting']/lst[@name='102']/arr[@name='text']/str='second <em>document</em>'");
+  }
+  
+  public void testPagination() {
+    assertQ("pagination test", 
+        req("q", "text:document", "sort", "id asc", "hl", "true", "rows", "1", "start", "1"),
+        "count(//lst[@name='highlighting']/*)=1",
+        "//lst[@name='highlighting']/lst[@name='102']/arr[@name='text']/str='second <em>document</em>'");
+  }
+  
+  public void testEmptySnippet() {
+    assertQ("null snippet test", 
+      req("q", "text:one OR *:*", "sort", "id asc", "hl", "true"),
+        "count(//lst[@name='highlighting']/*)=2",
+        "//lst[@name='highlighting']/lst[@name='101']/arr[@name='text']/str='document <em>one</em>'",
+        "count(//lst[@name='highlighting']/lst[@name='102']/arr[@name='text']/*)=0");
+  }
+  
+  public void testDifferentField() {
+    assertQ("highlighting text3", 
+        req("q", "text3:document", "sort", "id asc", "hl", "true", "hl.fl", "text3"),
+        "count(//lst[@name='highlighting']/*)=2",
+        "//lst[@name='highlighting']/lst[@name='101']/arr[@name='text3']/str='crappy <em>document</em>'",
+        "//lst[@name='highlighting']/lst[@name='102']/arr[@name='text3']/str='crappier <em>document</em>'");
+  }
+  
+  public void testTwoFields() {
+    assertQ("highlighting text and text3", 
+        req("q", "text:document text3:document", "sort", "id asc", "hl", "true", "hl.fl", "text,text3"),
+        "count(//lst[@name='highlighting']/*)=2",
+        "//lst[@name='highlighting']/lst[@name='101']/arr[@name='text']/str='<em>document</em> one'",
+        "//lst[@name='highlighting']/lst[@name='101']/arr[@name='text3']/str='crappy <em>document</em>'",
+        "//lst[@name='highlighting']/lst[@name='102']/arr[@name='text']/str='second <em>document</em>'",
+        "//lst[@name='highlighting']/lst[@name='102']/arr[@name='text3']/str='crappier <em>document</em>'");
+  }
+  
+  public void testMisconfiguredField() {
+    ignoreException("was indexed without offsets");
+    try {
+      assertQ("should fail, has no offsets",
+        req("q", "text2:document", "sort", "id asc", "hl", "true", "hl.fl", "text2"));
+      fail();
+    } catch (Exception expected) {
+      // expected
+    }
+    resetExceptionIgnores();
+  }
+}

